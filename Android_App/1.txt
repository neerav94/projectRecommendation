Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Figure 2: One of the discriminating characteristics of our application is that it can generalize within a class and can ignore
irrelevant characteristics, such as color. Results of recognizing an image containing a Gerbera ﬂower by Google Search-By-
Image service (left), in which all returned images match the color distribution but no Gerbera ﬂower is retrieved, and our
system (right) which correctly identiﬁes the ﬂower species of the query image and also returns somewhat similar categories as
choices 2 and 3. The web-based version of our application is shown here.

phone app1 (Figure 1) and as a web-based service2. While
some reference applications for ﬂowers already exist (e.g.
Flower Pedia, Audubon Wildﬂowers, What Flower, etc), our
app has the unique capability to automatically recognize the
ﬂower from an image. This is of much value to interested
users because no previous application can accomplish such
a task, especially from a mobile device. The system is capa-
ble of recognizing 578 diﬀerent ﬂower species and relies on
web-scale image dataset that has been collected from various
sources and has been carefully labeled by a team of expert
botanists. Our recognition system is speciﬁcally designed
to handle user-generated images collected ‘in the ﬁeld’ by
mobile phone users of various devices. The latter is impor-
tant since user-generated imagery may signiﬁcantly vary in
quality and will diﬀer from the images of ﬂowers that are
encountered on web-search engines. We describe details of
designing the system, developing the algorithms, as well as,
collecting the dataset that can support this capability.

Recognizing among diﬀerent types within the same gen-
eral class of objects is called ﬁne-grained classiﬁcation [13,
22, 28], and is one of the active areas of research in com-
puter vision. Unlike the so called basic-level recognition,
which refers to recognizing whether an object is a ﬂower,
or a cat, or a chair, ﬁne-grained recognition works within a
single base-level category and aims to recognize among its
subcategories, such as diﬀerent species of ﬂowers or diﬀer-
ent species of birds, plants, etc. Fine-grained classiﬁcation
requires knowledge and skills that only trained experts can
provide.
In the case of ﬂower recognition, only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do the task, and some-

1The app is developed for iPhone and is called ‘Mobile
Flora’.
It is currently under invitation-only beta testing,
and will soon be available for general use. Please email the
contact authors to send you an invitation.
2The web-based version of
http://mobileﬂora.heroku.com

the app is available at

times not without using further references [11], or following
a complex identiﬁcation procedure (see Section 4.3). Since
very few people can successfully recognize among a large
number of species, this makes the automatic ﬁne-grained
recognition application very valuable to non-experts.

General object recognition, and more speciﬁcally, ﬁne-
grained recognition, are still problems that are far from be-
ing solved algorithmically. Large-scale image retrieval sys-
tems [3, 4, 7, 25, 29] have been quite successful at retrieving
a duplicate or near-duplicate images [7, 4], similar images [4],
or extracting images ‘by sketch’ [25]. However, they do not
have the capability to generalize within a class, e.g. to recog-
nize that some object is a vacuum cleaner, for example, and
do not have the capabilities to discriminate between very re-
lated but diﬀerent classes of the same super-class, as is the
case with ﬂowers. Furthermore, the computer vision recog-
nition technologies that are available on the web are mostly
targetted towards retrieval of web content, rather than user
generated ‘ﬁeld’ images. With the advent of social media
and mobile phones, naturally, the demand for recognition of
user-generated images is increasing.

Another diﬀerentiating capability between our ﬂower recog-
nition technology and other available image search or image
retrieval systems, e.g. Google’s Search-By-Image service [4]
is that our system is speciﬁcally targeted towards recogni-
tion, in our case of ﬂower species. That is, our system is
trained to identify features that generalize within a class,
whereas other systems are more general and very often re-
turn irrelevant results (i.e. non-ﬂowers) or match the input
ﬂower image by color distribution or other general visual
features, rather than retrieving ﬂowers from the same class.
Figure 2 shows an example search of an image of a Gerbera
ﬂower by Google Search-By-Image service [4] and by our sys-
tem (which can be found at http://mobileﬂora.heroku.com).
As seen, although many images of Gerbera ﬂowers are avail-
able in Google’s image corpus, not a single Gerbera ﬂower
was retrieved. Instead our system is capable of recognizing

Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Figure 2: One of the discriminating characteristics of our application is that it can generalize within a class and can ignore
irrelevant characteristics, such as color. Results of recognizing an image containing a Gerbera ﬂower by Google Search-By-
Image service (left), in which all returned images match the color distribution but no Gerbera ﬂower is retrieved, and our
system (right) which correctly identiﬁes the ﬂower species of the query image and also returns somewhat similar categories as
choices 2 and 3. The web-based version of our application is shown here.

phone app1 (Figure 1) and as a web-based service2. While
some reference applications for ﬂowers already exist (e.g.
Flower Pedia, Audubon Wildﬂowers, What Flower, etc), our
app has the unique capability to automatically recognize the
ﬂower from an image. This is of much value to interested
users because no previous application can accomplish such
a task, especially from a mobile device. The system is capa-
ble of recognizing 578 diﬀerent ﬂower species and relies on
web-scale image dataset that has been collected from various
sources and has been carefully labeled by a team of expert
botanists. Our recognition system is speciﬁcally designed
to handle user-generated images collected ‘in the ﬁeld’ by
mobile phone users of various devices. The latter is impor-
tant since user-generated imagery may signiﬁcantly vary in
quality and will diﬀer from the images of ﬂowers that are
encountered on web-search engines. We describe details of
designing the system, developing the algorithms, as well as,
collecting the dataset that can support this capability.

Recognizing among diﬀerent types within the same gen-
eral class of objects is called ﬁne-grained classiﬁcation [13,
22, 28], and is one of the active areas of research in com-
puter vision. Unlike the so called basic-level recognition,
which refers to recognizing whether an object is a ﬂower,
or a cat, or a chair, ﬁne-grained recognition works within a
single base-level category and aims to recognize among its
subcategories, such as diﬀerent species of ﬂowers or diﬀer-
ent species of birds, plants, etc. Fine-grained classiﬁcation
requires knowledge and skills that only trained experts can
provide.
In the case of ﬂower recognition, only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do the task, and some-

1The app is developed for iPhone and is called ‘Mobile
Flora’.
It is currently under invitation-only beta testing,
and will soon be available for general use. Please email the
contact authors to send you an invitation.
2The web-based version of
http://mobileﬂora.heroku.com

the app is available at

times not without using further references [11], or following
a complex identiﬁcation procedure (see Section 4.3). Since
very few people can successfully recognize among a large
number of species, this makes the automatic ﬁne-grained
recognition application very valuable to non-experts.

General object recognition, and more speciﬁcally, ﬁne-
grained recognition, are still problems that are far from be-
ing solved algorithmically. Large-scale image retrieval sys-
tems [3, 4, 7, 25, 29] have been quite successful at retrieving
a duplicate or near-duplicate images [7, 4], similar images [4],
or extracting images ‘by sketch’ [25]. However, they do not
have the capability to generalize within a class, e.g. to recog-
nize that some object is a vacuum cleaner, for example, and
do not have the capabilities to discriminate between very re-
lated but diﬀerent classes of the same super-class, as is the
case with ﬂowers. Furthermore, the computer vision recog-
nition technologies that are available on the web are mostly
targetted towards retrieval of web content, rather than user
generated ‘ﬁeld’ images. With the advent of social media
and mobile phones, naturally, the demand for recognition of
user-generated images is increasing.

Another diﬀerentiating capability between our ﬂower recog-
nition technology and other available image search or image
retrieval systems, e.g. Google’s Search-By-Image service [4]
is that our system is speciﬁcally targeted towards recogni-
tion, in our case of ﬂower species. That is, our system is
trained to identify features that generalize within a class,
whereas other systems are more general and very often re-
turn irrelevant results (i.e. non-ﬂowers) or match the input
ﬂower image by color distribution or other general visual
features, rather than retrieving ﬂowers from the same class.
Figure 2 shows an example search of an image of a Gerbera
ﬂower by Google Search-By-Image service [4] and by our sys-
tem (which can be found at http://mobileﬂora.heroku.com).
As seen, although many images of Gerbera ﬂowers are avail-
able in Google’s image corpus, not a single Gerbera ﬂower
was retrieved. Instead our system is capable of recognizing

the type of ﬂower and providing related images. We note
here that this example shows typical retrieval diﬀerences, es-
pecially for ﬂower images that do not have a near-duplicate
counterpart on the web, e.g. images in personal collections,
or obtained by mobile devices, etc.

2. FINE-GRAINED RECOGNITION

CHALLENGES

Fine-grained classiﬁcation has its unique challenges be-
cause it aims to recognize among classes that will typically
be considered as a part of one large class in standard visual
object recognition tasks.

The main inherent challenge is that some species have
very ﬁne diﬀerences which are hard to notice for the com-
mon eye. For example, the categories that need to be dis-
criminated among each other can be very similar and would
require careful examination to determine the correct class.
Figure 3 shows examples from three diﬀerent ﬂower species:
Matricaria chamomilla (chamomile), Bellis perennis (com-
mon daisy), and Leucanthemum vulgare (ox-eye daisy). As
seen these three categories are very similar to each other.

Another challenge in a ﬁne-grained recognition task is the
variability in viewpoints, colors, and scales, that are encoun-
tered within the same class. These variabilities are mostly
problematic to an automatic system rather than a human
observer. Many ﬂowers can have diﬀerent colors but a per-
son can very easily factor out that invariance, which is not
necessarily true for an algorithm. Similarly, it is very easy
for a person to understand if the image contains a close-
up shot of a single ﬂower or multiple small ﬂowers of the
same type, but this problem can be quite challenging for an
automatic vision system.

Other potential issues speciﬁc to an automatic recognition
system is the background, which can serve as a distractor,
e.g. when the system assumes it is part of the ﬂower, occlu-
sions over the ﬂowers, and others.

3. FLOWER RECOGNITION

APPLICATION

Our system and the mobile phone app are intended to
recognize the image of a ﬂower of a blooming plant or tree.
Figure 7 of the Appendix shows the main functionalities of
the app.

The entry point of the app is to take a picture of a ﬂower
and submit it for recognition (Figure 7a, and also Figure 1).
While taking the picture, a ﬂower contour is overlayed to
indicate that the ﬂower is expected to take the full image
and be within the boundaries of the image. These are only
guidelines and, as our users will discover, the app is much
more powerful and is capable of recognizing a ﬂower that
takes signiﬁcantly more or less of the area that is desirable.
Multiple ﬂowers of the same type (also called ‘cluster ﬂow-
ers’) are also not a problem for recognition.

Results of the top ﬁve choices of potential classes that the
input image can belong to are shown to the user (Figure 7b).
Since some classes may be very similar, as seen in Figure 3,
we found it much more useful to provide several choices,
instead of the top one only.

After the user is oﬀered a set of options of what the input
ﬂower could be, they can examine and compare the images
from the proposed categories (Figure 7c) and make a deci-
sion which class the ﬂower belongs to (Figure 7d). Here, in

Figure 3: One of the challenges for ﬁne-grained recognition is
that some classes can be very similar to one another and only
experts can discriminate them. This makes an automatic
recognition system very valuable to non-experts. Each row
shows a diﬀerent ﬂower species.

our application, we have taken advantage of the fact that
people may not be very good at recognizing a particular
ﬂower, especially among such large variety of 578 diﬀerent
species, but when presented with a set of options they can
be pretty good at verifying which one it is.

More results are available upon request, but we provide
top ten choices at most to avoid showing too many irrelevant
results, since the more results shown the more likely they
will be very diﬀerent from the input category. If the top ten
results do not provide satisfactory choice, the users can give
a name to the ﬂower, if they like (Figure 7e). Otherwise the
ﬂower is stored in their album as an ‘unknown’ category. An
example user album is shown in Figure 7f.

For each ﬂower, which has been identiﬁed, we provided
its common name and a short description about this ﬂower,
e.g. where it originated from,
its uses, and some trivia
information (Figure 7g). We also provide wikipedia link
that describes this ﬂower. Additionally we provide informa-
tion about regions world-wide where the ﬂower grows (Fig-
ure 7h). All this information has been compiled by our team
of expert biologists. We further plan to provide information
about how to grow the ﬂower for interested users, as well as,
more reﬁned information about the regions where it grows.
We also show the images that the user has collected for a
ﬂower category (Figure 7i).

4. FLOWER DATASET

For the purposes of developing the ﬂower recognition ap-
plication, we have collected a large-scale ﬂower dataset. The
dataset contains 578 diﬀerent ﬂower species and over 250,000
images. This is the only web-scale ﬂowers dataset we are
aware of. While we have compiled a seemingly modest num-
ber of images (250,000 compared to billions of images for
general image search engines), our dataset has exhausted the

Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Figure 2: One of the discriminating characteristics of our application is that it can generalize within a class and can ignore
irrelevant characteristics, such as color. Results of recognizing an image containing a Gerbera ﬂower by Google Search-By-
Image service (left), in which all returned images match the color distribution but no Gerbera ﬂower is retrieved, and our
system (right) which correctly identiﬁes the ﬂower species of the query image and also returns somewhat similar categories as
choices 2 and 3. The web-based version of our application is shown here.

phone app1 (Figure 1) and as a web-based service2. While
some reference applications for ﬂowers already exist (e.g.
Flower Pedia, Audubon Wildﬂowers, What Flower, etc), our
app has the unique capability to automatically recognize the
ﬂower from an image. This is of much value to interested
users because no previous application can accomplish such
a task, especially from a mobile device. The system is capa-
ble of recognizing 578 diﬀerent ﬂower species and relies on
web-scale image dataset that has been collected from various
sources and has been carefully labeled by a team of expert
botanists. Our recognition system is speciﬁcally designed
to handle user-generated images collected ‘in the ﬁeld’ by
mobile phone users of various devices. The latter is impor-
tant since user-generated imagery may signiﬁcantly vary in
quality and will diﬀer from the images of ﬂowers that are
encountered on web-search engines. We describe details of
designing the system, developing the algorithms, as well as,
collecting the dataset that can support this capability.

Recognizing among diﬀerent types within the same gen-
eral class of objects is called ﬁne-grained classiﬁcation [13,
22, 28], and is one of the active areas of research in com-
puter vision. Unlike the so called basic-level recognition,
which refers to recognizing whether an object is a ﬂower,
or a cat, or a chair, ﬁne-grained recognition works within a
single base-level category and aims to recognize among its
subcategories, such as diﬀerent species of ﬂowers or diﬀer-
ent species of birds, plants, etc. Fine-grained classiﬁcation
requires knowledge and skills that only trained experts can
provide.
In the case of ﬂower recognition, only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do the task, and some-

1The app is developed for iPhone and is called ‘Mobile
Flora’.
It is currently under invitation-only beta testing,
and will soon be available for general use. Please email the
contact authors to send you an invitation.
2The web-based version of
http://mobileﬂora.heroku.com

the app is available at

times not without using further references [11], or following
a complex identiﬁcation procedure (see Section 4.3). Since
very few people can successfully recognize among a large
number of species, this makes the automatic ﬁne-grained
recognition application very valuable to non-experts.

General object recognition, and more speciﬁcally, ﬁne-
grained recognition, are still problems that are far from be-
ing solved algorithmically. Large-scale image retrieval sys-
tems [3, 4, 7, 25, 29] have been quite successful at retrieving
a duplicate or near-duplicate images [7, 4], similar images [4],
or extracting images ‘by sketch’ [25]. However, they do not
have the capability to generalize within a class, e.g. to recog-
nize that some object is a vacuum cleaner, for example, and
do not have the capabilities to discriminate between very re-
lated but diﬀerent classes of the same super-class, as is the
case with ﬂowers. Furthermore, the computer vision recog-
nition technologies that are available on the web are mostly
targetted towards retrieval of web content, rather than user
generated ‘ﬁeld’ images. With the advent of social media
and mobile phones, naturally, the demand for recognition of
user-generated images is increasing.

Another diﬀerentiating capability between our ﬂower recog-
nition technology and other available image search or image
retrieval systems, e.g. Google’s Search-By-Image service [4]
is that our system is speciﬁcally targeted towards recogni-
tion, in our case of ﬂower species. That is, our system is
trained to identify features that generalize within a class,
whereas other systems are more general and very often re-
turn irrelevant results (i.e. non-ﬂowers) or match the input
ﬂower image by color distribution or other general visual
features, rather than retrieving ﬂowers from the same class.
Figure 2 shows an example search of an image of a Gerbera
ﬂower by Google Search-By-Image service [4] and by our sys-
tem (which can be found at http://mobileﬂora.heroku.com).
As seen, although many images of Gerbera ﬂowers are avail-
able in Google’s image corpus, not a single Gerbera ﬂower
was retrieved. Instead our system is capable of recognizing

the type of ﬂower and providing related images. We note
here that this example shows typical retrieval diﬀerences, es-
pecially for ﬂower images that do not have a near-duplicate
counterpart on the web, e.g. images in personal collections,
or obtained by mobile devices, etc.

2. FINE-GRAINED RECOGNITION

CHALLENGES

Fine-grained classiﬁcation has its unique challenges be-
cause it aims to recognize among classes that will typically
be considered as a part of one large class in standard visual
object recognition tasks.

The main inherent challenge is that some species have
very ﬁne diﬀerences which are hard to notice for the com-
mon eye. For example, the categories that need to be dis-
criminated among each other can be very similar and would
require careful examination to determine the correct class.
Figure 3 shows examples from three diﬀerent ﬂower species:
Matricaria chamomilla (chamomile), Bellis perennis (com-
mon daisy), and Leucanthemum vulgare (ox-eye daisy). As
seen these three categories are very similar to each other.

Another challenge in a ﬁne-grained recognition task is the
variability in viewpoints, colors, and scales, that are encoun-
tered within the same class. These variabilities are mostly
problematic to an automatic system rather than a human
observer. Many ﬂowers can have diﬀerent colors but a per-
son can very easily factor out that invariance, which is not
necessarily true for an algorithm. Similarly, it is very easy
for a person to understand if the image contains a close-
up shot of a single ﬂower or multiple small ﬂowers of the
same type, but this problem can be quite challenging for an
automatic vision system.

Other potential issues speciﬁc to an automatic recognition
system is the background, which can serve as a distractor,
e.g. when the system assumes it is part of the ﬂower, occlu-
sions over the ﬂowers, and others.

3. FLOWER RECOGNITION

APPLICATION

Our system and the mobile phone app are intended to
recognize the image of a ﬂower of a blooming plant or tree.
Figure 7 of the Appendix shows the main functionalities of
the app.

The entry point of the app is to take a picture of a ﬂower
and submit it for recognition (Figure 7a, and also Figure 1).
While taking the picture, a ﬂower contour is overlayed to
indicate that the ﬂower is expected to take the full image
and be within the boundaries of the image. These are only
guidelines and, as our users will discover, the app is much
more powerful and is capable of recognizing a ﬂower that
takes signiﬁcantly more or less of the area that is desirable.
Multiple ﬂowers of the same type (also called ‘cluster ﬂow-
ers’) are also not a problem for recognition.

Results of the top ﬁve choices of potential classes that the
input image can belong to are shown to the user (Figure 7b).
Since some classes may be very similar, as seen in Figure 3,
we found it much more useful to provide several choices,
instead of the top one only.

After the user is oﬀered a set of options of what the input
ﬂower could be, they can examine and compare the images
from the proposed categories (Figure 7c) and make a deci-
sion which class the ﬂower belongs to (Figure 7d). Here, in

Figure 3: One of the challenges for ﬁne-grained recognition is
that some classes can be very similar to one another and only
experts can discriminate them. This makes an automatic
recognition system very valuable to non-experts. Each row
shows a diﬀerent ﬂower species.

our application, we have taken advantage of the fact that
people may not be very good at recognizing a particular
ﬂower, especially among such large variety of 578 diﬀerent
species, but when presented with a set of options they can
be pretty good at verifying which one it is.

More results are available upon request, but we provide
top ten choices at most to avoid showing too many irrelevant
results, since the more results shown the more likely they
will be very diﬀerent from the input category. If the top ten
results do not provide satisfactory choice, the users can give
a name to the ﬂower, if they like (Figure 7e). Otherwise the
ﬂower is stored in their album as an ‘unknown’ category. An
example user album is shown in Figure 7f.

For each ﬂower, which has been identiﬁed, we provided
its common name and a short description about this ﬂower,
e.g. where it originated from,
its uses, and some trivia
information (Figure 7g). We also provide wikipedia link
that describes this ﬂower. Additionally we provide informa-
tion about regions world-wide where the ﬂower grows (Fig-
ure 7h). All this information has been compiled by our team
of expert biologists. We further plan to provide information
about how to grow the ﬂower for interested users, as well as,
more reﬁned information about the regions where it grows.
We also show the images that the user has collected for a
ﬂower category (Figure 7i).

4. FLOWER DATASET

For the purposes of developing the ﬂower recognition ap-
plication, we have collected a large-scale ﬂower dataset. The
dataset contains 578 diﬀerent ﬂower species and over 250,000
images. This is the only web-scale ﬂowers dataset we are
aware of. While we have compiled a seemingly modest num-
ber of images (250,000 compared to billions of images for
general image search engines), our dataset has exhausted the

Figure 4: Example images from diﬀerent classes from the large-scale 578 ﬂower species dataset. A large variety of ﬂower
classes, as well as, intra-class variabilities, inter-class similarities, and changes in ﬂower scales are available in this dataset.

web resources of available ﬂower images, and for some rare
species we could not ﬁnd a suﬃcient number of images. Fur-
thermore, this is the largest dataset for ﬂower recognition,
and also the largest of its kind among any other ﬁne-grained
recognition datasets we are aware of [13, 20, 22, 24, 28]. All
previous datasets contain at most 200 diﬀerent species [28]
and the largest known ﬂower dataset contains 102 classes
and 8189 images [22]. Also compared to previous ﬂower
datasets, our dataset is targeted to be used ‘in the ﬁeld’ by
actual users, rather than for experimentation only. Thus
our dataset contains a lot of intra-class variability, changes
to scale, and quality variability than any previous dataset.
Figure 4 shows example images from diﬀerent classes, and
as seen the images are much more realistic, which makes
the recognition task harder. On the other hand, this data
is much closer to the data distribution an actual user will
encounter as they collect images by their phones and submit
them to the app.

Collecting, deﬁning and organizing the dataset for this
application was a very challenging task, which is why we
describe those eﬀorts here.

4.1 Deﬁning the set of classes

The ﬁrst important question to address is to determine
which set of classes the system is going to recognize. While
there are many ﬂowering plants that potentially exist world-
wide [6], the largest number of species that an expert botanist
is able to recognize is estimated to be about 30,000. Natu-
rally, here we are striving towards recognizing as many ﬂower
species as possible, so as to achieve a maximal coverage of

ﬂowers that users may encounter. However, some real-life
limitations exist. One issue is that collecting and curating
images from such a dataset would be prohibitive, especially
since only ﬂower experts can do the identiﬁcation and cu-
ration (i.e.
labeling, or placing into categories, and giving
the correct scientiﬁc name). A more serious issue is that
there are no suﬃcient number of images for the less com-
mon ﬂower categories. Furthermore, such a large number
of classes is also pushing the boundaries of what we can do
technologically to recognize automatically.

To address this problem, we focused on the most common
ﬂowers which have suﬃciently large coverage over all possi-
ble ﬂowers, and ignored the ’long tail’ set of ﬂowers. More
speciﬁcally, our goal is to select the most frequent classes
which cover 90% of all ﬂowers world-wide. Since, it is not
clear how to estimate such a set, we deﬁned the classes in two
steps. First, our team selected the set of top 500 most pop-
ular ﬂowers in the world. We further collected images from
the web when typing general ﬂower-related terms such as
’ﬂowers’, ’desert ﬂowers’, ’mountain ﬂowers’, ’wedding ﬂow-
ers’, ’spring ﬂowers’, ’exotic ﬂowers’, etc. The latter was an
attempt to retrieve ‘all possible’ types of ﬂowers and their
natural distribution. In this way we could only capture the
ﬂower distribution that occurs on the web, which reﬂects
areas of the world with at least some technology penetra-
tion, rather than the actual distribution which is unknown,
but this was a good approximation, given that the users of
our mobile app will also be representative of technologically
advanced regions. After collecting such a set, the biologists
on our team labeled each image and identiﬁed ﬂowers that

Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Figure 2: One of the discriminating characteristics of our application is that it can generalize within a class and can ignore
irrelevant characteristics, such as color. Results of recognizing an image containing a Gerbera ﬂower by Google Search-By-
Image service (left), in which all returned images match the color distribution but no Gerbera ﬂower is retrieved, and our
system (right) which correctly identiﬁes the ﬂower species of the query image and also returns somewhat similar categories as
choices 2 and 3. The web-based version of our application is shown here.

phone app1 (Figure 1) and as a web-based service2. While
some reference applications for ﬂowers already exist (e.g.
Flower Pedia, Audubon Wildﬂowers, What Flower, etc), our
app has the unique capability to automatically recognize the
ﬂower from an image. This is of much value to interested
users because no previous application can accomplish such
a task, especially from a mobile device. The system is capa-
ble of recognizing 578 diﬀerent ﬂower species and relies on
web-scale image dataset that has been collected from various
sources and has been carefully labeled by a team of expert
botanists. Our recognition system is speciﬁcally designed
to handle user-generated images collected ‘in the ﬁeld’ by
mobile phone users of various devices. The latter is impor-
tant since user-generated imagery may signiﬁcantly vary in
quality and will diﬀer from the images of ﬂowers that are
encountered on web-search engines. We describe details of
designing the system, developing the algorithms, as well as,
collecting the dataset that can support this capability.

Recognizing among diﬀerent types within the same gen-
eral class of objects is called ﬁne-grained classiﬁcation [13,
22, 28], and is one of the active areas of research in com-
puter vision. Unlike the so called basic-level recognition,
which refers to recognizing whether an object is a ﬂower,
or a cat, or a chair, ﬁne-grained recognition works within a
single base-level category and aims to recognize among its
subcategories, such as diﬀerent species of ﬂowers or diﬀer-
ent species of birds, plants, etc. Fine-grained classiﬁcation
requires knowledge and skills that only trained experts can
provide.
In the case of ﬂower recognition, only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do the task, and some-

1The app is developed for iPhone and is called ‘Mobile
Flora’.
It is currently under invitation-only beta testing,
and will soon be available for general use. Please email the
contact authors to send you an invitation.
2The web-based version of
http://mobileﬂora.heroku.com

the app is available at

times not without using further references [11], or following
a complex identiﬁcation procedure (see Section 4.3). Since
very few people can successfully recognize among a large
number of species, this makes the automatic ﬁne-grained
recognition application very valuable to non-experts.

General object recognition, and more speciﬁcally, ﬁne-
grained recognition, are still problems that are far from be-
ing solved algorithmically. Large-scale image retrieval sys-
tems [3, 4, 7, 25, 29] have been quite successful at retrieving
a duplicate or near-duplicate images [7, 4], similar images [4],
or extracting images ‘by sketch’ [25]. However, they do not
have the capability to generalize within a class, e.g. to recog-
nize that some object is a vacuum cleaner, for example, and
do not have the capabilities to discriminate between very re-
lated but diﬀerent classes of the same super-class, as is the
case with ﬂowers. Furthermore, the computer vision recog-
nition technologies that are available on the web are mostly
targetted towards retrieval of web content, rather than user
generated ‘ﬁeld’ images. With the advent of social media
and mobile phones, naturally, the demand for recognition of
user-generated images is increasing.

Another diﬀerentiating capability between our ﬂower recog-
nition technology and other available image search or image
retrieval systems, e.g. Google’s Search-By-Image service [4]
is that our system is speciﬁcally targeted towards recogni-
tion, in our case of ﬂower species. That is, our system is
trained to identify features that generalize within a class,
whereas other systems are more general and very often re-
turn irrelevant results (i.e. non-ﬂowers) or match the input
ﬂower image by color distribution or other general visual
features, rather than retrieving ﬂowers from the same class.
Figure 2 shows an example search of an image of a Gerbera
ﬂower by Google Search-By-Image service [4] and by our sys-
tem (which can be found at http://mobileﬂora.heroku.com).
As seen, although many images of Gerbera ﬂowers are avail-
able in Google’s image corpus, not a single Gerbera ﬂower
was retrieved. Instead our system is capable of recognizing

the type of ﬂower and providing related images. We note
here that this example shows typical retrieval diﬀerences, es-
pecially for ﬂower images that do not have a near-duplicate
counterpart on the web, e.g. images in personal collections,
or obtained by mobile devices, etc.

2. FINE-GRAINED RECOGNITION

CHALLENGES

Fine-grained classiﬁcation has its unique challenges be-
cause it aims to recognize among classes that will typically
be considered as a part of one large class in standard visual
object recognition tasks.

The main inherent challenge is that some species have
very ﬁne diﬀerences which are hard to notice for the com-
mon eye. For example, the categories that need to be dis-
criminated among each other can be very similar and would
require careful examination to determine the correct class.
Figure 3 shows examples from three diﬀerent ﬂower species:
Matricaria chamomilla (chamomile), Bellis perennis (com-
mon daisy), and Leucanthemum vulgare (ox-eye daisy). As
seen these three categories are very similar to each other.

Another challenge in a ﬁne-grained recognition task is the
variability in viewpoints, colors, and scales, that are encoun-
tered within the same class. These variabilities are mostly
problematic to an automatic system rather than a human
observer. Many ﬂowers can have diﬀerent colors but a per-
son can very easily factor out that invariance, which is not
necessarily true for an algorithm. Similarly, it is very easy
for a person to understand if the image contains a close-
up shot of a single ﬂower or multiple small ﬂowers of the
same type, but this problem can be quite challenging for an
automatic vision system.

Other potential issues speciﬁc to an automatic recognition
system is the background, which can serve as a distractor,
e.g. when the system assumes it is part of the ﬂower, occlu-
sions over the ﬂowers, and others.

3. FLOWER RECOGNITION

APPLICATION

Our system and the mobile phone app are intended to
recognize the image of a ﬂower of a blooming plant or tree.
Figure 7 of the Appendix shows the main functionalities of
the app.

The entry point of the app is to take a picture of a ﬂower
and submit it for recognition (Figure 7a, and also Figure 1).
While taking the picture, a ﬂower contour is overlayed to
indicate that the ﬂower is expected to take the full image
and be within the boundaries of the image. These are only
guidelines and, as our users will discover, the app is much
more powerful and is capable of recognizing a ﬂower that
takes signiﬁcantly more or less of the area that is desirable.
Multiple ﬂowers of the same type (also called ‘cluster ﬂow-
ers’) are also not a problem for recognition.

Results of the top ﬁve choices of potential classes that the
input image can belong to are shown to the user (Figure 7b).
Since some classes may be very similar, as seen in Figure 3,
we found it much more useful to provide several choices,
instead of the top one only.

After the user is oﬀered a set of options of what the input
ﬂower could be, they can examine and compare the images
from the proposed categories (Figure 7c) and make a deci-
sion which class the ﬂower belongs to (Figure 7d). Here, in

Figure 3: One of the challenges for ﬁne-grained recognition is
that some classes can be very similar to one another and only
experts can discriminate them. This makes an automatic
recognition system very valuable to non-experts. Each row
shows a diﬀerent ﬂower species.

our application, we have taken advantage of the fact that
people may not be very good at recognizing a particular
ﬂower, especially among such large variety of 578 diﬀerent
species, but when presented with a set of options they can
be pretty good at verifying which one it is.

More results are available upon request, but we provide
top ten choices at most to avoid showing too many irrelevant
results, since the more results shown the more likely they
will be very diﬀerent from the input category. If the top ten
results do not provide satisfactory choice, the users can give
a name to the ﬂower, if they like (Figure 7e). Otherwise the
ﬂower is stored in their album as an ‘unknown’ category. An
example user album is shown in Figure 7f.

For each ﬂower, which has been identiﬁed, we provided
its common name and a short description about this ﬂower,
e.g. where it originated from,
its uses, and some trivia
information (Figure 7g). We also provide wikipedia link
that describes this ﬂower. Additionally we provide informa-
tion about regions world-wide where the ﬂower grows (Fig-
ure 7h). All this information has been compiled by our team
of expert biologists. We further plan to provide information
about how to grow the ﬂower for interested users, as well as,
more reﬁned information about the regions where it grows.
We also show the images that the user has collected for a
ﬂower category (Figure 7i).

4. FLOWER DATASET

For the purposes of developing the ﬂower recognition ap-
plication, we have collected a large-scale ﬂower dataset. The
dataset contains 578 diﬀerent ﬂower species and over 250,000
images. This is the only web-scale ﬂowers dataset we are
aware of. While we have compiled a seemingly modest num-
ber of images (250,000 compared to billions of images for
general image search engines), our dataset has exhausted the

Figure 4: Example images from diﬀerent classes from the large-scale 578 ﬂower species dataset. A large variety of ﬂower
classes, as well as, intra-class variabilities, inter-class similarities, and changes in ﬂower scales are available in this dataset.

web resources of available ﬂower images, and for some rare
species we could not ﬁnd a suﬃcient number of images. Fur-
thermore, this is the largest dataset for ﬂower recognition,
and also the largest of its kind among any other ﬁne-grained
recognition datasets we are aware of [13, 20, 22, 24, 28]. All
previous datasets contain at most 200 diﬀerent species [28]
and the largest known ﬂower dataset contains 102 classes
and 8189 images [22]. Also compared to previous ﬂower
datasets, our dataset is targeted to be used ‘in the ﬁeld’ by
actual users, rather than for experimentation only. Thus
our dataset contains a lot of intra-class variability, changes
to scale, and quality variability than any previous dataset.
Figure 4 shows example images from diﬀerent classes, and
as seen the images are much more realistic, which makes
the recognition task harder. On the other hand, this data
is much closer to the data distribution an actual user will
encounter as they collect images by their phones and submit
them to the app.

Collecting, deﬁning and organizing the dataset for this
application was a very challenging task, which is why we
describe those eﬀorts here.

4.1 Deﬁning the set of classes

The ﬁrst important question to address is to determine
which set of classes the system is going to recognize. While
there are many ﬂowering plants that potentially exist world-
wide [6], the largest number of species that an expert botanist
is able to recognize is estimated to be about 30,000. Natu-
rally, here we are striving towards recognizing as many ﬂower
species as possible, so as to achieve a maximal coverage of

ﬂowers that users may encounter. However, some real-life
limitations exist. One issue is that collecting and curating
images from such a dataset would be prohibitive, especially
since only ﬂower experts can do the identiﬁcation and cu-
ration (i.e.
labeling, or placing into categories, and giving
the correct scientiﬁc name). A more serious issue is that
there are no suﬃcient number of images for the less com-
mon ﬂower categories. Furthermore, such a large number
of classes is also pushing the boundaries of what we can do
technologically to recognize automatically.

To address this problem, we focused on the most common
ﬂowers which have suﬃciently large coverage over all possi-
ble ﬂowers, and ignored the ’long tail’ set of ﬂowers. More
speciﬁcally, our goal is to select the most frequent classes
which cover 90% of all ﬂowers world-wide. Since, it is not
clear how to estimate such a set, we deﬁned the classes in two
steps. First, our team selected the set of top 500 most pop-
ular ﬂowers in the world. We further collected images from
the web when typing general ﬂower-related terms such as
’ﬂowers’, ’desert ﬂowers’, ’mountain ﬂowers’, ’wedding ﬂow-
ers’, ’spring ﬂowers’, ’exotic ﬂowers’, etc. The latter was an
attempt to retrieve ‘all possible’ types of ﬂowers and their
natural distribution. In this way we could only capture the
ﬂower distribution that occurs on the web, which reﬂects
areas of the world with at least some technology penetra-
tion, rather than the actual distribution which is unknown,
but this was a good approximation, given that the users of
our mobile app will also be representative of technologically
advanced regions. After collecting such a set, the biologists
on our team labeled each image and identiﬁed ﬂowers that

are not included in our pre-deﬁned set of classes. We then
included the next most common ﬂower species, until their
coverage reached 90%. As a result of that we obtained a set
of 578 species of ﬂowers.

This, of course, is only approximately capturing the full
coverage of our application, and only after actual users start
to use the application, will we know if our coverage is suﬃ-
cient. Naturally, our set of classes can evolve with usage of
the app, and this can improve the usefulness of the applica-
tion, as well.
4.2 Data collection

Since we are developing user-based application which is
going to work with user provided data, we do not necessar-
ily have such data to begin with. Here we describe how we
bootstrap the mechanism for data collection so that to im-
rove the data quality for our app. Initially we use images
collected from the web (Section 4.2.1), then we collected
user data by crowd-sourcing (Section 4.2.3), ﬁnally, after
the application is launched we can collect user-uploaded im-
ages (Section 7). In a sense the dataset evolves until we can
serve the users best.

4.2.1 Web crawling
Initially we collected data from the web, by automatically
downloading images corresponding to each ﬂower category
by keyword search on Bing and Google search engines. We
typically used the oﬃcial name of the ﬂower (e.g.
‘matri-
caria recutita’) and one or more of its common names (e.g.
chamomile). Online search engines were very helpful to pro-
vide ﬂower images, however since the search is done by key-
words, a lot of the returned images are irrelevant to the
ﬂower category and need to be ﬁltered out.

4.2.2 Near-duplicate elimination
Near-duplicate elimination is a process which removes ex-
amples that are visually very similar or almost identical.
Typically near-duplicates are images of the same object but
one of them has been modiﬁed in some way, e.g. by rescal-
ing, compression, by adding noise, or by adding a logo or
modifying a part of the image. Near-duplicate elimination
is needed because our application shows a set of images for
each proposed class and the user can look at them and de-
cide if the class is the best match to the query ﬂower. In
this situation, we really want to show diverse images. Sec-
ondly, near-duplicate images have almost identical feature
representations and are not useful for training or testing of
our algorithm.

For near-duplicate elimination we have extracted HOG
features [10] in the whole image. These features are then
encoded in 8196-dimensional global feature dictionary using
the Local Linear Coding method [26]. All the features in the
image are combined by a global max-pooling [26] to obtain a
ﬁnal 8196-dimensional representation similar to [21]. This is
essentially a signature-like summarization of the whole im-
age, so two images which diﬀer only due to rescaling, some
noise or re-compression will likely have very similar signa-
tures. Images whose signatures diﬀer by less than a prespec-
iﬁed threshold are eliminated as duplicates and a represen-
tative single image is kept in the ﬁnal collection. Note that
the feature representation is derived from the more complex
representation, presented later in Section 5 which is used for
ﬁnal classiﬁcation into diﬀerent classes.

Figure 5: Example images of a rose returned by Bing Search
Engine (top panel), example images of a rose obtained by a
mobile phone (bottom row). As seen, the quality of these
images is very diﬀerent, so images obtained online will not
be suﬃcient to cover the test distribution of mobile images.

We are not aware of any other system using the method
for near-duplicate detection described above and we chose
it for simplicity and ease of application. There are other
near-duplicate detection methods that have been proposed
and used in practice [17, 19], but we found the method we
have used as suﬃcient for our goals.

4.2.3 Collecting data via crowd-sourcing
Since our data were collected from web sources, naturally,
their distribution is very diﬀerent from the distribution of
images provided by mobile phone users. Figure 5 shows
example image for a ‘rose’ that are returned by Bing search
engine [2] and other examples which are collected ‘in the
ﬁeld’ by a mobile device. As seen those are very diﬀerent.
Therefore, we needed to collect extra images which are closer
to the user-generated content by mobile devices.

For that purpose we used a crowd-sourcing service for data
collection. Apart from obtaining images closer to the images
by our potential app user, we are aiming to get diversity both
in geographical locations and also from user to user (e.g. a
variety of user demographics, photography skill levels, etc.).
Mobile phone users on a crowd-sourcing service were in-
structed to capture ﬂower images. We gave fairly loose
guidelines so that they maximally match the users of the
app, who will not be given instructions but will be guided
by the UI to take ﬂower images for recognition. Most of the
users did what is expected, but for some users we had to
reﬁne and reinforce the instructions mostly for the sake of
collecting useful data for our algorithms. In general, what
we learned in the process was that we need to set up fairly
simple and straightforward requirements for crowd-sourcing
users.

Overall we are very pleased with the dataset collected from
crowd-coursing: some of the agents paid to collect images
were very industrious and creative and provided images of
exotic and rare plants, which they have collected from vaca-
tions or by visiting botanical gardens and arboreta. Figure 6

Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Figure 2: One of the discriminating characteristics of our application is that it can generalize within a class and can ignore
irrelevant characteristics, such as color. Results of recognizing an image containing a Gerbera ﬂower by Google Search-By-
Image service (left), in which all returned images match the color distribution but no Gerbera ﬂower is retrieved, and our
system (right) which correctly identiﬁes the ﬂower species of the query image and also returns somewhat similar categories as
choices 2 and 3. The web-based version of our application is shown here.

phone app1 (Figure 1) and as a web-based service2. While
some reference applications for ﬂowers already exist (e.g.
Flower Pedia, Audubon Wildﬂowers, What Flower, etc), our
app has the unique capability to automatically recognize the
ﬂower from an image. This is of much value to interested
users because no previous application can accomplish such
a task, especially from a mobile device. The system is capa-
ble of recognizing 578 diﬀerent ﬂower species and relies on
web-scale image dataset that has been collected from various
sources and has been carefully labeled by a team of expert
botanists. Our recognition system is speciﬁcally designed
to handle user-generated images collected ‘in the ﬁeld’ by
mobile phone users of various devices. The latter is impor-
tant since user-generated imagery may signiﬁcantly vary in
quality and will diﬀer from the images of ﬂowers that are
encountered on web-search engines. We describe details of
designing the system, developing the algorithms, as well as,
collecting the dataset that can support this capability.

Recognizing among diﬀerent types within the same gen-
eral class of objects is called ﬁne-grained classiﬁcation [13,
22, 28], and is one of the active areas of research in com-
puter vision. Unlike the so called basic-level recognition,
which refers to recognizing whether an object is a ﬂower,
or a cat, or a chair, ﬁne-grained recognition works within a
single base-level category and aims to recognize among its
subcategories, such as diﬀerent species of ﬂowers or diﬀer-
ent species of birds, plants, etc. Fine-grained classiﬁcation
requires knowledge and skills that only trained experts can
provide.
In the case of ﬂower recognition, only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do the task, and some-

1The app is developed for iPhone and is called ‘Mobile
Flora’.
It is currently under invitation-only beta testing,
and will soon be available for general use. Please email the
contact authors to send you an invitation.
2The web-based version of
http://mobileﬂora.heroku.com

the app is available at

times not without using further references [11], or following
a complex identiﬁcation procedure (see Section 4.3). Since
very few people can successfully recognize among a large
number of species, this makes the automatic ﬁne-grained
recognition application very valuable to non-experts.

General object recognition, and more speciﬁcally, ﬁne-
grained recognition, are still problems that are far from be-
ing solved algorithmically. Large-scale image retrieval sys-
tems [3, 4, 7, 25, 29] have been quite successful at retrieving
a duplicate or near-duplicate images [7, 4], similar images [4],
or extracting images ‘by sketch’ [25]. However, they do not
have the capability to generalize within a class, e.g. to recog-
nize that some object is a vacuum cleaner, for example, and
do not have the capabilities to discriminate between very re-
lated but diﬀerent classes of the same super-class, as is the
case with ﬂowers. Furthermore, the computer vision recog-
nition technologies that are available on the web are mostly
targetted towards retrieval of web content, rather than user
generated ‘ﬁeld’ images. With the advent of social media
and mobile phones, naturally, the demand for recognition of
user-generated images is increasing.

Another diﬀerentiating capability between our ﬂower recog-
nition technology and other available image search or image
retrieval systems, e.g. Google’s Search-By-Image service [4]
is that our system is speciﬁcally targeted towards recogni-
tion, in our case of ﬂower species. That is, our system is
trained to identify features that generalize within a class,
whereas other systems are more general and very often re-
turn irrelevant results (i.e. non-ﬂowers) or match the input
ﬂower image by color distribution or other general visual
features, rather than retrieving ﬂowers from the same class.
Figure 2 shows an example search of an image of a Gerbera
ﬂower by Google Search-By-Image service [4] and by our sys-
tem (which can be found at http://mobileﬂora.heroku.com).
As seen, although many images of Gerbera ﬂowers are avail-
able in Google’s image corpus, not a single Gerbera ﬂower
was retrieved. Instead our system is capable of recognizing

the type of ﬂower and providing related images. We note
here that this example shows typical retrieval diﬀerences, es-
pecially for ﬂower images that do not have a near-duplicate
counterpart on the web, e.g. images in personal collections,
or obtained by mobile devices, etc.

2. FINE-GRAINED RECOGNITION

CHALLENGES

Fine-grained classiﬁcation has its unique challenges be-
cause it aims to recognize among classes that will typically
be considered as a part of one large class in standard visual
object recognition tasks.

The main inherent challenge is that some species have
very ﬁne diﬀerences which are hard to notice for the com-
mon eye. For example, the categories that need to be dis-
criminated among each other can be very similar and would
require careful examination to determine the correct class.
Figure 3 shows examples from three diﬀerent ﬂower species:
Matricaria chamomilla (chamomile), Bellis perennis (com-
mon daisy), and Leucanthemum vulgare (ox-eye daisy). As
seen these three categories are very similar to each other.

Another challenge in a ﬁne-grained recognition task is the
variability in viewpoints, colors, and scales, that are encoun-
tered within the same class. These variabilities are mostly
problematic to an automatic system rather than a human
observer. Many ﬂowers can have diﬀerent colors but a per-
son can very easily factor out that invariance, which is not
necessarily true for an algorithm. Similarly, it is very easy
for a person to understand if the image contains a close-
up shot of a single ﬂower or multiple small ﬂowers of the
same type, but this problem can be quite challenging for an
automatic vision system.

Other potential issues speciﬁc to an automatic recognition
system is the background, which can serve as a distractor,
e.g. when the system assumes it is part of the ﬂower, occlu-
sions over the ﬂowers, and others.

3. FLOWER RECOGNITION

APPLICATION

Our system and the mobile phone app are intended to
recognize the image of a ﬂower of a blooming plant or tree.
Figure 7 of the Appendix shows the main functionalities of
the app.

The entry point of the app is to take a picture of a ﬂower
and submit it for recognition (Figure 7a, and also Figure 1).
While taking the picture, a ﬂower contour is overlayed to
indicate that the ﬂower is expected to take the full image
and be within the boundaries of the image. These are only
guidelines and, as our users will discover, the app is much
more powerful and is capable of recognizing a ﬂower that
takes signiﬁcantly more or less of the area that is desirable.
Multiple ﬂowers of the same type (also called ‘cluster ﬂow-
ers’) are also not a problem for recognition.

Results of the top ﬁve choices of potential classes that the
input image can belong to are shown to the user (Figure 7b).
Since some classes may be very similar, as seen in Figure 3,
we found it much more useful to provide several choices,
instead of the top one only.

After the user is oﬀered a set of options of what the input
ﬂower could be, they can examine and compare the images
from the proposed categories (Figure 7c) and make a deci-
sion which class the ﬂower belongs to (Figure 7d). Here, in

Figure 3: One of the challenges for ﬁne-grained recognition is
that some classes can be very similar to one another and only
experts can discriminate them. This makes an automatic
recognition system very valuable to non-experts. Each row
shows a diﬀerent ﬂower species.

our application, we have taken advantage of the fact that
people may not be very good at recognizing a particular
ﬂower, especially among such large variety of 578 diﬀerent
species, but when presented with a set of options they can
be pretty good at verifying which one it is.

More results are available upon request, but we provide
top ten choices at most to avoid showing too many irrelevant
results, since the more results shown the more likely they
will be very diﬀerent from the input category. If the top ten
results do not provide satisfactory choice, the users can give
a name to the ﬂower, if they like (Figure 7e). Otherwise the
ﬂower is stored in their album as an ‘unknown’ category. An
example user album is shown in Figure 7f.

For each ﬂower, which has been identiﬁed, we provided
its common name and a short description about this ﬂower,
e.g. where it originated from,
its uses, and some trivia
information (Figure 7g). We also provide wikipedia link
that describes this ﬂower. Additionally we provide informa-
tion about regions world-wide where the ﬂower grows (Fig-
ure 7h). All this information has been compiled by our team
of expert biologists. We further plan to provide information
about how to grow the ﬂower for interested users, as well as,
more reﬁned information about the regions where it grows.
We also show the images that the user has collected for a
ﬂower category (Figure 7i).

4. FLOWER DATASET

For the purposes of developing the ﬂower recognition ap-
plication, we have collected a large-scale ﬂower dataset. The
dataset contains 578 diﬀerent ﬂower species and over 250,000
images. This is the only web-scale ﬂowers dataset we are
aware of. While we have compiled a seemingly modest num-
ber of images (250,000 compared to billions of images for
general image search engines), our dataset has exhausted the

Figure 4: Example images from diﬀerent classes from the large-scale 578 ﬂower species dataset. A large variety of ﬂower
classes, as well as, intra-class variabilities, inter-class similarities, and changes in ﬂower scales are available in this dataset.

web resources of available ﬂower images, and for some rare
species we could not ﬁnd a suﬃcient number of images. Fur-
thermore, this is the largest dataset for ﬂower recognition,
and also the largest of its kind among any other ﬁne-grained
recognition datasets we are aware of [13, 20, 22, 24, 28]. All
previous datasets contain at most 200 diﬀerent species [28]
and the largest known ﬂower dataset contains 102 classes
and 8189 images [22]. Also compared to previous ﬂower
datasets, our dataset is targeted to be used ‘in the ﬁeld’ by
actual users, rather than for experimentation only. Thus
our dataset contains a lot of intra-class variability, changes
to scale, and quality variability than any previous dataset.
Figure 4 shows example images from diﬀerent classes, and
as seen the images are much more realistic, which makes
the recognition task harder. On the other hand, this data
is much closer to the data distribution an actual user will
encounter as they collect images by their phones and submit
them to the app.

Collecting, deﬁning and organizing the dataset for this
application was a very challenging task, which is why we
describe those eﬀorts here.

4.1 Deﬁning the set of classes

The ﬁrst important question to address is to determine
which set of classes the system is going to recognize. While
there are many ﬂowering plants that potentially exist world-
wide [6], the largest number of species that an expert botanist
is able to recognize is estimated to be about 30,000. Natu-
rally, here we are striving towards recognizing as many ﬂower
species as possible, so as to achieve a maximal coverage of

ﬂowers that users may encounter. However, some real-life
limitations exist. One issue is that collecting and curating
images from such a dataset would be prohibitive, especially
since only ﬂower experts can do the identiﬁcation and cu-
ration (i.e.
labeling, or placing into categories, and giving
the correct scientiﬁc name). A more serious issue is that
there are no suﬃcient number of images for the less com-
mon ﬂower categories. Furthermore, such a large number
of classes is also pushing the boundaries of what we can do
technologically to recognize automatically.

To address this problem, we focused on the most common
ﬂowers which have suﬃciently large coverage over all possi-
ble ﬂowers, and ignored the ’long tail’ set of ﬂowers. More
speciﬁcally, our goal is to select the most frequent classes
which cover 90% of all ﬂowers world-wide. Since, it is not
clear how to estimate such a set, we deﬁned the classes in two
steps. First, our team selected the set of top 500 most pop-
ular ﬂowers in the world. We further collected images from
the web when typing general ﬂower-related terms such as
’ﬂowers’, ’desert ﬂowers’, ’mountain ﬂowers’, ’wedding ﬂow-
ers’, ’spring ﬂowers’, ’exotic ﬂowers’, etc. The latter was an
attempt to retrieve ‘all possible’ types of ﬂowers and their
natural distribution. In this way we could only capture the
ﬂower distribution that occurs on the web, which reﬂects
areas of the world with at least some technology penetra-
tion, rather than the actual distribution which is unknown,
but this was a good approximation, given that the users of
our mobile app will also be representative of technologically
advanced regions. After collecting such a set, the biologists
on our team labeled each image and identiﬁed ﬂowers that

are not included in our pre-deﬁned set of classes. We then
included the next most common ﬂower species, until their
coverage reached 90%. As a result of that we obtained a set
of 578 species of ﬂowers.

This, of course, is only approximately capturing the full
coverage of our application, and only after actual users start
to use the application, will we know if our coverage is suﬃ-
cient. Naturally, our set of classes can evolve with usage of
the app, and this can improve the usefulness of the applica-
tion, as well.
4.2 Data collection

Since we are developing user-based application which is
going to work with user provided data, we do not necessar-
ily have such data to begin with. Here we describe how we
bootstrap the mechanism for data collection so that to im-
rove the data quality for our app. Initially we use images
collected from the web (Section 4.2.1), then we collected
user data by crowd-sourcing (Section 4.2.3), ﬁnally, after
the application is launched we can collect user-uploaded im-
ages (Section 7). In a sense the dataset evolves until we can
serve the users best.

4.2.1 Web crawling
Initially we collected data from the web, by automatically
downloading images corresponding to each ﬂower category
by keyword search on Bing and Google search engines. We
typically used the oﬃcial name of the ﬂower (e.g.
‘matri-
caria recutita’) and one or more of its common names (e.g.
chamomile). Online search engines were very helpful to pro-
vide ﬂower images, however since the search is done by key-
words, a lot of the returned images are irrelevant to the
ﬂower category and need to be ﬁltered out.

4.2.2 Near-duplicate elimination
Near-duplicate elimination is a process which removes ex-
amples that are visually very similar or almost identical.
Typically near-duplicates are images of the same object but
one of them has been modiﬁed in some way, e.g. by rescal-
ing, compression, by adding noise, or by adding a logo or
modifying a part of the image. Near-duplicate elimination
is needed because our application shows a set of images for
each proposed class and the user can look at them and de-
cide if the class is the best match to the query ﬂower. In
this situation, we really want to show diverse images. Sec-
ondly, near-duplicate images have almost identical feature
representations and are not useful for training or testing of
our algorithm.

For near-duplicate elimination we have extracted HOG
features [10] in the whole image. These features are then
encoded in 8196-dimensional global feature dictionary using
the Local Linear Coding method [26]. All the features in the
image are combined by a global max-pooling [26] to obtain a
ﬁnal 8196-dimensional representation similar to [21]. This is
essentially a signature-like summarization of the whole im-
age, so two images which diﬀer only due to rescaling, some
noise or re-compression will likely have very similar signa-
tures. Images whose signatures diﬀer by less than a prespec-
iﬁed threshold are eliminated as duplicates and a represen-
tative single image is kept in the ﬁnal collection. Note that
the feature representation is derived from the more complex
representation, presented later in Section 5 which is used for
ﬁnal classiﬁcation into diﬀerent classes.

Figure 5: Example images of a rose returned by Bing Search
Engine (top panel), example images of a rose obtained by a
mobile phone (bottom row). As seen, the quality of these
images is very diﬀerent, so images obtained online will not
be suﬃcient to cover the test distribution of mobile images.

We are not aware of any other system using the method
for near-duplicate detection described above and we chose
it for simplicity and ease of application. There are other
near-duplicate detection methods that have been proposed
and used in practice [17, 19], but we found the method we
have used as suﬃcient for our goals.

4.2.3 Collecting data via crowd-sourcing
Since our data were collected from web sources, naturally,
their distribution is very diﬀerent from the distribution of
images provided by mobile phone users. Figure 5 shows
example image for a ‘rose’ that are returned by Bing search
engine [2] and other examples which are collected ‘in the
ﬁeld’ by a mobile device. As seen those are very diﬀerent.
Therefore, we needed to collect extra images which are closer
to the user-generated content by mobile devices.

For that purpose we used a crowd-sourcing service for data
collection. Apart from obtaining images closer to the images
by our potential app user, we are aiming to get diversity both
in geographical locations and also from user to user (e.g. a
variety of user demographics, photography skill levels, etc.).
Mobile phone users on a crowd-sourcing service were in-
structed to capture ﬂower images. We gave fairly loose
guidelines so that they maximally match the users of the
app, who will not be given instructions but will be guided
by the UI to take ﬂower images for recognition. Most of the
users did what is expected, but for some users we had to
reﬁne and reinforce the instructions mostly for the sake of
collecting useful data for our algorithms. In general, what
we learned in the process was that we need to set up fairly
simple and straightforward requirements for crowd-sourcing
users.

Overall we are very pleased with the dataset collected from
crowd-coursing: some of the agents paid to collect images
were very industrious and creative and provided images of
exotic and rare plants, which they have collected from vaca-
tions or by visiting botanical gardens and arboreta. Figure 6

Figure 6: Example images collected by diﬀerent users via a crowd-sourcing service. As seen, the users provided very good
variety of ﬂower species from diﬀerent geo-locations but also images vastly varying in quality. Each row corresponds to a
separate user. Real mobile phone users were crucial in our experience, because their images reﬂect better the distribution of
images that our app users are likely to upload.

shows images that the users collected, which shows us the
diversity one can get thanks to crowd-sourcing. Naturally,
we also have some images that will be considered of ‘poor
quality’ in any other dataset, e.g. with shadows, blur, etc.,
but those are very useful for our application, because they
represent images that mobile phone users may take.
4.3 Labeling

A very demanding task is that of labeling the images with
their correct ﬂower class. We note here that only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do this task. For ex-
ample, Amazon’s Mechanical Turk [1] users would generally
not be able to provide such services because it is crucial
to recognize subtle diﬀerences between species. Also note
that labeling is a painstaking manual process, because each
image has to be examined to verify it belongs to a speciﬁc
category.
In some cases the identiﬁcation process can be
very complicated in which the botanists answer a sequence
of questions until the ﬂower is identiﬁed [11]. While there is
no established protocol, which makes the task so challeng-
ing, the botanists try to determine if it is a ﬂower or an
inﬂorescence (branch of ﬂowers, e.g. the ’ﬂower’ we see in a
Heliconia or a sunﬂower), the number of petals, stamens and
carpels; whether the petals (stamens or carples) are fused or
free, the type of symmetry of the ﬂoral whorls, etc. In order
to label all images that belong to a certain class, the biology
experts in our team followed this complex and demanding
procedure for ﬂower identiﬁcation.

In addition to labeling, the dataset had to be cleaned to re-
move images of non-ﬂowers or of artiﬁcial ﬂowers (e.g. draw-
ings, cartoons, clipart, etc). Although the process did not
require expert knowledge, it is very tedious.
Initially we

have collected more than 3.5 million images which had to be
ﬁltered.
4.4 Discussion

As is clear from this section, collecting and labeling this
data was a major accomplishment of our team, and is still
a work in progress. This dataset, with its 250,000 images
across 578 ﬂower categories, is the largest we are aware of.
For very common categories there are many images avail-
able, but we have limited the number we store so that the
dataset is more balanced. For the categories which are very
rare, although we attempted to collect several thousands
images that contain the respective categories, much fewer
results were available online. Moreover, the number of actu-
ally useful images (the ones that contain a ﬂower or ﬂowers
of this category only) was very small. The images we col-
lected, about 250,000 may seem relatively small compared to
web image corpora, but with this dataset we have exhausted
what search engines can return, e.g. for very rare categories.
Future work needs to address collecting examples for these
rare categories. User interactions with the app will serve as
a means to redeﬁne and expand the ﬂower dataset.
5. RECOGNITION ALGORITHM

A key to the application presented is its recognition engine

and algorithm, which we describe in this section.

The algorithm is based on the one of Lin et al. [21]. The
algorithm starts by feature extraction on the input image:
we ﬁrst extract HOG features [10] at 4 diﬀerent levels. These
features are then encoded in 8196-dimensional global feature
dictionary using the Local Linear Coding method of Wang
et al. [26]. The global dictionary is created by clustering
of low-level patches generated from a set of natural images

Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Figure 2: One of the discriminating characteristics of our application is that it can generalize within a class and can ignore
irrelevant characteristics, such as color. Results of recognizing an image containing a Gerbera ﬂower by Google Search-By-
Image service (left), in which all returned images match the color distribution but no Gerbera ﬂower is retrieved, and our
system (right) which correctly identiﬁes the ﬂower species of the query image and also returns somewhat similar categories as
choices 2 and 3. The web-based version of our application is shown here.

phone app1 (Figure 1) and as a web-based service2. While
some reference applications for ﬂowers already exist (e.g.
Flower Pedia, Audubon Wildﬂowers, What Flower, etc), our
app has the unique capability to automatically recognize the
ﬂower from an image. This is of much value to interested
users because no previous application can accomplish such
a task, especially from a mobile device. The system is capa-
ble of recognizing 578 diﬀerent ﬂower species and relies on
web-scale image dataset that has been collected from various
sources and has been carefully labeled by a team of expert
botanists. Our recognition system is speciﬁcally designed
to handle user-generated images collected ‘in the ﬁeld’ by
mobile phone users of various devices. The latter is impor-
tant since user-generated imagery may signiﬁcantly vary in
quality and will diﬀer from the images of ﬂowers that are
encountered on web-search engines. We describe details of
designing the system, developing the algorithms, as well as,
collecting the dataset that can support this capability.

Recognizing among diﬀerent types within the same gen-
eral class of objects is called ﬁne-grained classiﬁcation [13,
22, 28], and is one of the active areas of research in com-
puter vision. Unlike the so called basic-level recognition,
which refers to recognizing whether an object is a ﬂower,
or a cat, or a chair, ﬁne-grained recognition works within a
single base-level category and aims to recognize among its
subcategories, such as diﬀerent species of ﬂowers or diﬀer-
ent species of birds, plants, etc. Fine-grained classiﬁcation
requires knowledge and skills that only trained experts can
provide.
In the case of ﬂower recognition, only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do the task, and some-

1The app is developed for iPhone and is called ‘Mobile
Flora’.
It is currently under invitation-only beta testing,
and will soon be available for general use. Please email the
contact authors to send you an invitation.
2The web-based version of
http://mobileﬂora.heroku.com

the app is available at

times not without using further references [11], or following
a complex identiﬁcation procedure (see Section 4.3). Since
very few people can successfully recognize among a large
number of species, this makes the automatic ﬁne-grained
recognition application very valuable to non-experts.

General object recognition, and more speciﬁcally, ﬁne-
grained recognition, are still problems that are far from be-
ing solved algorithmically. Large-scale image retrieval sys-
tems [3, 4, 7, 25, 29] have been quite successful at retrieving
a duplicate or near-duplicate images [7, 4], similar images [4],
or extracting images ‘by sketch’ [25]. However, they do not
have the capability to generalize within a class, e.g. to recog-
nize that some object is a vacuum cleaner, for example, and
do not have the capabilities to discriminate between very re-
lated but diﬀerent classes of the same super-class, as is the
case with ﬂowers. Furthermore, the computer vision recog-
nition technologies that are available on the web are mostly
targetted towards retrieval of web content, rather than user
generated ‘ﬁeld’ images. With the advent of social media
and mobile phones, naturally, the demand for recognition of
user-generated images is increasing.

Another diﬀerentiating capability between our ﬂower recog-
nition technology and other available image search or image
retrieval systems, e.g. Google’s Search-By-Image service [4]
is that our system is speciﬁcally targeted towards recogni-
tion, in our case of ﬂower species. That is, our system is
trained to identify features that generalize within a class,
whereas other systems are more general and very often re-
turn irrelevant results (i.e. non-ﬂowers) or match the input
ﬂower image by color distribution or other general visual
features, rather than retrieving ﬂowers from the same class.
Figure 2 shows an example search of an image of a Gerbera
ﬂower by Google Search-By-Image service [4] and by our sys-
tem (which can be found at http://mobileﬂora.heroku.com).
As seen, although many images of Gerbera ﬂowers are avail-
able in Google’s image corpus, not a single Gerbera ﬂower
was retrieved. Instead our system is capable of recognizing

the type of ﬂower and providing related images. We note
here that this example shows typical retrieval diﬀerences, es-
pecially for ﬂower images that do not have a near-duplicate
counterpart on the web, e.g. images in personal collections,
or obtained by mobile devices, etc.

2. FINE-GRAINED RECOGNITION

CHALLENGES

Fine-grained classiﬁcation has its unique challenges be-
cause it aims to recognize among classes that will typically
be considered as a part of one large class in standard visual
object recognition tasks.

The main inherent challenge is that some species have
very ﬁne diﬀerences which are hard to notice for the com-
mon eye. For example, the categories that need to be dis-
criminated among each other can be very similar and would
require careful examination to determine the correct class.
Figure 3 shows examples from three diﬀerent ﬂower species:
Matricaria chamomilla (chamomile), Bellis perennis (com-
mon daisy), and Leucanthemum vulgare (ox-eye daisy). As
seen these three categories are very similar to each other.

Another challenge in a ﬁne-grained recognition task is the
variability in viewpoints, colors, and scales, that are encoun-
tered within the same class. These variabilities are mostly
problematic to an automatic system rather than a human
observer. Many ﬂowers can have diﬀerent colors but a per-
son can very easily factor out that invariance, which is not
necessarily true for an algorithm. Similarly, it is very easy
for a person to understand if the image contains a close-
up shot of a single ﬂower or multiple small ﬂowers of the
same type, but this problem can be quite challenging for an
automatic vision system.

Other potential issues speciﬁc to an automatic recognition
system is the background, which can serve as a distractor,
e.g. when the system assumes it is part of the ﬂower, occlu-
sions over the ﬂowers, and others.

3. FLOWER RECOGNITION

APPLICATION

Our system and the mobile phone app are intended to
recognize the image of a ﬂower of a blooming plant or tree.
Figure 7 of the Appendix shows the main functionalities of
the app.

The entry point of the app is to take a picture of a ﬂower
and submit it for recognition (Figure 7a, and also Figure 1).
While taking the picture, a ﬂower contour is overlayed to
indicate that the ﬂower is expected to take the full image
and be within the boundaries of the image. These are only
guidelines and, as our users will discover, the app is much
more powerful and is capable of recognizing a ﬂower that
takes signiﬁcantly more or less of the area that is desirable.
Multiple ﬂowers of the same type (also called ‘cluster ﬂow-
ers’) are also not a problem for recognition.

Results of the top ﬁve choices of potential classes that the
input image can belong to are shown to the user (Figure 7b).
Since some classes may be very similar, as seen in Figure 3,
we found it much more useful to provide several choices,
instead of the top one only.

After the user is oﬀered a set of options of what the input
ﬂower could be, they can examine and compare the images
from the proposed categories (Figure 7c) and make a deci-
sion which class the ﬂower belongs to (Figure 7d). Here, in

Figure 3: One of the challenges for ﬁne-grained recognition is
that some classes can be very similar to one another and only
experts can discriminate them. This makes an automatic
recognition system very valuable to non-experts. Each row
shows a diﬀerent ﬂower species.

our application, we have taken advantage of the fact that
people may not be very good at recognizing a particular
ﬂower, especially among such large variety of 578 diﬀerent
species, but when presented with a set of options they can
be pretty good at verifying which one it is.

More results are available upon request, but we provide
top ten choices at most to avoid showing too many irrelevant
results, since the more results shown the more likely they
will be very diﬀerent from the input category. If the top ten
results do not provide satisfactory choice, the users can give
a name to the ﬂower, if they like (Figure 7e). Otherwise the
ﬂower is stored in their album as an ‘unknown’ category. An
example user album is shown in Figure 7f.

For each ﬂower, which has been identiﬁed, we provided
its common name and a short description about this ﬂower,
e.g. where it originated from,
its uses, and some trivia
information (Figure 7g). We also provide wikipedia link
that describes this ﬂower. Additionally we provide informa-
tion about regions world-wide where the ﬂower grows (Fig-
ure 7h). All this information has been compiled by our team
of expert biologists. We further plan to provide information
about how to grow the ﬂower for interested users, as well as,
more reﬁned information about the regions where it grows.
We also show the images that the user has collected for a
ﬂower category (Figure 7i).

4. FLOWER DATASET

For the purposes of developing the ﬂower recognition ap-
plication, we have collected a large-scale ﬂower dataset. The
dataset contains 578 diﬀerent ﬂower species and over 250,000
images. This is the only web-scale ﬂowers dataset we are
aware of. While we have compiled a seemingly modest num-
ber of images (250,000 compared to billions of images for
general image search engines), our dataset has exhausted the

Figure 4: Example images from diﬀerent classes from the large-scale 578 ﬂower species dataset. A large variety of ﬂower
classes, as well as, intra-class variabilities, inter-class similarities, and changes in ﬂower scales are available in this dataset.

web resources of available ﬂower images, and for some rare
species we could not ﬁnd a suﬃcient number of images. Fur-
thermore, this is the largest dataset for ﬂower recognition,
and also the largest of its kind among any other ﬁne-grained
recognition datasets we are aware of [13, 20, 22, 24, 28]. All
previous datasets contain at most 200 diﬀerent species [28]
and the largest known ﬂower dataset contains 102 classes
and 8189 images [22]. Also compared to previous ﬂower
datasets, our dataset is targeted to be used ‘in the ﬁeld’ by
actual users, rather than for experimentation only. Thus
our dataset contains a lot of intra-class variability, changes
to scale, and quality variability than any previous dataset.
Figure 4 shows example images from diﬀerent classes, and
as seen the images are much more realistic, which makes
the recognition task harder. On the other hand, this data
is much closer to the data distribution an actual user will
encounter as they collect images by their phones and submit
them to the app.

Collecting, deﬁning and organizing the dataset for this
application was a very challenging task, which is why we
describe those eﬀorts here.

4.1 Deﬁning the set of classes

The ﬁrst important question to address is to determine
which set of classes the system is going to recognize. While
there are many ﬂowering plants that potentially exist world-
wide [6], the largest number of species that an expert botanist
is able to recognize is estimated to be about 30,000. Natu-
rally, here we are striving towards recognizing as many ﬂower
species as possible, so as to achieve a maximal coverage of

ﬂowers that users may encounter. However, some real-life
limitations exist. One issue is that collecting and curating
images from such a dataset would be prohibitive, especially
since only ﬂower experts can do the identiﬁcation and cu-
ration (i.e.
labeling, or placing into categories, and giving
the correct scientiﬁc name). A more serious issue is that
there are no suﬃcient number of images for the less com-
mon ﬂower categories. Furthermore, such a large number
of classes is also pushing the boundaries of what we can do
technologically to recognize automatically.

To address this problem, we focused on the most common
ﬂowers which have suﬃciently large coverage over all possi-
ble ﬂowers, and ignored the ’long tail’ set of ﬂowers. More
speciﬁcally, our goal is to select the most frequent classes
which cover 90% of all ﬂowers world-wide. Since, it is not
clear how to estimate such a set, we deﬁned the classes in two
steps. First, our team selected the set of top 500 most pop-
ular ﬂowers in the world. We further collected images from
the web when typing general ﬂower-related terms such as
’ﬂowers’, ’desert ﬂowers’, ’mountain ﬂowers’, ’wedding ﬂow-
ers’, ’spring ﬂowers’, ’exotic ﬂowers’, etc. The latter was an
attempt to retrieve ‘all possible’ types of ﬂowers and their
natural distribution. In this way we could only capture the
ﬂower distribution that occurs on the web, which reﬂects
areas of the world with at least some technology penetra-
tion, rather than the actual distribution which is unknown,
but this was a good approximation, given that the users of
our mobile app will also be representative of technologically
advanced regions. After collecting such a set, the biologists
on our team labeled each image and identiﬁed ﬂowers that

are not included in our pre-deﬁned set of classes. We then
included the next most common ﬂower species, until their
coverage reached 90%. As a result of that we obtained a set
of 578 species of ﬂowers.

This, of course, is only approximately capturing the full
coverage of our application, and only after actual users start
to use the application, will we know if our coverage is suﬃ-
cient. Naturally, our set of classes can evolve with usage of
the app, and this can improve the usefulness of the applica-
tion, as well.
4.2 Data collection

Since we are developing user-based application which is
going to work with user provided data, we do not necessar-
ily have such data to begin with. Here we describe how we
bootstrap the mechanism for data collection so that to im-
rove the data quality for our app. Initially we use images
collected from the web (Section 4.2.1), then we collected
user data by crowd-sourcing (Section 4.2.3), ﬁnally, after
the application is launched we can collect user-uploaded im-
ages (Section 7). In a sense the dataset evolves until we can
serve the users best.

4.2.1 Web crawling
Initially we collected data from the web, by automatically
downloading images corresponding to each ﬂower category
by keyword search on Bing and Google search engines. We
typically used the oﬃcial name of the ﬂower (e.g.
‘matri-
caria recutita’) and one or more of its common names (e.g.
chamomile). Online search engines were very helpful to pro-
vide ﬂower images, however since the search is done by key-
words, a lot of the returned images are irrelevant to the
ﬂower category and need to be ﬁltered out.

4.2.2 Near-duplicate elimination
Near-duplicate elimination is a process which removes ex-
amples that are visually very similar or almost identical.
Typically near-duplicates are images of the same object but
one of them has been modiﬁed in some way, e.g. by rescal-
ing, compression, by adding noise, or by adding a logo or
modifying a part of the image. Near-duplicate elimination
is needed because our application shows a set of images for
each proposed class and the user can look at them and de-
cide if the class is the best match to the query ﬂower. In
this situation, we really want to show diverse images. Sec-
ondly, near-duplicate images have almost identical feature
representations and are not useful for training or testing of
our algorithm.

For near-duplicate elimination we have extracted HOG
features [10] in the whole image. These features are then
encoded in 8196-dimensional global feature dictionary using
the Local Linear Coding method [26]. All the features in the
image are combined by a global max-pooling [26] to obtain a
ﬁnal 8196-dimensional representation similar to [21]. This is
essentially a signature-like summarization of the whole im-
age, so two images which diﬀer only due to rescaling, some
noise or re-compression will likely have very similar signa-
tures. Images whose signatures diﬀer by less than a prespec-
iﬁed threshold are eliminated as duplicates and a represen-
tative single image is kept in the ﬁnal collection. Note that
the feature representation is derived from the more complex
representation, presented later in Section 5 which is used for
ﬁnal classiﬁcation into diﬀerent classes.

Figure 5: Example images of a rose returned by Bing Search
Engine (top panel), example images of a rose obtained by a
mobile phone (bottom row). As seen, the quality of these
images is very diﬀerent, so images obtained online will not
be suﬃcient to cover the test distribution of mobile images.

We are not aware of any other system using the method
for near-duplicate detection described above and we chose
it for simplicity and ease of application. There are other
near-duplicate detection methods that have been proposed
and used in practice [17, 19], but we found the method we
have used as suﬃcient for our goals.

4.2.3 Collecting data via crowd-sourcing
Since our data were collected from web sources, naturally,
their distribution is very diﬀerent from the distribution of
images provided by mobile phone users. Figure 5 shows
example image for a ‘rose’ that are returned by Bing search
engine [2] and other examples which are collected ‘in the
ﬁeld’ by a mobile device. As seen those are very diﬀerent.
Therefore, we needed to collect extra images which are closer
to the user-generated content by mobile devices.

For that purpose we used a crowd-sourcing service for data
collection. Apart from obtaining images closer to the images
by our potential app user, we are aiming to get diversity both
in geographical locations and also from user to user (e.g. a
variety of user demographics, photography skill levels, etc.).
Mobile phone users on a crowd-sourcing service were in-
structed to capture ﬂower images. We gave fairly loose
guidelines so that they maximally match the users of the
app, who will not be given instructions but will be guided
by the UI to take ﬂower images for recognition. Most of the
users did what is expected, but for some users we had to
reﬁne and reinforce the instructions mostly for the sake of
collecting useful data for our algorithms. In general, what
we learned in the process was that we need to set up fairly
simple and straightforward requirements for crowd-sourcing
users.

Overall we are very pleased with the dataset collected from
crowd-coursing: some of the agents paid to collect images
were very industrious and creative and provided images of
exotic and rare plants, which they have collected from vaca-
tions or by visiting botanical gardens and arboreta. Figure 6

Figure 6: Example images collected by diﬀerent users via a crowd-sourcing service. As seen, the users provided very good
variety of ﬂower species from diﬀerent geo-locations but also images vastly varying in quality. Each row corresponds to a
separate user. Real mobile phone users were crucial in our experience, because their images reﬂect better the distribution of
images that our app users are likely to upload.

shows images that the users collected, which shows us the
diversity one can get thanks to crowd-sourcing. Naturally,
we also have some images that will be considered of ‘poor
quality’ in any other dataset, e.g. with shadows, blur, etc.,
but those are very useful for our application, because they
represent images that mobile phone users may take.
4.3 Labeling

A very demanding task is that of labeling the images with
their correct ﬂower class. We note here that only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do this task. For ex-
ample, Amazon’s Mechanical Turk [1] users would generally
not be able to provide such services because it is crucial
to recognize subtle diﬀerences between species. Also note
that labeling is a painstaking manual process, because each
image has to be examined to verify it belongs to a speciﬁc
category.
In some cases the identiﬁcation process can be
very complicated in which the botanists answer a sequence
of questions until the ﬂower is identiﬁed [11]. While there is
no established protocol, which makes the task so challeng-
ing, the botanists try to determine if it is a ﬂower or an
inﬂorescence (branch of ﬂowers, e.g. the ’ﬂower’ we see in a
Heliconia or a sunﬂower), the number of petals, stamens and
carpels; whether the petals (stamens or carples) are fused or
free, the type of symmetry of the ﬂoral whorls, etc. In order
to label all images that belong to a certain class, the biology
experts in our team followed this complex and demanding
procedure for ﬂower identiﬁcation.

In addition to labeling, the dataset had to be cleaned to re-
move images of non-ﬂowers or of artiﬁcial ﬂowers (e.g. draw-
ings, cartoons, clipart, etc). Although the process did not
require expert knowledge, it is very tedious.
Initially we

have collected more than 3.5 million images which had to be
ﬁltered.
4.4 Discussion

As is clear from this section, collecting and labeling this
data was a major accomplishment of our team, and is still
a work in progress. This dataset, with its 250,000 images
across 578 ﬂower categories, is the largest we are aware of.
For very common categories there are many images avail-
able, but we have limited the number we store so that the
dataset is more balanced. For the categories which are very
rare, although we attempted to collect several thousands
images that contain the respective categories, much fewer
results were available online. Moreover, the number of actu-
ally useful images (the ones that contain a ﬂower or ﬂowers
of this category only) was very small. The images we col-
lected, about 250,000 may seem relatively small compared to
web image corpora, but with this dataset we have exhausted
what search engines can return, e.g. for very rare categories.
Future work needs to address collecting examples for these
rare categories. User interactions with the app will serve as
a means to redeﬁne and expand the ﬂower dataset.
5. RECOGNITION ALGORITHM

A key to the application presented is its recognition engine

and algorithm, which we describe in this section.

The algorithm is based on the one of Lin et al. [21]. The
algorithm starts by feature extraction on the input image:
we ﬁrst extract HOG features [10] at 4 diﬀerent levels. These
features are then encoded in 8196-dimensional global feature
dictionary using the Local Linear Coding method of Wang
et al. [26]. The global dictionary is created by clustering
of low-level patches generated from a set of natural images

which are unrelated to the classiﬁcation task at hand. En-
coding by using global dictionaries have been shown to be
very advantageous for visual recognition [26]. After that, a
global max pooling of the encoded features in the image is
done, as well as, max poolings in a 3x3 grid of the image [21].
Combining all the poolings we obtain a vector representa-
tion which is 10 times the initial dictionary dimensionality,
or about 80,000-dimensional feature vector.

This feature representation is then classiﬁed into indi-
vidual 578 classes by a multi-class classiﬁcation algorithm,
such as Support Vector Machines (SVM). Our classiﬁcation
pipeline uses the 1-vs-all strategy of linear classiﬁcation and
we used a Stochastic Gradient Descent version of the algo-
rithm [21] for these purposes since our dataset of 250,000
images does not ﬁt on a single machine and standard SVM
implementations [12] cannot be applied.

This recognition algorithm is selected because of its per-
formance capabilities (see Section 6) and its runtime (for
this app we need real-time performance because in addition
to the recognition time, the user experiences additional time
latencies for uploading the image and downloading and vi-
sualizing the results). We also preferred it for being general
and easy to use.

Note that the recognition algorithm has been previously
used for general object recognition (e.g. recognize a ﬂower
from a piano from a dog, etc), but here we have applied it
for recognition into very closely related objects as is the case
of diﬀerent types of ﬂowers.

6. TESTING AND DEPLOYMENT

We evaluate here the performance of our recognition al-
gorithm. The results of the classiﬁcation performance are
shown in Table 1. We computed the accuracy of the test set
for top 1, top 2, . . ., top 10 returned results. As seen we can
achieve 52.3% for top 1, 75.2% for top 5, and 82.3% for top
10 results. Translating this to the context of the functional-
ity of the app, we found it useful to show top 5 suggestions
per query image, and then allow the user to expand to top
10 suggestions. That is, we can hope to provide the correct
answer among our suggestions in more than 82% of the time.
We tested our recognition on another ﬂower dataset, Ox-
ford 102 ﬂowers [22], which has been widely used in the
computer vision community. This is done as to see how
the performance of our algorithm compares to other known
methods in the literature, since we could not test all other
algorithms on our large-scale dataset (since most algorithms
are not publicly available, or they are not adequate for multi-
machine parallel computations). As seen in Table 2, our al-
gorithm achieves state-of-the-art performance, and is even a
little better than the best known algorithm for this dataset.
Apart from performing very well, it has other advantages,
e.g.
it is very general and easy to use. We want to note
that better performances for the Oxford dataset have been
reported [9], but they require segmentation of the image
which is still computationally ineﬃcient and cannot be used
directly since our app’s response should be real-time.

6.1 Practical considerations in deployment

Runtime. One important thing to consider is runtime.
Apart from the time needed to upload the image and down-
load the results, the most time is consumed in the recog-
nition algorithm itself. Currently our recognition algorithm

Table 1: Accuracy of our recognition algorithm for top 1 to
top 5, top 7 and top 10 retrieved classes. As seen for top 10
we achieve 82 percent correct.

Top 1 Top 2 Top 3 Top 4 Top 5 Top 7 Top 10
52.3

63.4

75.2

68.9

72.4

78.9

82.3

Table 2: Accuracy of our recognition algorithm, compared
to other baseline methods on the smaller Oxford 102 ﬂowers
dataset.

Method

Accuracy (%)

Kanan and Cottrell [18]

Nilsback and Zisserman [22]

Ito and Cubota [16]

Nilsback and Zisserman [23]

Ours

72.0
72.8
74.8
76.3
76.7

takes about 1-2 seconds (it typically works within 1 sec-
ond and only very rarely needs more time). The upload and
download times are relatively small since we resize the image
before uploading it and download only image thumbnails ini-
tially. Larger size images are downloaded in the background
to keep the latency small. The current response time is ad-
equate for our application, we believe, but we continue to
improve its eﬃciency.

Scalability. We have deployed our application on Sales-

force’s Heroku cloud platform [5]. It is available at
http://mobileﬂora.heroku.com.

Logging. We also considered logging (see Section 7 for
more details) so that we can track performance as the app is
being used, and gauge whether users often encounter prob-
lems with ﬂower misclassiﬁcation or with the app itself.

Ease of use. We have limited the app functionality to a

minimum so that it is easy and self-explanatory to use.

User perception. As any user-facing application, the
actual performance numbers do not matter if the user per-
ceives the results presented as inadequate. For example, we
could guess correctly the class at position 5, but if the top
4 results are very diﬀerent visually (e.g.
returning a yel-
low dandelion for an input image containing a red rose), the
user’s perception will be that the application performance
is poor. While our algorithm tends to return visually con-
sistent results as top choices (see Figure 2) thanks to the
feature representation we used, more can be done to enforce
visual similarity.

7. COLLECTING USER FEEDBACK

We have designed our app to collect information which we

are going to use for further improvement.

Usability. We enabled user feedback in which the users
can provide free-form comments, asking for features, report-
ing issues, bugs etc.

Recognition accuracy. We provided the users an op-
tion to expand the top 5 choices provided to top 10 choices.
As seen from our evaluation (Table 1) the accuracy among
the top 10 is 82.3%. We observed that showing more op-
tions is counter-productive, because although the chances of
showing the correct results increases, a lot more irrelevant
results start to be shown. If the ﬂower is not found among
the provided choices, the user has the option to specify its

Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Figure 2: One of the discriminating characteristics of our application is that it can generalize within a class and can ignore
irrelevant characteristics, such as color. Results of recognizing an image containing a Gerbera ﬂower by Google Search-By-
Image service (left), in which all returned images match the color distribution but no Gerbera ﬂower is retrieved, and our
system (right) which correctly identiﬁes the ﬂower species of the query image and also returns somewhat similar categories as
choices 2 and 3. The web-based version of our application is shown here.

phone app1 (Figure 1) and as a web-based service2. While
some reference applications for ﬂowers already exist (e.g.
Flower Pedia, Audubon Wildﬂowers, What Flower, etc), our
app has the unique capability to automatically recognize the
ﬂower from an image. This is of much value to interested
users because no previous application can accomplish such
a task, especially from a mobile device. The system is capa-
ble of recognizing 578 diﬀerent ﬂower species and relies on
web-scale image dataset that has been collected from various
sources and has been carefully labeled by a team of expert
botanists. Our recognition system is speciﬁcally designed
to handle user-generated images collected ‘in the ﬁeld’ by
mobile phone users of various devices. The latter is impor-
tant since user-generated imagery may signiﬁcantly vary in
quality and will diﬀer from the images of ﬂowers that are
encountered on web-search engines. We describe details of
designing the system, developing the algorithms, as well as,
collecting the dataset that can support this capability.

Recognizing among diﬀerent types within the same gen-
eral class of objects is called ﬁne-grained classiﬁcation [13,
22, 28], and is one of the active areas of research in com-
puter vision. Unlike the so called basic-level recognition,
which refers to recognizing whether an object is a ﬂower,
or a cat, or a chair, ﬁne-grained recognition works within a
single base-level category and aims to recognize among its
subcategories, such as diﬀerent species of ﬂowers or diﬀer-
ent species of birds, plants, etc. Fine-grained classiﬁcation
requires knowledge and skills that only trained experts can
provide.
In the case of ﬂower recognition, only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do the task, and some-

1The app is developed for iPhone and is called ‘Mobile
Flora’.
It is currently under invitation-only beta testing,
and will soon be available for general use. Please email the
contact authors to send you an invitation.
2The web-based version of
http://mobileﬂora.heroku.com

the app is available at

times not without using further references [11], or following
a complex identiﬁcation procedure (see Section 4.3). Since
very few people can successfully recognize among a large
number of species, this makes the automatic ﬁne-grained
recognition application very valuable to non-experts.

General object recognition, and more speciﬁcally, ﬁne-
grained recognition, are still problems that are far from be-
ing solved algorithmically. Large-scale image retrieval sys-
tems [3, 4, 7, 25, 29] have been quite successful at retrieving
a duplicate or near-duplicate images [7, 4], similar images [4],
or extracting images ‘by sketch’ [25]. However, they do not
have the capability to generalize within a class, e.g. to recog-
nize that some object is a vacuum cleaner, for example, and
do not have the capabilities to discriminate between very re-
lated but diﬀerent classes of the same super-class, as is the
case with ﬂowers. Furthermore, the computer vision recog-
nition technologies that are available on the web are mostly
targetted towards retrieval of web content, rather than user
generated ‘ﬁeld’ images. With the advent of social media
and mobile phones, naturally, the demand for recognition of
user-generated images is increasing.

Another diﬀerentiating capability between our ﬂower recog-
nition technology and other available image search or image
retrieval systems, e.g. Google’s Search-By-Image service [4]
is that our system is speciﬁcally targeted towards recogni-
tion, in our case of ﬂower species. That is, our system is
trained to identify features that generalize within a class,
whereas other systems are more general and very often re-
turn irrelevant results (i.e. non-ﬂowers) or match the input
ﬂower image by color distribution or other general visual
features, rather than retrieving ﬂowers from the same class.
Figure 2 shows an example search of an image of a Gerbera
ﬂower by Google Search-By-Image service [4] and by our sys-
tem (which can be found at http://mobileﬂora.heroku.com).
As seen, although many images of Gerbera ﬂowers are avail-
able in Google’s image corpus, not a single Gerbera ﬂower
was retrieved. Instead our system is capable of recognizing

the type of ﬂower and providing related images. We note
here that this example shows typical retrieval diﬀerences, es-
pecially for ﬂower images that do not have a near-duplicate
counterpart on the web, e.g. images in personal collections,
or obtained by mobile devices, etc.

2. FINE-GRAINED RECOGNITION

CHALLENGES

Fine-grained classiﬁcation has its unique challenges be-
cause it aims to recognize among classes that will typically
be considered as a part of one large class in standard visual
object recognition tasks.

The main inherent challenge is that some species have
very ﬁne diﬀerences which are hard to notice for the com-
mon eye. For example, the categories that need to be dis-
criminated among each other can be very similar and would
require careful examination to determine the correct class.
Figure 3 shows examples from three diﬀerent ﬂower species:
Matricaria chamomilla (chamomile), Bellis perennis (com-
mon daisy), and Leucanthemum vulgare (ox-eye daisy). As
seen these three categories are very similar to each other.

Another challenge in a ﬁne-grained recognition task is the
variability in viewpoints, colors, and scales, that are encoun-
tered within the same class. These variabilities are mostly
problematic to an automatic system rather than a human
observer. Many ﬂowers can have diﬀerent colors but a per-
son can very easily factor out that invariance, which is not
necessarily true for an algorithm. Similarly, it is very easy
for a person to understand if the image contains a close-
up shot of a single ﬂower or multiple small ﬂowers of the
same type, but this problem can be quite challenging for an
automatic vision system.

Other potential issues speciﬁc to an automatic recognition
system is the background, which can serve as a distractor,
e.g. when the system assumes it is part of the ﬂower, occlu-
sions over the ﬂowers, and others.

3. FLOWER RECOGNITION

APPLICATION

Our system and the mobile phone app are intended to
recognize the image of a ﬂower of a blooming plant or tree.
Figure 7 of the Appendix shows the main functionalities of
the app.

The entry point of the app is to take a picture of a ﬂower
and submit it for recognition (Figure 7a, and also Figure 1).
While taking the picture, a ﬂower contour is overlayed to
indicate that the ﬂower is expected to take the full image
and be within the boundaries of the image. These are only
guidelines and, as our users will discover, the app is much
more powerful and is capable of recognizing a ﬂower that
takes signiﬁcantly more or less of the area that is desirable.
Multiple ﬂowers of the same type (also called ‘cluster ﬂow-
ers’) are also not a problem for recognition.

Results of the top ﬁve choices of potential classes that the
input image can belong to are shown to the user (Figure 7b).
Since some classes may be very similar, as seen in Figure 3,
we found it much more useful to provide several choices,
instead of the top one only.

After the user is oﬀered a set of options of what the input
ﬂower could be, they can examine and compare the images
from the proposed categories (Figure 7c) and make a deci-
sion which class the ﬂower belongs to (Figure 7d). Here, in

Figure 3: One of the challenges for ﬁne-grained recognition is
that some classes can be very similar to one another and only
experts can discriminate them. This makes an automatic
recognition system very valuable to non-experts. Each row
shows a diﬀerent ﬂower species.

our application, we have taken advantage of the fact that
people may not be very good at recognizing a particular
ﬂower, especially among such large variety of 578 diﬀerent
species, but when presented with a set of options they can
be pretty good at verifying which one it is.

More results are available upon request, but we provide
top ten choices at most to avoid showing too many irrelevant
results, since the more results shown the more likely they
will be very diﬀerent from the input category. If the top ten
results do not provide satisfactory choice, the users can give
a name to the ﬂower, if they like (Figure 7e). Otherwise the
ﬂower is stored in their album as an ‘unknown’ category. An
example user album is shown in Figure 7f.

For each ﬂower, which has been identiﬁed, we provided
its common name and a short description about this ﬂower,
e.g. where it originated from,
its uses, and some trivia
information (Figure 7g). We also provide wikipedia link
that describes this ﬂower. Additionally we provide informa-
tion about regions world-wide where the ﬂower grows (Fig-
ure 7h). All this information has been compiled by our team
of expert biologists. We further plan to provide information
about how to grow the ﬂower for interested users, as well as,
more reﬁned information about the regions where it grows.
We also show the images that the user has collected for a
ﬂower category (Figure 7i).

4. FLOWER DATASET

For the purposes of developing the ﬂower recognition ap-
plication, we have collected a large-scale ﬂower dataset. The
dataset contains 578 diﬀerent ﬂower species and over 250,000
images. This is the only web-scale ﬂowers dataset we are
aware of. While we have compiled a seemingly modest num-
ber of images (250,000 compared to billions of images for
general image search engines), our dataset has exhausted the

Figure 4: Example images from diﬀerent classes from the large-scale 578 ﬂower species dataset. A large variety of ﬂower
classes, as well as, intra-class variabilities, inter-class similarities, and changes in ﬂower scales are available in this dataset.

web resources of available ﬂower images, and for some rare
species we could not ﬁnd a suﬃcient number of images. Fur-
thermore, this is the largest dataset for ﬂower recognition,
and also the largest of its kind among any other ﬁne-grained
recognition datasets we are aware of [13, 20, 22, 24, 28]. All
previous datasets contain at most 200 diﬀerent species [28]
and the largest known ﬂower dataset contains 102 classes
and 8189 images [22]. Also compared to previous ﬂower
datasets, our dataset is targeted to be used ‘in the ﬁeld’ by
actual users, rather than for experimentation only. Thus
our dataset contains a lot of intra-class variability, changes
to scale, and quality variability than any previous dataset.
Figure 4 shows example images from diﬀerent classes, and
as seen the images are much more realistic, which makes
the recognition task harder. On the other hand, this data
is much closer to the data distribution an actual user will
encounter as they collect images by their phones and submit
them to the app.

Collecting, deﬁning and organizing the dataset for this
application was a very challenging task, which is why we
describe those eﬀorts here.

4.1 Deﬁning the set of classes

The ﬁrst important question to address is to determine
which set of classes the system is going to recognize. While
there are many ﬂowering plants that potentially exist world-
wide [6], the largest number of species that an expert botanist
is able to recognize is estimated to be about 30,000. Natu-
rally, here we are striving towards recognizing as many ﬂower
species as possible, so as to achieve a maximal coverage of

ﬂowers that users may encounter. However, some real-life
limitations exist. One issue is that collecting and curating
images from such a dataset would be prohibitive, especially
since only ﬂower experts can do the identiﬁcation and cu-
ration (i.e.
labeling, or placing into categories, and giving
the correct scientiﬁc name). A more serious issue is that
there are no suﬃcient number of images for the less com-
mon ﬂower categories. Furthermore, such a large number
of classes is also pushing the boundaries of what we can do
technologically to recognize automatically.

To address this problem, we focused on the most common
ﬂowers which have suﬃciently large coverage over all possi-
ble ﬂowers, and ignored the ’long tail’ set of ﬂowers. More
speciﬁcally, our goal is to select the most frequent classes
which cover 90% of all ﬂowers world-wide. Since, it is not
clear how to estimate such a set, we deﬁned the classes in two
steps. First, our team selected the set of top 500 most pop-
ular ﬂowers in the world. We further collected images from
the web when typing general ﬂower-related terms such as
’ﬂowers’, ’desert ﬂowers’, ’mountain ﬂowers’, ’wedding ﬂow-
ers’, ’spring ﬂowers’, ’exotic ﬂowers’, etc. The latter was an
attempt to retrieve ‘all possible’ types of ﬂowers and their
natural distribution. In this way we could only capture the
ﬂower distribution that occurs on the web, which reﬂects
areas of the world with at least some technology penetra-
tion, rather than the actual distribution which is unknown,
but this was a good approximation, given that the users of
our mobile app will also be representative of technologically
advanced regions. After collecting such a set, the biologists
on our team labeled each image and identiﬁed ﬂowers that

are not included in our pre-deﬁned set of classes. We then
included the next most common ﬂower species, until their
coverage reached 90%. As a result of that we obtained a set
of 578 species of ﬂowers.

This, of course, is only approximately capturing the full
coverage of our application, and only after actual users start
to use the application, will we know if our coverage is suﬃ-
cient. Naturally, our set of classes can evolve with usage of
the app, and this can improve the usefulness of the applica-
tion, as well.
4.2 Data collection

Since we are developing user-based application which is
going to work with user provided data, we do not necessar-
ily have such data to begin with. Here we describe how we
bootstrap the mechanism for data collection so that to im-
rove the data quality for our app. Initially we use images
collected from the web (Section 4.2.1), then we collected
user data by crowd-sourcing (Section 4.2.3), ﬁnally, after
the application is launched we can collect user-uploaded im-
ages (Section 7). In a sense the dataset evolves until we can
serve the users best.

4.2.1 Web crawling
Initially we collected data from the web, by automatically
downloading images corresponding to each ﬂower category
by keyword search on Bing and Google search engines. We
typically used the oﬃcial name of the ﬂower (e.g.
‘matri-
caria recutita’) and one or more of its common names (e.g.
chamomile). Online search engines were very helpful to pro-
vide ﬂower images, however since the search is done by key-
words, a lot of the returned images are irrelevant to the
ﬂower category and need to be ﬁltered out.

4.2.2 Near-duplicate elimination
Near-duplicate elimination is a process which removes ex-
amples that are visually very similar or almost identical.
Typically near-duplicates are images of the same object but
one of them has been modiﬁed in some way, e.g. by rescal-
ing, compression, by adding noise, or by adding a logo or
modifying a part of the image. Near-duplicate elimination
is needed because our application shows a set of images for
each proposed class and the user can look at them and de-
cide if the class is the best match to the query ﬂower. In
this situation, we really want to show diverse images. Sec-
ondly, near-duplicate images have almost identical feature
representations and are not useful for training or testing of
our algorithm.

For near-duplicate elimination we have extracted HOG
features [10] in the whole image. These features are then
encoded in 8196-dimensional global feature dictionary using
the Local Linear Coding method [26]. All the features in the
image are combined by a global max-pooling [26] to obtain a
ﬁnal 8196-dimensional representation similar to [21]. This is
essentially a signature-like summarization of the whole im-
age, so two images which diﬀer only due to rescaling, some
noise or re-compression will likely have very similar signa-
tures. Images whose signatures diﬀer by less than a prespec-
iﬁed threshold are eliminated as duplicates and a represen-
tative single image is kept in the ﬁnal collection. Note that
the feature representation is derived from the more complex
representation, presented later in Section 5 which is used for
ﬁnal classiﬁcation into diﬀerent classes.

Figure 5: Example images of a rose returned by Bing Search
Engine (top panel), example images of a rose obtained by a
mobile phone (bottom row). As seen, the quality of these
images is very diﬀerent, so images obtained online will not
be suﬃcient to cover the test distribution of mobile images.

We are not aware of any other system using the method
for near-duplicate detection described above and we chose
it for simplicity and ease of application. There are other
near-duplicate detection methods that have been proposed
and used in practice [17, 19], but we found the method we
have used as suﬃcient for our goals.

4.2.3 Collecting data via crowd-sourcing
Since our data were collected from web sources, naturally,
their distribution is very diﬀerent from the distribution of
images provided by mobile phone users. Figure 5 shows
example image for a ‘rose’ that are returned by Bing search
engine [2] and other examples which are collected ‘in the
ﬁeld’ by a mobile device. As seen those are very diﬀerent.
Therefore, we needed to collect extra images which are closer
to the user-generated content by mobile devices.

For that purpose we used a crowd-sourcing service for data
collection. Apart from obtaining images closer to the images
by our potential app user, we are aiming to get diversity both
in geographical locations and also from user to user (e.g. a
variety of user demographics, photography skill levels, etc.).
Mobile phone users on a crowd-sourcing service were in-
structed to capture ﬂower images. We gave fairly loose
guidelines so that they maximally match the users of the
app, who will not be given instructions but will be guided
by the UI to take ﬂower images for recognition. Most of the
users did what is expected, but for some users we had to
reﬁne and reinforce the instructions mostly for the sake of
collecting useful data for our algorithms. In general, what
we learned in the process was that we need to set up fairly
simple and straightforward requirements for crowd-sourcing
users.

Overall we are very pleased with the dataset collected from
crowd-coursing: some of the agents paid to collect images
were very industrious and creative and provided images of
exotic and rare plants, which they have collected from vaca-
tions or by visiting botanical gardens and arboreta. Figure 6

Figure 6: Example images collected by diﬀerent users via a crowd-sourcing service. As seen, the users provided very good
variety of ﬂower species from diﬀerent geo-locations but also images vastly varying in quality. Each row corresponds to a
separate user. Real mobile phone users were crucial in our experience, because their images reﬂect better the distribution of
images that our app users are likely to upload.

shows images that the users collected, which shows us the
diversity one can get thanks to crowd-sourcing. Naturally,
we also have some images that will be considered of ‘poor
quality’ in any other dataset, e.g. with shadows, blur, etc.,
but those are very useful for our application, because they
represent images that mobile phone users may take.
4.3 Labeling

A very demanding task is that of labeling the images with
their correct ﬂower class. We note here that only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do this task. For ex-
ample, Amazon’s Mechanical Turk [1] users would generally
not be able to provide such services because it is crucial
to recognize subtle diﬀerences between species. Also note
that labeling is a painstaking manual process, because each
image has to be examined to verify it belongs to a speciﬁc
category.
In some cases the identiﬁcation process can be
very complicated in which the botanists answer a sequence
of questions until the ﬂower is identiﬁed [11]. While there is
no established protocol, which makes the task so challeng-
ing, the botanists try to determine if it is a ﬂower or an
inﬂorescence (branch of ﬂowers, e.g. the ’ﬂower’ we see in a
Heliconia or a sunﬂower), the number of petals, stamens and
carpels; whether the petals (stamens or carples) are fused or
free, the type of symmetry of the ﬂoral whorls, etc. In order
to label all images that belong to a certain class, the biology
experts in our team followed this complex and demanding
procedure for ﬂower identiﬁcation.

In addition to labeling, the dataset had to be cleaned to re-
move images of non-ﬂowers or of artiﬁcial ﬂowers (e.g. draw-
ings, cartoons, clipart, etc). Although the process did not
require expert knowledge, it is very tedious.
Initially we

have collected more than 3.5 million images which had to be
ﬁltered.
4.4 Discussion

As is clear from this section, collecting and labeling this
data was a major accomplishment of our team, and is still
a work in progress. This dataset, with its 250,000 images
across 578 ﬂower categories, is the largest we are aware of.
For very common categories there are many images avail-
able, but we have limited the number we store so that the
dataset is more balanced. For the categories which are very
rare, although we attempted to collect several thousands
images that contain the respective categories, much fewer
results were available online. Moreover, the number of actu-
ally useful images (the ones that contain a ﬂower or ﬂowers
of this category only) was very small. The images we col-
lected, about 250,000 may seem relatively small compared to
web image corpora, but with this dataset we have exhausted
what search engines can return, e.g. for very rare categories.
Future work needs to address collecting examples for these
rare categories. User interactions with the app will serve as
a means to redeﬁne and expand the ﬂower dataset.
5. RECOGNITION ALGORITHM

A key to the application presented is its recognition engine

and algorithm, which we describe in this section.

The algorithm is based on the one of Lin et al. [21]. The
algorithm starts by feature extraction on the input image:
we ﬁrst extract HOG features [10] at 4 diﬀerent levels. These
features are then encoded in 8196-dimensional global feature
dictionary using the Local Linear Coding method of Wang
et al. [26]. The global dictionary is created by clustering
of low-level patches generated from a set of natural images

which are unrelated to the classiﬁcation task at hand. En-
coding by using global dictionaries have been shown to be
very advantageous for visual recognition [26]. After that, a
global max pooling of the encoded features in the image is
done, as well as, max poolings in a 3x3 grid of the image [21].
Combining all the poolings we obtain a vector representa-
tion which is 10 times the initial dictionary dimensionality,
or about 80,000-dimensional feature vector.

This feature representation is then classiﬁed into indi-
vidual 578 classes by a multi-class classiﬁcation algorithm,
such as Support Vector Machines (SVM). Our classiﬁcation
pipeline uses the 1-vs-all strategy of linear classiﬁcation and
we used a Stochastic Gradient Descent version of the algo-
rithm [21] for these purposes since our dataset of 250,000
images does not ﬁt on a single machine and standard SVM
implementations [12] cannot be applied.

This recognition algorithm is selected because of its per-
formance capabilities (see Section 6) and its runtime (for
this app we need real-time performance because in addition
to the recognition time, the user experiences additional time
latencies for uploading the image and downloading and vi-
sualizing the results). We also preferred it for being general
and easy to use.

Note that the recognition algorithm has been previously
used for general object recognition (e.g. recognize a ﬂower
from a piano from a dog, etc), but here we have applied it
for recognition into very closely related objects as is the case
of diﬀerent types of ﬂowers.

6. TESTING AND DEPLOYMENT

We evaluate here the performance of our recognition al-
gorithm. The results of the classiﬁcation performance are
shown in Table 1. We computed the accuracy of the test set
for top 1, top 2, . . ., top 10 returned results. As seen we can
achieve 52.3% for top 1, 75.2% for top 5, and 82.3% for top
10 results. Translating this to the context of the functional-
ity of the app, we found it useful to show top 5 suggestions
per query image, and then allow the user to expand to top
10 suggestions. That is, we can hope to provide the correct
answer among our suggestions in more than 82% of the time.
We tested our recognition on another ﬂower dataset, Ox-
ford 102 ﬂowers [22], which has been widely used in the
computer vision community. This is done as to see how
the performance of our algorithm compares to other known
methods in the literature, since we could not test all other
algorithms on our large-scale dataset (since most algorithms
are not publicly available, or they are not adequate for multi-
machine parallel computations). As seen in Table 2, our al-
gorithm achieves state-of-the-art performance, and is even a
little better than the best known algorithm for this dataset.
Apart from performing very well, it has other advantages,
e.g.
it is very general and easy to use. We want to note
that better performances for the Oxford dataset have been
reported [9], but they require segmentation of the image
which is still computationally ineﬃcient and cannot be used
directly since our app’s response should be real-time.

6.1 Practical considerations in deployment

Runtime. One important thing to consider is runtime.
Apart from the time needed to upload the image and down-
load the results, the most time is consumed in the recog-
nition algorithm itself. Currently our recognition algorithm

Table 1: Accuracy of our recognition algorithm for top 1 to
top 5, top 7 and top 10 retrieved classes. As seen for top 10
we achieve 82 percent correct.

Top 1 Top 2 Top 3 Top 4 Top 5 Top 7 Top 10
52.3

63.4

75.2

68.9

72.4

78.9

82.3

Table 2: Accuracy of our recognition algorithm, compared
to other baseline methods on the smaller Oxford 102 ﬂowers
dataset.

Method

Accuracy (%)

Kanan and Cottrell [18]

Nilsback and Zisserman [22]

Ito and Cubota [16]

Nilsback and Zisserman [23]

Ours

72.0
72.8
74.8
76.3
76.7

takes about 1-2 seconds (it typically works within 1 sec-
ond and only very rarely needs more time). The upload and
download times are relatively small since we resize the image
before uploading it and download only image thumbnails ini-
tially. Larger size images are downloaded in the background
to keep the latency small. The current response time is ad-
equate for our application, we believe, but we continue to
improve its eﬃciency.

Scalability. We have deployed our application on Sales-

force’s Heroku cloud platform [5]. It is available at
http://mobileﬂora.heroku.com.

Logging. We also considered logging (see Section 7 for
more details) so that we can track performance as the app is
being used, and gauge whether users often encounter prob-
lems with ﬂower misclassiﬁcation or with the app itself.

Ease of use. We have limited the app functionality to a

minimum so that it is easy and self-explanatory to use.

User perception. As any user-facing application, the
actual performance numbers do not matter if the user per-
ceives the results presented as inadequate. For example, we
could guess correctly the class at position 5, but if the top
4 results are very diﬀerent visually (e.g.
returning a yel-
low dandelion for an input image containing a red rose), the
user’s perception will be that the application performance
is poor. While our algorithm tends to return visually con-
sistent results as top choices (see Figure 2) thanks to the
feature representation we used, more can be done to enforce
visual similarity.

7. COLLECTING USER FEEDBACK

We have designed our app to collect information which we

are going to use for further improvement.

Usability. We enabled user feedback in which the users
can provide free-form comments, asking for features, report-
ing issues, bugs etc.

Recognition accuracy. We provided the users an op-
tion to expand the top 5 choices provided to top 10 choices.
As seen from our evaluation (Table 1) the accuracy among
the top 10 is 82.3%. We observed that showing more op-
tions is counter-productive, because although the chances of
showing the correct results increases, a lot more irrelevant
results start to be shown. If the ﬂower is not found among
the provided choices, the user has the option to specify its

name. Both events of expanding the top choices and en-
tering a ﬂower name are recorded and sent to our servers.
This feedback is very useful to know for which examples our
recognition engine did not do well. Furthermore, since the
images that are submitted to our service are stored, we can
further evaluate our performance on user supplied images.

Dataset coverage. As already mentioned, we provided
the users an option to name a ﬂower which our system does
not identify correctly among the top 5 or top 10 choices.
The users can give a common name of a ﬂower (or whichever
name they chose). This is reﬂected in their own image collec-
tion, e.g. if they take an image of a rose we fail to recognize,
they can name the ﬂower ’rose’ and it will be assigned to an
album with other roses. This provides us feedback that our
recognition is incorrect, but also that a ﬂower class may not
exist in our dataset. This will help to expand the dataset
with new ﬂower species and thus to increase the coverage of
ﬂowers our application can provide.

Collecting the above mentioned information allows us to
improve the application’s functionality, increase the recogni-
tion performance, and expand the image corpus, all of which
will improve the app’s usability. In this way the application
can evolve and improve with its use. Note that this is done
without collecting any user-identiﬁable or private informa-
tion.

8. RELATED WORK

There are several mobile apps that can help the users to
identify ﬂowers, e.g. Audubon Wildﬂowers, Flower Pedia,
What Flower, etc., but these applications work as a refer-
ence guides without actually providing the capability to au-
tomatically recognize and identify the ﬂower from an input
image.

A recent mobile app for tree-leaf recognition has been de-
veloped. It is called Leafsnap [20] and can recognize among
184 diﬀerent tree leaves. Although the recognition technol-
ogy of LeafSnap is very diﬀerent from ours, we share the
common goal of bringing computer vision technology to an
actual product that people can use for free and to allow
them to do tasks that they would not be able to do with-
out these technologies. The Leafsnap app has attracted a
large number of users and has found a number of diﬀerent
applications [20]. We are not aware of any other mobile ap-
plications in the ﬁne-grained recognition domain, but clearly
they will be very useful, especially for mushroom or plant
recognition [14].

Several algorithms for ﬁne-grained recognition have been
proposed and applied to various of datasets: e.g. for birds
species recognition [8, 13], for ﬂowers recognition [22], and
for cats and dogs recognition [24]. These methods have
shown great promise experimentally, but have yet to be de-
ployed in practice.

Regarding the recognition technology, there are alterna-
tive solutions that achieve state-of-the-art recognition per-
formance, e.g. by applying metric learning [27], exemplar-
based matching and retrieval [20], trace-norm regularization
optimization [15] and possibly others. We hope that some
of these technologies will ﬁnd applications and be deployed
to more user-facing products.

gies is one of the areas that provides enormous opportunities
to make a diﬀerence in people’s everyday lives.

In this paper we demonstrate how to apply state-of-the-art
computer vision object recognition technology to discrimi-
nate between 578 diﬀerent species of ﬂowers and automat-
ically recognize an unknown ﬂower from an input image.
We have developed a mobile phone app that is available for
free to users. We discussed all aspects of development and
deployment of our recognition technology in practice.

We hope that the experiences shared in this paper can
generalize to other recognition tasks and their practical ap-
plications. For example, the recognition engine can be ap-
plied for recognition of species of birds [28] or mushrooms, or
for one of the most challenging tasks, plant recognition [14].
All of these tasks have much practical importance because
people often wonder whether a plant is dangerous, what is
it, etc.
9.1 Future work

As future work we plan to improve the classiﬁcation per-

formance and runtime of our algorithm.

One way to improve performance is to move towards hi-
erarchical representation so that the whole image collection
is a taxonomy. Taxonomic representation can open the door
to exploring classiﬁcation algorithms or cost metrics that do
not penalize as much misclassiﬁcation among very similar
classes (e.g. it may not be as problematic which orchid it is
as long as we do not confuse it with other non-related ﬂow-
ers). Flower species are naturally organized in a taxonomy,
so we can take advantage of that in the future.

Improvements to the dataset will also have impact of the
overall quality of the results. As already mentioned, we
plan to use user-uploaded images and images from crowd-
sourcing to improve the coverage of the dataset. Further
analysis of our classiﬁcation results revealed that some classes
may be too broad or a number of classes may be too sim-
ilar. For example, if a class contains too much variance it
may make sense to split it and train it separately. And, con-
versely, if a several classes invoke too much confusion among
each other, it may make sense to combine them for train-
ing and use a very specialized algorithm to recognize among
them.

Another avenue for future work is how to utilize the feed-
back provided by users. One interesting machine learning
problem is how to automatically clean or reconcile the la-
bels assigned by users, given that some may be noisy or not
trustworthy.

10. ACKNOWLEDGMENTS

We would like to thank the members of the Plant and
Microbial Biology Department at UC Berkeley: Andrew
Brown, Kelsie Morioka, Lily Liu, and Crystal Sun who pro-
vided the crucial curation and labeling of the ﬁnal dataset,
and Chodon Sass and Ana Maria Almeida who provided cu-
ration of an earlier version of the dataset. We thank Marco
Gloria who implemented the app. We also thank the numer-
ous participants in the crowdsourcing service who collected
diverse ﬂower images, and volunteers at NEC Labs America
who helped us with the data collection.

9. CONCLUSIONS AND FUTURE WORK
Applying computer vision and machine learning technolo-

11. REFERENCES
[1] Amazon Mechanical Turk:

https://www.mturk.com/mturk/welcome/.

Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Figure 2: One of the discriminating characteristics of our application is that it can generalize within a class and can ignore
irrelevant characteristics, such as color. Results of recognizing an image containing a Gerbera ﬂower by Google Search-By-
Image service (left), in which all returned images match the color distribution but no Gerbera ﬂower is retrieved, and our
system (right) which correctly identiﬁes the ﬂower species of the query image and also returns somewhat similar categories as
choices 2 and 3. The web-based version of our application is shown here.

phone app1 (Figure 1) and as a web-based service2. While
some reference applications for ﬂowers already exist (e.g.
Flower Pedia, Audubon Wildﬂowers, What Flower, etc), our
app has the unique capability to automatically recognize the
ﬂower from an image. This is of much value to interested
users because no previous application can accomplish such
a task, especially from a mobile device. The system is capa-
ble of recognizing 578 diﬀerent ﬂower species and relies on
web-scale image dataset that has been collected from various
sources and has been carefully labeled by a team of expert
botanists. Our recognition system is speciﬁcally designed
to handle user-generated images collected ‘in the ﬁeld’ by
mobile phone users of various devices. The latter is impor-
tant since user-generated imagery may signiﬁcantly vary in
quality and will diﬀer from the images of ﬂowers that are
encountered on web-search engines. We describe details of
designing the system, developing the algorithms, as well as,
collecting the dataset that can support this capability.

Recognizing among diﬀerent types within the same gen-
eral class of objects is called ﬁne-grained classiﬁcation [13,
22, 28], and is one of the active areas of research in com-
puter vision. Unlike the so called basic-level recognition,
which refers to recognizing whether an object is a ﬂower,
or a cat, or a chair, ﬁne-grained recognition works within a
single base-level category and aims to recognize among its
subcategories, such as diﬀerent species of ﬂowers or diﬀer-
ent species of birds, plants, etc. Fine-grained classiﬁcation
requires knowledge and skills that only trained experts can
provide.
In the case of ﬂower recognition, only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do the task, and some-

1The app is developed for iPhone and is called ‘Mobile
Flora’.
It is currently under invitation-only beta testing,
and will soon be available for general use. Please email the
contact authors to send you an invitation.
2The web-based version of
http://mobileﬂora.heroku.com

the app is available at

times not without using further references [11], or following
a complex identiﬁcation procedure (see Section 4.3). Since
very few people can successfully recognize among a large
number of species, this makes the automatic ﬁne-grained
recognition application very valuable to non-experts.

General object recognition, and more speciﬁcally, ﬁne-
grained recognition, are still problems that are far from be-
ing solved algorithmically. Large-scale image retrieval sys-
tems [3, 4, 7, 25, 29] have been quite successful at retrieving
a duplicate or near-duplicate images [7, 4], similar images [4],
or extracting images ‘by sketch’ [25]. However, they do not
have the capability to generalize within a class, e.g. to recog-
nize that some object is a vacuum cleaner, for example, and
do not have the capabilities to discriminate between very re-
lated but diﬀerent classes of the same super-class, as is the
case with ﬂowers. Furthermore, the computer vision recog-
nition technologies that are available on the web are mostly
targetted towards retrieval of web content, rather than user
generated ‘ﬁeld’ images. With the advent of social media
and mobile phones, naturally, the demand for recognition of
user-generated images is increasing.

Another diﬀerentiating capability between our ﬂower recog-
nition technology and other available image search or image
retrieval systems, e.g. Google’s Search-By-Image service [4]
is that our system is speciﬁcally targeted towards recogni-
tion, in our case of ﬂower species. That is, our system is
trained to identify features that generalize within a class,
whereas other systems are more general and very often re-
turn irrelevant results (i.e. non-ﬂowers) or match the input
ﬂower image by color distribution or other general visual
features, rather than retrieving ﬂowers from the same class.
Figure 2 shows an example search of an image of a Gerbera
ﬂower by Google Search-By-Image service [4] and by our sys-
tem (which can be found at http://mobileﬂora.heroku.com).
As seen, although many images of Gerbera ﬂowers are avail-
able in Google’s image corpus, not a single Gerbera ﬂower
was retrieved. Instead our system is capable of recognizing

the type of ﬂower and providing related images. We note
here that this example shows typical retrieval diﬀerences, es-
pecially for ﬂower images that do not have a near-duplicate
counterpart on the web, e.g. images in personal collections,
or obtained by mobile devices, etc.

2. FINE-GRAINED RECOGNITION

CHALLENGES

Fine-grained classiﬁcation has its unique challenges be-
cause it aims to recognize among classes that will typically
be considered as a part of one large class in standard visual
object recognition tasks.

The main inherent challenge is that some species have
very ﬁne diﬀerences which are hard to notice for the com-
mon eye. For example, the categories that need to be dis-
criminated among each other can be very similar and would
require careful examination to determine the correct class.
Figure 3 shows examples from three diﬀerent ﬂower species:
Matricaria chamomilla (chamomile), Bellis perennis (com-
mon daisy), and Leucanthemum vulgare (ox-eye daisy). As
seen these three categories are very similar to each other.

Another challenge in a ﬁne-grained recognition task is the
variability in viewpoints, colors, and scales, that are encoun-
tered within the same class. These variabilities are mostly
problematic to an automatic system rather than a human
observer. Many ﬂowers can have diﬀerent colors but a per-
son can very easily factor out that invariance, which is not
necessarily true for an algorithm. Similarly, it is very easy
for a person to understand if the image contains a close-
up shot of a single ﬂower or multiple small ﬂowers of the
same type, but this problem can be quite challenging for an
automatic vision system.

Other potential issues speciﬁc to an automatic recognition
system is the background, which can serve as a distractor,
e.g. when the system assumes it is part of the ﬂower, occlu-
sions over the ﬂowers, and others.

3. FLOWER RECOGNITION

APPLICATION

Our system and the mobile phone app are intended to
recognize the image of a ﬂower of a blooming plant or tree.
Figure 7 of the Appendix shows the main functionalities of
the app.

The entry point of the app is to take a picture of a ﬂower
and submit it for recognition (Figure 7a, and also Figure 1).
While taking the picture, a ﬂower contour is overlayed to
indicate that the ﬂower is expected to take the full image
and be within the boundaries of the image. These are only
guidelines and, as our users will discover, the app is much
more powerful and is capable of recognizing a ﬂower that
takes signiﬁcantly more or less of the area that is desirable.
Multiple ﬂowers of the same type (also called ‘cluster ﬂow-
ers’) are also not a problem for recognition.

Results of the top ﬁve choices of potential classes that the
input image can belong to are shown to the user (Figure 7b).
Since some classes may be very similar, as seen in Figure 3,
we found it much more useful to provide several choices,
instead of the top one only.

After the user is oﬀered a set of options of what the input
ﬂower could be, they can examine and compare the images
from the proposed categories (Figure 7c) and make a deci-
sion which class the ﬂower belongs to (Figure 7d). Here, in

Figure 3: One of the challenges for ﬁne-grained recognition is
that some classes can be very similar to one another and only
experts can discriminate them. This makes an automatic
recognition system very valuable to non-experts. Each row
shows a diﬀerent ﬂower species.

our application, we have taken advantage of the fact that
people may not be very good at recognizing a particular
ﬂower, especially among such large variety of 578 diﬀerent
species, but when presented with a set of options they can
be pretty good at verifying which one it is.

More results are available upon request, but we provide
top ten choices at most to avoid showing too many irrelevant
results, since the more results shown the more likely they
will be very diﬀerent from the input category. If the top ten
results do not provide satisfactory choice, the users can give
a name to the ﬂower, if they like (Figure 7e). Otherwise the
ﬂower is stored in their album as an ‘unknown’ category. An
example user album is shown in Figure 7f.

For each ﬂower, which has been identiﬁed, we provided
its common name and a short description about this ﬂower,
e.g. where it originated from,
its uses, and some trivia
information (Figure 7g). We also provide wikipedia link
that describes this ﬂower. Additionally we provide informa-
tion about regions world-wide where the ﬂower grows (Fig-
ure 7h). All this information has been compiled by our team
of expert biologists. We further plan to provide information
about how to grow the ﬂower for interested users, as well as,
more reﬁned information about the regions where it grows.
We also show the images that the user has collected for a
ﬂower category (Figure 7i).

4. FLOWER DATASET

For the purposes of developing the ﬂower recognition ap-
plication, we have collected a large-scale ﬂower dataset. The
dataset contains 578 diﬀerent ﬂower species and over 250,000
images. This is the only web-scale ﬂowers dataset we are
aware of. While we have compiled a seemingly modest num-
ber of images (250,000 compared to billions of images for
general image search engines), our dataset has exhausted the

Figure 4: Example images from diﬀerent classes from the large-scale 578 ﬂower species dataset. A large variety of ﬂower
classes, as well as, intra-class variabilities, inter-class similarities, and changes in ﬂower scales are available in this dataset.

web resources of available ﬂower images, and for some rare
species we could not ﬁnd a suﬃcient number of images. Fur-
thermore, this is the largest dataset for ﬂower recognition,
and also the largest of its kind among any other ﬁne-grained
recognition datasets we are aware of [13, 20, 22, 24, 28]. All
previous datasets contain at most 200 diﬀerent species [28]
and the largest known ﬂower dataset contains 102 classes
and 8189 images [22]. Also compared to previous ﬂower
datasets, our dataset is targeted to be used ‘in the ﬁeld’ by
actual users, rather than for experimentation only. Thus
our dataset contains a lot of intra-class variability, changes
to scale, and quality variability than any previous dataset.
Figure 4 shows example images from diﬀerent classes, and
as seen the images are much more realistic, which makes
the recognition task harder. On the other hand, this data
is much closer to the data distribution an actual user will
encounter as they collect images by their phones and submit
them to the app.

Collecting, deﬁning and organizing the dataset for this
application was a very challenging task, which is why we
describe those eﬀorts here.

4.1 Deﬁning the set of classes

The ﬁrst important question to address is to determine
which set of classes the system is going to recognize. While
there are many ﬂowering plants that potentially exist world-
wide [6], the largest number of species that an expert botanist
is able to recognize is estimated to be about 30,000. Natu-
rally, here we are striving towards recognizing as many ﬂower
species as possible, so as to achieve a maximal coverage of

ﬂowers that users may encounter. However, some real-life
limitations exist. One issue is that collecting and curating
images from such a dataset would be prohibitive, especially
since only ﬂower experts can do the identiﬁcation and cu-
ration (i.e.
labeling, or placing into categories, and giving
the correct scientiﬁc name). A more serious issue is that
there are no suﬃcient number of images for the less com-
mon ﬂower categories. Furthermore, such a large number
of classes is also pushing the boundaries of what we can do
technologically to recognize automatically.

To address this problem, we focused on the most common
ﬂowers which have suﬃciently large coverage over all possi-
ble ﬂowers, and ignored the ’long tail’ set of ﬂowers. More
speciﬁcally, our goal is to select the most frequent classes
which cover 90% of all ﬂowers world-wide. Since, it is not
clear how to estimate such a set, we deﬁned the classes in two
steps. First, our team selected the set of top 500 most pop-
ular ﬂowers in the world. We further collected images from
the web when typing general ﬂower-related terms such as
’ﬂowers’, ’desert ﬂowers’, ’mountain ﬂowers’, ’wedding ﬂow-
ers’, ’spring ﬂowers’, ’exotic ﬂowers’, etc. The latter was an
attempt to retrieve ‘all possible’ types of ﬂowers and their
natural distribution. In this way we could only capture the
ﬂower distribution that occurs on the web, which reﬂects
areas of the world with at least some technology penetra-
tion, rather than the actual distribution which is unknown,
but this was a good approximation, given that the users of
our mobile app will also be representative of technologically
advanced regions. After collecting such a set, the biologists
on our team labeled each image and identiﬁed ﬂowers that

are not included in our pre-deﬁned set of classes. We then
included the next most common ﬂower species, until their
coverage reached 90%. As a result of that we obtained a set
of 578 species of ﬂowers.

This, of course, is only approximately capturing the full
coverage of our application, and only after actual users start
to use the application, will we know if our coverage is suﬃ-
cient. Naturally, our set of classes can evolve with usage of
the app, and this can improve the usefulness of the applica-
tion, as well.
4.2 Data collection

Since we are developing user-based application which is
going to work with user provided data, we do not necessar-
ily have such data to begin with. Here we describe how we
bootstrap the mechanism for data collection so that to im-
rove the data quality for our app. Initially we use images
collected from the web (Section 4.2.1), then we collected
user data by crowd-sourcing (Section 4.2.3), ﬁnally, after
the application is launched we can collect user-uploaded im-
ages (Section 7). In a sense the dataset evolves until we can
serve the users best.

4.2.1 Web crawling
Initially we collected data from the web, by automatically
downloading images corresponding to each ﬂower category
by keyword search on Bing and Google search engines. We
typically used the oﬃcial name of the ﬂower (e.g.
‘matri-
caria recutita’) and one or more of its common names (e.g.
chamomile). Online search engines were very helpful to pro-
vide ﬂower images, however since the search is done by key-
words, a lot of the returned images are irrelevant to the
ﬂower category and need to be ﬁltered out.

4.2.2 Near-duplicate elimination
Near-duplicate elimination is a process which removes ex-
amples that are visually very similar or almost identical.
Typically near-duplicates are images of the same object but
one of them has been modiﬁed in some way, e.g. by rescal-
ing, compression, by adding noise, or by adding a logo or
modifying a part of the image. Near-duplicate elimination
is needed because our application shows a set of images for
each proposed class and the user can look at them and de-
cide if the class is the best match to the query ﬂower. In
this situation, we really want to show diverse images. Sec-
ondly, near-duplicate images have almost identical feature
representations and are not useful for training or testing of
our algorithm.

For near-duplicate elimination we have extracted HOG
features [10] in the whole image. These features are then
encoded in 8196-dimensional global feature dictionary using
the Local Linear Coding method [26]. All the features in the
image are combined by a global max-pooling [26] to obtain a
ﬁnal 8196-dimensional representation similar to [21]. This is
essentially a signature-like summarization of the whole im-
age, so two images which diﬀer only due to rescaling, some
noise or re-compression will likely have very similar signa-
tures. Images whose signatures diﬀer by less than a prespec-
iﬁed threshold are eliminated as duplicates and a represen-
tative single image is kept in the ﬁnal collection. Note that
the feature representation is derived from the more complex
representation, presented later in Section 5 which is used for
ﬁnal classiﬁcation into diﬀerent classes.

Figure 5: Example images of a rose returned by Bing Search
Engine (top panel), example images of a rose obtained by a
mobile phone (bottom row). As seen, the quality of these
images is very diﬀerent, so images obtained online will not
be suﬃcient to cover the test distribution of mobile images.

We are not aware of any other system using the method
for near-duplicate detection described above and we chose
it for simplicity and ease of application. There are other
near-duplicate detection methods that have been proposed
and used in practice [17, 19], but we found the method we
have used as suﬃcient for our goals.

4.2.3 Collecting data via crowd-sourcing
Since our data were collected from web sources, naturally,
their distribution is very diﬀerent from the distribution of
images provided by mobile phone users. Figure 5 shows
example image for a ‘rose’ that are returned by Bing search
engine [2] and other examples which are collected ‘in the
ﬁeld’ by a mobile device. As seen those are very diﬀerent.
Therefore, we needed to collect extra images which are closer
to the user-generated content by mobile devices.

For that purpose we used a crowd-sourcing service for data
collection. Apart from obtaining images closer to the images
by our potential app user, we are aiming to get diversity both
in geographical locations and also from user to user (e.g. a
variety of user demographics, photography skill levels, etc.).
Mobile phone users on a crowd-sourcing service were in-
structed to capture ﬂower images. We gave fairly loose
guidelines so that they maximally match the users of the
app, who will not be given instructions but will be guided
by the UI to take ﬂower images for recognition. Most of the
users did what is expected, but for some users we had to
reﬁne and reinforce the instructions mostly for the sake of
collecting useful data for our algorithms. In general, what
we learned in the process was that we need to set up fairly
simple and straightforward requirements for crowd-sourcing
users.

Overall we are very pleased with the dataset collected from
crowd-coursing: some of the agents paid to collect images
were very industrious and creative and provided images of
exotic and rare plants, which they have collected from vaca-
tions or by visiting botanical gardens and arboreta. Figure 6

Figure 6: Example images collected by diﬀerent users via a crowd-sourcing service. As seen, the users provided very good
variety of ﬂower species from diﬀerent geo-locations but also images vastly varying in quality. Each row corresponds to a
separate user. Real mobile phone users were crucial in our experience, because their images reﬂect better the distribution of
images that our app users are likely to upload.

shows images that the users collected, which shows us the
diversity one can get thanks to crowd-sourcing. Naturally,
we also have some images that will be considered of ‘poor
quality’ in any other dataset, e.g. with shadows, blur, etc.,
but those are very useful for our application, because they
represent images that mobile phone users may take.
4.3 Labeling

A very demanding task is that of labeling the images with
their correct ﬂower class. We note here that only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do this task. For ex-
ample, Amazon’s Mechanical Turk [1] users would generally
not be able to provide such services because it is crucial
to recognize subtle diﬀerences between species. Also note
that labeling is a painstaking manual process, because each
image has to be examined to verify it belongs to a speciﬁc
category.
In some cases the identiﬁcation process can be
very complicated in which the botanists answer a sequence
of questions until the ﬂower is identiﬁed [11]. While there is
no established protocol, which makes the task so challeng-
ing, the botanists try to determine if it is a ﬂower or an
inﬂorescence (branch of ﬂowers, e.g. the ’ﬂower’ we see in a
Heliconia or a sunﬂower), the number of petals, stamens and
carpels; whether the petals (stamens or carples) are fused or
free, the type of symmetry of the ﬂoral whorls, etc. In order
to label all images that belong to a certain class, the biology
experts in our team followed this complex and demanding
procedure for ﬂower identiﬁcation.

In addition to labeling, the dataset had to be cleaned to re-
move images of non-ﬂowers or of artiﬁcial ﬂowers (e.g. draw-
ings, cartoons, clipart, etc). Although the process did not
require expert knowledge, it is very tedious.
Initially we

have collected more than 3.5 million images which had to be
ﬁltered.
4.4 Discussion

As is clear from this section, collecting and labeling this
data was a major accomplishment of our team, and is still
a work in progress. This dataset, with its 250,000 images
across 578 ﬂower categories, is the largest we are aware of.
For very common categories there are many images avail-
able, but we have limited the number we store so that the
dataset is more balanced. For the categories which are very
rare, although we attempted to collect several thousands
images that contain the respective categories, much fewer
results were available online. Moreover, the number of actu-
ally useful images (the ones that contain a ﬂower or ﬂowers
of this category only) was very small. The images we col-
lected, about 250,000 may seem relatively small compared to
web image corpora, but with this dataset we have exhausted
what search engines can return, e.g. for very rare categories.
Future work needs to address collecting examples for these
rare categories. User interactions with the app will serve as
a means to redeﬁne and expand the ﬂower dataset.
5. RECOGNITION ALGORITHM

A key to the application presented is its recognition engine

and algorithm, which we describe in this section.

The algorithm is based on the one of Lin et al. [21]. The
algorithm starts by feature extraction on the input image:
we ﬁrst extract HOG features [10] at 4 diﬀerent levels. These
features are then encoded in 8196-dimensional global feature
dictionary using the Local Linear Coding method of Wang
et al. [26]. The global dictionary is created by clustering
of low-level patches generated from a set of natural images

which are unrelated to the classiﬁcation task at hand. En-
coding by using global dictionaries have been shown to be
very advantageous for visual recognition [26]. After that, a
global max pooling of the encoded features in the image is
done, as well as, max poolings in a 3x3 grid of the image [21].
Combining all the poolings we obtain a vector representa-
tion which is 10 times the initial dictionary dimensionality,
or about 80,000-dimensional feature vector.

This feature representation is then classiﬁed into indi-
vidual 578 classes by a multi-class classiﬁcation algorithm,
such as Support Vector Machines (SVM). Our classiﬁcation
pipeline uses the 1-vs-all strategy of linear classiﬁcation and
we used a Stochastic Gradient Descent version of the algo-
rithm [21] for these purposes since our dataset of 250,000
images does not ﬁt on a single machine and standard SVM
implementations [12] cannot be applied.

This recognition algorithm is selected because of its per-
formance capabilities (see Section 6) and its runtime (for
this app we need real-time performance because in addition
to the recognition time, the user experiences additional time
latencies for uploading the image and downloading and vi-
sualizing the results). We also preferred it for being general
and easy to use.

Note that the recognition algorithm has been previously
used for general object recognition (e.g. recognize a ﬂower
from a piano from a dog, etc), but here we have applied it
for recognition into very closely related objects as is the case
of diﬀerent types of ﬂowers.

6. TESTING AND DEPLOYMENT

We evaluate here the performance of our recognition al-
gorithm. The results of the classiﬁcation performance are
shown in Table 1. We computed the accuracy of the test set
for top 1, top 2, . . ., top 10 returned results. As seen we can
achieve 52.3% for top 1, 75.2% for top 5, and 82.3% for top
10 results. Translating this to the context of the functional-
ity of the app, we found it useful to show top 5 suggestions
per query image, and then allow the user to expand to top
10 suggestions. That is, we can hope to provide the correct
answer among our suggestions in more than 82% of the time.
We tested our recognition on another ﬂower dataset, Ox-
ford 102 ﬂowers [22], which has been widely used in the
computer vision community. This is done as to see how
the performance of our algorithm compares to other known
methods in the literature, since we could not test all other
algorithms on our large-scale dataset (since most algorithms
are not publicly available, or they are not adequate for multi-
machine parallel computations). As seen in Table 2, our al-
gorithm achieves state-of-the-art performance, and is even a
little better than the best known algorithm for this dataset.
Apart from performing very well, it has other advantages,
e.g.
it is very general and easy to use. We want to note
that better performances for the Oxford dataset have been
reported [9], but they require segmentation of the image
which is still computationally ineﬃcient and cannot be used
directly since our app’s response should be real-time.

6.1 Practical considerations in deployment

Runtime. One important thing to consider is runtime.
Apart from the time needed to upload the image and down-
load the results, the most time is consumed in the recog-
nition algorithm itself. Currently our recognition algorithm

Table 1: Accuracy of our recognition algorithm for top 1 to
top 5, top 7 and top 10 retrieved classes. As seen for top 10
we achieve 82 percent correct.

Top 1 Top 2 Top 3 Top 4 Top 5 Top 7 Top 10
52.3

63.4

75.2

68.9

72.4

78.9

82.3

Table 2: Accuracy of our recognition algorithm, compared
to other baseline methods on the smaller Oxford 102 ﬂowers
dataset.

Method

Accuracy (%)

Kanan and Cottrell [18]

Nilsback and Zisserman [22]

Ito and Cubota [16]

Nilsback and Zisserman [23]

Ours

72.0
72.8
74.8
76.3
76.7

takes about 1-2 seconds (it typically works within 1 sec-
ond and only very rarely needs more time). The upload and
download times are relatively small since we resize the image
before uploading it and download only image thumbnails ini-
tially. Larger size images are downloaded in the background
to keep the latency small. The current response time is ad-
equate for our application, we believe, but we continue to
improve its eﬃciency.

Scalability. We have deployed our application on Sales-

force’s Heroku cloud platform [5]. It is available at
http://mobileﬂora.heroku.com.

Logging. We also considered logging (see Section 7 for
more details) so that we can track performance as the app is
being used, and gauge whether users often encounter prob-
lems with ﬂower misclassiﬁcation or with the app itself.

Ease of use. We have limited the app functionality to a

minimum so that it is easy and self-explanatory to use.

User perception. As any user-facing application, the
actual performance numbers do not matter if the user per-
ceives the results presented as inadequate. For example, we
could guess correctly the class at position 5, but if the top
4 results are very diﬀerent visually (e.g.
returning a yel-
low dandelion for an input image containing a red rose), the
user’s perception will be that the application performance
is poor. While our algorithm tends to return visually con-
sistent results as top choices (see Figure 2) thanks to the
feature representation we used, more can be done to enforce
visual similarity.

7. COLLECTING USER FEEDBACK

We have designed our app to collect information which we

are going to use for further improvement.

Usability. We enabled user feedback in which the users
can provide free-form comments, asking for features, report-
ing issues, bugs etc.

Recognition accuracy. We provided the users an op-
tion to expand the top 5 choices provided to top 10 choices.
As seen from our evaluation (Table 1) the accuracy among
the top 10 is 82.3%. We observed that showing more op-
tions is counter-productive, because although the chances of
showing the correct results increases, a lot more irrelevant
results start to be shown. If the ﬂower is not found among
the provided choices, the user has the option to specify its

name. Both events of expanding the top choices and en-
tering a ﬂower name are recorded and sent to our servers.
This feedback is very useful to know for which examples our
recognition engine did not do well. Furthermore, since the
images that are submitted to our service are stored, we can
further evaluate our performance on user supplied images.

Dataset coverage. As already mentioned, we provided
the users an option to name a ﬂower which our system does
not identify correctly among the top 5 or top 10 choices.
The users can give a common name of a ﬂower (or whichever
name they chose). This is reﬂected in their own image collec-
tion, e.g. if they take an image of a rose we fail to recognize,
they can name the ﬂower ’rose’ and it will be assigned to an
album with other roses. This provides us feedback that our
recognition is incorrect, but also that a ﬂower class may not
exist in our dataset. This will help to expand the dataset
with new ﬂower species and thus to increase the coverage of
ﬂowers our application can provide.

Collecting the above mentioned information allows us to
improve the application’s functionality, increase the recogni-
tion performance, and expand the image corpus, all of which
will improve the app’s usability. In this way the application
can evolve and improve with its use. Note that this is done
without collecting any user-identiﬁable or private informa-
tion.

8. RELATED WORK

There are several mobile apps that can help the users to
identify ﬂowers, e.g. Audubon Wildﬂowers, Flower Pedia,
What Flower, etc., but these applications work as a refer-
ence guides without actually providing the capability to au-
tomatically recognize and identify the ﬂower from an input
image.

A recent mobile app for tree-leaf recognition has been de-
veloped. It is called Leafsnap [20] and can recognize among
184 diﬀerent tree leaves. Although the recognition technol-
ogy of LeafSnap is very diﬀerent from ours, we share the
common goal of bringing computer vision technology to an
actual product that people can use for free and to allow
them to do tasks that they would not be able to do with-
out these technologies. The Leafsnap app has attracted a
large number of users and has found a number of diﬀerent
applications [20]. We are not aware of any other mobile ap-
plications in the ﬁne-grained recognition domain, but clearly
they will be very useful, especially for mushroom or plant
recognition [14].

Several algorithms for ﬁne-grained recognition have been
proposed and applied to various of datasets: e.g. for birds
species recognition [8, 13], for ﬂowers recognition [22], and
for cats and dogs recognition [24]. These methods have
shown great promise experimentally, but have yet to be de-
ployed in practice.

Regarding the recognition technology, there are alterna-
tive solutions that achieve state-of-the-art recognition per-
formance, e.g. by applying metric learning [27], exemplar-
based matching and retrieval [20], trace-norm regularization
optimization [15] and possibly others. We hope that some
of these technologies will ﬁnd applications and be deployed
to more user-facing products.

gies is one of the areas that provides enormous opportunities
to make a diﬀerence in people’s everyday lives.

In this paper we demonstrate how to apply state-of-the-art
computer vision object recognition technology to discrimi-
nate between 578 diﬀerent species of ﬂowers and automat-
ically recognize an unknown ﬂower from an input image.
We have developed a mobile phone app that is available for
free to users. We discussed all aspects of development and
deployment of our recognition technology in practice.

We hope that the experiences shared in this paper can
generalize to other recognition tasks and their practical ap-
plications. For example, the recognition engine can be ap-
plied for recognition of species of birds [28] or mushrooms, or
for one of the most challenging tasks, plant recognition [14].
All of these tasks have much practical importance because
people often wonder whether a plant is dangerous, what is
it, etc.
9.1 Future work

As future work we plan to improve the classiﬁcation per-

formance and runtime of our algorithm.

One way to improve performance is to move towards hi-
erarchical representation so that the whole image collection
is a taxonomy. Taxonomic representation can open the door
to exploring classiﬁcation algorithms or cost metrics that do
not penalize as much misclassiﬁcation among very similar
classes (e.g. it may not be as problematic which orchid it is
as long as we do not confuse it with other non-related ﬂow-
ers). Flower species are naturally organized in a taxonomy,
so we can take advantage of that in the future.

Improvements to the dataset will also have impact of the
overall quality of the results. As already mentioned, we
plan to use user-uploaded images and images from crowd-
sourcing to improve the coverage of the dataset. Further
analysis of our classiﬁcation results revealed that some classes
may be too broad or a number of classes may be too sim-
ilar. For example, if a class contains too much variance it
may make sense to split it and train it separately. And, con-
versely, if a several classes invoke too much confusion among
each other, it may make sense to combine them for train-
ing and use a very specialized algorithm to recognize among
them.

Another avenue for future work is how to utilize the feed-
back provided by users. One interesting machine learning
problem is how to automatically clean or reconcile the la-
bels assigned by users, given that some may be noisy or not
trustworthy.

10. ACKNOWLEDGMENTS

We would like to thank the members of the Plant and
Microbial Biology Department at UC Berkeley: Andrew
Brown, Kelsie Morioka, Lily Liu, and Crystal Sun who pro-
vided the crucial curation and labeling of the ﬁnal dataset,
and Chodon Sass and Ana Maria Almeida who provided cu-
ration of an earlier version of the dataset. We thank Marco
Gloria who implemented the app. We also thank the numer-
ous participants in the crowdsourcing service who collected
diverse ﬂower images, and volunteers at NEC Labs America
who helped us with the data collection.

9. CONCLUSIONS AND FUTURE WORK
Applying computer vision and machine learning technolo-

11. REFERENCES
[1] Amazon Mechanical Turk:

https://www.mturk.com/mturk/welcome/.

[22] M.-E. Nilsback and A. Zisserman. A. automated

ﬂower classiﬁcation over a large number of classes.
Indian Conference on Computer Vision, Graphics and
Image Processing, 2008.

[23] M.-E. Nilsback and A. Zisserman. An automatic visual

ﬂora - segmentation and classiﬁcation of ﬂower
images. DPhil Thesis, University of Oxford, UK, 2009.

[24] O. Parkhi, A. Vedaldi, C. V. Jawahar, and

A. Zisserman. The truth about cats and dogs.
International Conference on Computer Vision, 2011.

[25] C. Wang, Z. Li, and L. Zhang. Mindﬁnder: Image

search by interactive sketching and tagging.
international World Wide Web Conference, 2010.

[26] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and

Y. Gong. Locality-constrained linear coding for image
classiﬁcation. Computer Vision and Pattern
Recognition, 2010.

[27] K. Weinberger, J. Blitzer, and L. Saul. Distance
metric learning for large margin nearest neighbor
classiﬁcation. Neural Information Processing Systems
Conference, 2006.

[28] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroﬀ,

S. Belongie, and P. Perona. Caltech-ucsd birds 200.
Technical Report CNS-TR-2010-001, California
Institute of Technology, 2010.

[29] H. Xu, J. Wang, and S. L. X.-S. Hua. Interactive

image search by 2d semantic map. international World
Wide Web Conference, 2010.

APPENDIX
We show the main functionalities of the app in Figure 7.

[2] Bing: http://www.bing.com/.
[3] Google Goggles:

http://www.google.com/mobile/goggles/.

[4] Google Search-By-Image service:
http://www.images.google.com/.

[5] Heroku: http://www.heroku.com/.
[6] The Plant List: http://www.theplantlist.org/.
[7] Tineye Reverse Image Search:

http://www.tineye.com/.

[8] S. Branson, C. Wah, B. Babenko, F. Schroﬀ,

P. Welinder, P. Perona, and S. Belongie. Visual
recognition with humans in the loop. European
Conference on Computer Vision, 2010.

[9] Y. Chai, V. Lempitsky, and A. Zisserman. Bicos: A

bi-level co-segmentation method for image
classiﬁcation. International Conference on Computer
Vision, 2011.

[10] N. Dalal and B. Triggs. Histograms of oriented

gradients for human detection. Computer Vision and
Pattern Recognition, 2005.

[11] T. Elpel. Botany in a Day. HOPS Press, 2000.
[12] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang,
and C.-J. Lin. Liblinear: A library for large linear
classiﬁcation. Journal of Machine Learning Research,
2008.

[13] R. Farrell, O. Oza, N. Zhang, V. Morariu, T. Darrell,

and L. Davis. Birdlets: Subordinate categorization
using volumetric primitives and pose-normalized
appearance. International Conference on Computer
Vision, 2011.

[14] H. Goeau, P. Bonnet, A. Joly, N. Boujemaa,

D. Barthelemy, J. Molino, P. Birnbaum, E. Mouysset,
and M. Picard. The CLEF 2011 plant image
classiﬁcation task. In CLEF 2011 working notes, 2011.

[15] Z. Harchaoui, M. Douze, M. Paulin, M. Dudik, and
J. Malick. Large-scale classiﬁcation with trace-norm
regularization. Computer Vision and Pattern
Recognition, 2012.

[16] S. Ito and S. Kubota. Object classﬁcation using
heterogeneous co-occurrence features. European
Conference on Computer Vision, 2010.

[17] A. Jaimes, S.-F. Chang, , and A. Loui. Detection of

non-identical duplicate consumer photographs.
Proceedings of 4th IEEE Paciﬁc-Rim Conference on
Multimedia, 2003.

[18] C. Kanan and G. Cottrell. Robust classiﬁcation of

objects, faces, and ﬂowers using natural image
statistics. Computer Vision and Pattern Recognition,
2010.

[19] Y. Ke, R. Sukthankar, and L. Huston. Eﬃcient

near-duplicate detection and sub-image retrieval.
ACM Int. Conf. on Multimedia, 2004.

[20] N. Kumar, P. Belhumeur, A. Biswas, D. Jacobs,

J. Kress, I. Lopez, and J. Soares. Leafsnap: A
computer vision system for automatic plant species
identiﬁcation. European Conference on Computer
Vision, 2012.

[21] Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu,

L. Cao, and T. Huang. Large-scale image classiﬁcation:
fast feature extraction and SVM training. Computer
Vision and Pattern Recognition, 2011.

Development and Deployment of a Large-Scale Flower

Recognition Mobile App

∗
Anelia Angelova
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA

anelia@nec-labs.com

Shenghuo Zhu
NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
zsh@nec-labs.com

Yuanqing Lin

NEC Labs America

10080 North Wolfe Road

Cupertino, CA, USA
ylin@nec-labs.com

Josephine Wong
Bio-Rad Laboratories
Pleasanton CA, USA

josephine_wong@bio-

rad.com

Chelsea Specht

Department of Plant and

Microbial Biology, UC Berkeley

Berkeley, CA, USA

cdspecht@berkeley.edu

ABSTRACT
Today’s major image search engines, which focus on search-
by-image-content, work by matching and retrieving of im-
ages that are already available on the web. With the prolif-
eration of user generated content, especially from mobile de-
vices, there is a need to develop applications which are more
content-aware, i.e. can understand and recognize what is in
the image, and which can handle the deteriorated quality
inherent to user images uploaded from mobile devices.

In this paper we describe a mobile phone application in-
tended to automatically recognize ﬂower images taken by
users and classify them into a set of ﬂower categories. The
app is served by a web-scale image search engine and re-
lies on computer vision recognition technology. The mobile
phone app is available free to users, and as a web interface.
We share experiences about designing, developing, and
deploying such a system in practice. We discuss practi-
cal aspects of applying the object recognition technology,
including runtime, scalability, and data collection which is
crucial for such data-driven application. More speciﬁcally,
we describe a large-scale dataset which was collected in a
number of stages to be as close as possible to the data dis-
tribution the actual users of the app will encounter. We
further describe our strategy for collecting user feedback,
which we view as an opportunity to improve the server-side
algorithms and datasets.

We envision that these issues will be encountered in sim-
ilar applications, e.g. for recognition of plants, mushrooms,
or bird species, all of which have practical importance for
users, and we hope this work will be also useful for develop-
ing other types of applications.

To our knowledge, this is the ﬁrst mobile app which can
automatically recognize as many as 578 species of ﬂowers,
and which is available for free to users. The ﬂower dataset
that serves the application, speciﬁcally collected for this
recognition task, is the ﬁrst and largest in its scale and scope.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Online Infor-

∗Contact author

Figure 1: Snapshots from the ﬂower recognition app for
iPhone. An image of a ﬂower is taken (left), our recogni-
tion engine processes the image and returns top ﬁve choices
of possible ﬂower classes that match the input ﬂower best
(right). The user can then make the best selection.

mation Services; I.4 [Image Processing and Computer
Vision]: Applications; I.5 [Pattern Recognition]: Imple-
mentation

General Terms
Experimentation, Performance, Algorithms

Keywords
Fine-grained categorization, visual object recognition, large-
scale datasets, crowd-sourcing, mobile applications

1.

INTRODUCTION

This paper presents an end-to-end visual recognition sys-
tem whose task is to automatically identify the species of
the ﬂower in an input image.
It is available as a mobile

Figure 2: One of the discriminating characteristics of our application is that it can generalize within a class and can ignore
irrelevant characteristics, such as color. Results of recognizing an image containing a Gerbera ﬂower by Google Search-By-
Image service (left), in which all returned images match the color distribution but no Gerbera ﬂower is retrieved, and our
system (right) which correctly identiﬁes the ﬂower species of the query image and also returns somewhat similar categories as
choices 2 and 3. The web-based version of our application is shown here.

phone app1 (Figure 1) and as a web-based service2. While
some reference applications for ﬂowers already exist (e.g.
Flower Pedia, Audubon Wildﬂowers, What Flower, etc), our
app has the unique capability to automatically recognize the
ﬂower from an image. This is of much value to interested
users because no previous application can accomplish such
a task, especially from a mobile device. The system is capa-
ble of recognizing 578 diﬀerent ﬂower species and relies on
web-scale image dataset that has been collected from various
sources and has been carefully labeled by a team of expert
botanists. Our recognition system is speciﬁcally designed
to handle user-generated images collected ‘in the ﬁeld’ by
mobile phone users of various devices. The latter is impor-
tant since user-generated imagery may signiﬁcantly vary in
quality and will diﬀer from the images of ﬂowers that are
encountered on web-search engines. We describe details of
designing the system, developing the algorithms, as well as,
collecting the dataset that can support this capability.

Recognizing among diﬀerent types within the same gen-
eral class of objects is called ﬁne-grained classiﬁcation [13,
22, 28], and is one of the active areas of research in com-
puter vision. Unlike the so called basic-level recognition,
which refers to recognizing whether an object is a ﬂower,
or a cat, or a chair, ﬁne-grained recognition works within a
single base-level category and aims to recognize among its
subcategories, such as diﬀerent species of ﬂowers or diﬀer-
ent species of birds, plants, etc. Fine-grained classiﬁcation
requires knowledge and skills that only trained experts can
provide.
In the case of ﬂower recognition, only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do the task, and some-

1The app is developed for iPhone and is called ‘Mobile
Flora’.
It is currently under invitation-only beta testing,
and will soon be available for general use. Please email the
contact authors to send you an invitation.
2The web-based version of
http://mobileﬂora.heroku.com

the app is available at

times not without using further references [11], or following
a complex identiﬁcation procedure (see Section 4.3). Since
very few people can successfully recognize among a large
number of species, this makes the automatic ﬁne-grained
recognition application very valuable to non-experts.

General object recognition, and more speciﬁcally, ﬁne-
grained recognition, are still problems that are far from be-
ing solved algorithmically. Large-scale image retrieval sys-
tems [3, 4, 7, 25, 29] have been quite successful at retrieving
a duplicate or near-duplicate images [7, 4], similar images [4],
or extracting images ‘by sketch’ [25]. However, they do not
have the capability to generalize within a class, e.g. to recog-
nize that some object is a vacuum cleaner, for example, and
do not have the capabilities to discriminate between very re-
lated but diﬀerent classes of the same super-class, as is the
case with ﬂowers. Furthermore, the computer vision recog-
nition technologies that are available on the web are mostly
targetted towards retrieval of web content, rather than user
generated ‘ﬁeld’ images. With the advent of social media
and mobile phones, naturally, the demand for recognition of
user-generated images is increasing.

Another diﬀerentiating capability between our ﬂower recog-
nition technology and other available image search or image
retrieval systems, e.g. Google’s Search-By-Image service [4]
is that our system is speciﬁcally targeted towards recogni-
tion, in our case of ﬂower species. That is, our system is
trained to identify features that generalize within a class,
whereas other systems are more general and very often re-
turn irrelevant results (i.e. non-ﬂowers) or match the input
ﬂower image by color distribution or other general visual
features, rather than retrieving ﬂowers from the same class.
Figure 2 shows an example search of an image of a Gerbera
ﬂower by Google Search-By-Image service [4] and by our sys-
tem (which can be found at http://mobileﬂora.heroku.com).
As seen, although many images of Gerbera ﬂowers are avail-
able in Google’s image corpus, not a single Gerbera ﬂower
was retrieved. Instead our system is capable of recognizing

the type of ﬂower and providing related images. We note
here that this example shows typical retrieval diﬀerences, es-
pecially for ﬂower images that do not have a near-duplicate
counterpart on the web, e.g. images in personal collections,
or obtained by mobile devices, etc.

2. FINE-GRAINED RECOGNITION

CHALLENGES

Fine-grained classiﬁcation has its unique challenges be-
cause it aims to recognize among classes that will typically
be considered as a part of one large class in standard visual
object recognition tasks.

The main inherent challenge is that some species have
very ﬁne diﬀerences which are hard to notice for the com-
mon eye. For example, the categories that need to be dis-
criminated among each other can be very similar and would
require careful examination to determine the correct class.
Figure 3 shows examples from three diﬀerent ﬂower species:
Matricaria chamomilla (chamomile), Bellis perennis (com-
mon daisy), and Leucanthemum vulgare (ox-eye daisy). As
seen these three categories are very similar to each other.

Another challenge in a ﬁne-grained recognition task is the
variability in viewpoints, colors, and scales, that are encoun-
tered within the same class. These variabilities are mostly
problematic to an automatic system rather than a human
observer. Many ﬂowers can have diﬀerent colors but a per-
son can very easily factor out that invariance, which is not
necessarily true for an algorithm. Similarly, it is very easy
for a person to understand if the image contains a close-
up shot of a single ﬂower or multiple small ﬂowers of the
same type, but this problem can be quite challenging for an
automatic vision system.

Other potential issues speciﬁc to an automatic recognition
system is the background, which can serve as a distractor,
e.g. when the system assumes it is part of the ﬂower, occlu-
sions over the ﬂowers, and others.

3. FLOWER RECOGNITION

APPLICATION

Our system and the mobile phone app are intended to
recognize the image of a ﬂower of a blooming plant or tree.
Figure 7 of the Appendix shows the main functionalities of
the app.

The entry point of the app is to take a picture of a ﬂower
and submit it for recognition (Figure 7a, and also Figure 1).
While taking the picture, a ﬂower contour is overlayed to
indicate that the ﬂower is expected to take the full image
and be within the boundaries of the image. These are only
guidelines and, as our users will discover, the app is much
more powerful and is capable of recognizing a ﬂower that
takes signiﬁcantly more or less of the area that is desirable.
Multiple ﬂowers of the same type (also called ‘cluster ﬂow-
ers’) are also not a problem for recognition.

Results of the top ﬁve choices of potential classes that the
input image can belong to are shown to the user (Figure 7b).
Since some classes may be very similar, as seen in Figure 3,
we found it much more useful to provide several choices,
instead of the top one only.

After the user is oﬀered a set of options of what the input
ﬂower could be, they can examine and compare the images
from the proposed categories (Figure 7c) and make a deci-
sion which class the ﬂower belongs to (Figure 7d). Here, in

Figure 3: One of the challenges for ﬁne-grained recognition is
that some classes can be very similar to one another and only
experts can discriminate them. This makes an automatic
recognition system very valuable to non-experts. Each row
shows a diﬀerent ﬂower species.

our application, we have taken advantage of the fact that
people may not be very good at recognizing a particular
ﬂower, especially among such large variety of 578 diﬀerent
species, but when presented with a set of options they can
be pretty good at verifying which one it is.

More results are available upon request, but we provide
top ten choices at most to avoid showing too many irrelevant
results, since the more results shown the more likely they
will be very diﬀerent from the input category. If the top ten
results do not provide satisfactory choice, the users can give
a name to the ﬂower, if they like (Figure 7e). Otherwise the
ﬂower is stored in their album as an ‘unknown’ category. An
example user album is shown in Figure 7f.

For each ﬂower, which has been identiﬁed, we provided
its common name and a short description about this ﬂower,
e.g. where it originated from,
its uses, and some trivia
information (Figure 7g). We also provide wikipedia link
that describes this ﬂower. Additionally we provide informa-
tion about regions world-wide where the ﬂower grows (Fig-
ure 7h). All this information has been compiled by our team
of expert biologists. We further plan to provide information
about how to grow the ﬂower for interested users, as well as,
more reﬁned information about the regions where it grows.
We also show the images that the user has collected for a
ﬂower category (Figure 7i).

4. FLOWER DATASET

For the purposes of developing the ﬂower recognition ap-
plication, we have collected a large-scale ﬂower dataset. The
dataset contains 578 diﬀerent ﬂower species and over 250,000
images. This is the only web-scale ﬂowers dataset we are
aware of. While we have compiled a seemingly modest num-
ber of images (250,000 compared to billions of images for
general image search engines), our dataset has exhausted the

Figure 4: Example images from diﬀerent classes from the large-scale 578 ﬂower species dataset. A large variety of ﬂower
classes, as well as, intra-class variabilities, inter-class similarities, and changes in ﬂower scales are available in this dataset.

web resources of available ﬂower images, and for some rare
species we could not ﬁnd a suﬃcient number of images. Fur-
thermore, this is the largest dataset for ﬂower recognition,
and also the largest of its kind among any other ﬁne-grained
recognition datasets we are aware of [13, 20, 22, 24, 28]. All
previous datasets contain at most 200 diﬀerent species [28]
and the largest known ﬂower dataset contains 102 classes
and 8189 images [22]. Also compared to previous ﬂower
datasets, our dataset is targeted to be used ‘in the ﬁeld’ by
actual users, rather than for experimentation only. Thus
our dataset contains a lot of intra-class variability, changes
to scale, and quality variability than any previous dataset.
Figure 4 shows example images from diﬀerent classes, and
as seen the images are much more realistic, which makes
the recognition task harder. On the other hand, this data
is much closer to the data distribution an actual user will
encounter as they collect images by their phones and submit
them to the app.

Collecting, deﬁning and organizing the dataset for this
application was a very challenging task, which is why we
describe those eﬀorts here.

4.1 Deﬁning the set of classes

The ﬁrst important question to address is to determine
which set of classes the system is going to recognize. While
there are many ﬂowering plants that potentially exist world-
wide [6], the largest number of species that an expert botanist
is able to recognize is estimated to be about 30,000. Natu-
rally, here we are striving towards recognizing as many ﬂower
species as possible, so as to achieve a maximal coverage of

ﬂowers that users may encounter. However, some real-life
limitations exist. One issue is that collecting and curating
images from such a dataset would be prohibitive, especially
since only ﬂower experts can do the identiﬁcation and cu-
ration (i.e.
labeling, or placing into categories, and giving
the correct scientiﬁc name). A more serious issue is that
there are no suﬃcient number of images for the less com-
mon ﬂower categories. Furthermore, such a large number
of classes is also pushing the boundaries of what we can do
technologically to recognize automatically.

To address this problem, we focused on the most common
ﬂowers which have suﬃciently large coverage over all possi-
ble ﬂowers, and ignored the ’long tail’ set of ﬂowers. More
speciﬁcally, our goal is to select the most frequent classes
which cover 90% of all ﬂowers world-wide. Since, it is not
clear how to estimate such a set, we deﬁned the classes in two
steps. First, our team selected the set of top 500 most pop-
ular ﬂowers in the world. We further collected images from
the web when typing general ﬂower-related terms such as
’ﬂowers’, ’desert ﬂowers’, ’mountain ﬂowers’, ’wedding ﬂow-
ers’, ’spring ﬂowers’, ’exotic ﬂowers’, etc. The latter was an
attempt to retrieve ‘all possible’ types of ﬂowers and their
natural distribution. In this way we could only capture the
ﬂower distribution that occurs on the web, which reﬂects
areas of the world with at least some technology penetra-
tion, rather than the actual distribution which is unknown,
but this was a good approximation, given that the users of
our mobile app will also be representative of technologically
advanced regions. After collecting such a set, the biologists
on our team labeled each image and identiﬁed ﬂowers that

are not included in our pre-deﬁned set of classes. We then
included the next most common ﬂower species, until their
coverage reached 90%. As a result of that we obtained a set
of 578 species of ﬂowers.

This, of course, is only approximately capturing the full
coverage of our application, and only after actual users start
to use the application, will we know if our coverage is suﬃ-
cient. Naturally, our set of classes can evolve with usage of
the app, and this can improve the usefulness of the applica-
tion, as well.
4.2 Data collection

Since we are developing user-based application which is
going to work with user provided data, we do not necessar-
ily have such data to begin with. Here we describe how we
bootstrap the mechanism for data collection so that to im-
rove the data quality for our app. Initially we use images
collected from the web (Section 4.2.1), then we collected
user data by crowd-sourcing (Section 4.2.3), ﬁnally, after
the application is launched we can collect user-uploaded im-
ages (Section 7). In a sense the dataset evolves until we can
serve the users best.

4.2.1 Web crawling
Initially we collected data from the web, by automatically
downloading images corresponding to each ﬂower category
by keyword search on Bing and Google search engines. We
typically used the oﬃcial name of the ﬂower (e.g.
‘matri-
caria recutita’) and one or more of its common names (e.g.
chamomile). Online search engines were very helpful to pro-
vide ﬂower images, however since the search is done by key-
words, a lot of the returned images are irrelevant to the
ﬂower category and need to be ﬁltered out.

4.2.2 Near-duplicate elimination
Near-duplicate elimination is a process which removes ex-
amples that are visually very similar or almost identical.
Typically near-duplicates are images of the same object but
one of them has been modiﬁed in some way, e.g. by rescal-
ing, compression, by adding noise, or by adding a logo or
modifying a part of the image. Near-duplicate elimination
is needed because our application shows a set of images for
each proposed class and the user can look at them and de-
cide if the class is the best match to the query ﬂower. In
this situation, we really want to show diverse images. Sec-
ondly, near-duplicate images have almost identical feature
representations and are not useful for training or testing of
our algorithm.

For near-duplicate elimination we have extracted HOG
features [10] in the whole image. These features are then
encoded in 8196-dimensional global feature dictionary using
the Local Linear Coding method [26]. All the features in the
image are combined by a global max-pooling [26] to obtain a
ﬁnal 8196-dimensional representation similar to [21]. This is
essentially a signature-like summarization of the whole im-
age, so two images which diﬀer only due to rescaling, some
noise or re-compression will likely have very similar signa-
tures. Images whose signatures diﬀer by less than a prespec-
iﬁed threshold are eliminated as duplicates and a represen-
tative single image is kept in the ﬁnal collection. Note that
the feature representation is derived from the more complex
representation, presented later in Section 5 which is used for
ﬁnal classiﬁcation into diﬀerent classes.

Figure 5: Example images of a rose returned by Bing Search
Engine (top panel), example images of a rose obtained by a
mobile phone (bottom row). As seen, the quality of these
images is very diﬀerent, so images obtained online will not
be suﬃcient to cover the test distribution of mobile images.

We are not aware of any other system using the method
for near-duplicate detection described above and we chose
it for simplicity and ease of application. There are other
near-duplicate detection methods that have been proposed
and used in practice [17, 19], but we found the method we
have used as suﬃcient for our goals.

4.2.3 Collecting data via crowd-sourcing
Since our data were collected from web sources, naturally,
their distribution is very diﬀerent from the distribution of
images provided by mobile phone users. Figure 5 shows
example image for a ‘rose’ that are returned by Bing search
engine [2] and other examples which are collected ‘in the
ﬁeld’ by a mobile device. As seen those are very diﬀerent.
Therefore, we needed to collect extra images which are closer
to the user-generated content by mobile devices.

For that purpose we used a crowd-sourcing service for data
collection. Apart from obtaining images closer to the images
by our potential app user, we are aiming to get diversity both
in geographical locations and also from user to user (e.g. a
variety of user demographics, photography skill levels, etc.).
Mobile phone users on a crowd-sourcing service were in-
structed to capture ﬂower images. We gave fairly loose
guidelines so that they maximally match the users of the
app, who will not be given instructions but will be guided
by the UI to take ﬂower images for recognition. Most of the
users did what is expected, but for some users we had to
reﬁne and reinforce the instructions mostly for the sake of
collecting useful data for our algorithms. In general, what
we learned in the process was that we need to set up fairly
simple and straightforward requirements for crowd-sourcing
users.

Overall we are very pleased with the dataset collected from
crowd-coursing: some of the agents paid to collect images
were very industrious and creative and provided images of
exotic and rare plants, which they have collected from vaca-
tions or by visiting botanical gardens and arboreta. Figure 6

Figure 6: Example images collected by diﬀerent users via a crowd-sourcing service. As seen, the users provided very good
variety of ﬂower species from diﬀerent geo-locations but also images vastly varying in quality. Each row corresponds to a
separate user. Real mobile phone users were crucial in our experience, because their images reﬂect better the distribution of
images that our app users are likely to upload.

shows images that the users collected, which shows us the
diversity one can get thanks to crowd-sourcing. Naturally,
we also have some images that will be considered of ‘poor
quality’ in any other dataset, e.g. with shadows, blur, etc.,
but those are very useful for our application, because they
represent images that mobile phone users may take.
4.3 Labeling

A very demanding task is that of labeling the images with
their correct ﬂower class. We note here that only an expert
botanist, or a person speciﬁcally trained to identify and dis-
criminate between ﬂower species can do this task. For ex-
ample, Amazon’s Mechanical Turk [1] users would generally
not be able to provide such services because it is crucial
to recognize subtle diﬀerences between species. Also note
that labeling is a painstaking manual process, because each
image has to be examined to verify it belongs to a speciﬁc
category.
In some cases the identiﬁcation process can be
very complicated in which the botanists answer a sequence
of questions until the ﬂower is identiﬁed [11]. While there is
no established protocol, which makes the task so challeng-
ing, the botanists try to determine if it is a ﬂower or an
inﬂorescence (branch of ﬂowers, e.g. the ’ﬂower’ we see in a
Heliconia or a sunﬂower), the number of petals, stamens and
carpels; whether the petals (stamens or carples) are fused or
free, the type of symmetry of the ﬂoral whorls, etc. In order
to label all images that belong to a certain class, the biology
experts in our team followed this complex and demanding
procedure for ﬂower identiﬁcation.

In addition to labeling, the dataset had to be cleaned to re-
move images of non-ﬂowers or of artiﬁcial ﬂowers (e.g. draw-
ings, cartoons, clipart, etc). Although the process did not
require expert knowledge, it is very tedious.
Initially we

have collected more than 3.5 million images which had to be
ﬁltered.
4.4 Discussion

As is clear from this section, collecting and labeling this
data was a major accomplishment of our team, and is still
a work in progress. This dataset, with its 250,000 images
across 578 ﬂower categories, is the largest we are aware of.
For very common categories there are many images avail-
able, but we have limited the number we store so that the
dataset is more balanced. For the categories which are very
rare, although we attempted to collect several thousands
images that contain the respective categories, much fewer
results were available online. Moreover, the number of actu-
ally useful images (the ones that contain a ﬂower or ﬂowers
of this category only) was very small. The images we col-
lected, about 250,000 may seem relatively small compared to
web image corpora, but with this dataset we have exhausted
what search engines can return, e.g. for very rare categories.
Future work needs to address collecting examples for these
rare categories. User interactions with the app will serve as
a means to redeﬁne and expand the ﬂower dataset.
5. RECOGNITION ALGORITHM

A key to the application presented is its recognition engine

and algorithm, which we describe in this section.

The algorithm is based on the one of Lin et al. [21]. The
algorithm starts by feature extraction on the input image:
we ﬁrst extract HOG features [10] at 4 diﬀerent levels. These
features are then encoded in 8196-dimensional global feature
dictionary using the Local Linear Coding method of Wang
et al. [26]. The global dictionary is created by clustering
of low-level patches generated from a set of natural images

which are unrelated to the classiﬁcation task at hand. En-
coding by using global dictionaries have been shown to be
very advantageous for visual recognition [26]. After that, a
global max pooling of the encoded features in the image is
done, as well as, max poolings in a 3x3 grid of the image [21].
Combining all the poolings we obtain a vector representa-
tion which is 10 times the initial dictionary dimensionality,
or about 80,000-dimensional feature vector.

This feature representation is then classiﬁed into indi-
vidual 578 classes by a multi-class classiﬁcation algorithm,
such as Support Vector Machines (SVM). Our classiﬁcation
pipeline uses the 1-vs-all strategy of linear classiﬁcation and
we used a Stochastic Gradient Descent version of the algo-
rithm [21] for these purposes since our dataset of 250,000
images does not ﬁt on a single machine and standard SVM
implementations [12] cannot be applied.

This recognition algorithm is selected because of its per-
formance capabilities (see Section 6) and its runtime (for
this app we need real-time performance because in addition
to the recognition time, the user experiences additional time
latencies for uploading the image and downloading and vi-
sualizing the results). We also preferred it for being general
and easy to use.

Note that the recognition algorithm has been previously
used for general object recognition (e.g. recognize a ﬂower
from a piano from a dog, etc), but here we have applied it
for recognition into very closely related objects as is the case
of diﬀerent types of ﬂowers.

6. TESTING AND DEPLOYMENT

We evaluate here the performance of our recognition al-
gorithm. The results of the classiﬁcation performance are
shown in Table 1. We computed the accuracy of the test set
for top 1, top 2, . . ., top 10 returned results. As seen we can
achieve 52.3% for top 1, 75.2% for top 5, and 82.3% for top
10 results. Translating this to the context of the functional-
ity of the app, we found it useful to show top 5 suggestions
per query image, and then allow the user to expand to top
10 suggestions. That is, we can hope to provide the correct
answer among our suggestions in more than 82% of the time.
We tested our recognition on another ﬂower dataset, Ox-
ford 102 ﬂowers [22], which has been widely used in the
computer vision community. This is done as to see how
the performance of our algorithm compares to other known
methods in the literature, since we could not test all other
algorithms on our large-scale dataset (since most algorithms
are not publicly available, or they are not adequate for multi-
machine parallel computations). As seen in Table 2, our al-
gorithm achieves state-of-the-art performance, and is even a
little better than the best known algorithm for this dataset.
Apart from performing very well, it has other advantages,
e.g.
it is very general and easy to use. We want to note
that better performances for the Oxford dataset have been
reported [9], but they require segmentation of the image
which is still computationally ineﬃcient and cannot be used
directly since our app’s response should be real-time.

6.1 Practical considerations in deployment

Runtime. One important thing to consider is runtime.
Apart from the time needed to upload the image and down-
load the results, the most time is consumed in the recog-
nition algorithm itself. Currently our recognition algorithm

Table 1: Accuracy of our recognition algorithm for top 1 to
top 5, top 7 and top 10 retrieved classes. As seen for top 10
we achieve 82 percent correct.

Top 1 Top 2 Top 3 Top 4 Top 5 Top 7 Top 10
52.3

63.4

75.2

68.9

72.4

78.9

82.3

Table 2: Accuracy of our recognition algorithm, compared
to other baseline methods on the smaller Oxford 102 ﬂowers
dataset.

Method

Accuracy (%)

Kanan and Cottrell [18]

Nilsback and Zisserman [22]

Ito and Cubota [16]

Nilsback and Zisserman [23]

Ours

72.0
72.8
74.8
76.3
76.7

takes about 1-2 seconds (it typically works within 1 sec-
ond and only very rarely needs more time). The upload and
download times are relatively small since we resize the image
before uploading it and download only image thumbnails ini-
tially. Larger size images are downloaded in the background
to keep the latency small. The current response time is ad-
equate for our application, we believe, but we continue to
improve its eﬃciency.

Scalability. We have deployed our application on Sales-

force’s Heroku cloud platform [5]. It is available at
http://mobileﬂora.heroku.com.

Logging. We also considered logging (see Section 7 for
more details) so that we can track performance as the app is
being used, and gauge whether users often encounter prob-
lems with ﬂower misclassiﬁcation or with the app itself.

Ease of use. We have limited the app functionality to a

minimum so that it is easy and self-explanatory to use.

User perception. As any user-facing application, the
actual performance numbers do not matter if the user per-
ceives the results presented as inadequate. For example, we
could guess correctly the class at position 5, but if the top
4 results are very diﬀerent visually (e.g.
returning a yel-
low dandelion for an input image containing a red rose), the
user’s perception will be that the application performance
is poor. While our algorithm tends to return visually con-
sistent results as top choices (see Figure 2) thanks to the
feature representation we used, more can be done to enforce
visual similarity.

7. COLLECTING USER FEEDBACK

We have designed our app to collect information which we

are going to use for further improvement.

Usability. We enabled user feedback in which the users
can provide free-form comments, asking for features, report-
ing issues, bugs etc.

Recognition accuracy. We provided the users an op-
tion to expand the top 5 choices provided to top 10 choices.
As seen from our evaluation (Table 1) the accuracy among
the top 10 is 82.3%. We observed that showing more op-
tions is counter-productive, because although the chances of
showing the correct results increases, a lot more irrelevant
results start to be shown. If the ﬂower is not found among
the provided choices, the user has the option to specify its

name. Both events of expanding the top choices and en-
tering a ﬂower name are recorded and sent to our servers.
This feedback is very useful to know for which examples our
recognition engine did not do well. Furthermore, since the
images that are submitted to our service are stored, we can
further evaluate our performance on user supplied images.

Dataset coverage. As already mentioned, we provided
the users an option to name a ﬂower which our system does
not identify correctly among the top 5 or top 10 choices.
The users can give a common name of a ﬂower (or whichever
name they chose). This is reﬂected in their own image collec-
tion, e.g. if they take an image of a rose we fail to recognize,
they can name the ﬂower ’rose’ and it will be assigned to an
album with other roses. This provides us feedback that our
recognition is incorrect, but also that a ﬂower class may not
exist in our dataset. This will help to expand the dataset
with new ﬂower species and thus to increase the coverage of
ﬂowers our application can provide.

Collecting the above mentioned information allows us to
improve the application’s functionality, increase the recogni-
tion performance, and expand the image corpus, all of which
will improve the app’s usability. In this way the application
can evolve and improve with its use. Note that this is done
without collecting any user-identiﬁable or private informa-
tion.

8. RELATED WORK

There are several mobile apps that can help the users to
identify ﬂowers, e.g. Audubon Wildﬂowers, Flower Pedia,
What Flower, etc., but these applications work as a refer-
ence guides without actually providing the capability to au-
tomatically recognize and identify the ﬂower from an input
image.

A recent mobile app for tree-leaf recognition has been de-
veloped. It is called Leafsnap [20] and can recognize among
184 diﬀerent tree leaves. Although the recognition technol-
ogy of LeafSnap is very diﬀerent from ours, we share the
common goal of bringing computer vision technology to an
actual product that people can use for free and to allow
them to do tasks that they would not be able to do with-
out these technologies. The Leafsnap app has attracted a
large number of users and has found a number of diﬀerent
applications [20]. We are not aware of any other mobile ap-
plications in the ﬁne-grained recognition domain, but clearly
they will be very useful, especially for mushroom or plant
recognition [14].

Several algorithms for ﬁne-grained recognition have been
proposed and applied to various of datasets: e.g. for birds
species recognition [8, 13], for ﬂowers recognition [22], and
for cats and dogs recognition [24]. These methods have
shown great promise experimentally, but have yet to be de-
ployed in practice.

Regarding the recognition technology, there are alterna-
tive solutions that achieve state-of-the-art recognition per-
formance, e.g. by applying metric learning [27], exemplar-
based matching and retrieval [20], trace-norm regularization
optimization [15] and possibly others. We hope that some
of these technologies will ﬁnd applications and be deployed
to more user-facing products.

gies is one of the areas that provides enormous opportunities
to make a diﬀerence in people’s everyday lives.

In this paper we demonstrate how to apply state-of-the-art
computer vision object recognition technology to discrimi-
nate between 578 diﬀerent species of ﬂowers and automat-
ically recognize an unknown ﬂower from an input image.
We have developed a mobile phone app that is available for
free to users. We discussed all aspects of development and
deployment of our recognition technology in practice.

We hope that the experiences shared in this paper can
generalize to other recognition tasks and their practical ap-
plications. For example, the recognition engine can be ap-
plied for recognition of species of birds [28] or mushrooms, or
for one of the most challenging tasks, plant recognition [14].
All of these tasks have much practical importance because
people often wonder whether a plant is dangerous, what is
it, etc.
9.1 Future work

As future work we plan to improve the classiﬁcation per-

formance and runtime of our algorithm.

One way to improve performance is to move towards hi-
erarchical representation so that the whole image collection
is a taxonomy. Taxonomic representation can open the door
to exploring classiﬁcation algorithms or cost metrics that do
not penalize as much misclassiﬁcation among very similar
classes (e.g. it may not be as problematic which orchid it is
as long as we do not confuse it with other non-related ﬂow-
ers). Flower species are naturally organized in a taxonomy,
so we can take advantage of that in the future.

Improvements to the dataset will also have impact of the
overall quality of the results. As already mentioned, we
plan to use user-uploaded images and images from crowd-
sourcing to improve the coverage of the dataset. Further
analysis of our classiﬁcation results revealed that some classes
may be too broad or a number of classes may be too sim-
ilar. For example, if a class contains too much variance it
may make sense to split it and train it separately. And, con-
versely, if a several classes invoke too much confusion among
each other, it may make sense to combine them for train-
ing and use a very specialized algorithm to recognize among
them.

Another avenue for future work is how to utilize the feed-
back provided by users. One interesting machine learning
problem is how to automatically clean or reconcile the la-
bels assigned by users, given that some may be noisy or not
trustworthy.

10. ACKNOWLEDGMENTS

We would like to thank the members of the Plant and
Microbial Biology Department at UC Berkeley: Andrew
Brown, Kelsie Morioka, Lily Liu, and Crystal Sun who pro-
vided the crucial curation and labeling of the ﬁnal dataset,
and Chodon Sass and Ana Maria Almeida who provided cu-
ration of an earlier version of the dataset. We thank Marco
Gloria who implemented the app. We also thank the numer-
ous participants in the crowdsourcing service who collected
diverse ﬂower images, and volunteers at NEC Labs America
who helped us with the data collection.

9. CONCLUSIONS AND FUTURE WORK
Applying computer vision and machine learning technolo-

11. REFERENCES
[1] Amazon Mechanical Turk:

https://www.mturk.com/mturk/welcome/.

[22] M.-E. Nilsback and A. Zisserman. A. automated

ﬂower classiﬁcation over a large number of classes.
Indian Conference on Computer Vision, Graphics and
Image Processing, 2008.

[23] M.-E. Nilsback and A. Zisserman. An automatic visual

ﬂora - segmentation and classiﬁcation of ﬂower
images. DPhil Thesis, University of Oxford, UK, 2009.

[24] O. Parkhi, A. Vedaldi, C. V. Jawahar, and

A. Zisserman. The truth about cats and dogs.
International Conference on Computer Vision, 2011.

[25] C. Wang, Z. Li, and L. Zhang. Mindﬁnder: Image

search by interactive sketching and tagging.
international World Wide Web Conference, 2010.

[26] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and

Y. Gong. Locality-constrained linear coding for image
classiﬁcation. Computer Vision and Pattern
Recognition, 2010.

[27] K. Weinberger, J. Blitzer, and L. Saul. Distance
metric learning for large margin nearest neighbor
classiﬁcation. Neural Information Processing Systems
Conference, 2006.

[28] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroﬀ,

S. Belongie, and P. Perona. Caltech-ucsd birds 200.
Technical Report CNS-TR-2010-001, California
Institute of Technology, 2010.

[29] H. Xu, J. Wang, and S. L. X.-S. Hua. Interactive

image search by 2d semantic map. international World
Wide Web Conference, 2010.

APPENDIX
We show the main functionalities of the app in Figure 7.

[2] Bing: http://www.bing.com/.
[3] Google Goggles:

http://www.google.com/mobile/goggles/.

[4] Google Search-By-Image service:
http://www.images.google.com/.

[5] Heroku: http://www.heroku.com/.
[6] The Plant List: http://www.theplantlist.org/.
[7] Tineye Reverse Image Search:

http://www.tineye.com/.

[8] S. Branson, C. Wah, B. Babenko, F. Schroﬀ,

P. Welinder, P. Perona, and S. Belongie. Visual
recognition with humans in the loop. European
Conference on Computer Vision, 2010.

[9] Y. Chai, V. Lempitsky, and A. Zisserman. Bicos: A

bi-level co-segmentation method for image
classiﬁcation. International Conference on Computer
Vision, 2011.

[10] N. Dalal and B. Triggs. Histograms of oriented

gradients for human detection. Computer Vision and
Pattern Recognition, 2005.

[11] T. Elpel. Botany in a Day. HOPS Press, 2000.
[12] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang,
and C.-J. Lin. Liblinear: A library for large linear
classiﬁcation. Journal of Machine Learning Research,
2008.

[13] R. Farrell, O. Oza, N. Zhang, V. Morariu, T. Darrell,

and L. Davis. Birdlets: Subordinate categorization
using volumetric primitives and pose-normalized
appearance. International Conference on Computer
Vision, 2011.

[14] H. Goeau, P. Bonnet, A. Joly, N. Boujemaa,

D. Barthelemy, J. Molino, P. Birnbaum, E. Mouysset,
and M. Picard. The CLEF 2011 plant image
classiﬁcation task. In CLEF 2011 working notes, 2011.

[15] Z. Harchaoui, M. Douze, M. Paulin, M. Dudik, and
J. Malick. Large-scale classiﬁcation with trace-norm
regularization. Computer Vision and Pattern
Recognition, 2012.

[16] S. Ito and S. Kubota. Object classﬁcation using
heterogeneous co-occurrence features. European
Conference on Computer Vision, 2010.

[17] A. Jaimes, S.-F. Chang, , and A. Loui. Detection of

non-identical duplicate consumer photographs.
Proceedings of 4th IEEE Paciﬁc-Rim Conference on
Multimedia, 2003.

[18] C. Kanan and G. Cottrell. Robust classiﬁcation of

objects, faces, and ﬂowers using natural image
statistics. Computer Vision and Pattern Recognition,
2010.

[19] Y. Ke, R. Sukthankar, and L. Huston. Eﬃcient

near-duplicate detection and sub-image retrieval.
ACM Int. Conf. on Multimedia, 2004.

[20] N. Kumar, P. Belhumeur, A. Biswas, D. Jacobs,

J. Kress, I. Lopez, and J. Soares. Leafsnap: A
computer vision system for automatic plant species
identiﬁcation. European Conference on Computer
Vision, 2012.

[21] Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu,

L. Cao, and T. Huang. Large-scale image classiﬁcation:
fast feature extraction and SVM training. Computer
Vision and Pattern Recognition, 2011.

(a) Take a picture.

(b) Returned suggested classes.

(c) View images to compare.

(d) Select a class.

(e) Suggest a label, if incorrect.

(f) User’s collection of ﬂowers.

(g) Information about the ﬂower.

(h) Information about where it grows.

(i) Images that belong to this class.

Figure 7: A walk-through of the app functionalities.

