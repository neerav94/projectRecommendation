Predicting Seizure Onset with Intracranial Electroencephalogram

(EEG) Data - Project Report

Alex Greaves, Arushi Raghuvanshi, Kai-Yuan Neo

December 2014

1 Abstract

Epileptic patients have little to no warning about an
oncoming seizure, and would beneﬁt from knowing
if they are about to have one because it allows them
more time to ﬁnd a safe place. The primary challenge
in seizure prediction is diﬀerentiating between the
preictal (pre-seizure) and interictal (baseline) states.
In this paper, we developed a method to address the
following goal:

Can we use EEG signals to accurately

classify the preictal state in human and dog
patients with naturally occurring epilepsy?

Using over 3,000 training examples, we investigated
multiple feature extraction and classiﬁcation algo-
rithms including discrete wavelet transform (DWT),
short-time Fourier transform (STFT), principal com-
ponent analysis (PCA), k-Nearest Neighbors, logis-
tic regression, and support vector machines. Using
STFT, PCA, and logistic regression we developed a
feature extraction and classiﬁcation algorithm to clas-
sify EEG signals as preictal or interictal with an Area
Under Curve (AUC) score of .75.

2 Introduction

2.1 Motivation

Over 65 million people worldwide currently live with
epilepsy and 1 in 26 Americans will develop epilepsy
in their lifetime. Patients with epilepsy are suscep-
tible to spontaneous seizures, which are particularly
dangerous if they occur in a potentially hazardous
environment, such as behind the wheel of a car. Cur-
rently, the majority of epilepsy research funding goes
towards anticonvulsant medications which are largely
ineﬀective for about 40% of patients, leaving them
just as susceptible to spontaneous seizures [1]. With
recent developments in the wearable space, there is
an increase in the usefulness of wearable devices that

can take EEG readings [2]. For the ﬁrst time, it is
possible to track a patient’s brain activity on a daily
basis. If we can develop an algorithm that uses these
EEG signals to predict when a seizure is going to oc-
cur, patients can be warned ahead of time that they
are about to have a seizure, allowing them to lead a
safer life.

2.2 Past Work

Until the last decade, seizure prediction was only pos-
sible by visual analysis of signals by certiﬁed neurol-
ogists. Since the advent of machine learning tech-
niques and modern computing, seizure prevention re-
searchers have focused eﬀorts on deciphering EEG
messages as a means of predicting seizures.

There were some early optimistic papers on seizure
prediction using EEG data. However, these results
were not tested in a rigorous, statistical fashion, and
didn’t perform better than random guessing when re-
produced in the wild [4]. Following this 2007 review,
there has been some progress in research in the space,
however no algorithms have come close to the neces-
sary accuracy needed for practical use [5]. In a re-
sponse to the need to diﬀerentiate between preictal
and interictal states, the American Epilepsy Society
posed this problem as a Kaggle competition challenge
[3].

One notable paper by Subasi et al. demonstrates
that DWT combined with PCA and SVMs are most
eﬀective at classifying their test data. Subasi’s exper-
iments have 2 shortcomings: they only train and test
on a small data set of 1600 samples, and their SVMs
only use 1 kernel function, the radial basis function.
The former is prone to over-ﬁtting, and the latter
means that there are potentially better-performing
kernel functions and parameters to tune this algo-
rithm [6].

Maiwald et al. test 3 non-linear prediction meth-
ods: eﬀective correlation dimension, dynamical simi-
larity index, and accumulated energy. These 3 predic-
tion methods analyze distinct features of EEG data,

1

Predicting Seizure Onset with Intracranial Electroencephalogram

(EEG) Data - Project Report

Alex Greaves, Arushi Raghuvanshi, Kai-Yuan Neo

December 2014

1 Abstract

Epileptic patients have little to no warning about an
oncoming seizure, and would beneﬁt from knowing
if they are about to have one because it allows them
more time to ﬁnd a safe place. The primary challenge
in seizure prediction is diﬀerentiating between the
preictal (pre-seizure) and interictal (baseline) states.
In this paper, we developed a method to address the
following goal:

Can we use EEG signals to accurately

classify the preictal state in human and dog
patients with naturally occurring epilepsy?

Using over 3,000 training examples, we investigated
multiple feature extraction and classiﬁcation algo-
rithms including discrete wavelet transform (DWT),
short-time Fourier transform (STFT), principal com-
ponent analysis (PCA), k-Nearest Neighbors, logis-
tic regression, and support vector machines. Using
STFT, PCA, and logistic regression we developed a
feature extraction and classiﬁcation algorithm to clas-
sify EEG signals as preictal or interictal with an Area
Under Curve (AUC) score of .75.

2 Introduction

2.1 Motivation

Over 65 million people worldwide currently live with
epilepsy and 1 in 26 Americans will develop epilepsy
in their lifetime. Patients with epilepsy are suscep-
tible to spontaneous seizures, which are particularly
dangerous if they occur in a potentially hazardous
environment, such as behind the wheel of a car. Cur-
rently, the majority of epilepsy research funding goes
towards anticonvulsant medications which are largely
ineﬀective for about 40% of patients, leaving them
just as susceptible to spontaneous seizures [1]. With
recent developments in the wearable space, there is
an increase in the usefulness of wearable devices that

can take EEG readings [2]. For the ﬁrst time, it is
possible to track a patient’s brain activity on a daily
basis. If we can develop an algorithm that uses these
EEG signals to predict when a seizure is going to oc-
cur, patients can be warned ahead of time that they
are about to have a seizure, allowing them to lead a
safer life.

2.2 Past Work

Until the last decade, seizure prediction was only pos-
sible by visual analysis of signals by certiﬁed neurol-
ogists. Since the advent of machine learning tech-
niques and modern computing, seizure prevention re-
searchers have focused eﬀorts on deciphering EEG
messages as a means of predicting seizures.

There were some early optimistic papers on seizure
prediction using EEG data. However, these results
were not tested in a rigorous, statistical fashion, and
didn’t perform better than random guessing when re-
produced in the wild [4]. Following this 2007 review,
there has been some progress in research in the space,
however no algorithms have come close to the neces-
sary accuracy needed for practical use [5]. In a re-
sponse to the need to diﬀerentiate between preictal
and interictal states, the American Epilepsy Society
posed this problem as a Kaggle competition challenge
[3].

One notable paper by Subasi et al. demonstrates
that DWT combined with PCA and SVMs are most
eﬀective at classifying their test data. Subasi’s exper-
iments have 2 shortcomings: they only train and test
on a small data set of 1600 samples, and their SVMs
only use 1 kernel function, the radial basis function.
The former is prone to over-ﬁtting, and the latter
means that there are potentially better-performing
kernel functions and parameters to tune this algo-
rithm [6].

Maiwald et al. test 3 non-linear prediction meth-
ods: eﬀective correlation dimension, dynamical simi-
larity index, and accumulated energy. These 3 predic-
tion methods analyze distinct features of EEG data,

1

but do not aggregate over multiple features like a ma-
chine learning approach would. Maiwald et al. prove
that the 3 proposed methods perform better than
random or periodic methods of classiﬁcation [7].

2.3 Our Work

With the current state of the ﬁeld as a baseline, we
will try diﬀerent feature extraction and classiﬁcation
algorithms with the goal of developing a classiﬁcation
model with low variance and bias. Given roughly 100
gigabytes of data provided through the Kaggle com-
petition, we seek to develop a high performing algo-
rithm for classifying EEG signal segments as preictal
or interictal.

3 Dataset

The data represents 10-minute clips of preictal and
interictal training and test readings for a ﬁxed num-
ber of electrodes on 5 epileptic dogs and 2 epileptic
humans [3]. The following number of preictal, inter-
ictal, and test clips are provided for each:

Figure 1: Number of preictal and interictal training
and test clips

Each training example consists of a matrix, a vec-
tor, and some metadata. The matrix represents elec-
trodes by time, and has samples of electrode values
for the patient over 10 minutes. The rows contain the
values of the signal at all time intervals for a given
electrode. The columns contain electrode readings at
a particular time segment. The names of the EEG
electrodes are given in a vector. The time between
interval, and total number of samples are given as
scalars.

An example of the matrix for one training example

is provided below:

-36
-26
24
30
17
-20
30
-90
-60

-44
-12
24
6
24
-20
14
-76
-65

-53
-6
19
-28
23
2
-3
-39
-72

-69
-19
3
-33
27
5
-10
-11
-69

-77
-31
-14
-19
15
9
5
36
-67

-86
-46
-25
1
1
3
12
73
-73

-97
-49
-13
17
-8
-13
13
85
-79

-114
-50
-15
8
0
-9
4
85
-70

-118
-44
-18
11
-3
-10
2
85
-52

...
...
...
...
...
...
...
...
...

Note that for each patient, there is no guarantee that
the EEG sensors were placed in the exact same loca-
tion on the patient’s brains.

4 Methodology

4.1 Feature Extraction

The raw EEG data is over 100 gigabytes, so we use
common EEG feature extraction methods to extract
useful features. The raw amplitude data is not use-
ful for prediction using traditional machine learning
methods. The ﬁrst challenge in classifying EEG data
is to get it into a form in which we can apply tradi-
tional machine learning techniques. In the past this
has been done using one of either Discrete Wavelet
Transform (DWT) or Short-Time Fourier Transform
(STFT), because these methods preserve time infor-
mation while extracting frequency information and
other features from EEG signals. Principal Compo-
nent Analysis is also commonly used for dimension-
ality reduction. [6]

4.1.1 Discrete Wavelet Transform

DWT is implemented using what are called quadra-
ture mirror ﬁlters (QMFs), which separate high and
low frequency components of the signal. There
are many variations of these QMFs, but a common
choice, one that we used, is form 4 Daubechies ﬁl-
ters (known as d4). When we pass the original sig-
nal through these ﬁlters we get two output signals
called the approximation coeﬃcients (low-frequency)
and detail coeﬃcients (high-frequency). We can fur-
ther decompose the approximation coeﬃcients into
their own approximation and detail coeﬃcients, the
output of which is now 1
4 the length of the original
signal. We can repeatedly take the low-frequency ﬁl-
ter output of this process and further decompose it
to an arbitrary degree. A typical choice is 5 levels of
decomposition and this is what we have implemented.
Therefore, DWT separates the signal into high and
low frequency bands by repeatedly performing sepa-
ration to arbitrary degree in a branching structure
(Figure 2), allowing adjustment for more course or
granular readings [8].
In Figure 2, h(n) represents

2

Predicting Seizure Onset with Intracranial Electroencephalogram

(EEG) Data - Project Report

Alex Greaves, Arushi Raghuvanshi, Kai-Yuan Neo

December 2014

1 Abstract

Epileptic patients have little to no warning about an
oncoming seizure, and would beneﬁt from knowing
if they are about to have one because it allows them
more time to ﬁnd a safe place. The primary challenge
in seizure prediction is diﬀerentiating between the
preictal (pre-seizure) and interictal (baseline) states.
In this paper, we developed a method to address the
following goal:

Can we use EEG signals to accurately

classify the preictal state in human and dog
patients with naturally occurring epilepsy?

Using over 3,000 training examples, we investigated
multiple feature extraction and classiﬁcation algo-
rithms including discrete wavelet transform (DWT),
short-time Fourier transform (STFT), principal com-
ponent analysis (PCA), k-Nearest Neighbors, logis-
tic regression, and support vector machines. Using
STFT, PCA, and logistic regression we developed a
feature extraction and classiﬁcation algorithm to clas-
sify EEG signals as preictal or interictal with an Area
Under Curve (AUC) score of .75.

2 Introduction

2.1 Motivation

Over 65 million people worldwide currently live with
epilepsy and 1 in 26 Americans will develop epilepsy
in their lifetime. Patients with epilepsy are suscep-
tible to spontaneous seizures, which are particularly
dangerous if they occur in a potentially hazardous
environment, such as behind the wheel of a car. Cur-
rently, the majority of epilepsy research funding goes
towards anticonvulsant medications which are largely
ineﬀective for about 40% of patients, leaving them
just as susceptible to spontaneous seizures [1]. With
recent developments in the wearable space, there is
an increase in the usefulness of wearable devices that

can take EEG readings [2]. For the ﬁrst time, it is
possible to track a patient’s brain activity on a daily
basis. If we can develop an algorithm that uses these
EEG signals to predict when a seizure is going to oc-
cur, patients can be warned ahead of time that they
are about to have a seizure, allowing them to lead a
safer life.

2.2 Past Work

Until the last decade, seizure prediction was only pos-
sible by visual analysis of signals by certiﬁed neurol-
ogists. Since the advent of machine learning tech-
niques and modern computing, seizure prevention re-
searchers have focused eﬀorts on deciphering EEG
messages as a means of predicting seizures.

There were some early optimistic papers on seizure
prediction using EEG data. However, these results
were not tested in a rigorous, statistical fashion, and
didn’t perform better than random guessing when re-
produced in the wild [4]. Following this 2007 review,
there has been some progress in research in the space,
however no algorithms have come close to the neces-
sary accuracy needed for practical use [5]. In a re-
sponse to the need to diﬀerentiate between preictal
and interictal states, the American Epilepsy Society
posed this problem as a Kaggle competition challenge
[3].

One notable paper by Subasi et al. demonstrates
that DWT combined with PCA and SVMs are most
eﬀective at classifying their test data. Subasi’s exper-
iments have 2 shortcomings: they only train and test
on a small data set of 1600 samples, and their SVMs
only use 1 kernel function, the radial basis function.
The former is prone to over-ﬁtting, and the latter
means that there are potentially better-performing
kernel functions and parameters to tune this algo-
rithm [6].

Maiwald et al. test 3 non-linear prediction meth-
ods: eﬀective correlation dimension, dynamical simi-
larity index, and accumulated energy. These 3 predic-
tion methods analyze distinct features of EEG data,

1

but do not aggregate over multiple features like a ma-
chine learning approach would. Maiwald et al. prove
that the 3 proposed methods perform better than
random or periodic methods of classiﬁcation [7].

2.3 Our Work

With the current state of the ﬁeld as a baseline, we
will try diﬀerent feature extraction and classiﬁcation
algorithms with the goal of developing a classiﬁcation
model with low variance and bias. Given roughly 100
gigabytes of data provided through the Kaggle com-
petition, we seek to develop a high performing algo-
rithm for classifying EEG signal segments as preictal
or interictal.

3 Dataset

The data represents 10-minute clips of preictal and
interictal training and test readings for a ﬁxed num-
ber of electrodes on 5 epileptic dogs and 2 epileptic
humans [3]. The following number of preictal, inter-
ictal, and test clips are provided for each:

Figure 1: Number of preictal and interictal training
and test clips

Each training example consists of a matrix, a vec-
tor, and some metadata. The matrix represents elec-
trodes by time, and has samples of electrode values
for the patient over 10 minutes. The rows contain the
values of the signal at all time intervals for a given
electrode. The columns contain electrode readings at
a particular time segment. The names of the EEG
electrodes are given in a vector. The time between
interval, and total number of samples are given as
scalars.

An example of the matrix for one training example

is provided below:

-36
-26
24
30
17
-20
30
-90
-60

-44
-12
24
6
24
-20
14
-76
-65

-53
-6
19
-28
23
2
-3
-39
-72

-69
-19
3
-33
27
5
-10
-11
-69

-77
-31
-14
-19
15
9
5
36
-67

-86
-46
-25
1
1
3
12
73
-73

-97
-49
-13
17
-8
-13
13
85
-79

-114
-50
-15
8
0
-9
4
85
-70

-118
-44
-18
11
-3
-10
2
85
-52

...
...
...
...
...
...
...
...
...

Note that for each patient, there is no guarantee that
the EEG sensors were placed in the exact same loca-
tion on the patient’s brains.

4 Methodology

4.1 Feature Extraction

The raw EEG data is over 100 gigabytes, so we use
common EEG feature extraction methods to extract
useful features. The raw amplitude data is not use-
ful for prediction using traditional machine learning
methods. The ﬁrst challenge in classifying EEG data
is to get it into a form in which we can apply tradi-
tional machine learning techniques. In the past this
has been done using one of either Discrete Wavelet
Transform (DWT) or Short-Time Fourier Transform
(STFT), because these methods preserve time infor-
mation while extracting frequency information and
other features from EEG signals. Principal Compo-
nent Analysis is also commonly used for dimension-
ality reduction. [6]

4.1.1 Discrete Wavelet Transform

DWT is implemented using what are called quadra-
ture mirror ﬁlters (QMFs), which separate high and
low frequency components of the signal. There
are many variations of these QMFs, but a common
choice, one that we used, is form 4 Daubechies ﬁl-
ters (known as d4). When we pass the original sig-
nal through these ﬁlters we get two output signals
called the approximation coeﬃcients (low-frequency)
and detail coeﬃcients (high-frequency). We can fur-
ther decompose the approximation coeﬃcients into
their own approximation and detail coeﬃcients, the
output of which is now 1
4 the length of the original
signal. We can repeatedly take the low-frequency ﬁl-
ter output of this process and further decompose it
to an arbitrary degree. A typical choice is 5 levels of
decomposition and this is what we have implemented.
Therefore, DWT separates the signal into high and
low frequency bands by repeatedly performing sepa-
ration to arbitrary degree in a branching structure
(Figure 2), allowing adjustment for more course or
granular readings [8].
In Figure 2, h(n) represents

2

the high pass ﬁlter and g(n) represents the lowpass
ﬁlter.

Figure 2: Branching structure of DWT

From the DWT coeﬃcients at each level, we took
as features the mean magnitude, mean power (magni-
tude squared), and standard deviation. In addition,
we used as features the ratios of the mean magnitudes
of adjacent levels.

4.1.2 Short Time Fourier Transform

STFT is used for the determination of sinusoidal fre-
quency and phase content of local sections of signal.
This feature extraction method was most eﬀective on
the Kaggle data. We used the Discrete Time - STFT
as given in Figure 3.

Figure 3: Equation for computing STFT

Where x[k] denotes a signal and w[k] denotes an
L-point window function. The STFT can be deﬁned
as the Fourier transform of the product x[k]w[k − m]
[9].

In order to extract features from the EEG data, we
performed STFT twice on each 10 minute segment
(the second time with a time oﬀset) to obtain fre-
quency spectra for each 30-second segment and then
for each 60-second segment. From this, we extracted
features by computing mean power of the magnitude
at each frequency within 1Hz bands up to 120Hz.

4.1.3 Principal Component Analysis

PCA is used to reduce the dimensionality of features
by converting a set of possibly correlated variables
into a set of linearly uncorrelated variables.
It is
an unsupervised projection that can project a high
dimensional feature space onto a low dimensional

3

hyperplane. The ﬁrst principal component has the
largest possible variance, and subsequent components
have the next highest variances with the constraint of
being orthogonal to the preceding components. We
can truncate a list of features by only taking the ﬁrst
n principal components. These components have the
largest variances, and likely have larger eﬀects on the
classiﬁcation.

PCA improves eﬃciency of EEG classiﬁcation be-
cause certain channels are placed on areas of the head
that will be more relevant to seizure detection than
others. PCA can reduce these noisy dimensions.

However, PCA is an unsupervised projection algo-
rithm, so it can optimize out relevant information if
the features are not pre-processed in a way that ﬁts
the model. When run on our DWT features, PCA
resulted in poor performance.

Even for our STFT features dimensionality reduc-
tion didn’t improve our performance with this data
set, but the normalization and variance maximization
of PCA led to better results. We ﬁnd the principal
components by maximizing the equation given in Fig-
ure 4 [10].

Figure 4: Closed form equation for ﬁnding the prin-
cipal component

4.2 Classiﬁcation

We used three standard classiﬁcation algorithms: k-
Nearest Neighbors, logistic regression, and support
vector machines. We tuned the parameters for these
algorithms by balancing overﬁtting and underﬁtting
and maximizing the Area Under Curve (AUC) score.

4.2.1 k-Nearest Neighbor

As a baseline, we used k-NN to classify nodes based
on their k closest neighbors in the feature space. To
calculate the distance, we used an L2 norm, and we
looked at k = 22 nearest neighbors, weighting pre-
ictal segments by a factor of 8
3 . k and the preictal
segment weight are hyperparameters tuned to yield
the best result. Out of the classiﬁcation algorithms
we used, this performed the worst, most likely due to
the radically fewer number of preictal training exam-
ples than interictal examples. However, it provided a
starting point for classiﬁcation that brought us above
the random guessing threshold.

Predicting Seizure Onset with Intracranial Electroencephalogram

(EEG) Data - Project Report

Alex Greaves, Arushi Raghuvanshi, Kai-Yuan Neo

December 2014

1 Abstract

Epileptic patients have little to no warning about an
oncoming seizure, and would beneﬁt from knowing
if they are about to have one because it allows them
more time to ﬁnd a safe place. The primary challenge
in seizure prediction is diﬀerentiating between the
preictal (pre-seizure) and interictal (baseline) states.
In this paper, we developed a method to address the
following goal:

Can we use EEG signals to accurately

classify the preictal state in human and dog
patients with naturally occurring epilepsy?

Using over 3,000 training examples, we investigated
multiple feature extraction and classiﬁcation algo-
rithms including discrete wavelet transform (DWT),
short-time Fourier transform (STFT), principal com-
ponent analysis (PCA), k-Nearest Neighbors, logis-
tic regression, and support vector machines. Using
STFT, PCA, and logistic regression we developed a
feature extraction and classiﬁcation algorithm to clas-
sify EEG signals as preictal or interictal with an Area
Under Curve (AUC) score of .75.

2 Introduction

2.1 Motivation

Over 65 million people worldwide currently live with
epilepsy and 1 in 26 Americans will develop epilepsy
in their lifetime. Patients with epilepsy are suscep-
tible to spontaneous seizures, which are particularly
dangerous if they occur in a potentially hazardous
environment, such as behind the wheel of a car. Cur-
rently, the majority of epilepsy research funding goes
towards anticonvulsant medications which are largely
ineﬀective for about 40% of patients, leaving them
just as susceptible to spontaneous seizures [1]. With
recent developments in the wearable space, there is
an increase in the usefulness of wearable devices that

can take EEG readings [2]. For the ﬁrst time, it is
possible to track a patient’s brain activity on a daily
basis. If we can develop an algorithm that uses these
EEG signals to predict when a seizure is going to oc-
cur, patients can be warned ahead of time that they
are about to have a seizure, allowing them to lead a
safer life.

2.2 Past Work

Until the last decade, seizure prediction was only pos-
sible by visual analysis of signals by certiﬁed neurol-
ogists. Since the advent of machine learning tech-
niques and modern computing, seizure prevention re-
searchers have focused eﬀorts on deciphering EEG
messages as a means of predicting seizures.

There were some early optimistic papers on seizure
prediction using EEG data. However, these results
were not tested in a rigorous, statistical fashion, and
didn’t perform better than random guessing when re-
produced in the wild [4]. Following this 2007 review,
there has been some progress in research in the space,
however no algorithms have come close to the neces-
sary accuracy needed for practical use [5]. In a re-
sponse to the need to diﬀerentiate between preictal
and interictal states, the American Epilepsy Society
posed this problem as a Kaggle competition challenge
[3].

One notable paper by Subasi et al. demonstrates
that DWT combined with PCA and SVMs are most
eﬀective at classifying their test data. Subasi’s exper-
iments have 2 shortcomings: they only train and test
on a small data set of 1600 samples, and their SVMs
only use 1 kernel function, the radial basis function.
The former is prone to over-ﬁtting, and the latter
means that there are potentially better-performing
kernel functions and parameters to tune this algo-
rithm [6].

Maiwald et al. test 3 non-linear prediction meth-
ods: eﬀective correlation dimension, dynamical simi-
larity index, and accumulated energy. These 3 predic-
tion methods analyze distinct features of EEG data,

1

but do not aggregate over multiple features like a ma-
chine learning approach would. Maiwald et al. prove
that the 3 proposed methods perform better than
random or periodic methods of classiﬁcation [7].

2.3 Our Work

With the current state of the ﬁeld as a baseline, we
will try diﬀerent feature extraction and classiﬁcation
algorithms with the goal of developing a classiﬁcation
model with low variance and bias. Given roughly 100
gigabytes of data provided through the Kaggle com-
petition, we seek to develop a high performing algo-
rithm for classifying EEG signal segments as preictal
or interictal.

3 Dataset

The data represents 10-minute clips of preictal and
interictal training and test readings for a ﬁxed num-
ber of electrodes on 5 epileptic dogs and 2 epileptic
humans [3]. The following number of preictal, inter-
ictal, and test clips are provided for each:

Figure 1: Number of preictal and interictal training
and test clips

Each training example consists of a matrix, a vec-
tor, and some metadata. The matrix represents elec-
trodes by time, and has samples of electrode values
for the patient over 10 minutes. The rows contain the
values of the signal at all time intervals for a given
electrode. The columns contain electrode readings at
a particular time segment. The names of the EEG
electrodes are given in a vector. The time between
interval, and total number of samples are given as
scalars.

An example of the matrix for one training example

is provided below:

-36
-26
24
30
17
-20
30
-90
-60

-44
-12
24
6
24
-20
14
-76
-65

-53
-6
19
-28
23
2
-3
-39
-72

-69
-19
3
-33
27
5
-10
-11
-69

-77
-31
-14
-19
15
9
5
36
-67

-86
-46
-25
1
1
3
12
73
-73

-97
-49
-13
17
-8
-13
13
85
-79

-114
-50
-15
8
0
-9
4
85
-70

-118
-44
-18
11
-3
-10
2
85
-52

...
...
...
...
...
...
...
...
...

Note that for each patient, there is no guarantee that
the EEG sensors were placed in the exact same loca-
tion on the patient’s brains.

4 Methodology

4.1 Feature Extraction

The raw EEG data is over 100 gigabytes, so we use
common EEG feature extraction methods to extract
useful features. The raw amplitude data is not use-
ful for prediction using traditional machine learning
methods. The ﬁrst challenge in classifying EEG data
is to get it into a form in which we can apply tradi-
tional machine learning techniques. In the past this
has been done using one of either Discrete Wavelet
Transform (DWT) or Short-Time Fourier Transform
(STFT), because these methods preserve time infor-
mation while extracting frequency information and
other features from EEG signals. Principal Compo-
nent Analysis is also commonly used for dimension-
ality reduction. [6]

4.1.1 Discrete Wavelet Transform

DWT is implemented using what are called quadra-
ture mirror ﬁlters (QMFs), which separate high and
low frequency components of the signal. There
are many variations of these QMFs, but a common
choice, one that we used, is form 4 Daubechies ﬁl-
ters (known as d4). When we pass the original sig-
nal through these ﬁlters we get two output signals
called the approximation coeﬃcients (low-frequency)
and detail coeﬃcients (high-frequency). We can fur-
ther decompose the approximation coeﬃcients into
their own approximation and detail coeﬃcients, the
output of which is now 1
4 the length of the original
signal. We can repeatedly take the low-frequency ﬁl-
ter output of this process and further decompose it
to an arbitrary degree. A typical choice is 5 levels of
decomposition and this is what we have implemented.
Therefore, DWT separates the signal into high and
low frequency bands by repeatedly performing sepa-
ration to arbitrary degree in a branching structure
(Figure 2), allowing adjustment for more course or
granular readings [8].
In Figure 2, h(n) represents

2

the high pass ﬁlter and g(n) represents the lowpass
ﬁlter.

Figure 2: Branching structure of DWT

From the DWT coeﬃcients at each level, we took
as features the mean magnitude, mean power (magni-
tude squared), and standard deviation. In addition,
we used as features the ratios of the mean magnitudes
of adjacent levels.

4.1.2 Short Time Fourier Transform

STFT is used for the determination of sinusoidal fre-
quency and phase content of local sections of signal.
This feature extraction method was most eﬀective on
the Kaggle data. We used the Discrete Time - STFT
as given in Figure 3.

Figure 3: Equation for computing STFT

Where x[k] denotes a signal and w[k] denotes an
L-point window function. The STFT can be deﬁned
as the Fourier transform of the product x[k]w[k − m]
[9].

In order to extract features from the EEG data, we
performed STFT twice on each 10 minute segment
(the second time with a time oﬀset) to obtain fre-
quency spectra for each 30-second segment and then
for each 60-second segment. From this, we extracted
features by computing mean power of the magnitude
at each frequency within 1Hz bands up to 120Hz.

4.1.3 Principal Component Analysis

PCA is used to reduce the dimensionality of features
by converting a set of possibly correlated variables
into a set of linearly uncorrelated variables.
It is
an unsupervised projection that can project a high
dimensional feature space onto a low dimensional

3

hyperplane. The ﬁrst principal component has the
largest possible variance, and subsequent components
have the next highest variances with the constraint of
being orthogonal to the preceding components. We
can truncate a list of features by only taking the ﬁrst
n principal components. These components have the
largest variances, and likely have larger eﬀects on the
classiﬁcation.

PCA improves eﬃciency of EEG classiﬁcation be-
cause certain channels are placed on areas of the head
that will be more relevant to seizure detection than
others. PCA can reduce these noisy dimensions.

However, PCA is an unsupervised projection algo-
rithm, so it can optimize out relevant information if
the features are not pre-processed in a way that ﬁts
the model. When run on our DWT features, PCA
resulted in poor performance.

Even for our STFT features dimensionality reduc-
tion didn’t improve our performance with this data
set, but the normalization and variance maximization
of PCA led to better results. We ﬁnd the principal
components by maximizing the equation given in Fig-
ure 4 [10].

Figure 4: Closed form equation for ﬁnding the prin-
cipal component

4.2 Classiﬁcation

We used three standard classiﬁcation algorithms: k-
Nearest Neighbors, logistic regression, and support
vector machines. We tuned the parameters for these
algorithms by balancing overﬁtting and underﬁtting
and maximizing the Area Under Curve (AUC) score.

4.2.1 k-Nearest Neighbor

As a baseline, we used k-NN to classify nodes based
on their k closest neighbors in the feature space. To
calculate the distance, we used an L2 norm, and we
looked at k = 22 nearest neighbors, weighting pre-
ictal segments by a factor of 8
3 . k and the preictal
segment weight are hyperparameters tuned to yield
the best result. Out of the classiﬁcation algorithms
we used, this performed the worst, most likely due to
the radically fewer number of preictal training exam-
ples than interictal examples. However, it provided a
starting point for classiﬁcation that brought us above
the random guessing threshold.

4.2.2 Logistic Regression

5 Results

Logistic Regression (LR) has been successfully used
for EEG classiﬁcation in literature.
It is a ﬂexible
model that makes few assumptions on the prior struc-
ture of the data. In our implementation, LR is the
best-performing model.

LR calculates probability of positive result based
on features input into the logistic function. Our hy-
pothesis function is described as follows [10]:

Correctness scores are generated by calculating the
area under the receiving operator characteristic
(ROC) curve, which takes into account both accu-
racy and sensitivity.

Figure 7: Correctness scores of each feature extrac-
tion method with each classiﬁcation model

Figure 5: Our hypothesis and the underlying logistic
function

We then used stochastic gradient descent to op-
timize the parameters θ. We trained one model on
all 30-second samples and another on all 60-second
samples, assigning double the weight for preictal seg-
ments. Then, for each test segment, we summed
the output for all 30-second and 60-second samples.
Classiﬁcation was done by comparing this sum to a
threshold, which was another hyperparameter.

4.2.3 Support Vector Machine

Support Vector Machines (SVMs) are models which
assume a (mostly) linearly separable data set. This
set can consist of features mapped to a high di-
mensional space in which they are linearly separa-
ble. By creating a margin which maximizes the func-
tional and geometric margins between sets, SVMs
have received attention in biomedical applications.
We found that most kernel functions overﬁt our data
using SVM. The linear kernel performed the best and
provided an improvement over k-NN, but still overﬁt
the data to some extent.

Figure 6: SVM objective function to calculate opti-
mal margin with error penalty

4

Figure 8: Correctness scores of the best feature
extraction-classiﬁcation model pairs

Figure 9: Our performance on Kaggle

6 Discussion

Short-time Fourier transform (STFT) feature extrac-
tion, principal component analysis (PCA) dimension-
ality reduction and normalization, and logistic regres-
sion (LR) model prediction provided the most accu-
rate result.

STFT provided robust, domain-speciﬁc feature ex-
traction because they provided frequency and phase
information on every window of each 10-minute EEG
clip. Taking the average of frequencies from 30- and
60-second window sizes provided an even more ro-
bust classiﬁcation feature. The features were domain-
speciﬁc because STFT is a method speciﬁcally uti-
lized for time-series signal data.

Predicting Seizure Onset with Intracranial Electroencephalogram

(EEG) Data - Project Report

Alex Greaves, Arushi Raghuvanshi, Kai-Yuan Neo

December 2014

1 Abstract

Epileptic patients have little to no warning about an
oncoming seizure, and would beneﬁt from knowing
if they are about to have one because it allows them
more time to ﬁnd a safe place. The primary challenge
in seizure prediction is diﬀerentiating between the
preictal (pre-seizure) and interictal (baseline) states.
In this paper, we developed a method to address the
following goal:

Can we use EEG signals to accurately

classify the preictal state in human and dog
patients with naturally occurring epilepsy?

Using over 3,000 training examples, we investigated
multiple feature extraction and classiﬁcation algo-
rithms including discrete wavelet transform (DWT),
short-time Fourier transform (STFT), principal com-
ponent analysis (PCA), k-Nearest Neighbors, logis-
tic regression, and support vector machines. Using
STFT, PCA, and logistic regression we developed a
feature extraction and classiﬁcation algorithm to clas-
sify EEG signals as preictal or interictal with an Area
Under Curve (AUC) score of .75.

2 Introduction

2.1 Motivation

Over 65 million people worldwide currently live with
epilepsy and 1 in 26 Americans will develop epilepsy
in their lifetime. Patients with epilepsy are suscep-
tible to spontaneous seizures, which are particularly
dangerous if they occur in a potentially hazardous
environment, such as behind the wheel of a car. Cur-
rently, the majority of epilepsy research funding goes
towards anticonvulsant medications which are largely
ineﬀective for about 40% of patients, leaving them
just as susceptible to spontaneous seizures [1]. With
recent developments in the wearable space, there is
an increase in the usefulness of wearable devices that

can take EEG readings [2]. For the ﬁrst time, it is
possible to track a patient’s brain activity on a daily
basis. If we can develop an algorithm that uses these
EEG signals to predict when a seizure is going to oc-
cur, patients can be warned ahead of time that they
are about to have a seizure, allowing them to lead a
safer life.

2.2 Past Work

Until the last decade, seizure prediction was only pos-
sible by visual analysis of signals by certiﬁed neurol-
ogists. Since the advent of machine learning tech-
niques and modern computing, seizure prevention re-
searchers have focused eﬀorts on deciphering EEG
messages as a means of predicting seizures.

There were some early optimistic papers on seizure
prediction using EEG data. However, these results
were not tested in a rigorous, statistical fashion, and
didn’t perform better than random guessing when re-
produced in the wild [4]. Following this 2007 review,
there has been some progress in research in the space,
however no algorithms have come close to the neces-
sary accuracy needed for practical use [5]. In a re-
sponse to the need to diﬀerentiate between preictal
and interictal states, the American Epilepsy Society
posed this problem as a Kaggle competition challenge
[3].

One notable paper by Subasi et al. demonstrates
that DWT combined with PCA and SVMs are most
eﬀective at classifying their test data. Subasi’s exper-
iments have 2 shortcomings: they only train and test
on a small data set of 1600 samples, and their SVMs
only use 1 kernel function, the radial basis function.
The former is prone to over-ﬁtting, and the latter
means that there are potentially better-performing
kernel functions and parameters to tune this algo-
rithm [6].

Maiwald et al. test 3 non-linear prediction meth-
ods: eﬀective correlation dimension, dynamical simi-
larity index, and accumulated energy. These 3 predic-
tion methods analyze distinct features of EEG data,

1

but do not aggregate over multiple features like a ma-
chine learning approach would. Maiwald et al. prove
that the 3 proposed methods perform better than
random or periodic methods of classiﬁcation [7].

2.3 Our Work

With the current state of the ﬁeld as a baseline, we
will try diﬀerent feature extraction and classiﬁcation
algorithms with the goal of developing a classiﬁcation
model with low variance and bias. Given roughly 100
gigabytes of data provided through the Kaggle com-
petition, we seek to develop a high performing algo-
rithm for classifying EEG signal segments as preictal
or interictal.

3 Dataset

The data represents 10-minute clips of preictal and
interictal training and test readings for a ﬁxed num-
ber of electrodes on 5 epileptic dogs and 2 epileptic
humans [3]. The following number of preictal, inter-
ictal, and test clips are provided for each:

Figure 1: Number of preictal and interictal training
and test clips

Each training example consists of a matrix, a vec-
tor, and some metadata. The matrix represents elec-
trodes by time, and has samples of electrode values
for the patient over 10 minutes. The rows contain the
values of the signal at all time intervals for a given
electrode. The columns contain electrode readings at
a particular time segment. The names of the EEG
electrodes are given in a vector. The time between
interval, and total number of samples are given as
scalars.

An example of the matrix for one training example

is provided below:

-36
-26
24
30
17
-20
30
-90
-60

-44
-12
24
6
24
-20
14
-76
-65

-53
-6
19
-28
23
2
-3
-39
-72

-69
-19
3
-33
27
5
-10
-11
-69

-77
-31
-14
-19
15
9
5
36
-67

-86
-46
-25
1
1
3
12
73
-73

-97
-49
-13
17
-8
-13
13
85
-79

-114
-50
-15
8
0
-9
4
85
-70

-118
-44
-18
11
-3
-10
2
85
-52

...
...
...
...
...
...
...
...
...

Note that for each patient, there is no guarantee that
the EEG sensors were placed in the exact same loca-
tion on the patient’s brains.

4 Methodology

4.1 Feature Extraction

The raw EEG data is over 100 gigabytes, so we use
common EEG feature extraction methods to extract
useful features. The raw amplitude data is not use-
ful for prediction using traditional machine learning
methods. The ﬁrst challenge in classifying EEG data
is to get it into a form in which we can apply tradi-
tional machine learning techniques. In the past this
has been done using one of either Discrete Wavelet
Transform (DWT) or Short-Time Fourier Transform
(STFT), because these methods preserve time infor-
mation while extracting frequency information and
other features from EEG signals. Principal Compo-
nent Analysis is also commonly used for dimension-
ality reduction. [6]

4.1.1 Discrete Wavelet Transform

DWT is implemented using what are called quadra-
ture mirror ﬁlters (QMFs), which separate high and
low frequency components of the signal. There
are many variations of these QMFs, but a common
choice, one that we used, is form 4 Daubechies ﬁl-
ters (known as d4). When we pass the original sig-
nal through these ﬁlters we get two output signals
called the approximation coeﬃcients (low-frequency)
and detail coeﬃcients (high-frequency). We can fur-
ther decompose the approximation coeﬃcients into
their own approximation and detail coeﬃcients, the
output of which is now 1
4 the length of the original
signal. We can repeatedly take the low-frequency ﬁl-
ter output of this process and further decompose it
to an arbitrary degree. A typical choice is 5 levels of
decomposition and this is what we have implemented.
Therefore, DWT separates the signal into high and
low frequency bands by repeatedly performing sepa-
ration to arbitrary degree in a branching structure
(Figure 2), allowing adjustment for more course or
granular readings [8].
In Figure 2, h(n) represents

2

the high pass ﬁlter and g(n) represents the lowpass
ﬁlter.

Figure 2: Branching structure of DWT

From the DWT coeﬃcients at each level, we took
as features the mean magnitude, mean power (magni-
tude squared), and standard deviation. In addition,
we used as features the ratios of the mean magnitudes
of adjacent levels.

4.1.2 Short Time Fourier Transform

STFT is used for the determination of sinusoidal fre-
quency and phase content of local sections of signal.
This feature extraction method was most eﬀective on
the Kaggle data. We used the Discrete Time - STFT
as given in Figure 3.

Figure 3: Equation for computing STFT

Where x[k] denotes a signal and w[k] denotes an
L-point window function. The STFT can be deﬁned
as the Fourier transform of the product x[k]w[k − m]
[9].

In order to extract features from the EEG data, we
performed STFT twice on each 10 minute segment
(the second time with a time oﬀset) to obtain fre-
quency spectra for each 30-second segment and then
for each 60-second segment. From this, we extracted
features by computing mean power of the magnitude
at each frequency within 1Hz bands up to 120Hz.

4.1.3 Principal Component Analysis

PCA is used to reduce the dimensionality of features
by converting a set of possibly correlated variables
into a set of linearly uncorrelated variables.
It is
an unsupervised projection that can project a high
dimensional feature space onto a low dimensional

3

hyperplane. The ﬁrst principal component has the
largest possible variance, and subsequent components
have the next highest variances with the constraint of
being orthogonal to the preceding components. We
can truncate a list of features by only taking the ﬁrst
n principal components. These components have the
largest variances, and likely have larger eﬀects on the
classiﬁcation.

PCA improves eﬃciency of EEG classiﬁcation be-
cause certain channels are placed on areas of the head
that will be more relevant to seizure detection than
others. PCA can reduce these noisy dimensions.

However, PCA is an unsupervised projection algo-
rithm, so it can optimize out relevant information if
the features are not pre-processed in a way that ﬁts
the model. When run on our DWT features, PCA
resulted in poor performance.

Even for our STFT features dimensionality reduc-
tion didn’t improve our performance with this data
set, but the normalization and variance maximization
of PCA led to better results. We ﬁnd the principal
components by maximizing the equation given in Fig-
ure 4 [10].

Figure 4: Closed form equation for ﬁnding the prin-
cipal component

4.2 Classiﬁcation

We used three standard classiﬁcation algorithms: k-
Nearest Neighbors, logistic regression, and support
vector machines. We tuned the parameters for these
algorithms by balancing overﬁtting and underﬁtting
and maximizing the Area Under Curve (AUC) score.

4.2.1 k-Nearest Neighbor

As a baseline, we used k-NN to classify nodes based
on their k closest neighbors in the feature space. To
calculate the distance, we used an L2 norm, and we
looked at k = 22 nearest neighbors, weighting pre-
ictal segments by a factor of 8
3 . k and the preictal
segment weight are hyperparameters tuned to yield
the best result. Out of the classiﬁcation algorithms
we used, this performed the worst, most likely due to
the radically fewer number of preictal training exam-
ples than interictal examples. However, it provided a
starting point for classiﬁcation that brought us above
the random guessing threshold.

4.2.2 Logistic Regression

5 Results

Logistic Regression (LR) has been successfully used
for EEG classiﬁcation in literature.
It is a ﬂexible
model that makes few assumptions on the prior struc-
ture of the data. In our implementation, LR is the
best-performing model.

LR calculates probability of positive result based
on features input into the logistic function. Our hy-
pothesis function is described as follows [10]:

Correctness scores are generated by calculating the
area under the receiving operator characteristic
(ROC) curve, which takes into account both accu-
racy and sensitivity.

Figure 7: Correctness scores of each feature extrac-
tion method with each classiﬁcation model

Figure 5: Our hypothesis and the underlying logistic
function

We then used stochastic gradient descent to op-
timize the parameters θ. We trained one model on
all 30-second samples and another on all 60-second
samples, assigning double the weight for preictal seg-
ments. Then, for each test segment, we summed
the output for all 30-second and 60-second samples.
Classiﬁcation was done by comparing this sum to a
threshold, which was another hyperparameter.

4.2.3 Support Vector Machine

Support Vector Machines (SVMs) are models which
assume a (mostly) linearly separable data set. This
set can consist of features mapped to a high di-
mensional space in which they are linearly separa-
ble. By creating a margin which maximizes the func-
tional and geometric margins between sets, SVMs
have received attention in biomedical applications.
We found that most kernel functions overﬁt our data
using SVM. The linear kernel performed the best and
provided an improvement over k-NN, but still overﬁt
the data to some extent.

Figure 6: SVM objective function to calculate opti-
mal margin with error penalty

4

Figure 8: Correctness scores of the best feature
extraction-classiﬁcation model pairs

Figure 9: Our performance on Kaggle

6 Discussion

Short-time Fourier transform (STFT) feature extrac-
tion, principal component analysis (PCA) dimension-
ality reduction and normalization, and logistic regres-
sion (LR) model prediction provided the most accu-
rate result.

STFT provided robust, domain-speciﬁc feature ex-
traction because they provided frequency and phase
information on every window of each 10-minute EEG
clip. Taking the average of frequencies from 30- and
60-second window sizes provided an even more ro-
bust classiﬁcation feature. The features were domain-
speciﬁc because STFT is a method speciﬁcally uti-
lized for time-series signal data.

[2] Nine health wearables for your head (2013,
[Online], Avail-
http://mobihealthnews.com/24401/nine-

August 5), mobihealthnews
able:
health-wearables-for-your-head/

[3] American Epilepsy Society Seizure Prediction
Challenge (2014, August 25), Kaggle [Online],
Available:
http://www.kaggle.com/c/seizure-
prediction

[4] Mormann, Florian, Ralph G. Andrzejak, et al.,
”Seizure prediction: the long and winding road,”
in Brain, 2007, pp. 314-333.

[5] Binder, Devin K. and Sheryl R. Haut, ”Toward
new paradigms of seizure detection,” Epilepsy &
Behavior 26, 2013, pp. 247-252.

[6] Subasi, Abdulhamit and M. Ismail Gursoy, “EEG
signal classiﬁcation using PCA, ICA, LDA and
support vector machines,” in Expert Systems
with Applications, 2010, pp. 8659-8666.

[7] T. Maiwald, M. Winterhalder, R. Aschenbrenner-
Scheibe, H.U. Voss, A. Schulze-Bonhage, J. Tim-
mer, “Comparison of three nonlinear seizure pre-
diction methods by means of the seizure pre-
diction characteristic,” Physica D 194, 2004,
357–368.

[8] Paul, Manoranjan and Mohammed Zavid Parvez,
”EEG Signal Classiﬁcation using Frequency Band
Analysis towards Epileptic Seizure Prediction,” in
IEEE International conference on Computer and
Information Technology, Khulna, Bangladesh,
2013.

[9] Suleiman, Raouf Abdul-Bary and Toka Abdul-
Hameed Fatehi, ”Features Extraction Techniques
of EEG Signal for BCI Applications,” Faculty of
Computer and Information Engineering Depart-
ment College of Electronics Engineering, Univer-
sity of Mosul, Iraq, 2007.

[10] Ng, Andrew, Lecture Notes, 2014.

PCA combined with STFT provided better results
than STFT alone because of PCA’s normalization
step. Reducing dimension of the raw signals (per-
forming PCA without STFT) reduced classiﬁcation
accuracy to below the baseline because it removed
key latent elements of the data.

DWT, like PCA, removed key latent elements of
the EEG data and thus did not perform as well as
STFT.

We were surprised that LR performed better than
SVM, but concluded it was because logistic regres-
sion made the fewest prior assumptions on the data
set. In particular, logistic regression classiﬁes with a
probability based on a data point’s distance from the
regression line, whereas SVMs classify strictly and fail
regularly when there is too much noise in the train-
ing data. To optimize SVM performance and prevent
overﬁtting, we would like to tune our objective func-
tion to support more variance.

kNN was an eﬀective baseline algorithm, but did
not provide a nuanced enough deﬁnition of distance
between data points to accurately classify test data.
Our classiﬁcation results performed better than
about 95% of competitors on Kaggle and would have
been good enough to place us 27th out of 504 en-
trants.

7 Future Work

First, we would like to improve our SVM model to
perform better than logistic regression. The litera-
ture we have read shows that SVMs perform opti-
mally, and we believe the Kaggle data set should not
be an outlier.

In future work, we would like to attempt more com-
plex classiﬁcation algorithms such as neural networks
and random forest walks. We attempted to apply
neural networks for this project, however we experi-
enced severe over-ﬁtting and a classiﬁcation of non-
seizure for virtually all data points. We would like to
tune neural networks and run them on faster proces-
sors to evaluate their potential with this data set.

The random forest model is a recently discovered,
increasingly popular mode of classiﬁcation and we
would like to evaluate its performance in binary clas-
siﬁcation of EEG data.

References

[1] Epilepsy Facts (2014, December 9), Citizens
United for Research in Epilepsy [Online], Avail-
able: http://www.cureepilepsy.org.

5

