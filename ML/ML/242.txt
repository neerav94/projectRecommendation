Prediction of Yelp Ratings Based on Reviewer Comments

Segmented by Business Type

Kent Lee1∗

and James Ross2∗

1Stanford University, Department of Structural Biology, Biophysics Program

2Stanford University, Department of Computer Science

I. INTRODUCTION

Yelp[1] is a popular website that allows users to
submit numerical ratings (integers 1-5, inclusive, 5
being positive sentiment) and text reviews (reviewer’s
comments) to express their experiences interacting
with various businesses. This information helps others
determine which businesses best ﬁt their needs. The
prevalence and utility of rating and review services
provide the opportunity and motivation to study the
prediction of ratings based on reviews. While these
services are prevalent, many internet forums are ded-
icated to publishing user reviews of products such
as laptops[3] and vehicles[4] without quantitative
ranking. Often, forums hosting qualitative reviews
such as "laptop stopped working but helpdesk was
useless!" and "customer service was horrible" would
beneﬁt from an algorithm predicting rating from re-
views to summarize user sentiment quantitatively.
Furthermore, prediction algorithms allow rating and
review websites gather additional rating information
by predicting from external enthusiast forums where
reviews are more descriptive.

Our goal was to apply machine learning techniques
to Yelp’s published dataset to accurately predict rat-
ings from reviews. An input of a string of words
into our Multinomial Naive Bayes algorithm would
output a predicted rating between 1 and 5 describ-
ing the customer’s sentiment regarding a business.
While this has previously been attempted[5][6], most
approaches do not focus on training data segmen-
tation. We believed that segmenting training data
by business sector (e.g. restaurants) allows more
accurate predictions because feature sets become
less "diluted" or noisy and words gain meaningful
value when given a focused context. For example, an
auto mechanic business customer may be "extremely
satisﬁed with the new cold air intake system!" while
a restaurant customer may say that "the food took a
while and came out cold!" Taken together, if feature

∗These authors contributed equally to this work

size was ﬁxed, the word "system" would offer as
little predictive value in ranking restaurants as "food"
would for the auto business. Furthermore, "cold" is
sentimentally negative in the restaurant review, yet
positive in the auto business review. Training data
segmentation aims to reduce such uninformative oc-
currences.

II. RELATED WORK

Interesting approaches involve looking up the quan-
tiﬁed sentiment of each word from SentiWordNet and
summing the positive/negative/neutral in a document
and ﬁtting to regression rather than a completely
probabilistic approach[12][11]. While I think this is
the best method, the disadvantages are being lim-
ited to characterized words. Groups using non-word
features such as part-of-speech is an approach we
have not considered[10]. A useful troubleshooting
approach is displaying the highest impact features
in a prediction[9]. The potential to use latent factors
through matrix factorization seemed to have poten-
tial, but others who tried it received lower MSE than
us[13]

III. DATASET AND FEATURES

A. Dataset

Data was

downloaded

from Yelp’s Dataset
Challenge[2] in JSON format containing 1.6 million
reviews and 61 thousand businesses. The dataset
was trimmed by a custom C# script to yield a dataset
containing entries with only metrics of
interest:
review text on the business and reviewer rating of
the business. This formed the dataset allReviews. To
create a dataset containing only reviews from one
business segment, we identiﬁed the top represented
business segment in allReviews and created a dataset
containing only entries from this segment. The top
represented segment was Restaurant (21,892 entries)
so all reviews on restaurants made up segReviews.
Thus, allReviews and its subset segReviews are the
two datasets that were used. Each dataset contained

Prediction of Yelp Ratings Based on Reviewer Comments

Segmented by Business Type

Kent Lee1∗

and James Ross2∗

1Stanford University, Department of Structural Biology, Biophysics Program

2Stanford University, Department of Computer Science

I. INTRODUCTION

Yelp[1] is a popular website that allows users to
submit numerical ratings (integers 1-5, inclusive, 5
being positive sentiment) and text reviews (reviewer’s
comments) to express their experiences interacting
with various businesses. This information helps others
determine which businesses best ﬁt their needs. The
prevalence and utility of rating and review services
provide the opportunity and motivation to study the
prediction of ratings based on reviews. While these
services are prevalent, many internet forums are ded-
icated to publishing user reviews of products such
as laptops[3] and vehicles[4] without quantitative
ranking. Often, forums hosting qualitative reviews
such as "laptop stopped working but helpdesk was
useless!" and "customer service was horrible" would
beneﬁt from an algorithm predicting rating from re-
views to summarize user sentiment quantitatively.
Furthermore, prediction algorithms allow rating and
review websites gather additional rating information
by predicting from external enthusiast forums where
reviews are more descriptive.

Our goal was to apply machine learning techniques
to Yelp’s published dataset to accurately predict rat-
ings from reviews. An input of a string of words
into our Multinomial Naive Bayes algorithm would
output a predicted rating between 1 and 5 describ-
ing the customer’s sentiment regarding a business.
While this has previously been attempted[5][6], most
approaches do not focus on training data segmen-
tation. We believed that segmenting training data
by business sector (e.g. restaurants) allows more
accurate predictions because feature sets become
less "diluted" or noisy and words gain meaningful
value when given a focused context. For example, an
auto mechanic business customer may be "extremely
satisﬁed with the new cold air intake system!" while
a restaurant customer may say that "the food took a
while and came out cold!" Taken together, if feature

∗These authors contributed equally to this work

size was ﬁxed, the word "system" would offer as
little predictive value in ranking restaurants as "food"
would for the auto business. Furthermore, "cold" is
sentimentally negative in the restaurant review, yet
positive in the auto business review. Training data
segmentation aims to reduce such uninformative oc-
currences.

II. RELATED WORK

Interesting approaches involve looking up the quan-
tiﬁed sentiment of each word from SentiWordNet and
summing the positive/negative/neutral in a document
and ﬁtting to regression rather than a completely
probabilistic approach[12][11]. While I think this is
the best method, the disadvantages are being lim-
ited to characterized words. Groups using non-word
features such as part-of-speech is an approach we
have not considered[10]. A useful troubleshooting
approach is displaying the highest impact features
in a prediction[9]. The potential to use latent factors
through matrix factorization seemed to have poten-
tial, but others who tried it received lower MSE than
us[13]

III. DATASET AND FEATURES

A. Dataset

Data was

downloaded

from Yelp’s Dataset
Challenge[2] in JSON format containing 1.6 million
reviews and 61 thousand businesses. The dataset
was trimmed by a custom C# script to yield a dataset
containing entries with only metrics of
interest:
review text on the business and reviewer rating of
the business. This formed the dataset allReviews. To
create a dataset containing only reviews from one
business segment, we identiﬁed the top represented
business segment in allReviews and created a dataset
containing only entries from this segment. The top
represented segment was Restaurant (21,892 entries)
so all reviews on restaurants made up segReviews.
Thus, allReviews and its subset segReviews are the
two datasets that were used. Each dataset contained

8,000 entries as training set, 1,000 as validation set,
and 1,000 as test set. Set sizes were smaller than
available data due to hardware constraints during
optimization. Data was converted from the JSON
format to the form of
***Reviews_Ratings.txt:

that may collectively have sentimental value while
sentimentally-neutral individually. SelectKBest using
a Chi-squared distribution was used to select features
that were found to have a dependence on speciﬁc
labels. SelectKBest and Ngrams were implemented
through scikitlearn[8].

1 5
2 4
3 4
4 ...

***Reviews_Reviews.txt:

1 this vegas thai place can be tricky to find

it is off the strip hidden away in a
strip mall ...

2 yum yum thai iced coffee fried tofu starter

and ...

3 the commercial center mall is not the most
obvious choice for las vegas best thai
...

4 ...

B. Preliminary Dataset Processing

allReviews and segReviews were processed as fol-
lows. For each review entry, all characters were re-
moved except for whitespaces and English alphabet
characters. All characters were converted to low-
ercase. Each occurrence of a whitespace character
was replaced with one space character. All consecu-
tive space characters were replaced with one space
character. This preempts potential string parsing and
string comparing problems, improves compatibility
with potential dictionaries and third party algorithms
(e.g. stop word corpus, stemming algorithms), and
most importantly, normalizes and combines similar
features (e.g. don’t vs dont, Thai vs thai, delicious vs
delicious!).

C. Dataset Processing During Optimization

Stop words based on Natural Language Toolkit’s
(NLTK)[7] stop word corpus were removed to reduce
common neutral-sentiment features that provide little
predictive value. Stemming was introduced by ac-
cessing NLTK’s implementation of Porter’s stemming
algorithm, which allows for further normalization and
combination of similar features.

A. Multinomial Naive Bayes

IV. Methods

Like many learning algorithms, Multinomial Naive
Bayes ﬁrst attempts to derive parameters that max-
imize the joint likelihood of the training data, given
by
L(Φy=0, ..., Φy=p−1, Φj|y=0, ..., Φj|y=p) =

p(x(i), y(i))

m(cid:89)

i=1

(1)
where Φy=q = P (y = q), Φj|y=q = P (xj = 1|y = q), q is
a number in the range of classes, j is a number in the
range of features, m is the number of training exam-
ples, p is the number of classes, and (x(i), y(i)) are the
ith training example. Making the "naive" assumption
that given any y, all xi are mutually independent, the
solution to the maximization is

(cid:80)m
(cid:80)m

(cid:111)
j = 1 ∧ y(i) = 1
x(i)

1(cid:8)y(i) = q(cid:9)

(cid:110)
(cid:80)m
1(cid:8)y(i) = q(cid:9)

i=1

1

i=1

i=1

m

Φj|y=q =

Φy=q =

(2)

(3)

Once the parameters from equations 2 and 3 are
ﬁt from the training data, prediction of a given test
example is simply calculating argmaxq P (y = q|x) =
P (x=1|y=q)P (y=q)
, where q is the class with the highest
posterior probability of all classes and x is the feature
vector.

P (x)

B. Support Vector Classiﬁer

SVC attempts to deﬁne a line (or hyperplane in
higher dimensions) that separates classes in a plot
of the training set that maximizes the distance (also
known as the margin) between the line and the closest
training example of each class. This line is called
the decision boundary. If line is given by f (x) =
(wT x + b), it can be shown that the decision boundary
parameters can be set by

minγ,w,b

|w|2 + C

1
2

m(cid:88)

i=1

ξi

D. Feature Selection

with the constraint that

After preliminary processing, the baseline feature
size was 19,898 for allReviews and 18,102 for segRe-
views. Manual removal of features was attempted by
looking through the feature list. Ngrams were used to
increase the feature set to include groups of words

y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m
ξi ≥ 0, i = 1, ..., m

where (x(i), y(i)) are the ith training example, w and b
are parameters of the line, C and ξ are penalty terms

(4)

(5)
(6)

Prediction of Yelp Ratings Based on Reviewer Comments

Segmented by Business Type

Kent Lee1∗

and James Ross2∗

1Stanford University, Department of Structural Biology, Biophysics Program

2Stanford University, Department of Computer Science

I. INTRODUCTION

Yelp[1] is a popular website that allows users to
submit numerical ratings (integers 1-5, inclusive, 5
being positive sentiment) and text reviews (reviewer’s
comments) to express their experiences interacting
with various businesses. This information helps others
determine which businesses best ﬁt their needs. The
prevalence and utility of rating and review services
provide the opportunity and motivation to study the
prediction of ratings based on reviews. While these
services are prevalent, many internet forums are ded-
icated to publishing user reviews of products such
as laptops[3] and vehicles[4] without quantitative
ranking. Often, forums hosting qualitative reviews
such as "laptop stopped working but helpdesk was
useless!" and "customer service was horrible" would
beneﬁt from an algorithm predicting rating from re-
views to summarize user sentiment quantitatively.
Furthermore, prediction algorithms allow rating and
review websites gather additional rating information
by predicting from external enthusiast forums where
reviews are more descriptive.

Our goal was to apply machine learning techniques
to Yelp’s published dataset to accurately predict rat-
ings from reviews. An input of a string of words
into our Multinomial Naive Bayes algorithm would
output a predicted rating between 1 and 5 describ-
ing the customer’s sentiment regarding a business.
While this has previously been attempted[5][6], most
approaches do not focus on training data segmen-
tation. We believed that segmenting training data
by business sector (e.g. restaurants) allows more
accurate predictions because feature sets become
less "diluted" or noisy and words gain meaningful
value when given a focused context. For example, an
auto mechanic business customer may be "extremely
satisﬁed with the new cold air intake system!" while
a restaurant customer may say that "the food took a
while and came out cold!" Taken together, if feature

∗These authors contributed equally to this work

size was ﬁxed, the word "system" would offer as
little predictive value in ranking restaurants as "food"
would for the auto business. Furthermore, "cold" is
sentimentally negative in the restaurant review, yet
positive in the auto business review. Training data
segmentation aims to reduce such uninformative oc-
currences.

II. RELATED WORK

Interesting approaches involve looking up the quan-
tiﬁed sentiment of each word from SentiWordNet and
summing the positive/negative/neutral in a document
and ﬁtting to regression rather than a completely
probabilistic approach[12][11]. While I think this is
the best method, the disadvantages are being lim-
ited to characterized words. Groups using non-word
features such as part-of-speech is an approach we
have not considered[10]. A useful troubleshooting
approach is displaying the highest impact features
in a prediction[9]. The potential to use latent factors
through matrix factorization seemed to have poten-
tial, but others who tried it received lower MSE than
us[13]

III. DATASET AND FEATURES

A. Dataset

Data was

downloaded

from Yelp’s Dataset
Challenge[2] in JSON format containing 1.6 million
reviews and 61 thousand businesses. The dataset
was trimmed by a custom C# script to yield a dataset
containing entries with only metrics of
interest:
review text on the business and reviewer rating of
the business. This formed the dataset allReviews. To
create a dataset containing only reviews from one
business segment, we identiﬁed the top represented
business segment in allReviews and created a dataset
containing only entries from this segment. The top
represented segment was Restaurant (21,892 entries)
so all reviews on restaurants made up segReviews.
Thus, allReviews and its subset segReviews are the
two datasets that were used. Each dataset contained

8,000 entries as training set, 1,000 as validation set,
and 1,000 as test set. Set sizes were smaller than
available data due to hardware constraints during
optimization. Data was converted from the JSON
format to the form of
***Reviews_Ratings.txt:

that may collectively have sentimental value while
sentimentally-neutral individually. SelectKBest using
a Chi-squared distribution was used to select features
that were found to have a dependence on speciﬁc
labels. SelectKBest and Ngrams were implemented
through scikitlearn[8].

1 5
2 4
3 4
4 ...

***Reviews_Reviews.txt:

1 this vegas thai place can be tricky to find

it is off the strip hidden away in a
strip mall ...

2 yum yum thai iced coffee fried tofu starter

and ...

3 the commercial center mall is not the most
obvious choice for las vegas best thai
...

4 ...

B. Preliminary Dataset Processing

allReviews and segReviews were processed as fol-
lows. For each review entry, all characters were re-
moved except for whitespaces and English alphabet
characters. All characters were converted to low-
ercase. Each occurrence of a whitespace character
was replaced with one space character. All consecu-
tive space characters were replaced with one space
character. This preempts potential string parsing and
string comparing problems, improves compatibility
with potential dictionaries and third party algorithms
(e.g. stop word corpus, stemming algorithms), and
most importantly, normalizes and combines similar
features (e.g. don’t vs dont, Thai vs thai, delicious vs
delicious!).

C. Dataset Processing During Optimization

Stop words based on Natural Language Toolkit’s
(NLTK)[7] stop word corpus were removed to reduce
common neutral-sentiment features that provide little
predictive value. Stemming was introduced by ac-
cessing NLTK’s implementation of Porter’s stemming
algorithm, which allows for further normalization and
combination of similar features.

A. Multinomial Naive Bayes

IV. Methods

Like many learning algorithms, Multinomial Naive
Bayes ﬁrst attempts to derive parameters that max-
imize the joint likelihood of the training data, given
by
L(Φy=0, ..., Φy=p−1, Φj|y=0, ..., Φj|y=p) =

p(x(i), y(i))

m(cid:89)

i=1

(1)
where Φy=q = P (y = q), Φj|y=q = P (xj = 1|y = q), q is
a number in the range of classes, j is a number in the
range of features, m is the number of training exam-
ples, p is the number of classes, and (x(i), y(i)) are the
ith training example. Making the "naive" assumption
that given any y, all xi are mutually independent, the
solution to the maximization is

(cid:80)m
(cid:80)m

(cid:111)
j = 1 ∧ y(i) = 1
x(i)

1(cid:8)y(i) = q(cid:9)

(cid:110)
(cid:80)m
1(cid:8)y(i) = q(cid:9)

i=1

1

i=1

i=1

m

Φj|y=q =

Φy=q =

(2)

(3)

Once the parameters from equations 2 and 3 are
ﬁt from the training data, prediction of a given test
example is simply calculating argmaxq P (y = q|x) =
P (x=1|y=q)P (y=q)
, where q is the class with the highest
posterior probability of all classes and x is the feature
vector.

P (x)

B. Support Vector Classiﬁer

SVC attempts to deﬁne a line (or hyperplane in
higher dimensions) that separates classes in a plot
of the training set that maximizes the distance (also
known as the margin) between the line and the closest
training example of each class. This line is called
the decision boundary. If line is given by f (x) =
(wT x + b), it can be shown that the decision boundary
parameters can be set by

minγ,w,b

|w|2 + C

1
2

m(cid:88)

i=1

ξi

D. Feature Selection

with the constraint that

After preliminary processing, the baseline feature
size was 19,898 for allReviews and 18,102 for segRe-
views. Manual removal of features was attempted by
looking through the feature list. Ngrams were used to
increase the feature set to include groups of words

y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m
ξi ≥ 0, i = 1, ..., m

where (x(i), y(i)) are the ith training example, w and b
are parameters of the line, C and ξ are penalty terms

(4)

(5)
(6)

that penalize the objective function for each training
example on the wrong side of the decision boundary,
and m is the number of training examples. After the
optimization problem is solved, the decision boundary
is known and classiﬁcation can be made by determin-
ing which side the training example lies with respect
to the decision boundary. It turns out that the deci-
sion boundary depends only on the training examples
closest to the decision boundary (known as support
vectors), which makes training SVCs computationally
efﬁcient. Furthermore, it turns out that the solution
can be written as a linear combination of (cid:104)x(i), x(cid:105),
which is the form of a Kernel, allowing efﬁcient com-
putation for large feature sizes. Thus, features are of-
ten transformed into other feature spaces to achieve
better data separation. Here, we used a linear kernel.
For classiﬁcation with more than 2 classes, simply
classify one class versus the rest of the classes, for
each class. We applied scikitlearn’s implementation
of both algorithms. Immediately before running any
algorithm, the reviews were ﬁrst tokenized with the
exception of experiments with SelectKBest, where the
features are tokenized, then run through SelectKBest,
then run through the machine learning algorithm.

V. Discussion

A. Metrics

The main metric used was accuracy, given by the
number of validation or test examples correctly la-
beled divided by the total examples to be predicted.
A higher accuracy suggested that the predictor was
often able to correctly classify the reviews. While
cross validation was not done on validation and test
data, 10-fold cross validation was used in generat-
ing learning curves. In the ﬁnal comparison, mean
squared error (MSE) was used to determine the av-
erage difference between the predicted label and the
actual label: 1
n

(yi,pred − yi,actual)2.

(cid:80)n−1

0

B. Baselining

To construct an initial accuracy comparison be-
tween predictors trained on unsegmented and seg-
mented datasets, baseline prediction accuracies of
allReviews and segReviews were each determined by
training a Multinomial Naive Bayes (MNB) algorithm
on 6,000 training examples and testing on 1,000 test
examples, with a vocabulary of the 5,000 highest
frequency words. MNB was used because it was one
of the simplest and quickest to train classiﬁcation
algorithms available. While we started with MNB, we
later attempted SVC after optimizing our features.
The number of training examples and feature size
initially used were picked arbitrarily to ensure room
for optimization in either direction. This prevented

wasting time optimizing the model in the beginning
when many decisions were still in ﬂux.

Table I presents the data that will be discussed in
the rest of the discussion section. Attention drawn to
speciﬁc rows will be of the form "R#".

The baseline showed that segReviews (R2) had
higher accuracies than allReviews (R1) for the test
set (0.55 and 0.52 respectively), which supported our
hypothesis that segmented training examples yield
better predictors than unsegmented. However, lack
of difference in training errors (both 0.72) led us to
suspect that test set accuracy differences may have
been noise.

C. Stop Words and Stemming

Once a baseline was formed, it was more efﬁcient
to focus on improving only segReviews accuracy in-
stead of applying each subsequent change to both
segReviews and allReviews. Once performance on
segReviews was satisfactory, we returned to apply the
same optimizations to allReviews and compare accu-
racies again. Thus from here onwards, optimization
was against segReviews only. Stop words and stem-
ming is known to help with feature normalization and
reducing the possible total vocabulary by removing
words that are either neutral or common in all classes
(e.g. "the", "and", "was") and by replacing words
stemming from the same root with the root itself
(e.g. replacing "purchase", "purchased", "purchasing"
with "purchas", collapses 5 features into 1). After re-
moving stop words (R4), training accuracy increased
marginally to 0.73 while validation accuracy remained
constant at 0.53 as compared to validation baseline
(R3). Stemming (R5) decreased training accuracy to
0.72 and validation accuracy decreased to 0.52. A
plausible explanation for no signiﬁcant improvement
after stemming and removing stop words was that our
feature set was still too large for such optimizations to
make a difference; the top features may still contain
words that have little sentimental value. It is also
possible that the features used may contain few stop
words and words affected by stemming.

D. Learning Curve Analysis

We wanted a rational approach to improve our
model rather than attempting random optimization
pathways so we generated a learning curve with
scikitlearn to test for overﬁtting or underﬁtting. Plots
have points representing averages of 10-fold cross
validation, with shaded regions being standard de-
viation. As seen in Figure 1, training accuracy was
signiﬁcantly higher than cross validation accuracy,
indicative of high variance and overﬁtting. The high
number of features allowed the algorithm more de-
grees of
freedom to ﬁt the training data better,

Prediction of Yelp Ratings Based on Reviewer Comments

Segmented by Business Type

Kent Lee1∗

and James Ross2∗

1Stanford University, Department of Structural Biology, Biophysics Program

2Stanford University, Department of Computer Science

I. INTRODUCTION

Yelp[1] is a popular website that allows users to
submit numerical ratings (integers 1-5, inclusive, 5
being positive sentiment) and text reviews (reviewer’s
comments) to express their experiences interacting
with various businesses. This information helps others
determine which businesses best ﬁt their needs. The
prevalence and utility of rating and review services
provide the opportunity and motivation to study the
prediction of ratings based on reviews. While these
services are prevalent, many internet forums are ded-
icated to publishing user reviews of products such
as laptops[3] and vehicles[4] without quantitative
ranking. Often, forums hosting qualitative reviews
such as "laptop stopped working but helpdesk was
useless!" and "customer service was horrible" would
beneﬁt from an algorithm predicting rating from re-
views to summarize user sentiment quantitatively.
Furthermore, prediction algorithms allow rating and
review websites gather additional rating information
by predicting from external enthusiast forums where
reviews are more descriptive.

Our goal was to apply machine learning techniques
to Yelp’s published dataset to accurately predict rat-
ings from reviews. An input of a string of words
into our Multinomial Naive Bayes algorithm would
output a predicted rating between 1 and 5 describ-
ing the customer’s sentiment regarding a business.
While this has previously been attempted[5][6], most
approaches do not focus on training data segmen-
tation. We believed that segmenting training data
by business sector (e.g. restaurants) allows more
accurate predictions because feature sets become
less "diluted" or noisy and words gain meaningful
value when given a focused context. For example, an
auto mechanic business customer may be "extremely
satisﬁed with the new cold air intake system!" while
a restaurant customer may say that "the food took a
while and came out cold!" Taken together, if feature

∗These authors contributed equally to this work

size was ﬁxed, the word "system" would offer as
little predictive value in ranking restaurants as "food"
would for the auto business. Furthermore, "cold" is
sentimentally negative in the restaurant review, yet
positive in the auto business review. Training data
segmentation aims to reduce such uninformative oc-
currences.

II. RELATED WORK

Interesting approaches involve looking up the quan-
tiﬁed sentiment of each word from SentiWordNet and
summing the positive/negative/neutral in a document
and ﬁtting to regression rather than a completely
probabilistic approach[12][11]. While I think this is
the best method, the disadvantages are being lim-
ited to characterized words. Groups using non-word
features such as part-of-speech is an approach we
have not considered[10]. A useful troubleshooting
approach is displaying the highest impact features
in a prediction[9]. The potential to use latent factors
through matrix factorization seemed to have poten-
tial, but others who tried it received lower MSE than
us[13]

III. DATASET AND FEATURES

A. Dataset

Data was

downloaded

from Yelp’s Dataset
Challenge[2] in JSON format containing 1.6 million
reviews and 61 thousand businesses. The dataset
was trimmed by a custom C# script to yield a dataset
containing entries with only metrics of
interest:
review text on the business and reviewer rating of
the business. This formed the dataset allReviews. To
create a dataset containing only reviews from one
business segment, we identiﬁed the top represented
business segment in allReviews and created a dataset
containing only entries from this segment. The top
represented segment was Restaurant (21,892 entries)
so all reviews on restaurants made up segReviews.
Thus, allReviews and its subset segReviews are the
two datasets that were used. Each dataset contained

8,000 entries as training set, 1,000 as validation set,
and 1,000 as test set. Set sizes were smaller than
available data due to hardware constraints during
optimization. Data was converted from the JSON
format to the form of
***Reviews_Ratings.txt:

that may collectively have sentimental value while
sentimentally-neutral individually. SelectKBest using
a Chi-squared distribution was used to select features
that were found to have a dependence on speciﬁc
labels. SelectKBest and Ngrams were implemented
through scikitlearn[8].

1 5
2 4
3 4
4 ...

***Reviews_Reviews.txt:

1 this vegas thai place can be tricky to find

it is off the strip hidden away in a
strip mall ...

2 yum yum thai iced coffee fried tofu starter

and ...

3 the commercial center mall is not the most
obvious choice for las vegas best thai
...

4 ...

B. Preliminary Dataset Processing

allReviews and segReviews were processed as fol-
lows. For each review entry, all characters were re-
moved except for whitespaces and English alphabet
characters. All characters were converted to low-
ercase. Each occurrence of a whitespace character
was replaced with one space character. All consecu-
tive space characters were replaced with one space
character. This preempts potential string parsing and
string comparing problems, improves compatibility
with potential dictionaries and third party algorithms
(e.g. stop word corpus, stemming algorithms), and
most importantly, normalizes and combines similar
features (e.g. don’t vs dont, Thai vs thai, delicious vs
delicious!).

C. Dataset Processing During Optimization

Stop words based on Natural Language Toolkit’s
(NLTK)[7] stop word corpus were removed to reduce
common neutral-sentiment features that provide little
predictive value. Stemming was introduced by ac-
cessing NLTK’s implementation of Porter’s stemming
algorithm, which allows for further normalization and
combination of similar features.

A. Multinomial Naive Bayes

IV. Methods

Like many learning algorithms, Multinomial Naive
Bayes ﬁrst attempts to derive parameters that max-
imize the joint likelihood of the training data, given
by
L(Φy=0, ..., Φy=p−1, Φj|y=0, ..., Φj|y=p) =

p(x(i), y(i))

m(cid:89)

i=1

(1)
where Φy=q = P (y = q), Φj|y=q = P (xj = 1|y = q), q is
a number in the range of classes, j is a number in the
range of features, m is the number of training exam-
ples, p is the number of classes, and (x(i), y(i)) are the
ith training example. Making the "naive" assumption
that given any y, all xi are mutually independent, the
solution to the maximization is

(cid:80)m
(cid:80)m

(cid:111)
j = 1 ∧ y(i) = 1
x(i)

1(cid:8)y(i) = q(cid:9)

(cid:110)
(cid:80)m
1(cid:8)y(i) = q(cid:9)

i=1

1

i=1

i=1

m

Φj|y=q =

Φy=q =

(2)

(3)

Once the parameters from equations 2 and 3 are
ﬁt from the training data, prediction of a given test
example is simply calculating argmaxq P (y = q|x) =
P (x=1|y=q)P (y=q)
, where q is the class with the highest
posterior probability of all classes and x is the feature
vector.

P (x)

B. Support Vector Classiﬁer

SVC attempts to deﬁne a line (or hyperplane in
higher dimensions) that separates classes in a plot
of the training set that maximizes the distance (also
known as the margin) between the line and the closest
training example of each class. This line is called
the decision boundary. If line is given by f (x) =
(wT x + b), it can be shown that the decision boundary
parameters can be set by

minγ,w,b

|w|2 + C

1
2

m(cid:88)

i=1

ξi

D. Feature Selection

with the constraint that

After preliminary processing, the baseline feature
size was 19,898 for allReviews and 18,102 for segRe-
views. Manual removal of features was attempted by
looking through the feature list. Ngrams were used to
increase the feature set to include groups of words

y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m
ξi ≥ 0, i = 1, ..., m

where (x(i), y(i)) are the ith training example, w and b
are parameters of the line, C and ξ are penalty terms

(4)

(5)
(6)

that penalize the objective function for each training
example on the wrong side of the decision boundary,
and m is the number of training examples. After the
optimization problem is solved, the decision boundary
is known and classiﬁcation can be made by determin-
ing which side the training example lies with respect
to the decision boundary. It turns out that the deci-
sion boundary depends only on the training examples
closest to the decision boundary (known as support
vectors), which makes training SVCs computationally
efﬁcient. Furthermore, it turns out that the solution
can be written as a linear combination of (cid:104)x(i), x(cid:105),
which is the form of a Kernel, allowing efﬁcient com-
putation for large feature sizes. Thus, features are of-
ten transformed into other feature spaces to achieve
better data separation. Here, we used a linear kernel.
For classiﬁcation with more than 2 classes, simply
classify one class versus the rest of the classes, for
each class. We applied scikitlearn’s implementation
of both algorithms. Immediately before running any
algorithm, the reviews were ﬁrst tokenized with the
exception of experiments with SelectKBest, where the
features are tokenized, then run through SelectKBest,
then run through the machine learning algorithm.

V. Discussion

A. Metrics

The main metric used was accuracy, given by the
number of validation or test examples correctly la-
beled divided by the total examples to be predicted.
A higher accuracy suggested that the predictor was
often able to correctly classify the reviews. While
cross validation was not done on validation and test
data, 10-fold cross validation was used in generat-
ing learning curves. In the ﬁnal comparison, mean
squared error (MSE) was used to determine the av-
erage difference between the predicted label and the
actual label: 1
n

(yi,pred − yi,actual)2.

(cid:80)n−1

0

B. Baselining

To construct an initial accuracy comparison be-
tween predictors trained on unsegmented and seg-
mented datasets, baseline prediction accuracies of
allReviews and segReviews were each determined by
training a Multinomial Naive Bayes (MNB) algorithm
on 6,000 training examples and testing on 1,000 test
examples, with a vocabulary of the 5,000 highest
frequency words. MNB was used because it was one
of the simplest and quickest to train classiﬁcation
algorithms available. While we started with MNB, we
later attempted SVC after optimizing our features.
The number of training examples and feature size
initially used were picked arbitrarily to ensure room
for optimization in either direction. This prevented

wasting time optimizing the model in the beginning
when many decisions were still in ﬂux.

Table I presents the data that will be discussed in
the rest of the discussion section. Attention drawn to
speciﬁc rows will be of the form "R#".

The baseline showed that segReviews (R2) had
higher accuracies than allReviews (R1) for the test
set (0.55 and 0.52 respectively), which supported our
hypothesis that segmented training examples yield
better predictors than unsegmented. However, lack
of difference in training errors (both 0.72) led us to
suspect that test set accuracy differences may have
been noise.

C. Stop Words and Stemming

Once a baseline was formed, it was more efﬁcient
to focus on improving only segReviews accuracy in-
stead of applying each subsequent change to both
segReviews and allReviews. Once performance on
segReviews was satisfactory, we returned to apply the
same optimizations to allReviews and compare accu-
racies again. Thus from here onwards, optimization
was against segReviews only. Stop words and stem-
ming is known to help with feature normalization and
reducing the possible total vocabulary by removing
words that are either neutral or common in all classes
(e.g. "the", "and", "was") and by replacing words
stemming from the same root with the root itself
(e.g. replacing "purchase", "purchased", "purchasing"
with "purchas", collapses 5 features into 1). After re-
moving stop words (R4), training accuracy increased
marginally to 0.73 while validation accuracy remained
constant at 0.53 as compared to validation baseline
(R3). Stemming (R5) decreased training accuracy to
0.72 and validation accuracy decreased to 0.52. A
plausible explanation for no signiﬁcant improvement
after stemming and removing stop words was that our
feature set was still too large for such optimizations to
make a difference; the top features may still contain
words that have little sentimental value. It is also
possible that the features used may contain few stop
words and words affected by stemming.

D. Learning Curve Analysis

We wanted a rational approach to improve our
model rather than attempting random optimization
pathways so we generated a learning curve with
scikitlearn to test for overﬁtting or underﬁtting. Plots
have points representing averages of 10-fold cross
validation, with shaded regions being standard de-
viation. As seen in Figure 1, training accuracy was
signiﬁcantly higher than cross validation accuracy,
indicative of high variance and overﬁtting. The high
number of features allowed the algorithm more de-
grees of
freedom to ﬁt the training data better,

Row Procedure
1
2
3
4
5
6
7
8
9
10
11
12
13
14

Baseline
Baseline
Baseline
A = Baseline + Stop Words
B = A + Stemming
C = B + FeatureSize=200
D = C + TrainingExamples=8000
E = D + Manual Stop Words
F = E + skb=500 + FeatureSize = 5000
G = F + Bigram + Trigrams (Optimal)
G (Optimal)
G (Optimal)
G (Optimal)
G (Optimal)

Dataset
allReviews
segReview
segReview
segReview
segReview
segReview
segReview
segReview
segReview
segReview
segReview
allReviews
segReviews
allReviews

Type
Test Data
Test Data
Validation
Validation
Validation
Validation
Validation
Validation
Validation
Validation
Test Data
Test Data
Test Data
Test Data

TABLE I

Validate/Test Accuracy
0.515
0.554
0.534
0.533
0.515
0.542
0.521
0.535
0.589
0.597
0.632
0.548
0.616
0.558

Algorithm

Train Accuracy MSE
0.721
0.724
0.724
0.732
0.722
0.544
0.535
0.559
0.628
0.635
0.635
0.582
0.709
0.672

0.869 MNB
0.61
MNB
0.799 MNB
0.777 MNB
0.774 MNB
1.151 MNB
1.12
MNB
1.009 MNB
0.685 MNB
0.667 MNB
0.522 MNB
0.833 MNB
SVC
0.563
0.864
SVC

The effect of optimization procedure on prediction accuracy and mean squared error based on dataset and algorithm used.

Fig. 1. Learning curve after stop word removal and stemming with
5,000 features shows high variance. Score is prediction accuracy.

Fig. 2.
with 200 features is optimal. Score is prediction accuracy

Learning curve after stop word removal and stemming

Fig. 3.
with 20 features shows high bias. Score is prediction accuracy

Learning curve after stop word removal and stemming

yielding signiﬁcantly higher training accuracy. To re-
duce overﬁtting, we attempt to reduce the number
of features. Figure 2 shows the learning curve with
200 features while Figure 3 shows underﬁtting when
feature size decreased to 20, where training accu-
racy decreased dramatically from a maximum of 0.82
to a maximum of 0.54, indicating high bias. After
empirical optimization, stop word removal and stem-
ming with 200 features was optimal (R6), yielding
lower training and higher validation accuracy (0.54
for both training and validation accuracy). Figure 2
shows no more overﬁtting or underﬁtting. The in-
crease in validation accuracy was encouraging and
the decrease in training accuracy was associated with
the correction from overﬁtting. With 6,000 training
examples and 200 features, this agrees with the idea
that the number of features should be signiﬁcantly
smaller than the number of training examples. The
convergence between training and cross validation
accuracy implies that the predictor generalizes well to
unseen data. Figure 2 shows slight upwards slope in

Prediction of Yelp Ratings Based on Reviewer Comments

Segmented by Business Type

Kent Lee1∗

and James Ross2∗

1Stanford University, Department of Structural Biology, Biophysics Program

2Stanford University, Department of Computer Science

I. INTRODUCTION

Yelp[1] is a popular website that allows users to
submit numerical ratings (integers 1-5, inclusive, 5
being positive sentiment) and text reviews (reviewer’s
comments) to express their experiences interacting
with various businesses. This information helps others
determine which businesses best ﬁt their needs. The
prevalence and utility of rating and review services
provide the opportunity and motivation to study the
prediction of ratings based on reviews. While these
services are prevalent, many internet forums are ded-
icated to publishing user reviews of products such
as laptops[3] and vehicles[4] without quantitative
ranking. Often, forums hosting qualitative reviews
such as "laptop stopped working but helpdesk was
useless!" and "customer service was horrible" would
beneﬁt from an algorithm predicting rating from re-
views to summarize user sentiment quantitatively.
Furthermore, prediction algorithms allow rating and
review websites gather additional rating information
by predicting from external enthusiast forums where
reviews are more descriptive.

Our goal was to apply machine learning techniques
to Yelp’s published dataset to accurately predict rat-
ings from reviews. An input of a string of words
into our Multinomial Naive Bayes algorithm would
output a predicted rating between 1 and 5 describ-
ing the customer’s sentiment regarding a business.
While this has previously been attempted[5][6], most
approaches do not focus on training data segmen-
tation. We believed that segmenting training data
by business sector (e.g. restaurants) allows more
accurate predictions because feature sets become
less "diluted" or noisy and words gain meaningful
value when given a focused context. For example, an
auto mechanic business customer may be "extremely
satisﬁed with the new cold air intake system!" while
a restaurant customer may say that "the food took a
while and came out cold!" Taken together, if feature

∗These authors contributed equally to this work

size was ﬁxed, the word "system" would offer as
little predictive value in ranking restaurants as "food"
would for the auto business. Furthermore, "cold" is
sentimentally negative in the restaurant review, yet
positive in the auto business review. Training data
segmentation aims to reduce such uninformative oc-
currences.

II. RELATED WORK

Interesting approaches involve looking up the quan-
tiﬁed sentiment of each word from SentiWordNet and
summing the positive/negative/neutral in a document
and ﬁtting to regression rather than a completely
probabilistic approach[12][11]. While I think this is
the best method, the disadvantages are being lim-
ited to characterized words. Groups using non-word
features such as part-of-speech is an approach we
have not considered[10]. A useful troubleshooting
approach is displaying the highest impact features
in a prediction[9]. The potential to use latent factors
through matrix factorization seemed to have poten-
tial, but others who tried it received lower MSE than
us[13]

III. DATASET AND FEATURES

A. Dataset

Data was

downloaded

from Yelp’s Dataset
Challenge[2] in JSON format containing 1.6 million
reviews and 61 thousand businesses. The dataset
was trimmed by a custom C# script to yield a dataset
containing entries with only metrics of
interest:
review text on the business and reviewer rating of
the business. This formed the dataset allReviews. To
create a dataset containing only reviews from one
business segment, we identiﬁed the top represented
business segment in allReviews and created a dataset
containing only entries from this segment. The top
represented segment was Restaurant (21,892 entries)
so all reviews on restaurants made up segReviews.
Thus, allReviews and its subset segReviews are the
two datasets that were used. Each dataset contained

8,000 entries as training set, 1,000 as validation set,
and 1,000 as test set. Set sizes were smaller than
available data due to hardware constraints during
optimization. Data was converted from the JSON
format to the form of
***Reviews_Ratings.txt:

that may collectively have sentimental value while
sentimentally-neutral individually. SelectKBest using
a Chi-squared distribution was used to select features
that were found to have a dependence on speciﬁc
labels. SelectKBest and Ngrams were implemented
through scikitlearn[8].

1 5
2 4
3 4
4 ...

***Reviews_Reviews.txt:

1 this vegas thai place can be tricky to find

it is off the strip hidden away in a
strip mall ...

2 yum yum thai iced coffee fried tofu starter

and ...

3 the commercial center mall is not the most
obvious choice for las vegas best thai
...

4 ...

B. Preliminary Dataset Processing

allReviews and segReviews were processed as fol-
lows. For each review entry, all characters were re-
moved except for whitespaces and English alphabet
characters. All characters were converted to low-
ercase. Each occurrence of a whitespace character
was replaced with one space character. All consecu-
tive space characters were replaced with one space
character. This preempts potential string parsing and
string comparing problems, improves compatibility
with potential dictionaries and third party algorithms
(e.g. stop word corpus, stemming algorithms), and
most importantly, normalizes and combines similar
features (e.g. don’t vs dont, Thai vs thai, delicious vs
delicious!).

C. Dataset Processing During Optimization

Stop words based on Natural Language Toolkit’s
(NLTK)[7] stop word corpus were removed to reduce
common neutral-sentiment features that provide little
predictive value. Stemming was introduced by ac-
cessing NLTK’s implementation of Porter’s stemming
algorithm, which allows for further normalization and
combination of similar features.

A. Multinomial Naive Bayes

IV. Methods

Like many learning algorithms, Multinomial Naive
Bayes ﬁrst attempts to derive parameters that max-
imize the joint likelihood of the training data, given
by
L(Φy=0, ..., Φy=p−1, Φj|y=0, ..., Φj|y=p) =

p(x(i), y(i))

m(cid:89)

i=1

(1)
where Φy=q = P (y = q), Φj|y=q = P (xj = 1|y = q), q is
a number in the range of classes, j is a number in the
range of features, m is the number of training exam-
ples, p is the number of classes, and (x(i), y(i)) are the
ith training example. Making the "naive" assumption
that given any y, all xi are mutually independent, the
solution to the maximization is

(cid:80)m
(cid:80)m

(cid:111)
j = 1 ∧ y(i) = 1
x(i)

1(cid:8)y(i) = q(cid:9)

(cid:110)
(cid:80)m
1(cid:8)y(i) = q(cid:9)

i=1

1

i=1

i=1

m

Φj|y=q =

Φy=q =

(2)

(3)

Once the parameters from equations 2 and 3 are
ﬁt from the training data, prediction of a given test
example is simply calculating argmaxq P (y = q|x) =
P (x=1|y=q)P (y=q)
, where q is the class with the highest
posterior probability of all classes and x is the feature
vector.

P (x)

B. Support Vector Classiﬁer

SVC attempts to deﬁne a line (or hyperplane in
higher dimensions) that separates classes in a plot
of the training set that maximizes the distance (also
known as the margin) between the line and the closest
training example of each class. This line is called
the decision boundary. If line is given by f (x) =
(wT x + b), it can be shown that the decision boundary
parameters can be set by

minγ,w,b

|w|2 + C

1
2

m(cid:88)

i=1

ξi

D. Feature Selection

with the constraint that

After preliminary processing, the baseline feature
size was 19,898 for allReviews and 18,102 for segRe-
views. Manual removal of features was attempted by
looking through the feature list. Ngrams were used to
increase the feature set to include groups of words

y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m
ξi ≥ 0, i = 1, ..., m

where (x(i), y(i)) are the ith training example, w and b
are parameters of the line, C and ξ are penalty terms

(4)

(5)
(6)

that penalize the objective function for each training
example on the wrong side of the decision boundary,
and m is the number of training examples. After the
optimization problem is solved, the decision boundary
is known and classiﬁcation can be made by determin-
ing which side the training example lies with respect
to the decision boundary. It turns out that the deci-
sion boundary depends only on the training examples
closest to the decision boundary (known as support
vectors), which makes training SVCs computationally
efﬁcient. Furthermore, it turns out that the solution
can be written as a linear combination of (cid:104)x(i), x(cid:105),
which is the form of a Kernel, allowing efﬁcient com-
putation for large feature sizes. Thus, features are of-
ten transformed into other feature spaces to achieve
better data separation. Here, we used a linear kernel.
For classiﬁcation with more than 2 classes, simply
classify one class versus the rest of the classes, for
each class. We applied scikitlearn’s implementation
of both algorithms. Immediately before running any
algorithm, the reviews were ﬁrst tokenized with the
exception of experiments with SelectKBest, where the
features are tokenized, then run through SelectKBest,
then run through the machine learning algorithm.

V. Discussion

A. Metrics

The main metric used was accuracy, given by the
number of validation or test examples correctly la-
beled divided by the total examples to be predicted.
A higher accuracy suggested that the predictor was
often able to correctly classify the reviews. While
cross validation was not done on validation and test
data, 10-fold cross validation was used in generat-
ing learning curves. In the ﬁnal comparison, mean
squared error (MSE) was used to determine the av-
erage difference between the predicted label and the
actual label: 1
n

(yi,pred − yi,actual)2.

(cid:80)n−1

0

B. Baselining

To construct an initial accuracy comparison be-
tween predictors trained on unsegmented and seg-
mented datasets, baseline prediction accuracies of
allReviews and segReviews were each determined by
training a Multinomial Naive Bayes (MNB) algorithm
on 6,000 training examples and testing on 1,000 test
examples, with a vocabulary of the 5,000 highest
frequency words. MNB was used because it was one
of the simplest and quickest to train classiﬁcation
algorithms available. While we started with MNB, we
later attempted SVC after optimizing our features.
The number of training examples and feature size
initially used were picked arbitrarily to ensure room
for optimization in either direction. This prevented

wasting time optimizing the model in the beginning
when many decisions were still in ﬂux.

Table I presents the data that will be discussed in
the rest of the discussion section. Attention drawn to
speciﬁc rows will be of the form "R#".

The baseline showed that segReviews (R2) had
higher accuracies than allReviews (R1) for the test
set (0.55 and 0.52 respectively), which supported our
hypothesis that segmented training examples yield
better predictors than unsegmented. However, lack
of difference in training errors (both 0.72) led us to
suspect that test set accuracy differences may have
been noise.

C. Stop Words and Stemming

Once a baseline was formed, it was more efﬁcient
to focus on improving only segReviews accuracy in-
stead of applying each subsequent change to both
segReviews and allReviews. Once performance on
segReviews was satisfactory, we returned to apply the
same optimizations to allReviews and compare accu-
racies again. Thus from here onwards, optimization
was against segReviews only. Stop words and stem-
ming is known to help with feature normalization and
reducing the possible total vocabulary by removing
words that are either neutral or common in all classes
(e.g. "the", "and", "was") and by replacing words
stemming from the same root with the root itself
(e.g. replacing "purchase", "purchased", "purchasing"
with "purchas", collapses 5 features into 1). After re-
moving stop words (R4), training accuracy increased
marginally to 0.73 while validation accuracy remained
constant at 0.53 as compared to validation baseline
(R3). Stemming (R5) decreased training accuracy to
0.72 and validation accuracy decreased to 0.52. A
plausible explanation for no signiﬁcant improvement
after stemming and removing stop words was that our
feature set was still too large for such optimizations to
make a difference; the top features may still contain
words that have little sentimental value. It is also
possible that the features used may contain few stop
words and words affected by stemming.

D. Learning Curve Analysis

We wanted a rational approach to improve our
model rather than attempting random optimization
pathways so we generated a learning curve with
scikitlearn to test for overﬁtting or underﬁtting. Plots
have points representing averages of 10-fold cross
validation, with shaded regions being standard de-
viation. As seen in Figure 1, training accuracy was
signiﬁcantly higher than cross validation accuracy,
indicative of high variance and overﬁtting. The high
number of features allowed the algorithm more de-
grees of
freedom to ﬁt the training data better,

Row Procedure
1
2
3
4
5
6
7
8
9
10
11
12
13
14

Baseline
Baseline
Baseline
A = Baseline + Stop Words
B = A + Stemming
C = B + FeatureSize=200
D = C + TrainingExamples=8000
E = D + Manual Stop Words
F = E + skb=500 + FeatureSize = 5000
G = F + Bigram + Trigrams (Optimal)
G (Optimal)
G (Optimal)
G (Optimal)
G (Optimal)

Dataset
allReviews
segReview
segReview
segReview
segReview
segReview
segReview
segReview
segReview
segReview
segReview
allReviews
segReviews
allReviews

Type
Test Data
Test Data
Validation
Validation
Validation
Validation
Validation
Validation
Validation
Validation
Test Data
Test Data
Test Data
Test Data

TABLE I

Validate/Test Accuracy
0.515
0.554
0.534
0.533
0.515
0.542
0.521
0.535
0.589
0.597
0.632
0.548
0.616
0.558

Algorithm

Train Accuracy MSE
0.721
0.724
0.724
0.732
0.722
0.544
0.535
0.559
0.628
0.635
0.635
0.582
0.709
0.672

0.869 MNB
0.61
MNB
0.799 MNB
0.777 MNB
0.774 MNB
1.151 MNB
1.12
MNB
1.009 MNB
0.685 MNB
0.667 MNB
0.522 MNB
0.833 MNB
SVC
0.563
0.864
SVC

The effect of optimization procedure on prediction accuracy and mean squared error based on dataset and algorithm used.

Fig. 1. Learning curve after stop word removal and stemming with
5,000 features shows high variance. Score is prediction accuracy.

Fig. 2.
with 200 features is optimal. Score is prediction accuracy

Learning curve after stop word removal and stemming

Fig. 3.
with 20 features shows high bias. Score is prediction accuracy

Learning curve after stop word removal and stemming

yielding signiﬁcantly higher training accuracy. To re-
duce overﬁtting, we attempt to reduce the number
of features. Figure 2 shows the learning curve with
200 features while Figure 3 shows underﬁtting when
feature size decreased to 20, where training accu-
racy decreased dramatically from a maximum of 0.82
to a maximum of 0.54, indicating high bias. After
empirical optimization, stop word removal and stem-
ming with 200 features was optimal (R6), yielding
lower training and higher validation accuracy (0.54
for both training and validation accuracy). Figure 2
shows no more overﬁtting or underﬁtting. The in-
crease in validation accuracy was encouraging and
the decrease in training accuracy was associated with
the correction from overﬁtting. With 6,000 training
examples and 200 features, this agrees with the idea
that the number of features should be signiﬁcantly
smaller than the number of training examples. The
convergence between training and cross validation
accuracy implies that the predictor generalizes well to
unseen data. Figure 2 shows slight upwards slope in

cross validation accuracy, suggesting that increasing
training examples may increase validation accuracy
further. Increasing training set size from 6,000 to
8,000 revealed clear convergence (plot not shown)
of training and cross validation accuracy, but de-
creased validation accuracy to 0.52 (R7), suggesting
that cross validation learning curve has plateaued
and no more optimization of features size or training
examples is necessary.

E. Feature Optimization

Starting with manual

feature optimization, 83
sentiment-neutral words (e.g. food, burger, dish, Thai,
chicken, Bellagio) were identiﬁed from the features
to add to the stop words list, with the next highest
frequency words replacing those removed. The goal
of this is similar to that of removing stop words
and the process (R8) increased test accuracy to
0.54, speaking to the value of customized stop word
lists. However, it is nontrivial to manually remove
all words we considered sentiment-neutral given the
large vocabulary. Furthermore, it is likely that using
the highest frequency features may shadow some
highly predictive mid-frequency features. This led us
to SelectKBest using the Chi-squared, which evalu-
ates the likelihood that a given feature is independent
of a label and selects features that are not inde-
pendent of a label. Chi-squared describes how likely
the appearance of a speciﬁc feature for a speciﬁc
label is due to random chance based on the training
features and labels. From 5,000 features, SelectKBest
selected 500 features whose appearance for a label
are unlikely to be due to random chance (and thus are
dependent on the label). This signiﬁcantly improved
validation accuracies to 0.59 after some optimiza-
tion (R9). Bigram and trigram features were used
to further increase the features SelectKBest could
choose from, increasing validation accuracy to 0.60
(R10). nGram helps because some features have little
correlation with labels independently, but more cor-
relation when grouped together, giving SelectKBest
more dependent features. Select features identiﬁed
by SelectKBest are: "pretti damn", "goooood", "lick
plate", "ﬂavorless", "oh well", "perfect", "realli tast",
"tasteless", "would highli", and "appoint".

F. Confusion Matrix Analysis

Figure 4 shows a confusion matrix for the optimal
state (R10). The matrix has percentages that add to
100 across rows. Adjacent misclassiﬁcations make
up a majority of errors, which is understandable
since the difference between 4 and a 5 is slight and
subjective. We are pleased to see the majority of
classiﬁcations in each row are correct (red in the

Fig. 4. Confusion matrix for fully optimized segReviews with MNB
in percentages for each row. Total sample size is 999. Calculated
by scikitlearn

diagonal). Troubling areas are actual = 2, predicted
= 4 and actual = 3 and predicted = 1 and 5.

G. Return to Baseline

Satisﬁed with the improvements, we compare test
data accuracies between allReviews and segReviews
after applying our optimizations to both and against
baseline. Baseline (before optimization), segReviews
(R2) was 0.03 more accurate than allReviews (R1)
(0.55 against 0.52, respectively), but after optimiza-
tion, segReviews (R11) was 0.08 more accurate than
allReviews (R12) (0.63 against 0.55, respectively).
Additionally, optimization (R11) increased segReviews
(R2) accuracy by 0.08 compared to baseline. With
SVC, after applying the MNB optimization, segRe-
views (R13) was 0.06 more accurate than allReviews
(R14) (0.62 against 0.56, respectively).

VI. Conclusion

We have shown that training data segmentation
has a signiﬁcant effect on prediction accuracy, espe-
cially if incremental upon other dataset optimizations.
Training only on Yelp’s restaurant data shows that
an accuracy improvement of 0.08 over unsegmented
training data when given the same optimization pro-
cedure with MNB. The highest performing algorithm
was MNB with NLTK stop word removal, Proter’s
stemming, manual stop word removal, 8,000 training
examples, bigram and trigrams, and Select 500 best
from 5,000 features. This was better than SVM likely
because the whole optimization process was done
with MNB. At optimal, our MNB MSE was 0.522, so
on average, our prediction was off less than one a
Yelp rating ( 0.71).

For future work,

implementing forward search,
backward search, and mutual
information calcula-
tions to select better features may yield better se-
lections than SelectKBest. Introducing weights to
tokenization through TF-IDF, with TF factor being
raw counts, binary, or log normalized (1+log(feature
frequency)) may help adjust for frequently appear-
ing, low impact words. Finally, Applying segmented
training sets to other business segments would help
conﬁrm our ﬁnding.

Prediction of Yelp Ratings Based on Reviewer Comments

Segmented by Business Type

Kent Lee1∗

and James Ross2∗

1Stanford University, Department of Structural Biology, Biophysics Program

2Stanford University, Department of Computer Science

I. INTRODUCTION

Yelp[1] is a popular website that allows users to
submit numerical ratings (integers 1-5, inclusive, 5
being positive sentiment) and text reviews (reviewer’s
comments) to express their experiences interacting
with various businesses. This information helps others
determine which businesses best ﬁt their needs. The
prevalence and utility of rating and review services
provide the opportunity and motivation to study the
prediction of ratings based on reviews. While these
services are prevalent, many internet forums are ded-
icated to publishing user reviews of products such
as laptops[3] and vehicles[4] without quantitative
ranking. Often, forums hosting qualitative reviews
such as "laptop stopped working but helpdesk was
useless!" and "customer service was horrible" would
beneﬁt from an algorithm predicting rating from re-
views to summarize user sentiment quantitatively.
Furthermore, prediction algorithms allow rating and
review websites gather additional rating information
by predicting from external enthusiast forums where
reviews are more descriptive.

Our goal was to apply machine learning techniques
to Yelp’s published dataset to accurately predict rat-
ings from reviews. An input of a string of words
into our Multinomial Naive Bayes algorithm would
output a predicted rating between 1 and 5 describ-
ing the customer’s sentiment regarding a business.
While this has previously been attempted[5][6], most
approaches do not focus on training data segmen-
tation. We believed that segmenting training data
by business sector (e.g. restaurants) allows more
accurate predictions because feature sets become
less "diluted" or noisy and words gain meaningful
value when given a focused context. For example, an
auto mechanic business customer may be "extremely
satisﬁed with the new cold air intake system!" while
a restaurant customer may say that "the food took a
while and came out cold!" Taken together, if feature

∗These authors contributed equally to this work

size was ﬁxed, the word "system" would offer as
little predictive value in ranking restaurants as "food"
would for the auto business. Furthermore, "cold" is
sentimentally negative in the restaurant review, yet
positive in the auto business review. Training data
segmentation aims to reduce such uninformative oc-
currences.

II. RELATED WORK

Interesting approaches involve looking up the quan-
tiﬁed sentiment of each word from SentiWordNet and
summing the positive/negative/neutral in a document
and ﬁtting to regression rather than a completely
probabilistic approach[12][11]. While I think this is
the best method, the disadvantages are being lim-
ited to characterized words. Groups using non-word
features such as part-of-speech is an approach we
have not considered[10]. A useful troubleshooting
approach is displaying the highest impact features
in a prediction[9]. The potential to use latent factors
through matrix factorization seemed to have poten-
tial, but others who tried it received lower MSE than
us[13]

III. DATASET AND FEATURES

A. Dataset

Data was

downloaded

from Yelp’s Dataset
Challenge[2] in JSON format containing 1.6 million
reviews and 61 thousand businesses. The dataset
was trimmed by a custom C# script to yield a dataset
containing entries with only metrics of
interest:
review text on the business and reviewer rating of
the business. This formed the dataset allReviews. To
create a dataset containing only reviews from one
business segment, we identiﬁed the top represented
business segment in allReviews and created a dataset
containing only entries from this segment. The top
represented segment was Restaurant (21,892 entries)
so all reviews on restaurants made up segReviews.
Thus, allReviews and its subset segReviews are the
two datasets that were used. Each dataset contained

8,000 entries as training set, 1,000 as validation set,
and 1,000 as test set. Set sizes were smaller than
available data due to hardware constraints during
optimization. Data was converted from the JSON
format to the form of
***Reviews_Ratings.txt:

that may collectively have sentimental value while
sentimentally-neutral individually. SelectKBest using
a Chi-squared distribution was used to select features
that were found to have a dependence on speciﬁc
labels. SelectKBest and Ngrams were implemented
through scikitlearn[8].

1 5
2 4
3 4
4 ...

***Reviews_Reviews.txt:

1 this vegas thai place can be tricky to find

it is off the strip hidden away in a
strip mall ...

2 yum yum thai iced coffee fried tofu starter

and ...

3 the commercial center mall is not the most
obvious choice for las vegas best thai
...

4 ...

B. Preliminary Dataset Processing

allReviews and segReviews were processed as fol-
lows. For each review entry, all characters were re-
moved except for whitespaces and English alphabet
characters. All characters were converted to low-
ercase. Each occurrence of a whitespace character
was replaced with one space character. All consecu-
tive space characters were replaced with one space
character. This preempts potential string parsing and
string comparing problems, improves compatibility
with potential dictionaries and third party algorithms
(e.g. stop word corpus, stemming algorithms), and
most importantly, normalizes and combines similar
features (e.g. don’t vs dont, Thai vs thai, delicious vs
delicious!).

C. Dataset Processing During Optimization

Stop words based on Natural Language Toolkit’s
(NLTK)[7] stop word corpus were removed to reduce
common neutral-sentiment features that provide little
predictive value. Stemming was introduced by ac-
cessing NLTK’s implementation of Porter’s stemming
algorithm, which allows for further normalization and
combination of similar features.

A. Multinomial Naive Bayes

IV. Methods

Like many learning algorithms, Multinomial Naive
Bayes ﬁrst attempts to derive parameters that max-
imize the joint likelihood of the training data, given
by
L(Φy=0, ..., Φy=p−1, Φj|y=0, ..., Φj|y=p) =

p(x(i), y(i))

m(cid:89)

i=1

(1)
where Φy=q = P (y = q), Φj|y=q = P (xj = 1|y = q), q is
a number in the range of classes, j is a number in the
range of features, m is the number of training exam-
ples, p is the number of classes, and (x(i), y(i)) are the
ith training example. Making the "naive" assumption
that given any y, all xi are mutually independent, the
solution to the maximization is

(cid:80)m
(cid:80)m

(cid:111)
j = 1 ∧ y(i) = 1
x(i)

1(cid:8)y(i) = q(cid:9)

(cid:110)
(cid:80)m
1(cid:8)y(i) = q(cid:9)

i=1

1

i=1

i=1

m

Φj|y=q =

Φy=q =

(2)

(3)

Once the parameters from equations 2 and 3 are
ﬁt from the training data, prediction of a given test
example is simply calculating argmaxq P (y = q|x) =
P (x=1|y=q)P (y=q)
, where q is the class with the highest
posterior probability of all classes and x is the feature
vector.

P (x)

B. Support Vector Classiﬁer

SVC attempts to deﬁne a line (or hyperplane in
higher dimensions) that separates classes in a plot
of the training set that maximizes the distance (also
known as the margin) between the line and the closest
training example of each class. This line is called
the decision boundary. If line is given by f (x) =
(wT x + b), it can be shown that the decision boundary
parameters can be set by

minγ,w,b

|w|2 + C

1
2

m(cid:88)

i=1

ξi

D. Feature Selection

with the constraint that

After preliminary processing, the baseline feature
size was 19,898 for allReviews and 18,102 for segRe-
views. Manual removal of features was attempted by
looking through the feature list. Ngrams were used to
increase the feature set to include groups of words

y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m
ξi ≥ 0, i = 1, ..., m

where (x(i), y(i)) are the ith training example, w and b
are parameters of the line, C and ξ are penalty terms

(4)

(5)
(6)

that penalize the objective function for each training
example on the wrong side of the decision boundary,
and m is the number of training examples. After the
optimization problem is solved, the decision boundary
is known and classiﬁcation can be made by determin-
ing which side the training example lies with respect
to the decision boundary. It turns out that the deci-
sion boundary depends only on the training examples
closest to the decision boundary (known as support
vectors), which makes training SVCs computationally
efﬁcient. Furthermore, it turns out that the solution
can be written as a linear combination of (cid:104)x(i), x(cid:105),
which is the form of a Kernel, allowing efﬁcient com-
putation for large feature sizes. Thus, features are of-
ten transformed into other feature spaces to achieve
better data separation. Here, we used a linear kernel.
For classiﬁcation with more than 2 classes, simply
classify one class versus the rest of the classes, for
each class. We applied scikitlearn’s implementation
of both algorithms. Immediately before running any
algorithm, the reviews were ﬁrst tokenized with the
exception of experiments with SelectKBest, where the
features are tokenized, then run through SelectKBest,
then run through the machine learning algorithm.

V. Discussion

A. Metrics

The main metric used was accuracy, given by the
number of validation or test examples correctly la-
beled divided by the total examples to be predicted.
A higher accuracy suggested that the predictor was
often able to correctly classify the reviews. While
cross validation was not done on validation and test
data, 10-fold cross validation was used in generat-
ing learning curves. In the ﬁnal comparison, mean
squared error (MSE) was used to determine the av-
erage difference between the predicted label and the
actual label: 1
n

(yi,pred − yi,actual)2.

(cid:80)n−1

0

B. Baselining

To construct an initial accuracy comparison be-
tween predictors trained on unsegmented and seg-
mented datasets, baseline prediction accuracies of
allReviews and segReviews were each determined by
training a Multinomial Naive Bayes (MNB) algorithm
on 6,000 training examples and testing on 1,000 test
examples, with a vocabulary of the 5,000 highest
frequency words. MNB was used because it was one
of the simplest and quickest to train classiﬁcation
algorithms available. While we started with MNB, we
later attempted SVC after optimizing our features.
The number of training examples and feature size
initially used were picked arbitrarily to ensure room
for optimization in either direction. This prevented

wasting time optimizing the model in the beginning
when many decisions were still in ﬂux.

Table I presents the data that will be discussed in
the rest of the discussion section. Attention drawn to
speciﬁc rows will be of the form "R#".

The baseline showed that segReviews (R2) had
higher accuracies than allReviews (R1) for the test
set (0.55 and 0.52 respectively), which supported our
hypothesis that segmented training examples yield
better predictors than unsegmented. However, lack
of difference in training errors (both 0.72) led us to
suspect that test set accuracy differences may have
been noise.

C. Stop Words and Stemming

Once a baseline was formed, it was more efﬁcient
to focus on improving only segReviews accuracy in-
stead of applying each subsequent change to both
segReviews and allReviews. Once performance on
segReviews was satisfactory, we returned to apply the
same optimizations to allReviews and compare accu-
racies again. Thus from here onwards, optimization
was against segReviews only. Stop words and stem-
ming is known to help with feature normalization and
reducing the possible total vocabulary by removing
words that are either neutral or common in all classes
(e.g. "the", "and", "was") and by replacing words
stemming from the same root with the root itself
(e.g. replacing "purchase", "purchased", "purchasing"
with "purchas", collapses 5 features into 1). After re-
moving stop words (R4), training accuracy increased
marginally to 0.73 while validation accuracy remained
constant at 0.53 as compared to validation baseline
(R3). Stemming (R5) decreased training accuracy to
0.72 and validation accuracy decreased to 0.52. A
plausible explanation for no signiﬁcant improvement
after stemming and removing stop words was that our
feature set was still too large for such optimizations to
make a difference; the top features may still contain
words that have little sentimental value. It is also
possible that the features used may contain few stop
words and words affected by stemming.

D. Learning Curve Analysis

We wanted a rational approach to improve our
model rather than attempting random optimization
pathways so we generated a learning curve with
scikitlearn to test for overﬁtting or underﬁtting. Plots
have points representing averages of 10-fold cross
validation, with shaded regions being standard de-
viation. As seen in Figure 1, training accuracy was
signiﬁcantly higher than cross validation accuracy,
indicative of high variance and overﬁtting. The high
number of features allowed the algorithm more de-
grees of
freedom to ﬁt the training data better,

Row Procedure
1
2
3
4
5
6
7
8
9
10
11
12
13
14

Baseline
Baseline
Baseline
A = Baseline + Stop Words
B = A + Stemming
C = B + FeatureSize=200
D = C + TrainingExamples=8000
E = D + Manual Stop Words
F = E + skb=500 + FeatureSize = 5000
G = F + Bigram + Trigrams (Optimal)
G (Optimal)
G (Optimal)
G (Optimal)
G (Optimal)

Dataset
allReviews
segReview
segReview
segReview
segReview
segReview
segReview
segReview
segReview
segReview
segReview
allReviews
segReviews
allReviews

Type
Test Data
Test Data
Validation
Validation
Validation
Validation
Validation
Validation
Validation
Validation
Test Data
Test Data
Test Data
Test Data

TABLE I

Validate/Test Accuracy
0.515
0.554
0.534
0.533
0.515
0.542
0.521
0.535
0.589
0.597
0.632
0.548
0.616
0.558

Algorithm

Train Accuracy MSE
0.721
0.724
0.724
0.732
0.722
0.544
0.535
0.559
0.628
0.635
0.635
0.582
0.709
0.672

0.869 MNB
0.61
MNB
0.799 MNB
0.777 MNB
0.774 MNB
1.151 MNB
1.12
MNB
1.009 MNB
0.685 MNB
0.667 MNB
0.522 MNB
0.833 MNB
SVC
0.563
0.864
SVC

The effect of optimization procedure on prediction accuracy and mean squared error based on dataset and algorithm used.

Fig. 1. Learning curve after stop word removal and stemming with
5,000 features shows high variance. Score is prediction accuracy.

Fig. 2.
with 200 features is optimal. Score is prediction accuracy

Learning curve after stop word removal and stemming

Fig. 3.
with 20 features shows high bias. Score is prediction accuracy

Learning curve after stop word removal and stemming

yielding signiﬁcantly higher training accuracy. To re-
duce overﬁtting, we attempt to reduce the number
of features. Figure 2 shows the learning curve with
200 features while Figure 3 shows underﬁtting when
feature size decreased to 20, where training accu-
racy decreased dramatically from a maximum of 0.82
to a maximum of 0.54, indicating high bias. After
empirical optimization, stop word removal and stem-
ming with 200 features was optimal (R6), yielding
lower training and higher validation accuracy (0.54
for both training and validation accuracy). Figure 2
shows no more overﬁtting or underﬁtting. The in-
crease in validation accuracy was encouraging and
the decrease in training accuracy was associated with
the correction from overﬁtting. With 6,000 training
examples and 200 features, this agrees with the idea
that the number of features should be signiﬁcantly
smaller than the number of training examples. The
convergence between training and cross validation
accuracy implies that the predictor generalizes well to
unseen data. Figure 2 shows slight upwards slope in

cross validation accuracy, suggesting that increasing
training examples may increase validation accuracy
further. Increasing training set size from 6,000 to
8,000 revealed clear convergence (plot not shown)
of training and cross validation accuracy, but de-
creased validation accuracy to 0.52 (R7), suggesting
that cross validation learning curve has plateaued
and no more optimization of features size or training
examples is necessary.

E. Feature Optimization

Starting with manual

feature optimization, 83
sentiment-neutral words (e.g. food, burger, dish, Thai,
chicken, Bellagio) were identiﬁed from the features
to add to the stop words list, with the next highest
frequency words replacing those removed. The goal
of this is similar to that of removing stop words
and the process (R8) increased test accuracy to
0.54, speaking to the value of customized stop word
lists. However, it is nontrivial to manually remove
all words we considered sentiment-neutral given the
large vocabulary. Furthermore, it is likely that using
the highest frequency features may shadow some
highly predictive mid-frequency features. This led us
to SelectKBest using the Chi-squared, which evalu-
ates the likelihood that a given feature is independent
of a label and selects features that are not inde-
pendent of a label. Chi-squared describes how likely
the appearance of a speciﬁc feature for a speciﬁc
label is due to random chance based on the training
features and labels. From 5,000 features, SelectKBest
selected 500 features whose appearance for a label
are unlikely to be due to random chance (and thus are
dependent on the label). This signiﬁcantly improved
validation accuracies to 0.59 after some optimiza-
tion (R9). Bigram and trigram features were used
to further increase the features SelectKBest could
choose from, increasing validation accuracy to 0.60
(R10). nGram helps because some features have little
correlation with labels independently, but more cor-
relation when grouped together, giving SelectKBest
more dependent features. Select features identiﬁed
by SelectKBest are: "pretti damn", "goooood", "lick
plate", "ﬂavorless", "oh well", "perfect", "realli tast",
"tasteless", "would highli", and "appoint".

F. Confusion Matrix Analysis

Figure 4 shows a confusion matrix for the optimal
state (R10). The matrix has percentages that add to
100 across rows. Adjacent misclassiﬁcations make
up a majority of errors, which is understandable
since the difference between 4 and a 5 is slight and
subjective. We are pleased to see the majority of
classiﬁcations in each row are correct (red in the

Fig. 4. Confusion matrix for fully optimized segReviews with MNB
in percentages for each row. Total sample size is 999. Calculated
by scikitlearn

diagonal). Troubling areas are actual = 2, predicted
= 4 and actual = 3 and predicted = 1 and 5.

G. Return to Baseline

Satisﬁed with the improvements, we compare test
data accuracies between allReviews and segReviews
after applying our optimizations to both and against
baseline. Baseline (before optimization), segReviews
(R2) was 0.03 more accurate than allReviews (R1)
(0.55 against 0.52, respectively), but after optimiza-
tion, segReviews (R11) was 0.08 more accurate than
allReviews (R12) (0.63 against 0.55, respectively).
Additionally, optimization (R11) increased segReviews
(R2) accuracy by 0.08 compared to baseline. With
SVC, after applying the MNB optimization, segRe-
views (R13) was 0.06 more accurate than allReviews
(R14) (0.62 against 0.56, respectively).

VI. Conclusion

We have shown that training data segmentation
has a signiﬁcant effect on prediction accuracy, espe-
cially if incremental upon other dataset optimizations.
Training only on Yelp’s restaurant data shows that
an accuracy improvement of 0.08 over unsegmented
training data when given the same optimization pro-
cedure with MNB. The highest performing algorithm
was MNB with NLTK stop word removal, Proter’s
stemming, manual stop word removal, 8,000 training
examples, bigram and trigrams, and Select 500 best
from 5,000 features. This was better than SVM likely
because the whole optimization process was done
with MNB. At optimal, our MNB MSE was 0.522, so
on average, our prediction was off less than one a
Yelp rating ( 0.71).

For future work,

implementing forward search,
backward search, and mutual
information calcula-
tions to select better features may yield better se-
lections than SelectKBest. Introducing weights to
tokenization through TF-IDF, with TF factor being
raw counts, binary, or log normalized (1+log(feature
frequency)) may help adjust for frequently appear-
ing, low impact words. Finally, Applying segmented
training sets to other business segments would help
conﬁrm our ﬁnding.

[11] Qu, L., Ifrim, G., Weikum, G. (2010, August). The bag-of-
opinions method for review rating prediction from sparse text
patterns. In Proceedings of the 23rd International Conference
on Computational Linguistics (pp. 913-921). Association for
Computational Linguistics.

[12] Siersdorfer, S., Chelaru, S., Nejdl, W., San Pedro, J. (2010,
April). How useful are your comments?: analyzing and pre-
dicting youtube comments and comment ratings. In Proceed-
ings of the 19th international conference on World wide web
(pp. 891-900). ACM.

[13] Feng, Y., Zhengli, S. Yelp User Rating Prediction.

http://cs229.stanford.edu/proj2014/Yifei%20Feng
,%20Zhengli%20Sun,%20Yelp%20User%20Rating%20
Prediction.pdf

References

[1] Yelp.

(n.d.).

Retrieved December

7,

2015,

from

http://www.yelp.com/

[2] Yelp Dataset Challenge. (n.d.). Retrieved December 7, 2015,

from http://www.yelp.com/dataset_challenge

[3] NotebookReview. (n.d.). Retrieved December 7, 2015, from

http://forum.notebookreview.com/

&

Prices

[4] Values
trieved
http://forums.edmunds.com/discussions/tagged/x/values-
prices-paid

Paid.
7,

December

(n.d.).

2015,

Re-
from

[5] Li, C., Zhang, J. (n.d.). Prediction of Yelp Review Star Rat-
ing using Sentiment Analysis. Retrieved December 7, 2015,
from http://cs229.stanford.edu/proj2014/Chen Li, Jin Zhang,
Prediction of Yelp Review Star Rating using Sentiment Anal-
ysis.pdf

[6] Xu, Y., Wu, X., & Wang, Q. (n.d.). Sentiment Analysis of
Yelpâ˘AŸs Ratings Based on Text Reviews. Retrieved Decem-
ber 7, 2015, from http://cs229.stanford.edu/proj2014/Yun Xu,
Xinhui Wu, Qinxia Wang, Sentiment Analysis of Yelp’s Ratings
Based on Text Reviews.pdf

[7] Bird, S., Klein, E., and Loper, E. Natural Language Processing

with Python. O’Reilly Media, 2009

[8] Pedregosa et al. Scikit-learn: Machine Learning in Python.

JMLR 12, pp. 2825-2830, 2011.

[9] Mooney, R. J., Bennett, P. N., Roy, L. (1998). Book recommend-
ing using text categorization with extracted information. In
Proc. Recommender Systems Papers from 1998 Workshop,
Technical Report WS-98-08.

[10] Gupta, N., Di Fabbrizio, G., Haffner, P. (2010, June). Cap-
turing the stars: predicting ratings for service and product
reviews. In Proceedings of the NAACL HLT 2010 Workshop on
Semantic Search (pp. 36-43). Association for Computational
Linguistics.

