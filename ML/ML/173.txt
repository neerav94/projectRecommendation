Machine learning for thyroid cancer diagnosis

Rajiv Krishnakumar
Department of Applied Physics, Stanford University, CA 94305 USA
Raghu Mahajan
Department of Physics, Stanford University, CA 94305 USA
Akash V. Maharaj
Department of Physics, Stanford University, CA 94305 USA

RAJK@STANFORD.EDU

RM89@STANFORD.EDU

AMAHARAJ@STANFORD.EDU

Abstract

We investigate the use of high throughput gene
expression data in the diagnosis of thyroid can-
cers. Using logistic regression and support vec-
tor machines (SVMs), we develop a classiﬁer
which gives similar performance (89% sensitiv-
ity and 80% speciﬁcity) to the currently best-
known classiﬁer, but uses signiﬁcantly fewer fea-
tures. We used two different techniques, princi-
pal components analysis and mutual information
score, to select features. The results do not de-
pend signiﬁcantly on which method is used for
feature selection.

1. Introduction and related work
High throughput gene expression data is now readily avail-
able for many diseases and has been used extensively to de-
velop classiﬁers to help physicians diagnose and treat these
diseases. Thyroid cancer is one of those diseases.
Currently, a technique known as ﬁne-needle aspiration
(FNA) is used to determine whether a thyroid nodule is ma-
lignant or benign. Even though over 95% of the total cases
end up being benign (Howlader et al., 2011), many more
are diagnosed as malignant or ‘indeterminate’, i.e. unclear
diagnosis, after performing FNA. Patients with indetermi-
nate diagnoses (as well as those with malignant diagnoses)
then undergo diagnostic surgery to remove the tumor or be-
nign thyroid lesion; only about 30% are subsequently found
to be malignant post operation (Welker & Orlov, 2003).
This imperfect system leads to many unnecessary surgical
operations, enhancing risk for the patient and an increase
in healthcare costs.

CS229 Final Project Report,
December 11, 2015

A recent study (Alexander et al., 2012) has shown the po-
tential for diagnosing thyroid cancer using gene expression
data. The data consists of 367 samples, each having about
O(104) features (genes) and each classiﬁed as malignant
or benign by post-operation inspection. Using support vec-
tor machines the investigators reported a new diagnostic
test based on a narrowed feature set consisting of O(102)
features. While the diagnostic test has a reasonable false
negative rate of 8%, it has a high (48%) false positive rate.
In addition, the use of 173 genes with just 265 patients ren-
ders any classiﬁer liable to overﬁtting. Thus, our principal
goal is to investigate novel machine learning approaches
for development of a classiﬁer that outperforms the current
state of the art, namely higher speciﬁcity while maintaining
or improving the current sensitivity, with the use of fewer
features.
In Sec. 2 we discuss the nature of the data. In Sec 3 we dis-
cuss feature selection based on mutual information scores,
and also via principal components analysis.
In Sec. 4
we implement logistic regression and Support Vector Ma-
chines with linear kernels. We discuss the tuning of pa-
rameters such as relative weights assigned to the benign
and malignant samples, and varying (cid:96)1 regularizations for
the SVM. We end with a summary of our major results in
Sec. 5 and discussion of future work in Sec. 6.

2. Gene expression data
The dataset which forms the basis of this project was ﬁrst
employed by (Alexander et al., 2012) in a study on the use
of gene expression data for pre-operative thyroid cancer di-
agnosis. A full description of the clinical procedures em-
ployed in acquiring the gene expression data is beyond the
scope of this work; interested readers can consult (Alexan-
der et al., 2012). Basic attributes of the dataset (OnlineRef,
2012) are summarized in Table 1. The relevant part of
dataset consists of m = 265 patients (or more callously,

Machine learning for thyroid cancer diagnosis

Rajiv Krishnakumar
Department of Applied Physics, Stanford University, CA 94305 USA
Raghu Mahajan
Department of Physics, Stanford University, CA 94305 USA
Akash V. Maharaj
Department of Physics, Stanford University, CA 94305 USA

RAJK@STANFORD.EDU

RM89@STANFORD.EDU

AMAHARAJ@STANFORD.EDU

Abstract

We investigate the use of high throughput gene
expression data in the diagnosis of thyroid can-
cers. Using logistic regression and support vec-
tor machines (SVMs), we develop a classiﬁer
which gives similar performance (89% sensitiv-
ity and 80% speciﬁcity) to the currently best-
known classiﬁer, but uses signiﬁcantly fewer fea-
tures. We used two different techniques, princi-
pal components analysis and mutual information
score, to select features. The results do not de-
pend signiﬁcantly on which method is used for
feature selection.

1. Introduction and related work
High throughput gene expression data is now readily avail-
able for many diseases and has been used extensively to de-
velop classiﬁers to help physicians diagnose and treat these
diseases. Thyroid cancer is one of those diseases.
Currently, a technique known as ﬁne-needle aspiration
(FNA) is used to determine whether a thyroid nodule is ma-
lignant or benign. Even though over 95% of the total cases
end up being benign (Howlader et al., 2011), many more
are diagnosed as malignant or ‘indeterminate’, i.e. unclear
diagnosis, after performing FNA. Patients with indetermi-
nate diagnoses (as well as those with malignant diagnoses)
then undergo diagnostic surgery to remove the tumor or be-
nign thyroid lesion; only about 30% are subsequently found
to be malignant post operation (Welker & Orlov, 2003).
This imperfect system leads to many unnecessary surgical
operations, enhancing risk for the patient and an increase
in healthcare costs.

CS229 Final Project Report,
December 11, 2015

A recent study (Alexander et al., 2012) has shown the po-
tential for diagnosing thyroid cancer using gene expression
data. The data consists of 367 samples, each having about
O(104) features (genes) and each classiﬁed as malignant
or benign by post-operation inspection. Using support vec-
tor machines the investigators reported a new diagnostic
test based on a narrowed feature set consisting of O(102)
features. While the diagnostic test has a reasonable false
negative rate of 8%, it has a high (48%) false positive rate.
In addition, the use of 173 genes with just 265 patients ren-
ders any classiﬁer liable to overﬁtting. Thus, our principal
goal is to investigate novel machine learning approaches
for development of a classiﬁer that outperforms the current
state of the art, namely higher speciﬁcity while maintaining
or improving the current sensitivity, with the use of fewer
features.
In Sec. 2 we discuss the nature of the data. In Sec 3 we dis-
cuss feature selection based on mutual information scores,
and also via principal components analysis.
In Sec. 4
we implement logistic regression and Support Vector Ma-
chines with linear kernels. We discuss the tuning of pa-
rameters such as relative weights assigned to the benign
and malignant samples, and varying (cid:96)1 regularizations for
the SVM. We end with a summary of our major results in
Sec. 5 and discussion of future work in Sec. 6.

2. Gene expression data
The dataset which forms the basis of this project was ﬁrst
employed by (Alexander et al., 2012) in a study on the use
of gene expression data for pre-operative thyroid cancer di-
agnosis. A full description of the clinical procedures em-
ployed in acquiring the gene expression data is beyond the
scope of this work; interested readers can consult (Alexan-
der et al., 2012). Basic attributes of the dataset (OnlineRef,
2012) are summarized in Table 1. The relevant part of
dataset consists of m = 265 patients (or more callously,

Machine learning for thyroid cancer diagnosis

Table 1. Key attributes of the dataset.

Number of patients in dataset
Result of biopsy (cytology)

Benign
Malignant
Indeterminate
Post operative diagnosis of
indeterminate tumors (ground truths)

Benign
Malignant

367

55
47
265

180
85

Number of features (gene expressions)

Raw dataset
Pre-normalized and selected

∼ 25000

173

‘training examples’) with indeterminate biopsy results, of
which the ‘ground truth’ classiﬁcations of tumors are 85
malignant and 180 benign. A further m(cid:48) = 102 patients
whose biopsies were not indeterminate (47 benign, 55 ma-
lignant) are also included in the primary dataset and these
will later form our validation set1. Each training example
consists of n = 173 features, corresponding to a select set
of gene expressions obtained from the biopsy and subse-
quent microarray assay of a single thyroid nodule.

2.1. Pre-processing of data set

As part of the publicly available dataset (OnlineRef, 2012),
we were able to obtain the normalized gene expression data
for 173 genes for all 367 patients. These 173 were chosen
by the authors of (Alexander et al., 2012), based on a se-
quential procedure involving ‘Limma’ analysis and is de-
tailed in the supplemental material of their paper. We used
only this subset of features in our analysis.
We further normalize each gene expression to zero mean
and unit standard deviation. As mentioned previously, the
relative intensities of different genes is likely to be mean-
ingless (an artifact of the microarray procedure), and so this
choice of zero mean and unit standard deviation constitutes
a weaker modeling assumption than otherwise. Figure 1
provides a color map of these n = 173 features for the
m = 265 patients in our dataset. We note here that an at-
tempt at quantile normalization severely decreased the per-
formance of our classiﬁers, so we have not employed this
additional pre-processing step.

1The fact that biopsy results are based on the personal opin-
ion of evaluators suggests that there is in principle no profound
genetic difference between indeterminate vs.
‘pre-determined’
cytology diagnoses. This is our justiﬁcation for using the ‘pre-
determined’ samples as a validation set.

Figure 1. Color map visualization of gene expression data for a re-
duced feature set of 173 features. Because relative magnitudes of
different genes are (in principle) meaningless, each feature (gene)
has been normalized to zero mean and a standard deviation of
1. The black line at patient number 180 separates benign (above
line) from malignant samples (below line).

3. Feature Selection
With 173 features and 265 samples, smart feature selection
is crucial to avoid overﬁtting. We have adopted two com-
plementary approaches in this work: Mutual Information
(MI) and Principal Components Analysis (PCA). Happily,
as we will see in subsequent sections, the predictive power
of the resulting classiﬁer is similar regardless of whether
we use the mutual information statistic or PCA to select
the subset of genes.

3.1. Mutual Information

Our ﬁrst method of feature selection involved computation
of mutual information scores. This has the advantage of
hand picking the the most informative genes, opening the
possibility of building a classiﬁer which uses a few select
genes that can easily be sequenced in clinical tests. We
computed the Mutual Information score M I(xj, y) of each
gene xi deﬁned as

(cid:88)

(cid:88)

xj

y

M I(xj, y) =

p(xj, y) log

p(xj, y)
p(xj)p(y)

(1)

where p(xi, y) is the joint distribution of gene xj and diag-
nosis y, and we binned the gene expression levels into 43
bins to make xj a discrete variable. The names and TCIDs
(taken from the supplemental material in (Alexander et al.,
2012)) of the top ten genes are listed in Table 2. A his-
togram is shown in Figure 2.

20406080100120140160Gene/Feature50100150200250Patient/Sample-4-2024681012Machine learning for thyroid cancer diagnosis

Rajiv Krishnakumar
Department of Applied Physics, Stanford University, CA 94305 USA
Raghu Mahajan
Department of Physics, Stanford University, CA 94305 USA
Akash V. Maharaj
Department of Physics, Stanford University, CA 94305 USA

RAJK@STANFORD.EDU

RM89@STANFORD.EDU

AMAHARAJ@STANFORD.EDU

Abstract

We investigate the use of high throughput gene
expression data in the diagnosis of thyroid can-
cers. Using logistic regression and support vec-
tor machines (SVMs), we develop a classiﬁer
which gives similar performance (89% sensitiv-
ity and 80% speciﬁcity) to the currently best-
known classiﬁer, but uses signiﬁcantly fewer fea-
tures. We used two different techniques, princi-
pal components analysis and mutual information
score, to select features. The results do not de-
pend signiﬁcantly on which method is used for
feature selection.

1. Introduction and related work
High throughput gene expression data is now readily avail-
able for many diseases and has been used extensively to de-
velop classiﬁers to help physicians diagnose and treat these
diseases. Thyroid cancer is one of those diseases.
Currently, a technique known as ﬁne-needle aspiration
(FNA) is used to determine whether a thyroid nodule is ma-
lignant or benign. Even though over 95% of the total cases
end up being benign (Howlader et al., 2011), many more
are diagnosed as malignant or ‘indeterminate’, i.e. unclear
diagnosis, after performing FNA. Patients with indetermi-
nate diagnoses (as well as those with malignant diagnoses)
then undergo diagnostic surgery to remove the tumor or be-
nign thyroid lesion; only about 30% are subsequently found
to be malignant post operation (Welker & Orlov, 2003).
This imperfect system leads to many unnecessary surgical
operations, enhancing risk for the patient and an increase
in healthcare costs.

CS229 Final Project Report,
December 11, 2015

A recent study (Alexander et al., 2012) has shown the po-
tential for diagnosing thyroid cancer using gene expression
data. The data consists of 367 samples, each having about
O(104) features (genes) and each classiﬁed as malignant
or benign by post-operation inspection. Using support vec-
tor machines the investigators reported a new diagnostic
test based on a narrowed feature set consisting of O(102)
features. While the diagnostic test has a reasonable false
negative rate of 8%, it has a high (48%) false positive rate.
In addition, the use of 173 genes with just 265 patients ren-
ders any classiﬁer liable to overﬁtting. Thus, our principal
goal is to investigate novel machine learning approaches
for development of a classiﬁer that outperforms the current
state of the art, namely higher speciﬁcity while maintaining
or improving the current sensitivity, with the use of fewer
features.
In Sec. 2 we discuss the nature of the data. In Sec 3 we dis-
cuss feature selection based on mutual information scores,
and also via principal components analysis.
In Sec. 4
we implement logistic regression and Support Vector Ma-
chines with linear kernels. We discuss the tuning of pa-
rameters such as relative weights assigned to the benign
and malignant samples, and varying (cid:96)1 regularizations for
the SVM. We end with a summary of our major results in
Sec. 5 and discussion of future work in Sec. 6.

2. Gene expression data
The dataset which forms the basis of this project was ﬁrst
employed by (Alexander et al., 2012) in a study on the use
of gene expression data for pre-operative thyroid cancer di-
agnosis. A full description of the clinical procedures em-
ployed in acquiring the gene expression data is beyond the
scope of this work; interested readers can consult (Alexan-
der et al., 2012). Basic attributes of the dataset (OnlineRef,
2012) are summarized in Table 1. The relevant part of
dataset consists of m = 265 patients (or more callously,

Machine learning for thyroid cancer diagnosis

Table 1. Key attributes of the dataset.

Number of patients in dataset
Result of biopsy (cytology)

Benign
Malignant
Indeterminate
Post operative diagnosis of
indeterminate tumors (ground truths)

Benign
Malignant

367

55
47
265

180
85

Number of features (gene expressions)

Raw dataset
Pre-normalized and selected

∼ 25000

173

‘training examples’) with indeterminate biopsy results, of
which the ‘ground truth’ classiﬁcations of tumors are 85
malignant and 180 benign. A further m(cid:48) = 102 patients
whose biopsies were not indeterminate (47 benign, 55 ma-
lignant) are also included in the primary dataset and these
will later form our validation set1. Each training example
consists of n = 173 features, corresponding to a select set
of gene expressions obtained from the biopsy and subse-
quent microarray assay of a single thyroid nodule.

2.1. Pre-processing of data set

As part of the publicly available dataset (OnlineRef, 2012),
we were able to obtain the normalized gene expression data
for 173 genes for all 367 patients. These 173 were chosen
by the authors of (Alexander et al., 2012), based on a se-
quential procedure involving ‘Limma’ analysis and is de-
tailed in the supplemental material of their paper. We used
only this subset of features in our analysis.
We further normalize each gene expression to zero mean
and unit standard deviation. As mentioned previously, the
relative intensities of different genes is likely to be mean-
ingless (an artifact of the microarray procedure), and so this
choice of zero mean and unit standard deviation constitutes
a weaker modeling assumption than otherwise. Figure 1
provides a color map of these n = 173 features for the
m = 265 patients in our dataset. We note here that an at-
tempt at quantile normalization severely decreased the per-
formance of our classiﬁers, so we have not employed this
additional pre-processing step.

1The fact that biopsy results are based on the personal opin-
ion of evaluators suggests that there is in principle no profound
genetic difference between indeterminate vs.
‘pre-determined’
cytology diagnoses. This is our justiﬁcation for using the ‘pre-
determined’ samples as a validation set.

Figure 1. Color map visualization of gene expression data for a re-
duced feature set of 173 features. Because relative magnitudes of
different genes are (in principle) meaningless, each feature (gene)
has been normalized to zero mean and a standard deviation of
1. The black line at patient number 180 separates benign (above
line) from malignant samples (below line).

3. Feature Selection
With 173 features and 265 samples, smart feature selection
is crucial to avoid overﬁtting. We have adopted two com-
plementary approaches in this work: Mutual Information
(MI) and Principal Components Analysis (PCA). Happily,
as we will see in subsequent sections, the predictive power
of the resulting classiﬁer is similar regardless of whether
we use the mutual information statistic or PCA to select
the subset of genes.

3.1. Mutual Information

Our ﬁrst method of feature selection involved computation
of mutual information scores. This has the advantage of
hand picking the the most informative genes, opening the
possibility of building a classiﬁer which uses a few select
genes that can easily be sequenced in clinical tests. We
computed the Mutual Information score M I(xj, y) of each
gene xi deﬁned as

(cid:88)

(cid:88)

xj

y

M I(xj, y) =

p(xj, y) log

p(xj, y)
p(xj)p(y)

(1)

where p(xi, y) is the joint distribution of gene xj and diag-
nosis y, and we binned the gene expression levels into 43
bins to make xj a discrete variable. The names and TCIDs
(taken from the supplemental material in (Alexander et al.,
2012)) of the top ten genes are listed in Table 2. A his-
togram is shown in Figure 2.

20406080100120140160Gene/Feature50100150200250Patient/Sample-4-2024681012Machine learning for thyroid cancer diagnosis

MI Rank Gene names

LIPH
MDK
PROS1
LRP1B

Gene Description
lipase, member H

TCID
2708855
3329343 midkine (neurite growth-promoting factor 2)
2685304
2578790

protein S (alpha)
low density lipoprotein receptor-related protein 1B

1.
2.
3.
4.

5.

MAPK6
PKHD1L1

3111561 mitogen-activated protein kinase 6

polycystic kidney and hepatic disease 1 (autosomal recessive)-like 1

GABRB2
CLDN16

6.
7.
DPP6
8.
9.
TIMP1
10. MPPED2

gamma-aminobutyric acid (GABA) A receptor, beta 2
claudin 16
dipeptidyl-peptidase 6

2884845
2657808
3032647
3976341 TIMP metallopeptidase inhibitor 1
367673 metallophosphoesterase domain containing 2

Table 2. List of the top ten highest correlated genes ranked via
their Mutual Information (MI) score.

3.2. Principal Components Analysis

the covariance matrix ˆΣ = (cid:80)m

To have more conﬁdence in our feature selection, we also
did feature selection using a completely different method:
principal components analysis (PCA). This method com-
plements MI by selecting linear subspace of all n = 173
genes with the highest variances (and hence largest amount
of ‘information’). In PCA we compute the eigenvectors of
i=1 x(i)x(i)T where x(i) ∈
Rn is a vector containing all genes of patient i. Selecting
the top k principal components corresponds to transform-
ing each x(i) by a matrix W = (u1, u2, . . . , uk) where
u1 ∈ Rn is the principal eigenvector of ˆΣ etc.. A histogram
of the eigenvalues of ˆΣ is shown in Figure 2.

4. Models
4.1. Logistic Regression
Given the binary classiﬁcation problem with inputs X ∈
Rn a simple ﬁrst classiﬁcation algorithm is logistic regres-
sion (Dobson & Barnett, 2008). Logistic regression uses
the function

hθ(x) =

1

1 + e−θT x

(2)

0 or 1 (i.e. benign or malignant respectively). Here, θ ∈
Rn+1 is a vector of coefﬁcients which can be determined by
maximum likelihood estimation. Although hθ(x) takes on
continuous values between 0 and 1, the algorithm sets the
prediction to 1 if hθ(x) > 0.5 and 0 otherwise. Given the
training examples x(i), and their corresponding outcomes
y(i). We can write down the likelihood

m(cid:89)
m(cid:89)

i=1

L(θ) =

=

w(i)p(y(i)|x(i); θ)

w(i) (hθ(x))y(i)

(1 − hθ(x))1−y(i)

,

(3)

(4)

i=1

where w(i) is a weight which is dependent on whether the
sample was malignant or benign. The (log) likelihood can
now be maximized using gradient ascent. The weights al-
low us to impose a higher penalty when predicting a ma-
lignant sample incorrectly compared to when predicting a
benign sample incorrectly.
We implement logistic regression using MATLAB’s multi-
nomial logistic regression function, mnrfit. To assess the
success of this algorithm, we employ hold k-fold cross vali-
dation, with k = 10. In Fig. 3 we plot the training error and
empirical test error as the number of features is varied. We
trained different classiﬁers by tweaking the number of fea-
tures kept and also the relative weights (1:4 for benign vs.
malignant samples) assigned to the samples. It is important
to correctly diagnose the patients who have a malignant tu-
mor, so we impose a big penalty for misclassifying the ma-
lignant samples. We do this for features selected both using
mutual information and PCA.

to determine whether a sample of with a vector of features
x ∈ Rn+1 (in our case the n different gene expressions,
with an extra constant feature x0 = 1) has an outcome of

Figure 2. Histograms from MI (left) and PCA (right), showing the
number of genes (principal components) with a given mutual in-
formation score (eigenvalue). The features on the right side of
these histograms are the most important. Note the logarithmic
scale for PCA rankings.

Figure 3. Learning curves for logistic regression, unweighted (up-
per row), and weighted 1:4 benign:malignant samples (lower
row). MI on left column and PCA on right column.

00.050.10.150.2Mutual Information05101520Number of Genes-6-4-2024Log(λ)051015Number of Eigenvalues010203040506070Number of Features0.10.150.20.25ErrorTest ErrorTraining Error010203040506070Number of Features0.10.150.20.25ErrorTest ErrorTraining Error010203040506070Number of Features0.10.150.20.250.30.35ErrorTest ErrorTraining Error010203040506070Number of Features0.050.10.150.20.250.30.350.4ErrorTest ErrorTraining ErrorMachine learning for thyroid cancer diagnosis

Rajiv Krishnakumar
Department of Applied Physics, Stanford University, CA 94305 USA
Raghu Mahajan
Department of Physics, Stanford University, CA 94305 USA
Akash V. Maharaj
Department of Physics, Stanford University, CA 94305 USA

RAJK@STANFORD.EDU

RM89@STANFORD.EDU

AMAHARAJ@STANFORD.EDU

Abstract

We investigate the use of high throughput gene
expression data in the diagnosis of thyroid can-
cers. Using logistic regression and support vec-
tor machines (SVMs), we develop a classiﬁer
which gives similar performance (89% sensitiv-
ity and 80% speciﬁcity) to the currently best-
known classiﬁer, but uses signiﬁcantly fewer fea-
tures. We used two different techniques, princi-
pal components analysis and mutual information
score, to select features. The results do not de-
pend signiﬁcantly on which method is used for
feature selection.

1. Introduction and related work
High throughput gene expression data is now readily avail-
able for many diseases and has been used extensively to de-
velop classiﬁers to help physicians diagnose and treat these
diseases. Thyroid cancer is one of those diseases.
Currently, a technique known as ﬁne-needle aspiration
(FNA) is used to determine whether a thyroid nodule is ma-
lignant or benign. Even though over 95% of the total cases
end up being benign (Howlader et al., 2011), many more
are diagnosed as malignant or ‘indeterminate’, i.e. unclear
diagnosis, after performing FNA. Patients with indetermi-
nate diagnoses (as well as those with malignant diagnoses)
then undergo diagnostic surgery to remove the tumor or be-
nign thyroid lesion; only about 30% are subsequently found
to be malignant post operation (Welker & Orlov, 2003).
This imperfect system leads to many unnecessary surgical
operations, enhancing risk for the patient and an increase
in healthcare costs.

CS229 Final Project Report,
December 11, 2015

A recent study (Alexander et al., 2012) has shown the po-
tential for diagnosing thyroid cancer using gene expression
data. The data consists of 367 samples, each having about
O(104) features (genes) and each classiﬁed as malignant
or benign by post-operation inspection. Using support vec-
tor machines the investigators reported a new diagnostic
test based on a narrowed feature set consisting of O(102)
features. While the diagnostic test has a reasonable false
negative rate of 8%, it has a high (48%) false positive rate.
In addition, the use of 173 genes with just 265 patients ren-
ders any classiﬁer liable to overﬁtting. Thus, our principal
goal is to investigate novel machine learning approaches
for development of a classiﬁer that outperforms the current
state of the art, namely higher speciﬁcity while maintaining
or improving the current sensitivity, with the use of fewer
features.
In Sec. 2 we discuss the nature of the data. In Sec 3 we dis-
cuss feature selection based on mutual information scores,
and also via principal components analysis.
In Sec. 4
we implement logistic regression and Support Vector Ma-
chines with linear kernels. We discuss the tuning of pa-
rameters such as relative weights assigned to the benign
and malignant samples, and varying (cid:96)1 regularizations for
the SVM. We end with a summary of our major results in
Sec. 5 and discussion of future work in Sec. 6.

2. Gene expression data
The dataset which forms the basis of this project was ﬁrst
employed by (Alexander et al., 2012) in a study on the use
of gene expression data for pre-operative thyroid cancer di-
agnosis. A full description of the clinical procedures em-
ployed in acquiring the gene expression data is beyond the
scope of this work; interested readers can consult (Alexan-
der et al., 2012). Basic attributes of the dataset (OnlineRef,
2012) are summarized in Table 1. The relevant part of
dataset consists of m = 265 patients (or more callously,

Machine learning for thyroid cancer diagnosis

Table 1. Key attributes of the dataset.

Number of patients in dataset
Result of biopsy (cytology)

Benign
Malignant
Indeterminate
Post operative diagnosis of
indeterminate tumors (ground truths)

Benign
Malignant

367

55
47
265

180
85

Number of features (gene expressions)

Raw dataset
Pre-normalized and selected

∼ 25000

173

‘training examples’) with indeterminate biopsy results, of
which the ‘ground truth’ classiﬁcations of tumors are 85
malignant and 180 benign. A further m(cid:48) = 102 patients
whose biopsies were not indeterminate (47 benign, 55 ma-
lignant) are also included in the primary dataset and these
will later form our validation set1. Each training example
consists of n = 173 features, corresponding to a select set
of gene expressions obtained from the biopsy and subse-
quent microarray assay of a single thyroid nodule.

2.1. Pre-processing of data set

As part of the publicly available dataset (OnlineRef, 2012),
we were able to obtain the normalized gene expression data
for 173 genes for all 367 patients. These 173 were chosen
by the authors of (Alexander et al., 2012), based on a se-
quential procedure involving ‘Limma’ analysis and is de-
tailed in the supplemental material of their paper. We used
only this subset of features in our analysis.
We further normalize each gene expression to zero mean
and unit standard deviation. As mentioned previously, the
relative intensities of different genes is likely to be mean-
ingless (an artifact of the microarray procedure), and so this
choice of zero mean and unit standard deviation constitutes
a weaker modeling assumption than otherwise. Figure 1
provides a color map of these n = 173 features for the
m = 265 patients in our dataset. We note here that an at-
tempt at quantile normalization severely decreased the per-
formance of our classiﬁers, so we have not employed this
additional pre-processing step.

1The fact that biopsy results are based on the personal opin-
ion of evaluators suggests that there is in principle no profound
genetic difference between indeterminate vs.
‘pre-determined’
cytology diagnoses. This is our justiﬁcation for using the ‘pre-
determined’ samples as a validation set.

Figure 1. Color map visualization of gene expression data for a re-
duced feature set of 173 features. Because relative magnitudes of
different genes are (in principle) meaningless, each feature (gene)
has been normalized to zero mean and a standard deviation of
1. The black line at patient number 180 separates benign (above
line) from malignant samples (below line).

3. Feature Selection
With 173 features and 265 samples, smart feature selection
is crucial to avoid overﬁtting. We have adopted two com-
plementary approaches in this work: Mutual Information
(MI) and Principal Components Analysis (PCA). Happily,
as we will see in subsequent sections, the predictive power
of the resulting classiﬁer is similar regardless of whether
we use the mutual information statistic or PCA to select
the subset of genes.

3.1. Mutual Information

Our ﬁrst method of feature selection involved computation
of mutual information scores. This has the advantage of
hand picking the the most informative genes, opening the
possibility of building a classiﬁer which uses a few select
genes that can easily be sequenced in clinical tests. We
computed the Mutual Information score M I(xj, y) of each
gene xi deﬁned as

(cid:88)

(cid:88)

xj

y

M I(xj, y) =

p(xj, y) log

p(xj, y)
p(xj)p(y)

(1)

where p(xi, y) is the joint distribution of gene xj and diag-
nosis y, and we binned the gene expression levels into 43
bins to make xj a discrete variable. The names and TCIDs
(taken from the supplemental material in (Alexander et al.,
2012)) of the top ten genes are listed in Table 2. A his-
togram is shown in Figure 2.

20406080100120140160Gene/Feature50100150200250Patient/Sample-4-2024681012Machine learning for thyroid cancer diagnosis

MI Rank Gene names

LIPH
MDK
PROS1
LRP1B

Gene Description
lipase, member H

TCID
2708855
3329343 midkine (neurite growth-promoting factor 2)
2685304
2578790

protein S (alpha)
low density lipoprotein receptor-related protein 1B

1.
2.
3.
4.

5.

MAPK6
PKHD1L1

3111561 mitogen-activated protein kinase 6

polycystic kidney and hepatic disease 1 (autosomal recessive)-like 1

GABRB2
CLDN16

6.
7.
DPP6
8.
9.
TIMP1
10. MPPED2

gamma-aminobutyric acid (GABA) A receptor, beta 2
claudin 16
dipeptidyl-peptidase 6

2884845
2657808
3032647
3976341 TIMP metallopeptidase inhibitor 1
367673 metallophosphoesterase domain containing 2

Table 2. List of the top ten highest correlated genes ranked via
their Mutual Information (MI) score.

3.2. Principal Components Analysis

the covariance matrix ˆΣ = (cid:80)m

To have more conﬁdence in our feature selection, we also
did feature selection using a completely different method:
principal components analysis (PCA). This method com-
plements MI by selecting linear subspace of all n = 173
genes with the highest variances (and hence largest amount
of ‘information’). In PCA we compute the eigenvectors of
i=1 x(i)x(i)T where x(i) ∈
Rn is a vector containing all genes of patient i. Selecting
the top k principal components corresponds to transform-
ing each x(i) by a matrix W = (u1, u2, . . . , uk) where
u1 ∈ Rn is the principal eigenvector of ˆΣ etc.. A histogram
of the eigenvalues of ˆΣ is shown in Figure 2.

4. Models
4.1. Logistic Regression
Given the binary classiﬁcation problem with inputs X ∈
Rn a simple ﬁrst classiﬁcation algorithm is logistic regres-
sion (Dobson & Barnett, 2008). Logistic regression uses
the function

hθ(x) =

1

1 + e−θT x

(2)

0 or 1 (i.e. benign or malignant respectively). Here, θ ∈
Rn+1 is a vector of coefﬁcients which can be determined by
maximum likelihood estimation. Although hθ(x) takes on
continuous values between 0 and 1, the algorithm sets the
prediction to 1 if hθ(x) > 0.5 and 0 otherwise. Given the
training examples x(i), and their corresponding outcomes
y(i). We can write down the likelihood

m(cid:89)
m(cid:89)

i=1

L(θ) =

=

w(i)p(y(i)|x(i); θ)

w(i) (hθ(x))y(i)

(1 − hθ(x))1−y(i)

,

(3)

(4)

i=1

where w(i) is a weight which is dependent on whether the
sample was malignant or benign. The (log) likelihood can
now be maximized using gradient ascent. The weights al-
low us to impose a higher penalty when predicting a ma-
lignant sample incorrectly compared to when predicting a
benign sample incorrectly.
We implement logistic regression using MATLAB’s multi-
nomial logistic regression function, mnrfit. To assess the
success of this algorithm, we employ hold k-fold cross vali-
dation, with k = 10. In Fig. 3 we plot the training error and
empirical test error as the number of features is varied. We
trained different classiﬁers by tweaking the number of fea-
tures kept and also the relative weights (1:4 for benign vs.
malignant samples) assigned to the samples. It is important
to correctly diagnose the patients who have a malignant tu-
mor, so we impose a big penalty for misclassifying the ma-
lignant samples. We do this for features selected both using
mutual information and PCA.

to determine whether a sample of with a vector of features
x ∈ Rn+1 (in our case the n different gene expressions,
with an extra constant feature x0 = 1) has an outcome of

Figure 2. Histograms from MI (left) and PCA (right), showing the
number of genes (principal components) with a given mutual in-
formation score (eigenvalue). The features on the right side of
these histograms are the most important. Note the logarithmic
scale for PCA rankings.

Figure 3. Learning curves for logistic regression, unweighted (up-
per row), and weighted 1:4 benign:malignant samples (lower
row). MI on left column and PCA on right column.

00.050.10.150.2Mutual Information05101520Number of Genes-6-4-2024Log(λ)051015Number of Eigenvalues010203040506070Number of Features0.10.150.20.25ErrorTest ErrorTraining Error010203040506070Number of Features0.10.150.20.25ErrorTest ErrorTraining Error010203040506070Number of Features0.10.150.20.250.30.35ErrorTest ErrorTraining Error010203040506070Number of Features0.050.10.150.20.250.30.350.4ErrorTest ErrorTraining ErrorMachine learning for thyroid cancer diagnosis

Figure 4. Learning curves for SVM, weighted, MI on left and
PCA on right, each with a (cid:96)1 parameter of C = 0.1. The weighted
decision boundary uses relative weighting of 1:4 (malignant sam-
ples are assigned a higher weight).

4.2. Support Vector Machines

The second learning algorithm we have implemented is a
Support Vector Machine (SVM), using a linear kernel with
(cid:96)1 regularization. Use of (cid:96)1 regularization is necessary be-
cause the data is (empirically) not strictly linearly separa-
ble.

SVMs work by ﬁnding the optimal hyperplane(cid:80)

j wjxj+b
that separates/classiﬁes the data. Here, b is a constant offset
and wj is a coefﬁcient for each gene/feature j in our data.
Solving for the optimal wj and b amounts to a constrained
convex optimization problem, where we must minimize

m(cid:88)

i=1

||w||2 + C

1
2

ξi

(5)

αi − m(cid:88)

m(cid:88)

i=1

w.r.t. w and b, subject to the constraints y(i)(wT x(i) + b) ≥
1 − ξi and ξi ≥ 0 ∀i (ξi are known as slack variables,
and C is the constraint parameter which penalizes incor-
rectly classiﬁed data). Solution of this problem is simpler
if we consider a dual optimization problem where a series
of standard manipulations (Bishop, 2006) leads to the du-
alized optimization problem:

y(i)y(j)αiαjx(i)T

x(j)

(6)

max

α

i,j=1

W (α) =

subject to the constraints 0 ≤ αi ≤ C and(cid:80)m

i=1 αiy(i) =
0. We implement the SVM with this linear kernel, using the
Matlab routine fitcsvm which proceeds via the Sequen-
tial Minimal Optimization (SMO) algorithm (Platt, 1998).
To estimate our errors, we again employ k-fold cross vali-
dation with k = 10 and impose a big penalty for misclas-
sifying the malignant samples. In Figure 4, we show the
learning curves for the SVM using a relative weighting of
1:4 for the benign vs. malignant samples and an (cid:96)1 parame-
ter of C = 0.1 (which we optimized for by considering C’s
over 4 orders of magnitude). It is clear that optimal perfor-
mance for both MI and PCA selected features is achieved
for fewer than 20 features kept.

Figure 5. Decision boundaries for SVM, using only the best two
genes selected using mutual information (top ﬁgure) and top two
principal components PCA (bottom ﬁgure). We used a relative
weighting of 1:4 (higher weights for malignant samples) and the
(cid:96)1 parameter C = 0.1. Data points are labelled by their pre- and
post-operative diagnosis (B = Benign, I = Indeterminate, M = Ma-
lignant), so for example (I-M) indicates a pre operative diagnosis
of indeterminate and a post operative diagnosis of Malignant. The
clustering of Malignant samples is obvious in both MI and PCA
variables. It is also clear that unweighted boundary achieves ex-
cellent classiﬁcation of benign samples (leading to low false pos-
itive errors for unweighted classiﬁers in Table 3).

While the learning curves of Fig. 4 suggest that ∼ 10 fea-
tures should be used in our ﬁnal classiﬁer, we have found
that a two feature scatter plot is an informative visualization
of the structure of our data demonstrating the lack of linear
separability which persists in higher dimensions. Plotted
in Figure 5 is the full data set (test and training sets, along
with the validation set), in addition to the decision bound-
aries obtained by keeping the top two features.

5. Final Results
After exploring the parameter space of the models de-
scribed in Section 4, we decided to work with a subset of 10
features, since its in that ballpark where the test and train-
ing errors seem comparable and small. Having used the
training curves to select these 10 features (both genes from
MI and then components from PCA), we then train on the

0204060Number of Features0.160.180.20.220.240.260.280.3ErrorTest ErrorTraining Error0204060Number of Features0.10.150.20.250.30.35ErrorTest ErrorTraining Error-1.5-1-0.500.511.522.5Gene 1: Lipase, Member H. (TCID: 2708855)-1.5-1-0.500.511.522.533.5Gene 2: Midkine. (TCID: 3329343)B - BI - BB - MI - MM - MWeighted BoundaryUnweighted Boundary-15-10-5051015Principal Component #1-8-6-4-202468Principal Component #2B - BI - BB - MI - MM - MWeighted BoundaryUnweighted BoundaryMachine learning for thyroid cancer diagnosis

Rajiv Krishnakumar
Department of Applied Physics, Stanford University, CA 94305 USA
Raghu Mahajan
Department of Physics, Stanford University, CA 94305 USA
Akash V. Maharaj
Department of Physics, Stanford University, CA 94305 USA

RAJK@STANFORD.EDU

RM89@STANFORD.EDU

AMAHARAJ@STANFORD.EDU

Abstract

We investigate the use of high throughput gene
expression data in the diagnosis of thyroid can-
cers. Using logistic regression and support vec-
tor machines (SVMs), we develop a classiﬁer
which gives similar performance (89% sensitiv-
ity and 80% speciﬁcity) to the currently best-
known classiﬁer, but uses signiﬁcantly fewer fea-
tures. We used two different techniques, princi-
pal components analysis and mutual information
score, to select features. The results do not de-
pend signiﬁcantly on which method is used for
feature selection.

1. Introduction and related work
High throughput gene expression data is now readily avail-
able for many diseases and has been used extensively to de-
velop classiﬁers to help physicians diagnose and treat these
diseases. Thyroid cancer is one of those diseases.
Currently, a technique known as ﬁne-needle aspiration
(FNA) is used to determine whether a thyroid nodule is ma-
lignant or benign. Even though over 95% of the total cases
end up being benign (Howlader et al., 2011), many more
are diagnosed as malignant or ‘indeterminate’, i.e. unclear
diagnosis, after performing FNA. Patients with indetermi-
nate diagnoses (as well as those with malignant diagnoses)
then undergo diagnostic surgery to remove the tumor or be-
nign thyroid lesion; only about 30% are subsequently found
to be malignant post operation (Welker & Orlov, 2003).
This imperfect system leads to many unnecessary surgical
operations, enhancing risk for the patient and an increase
in healthcare costs.

CS229 Final Project Report,
December 11, 2015

A recent study (Alexander et al., 2012) has shown the po-
tential for diagnosing thyroid cancer using gene expression
data. The data consists of 367 samples, each having about
O(104) features (genes) and each classiﬁed as malignant
or benign by post-operation inspection. Using support vec-
tor machines the investigators reported a new diagnostic
test based on a narrowed feature set consisting of O(102)
features. While the diagnostic test has a reasonable false
negative rate of 8%, it has a high (48%) false positive rate.
In addition, the use of 173 genes with just 265 patients ren-
ders any classiﬁer liable to overﬁtting. Thus, our principal
goal is to investigate novel machine learning approaches
for development of a classiﬁer that outperforms the current
state of the art, namely higher speciﬁcity while maintaining
or improving the current sensitivity, with the use of fewer
features.
In Sec. 2 we discuss the nature of the data. In Sec 3 we dis-
cuss feature selection based on mutual information scores,
and also via principal components analysis.
In Sec. 4
we implement logistic regression and Support Vector Ma-
chines with linear kernels. We discuss the tuning of pa-
rameters such as relative weights assigned to the benign
and malignant samples, and varying (cid:96)1 regularizations for
the SVM. We end with a summary of our major results in
Sec. 5 and discussion of future work in Sec. 6.

2. Gene expression data
The dataset which forms the basis of this project was ﬁrst
employed by (Alexander et al., 2012) in a study on the use
of gene expression data for pre-operative thyroid cancer di-
agnosis. A full description of the clinical procedures em-
ployed in acquiring the gene expression data is beyond the
scope of this work; interested readers can consult (Alexan-
der et al., 2012). Basic attributes of the dataset (OnlineRef,
2012) are summarized in Table 1. The relevant part of
dataset consists of m = 265 patients (or more callously,

Machine learning for thyroid cancer diagnosis

Table 1. Key attributes of the dataset.

Number of patients in dataset
Result of biopsy (cytology)

Benign
Malignant
Indeterminate
Post operative diagnosis of
indeterminate tumors (ground truths)

Benign
Malignant

367

55
47
265

180
85

Number of features (gene expressions)

Raw dataset
Pre-normalized and selected

∼ 25000

173

‘training examples’) with indeterminate biopsy results, of
which the ‘ground truth’ classiﬁcations of tumors are 85
malignant and 180 benign. A further m(cid:48) = 102 patients
whose biopsies were not indeterminate (47 benign, 55 ma-
lignant) are also included in the primary dataset and these
will later form our validation set1. Each training example
consists of n = 173 features, corresponding to a select set
of gene expressions obtained from the biopsy and subse-
quent microarray assay of a single thyroid nodule.

2.1. Pre-processing of data set

As part of the publicly available dataset (OnlineRef, 2012),
we were able to obtain the normalized gene expression data
for 173 genes for all 367 patients. These 173 were chosen
by the authors of (Alexander et al., 2012), based on a se-
quential procedure involving ‘Limma’ analysis and is de-
tailed in the supplemental material of their paper. We used
only this subset of features in our analysis.
We further normalize each gene expression to zero mean
and unit standard deviation. As mentioned previously, the
relative intensities of different genes is likely to be mean-
ingless (an artifact of the microarray procedure), and so this
choice of zero mean and unit standard deviation constitutes
a weaker modeling assumption than otherwise. Figure 1
provides a color map of these n = 173 features for the
m = 265 patients in our dataset. We note here that an at-
tempt at quantile normalization severely decreased the per-
formance of our classiﬁers, so we have not employed this
additional pre-processing step.

1The fact that biopsy results are based on the personal opin-
ion of evaluators suggests that there is in principle no profound
genetic difference between indeterminate vs.
‘pre-determined’
cytology diagnoses. This is our justiﬁcation for using the ‘pre-
determined’ samples as a validation set.

Figure 1. Color map visualization of gene expression data for a re-
duced feature set of 173 features. Because relative magnitudes of
different genes are (in principle) meaningless, each feature (gene)
has been normalized to zero mean and a standard deviation of
1. The black line at patient number 180 separates benign (above
line) from malignant samples (below line).

3. Feature Selection
With 173 features and 265 samples, smart feature selection
is crucial to avoid overﬁtting. We have adopted two com-
plementary approaches in this work: Mutual Information
(MI) and Principal Components Analysis (PCA). Happily,
as we will see in subsequent sections, the predictive power
of the resulting classiﬁer is similar regardless of whether
we use the mutual information statistic or PCA to select
the subset of genes.

3.1. Mutual Information

Our ﬁrst method of feature selection involved computation
of mutual information scores. This has the advantage of
hand picking the the most informative genes, opening the
possibility of building a classiﬁer which uses a few select
genes that can easily be sequenced in clinical tests. We
computed the Mutual Information score M I(xj, y) of each
gene xi deﬁned as

(cid:88)

(cid:88)

xj

y

M I(xj, y) =

p(xj, y) log

p(xj, y)
p(xj)p(y)

(1)

where p(xi, y) is the joint distribution of gene xj and diag-
nosis y, and we binned the gene expression levels into 43
bins to make xj a discrete variable. The names and TCIDs
(taken from the supplemental material in (Alexander et al.,
2012)) of the top ten genes are listed in Table 2. A his-
togram is shown in Figure 2.

20406080100120140160Gene/Feature50100150200250Patient/Sample-4-2024681012Machine learning for thyroid cancer diagnosis

MI Rank Gene names

LIPH
MDK
PROS1
LRP1B

Gene Description
lipase, member H

TCID
2708855
3329343 midkine (neurite growth-promoting factor 2)
2685304
2578790

protein S (alpha)
low density lipoprotein receptor-related protein 1B

1.
2.
3.
4.

5.

MAPK6
PKHD1L1

3111561 mitogen-activated protein kinase 6

polycystic kidney and hepatic disease 1 (autosomal recessive)-like 1

GABRB2
CLDN16

6.
7.
DPP6
8.
9.
TIMP1
10. MPPED2

gamma-aminobutyric acid (GABA) A receptor, beta 2
claudin 16
dipeptidyl-peptidase 6

2884845
2657808
3032647
3976341 TIMP metallopeptidase inhibitor 1
367673 metallophosphoesterase domain containing 2

Table 2. List of the top ten highest correlated genes ranked via
their Mutual Information (MI) score.

3.2. Principal Components Analysis

the covariance matrix ˆΣ = (cid:80)m

To have more conﬁdence in our feature selection, we also
did feature selection using a completely different method:
principal components analysis (PCA). This method com-
plements MI by selecting linear subspace of all n = 173
genes with the highest variances (and hence largest amount
of ‘information’). In PCA we compute the eigenvectors of
i=1 x(i)x(i)T where x(i) ∈
Rn is a vector containing all genes of patient i. Selecting
the top k principal components corresponds to transform-
ing each x(i) by a matrix W = (u1, u2, . . . , uk) where
u1 ∈ Rn is the principal eigenvector of ˆΣ etc.. A histogram
of the eigenvalues of ˆΣ is shown in Figure 2.

4. Models
4.1. Logistic Regression
Given the binary classiﬁcation problem with inputs X ∈
Rn a simple ﬁrst classiﬁcation algorithm is logistic regres-
sion (Dobson & Barnett, 2008). Logistic regression uses
the function

hθ(x) =

1

1 + e−θT x

(2)

0 or 1 (i.e. benign or malignant respectively). Here, θ ∈
Rn+1 is a vector of coefﬁcients which can be determined by
maximum likelihood estimation. Although hθ(x) takes on
continuous values between 0 and 1, the algorithm sets the
prediction to 1 if hθ(x) > 0.5 and 0 otherwise. Given the
training examples x(i), and their corresponding outcomes
y(i). We can write down the likelihood

m(cid:89)
m(cid:89)

i=1

L(θ) =

=

w(i)p(y(i)|x(i); θ)

w(i) (hθ(x))y(i)

(1 − hθ(x))1−y(i)

,

(3)

(4)

i=1

where w(i) is a weight which is dependent on whether the
sample was malignant or benign. The (log) likelihood can
now be maximized using gradient ascent. The weights al-
low us to impose a higher penalty when predicting a ma-
lignant sample incorrectly compared to when predicting a
benign sample incorrectly.
We implement logistic regression using MATLAB’s multi-
nomial logistic regression function, mnrfit. To assess the
success of this algorithm, we employ hold k-fold cross vali-
dation, with k = 10. In Fig. 3 we plot the training error and
empirical test error as the number of features is varied. We
trained different classiﬁers by tweaking the number of fea-
tures kept and also the relative weights (1:4 for benign vs.
malignant samples) assigned to the samples. It is important
to correctly diagnose the patients who have a malignant tu-
mor, so we impose a big penalty for misclassifying the ma-
lignant samples. We do this for features selected both using
mutual information and PCA.

to determine whether a sample of with a vector of features
x ∈ Rn+1 (in our case the n different gene expressions,
with an extra constant feature x0 = 1) has an outcome of

Figure 2. Histograms from MI (left) and PCA (right), showing the
number of genes (principal components) with a given mutual in-
formation score (eigenvalue). The features on the right side of
these histograms are the most important. Note the logarithmic
scale for PCA rankings.

Figure 3. Learning curves for logistic regression, unweighted (up-
per row), and weighted 1:4 benign:malignant samples (lower
row). MI on left column and PCA on right column.

00.050.10.150.2Mutual Information05101520Number of Genes-6-4-2024Log(λ)051015Number of Eigenvalues010203040506070Number of Features0.10.150.20.25ErrorTest ErrorTraining Error010203040506070Number of Features0.10.150.20.25ErrorTest ErrorTraining Error010203040506070Number of Features0.10.150.20.250.30.35ErrorTest ErrorTraining Error010203040506070Number of Features0.050.10.150.20.250.30.350.4ErrorTest ErrorTraining ErrorMachine learning for thyroid cancer diagnosis

Figure 4. Learning curves for SVM, weighted, MI on left and
PCA on right, each with a (cid:96)1 parameter of C = 0.1. The weighted
decision boundary uses relative weighting of 1:4 (malignant sam-
ples are assigned a higher weight).

4.2. Support Vector Machines

The second learning algorithm we have implemented is a
Support Vector Machine (SVM), using a linear kernel with
(cid:96)1 regularization. Use of (cid:96)1 regularization is necessary be-
cause the data is (empirically) not strictly linearly separa-
ble.

SVMs work by ﬁnding the optimal hyperplane(cid:80)

j wjxj+b
that separates/classiﬁes the data. Here, b is a constant offset
and wj is a coefﬁcient for each gene/feature j in our data.
Solving for the optimal wj and b amounts to a constrained
convex optimization problem, where we must minimize

m(cid:88)

i=1

||w||2 + C

1
2

ξi

(5)

αi − m(cid:88)

m(cid:88)

i=1

w.r.t. w and b, subject to the constraints y(i)(wT x(i) + b) ≥
1 − ξi and ξi ≥ 0 ∀i (ξi are known as slack variables,
and C is the constraint parameter which penalizes incor-
rectly classiﬁed data). Solution of this problem is simpler
if we consider a dual optimization problem where a series
of standard manipulations (Bishop, 2006) leads to the du-
alized optimization problem:

y(i)y(j)αiαjx(i)T

x(j)

(6)

max

α

i,j=1

W (α) =

subject to the constraints 0 ≤ αi ≤ C and(cid:80)m

i=1 αiy(i) =
0. We implement the SVM with this linear kernel, using the
Matlab routine fitcsvm which proceeds via the Sequen-
tial Minimal Optimization (SMO) algorithm (Platt, 1998).
To estimate our errors, we again employ k-fold cross vali-
dation with k = 10 and impose a big penalty for misclas-
sifying the malignant samples. In Figure 4, we show the
learning curves for the SVM using a relative weighting of
1:4 for the benign vs. malignant samples and an (cid:96)1 parame-
ter of C = 0.1 (which we optimized for by considering C’s
over 4 orders of magnitude). It is clear that optimal perfor-
mance for both MI and PCA selected features is achieved
for fewer than 20 features kept.

Figure 5. Decision boundaries for SVM, using only the best two
genes selected using mutual information (top ﬁgure) and top two
principal components PCA (bottom ﬁgure). We used a relative
weighting of 1:4 (higher weights for malignant samples) and the
(cid:96)1 parameter C = 0.1. Data points are labelled by their pre- and
post-operative diagnosis (B = Benign, I = Indeterminate, M = Ma-
lignant), so for example (I-M) indicates a pre operative diagnosis
of indeterminate and a post operative diagnosis of Malignant. The
clustering of Malignant samples is obvious in both MI and PCA
variables. It is also clear that unweighted boundary achieves ex-
cellent classiﬁcation of benign samples (leading to low false pos-
itive errors for unweighted classiﬁers in Table 3).

While the learning curves of Fig. 4 suggest that ∼ 10 fea-
tures should be used in our ﬁnal classiﬁer, we have found
that a two feature scatter plot is an informative visualization
of the structure of our data demonstrating the lack of linear
separability which persists in higher dimensions. Plotted
in Figure 5 is the full data set (test and training sets, along
with the validation set), in addition to the decision bound-
aries obtained by keeping the top two features.

5. Final Results
After exploring the parameter space of the models de-
scribed in Section 4, we decided to work with a subset of 10
features, since its in that ballpark where the test and train-
ing errors seem comparable and small. Having used the
training curves to select these 10 features (both genes from
MI and then components from PCA), we then train on the

0204060Number of Features0.160.180.20.220.240.260.280.3ErrorTest ErrorTraining Error0204060Number of Features0.10.150.20.250.30.35ErrorTest ErrorTraining Error-1.5-1-0.500.511.522.5Gene 1: Lipase, Member H. (TCID: 2708855)-1.5-1-0.500.511.522.533.5Gene 2: Midkine. (TCID: 3329343)B - BI - BB - MI - MM - MWeighted BoundaryUnweighted Boundary-15-10-5051015Principal Component #1-8-6-4-202468Principal Component #2B - BI - BB - MI - MM - MWeighted BoundaryUnweighted BoundaryMachine learning for thyroid cancer diagnosis

Table 3. False Positive and False negative rates as well as total errors for the various classiﬁers used in our study. Both Logistic regres-
sion and SVMs were trained on the top 10 features obtained by feature selection using Mutual Information (MI) scores and Principal
Components Analysis (PCA). Errors reported here are obtained by training only on the full set of 265 training examples (i.e. the test
+ training set used in plotting learning curves of Sec. 4.1 and 4.2), while we have introduced a Validation Set comprising an extra
102 patients whose cytology was determined before hand. Weighted classiﬁers have signiﬁcantly lower False negative rates (higher
sensitivity) and so are the preferred classiﬁers in our work.

Classiﬁer

False Positive Percentage

False Negative Percentage

Total Misclassiﬁcation Percentage

Train &Test Set

Validation Set

Total

Train + Test Set

Validation Set

Total

Train + Test Set

Validation Set

Total

MI

PCA

Logistic Unweighted
Logistic Weighted

SVM Unweighted
SVM Weighted

Logistic Unweighted
Logistic Weighted

SVM Unweighted
SVM Weighted

6.7
26

28
24

7.2
32

2.8
20

0
0

0
14

0
2.3

0
16

5.5
14

2.2
22

2.7
21

2.2
19

38
9.4

51
15

39
11

54
15

15
6.9

7.3
3.6

17
8.6

9.0
5.5

29
15

34
11

34
16

35
11

17
21

18
21

17
25

19
18

8.8
3.9

4.9
7.8

9.8
5.9

4.9
9.8

13
15

14
17

15
19

15
16

full m = 265 indeterminate patients. Finally, we include
the previously ‘unexposed’ m(cid:48) = 102 samples in the vali-
dation set for the ﬁrst time, and predict on all 367 samples.
We report on ﬁnal error estimates2 for both logistic regres-
sion and SVMs in Table 3. Note that for the SVM, we use
a linear kernel with an (cid:96)1 parameter C = 0.1, and wher-
ever weighted classiﬁers were used the weights were 1 : 4
benign to malignant.
This study presents a unique challenge. While it is crucial
that a cancer classiﬁer accurately predict malignant sam-
ples (i.e. low false negative rates or high sensitivity), recall
that most patients with indeterminate biopsies end up hav-
ing unnecessary surgery. Thus a parallel goal of our work is
to reduce unnecessary surgeries for patients with benign tu-
mors, requiring a low false positive rate (high speciﬁcity).
An optimal balance of these competing interests is perhaps
achieved by the weighted SVM classiﬁers. With a total
false negative rate of 11% (i.e. 89% sensitivity), and a false
positive rate of ≈ 20% (i.e. 80% speciﬁcity) this is clearly
a competitive classiﬁer.

6. Summary and Future avenues
The primary battle in this project was high variance, and
so we have been very careful in feature selection. We have
used a small subset of the curated list of 173 genes, based
on their mutual information score and PCA. We selected
our model after a thorough analysis of the learning curves,
while varying the relative weights and the (cid:96)1 parameter for
the SVM. Our classiﬁer gives similar performance to the
currently best-known classiﬁer, but uses signiﬁcantly fewer
features: 10 features as opposed to 167. This narrowing of

2The results are presented in a manner that is consistent with
cancer biology research, and hence we have not shown P, R and
F1 values or confusion matrices; these can be inferred from Table
3.

the list of contributing genes can possibly allow for a more
targeted approach to investigating the genetic characteris-
tics of thyroid cancer.
A natural next step is to test the robustness of our gene
selection by implementing other feature selecting methods
(such as random forests, forward search) and seeing if they
pick the same genes. In addition, it is worth noting that
we have only had access to 173 genes out of ∼25000, and
an analysis of the full set of genes from the experiment
would make our feature selection process more complete.
Unfortunately this goal faces an administrative hurdle; we
were told by Dr. Giulia C. Kennedy, one of the authors of
(Alexander et al., 2012), that this data is in fact proprietary.
It is also interesting to note that the errors of our classiﬁers
do not depend on whether we use PCA or mutual informa-
tion to select our features. However this result is not trivial,
especially because the top 10 genes do not have signiﬁcant
weights in the top principle components. This curious re-
sult warrants further investigation.
Finally, in addition to strengthening our feature selection,
we plan to see if more advanced classiﬁcation techniques,
such as neural networks, may give better performance.
However it is possible that this may require larger data sets
and more extensive clinical trials.

7. Acknowledgements
We are thankful to Kevin Brennan and Olivier Gevaert at
the Stanford Center for Biomedical Informatics Research
for suggesting this project, providing us with the data, and
helping us try to unlock information in the CEL ﬁles. We
also thank Peter Li for insightful comments and the genesis
of this project. Finally we would like to thank our project
TA Irene Kaplow for helpful comments on the midterm
project report and during the poster session.

Machine learning for thyroid cancer diagnosis

Rajiv Krishnakumar
Department of Applied Physics, Stanford University, CA 94305 USA
Raghu Mahajan
Department of Physics, Stanford University, CA 94305 USA
Akash V. Maharaj
Department of Physics, Stanford University, CA 94305 USA

RAJK@STANFORD.EDU

RM89@STANFORD.EDU

AMAHARAJ@STANFORD.EDU

Abstract

We investigate the use of high throughput gene
expression data in the diagnosis of thyroid can-
cers. Using logistic regression and support vec-
tor machines (SVMs), we develop a classiﬁer
which gives similar performance (89% sensitiv-
ity and 80% speciﬁcity) to the currently best-
known classiﬁer, but uses signiﬁcantly fewer fea-
tures. We used two different techniques, princi-
pal components analysis and mutual information
score, to select features. The results do not de-
pend signiﬁcantly on which method is used for
feature selection.

1. Introduction and related work
High throughput gene expression data is now readily avail-
able for many diseases and has been used extensively to de-
velop classiﬁers to help physicians diagnose and treat these
diseases. Thyroid cancer is one of those diseases.
Currently, a technique known as ﬁne-needle aspiration
(FNA) is used to determine whether a thyroid nodule is ma-
lignant or benign. Even though over 95% of the total cases
end up being benign (Howlader et al., 2011), many more
are diagnosed as malignant or ‘indeterminate’, i.e. unclear
diagnosis, after performing FNA. Patients with indetermi-
nate diagnoses (as well as those with malignant diagnoses)
then undergo diagnostic surgery to remove the tumor or be-
nign thyroid lesion; only about 30% are subsequently found
to be malignant post operation (Welker & Orlov, 2003).
This imperfect system leads to many unnecessary surgical
operations, enhancing risk for the patient and an increase
in healthcare costs.

CS229 Final Project Report,
December 11, 2015

A recent study (Alexander et al., 2012) has shown the po-
tential for diagnosing thyroid cancer using gene expression
data. The data consists of 367 samples, each having about
O(104) features (genes) and each classiﬁed as malignant
or benign by post-operation inspection. Using support vec-
tor machines the investigators reported a new diagnostic
test based on a narrowed feature set consisting of O(102)
features. While the diagnostic test has a reasonable false
negative rate of 8%, it has a high (48%) false positive rate.
In addition, the use of 173 genes with just 265 patients ren-
ders any classiﬁer liable to overﬁtting. Thus, our principal
goal is to investigate novel machine learning approaches
for development of a classiﬁer that outperforms the current
state of the art, namely higher speciﬁcity while maintaining
or improving the current sensitivity, with the use of fewer
features.
In Sec. 2 we discuss the nature of the data. In Sec 3 we dis-
cuss feature selection based on mutual information scores,
and also via principal components analysis.
In Sec. 4
we implement logistic regression and Support Vector Ma-
chines with linear kernels. We discuss the tuning of pa-
rameters such as relative weights assigned to the benign
and malignant samples, and varying (cid:96)1 regularizations for
the SVM. We end with a summary of our major results in
Sec. 5 and discussion of future work in Sec. 6.

2. Gene expression data
The dataset which forms the basis of this project was ﬁrst
employed by (Alexander et al., 2012) in a study on the use
of gene expression data for pre-operative thyroid cancer di-
agnosis. A full description of the clinical procedures em-
ployed in acquiring the gene expression data is beyond the
scope of this work; interested readers can consult (Alexan-
der et al., 2012). Basic attributes of the dataset (OnlineRef,
2012) are summarized in Table 1. The relevant part of
dataset consists of m = 265 patients (or more callously,

Machine learning for thyroid cancer diagnosis

Table 1. Key attributes of the dataset.

Number of patients in dataset
Result of biopsy (cytology)

Benign
Malignant
Indeterminate
Post operative diagnosis of
indeterminate tumors (ground truths)

Benign
Malignant

367

55
47
265

180
85

Number of features (gene expressions)

Raw dataset
Pre-normalized and selected

∼ 25000

173

‘training examples’) with indeterminate biopsy results, of
which the ‘ground truth’ classiﬁcations of tumors are 85
malignant and 180 benign. A further m(cid:48) = 102 patients
whose biopsies were not indeterminate (47 benign, 55 ma-
lignant) are also included in the primary dataset and these
will later form our validation set1. Each training example
consists of n = 173 features, corresponding to a select set
of gene expressions obtained from the biopsy and subse-
quent microarray assay of a single thyroid nodule.

2.1. Pre-processing of data set

As part of the publicly available dataset (OnlineRef, 2012),
we were able to obtain the normalized gene expression data
for 173 genes for all 367 patients. These 173 were chosen
by the authors of (Alexander et al., 2012), based on a se-
quential procedure involving ‘Limma’ analysis and is de-
tailed in the supplemental material of their paper. We used
only this subset of features in our analysis.
We further normalize each gene expression to zero mean
and unit standard deviation. As mentioned previously, the
relative intensities of different genes is likely to be mean-
ingless (an artifact of the microarray procedure), and so this
choice of zero mean and unit standard deviation constitutes
a weaker modeling assumption than otherwise. Figure 1
provides a color map of these n = 173 features for the
m = 265 patients in our dataset. We note here that an at-
tempt at quantile normalization severely decreased the per-
formance of our classiﬁers, so we have not employed this
additional pre-processing step.

1The fact that biopsy results are based on the personal opin-
ion of evaluators suggests that there is in principle no profound
genetic difference between indeterminate vs.
‘pre-determined’
cytology diagnoses. This is our justiﬁcation for using the ‘pre-
determined’ samples as a validation set.

Figure 1. Color map visualization of gene expression data for a re-
duced feature set of 173 features. Because relative magnitudes of
different genes are (in principle) meaningless, each feature (gene)
has been normalized to zero mean and a standard deviation of
1. The black line at patient number 180 separates benign (above
line) from malignant samples (below line).

3. Feature Selection
With 173 features and 265 samples, smart feature selection
is crucial to avoid overﬁtting. We have adopted two com-
plementary approaches in this work: Mutual Information
(MI) and Principal Components Analysis (PCA). Happily,
as we will see in subsequent sections, the predictive power
of the resulting classiﬁer is similar regardless of whether
we use the mutual information statistic or PCA to select
the subset of genes.

3.1. Mutual Information

Our ﬁrst method of feature selection involved computation
of mutual information scores. This has the advantage of
hand picking the the most informative genes, opening the
possibility of building a classiﬁer which uses a few select
genes that can easily be sequenced in clinical tests. We
computed the Mutual Information score M I(xj, y) of each
gene xi deﬁned as

(cid:88)

(cid:88)

xj

y

M I(xj, y) =

p(xj, y) log

p(xj, y)
p(xj)p(y)

(1)

where p(xi, y) is the joint distribution of gene xj and diag-
nosis y, and we binned the gene expression levels into 43
bins to make xj a discrete variable. The names and TCIDs
(taken from the supplemental material in (Alexander et al.,
2012)) of the top ten genes are listed in Table 2. A his-
togram is shown in Figure 2.

20406080100120140160Gene/Feature50100150200250Patient/Sample-4-2024681012Machine learning for thyroid cancer diagnosis

MI Rank Gene names

LIPH
MDK
PROS1
LRP1B

Gene Description
lipase, member H

TCID
2708855
3329343 midkine (neurite growth-promoting factor 2)
2685304
2578790

protein S (alpha)
low density lipoprotein receptor-related protein 1B

1.
2.
3.
4.

5.

MAPK6
PKHD1L1

3111561 mitogen-activated protein kinase 6

polycystic kidney and hepatic disease 1 (autosomal recessive)-like 1

GABRB2
CLDN16

6.
7.
DPP6
8.
9.
TIMP1
10. MPPED2

gamma-aminobutyric acid (GABA) A receptor, beta 2
claudin 16
dipeptidyl-peptidase 6

2884845
2657808
3032647
3976341 TIMP metallopeptidase inhibitor 1
367673 metallophosphoesterase domain containing 2

Table 2. List of the top ten highest correlated genes ranked via
their Mutual Information (MI) score.

3.2. Principal Components Analysis

the covariance matrix ˆΣ = (cid:80)m

To have more conﬁdence in our feature selection, we also
did feature selection using a completely different method:
principal components analysis (PCA). This method com-
plements MI by selecting linear subspace of all n = 173
genes with the highest variances (and hence largest amount
of ‘information’). In PCA we compute the eigenvectors of
i=1 x(i)x(i)T where x(i) ∈
Rn is a vector containing all genes of patient i. Selecting
the top k principal components corresponds to transform-
ing each x(i) by a matrix W = (u1, u2, . . . , uk) where
u1 ∈ Rn is the principal eigenvector of ˆΣ etc.. A histogram
of the eigenvalues of ˆΣ is shown in Figure 2.

4. Models
4.1. Logistic Regression
Given the binary classiﬁcation problem with inputs X ∈
Rn a simple ﬁrst classiﬁcation algorithm is logistic regres-
sion (Dobson & Barnett, 2008). Logistic regression uses
the function

hθ(x) =

1

1 + e−θT x

(2)

0 or 1 (i.e. benign or malignant respectively). Here, θ ∈
Rn+1 is a vector of coefﬁcients which can be determined by
maximum likelihood estimation. Although hθ(x) takes on
continuous values between 0 and 1, the algorithm sets the
prediction to 1 if hθ(x) > 0.5 and 0 otherwise. Given the
training examples x(i), and their corresponding outcomes
y(i). We can write down the likelihood

m(cid:89)
m(cid:89)

i=1

L(θ) =

=

w(i)p(y(i)|x(i); θ)

w(i) (hθ(x))y(i)

(1 − hθ(x))1−y(i)

,

(3)

(4)

i=1

where w(i) is a weight which is dependent on whether the
sample was malignant or benign. The (log) likelihood can
now be maximized using gradient ascent. The weights al-
low us to impose a higher penalty when predicting a ma-
lignant sample incorrectly compared to when predicting a
benign sample incorrectly.
We implement logistic regression using MATLAB’s multi-
nomial logistic regression function, mnrfit. To assess the
success of this algorithm, we employ hold k-fold cross vali-
dation, with k = 10. In Fig. 3 we plot the training error and
empirical test error as the number of features is varied. We
trained different classiﬁers by tweaking the number of fea-
tures kept and also the relative weights (1:4 for benign vs.
malignant samples) assigned to the samples. It is important
to correctly diagnose the patients who have a malignant tu-
mor, so we impose a big penalty for misclassifying the ma-
lignant samples. We do this for features selected both using
mutual information and PCA.

to determine whether a sample of with a vector of features
x ∈ Rn+1 (in our case the n different gene expressions,
with an extra constant feature x0 = 1) has an outcome of

Figure 2. Histograms from MI (left) and PCA (right), showing the
number of genes (principal components) with a given mutual in-
formation score (eigenvalue). The features on the right side of
these histograms are the most important. Note the logarithmic
scale for PCA rankings.

Figure 3. Learning curves for logistic regression, unweighted (up-
per row), and weighted 1:4 benign:malignant samples (lower
row). MI on left column and PCA on right column.

00.050.10.150.2Mutual Information05101520Number of Genes-6-4-2024Log(λ)051015Number of Eigenvalues010203040506070Number of Features0.10.150.20.25ErrorTest ErrorTraining Error010203040506070Number of Features0.10.150.20.25ErrorTest ErrorTraining Error010203040506070Number of Features0.10.150.20.250.30.35ErrorTest ErrorTraining Error010203040506070Number of Features0.050.10.150.20.250.30.350.4ErrorTest ErrorTraining ErrorMachine learning for thyroid cancer diagnosis

Figure 4. Learning curves for SVM, weighted, MI on left and
PCA on right, each with a (cid:96)1 parameter of C = 0.1. The weighted
decision boundary uses relative weighting of 1:4 (malignant sam-
ples are assigned a higher weight).

4.2. Support Vector Machines

The second learning algorithm we have implemented is a
Support Vector Machine (SVM), using a linear kernel with
(cid:96)1 regularization. Use of (cid:96)1 regularization is necessary be-
cause the data is (empirically) not strictly linearly separa-
ble.

SVMs work by ﬁnding the optimal hyperplane(cid:80)

j wjxj+b
that separates/classiﬁes the data. Here, b is a constant offset
and wj is a coefﬁcient for each gene/feature j in our data.
Solving for the optimal wj and b amounts to a constrained
convex optimization problem, where we must minimize

m(cid:88)

i=1

||w||2 + C

1
2

ξi

(5)

αi − m(cid:88)

m(cid:88)

i=1

w.r.t. w and b, subject to the constraints y(i)(wT x(i) + b) ≥
1 − ξi and ξi ≥ 0 ∀i (ξi are known as slack variables,
and C is the constraint parameter which penalizes incor-
rectly classiﬁed data). Solution of this problem is simpler
if we consider a dual optimization problem where a series
of standard manipulations (Bishop, 2006) leads to the du-
alized optimization problem:

y(i)y(j)αiαjx(i)T

x(j)

(6)

max

α

i,j=1

W (α) =

subject to the constraints 0 ≤ αi ≤ C and(cid:80)m

i=1 αiy(i) =
0. We implement the SVM with this linear kernel, using the
Matlab routine fitcsvm which proceeds via the Sequen-
tial Minimal Optimization (SMO) algorithm (Platt, 1998).
To estimate our errors, we again employ k-fold cross vali-
dation with k = 10 and impose a big penalty for misclas-
sifying the malignant samples. In Figure 4, we show the
learning curves for the SVM using a relative weighting of
1:4 for the benign vs. malignant samples and an (cid:96)1 parame-
ter of C = 0.1 (which we optimized for by considering C’s
over 4 orders of magnitude). It is clear that optimal perfor-
mance for both MI and PCA selected features is achieved
for fewer than 20 features kept.

Figure 5. Decision boundaries for SVM, using only the best two
genes selected using mutual information (top ﬁgure) and top two
principal components PCA (bottom ﬁgure). We used a relative
weighting of 1:4 (higher weights for malignant samples) and the
(cid:96)1 parameter C = 0.1. Data points are labelled by their pre- and
post-operative diagnosis (B = Benign, I = Indeterminate, M = Ma-
lignant), so for example (I-M) indicates a pre operative diagnosis
of indeterminate and a post operative diagnosis of Malignant. The
clustering of Malignant samples is obvious in both MI and PCA
variables. It is also clear that unweighted boundary achieves ex-
cellent classiﬁcation of benign samples (leading to low false pos-
itive errors for unweighted classiﬁers in Table 3).

While the learning curves of Fig. 4 suggest that ∼ 10 fea-
tures should be used in our ﬁnal classiﬁer, we have found
that a two feature scatter plot is an informative visualization
of the structure of our data demonstrating the lack of linear
separability which persists in higher dimensions. Plotted
in Figure 5 is the full data set (test and training sets, along
with the validation set), in addition to the decision bound-
aries obtained by keeping the top two features.

5. Final Results
After exploring the parameter space of the models de-
scribed in Section 4, we decided to work with a subset of 10
features, since its in that ballpark where the test and train-
ing errors seem comparable and small. Having used the
training curves to select these 10 features (both genes from
MI and then components from PCA), we then train on the

0204060Number of Features0.160.180.20.220.240.260.280.3ErrorTest ErrorTraining Error0204060Number of Features0.10.150.20.250.30.35ErrorTest ErrorTraining Error-1.5-1-0.500.511.522.5Gene 1: Lipase, Member H. (TCID: 2708855)-1.5-1-0.500.511.522.533.5Gene 2: Midkine. (TCID: 3329343)B - BI - BB - MI - MM - MWeighted BoundaryUnweighted Boundary-15-10-5051015Principal Component #1-8-6-4-202468Principal Component #2B - BI - BB - MI - MM - MWeighted BoundaryUnweighted BoundaryMachine learning for thyroid cancer diagnosis

Table 3. False Positive and False negative rates as well as total errors for the various classiﬁers used in our study. Both Logistic regres-
sion and SVMs were trained on the top 10 features obtained by feature selection using Mutual Information (MI) scores and Principal
Components Analysis (PCA). Errors reported here are obtained by training only on the full set of 265 training examples (i.e. the test
+ training set used in plotting learning curves of Sec. 4.1 and 4.2), while we have introduced a Validation Set comprising an extra
102 patients whose cytology was determined before hand. Weighted classiﬁers have signiﬁcantly lower False negative rates (higher
sensitivity) and so are the preferred classiﬁers in our work.

Classiﬁer

False Positive Percentage

False Negative Percentage

Total Misclassiﬁcation Percentage

Train &Test Set

Validation Set

Total

Train + Test Set

Validation Set

Total

Train + Test Set

Validation Set

Total

MI

PCA

Logistic Unweighted
Logistic Weighted

SVM Unweighted
SVM Weighted

Logistic Unweighted
Logistic Weighted

SVM Unweighted
SVM Weighted

6.7
26

28
24

7.2
32

2.8
20

0
0

0
14

0
2.3

0
16

5.5
14

2.2
22

2.7
21

2.2
19

38
9.4

51
15

39
11

54
15

15
6.9

7.3
3.6

17
8.6

9.0
5.5

29
15

34
11

34
16

35
11

17
21

18
21

17
25

19
18

8.8
3.9

4.9
7.8

9.8
5.9

4.9
9.8

13
15

14
17

15
19

15
16

full m = 265 indeterminate patients. Finally, we include
the previously ‘unexposed’ m(cid:48) = 102 samples in the vali-
dation set for the ﬁrst time, and predict on all 367 samples.
We report on ﬁnal error estimates2 for both logistic regres-
sion and SVMs in Table 3. Note that for the SVM, we use
a linear kernel with an (cid:96)1 parameter C = 0.1, and wher-
ever weighted classiﬁers were used the weights were 1 : 4
benign to malignant.
This study presents a unique challenge. While it is crucial
that a cancer classiﬁer accurately predict malignant sam-
ples (i.e. low false negative rates or high sensitivity), recall
that most patients with indeterminate biopsies end up hav-
ing unnecessary surgery. Thus a parallel goal of our work is
to reduce unnecessary surgeries for patients with benign tu-
mors, requiring a low false positive rate (high speciﬁcity).
An optimal balance of these competing interests is perhaps
achieved by the weighted SVM classiﬁers. With a total
false negative rate of 11% (i.e. 89% sensitivity), and a false
positive rate of ≈ 20% (i.e. 80% speciﬁcity) this is clearly
a competitive classiﬁer.

6. Summary and Future avenues
The primary battle in this project was high variance, and
so we have been very careful in feature selection. We have
used a small subset of the curated list of 173 genes, based
on their mutual information score and PCA. We selected
our model after a thorough analysis of the learning curves,
while varying the relative weights and the (cid:96)1 parameter for
the SVM. Our classiﬁer gives similar performance to the
currently best-known classiﬁer, but uses signiﬁcantly fewer
features: 10 features as opposed to 167. This narrowing of

2The results are presented in a manner that is consistent with
cancer biology research, and hence we have not shown P, R and
F1 values or confusion matrices; these can be inferred from Table
3.

the list of contributing genes can possibly allow for a more
targeted approach to investigating the genetic characteris-
tics of thyroid cancer.
A natural next step is to test the robustness of our gene
selection by implementing other feature selecting methods
(such as random forests, forward search) and seeing if they
pick the same genes. In addition, it is worth noting that
we have only had access to 173 genes out of ∼25000, and
an analysis of the full set of genes from the experiment
would make our feature selection process more complete.
Unfortunately this goal faces an administrative hurdle; we
were told by Dr. Giulia C. Kennedy, one of the authors of
(Alexander et al., 2012), that this data is in fact proprietary.
It is also interesting to note that the errors of our classiﬁers
do not depend on whether we use PCA or mutual informa-
tion to select our features. However this result is not trivial,
especially because the top 10 genes do not have signiﬁcant
weights in the top principle components. This curious re-
sult warrants further investigation.
Finally, in addition to strengthening our feature selection,
we plan to see if more advanced classiﬁcation techniques,
such as neural networks, may give better performance.
However it is possible that this may require larger data sets
and more extensive clinical trials.

7. Acknowledgements
We are thankful to Kevin Brennan and Olivier Gevaert at
the Stanford Center for Biomedical Informatics Research
for suggesting this project, providing us with the data, and
helping us try to unlock information in the CEL ﬁles. We
also thank Peter Li for insightful comments and the genesis
of this project. Finally we would like to thank our project
TA Irene Kaplow for helpful comments on the midterm
project report and during the poster session.

Machine learning for thyroid cancer diagnosis

References
Alexander, Erik K, Kennedy, Giulia C, Baloch, Zubair W,
Cibas, Edmund S, Chudova, Darya, Diggans, James,
Friedman, Lyssa, Kloos, Richard T, LiVolsi, Virginia A,
Mandel, Susan J, et al. Preoperative diagnosis of benign
thyroid nodules with indeterminate cytology. New Eng-
land Journal of Medicine, 367(8):705–715, 2012.

Bishop, Christopher M. Pattern recognition and machine
Information science and statistics. Springer,

learning.
New York, 2006. ISBN 978-0-387-31073-2.

Dobson, Annette J. and Barnett, Adrian G. An introduction
to generalized linear models. Number 77 in Texts in
statistical science series. CRC Press, Boca Raton, Fla.,
3. ed edition, 2008. ISBN 978-1-58488-950-2.

Howlader, N, Noone, AM, Krapcho, M, Neyman, N,
Aminou, R, Waldron, W, Altekruse, SF, Kosary, CL,
Ruhl, J, Tatalovich, Z, et al. Seer cancer statistics review,
1975–2008. Bethesda, MD: National Cancer Institute,
19, 2011.

OnlineRef,

2012.

URL http://www.ncbi.

nlm.nih.gov/geo/query/acc.cgi?acc=
GSE34289.

Platt, John C. Sequential minimal optimization: A fast al-
gorithm for training support vector machines. Technical
report, ADVANCES IN KERNEL METHODS - SUP-
PORT VECTOR LEARNING, 1998.

Welker, Mary Jo and Orlov, Diane. Thyroid nodules. Amer-

ican family physician, 67(3):559–566, 2003.

