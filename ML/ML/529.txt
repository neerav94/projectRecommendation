On-line kernel learning for active sensor networks

Stefan Jorgensen*

I. Introduction

In this algorithmic project we consider how to leverage
increasingly capable mobile sensor networks to simulta-
neously exploit a changing environment and maintain
low uncertainty about the previously explored regions.
This problem, known as ‘coverage-estimation’ or ‘active
sensing’ in the literature, has applications in many ﬁelds:
tracking interesting oceanographic or atmospheric data
using a robotic network, ﬁnding and clearing environ-
mental hazards such as pollutants [1] or radioactive
waste [2], searching for humans in wreckage [3], and even
game theoretical formulations such as the multi-armed
bandit problem [4]. Much of the literature focuses on
either the estimation problem of exploring an unknown
environment or the coverage problem of exploiting a
known environment.

A. Contribution

The main contribution of this project is to apply on-
line adaptive kernel estimation techniques to the cover-
age estimation problem in an environment which reacts
to observation. To the author’s knowledge, this approach
has not been tried in the published literature.

B. Related work

In [5] a kernel based Gaussian Regression approach
to coverage estimation is proposed by Carron et al.,
where a centralized base station computes trajectories
for each agent based on the measurement history up to
the current time. The data dimension grows linearly with
time i and hence the complexity grows as O(i3) due to a
matrix inversion of the problem data. Nodes are directed
toward areas with higher variance in order to improve
the estimate. Once the maximum estimate variance is
below a predeﬁned threshold, the algorithm switches to
a partitioning algorithm, which assumes the environment
is static and moves agents to a centroidal Voronoi par-
tition (a well-known optimal coverage conﬁguration for
distance-based problems, [6]). The algorithm in [5] is not
well suited for on-line learning tasks since it does not
provide the network with the ability to switch back from
this exploitation state to the former exploration state,
and the complexity of the exploration algorithm grows
as O(i3).

A distributed, parametric and non-parametric ap-
proach to the same problem is proposed in [7]. The

*Department of Electrical Engineering, Stanford University.
Project advised by Dr. Marco Pavone. This work was funded in
part by NSF grant DGE-114747. stefantj@stanford.edu

statistical model used is more general than [5] because
they consider distributions which are not Gaussian Ran-
dom ﬁelds. Starting from an optimal centralized policy,
the authors develop a distributed approximation by as-
suming that the agent locations are all independently
identically distributed and using a consensus ﬁlter to
explicitly evaluate the kernel based feature vector for
neighboring measurements (note that this precludes an
implicit inﬁnite dimensional feature vector). Only the
one-shot estimation case is considered, so the active
sensing question of how to move sensors is not addressed.
A method for choosing the most signiﬁcant features is
proposed, and it is claimed that this computation can be
run online. However, given the weak assumptions made
on the ad-hoc network, it is non-trivial to guarantee
that at any given time, the agents have agreed on the
basis functions (and ordering) to use in the algorithm.
Without this guarantee, the inner-product evaluations
that the algorithm relies on are not meaningful. If the
‘important’ features are chosen ahead of time, the kernel
method reduces to a parametric method similar to those
described in [8].

C. Motivation of proposed work

functions to estimate general

The approach to active sensing of a scalar ﬁeld taken in
this project is inspired by the non-parametric methods
used in [5]. Parametric methods suﬀer from the same
fundamental problem of linear estimators: due to the
ﬁnite number of basis functions used to parametrize
the space, parametric methods require a large number
of
functions well (and
there will always be a pathological case which is poorly
estimated). In problems where prior knowledge is not
available or where the statistics may change signiﬁ-
cantly, nonparametric kernel based methods provide an
adaptive, general learning framework. They still require
knowledge of the expected ‘smoothness’ via the kernel
bandwidth.

D. Statement of work

This work examines the coverage estimation problem
with kernel adaptive ﬁlters, which kernelize familiar lin-
ear estimators. In particular, kernel least-mean squares
(KLMS) and kernel recursive least squares (KRLS) are
used. Derivations and variants of the KLMS and KRLS
algorithm are summarized in [9]. The goal of this project
is to provide an on-line, kernel based approach to the
problem of maximizing the minimum value of time-
varying, reactive ﬁelds.

On-line kernel learning for active sensor networks

Stefan Jorgensen*

I. Introduction

In this algorithmic project we consider how to leverage
increasingly capable mobile sensor networks to simulta-
neously exploit a changing environment and maintain
low uncertainty about the previously explored regions.
This problem, known as ‘coverage-estimation’ or ‘active
sensing’ in the literature, has applications in many ﬁelds:
tracking interesting oceanographic or atmospheric data
using a robotic network, ﬁnding and clearing environ-
mental hazards such as pollutants [1] or radioactive
waste [2], searching for humans in wreckage [3], and even
game theoretical formulations such as the multi-armed
bandit problem [4]. Much of the literature focuses on
either the estimation problem of exploring an unknown
environment or the coverage problem of exploiting a
known environment.

A. Contribution

The main contribution of this project is to apply on-
line adaptive kernel estimation techniques to the cover-
age estimation problem in an environment which reacts
to observation. To the author’s knowledge, this approach
has not been tried in the published literature.

B. Related work

In [5] a kernel based Gaussian Regression approach
to coverage estimation is proposed by Carron et al.,
where a centralized base station computes trajectories
for each agent based on the measurement history up to
the current time. The data dimension grows linearly with
time i and hence the complexity grows as O(i3) due to a
matrix inversion of the problem data. Nodes are directed
toward areas with higher variance in order to improve
the estimate. Once the maximum estimate variance is
below a predeﬁned threshold, the algorithm switches to
a partitioning algorithm, which assumes the environment
is static and moves agents to a centroidal Voronoi par-
tition (a well-known optimal coverage conﬁguration for
distance-based problems, [6]). The algorithm in [5] is not
well suited for on-line learning tasks since it does not
provide the network with the ability to switch back from
this exploitation state to the former exploration state,
and the complexity of the exploration algorithm grows
as O(i3).

A distributed, parametric and non-parametric ap-
proach to the same problem is proposed in [7]. The

*Department of Electrical Engineering, Stanford University.
Project advised by Dr. Marco Pavone. This work was funded in
part by NSF grant DGE-114747. stefantj@stanford.edu

statistical model used is more general than [5] because
they consider distributions which are not Gaussian Ran-
dom ﬁelds. Starting from an optimal centralized policy,
the authors develop a distributed approximation by as-
suming that the agent locations are all independently
identically distributed and using a consensus ﬁlter to
explicitly evaluate the kernel based feature vector for
neighboring measurements (note that this precludes an
implicit inﬁnite dimensional feature vector). Only the
one-shot estimation case is considered, so the active
sensing question of how to move sensors is not addressed.
A method for choosing the most signiﬁcant features is
proposed, and it is claimed that this computation can be
run online. However, given the weak assumptions made
on the ad-hoc network, it is non-trivial to guarantee
that at any given time, the agents have agreed on the
basis functions (and ordering) to use in the algorithm.
Without this guarantee, the inner-product evaluations
that the algorithm relies on are not meaningful. If the
‘important’ features are chosen ahead of time, the kernel
method reduces to a parametric method similar to those
described in [8].

C. Motivation of proposed work

functions to estimate general

The approach to active sensing of a scalar ﬁeld taken in
this project is inspired by the non-parametric methods
used in [5]. Parametric methods suﬀer from the same
fundamental problem of linear estimators: due to the
ﬁnite number of basis functions used to parametrize
the space, parametric methods require a large number
of
functions well (and
there will always be a pathological case which is poorly
estimated). In problems where prior knowledge is not
available or where the statistics may change signiﬁ-
cantly, nonparametric kernel based methods provide an
adaptive, general learning framework. They still require
knowledge of the expected ‘smoothness’ via the kernel
bandwidth.

D. Statement of work

This work examines the coverage estimation problem
with kernel adaptive ﬁlters, which kernelize familiar lin-
ear estimators. In particular, kernel least-mean squares
(KLMS) and kernel recursive least squares (KRLS) are
used. Derivations and variants of the KLMS and KRLS
algorithm are summarized in [9]. The goal of this project
is to provide an on-line, kernel based approach to the
problem of maximizing the minimum value of time-
varying, reactive ﬁelds.

II. Problem formulation

A. Model

Assume we have N mobile sensors, and a central base
station. We denote measurement times by the index i.
n in a polygon X , and takes
Agent n has position xi
n) ⊂ X . The mea-
measurements over a footprint B(xi
surements follow the model

yi,n(x) = f (x, T (x, i), i) + νi,n

for x ∈ B(xi
is sensor noise and T (x, i) is given by

n), where i is the time of measurement, νi,n

N(cid:88)

i(cid:88)

n=1

j=0

T (x, i) =

1
i

1{x ∈ B(xj

n)}

which is the fraction of total time that the point x has
been observed up to time i. In this context, 1(·) is the
indicator function. If multiple agents observe the same
location, this fraction can be greater than 1 (though
not greater than N). Agents move according to noise-less
integrator dynamics,

n = xi−1
xi

n + ui
n

B. System requirements

We have the following communication and computa-

tion requirements:
(C1) agent n can identify itself to the base station and

(C2) agent n can measure: y(x) = f (x, T (x, i), i) + vi,

can send information to the base station;
over a region B(xi

n).

(C3) agent n can measure its position xi
n

The base station must be able to:

(C4) store measurements taken at time a given time step
by all agents, in addition to the current estimates
of f

(C5) compute partitions on X
(C6) send information to each robot

The goal of the network is to develop a observa-
tion policy T (x, i) to maximize the minimum value of
f (x, T (x, i), i).

C. Example problems

1) Scalar ﬁeld estimation: This applies immediately,
where we take f to be the time-varying error in mea-
surement.

p(x)

2) Sweeping and Persistent robotic tasks: This prob-
lem is proposed in [10], where they describe the ﬁeld by
the diﬀerential equation

˙f (x) =

p(x) − c(x)
0

if x (cid:54)∈ ∪N
if x ∈ ∪N
if x ∈ ∪N

n=1B(xn)
n=1B(xn) and f (x) > 0
n=1B(xn) and f (x) = 0
Integrating, we ﬁnd that (assuming that p(x) and c(x)
are independent of time)
f (x, i) = ip(x) + f (x, 0) − T (x, t)c(x) = f (x, T (x, i), i).

Their policy seeks to minimize the maximum of the
ﬁeld values, which ﬁts out model by a simple change of
variables.
3) Border patrol and intruder detection: Assume that
there are intruders trying to penetrate a region X unde-
tected. Their risk of failure is dictated by environmental
factors and being caught by a patrol. Intruders have ac-
cess to the patrol frequency T (x, i) along the region, but
not to real time position information of patrols. At every
time step i, the intruder examines the environment and
frequency of patrols, and compares it to a risk threshold
τi. We also assume that intruders are impatient, so their
risk threshold increases over time: τi > τi−1.

Patrolling agents move to a location and wait until
they see an intruder or grow impatient, then move on in
order to maximize the minimum intrusion time over X .
In a game theoretic context maximizing the minimum is
the Nash Equilibrium, since to choose any other policy
will reduce the patrolling agents value function (by def-
inition), and the intruders cannot unilaterally increase
their value function (to get in quickly).

We can formulate this problem as maximizing the
minimum of a ﬁeld representing the risk of failure (for
intruders) over the patrol frequencies T (x, i). Explicitly
constructing this problem analytically typically requires
strong statistical assumptions (such as are used in multi-
armed bandit problems), but even without those we can
say that this is a problem of maximizing the minimum of
some function of space, time and average coverage time.
Thus, this ﬁts the model from Section II-A.

III. Proposed Solution

Our proposed solution is outlined in algorithm 1, which
is very similar to [5], with two major diﬀerences: an
adaptive kernel estimator is used in place of Gaussian
Regression, and the ﬁeld is assumed to respond to obser-
vation. Coverage is achieved by running a range-limited
version of Lloyd’s clustering algorithm [11]: each agent
ﬁnds the centroid of a reward function based on f over
all points nearest to itself and travels there. In a static
environment, this algorithm will eventually converge to
the centroidal Voronoi partition, which is the solution to
many locational optimization problems. In our case, the
ﬁeld is time varying, so the locations of agents will not
converge. Instead, we look for convergence of the smallest
ﬁeld values.

The methods

estimator.update(u, d) and d =
estimator.predict d(u), and r = estimator.predict r(u)
are described in [9] for KLMS and KRLS, but are not
repeated here for brevity. The argument u is the mea-
surement vector, d is the scalar output of the model (or
observation) and r is the variance of the estimate d.

The method moveReward(map, xi

j) only considers the
points which are closest to agent j (e.g., in its Voronoi
partition). The method reward(d, r) is described next.

On-line kernel learning for active sensor networks

Stefan Jorgensen*

I. Introduction

In this algorithmic project we consider how to leverage
increasingly capable mobile sensor networks to simulta-
neously exploit a changing environment and maintain
low uncertainty about the previously explored regions.
This problem, known as ‘coverage-estimation’ or ‘active
sensing’ in the literature, has applications in many ﬁelds:
tracking interesting oceanographic or atmospheric data
using a robotic network, ﬁnding and clearing environ-
mental hazards such as pollutants [1] or radioactive
waste [2], searching for humans in wreckage [3], and even
game theoretical formulations such as the multi-armed
bandit problem [4]. Much of the literature focuses on
either the estimation problem of exploring an unknown
environment or the coverage problem of exploiting a
known environment.

A. Contribution

The main contribution of this project is to apply on-
line adaptive kernel estimation techniques to the cover-
age estimation problem in an environment which reacts
to observation. To the author’s knowledge, this approach
has not been tried in the published literature.

B. Related work

In [5] a kernel based Gaussian Regression approach
to coverage estimation is proposed by Carron et al.,
where a centralized base station computes trajectories
for each agent based on the measurement history up to
the current time. The data dimension grows linearly with
time i and hence the complexity grows as O(i3) due to a
matrix inversion of the problem data. Nodes are directed
toward areas with higher variance in order to improve
the estimate. Once the maximum estimate variance is
below a predeﬁned threshold, the algorithm switches to
a partitioning algorithm, which assumes the environment
is static and moves agents to a centroidal Voronoi par-
tition (a well-known optimal coverage conﬁguration for
distance-based problems, [6]). The algorithm in [5] is not
well suited for on-line learning tasks since it does not
provide the network with the ability to switch back from
this exploitation state to the former exploration state,
and the complexity of the exploration algorithm grows
as O(i3).

A distributed, parametric and non-parametric ap-
proach to the same problem is proposed in [7]. The

*Department of Electrical Engineering, Stanford University.
Project advised by Dr. Marco Pavone. This work was funded in
part by NSF grant DGE-114747. stefantj@stanford.edu

statistical model used is more general than [5] because
they consider distributions which are not Gaussian Ran-
dom ﬁelds. Starting from an optimal centralized policy,
the authors develop a distributed approximation by as-
suming that the agent locations are all independently
identically distributed and using a consensus ﬁlter to
explicitly evaluate the kernel based feature vector for
neighboring measurements (note that this precludes an
implicit inﬁnite dimensional feature vector). Only the
one-shot estimation case is considered, so the active
sensing question of how to move sensors is not addressed.
A method for choosing the most signiﬁcant features is
proposed, and it is claimed that this computation can be
run online. However, given the weak assumptions made
on the ad-hoc network, it is non-trivial to guarantee
that at any given time, the agents have agreed on the
basis functions (and ordering) to use in the algorithm.
Without this guarantee, the inner-product evaluations
that the algorithm relies on are not meaningful. If the
‘important’ features are chosen ahead of time, the kernel
method reduces to a parametric method similar to those
described in [8].

C. Motivation of proposed work

functions to estimate general

The approach to active sensing of a scalar ﬁeld taken in
this project is inspired by the non-parametric methods
used in [5]. Parametric methods suﬀer from the same
fundamental problem of linear estimators: due to the
ﬁnite number of basis functions used to parametrize
the space, parametric methods require a large number
of
functions well (and
there will always be a pathological case which is poorly
estimated). In problems where prior knowledge is not
available or where the statistics may change signiﬁ-
cantly, nonparametric kernel based methods provide an
adaptive, general learning framework. They still require
knowledge of the expected ‘smoothness’ via the kernel
bandwidth.

D. Statement of work

This work examines the coverage estimation problem
with kernel adaptive ﬁlters, which kernelize familiar lin-
ear estimators. In particular, kernel least-mean squares
(KLMS) and kernel recursive least squares (KRLS) are
used. Derivations and variants of the KLMS and KRLS
algorithm are summarized in [9]. The goal of this project
is to provide an on-line, kernel based approach to the
problem of maximizing the minimum value of time-
varying, reactive ﬁelds.

II. Problem formulation

A. Model

Assume we have N mobile sensors, and a central base
station. We denote measurement times by the index i.
n in a polygon X , and takes
Agent n has position xi
n) ⊂ X . The mea-
measurements over a footprint B(xi
surements follow the model

yi,n(x) = f (x, T (x, i), i) + νi,n

for x ∈ B(xi
is sensor noise and T (x, i) is given by

n), where i is the time of measurement, νi,n

N(cid:88)

i(cid:88)

n=1

j=0

T (x, i) =

1
i

1{x ∈ B(xj

n)}

which is the fraction of total time that the point x has
been observed up to time i. In this context, 1(·) is the
indicator function. If multiple agents observe the same
location, this fraction can be greater than 1 (though
not greater than N). Agents move according to noise-less
integrator dynamics,

n = xi−1
xi

n + ui
n

B. System requirements

We have the following communication and computa-

tion requirements:
(C1) agent n can identify itself to the base station and

(C2) agent n can measure: y(x) = f (x, T (x, i), i) + vi,

can send information to the base station;
over a region B(xi

n).

(C3) agent n can measure its position xi
n

The base station must be able to:

(C4) store measurements taken at time a given time step
by all agents, in addition to the current estimates
of f

(C5) compute partitions on X
(C6) send information to each robot

The goal of the network is to develop a observa-
tion policy T (x, i) to maximize the minimum value of
f (x, T (x, i), i).

C. Example problems

1) Scalar ﬁeld estimation: This applies immediately,
where we take f to be the time-varying error in mea-
surement.

p(x)

2) Sweeping and Persistent robotic tasks: This prob-
lem is proposed in [10], where they describe the ﬁeld by
the diﬀerential equation

˙f (x) =

p(x) − c(x)
0

if x (cid:54)∈ ∪N
if x ∈ ∪N
if x ∈ ∪N

n=1B(xn)
n=1B(xn) and f (x) > 0
n=1B(xn) and f (x) = 0
Integrating, we ﬁnd that (assuming that p(x) and c(x)
are independent of time)
f (x, i) = ip(x) + f (x, 0) − T (x, t)c(x) = f (x, T (x, i), i).

Their policy seeks to minimize the maximum of the
ﬁeld values, which ﬁts out model by a simple change of
variables.
3) Border patrol and intruder detection: Assume that
there are intruders trying to penetrate a region X unde-
tected. Their risk of failure is dictated by environmental
factors and being caught by a patrol. Intruders have ac-
cess to the patrol frequency T (x, i) along the region, but
not to real time position information of patrols. At every
time step i, the intruder examines the environment and
frequency of patrols, and compares it to a risk threshold
τi. We also assume that intruders are impatient, so their
risk threshold increases over time: τi > τi−1.

Patrolling agents move to a location and wait until
they see an intruder or grow impatient, then move on in
order to maximize the minimum intrusion time over X .
In a game theoretic context maximizing the minimum is
the Nash Equilibrium, since to choose any other policy
will reduce the patrolling agents value function (by def-
inition), and the intruders cannot unilaterally increase
their value function (to get in quickly).

We can formulate this problem as maximizing the
minimum of a ﬁeld representing the risk of failure (for
intruders) over the patrol frequencies T (x, i). Explicitly
constructing this problem analytically typically requires
strong statistical assumptions (such as are used in multi-
armed bandit problems), but even without those we can
say that this is a problem of maximizing the minimum of
some function of space, time and average coverage time.
Thus, this ﬁts the model from Section II-A.

III. Proposed Solution

Our proposed solution is outlined in algorithm 1, which
is very similar to [5], with two major diﬀerences: an
adaptive kernel estimator is used in place of Gaussian
Regression, and the ﬁeld is assumed to respond to obser-
vation. Coverage is achieved by running a range-limited
version of Lloyd’s clustering algorithm [11]: each agent
ﬁnds the centroid of a reward function based on f over
all points nearest to itself and travels there. In a static
environment, this algorithm will eventually converge to
the centroidal Voronoi partition, which is the solution to
many locational optimization problems. In our case, the
ﬁeld is time varying, so the locations of agents will not
converge. Instead, we look for convergence of the smallest
ﬁeld values.

The methods

estimator.update(u, d) and d =
estimator.predict d(u), and r = estimator.predict r(u)
are described in [9] for KLMS and KRLS, but are not
repeated here for brevity. The argument u is the mea-
surement vector, d is the scalar output of the model (or
observation) and r is the variance of the estimate d.

The method moveReward(map, xi

j) only considers the
points which are closest to agent j (e.g., in its Voronoi
partition). The method reward(d, r) is described next.

minimization function from the multi-armed bandit
problem.

IV. Simulations

Algorithm 1 was implemented in C in order to allow
reasonable simulation time for modest size problems.
Numerous problem settings were explored, with various
environments and reward functions. In the following,
only the most recent results are presented as they most
clearly express the properties of our algorithm.

A. Problem set-up

The equation used by intruders to assess diﬃculty at

point x is

D(x) = 10(prior(x) + T (x, i)).

The prior function used for this problem is of the form
sin(x) sin(y), and is shown in Fig. 1. Note the severe non-
convexity of the prior, with two distinct local minima
regions. The intruder impatience starts at 0.1 and grows
by 1 with each time step, and an event occurs when
the impatience at x exceeds D(x) (impatience resets on
event). A Gaussian kernel was chosen with bandwidth
parameter 10. Agents have impatience of 10 (if nothing
is seen for 10 iterations, they move on).

Simulations are run over a 50x50 grid with 3 agents
that can see manhattan distance 3. This means that for
every time step, 75 of 2500 grid points are observed (3%
of the region).

Algorithm 1: Coverage Estimation

for i > 1 do

for x ∈ X do

d = estimator.predictd([x,T (x, i),i])
r = estimator.predictr([x,T (x, i),i])
map(x) = reward(d, r)

end for
p = partition(map)
c = centroids(p)
for j ← 1, N do

if (event or timeout) then

j, i), i], y(xi

j))

j, T (xi

estimator.update([xi
j ← moveReward(map, xi
xi+1
j)
j ← xi
xi+1
end if

else

j

end for

end for

A. Reward Function Selection

The behavior of the algorithm will depend greatly on
the choice of reward function reward(d, r), which drives
agent motion. Three functions were considered in this
project:
1) reward(d, r) = exp(−d): This method directly
rewards traveling to areas with short intrusion times (low
risk). The big downside is that there is no beneﬁt given to
exploration, which results in poor performance of regions
which contain many local minima.
2) reward(d, r) = (b−d)2/(r2+(b−d)2): This function
comes from the Cantelli probability inequality which
states that for a positive, real random variable x with
mean µ and variance σ2, Pr(x − µ ≥ a) ≤ σ2/(σ2 + a2).
Switching then probability and substituting b = a + µ
yields:

Pr(x ≤ b) ≥ (b − µ)2

σ2 + (b − µ)2 .

This choice of reward then corresponds to a conservative
policy which minimizes risk. This runs into a similar
problem as with the direct method, because when the
variance r is large, the lower bound tends to 0. This is
addressed in the next function.

3) reward(d, r) = −d +(cid:112)2r log(i)/s + 3r log(i)/s:

This is based on a function developed in [12] based
on a multi-armed bandit formulation. The multi-armed
bandit problem is the idealized coverage-estimation prob-
lem: everything is independent, identically distributed,
Markov, and time-invariant. A regret bound is also given
in [12]. Here s = iT (x, i) is the number of times x has
been sampled and b is the support of the reward function
(in this formulation, the impatience of the patrolling
agents). This approach tries to balance exploration and
exploitation in a meaningful way.

The ﬁrst two methods do not explore well. Unless
otherwise speciﬁed, reward(d, r) is taken as the regret-

Fig. 1. Prior ﬁeld diﬃculty

B. Desired results

Intuitively, we should see agents focusing their eﬀorts
in the upper-left and lower-right corners, to compensate
for the relatively easy environment there. Fig. 2 shows
the diﬀerence between the initial and ﬁnal diﬃculty
function for the KRLS implementation. Qualitatively,
the patrols are taking the right action and we quantify
how good their responses were below.

01020304050607080901000102030405060708090100  51015202530On-line kernel learning for active sensor networks

Stefan Jorgensen*

I. Introduction

In this algorithmic project we consider how to leverage
increasingly capable mobile sensor networks to simulta-
neously exploit a changing environment and maintain
low uncertainty about the previously explored regions.
This problem, known as ‘coverage-estimation’ or ‘active
sensing’ in the literature, has applications in many ﬁelds:
tracking interesting oceanographic or atmospheric data
using a robotic network, ﬁnding and clearing environ-
mental hazards such as pollutants [1] or radioactive
waste [2], searching for humans in wreckage [3], and even
game theoretical formulations such as the multi-armed
bandit problem [4]. Much of the literature focuses on
either the estimation problem of exploring an unknown
environment or the coverage problem of exploiting a
known environment.

A. Contribution

The main contribution of this project is to apply on-
line adaptive kernel estimation techniques to the cover-
age estimation problem in an environment which reacts
to observation. To the author’s knowledge, this approach
has not been tried in the published literature.

B. Related work

In [5] a kernel based Gaussian Regression approach
to coverage estimation is proposed by Carron et al.,
where a centralized base station computes trajectories
for each agent based on the measurement history up to
the current time. The data dimension grows linearly with
time i and hence the complexity grows as O(i3) due to a
matrix inversion of the problem data. Nodes are directed
toward areas with higher variance in order to improve
the estimate. Once the maximum estimate variance is
below a predeﬁned threshold, the algorithm switches to
a partitioning algorithm, which assumes the environment
is static and moves agents to a centroidal Voronoi par-
tition (a well-known optimal coverage conﬁguration for
distance-based problems, [6]). The algorithm in [5] is not
well suited for on-line learning tasks since it does not
provide the network with the ability to switch back from
this exploitation state to the former exploration state,
and the complexity of the exploration algorithm grows
as O(i3).

A distributed, parametric and non-parametric ap-
proach to the same problem is proposed in [7]. The

*Department of Electrical Engineering, Stanford University.
Project advised by Dr. Marco Pavone. This work was funded in
part by NSF grant DGE-114747. stefantj@stanford.edu

statistical model used is more general than [5] because
they consider distributions which are not Gaussian Ran-
dom ﬁelds. Starting from an optimal centralized policy,
the authors develop a distributed approximation by as-
suming that the agent locations are all independently
identically distributed and using a consensus ﬁlter to
explicitly evaluate the kernel based feature vector for
neighboring measurements (note that this precludes an
implicit inﬁnite dimensional feature vector). Only the
one-shot estimation case is considered, so the active
sensing question of how to move sensors is not addressed.
A method for choosing the most signiﬁcant features is
proposed, and it is claimed that this computation can be
run online. However, given the weak assumptions made
on the ad-hoc network, it is non-trivial to guarantee
that at any given time, the agents have agreed on the
basis functions (and ordering) to use in the algorithm.
Without this guarantee, the inner-product evaluations
that the algorithm relies on are not meaningful. If the
‘important’ features are chosen ahead of time, the kernel
method reduces to a parametric method similar to those
described in [8].

C. Motivation of proposed work

functions to estimate general

The approach to active sensing of a scalar ﬁeld taken in
this project is inspired by the non-parametric methods
used in [5]. Parametric methods suﬀer from the same
fundamental problem of linear estimators: due to the
ﬁnite number of basis functions used to parametrize
the space, parametric methods require a large number
of
functions well (and
there will always be a pathological case which is poorly
estimated). In problems where prior knowledge is not
available or where the statistics may change signiﬁ-
cantly, nonparametric kernel based methods provide an
adaptive, general learning framework. They still require
knowledge of the expected ‘smoothness’ via the kernel
bandwidth.

D. Statement of work

This work examines the coverage estimation problem
with kernel adaptive ﬁlters, which kernelize familiar lin-
ear estimators. In particular, kernel least-mean squares
(KLMS) and kernel recursive least squares (KRLS) are
used. Derivations and variants of the KLMS and KRLS
algorithm are summarized in [9]. The goal of this project
is to provide an on-line, kernel based approach to the
problem of maximizing the minimum value of time-
varying, reactive ﬁelds.

II. Problem formulation

A. Model

Assume we have N mobile sensors, and a central base
station. We denote measurement times by the index i.
n in a polygon X , and takes
Agent n has position xi
n) ⊂ X . The mea-
measurements over a footprint B(xi
surements follow the model

yi,n(x) = f (x, T (x, i), i) + νi,n

for x ∈ B(xi
is sensor noise and T (x, i) is given by

n), where i is the time of measurement, νi,n

N(cid:88)

i(cid:88)

n=1

j=0

T (x, i) =

1
i

1{x ∈ B(xj

n)}

which is the fraction of total time that the point x has
been observed up to time i. In this context, 1(·) is the
indicator function. If multiple agents observe the same
location, this fraction can be greater than 1 (though
not greater than N). Agents move according to noise-less
integrator dynamics,

n = xi−1
xi

n + ui
n

B. System requirements

We have the following communication and computa-

tion requirements:
(C1) agent n can identify itself to the base station and

(C2) agent n can measure: y(x) = f (x, T (x, i), i) + vi,

can send information to the base station;
over a region B(xi

n).

(C3) agent n can measure its position xi
n

The base station must be able to:

(C4) store measurements taken at time a given time step
by all agents, in addition to the current estimates
of f

(C5) compute partitions on X
(C6) send information to each robot

The goal of the network is to develop a observa-
tion policy T (x, i) to maximize the minimum value of
f (x, T (x, i), i).

C. Example problems

1) Scalar ﬁeld estimation: This applies immediately,
where we take f to be the time-varying error in mea-
surement.

p(x)

2) Sweeping and Persistent robotic tasks: This prob-
lem is proposed in [10], where they describe the ﬁeld by
the diﬀerential equation

˙f (x) =

p(x) − c(x)
0

if x (cid:54)∈ ∪N
if x ∈ ∪N
if x ∈ ∪N

n=1B(xn)
n=1B(xn) and f (x) > 0
n=1B(xn) and f (x) = 0
Integrating, we ﬁnd that (assuming that p(x) and c(x)
are independent of time)
f (x, i) = ip(x) + f (x, 0) − T (x, t)c(x) = f (x, T (x, i), i).

Their policy seeks to minimize the maximum of the
ﬁeld values, which ﬁts out model by a simple change of
variables.
3) Border patrol and intruder detection: Assume that
there are intruders trying to penetrate a region X unde-
tected. Their risk of failure is dictated by environmental
factors and being caught by a patrol. Intruders have ac-
cess to the patrol frequency T (x, i) along the region, but
not to real time position information of patrols. At every
time step i, the intruder examines the environment and
frequency of patrols, and compares it to a risk threshold
τi. We also assume that intruders are impatient, so their
risk threshold increases over time: τi > τi−1.

Patrolling agents move to a location and wait until
they see an intruder or grow impatient, then move on in
order to maximize the minimum intrusion time over X .
In a game theoretic context maximizing the minimum is
the Nash Equilibrium, since to choose any other policy
will reduce the patrolling agents value function (by def-
inition), and the intruders cannot unilaterally increase
their value function (to get in quickly).

We can formulate this problem as maximizing the
minimum of a ﬁeld representing the risk of failure (for
intruders) over the patrol frequencies T (x, i). Explicitly
constructing this problem analytically typically requires
strong statistical assumptions (such as are used in multi-
armed bandit problems), but even without those we can
say that this is a problem of maximizing the minimum of
some function of space, time and average coverage time.
Thus, this ﬁts the model from Section II-A.

III. Proposed Solution

Our proposed solution is outlined in algorithm 1, which
is very similar to [5], with two major diﬀerences: an
adaptive kernel estimator is used in place of Gaussian
Regression, and the ﬁeld is assumed to respond to obser-
vation. Coverage is achieved by running a range-limited
version of Lloyd’s clustering algorithm [11]: each agent
ﬁnds the centroid of a reward function based on f over
all points nearest to itself and travels there. In a static
environment, this algorithm will eventually converge to
the centroidal Voronoi partition, which is the solution to
many locational optimization problems. In our case, the
ﬁeld is time varying, so the locations of agents will not
converge. Instead, we look for convergence of the smallest
ﬁeld values.

The methods

estimator.update(u, d) and d =
estimator.predict d(u), and r = estimator.predict r(u)
are described in [9] for KLMS and KRLS, but are not
repeated here for brevity. The argument u is the mea-
surement vector, d is the scalar output of the model (or
observation) and r is the variance of the estimate d.

The method moveReward(map, xi

j) only considers the
points which are closest to agent j (e.g., in its Voronoi
partition). The method reward(d, r) is described next.

minimization function from the multi-armed bandit
problem.

IV. Simulations

Algorithm 1 was implemented in C in order to allow
reasonable simulation time for modest size problems.
Numerous problem settings were explored, with various
environments and reward functions. In the following,
only the most recent results are presented as they most
clearly express the properties of our algorithm.

A. Problem set-up

The equation used by intruders to assess diﬃculty at

point x is

D(x) = 10(prior(x) + T (x, i)).

The prior function used for this problem is of the form
sin(x) sin(y), and is shown in Fig. 1. Note the severe non-
convexity of the prior, with two distinct local minima
regions. The intruder impatience starts at 0.1 and grows
by 1 with each time step, and an event occurs when
the impatience at x exceeds D(x) (impatience resets on
event). A Gaussian kernel was chosen with bandwidth
parameter 10. Agents have impatience of 10 (if nothing
is seen for 10 iterations, they move on).

Simulations are run over a 50x50 grid with 3 agents
that can see manhattan distance 3. This means that for
every time step, 75 of 2500 grid points are observed (3%
of the region).

Algorithm 1: Coverage Estimation

for i > 1 do

for x ∈ X do

d = estimator.predictd([x,T (x, i),i])
r = estimator.predictr([x,T (x, i),i])
map(x) = reward(d, r)

end for
p = partition(map)
c = centroids(p)
for j ← 1, N do

if (event or timeout) then

j, i), i], y(xi

j))

j, T (xi

estimator.update([xi
j ← moveReward(map, xi
xi+1
j)
j ← xi
xi+1
end if

else

j

end for

end for

A. Reward Function Selection

The behavior of the algorithm will depend greatly on
the choice of reward function reward(d, r), which drives
agent motion. Three functions were considered in this
project:
1) reward(d, r) = exp(−d): This method directly
rewards traveling to areas with short intrusion times (low
risk). The big downside is that there is no beneﬁt given to
exploration, which results in poor performance of regions
which contain many local minima.
2) reward(d, r) = (b−d)2/(r2+(b−d)2): This function
comes from the Cantelli probability inequality which
states that for a positive, real random variable x with
mean µ and variance σ2, Pr(x − µ ≥ a) ≤ σ2/(σ2 + a2).
Switching then probability and substituting b = a + µ
yields:

Pr(x ≤ b) ≥ (b − µ)2

σ2 + (b − µ)2 .

This choice of reward then corresponds to a conservative
policy which minimizes risk. This runs into a similar
problem as with the direct method, because when the
variance r is large, the lower bound tends to 0. This is
addressed in the next function.

3) reward(d, r) = −d +(cid:112)2r log(i)/s + 3r log(i)/s:

This is based on a function developed in [12] based
on a multi-armed bandit formulation. The multi-armed
bandit problem is the idealized coverage-estimation prob-
lem: everything is independent, identically distributed,
Markov, and time-invariant. A regret bound is also given
in [12]. Here s = iT (x, i) is the number of times x has
been sampled and b is the support of the reward function
(in this formulation, the impatience of the patrolling
agents). This approach tries to balance exploration and
exploitation in a meaningful way.

The ﬁrst two methods do not explore well. Unless
otherwise speciﬁed, reward(d, r) is taken as the regret-

Fig. 1. Prior ﬁeld diﬃculty

B. Desired results

Intuitively, we should see agents focusing their eﬀorts
in the upper-left and lower-right corners, to compensate
for the relatively easy environment there. Fig. 2 shows
the diﬀerence between the initial and ﬁnal diﬃculty
function for the KRLS implementation. Qualitatively,
the patrols are taking the right action and we quantify
how good their responses were below.

01020304050607080901000102030405060708090100  51015202530Fig. 2. Change in perceived diﬃculty over 1230 iterations, single
KRLS trial

Fig. 3. KLMS results, averaged over 180 trials

An upper bound for the maximum minimum ﬁeld value
was computed using a centralized, omniscient controller
with no constraints on how to apply patrol eﬀort. In
particular, frequency allocation at a given time step can
be scattered and is not restricted to points within a given
manhattan distance of the patrolling agents.

Ideally, the minimum ﬁeld value will approach this
upper bound as time goes to inﬁnity. From the problem
structure, we also expect the variance of the ﬁeld to
decrease over time. If there are suﬃcient agents, then
the diﬃculty ﬁeld could theoretically be ﬂattened. In
our case, we only consider a sparse patrol scheme so we
simply look for decreasing variance over time. We expect
a transient at the beginning while agents are exploring
the region.

C. KLMS results

The minimum and variance of the ﬁeld for the KLMS
algorithm are shown in Fig. 3. The curves were generated
by averaging the results of 180 runs of 10,000 iterations.
Clearly, the algorithm is not able to generate a coherent
policy, since the ﬁeld variance settles near its initial value.
This implies that the algorithm is simply generating
random walk trajectories, which is veriﬁed by looking at
the change in perceived diﬃculty for a single trial (shown
in Fig. 4). From this we can conclude that the random
walk trajectory is able to accomplish approximately 60%
of the optimal maximum minimum ﬁeld value.

Our best explanation for the poor performance of
KLMS is that the algorithm is not able to learn ‘quickly
enough’ for the given problem dynamics, since in some
more pedantic environments KLMS performed more rea-
sonably. This lack of generalizability is a serious problem,
and even though KLMS has linear time complexity (ver-
sus O(i2) for KRLS), it is not suitable for our application.

Fig. 4. Change in perceived diﬃculty over 10,000 iterations, single
KLMS trial

D. KRLS results

The minimum and variance of the ﬁeld for the KRLS
algorithm are shown in Fig. 5. The curves were generated
by averaging the results of 30 runs of 3,000 iterations.
The reduced number of
iterations is because of the
signiﬁcant increase in complexity for KRLS. After ap-
proximately 250 iterations, the agents (implicitly) decide
on the locations of interest and begin to focus their eﬀorts
there. After 3000 iterations, the variance reduces by
36% from its initial value, and the maximum minimum
achieves 83.5% of the upper bound. Given the very
minimal modeling and tuning of this problem this is
impressive.

Sparsiﬁcation techniques such as the ones outlined in
[9] were implemented, but turned out to be very sensitive.
A 10% diﬀerence in threshold value switched behavior
from overly aggressive (never accepting a new point) to
useless (accepting all points). Due to this sensitivity, it
is not practical to use for a generic problem scenario.

Table 1 summarizes the simulation results.

01020304050607080901000102030405060708090100  12345678100020003000400050006000700080009000100000.511.522.5IterationNormalized Values  MinimumStandard deviationOn-line kernel learning for active sensor networks

Stefan Jorgensen*

I. Introduction

In this algorithmic project we consider how to leverage
increasingly capable mobile sensor networks to simulta-
neously exploit a changing environment and maintain
low uncertainty about the previously explored regions.
This problem, known as ‘coverage-estimation’ or ‘active
sensing’ in the literature, has applications in many ﬁelds:
tracking interesting oceanographic or atmospheric data
using a robotic network, ﬁnding and clearing environ-
mental hazards such as pollutants [1] or radioactive
waste [2], searching for humans in wreckage [3], and even
game theoretical formulations such as the multi-armed
bandit problem [4]. Much of the literature focuses on
either the estimation problem of exploring an unknown
environment or the coverage problem of exploiting a
known environment.

A. Contribution

The main contribution of this project is to apply on-
line adaptive kernel estimation techniques to the cover-
age estimation problem in an environment which reacts
to observation. To the author’s knowledge, this approach
has not been tried in the published literature.

B. Related work

In [5] a kernel based Gaussian Regression approach
to coverage estimation is proposed by Carron et al.,
where a centralized base station computes trajectories
for each agent based on the measurement history up to
the current time. The data dimension grows linearly with
time i and hence the complexity grows as O(i3) due to a
matrix inversion of the problem data. Nodes are directed
toward areas with higher variance in order to improve
the estimate. Once the maximum estimate variance is
below a predeﬁned threshold, the algorithm switches to
a partitioning algorithm, which assumes the environment
is static and moves agents to a centroidal Voronoi par-
tition (a well-known optimal coverage conﬁguration for
distance-based problems, [6]). The algorithm in [5] is not
well suited for on-line learning tasks since it does not
provide the network with the ability to switch back from
this exploitation state to the former exploration state,
and the complexity of the exploration algorithm grows
as O(i3).

A distributed, parametric and non-parametric ap-
proach to the same problem is proposed in [7]. The

*Department of Electrical Engineering, Stanford University.
Project advised by Dr. Marco Pavone. This work was funded in
part by NSF grant DGE-114747. stefantj@stanford.edu

statistical model used is more general than [5] because
they consider distributions which are not Gaussian Ran-
dom ﬁelds. Starting from an optimal centralized policy,
the authors develop a distributed approximation by as-
suming that the agent locations are all independently
identically distributed and using a consensus ﬁlter to
explicitly evaluate the kernel based feature vector for
neighboring measurements (note that this precludes an
implicit inﬁnite dimensional feature vector). Only the
one-shot estimation case is considered, so the active
sensing question of how to move sensors is not addressed.
A method for choosing the most signiﬁcant features is
proposed, and it is claimed that this computation can be
run online. However, given the weak assumptions made
on the ad-hoc network, it is non-trivial to guarantee
that at any given time, the agents have agreed on the
basis functions (and ordering) to use in the algorithm.
Without this guarantee, the inner-product evaluations
that the algorithm relies on are not meaningful. If the
‘important’ features are chosen ahead of time, the kernel
method reduces to a parametric method similar to those
described in [8].

C. Motivation of proposed work

functions to estimate general

The approach to active sensing of a scalar ﬁeld taken in
this project is inspired by the non-parametric methods
used in [5]. Parametric methods suﬀer from the same
fundamental problem of linear estimators: due to the
ﬁnite number of basis functions used to parametrize
the space, parametric methods require a large number
of
functions well (and
there will always be a pathological case which is poorly
estimated). In problems where prior knowledge is not
available or where the statistics may change signiﬁ-
cantly, nonparametric kernel based methods provide an
adaptive, general learning framework. They still require
knowledge of the expected ‘smoothness’ via the kernel
bandwidth.

D. Statement of work

This work examines the coverage estimation problem
with kernel adaptive ﬁlters, which kernelize familiar lin-
ear estimators. In particular, kernel least-mean squares
(KLMS) and kernel recursive least squares (KRLS) are
used. Derivations and variants of the KLMS and KRLS
algorithm are summarized in [9]. The goal of this project
is to provide an on-line, kernel based approach to the
problem of maximizing the minimum value of time-
varying, reactive ﬁelds.

II. Problem formulation

A. Model

Assume we have N mobile sensors, and a central base
station. We denote measurement times by the index i.
n in a polygon X , and takes
Agent n has position xi
n) ⊂ X . The mea-
measurements over a footprint B(xi
surements follow the model

yi,n(x) = f (x, T (x, i), i) + νi,n

for x ∈ B(xi
is sensor noise and T (x, i) is given by

n), where i is the time of measurement, νi,n

N(cid:88)

i(cid:88)

n=1

j=0

T (x, i) =

1
i

1{x ∈ B(xj

n)}

which is the fraction of total time that the point x has
been observed up to time i. In this context, 1(·) is the
indicator function. If multiple agents observe the same
location, this fraction can be greater than 1 (though
not greater than N). Agents move according to noise-less
integrator dynamics,

n = xi−1
xi

n + ui
n

B. System requirements

We have the following communication and computa-

tion requirements:
(C1) agent n can identify itself to the base station and

(C2) agent n can measure: y(x) = f (x, T (x, i), i) + vi,

can send information to the base station;
over a region B(xi

n).

(C3) agent n can measure its position xi
n

The base station must be able to:

(C4) store measurements taken at time a given time step
by all agents, in addition to the current estimates
of f

(C5) compute partitions on X
(C6) send information to each robot

The goal of the network is to develop a observa-
tion policy T (x, i) to maximize the minimum value of
f (x, T (x, i), i).

C. Example problems

1) Scalar ﬁeld estimation: This applies immediately,
where we take f to be the time-varying error in mea-
surement.

p(x)

2) Sweeping and Persistent robotic tasks: This prob-
lem is proposed in [10], where they describe the ﬁeld by
the diﬀerential equation

˙f (x) =

p(x) − c(x)
0

if x (cid:54)∈ ∪N
if x ∈ ∪N
if x ∈ ∪N

n=1B(xn)
n=1B(xn) and f (x) > 0
n=1B(xn) and f (x) = 0
Integrating, we ﬁnd that (assuming that p(x) and c(x)
are independent of time)
f (x, i) = ip(x) + f (x, 0) − T (x, t)c(x) = f (x, T (x, i), i).

Their policy seeks to minimize the maximum of the
ﬁeld values, which ﬁts out model by a simple change of
variables.
3) Border patrol and intruder detection: Assume that
there are intruders trying to penetrate a region X unde-
tected. Their risk of failure is dictated by environmental
factors and being caught by a patrol. Intruders have ac-
cess to the patrol frequency T (x, i) along the region, but
not to real time position information of patrols. At every
time step i, the intruder examines the environment and
frequency of patrols, and compares it to a risk threshold
τi. We also assume that intruders are impatient, so their
risk threshold increases over time: τi > τi−1.

Patrolling agents move to a location and wait until
they see an intruder or grow impatient, then move on in
order to maximize the minimum intrusion time over X .
In a game theoretic context maximizing the minimum is
the Nash Equilibrium, since to choose any other policy
will reduce the patrolling agents value function (by def-
inition), and the intruders cannot unilaterally increase
their value function (to get in quickly).

We can formulate this problem as maximizing the
minimum of a ﬁeld representing the risk of failure (for
intruders) over the patrol frequencies T (x, i). Explicitly
constructing this problem analytically typically requires
strong statistical assumptions (such as are used in multi-
armed bandit problems), but even without those we can
say that this is a problem of maximizing the minimum of
some function of space, time and average coverage time.
Thus, this ﬁts the model from Section II-A.

III. Proposed Solution

Our proposed solution is outlined in algorithm 1, which
is very similar to [5], with two major diﬀerences: an
adaptive kernel estimator is used in place of Gaussian
Regression, and the ﬁeld is assumed to respond to obser-
vation. Coverage is achieved by running a range-limited
version of Lloyd’s clustering algorithm [11]: each agent
ﬁnds the centroid of a reward function based on f over
all points nearest to itself and travels there. In a static
environment, this algorithm will eventually converge to
the centroidal Voronoi partition, which is the solution to
many locational optimization problems. In our case, the
ﬁeld is time varying, so the locations of agents will not
converge. Instead, we look for convergence of the smallest
ﬁeld values.

The methods

estimator.update(u, d) and d =
estimator.predict d(u), and r = estimator.predict r(u)
are described in [9] for KLMS and KRLS, but are not
repeated here for brevity. The argument u is the mea-
surement vector, d is the scalar output of the model (or
observation) and r is the variance of the estimate d.

The method moveReward(map, xi

j) only considers the
points which are closest to agent j (e.g., in its Voronoi
partition). The method reward(d, r) is described next.

minimization function from the multi-armed bandit
problem.

IV. Simulations

Algorithm 1 was implemented in C in order to allow
reasonable simulation time for modest size problems.
Numerous problem settings were explored, with various
environments and reward functions. In the following,
only the most recent results are presented as they most
clearly express the properties of our algorithm.

A. Problem set-up

The equation used by intruders to assess diﬃculty at

point x is

D(x) = 10(prior(x) + T (x, i)).

The prior function used for this problem is of the form
sin(x) sin(y), and is shown in Fig. 1. Note the severe non-
convexity of the prior, with two distinct local minima
regions. The intruder impatience starts at 0.1 and grows
by 1 with each time step, and an event occurs when
the impatience at x exceeds D(x) (impatience resets on
event). A Gaussian kernel was chosen with bandwidth
parameter 10. Agents have impatience of 10 (if nothing
is seen for 10 iterations, they move on).

Simulations are run over a 50x50 grid with 3 agents
that can see manhattan distance 3. This means that for
every time step, 75 of 2500 grid points are observed (3%
of the region).

Algorithm 1: Coverage Estimation

for i > 1 do

for x ∈ X do

d = estimator.predictd([x,T (x, i),i])
r = estimator.predictr([x,T (x, i),i])
map(x) = reward(d, r)

end for
p = partition(map)
c = centroids(p)
for j ← 1, N do

if (event or timeout) then

j, i), i], y(xi

j))

j, T (xi

estimator.update([xi
j ← moveReward(map, xi
xi+1
j)
j ← xi
xi+1
end if

else

j

end for

end for

A. Reward Function Selection

The behavior of the algorithm will depend greatly on
the choice of reward function reward(d, r), which drives
agent motion. Three functions were considered in this
project:
1) reward(d, r) = exp(−d): This method directly
rewards traveling to areas with short intrusion times (low
risk). The big downside is that there is no beneﬁt given to
exploration, which results in poor performance of regions
which contain many local minima.
2) reward(d, r) = (b−d)2/(r2+(b−d)2): This function
comes from the Cantelli probability inequality which
states that for a positive, real random variable x with
mean µ and variance σ2, Pr(x − µ ≥ a) ≤ σ2/(σ2 + a2).
Switching then probability and substituting b = a + µ
yields:

Pr(x ≤ b) ≥ (b − µ)2

σ2 + (b − µ)2 .

This choice of reward then corresponds to a conservative
policy which minimizes risk. This runs into a similar
problem as with the direct method, because when the
variance r is large, the lower bound tends to 0. This is
addressed in the next function.

3) reward(d, r) = −d +(cid:112)2r log(i)/s + 3r log(i)/s:

This is based on a function developed in [12] based
on a multi-armed bandit formulation. The multi-armed
bandit problem is the idealized coverage-estimation prob-
lem: everything is independent, identically distributed,
Markov, and time-invariant. A regret bound is also given
in [12]. Here s = iT (x, i) is the number of times x has
been sampled and b is the support of the reward function
(in this formulation, the impatience of the patrolling
agents). This approach tries to balance exploration and
exploitation in a meaningful way.

The ﬁrst two methods do not explore well. Unless
otherwise speciﬁed, reward(d, r) is taken as the regret-

Fig. 1. Prior ﬁeld diﬃculty

B. Desired results

Intuitively, we should see agents focusing their eﬀorts
in the upper-left and lower-right corners, to compensate
for the relatively easy environment there. Fig. 2 shows
the diﬀerence between the initial and ﬁnal diﬃculty
function for the KRLS implementation. Qualitatively,
the patrols are taking the right action and we quantify
how good their responses were below.

01020304050607080901000102030405060708090100  51015202530Fig. 2. Change in perceived diﬃculty over 1230 iterations, single
KRLS trial

Fig. 3. KLMS results, averaged over 180 trials

An upper bound for the maximum minimum ﬁeld value
was computed using a centralized, omniscient controller
with no constraints on how to apply patrol eﬀort. In
particular, frequency allocation at a given time step can
be scattered and is not restricted to points within a given
manhattan distance of the patrolling agents.

Ideally, the minimum ﬁeld value will approach this
upper bound as time goes to inﬁnity. From the problem
structure, we also expect the variance of the ﬁeld to
decrease over time. If there are suﬃcient agents, then
the diﬃculty ﬁeld could theoretically be ﬂattened. In
our case, we only consider a sparse patrol scheme so we
simply look for decreasing variance over time. We expect
a transient at the beginning while agents are exploring
the region.

C. KLMS results

The minimum and variance of the ﬁeld for the KLMS
algorithm are shown in Fig. 3. The curves were generated
by averaging the results of 180 runs of 10,000 iterations.
Clearly, the algorithm is not able to generate a coherent
policy, since the ﬁeld variance settles near its initial value.
This implies that the algorithm is simply generating
random walk trajectories, which is veriﬁed by looking at
the change in perceived diﬃculty for a single trial (shown
in Fig. 4). From this we can conclude that the random
walk trajectory is able to accomplish approximately 60%
of the optimal maximum minimum ﬁeld value.

Our best explanation for the poor performance of
KLMS is that the algorithm is not able to learn ‘quickly
enough’ for the given problem dynamics, since in some
more pedantic environments KLMS performed more rea-
sonably. This lack of generalizability is a serious problem,
and even though KLMS has linear time complexity (ver-
sus O(i2) for KRLS), it is not suitable for our application.

Fig. 4. Change in perceived diﬃculty over 10,000 iterations, single
KLMS trial

D. KRLS results

The minimum and variance of the ﬁeld for the KRLS
algorithm are shown in Fig. 5. The curves were generated
by averaging the results of 30 runs of 3,000 iterations.
The reduced number of
iterations is because of the
signiﬁcant increase in complexity for KRLS. After ap-
proximately 250 iterations, the agents (implicitly) decide
on the locations of interest and begin to focus their eﬀorts
there. After 3000 iterations, the variance reduces by
36% from its initial value, and the maximum minimum
achieves 83.5% of the upper bound. Given the very
minimal modeling and tuning of this problem this is
impressive.

Sparsiﬁcation techniques such as the ones outlined in
[9] were implemented, but turned out to be very sensitive.
A 10% diﬀerence in threshold value switched behavior
from overly aggressive (never accepting a new point) to
useless (accepting all points). Due to this sensitivity, it
is not practical to use for a generic problem scenario.

Table 1 summarizes the simulation results.

01020304050607080901000102030405060708090100  12345678100020003000400050006000700080009000100000.511.522.5IterationNormalized Values  MinimumStandard deviationFuture work should focus on developing theoretical
guarantees for applying this algorithm (or a variant) to
the interactive coverage-estimation problem. Properties
such as convergence time (as related to environment
dynamics), expected regret, minimum number of agents
to achieve a given maximum minimum ﬁeld value, and
probabilistic service guarantees are derived for similar
problems and would provide much insight to this prob-
lem. There are quite a few applications in the literature
and industry which ﬁt the model that was formulated in
Section II-A, and there is the potential for combining the
results from several ﬁelds if a theoretical backing can be
developed here.

References

[1] J. Oyekan, Huosheng Hu, and Dongbing Gu. A novel bio-
inspired distributed coverage controller for pollution mon-
itoring.
In Mechatronics and Automation (ICMA), 2011
International Conference on, pages 1651–1656, Aug 2011.

[2] R. Andres Cortez, Herbert G. Tanner, and Ron Lumia. Dis-
In Tracts in Advanced

tributed robotic radiation mapping.
Robotics, pages 147–156. Springer, 2009.

[3] Vijay Kumar, D. Rus, and Sanjiv Singh. Robot and sensor
networks for ﬁrst responders. Pervasive Computing, IEEE,
3(4):24–33, Oct 2004.

[4] Jerome Le Ny, Munther Dahleh, and Eric Feron. Multi-
uav dynamic routing with partial observations using restless
bandit allocation indices.
In American Control Conference,
2008, pages 4220–4225. IEEE, 2008.

[5] Andrea Carron, Marco Todescato, Ruggero Carli, Luca Schen-
ato, and Gianluigi Pillonetto. Multi-agents adaptive estima-
tion and coverage control using gaussian regression. CoRR,
abs/1407.5807, 2014.

[6] Qiang Du, Vance Faber, and Max Gunzburger. Centroidal
SIAM

voronoi tessellations: applications and algorithms.
review, 41(4):637–676, 1999.

[7] Damiano Varagnolo, Gianluigi Pillonetto, and Luca Schen-
ato. Distributed parametric and nonparametric regression
with on-line performance bounds computation. Automatica,
48(10):2468–2481, 2012.

[8] Mac Schwager, Daniela Rus, and Jean-Jacques Slotine. De-
centralized, adaptive coverage control for networked robots.
The International Journal of Robotics Research, 28(3):357–
375, 2009.

[9] Weifeng Liu, Jose C Principe, and Simon Haykin. Kernel
Adaptive Filtering: A Comprehensive Introduction, volume 57.
John Wiley & Sons, 2011.

[10] Stephen L Smith, Mac Schwager, and Daniela Rus. Persistent
robotic tasks: Monitoring and sweeping in changing environ-
ments. Robotics, IEEE Transactions on, 28(2):410–426, 2012.
[11] Francesco Bullo, Jorge Cort´es, and Sonia Martinez. Dis-
tributed control of robotic networks: a mathematical approach
to motion coordination algorithms.
Princeton University
Press, 2009.

Fig. 5. KRLS results, averaged over 30 trials

Metric (normalized) KLMS KRLS

Ideal

Minimum
Variance

0.6331
0.9651

0.8355
0.6423

1.0
0.0

TABLE I

Summary of results

V. Conclusions

This project applied on-line adaptive kernel estimation
techniques to the coverage estimation problem in an
environment which reacts to observation. This approach
has not been tried in the literature, and for KRLS there
is potential for success. The KLMS approach suﬀered
from sensitivity to problem dynamics, the speciﬁcs of
which should be investigated in future work. The KRLS
approach was able to get within 83.5% of an upper bound
of the solution, which is good considering that the bound
is not particularly tight.

The results are particularly impressive given then non-
convexity of the problem and existence of local minima.
The three agents learn to divide their time among the two
local minima, crossing a less desirable region occasionally
to balance eﬀort. The main drawback from using KRLS
is its need for many samples to learn, and quadratic
complexity in the number of samples used. Sparsiﬁcation
techniques were tried, but proved too diﬃcult to tune for
general applications.

0500100015002000250030000.511.522.5IterationNormalized Values  MinimumStandard deviationOn-line kernel learning for active sensor networks

Stefan Jorgensen*

I. Introduction

In this algorithmic project we consider how to leverage
increasingly capable mobile sensor networks to simulta-
neously exploit a changing environment and maintain
low uncertainty about the previously explored regions.
This problem, known as ‘coverage-estimation’ or ‘active
sensing’ in the literature, has applications in many ﬁelds:
tracking interesting oceanographic or atmospheric data
using a robotic network, ﬁnding and clearing environ-
mental hazards such as pollutants [1] or radioactive
waste [2], searching for humans in wreckage [3], and even
game theoretical formulations such as the multi-armed
bandit problem [4]. Much of the literature focuses on
either the estimation problem of exploring an unknown
environment or the coverage problem of exploiting a
known environment.

A. Contribution

The main contribution of this project is to apply on-
line adaptive kernel estimation techniques to the cover-
age estimation problem in an environment which reacts
to observation. To the author’s knowledge, this approach
has not been tried in the published literature.

B. Related work

In [5] a kernel based Gaussian Regression approach
to coverage estimation is proposed by Carron et al.,
where a centralized base station computes trajectories
for each agent based on the measurement history up to
the current time. The data dimension grows linearly with
time i and hence the complexity grows as O(i3) due to a
matrix inversion of the problem data. Nodes are directed
toward areas with higher variance in order to improve
the estimate. Once the maximum estimate variance is
below a predeﬁned threshold, the algorithm switches to
a partitioning algorithm, which assumes the environment
is static and moves agents to a centroidal Voronoi par-
tition (a well-known optimal coverage conﬁguration for
distance-based problems, [6]). The algorithm in [5] is not
well suited for on-line learning tasks since it does not
provide the network with the ability to switch back from
this exploitation state to the former exploration state,
and the complexity of the exploration algorithm grows
as O(i3).

A distributed, parametric and non-parametric ap-
proach to the same problem is proposed in [7]. The

*Department of Electrical Engineering, Stanford University.
Project advised by Dr. Marco Pavone. This work was funded in
part by NSF grant DGE-114747. stefantj@stanford.edu

statistical model used is more general than [5] because
they consider distributions which are not Gaussian Ran-
dom ﬁelds. Starting from an optimal centralized policy,
the authors develop a distributed approximation by as-
suming that the agent locations are all independently
identically distributed and using a consensus ﬁlter to
explicitly evaluate the kernel based feature vector for
neighboring measurements (note that this precludes an
implicit inﬁnite dimensional feature vector). Only the
one-shot estimation case is considered, so the active
sensing question of how to move sensors is not addressed.
A method for choosing the most signiﬁcant features is
proposed, and it is claimed that this computation can be
run online. However, given the weak assumptions made
on the ad-hoc network, it is non-trivial to guarantee
that at any given time, the agents have agreed on the
basis functions (and ordering) to use in the algorithm.
Without this guarantee, the inner-product evaluations
that the algorithm relies on are not meaningful. If the
‘important’ features are chosen ahead of time, the kernel
method reduces to a parametric method similar to those
described in [8].

C. Motivation of proposed work

functions to estimate general

The approach to active sensing of a scalar ﬁeld taken in
this project is inspired by the non-parametric methods
used in [5]. Parametric methods suﬀer from the same
fundamental problem of linear estimators: due to the
ﬁnite number of basis functions used to parametrize
the space, parametric methods require a large number
of
functions well (and
there will always be a pathological case which is poorly
estimated). In problems where prior knowledge is not
available or where the statistics may change signiﬁ-
cantly, nonparametric kernel based methods provide an
adaptive, general learning framework. They still require
knowledge of the expected ‘smoothness’ via the kernel
bandwidth.

D. Statement of work

This work examines the coverage estimation problem
with kernel adaptive ﬁlters, which kernelize familiar lin-
ear estimators. In particular, kernel least-mean squares
(KLMS) and kernel recursive least squares (KRLS) are
used. Derivations and variants of the KLMS and KRLS
algorithm are summarized in [9]. The goal of this project
is to provide an on-line, kernel based approach to the
problem of maximizing the minimum value of time-
varying, reactive ﬁelds.

II. Problem formulation

A. Model

Assume we have N mobile sensors, and a central base
station. We denote measurement times by the index i.
n in a polygon X , and takes
Agent n has position xi
n) ⊂ X . The mea-
measurements over a footprint B(xi
surements follow the model

yi,n(x) = f (x, T (x, i), i) + νi,n

for x ∈ B(xi
is sensor noise and T (x, i) is given by

n), where i is the time of measurement, νi,n

N(cid:88)

i(cid:88)

n=1

j=0

T (x, i) =

1
i

1{x ∈ B(xj

n)}

which is the fraction of total time that the point x has
been observed up to time i. In this context, 1(·) is the
indicator function. If multiple agents observe the same
location, this fraction can be greater than 1 (though
not greater than N). Agents move according to noise-less
integrator dynamics,

n = xi−1
xi

n + ui
n

B. System requirements

We have the following communication and computa-

tion requirements:
(C1) agent n can identify itself to the base station and

(C2) agent n can measure: y(x) = f (x, T (x, i), i) + vi,

can send information to the base station;
over a region B(xi

n).

(C3) agent n can measure its position xi
n

The base station must be able to:

(C4) store measurements taken at time a given time step
by all agents, in addition to the current estimates
of f

(C5) compute partitions on X
(C6) send information to each robot

The goal of the network is to develop a observa-
tion policy T (x, i) to maximize the minimum value of
f (x, T (x, i), i).

C. Example problems

1) Scalar ﬁeld estimation: This applies immediately,
where we take f to be the time-varying error in mea-
surement.

p(x)

2) Sweeping and Persistent robotic tasks: This prob-
lem is proposed in [10], where they describe the ﬁeld by
the diﬀerential equation

˙f (x) =

p(x) − c(x)
0

if x (cid:54)∈ ∪N
if x ∈ ∪N
if x ∈ ∪N

n=1B(xn)
n=1B(xn) and f (x) > 0
n=1B(xn) and f (x) = 0
Integrating, we ﬁnd that (assuming that p(x) and c(x)
are independent of time)
f (x, i) = ip(x) + f (x, 0) − T (x, t)c(x) = f (x, T (x, i), i).

Their policy seeks to minimize the maximum of the
ﬁeld values, which ﬁts out model by a simple change of
variables.
3) Border patrol and intruder detection: Assume that
there are intruders trying to penetrate a region X unde-
tected. Their risk of failure is dictated by environmental
factors and being caught by a patrol. Intruders have ac-
cess to the patrol frequency T (x, i) along the region, but
not to real time position information of patrols. At every
time step i, the intruder examines the environment and
frequency of patrols, and compares it to a risk threshold
τi. We also assume that intruders are impatient, so their
risk threshold increases over time: τi > τi−1.

Patrolling agents move to a location and wait until
they see an intruder or grow impatient, then move on in
order to maximize the minimum intrusion time over X .
In a game theoretic context maximizing the minimum is
the Nash Equilibrium, since to choose any other policy
will reduce the patrolling agents value function (by def-
inition), and the intruders cannot unilaterally increase
their value function (to get in quickly).

We can formulate this problem as maximizing the
minimum of a ﬁeld representing the risk of failure (for
intruders) over the patrol frequencies T (x, i). Explicitly
constructing this problem analytically typically requires
strong statistical assumptions (such as are used in multi-
armed bandit problems), but even without those we can
say that this is a problem of maximizing the minimum of
some function of space, time and average coverage time.
Thus, this ﬁts the model from Section II-A.

III. Proposed Solution

Our proposed solution is outlined in algorithm 1, which
is very similar to [5], with two major diﬀerences: an
adaptive kernel estimator is used in place of Gaussian
Regression, and the ﬁeld is assumed to respond to obser-
vation. Coverage is achieved by running a range-limited
version of Lloyd’s clustering algorithm [11]: each agent
ﬁnds the centroid of a reward function based on f over
all points nearest to itself and travels there. In a static
environment, this algorithm will eventually converge to
the centroidal Voronoi partition, which is the solution to
many locational optimization problems. In our case, the
ﬁeld is time varying, so the locations of agents will not
converge. Instead, we look for convergence of the smallest
ﬁeld values.

The methods

estimator.update(u, d) and d =
estimator.predict d(u), and r = estimator.predict r(u)
are described in [9] for KLMS and KRLS, but are not
repeated here for brevity. The argument u is the mea-
surement vector, d is the scalar output of the model (or
observation) and r is the variance of the estimate d.

The method moveReward(map, xi

j) only considers the
points which are closest to agent j (e.g., in its Voronoi
partition). The method reward(d, r) is described next.

minimization function from the multi-armed bandit
problem.

IV. Simulations

Algorithm 1 was implemented in C in order to allow
reasonable simulation time for modest size problems.
Numerous problem settings were explored, with various
environments and reward functions. In the following,
only the most recent results are presented as they most
clearly express the properties of our algorithm.

A. Problem set-up

The equation used by intruders to assess diﬃculty at

point x is

D(x) = 10(prior(x) + T (x, i)).

The prior function used for this problem is of the form
sin(x) sin(y), and is shown in Fig. 1. Note the severe non-
convexity of the prior, with two distinct local minima
regions. The intruder impatience starts at 0.1 and grows
by 1 with each time step, and an event occurs when
the impatience at x exceeds D(x) (impatience resets on
event). A Gaussian kernel was chosen with bandwidth
parameter 10. Agents have impatience of 10 (if nothing
is seen for 10 iterations, they move on).

Simulations are run over a 50x50 grid with 3 agents
that can see manhattan distance 3. This means that for
every time step, 75 of 2500 grid points are observed (3%
of the region).

Algorithm 1: Coverage Estimation

for i > 1 do

for x ∈ X do

d = estimator.predictd([x,T (x, i),i])
r = estimator.predictr([x,T (x, i),i])
map(x) = reward(d, r)

end for
p = partition(map)
c = centroids(p)
for j ← 1, N do

if (event or timeout) then

j, i), i], y(xi

j))

j, T (xi

estimator.update([xi
j ← moveReward(map, xi
xi+1
j)
j ← xi
xi+1
end if

else

j

end for

end for

A. Reward Function Selection

The behavior of the algorithm will depend greatly on
the choice of reward function reward(d, r), which drives
agent motion. Three functions were considered in this
project:
1) reward(d, r) = exp(−d): This method directly
rewards traveling to areas with short intrusion times (low
risk). The big downside is that there is no beneﬁt given to
exploration, which results in poor performance of regions
which contain many local minima.
2) reward(d, r) = (b−d)2/(r2+(b−d)2): This function
comes from the Cantelli probability inequality which
states that for a positive, real random variable x with
mean µ and variance σ2, Pr(x − µ ≥ a) ≤ σ2/(σ2 + a2).
Switching then probability and substituting b = a + µ
yields:

Pr(x ≤ b) ≥ (b − µ)2

σ2 + (b − µ)2 .

This choice of reward then corresponds to a conservative
policy which minimizes risk. This runs into a similar
problem as with the direct method, because when the
variance r is large, the lower bound tends to 0. This is
addressed in the next function.

3) reward(d, r) = −d +(cid:112)2r log(i)/s + 3r log(i)/s:

This is based on a function developed in [12] based
on a multi-armed bandit formulation. The multi-armed
bandit problem is the idealized coverage-estimation prob-
lem: everything is independent, identically distributed,
Markov, and time-invariant. A regret bound is also given
in [12]. Here s = iT (x, i) is the number of times x has
been sampled and b is the support of the reward function
(in this formulation, the impatience of the patrolling
agents). This approach tries to balance exploration and
exploitation in a meaningful way.

The ﬁrst two methods do not explore well. Unless
otherwise speciﬁed, reward(d, r) is taken as the regret-

Fig. 1. Prior ﬁeld diﬃculty

B. Desired results

Intuitively, we should see agents focusing their eﬀorts
in the upper-left and lower-right corners, to compensate
for the relatively easy environment there. Fig. 2 shows
the diﬀerence between the initial and ﬁnal diﬃculty
function for the KRLS implementation. Qualitatively,
the patrols are taking the right action and we quantify
how good their responses were below.

01020304050607080901000102030405060708090100  51015202530Fig. 2. Change in perceived diﬃculty over 1230 iterations, single
KRLS trial

Fig. 3. KLMS results, averaged over 180 trials

An upper bound for the maximum minimum ﬁeld value
was computed using a centralized, omniscient controller
with no constraints on how to apply patrol eﬀort. In
particular, frequency allocation at a given time step can
be scattered and is not restricted to points within a given
manhattan distance of the patrolling agents.

Ideally, the minimum ﬁeld value will approach this
upper bound as time goes to inﬁnity. From the problem
structure, we also expect the variance of the ﬁeld to
decrease over time. If there are suﬃcient agents, then
the diﬃculty ﬁeld could theoretically be ﬂattened. In
our case, we only consider a sparse patrol scheme so we
simply look for decreasing variance over time. We expect
a transient at the beginning while agents are exploring
the region.

C. KLMS results

The minimum and variance of the ﬁeld for the KLMS
algorithm are shown in Fig. 3. The curves were generated
by averaging the results of 180 runs of 10,000 iterations.
Clearly, the algorithm is not able to generate a coherent
policy, since the ﬁeld variance settles near its initial value.
This implies that the algorithm is simply generating
random walk trajectories, which is veriﬁed by looking at
the change in perceived diﬃculty for a single trial (shown
in Fig. 4). From this we can conclude that the random
walk trajectory is able to accomplish approximately 60%
of the optimal maximum minimum ﬁeld value.

Our best explanation for the poor performance of
KLMS is that the algorithm is not able to learn ‘quickly
enough’ for the given problem dynamics, since in some
more pedantic environments KLMS performed more rea-
sonably. This lack of generalizability is a serious problem,
and even though KLMS has linear time complexity (ver-
sus O(i2) for KRLS), it is not suitable for our application.

Fig. 4. Change in perceived diﬃculty over 10,000 iterations, single
KLMS trial

D. KRLS results

The minimum and variance of the ﬁeld for the KRLS
algorithm are shown in Fig. 5. The curves were generated
by averaging the results of 30 runs of 3,000 iterations.
The reduced number of
iterations is because of the
signiﬁcant increase in complexity for KRLS. After ap-
proximately 250 iterations, the agents (implicitly) decide
on the locations of interest and begin to focus their eﬀorts
there. After 3000 iterations, the variance reduces by
36% from its initial value, and the maximum minimum
achieves 83.5% of the upper bound. Given the very
minimal modeling and tuning of this problem this is
impressive.

Sparsiﬁcation techniques such as the ones outlined in
[9] were implemented, but turned out to be very sensitive.
A 10% diﬀerence in threshold value switched behavior
from overly aggressive (never accepting a new point) to
useless (accepting all points). Due to this sensitivity, it
is not practical to use for a generic problem scenario.

Table 1 summarizes the simulation results.

01020304050607080901000102030405060708090100  12345678100020003000400050006000700080009000100000.511.522.5IterationNormalized Values  MinimumStandard deviationFuture work should focus on developing theoretical
guarantees for applying this algorithm (or a variant) to
the interactive coverage-estimation problem. Properties
such as convergence time (as related to environment
dynamics), expected regret, minimum number of agents
to achieve a given maximum minimum ﬁeld value, and
probabilistic service guarantees are derived for similar
problems and would provide much insight to this prob-
lem. There are quite a few applications in the literature
and industry which ﬁt the model that was formulated in
Section II-A, and there is the potential for combining the
results from several ﬁelds if a theoretical backing can be
developed here.

References

[1] J. Oyekan, Huosheng Hu, and Dongbing Gu. A novel bio-
inspired distributed coverage controller for pollution mon-
itoring.
In Mechatronics and Automation (ICMA), 2011
International Conference on, pages 1651–1656, Aug 2011.

[2] R. Andres Cortez, Herbert G. Tanner, and Ron Lumia. Dis-
In Tracts in Advanced

tributed robotic radiation mapping.
Robotics, pages 147–156. Springer, 2009.

[3] Vijay Kumar, D. Rus, and Sanjiv Singh. Robot and sensor
networks for ﬁrst responders. Pervasive Computing, IEEE,
3(4):24–33, Oct 2004.

[4] Jerome Le Ny, Munther Dahleh, and Eric Feron. Multi-
uav dynamic routing with partial observations using restless
bandit allocation indices.
In American Control Conference,
2008, pages 4220–4225. IEEE, 2008.

[5] Andrea Carron, Marco Todescato, Ruggero Carli, Luca Schen-
ato, and Gianluigi Pillonetto. Multi-agents adaptive estima-
tion and coverage control using gaussian regression. CoRR,
abs/1407.5807, 2014.

[6] Qiang Du, Vance Faber, and Max Gunzburger. Centroidal
SIAM

voronoi tessellations: applications and algorithms.
review, 41(4):637–676, 1999.

[7] Damiano Varagnolo, Gianluigi Pillonetto, and Luca Schen-
ato. Distributed parametric and nonparametric regression
with on-line performance bounds computation. Automatica,
48(10):2468–2481, 2012.

[8] Mac Schwager, Daniela Rus, and Jean-Jacques Slotine. De-
centralized, adaptive coverage control for networked robots.
The International Journal of Robotics Research, 28(3):357–
375, 2009.

[9] Weifeng Liu, Jose C Principe, and Simon Haykin. Kernel
Adaptive Filtering: A Comprehensive Introduction, volume 57.
John Wiley & Sons, 2011.

[10] Stephen L Smith, Mac Schwager, and Daniela Rus. Persistent
robotic tasks: Monitoring and sweeping in changing environ-
ments. Robotics, IEEE Transactions on, 28(2):410–426, 2012.
[11] Francesco Bullo, Jorge Cort´es, and Sonia Martinez. Dis-
tributed control of robotic networks: a mathematical approach
to motion coordination algorithms.
Princeton University
Press, 2009.

Fig. 5. KRLS results, averaged over 30 trials

Metric (normalized) KLMS KRLS

Ideal

Minimum
Variance

0.6331
0.9651

0.8355
0.6423

1.0
0.0

TABLE I

Summary of results

V. Conclusions

This project applied on-line adaptive kernel estimation
techniques to the coverage estimation problem in an
environment which reacts to observation. This approach
has not been tried in the literature, and for KRLS there
is potential for success. The KLMS approach suﬀered
from sensitivity to problem dynamics, the speciﬁcs of
which should be investigated in future work. The KRLS
approach was able to get within 83.5% of an upper bound
of the solution, which is good considering that the bound
is not particularly tight.

The results are particularly impressive given then non-
convexity of the problem and existence of local minima.
The three agents learn to divide their time among the two
local minima, crossing a less desirable region occasionally
to balance eﬀort. The main drawback from using KRLS
is its need for many samples to learn, and quadratic
complexity in the number of samples used. Sparsiﬁcation
techniques were tried, but proved too diﬃcult to tune for
general applications.

0500100015002000250030000.511.522.5IterationNormalized Values  MinimumStandard deviation[12] Jean-Yves Audibert, R´emi Munos, and Csaba Szepesv´ari.
Exploration–exploitation tradeoﬀ using variance estimates
in multi-armed bandits.
Theoretical Computer Science,
410(19):1876–1902, 2009.

