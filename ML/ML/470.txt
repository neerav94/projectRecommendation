Introduction	 Â 	 Â 

Error	 Â Detection	 Â based	 Â on	 Â neural	 Â signals	 Â 

	 Â 
Nir	 Â Even-Â­â€Chen	 Â and	 Â Igor	 Â Berman,	 Â Electrical	 Â Engineering,	 Â Stanford	 Â 	 Â 
Brain	 Â computer	 Â interface	 Â (BCI)	 Â is	 Â a	 Â direct	 Â communication	 Â pathway	 Â between	 Â the	 Â 
brain	 Â and	 Â an	 Â external	 Â device.	 Â In	 Â Professor	 Â Shenoyâ€™s	 Â Lab	 Â primates	 Â are	 Â trained	 Â to	 Â use	 Â 
BCI	 Â to	 Â control	 Â 2D	 Â cursor	 Â to	 Â accomplish	 Â different	 Â tasks.	 Â The	 Â primate	 Â controls	 Â the	 Â 
cursor	 Â through	 Â its	 Â neural	 Â activity,	 Â which	 Â is	 Â being	 Â recorded	 Â by	 Â an	 Â array	 Â of	 Â electrodes	 Â 
implanted	 Â in	 Â the	 Â motor	 Â cortex.	 Â 
Decoding	 Â neural	 Â activity	 Â is	 Â a	 Â challenging	 Â task	 Â prone	 Â to	 Â error.	 Â In	 Â this	 Â work,	 Â we	 Â 
propose	 Â a	 Â binary	 Â classifier	 Â that	 Â uses	 Â the	 Â same	 Â neural	 Â inputs	 Â from	 Â the	 Â motor	 Â cortex	 Â 
to	 Â detect	 Â errors.	 Â 	 Â 
This	 Â classifier	 Â can	 Â alert	 Â the	 Â system	 Â when	 Â an	 Â error	 Â occurs	 Â and	 Â enable	 Â auto-Â­â€
correction,	 Â as	 Â well	 Â as	 Â provide	 Â feedback	 Â regarding	 Â the	 Â primateâ€™s	 Â understanding	 Â of	 Â 
failure	 Â and	 Â success.	 Â Eventually,	 Â this	 Â method	 Â can	 Â be	 Â used	 Â to	 Â enable	 Â paralyzed	 Â 
patients	 Â to	 Â communicate	 Â with	 Â their	 Â environment.	 Â 
In	 Â the	 Â first	 Â part	 Â of	 Â the	 Â project	 Â weâ€™ve	 Â focused	 Â on	 Â reducing	 Â the	 Â dataâ€™s	 Â dimensionality	 Â 
through	 Â preprocessing.	 Â In	 Â the	 Â second	 Â part,	 Â we	 Â compared	 Â various	 Â classification	 Â 
techniques	 Â on	 Â the	 Â data,	 Â eventually	 Â achieving	 Â a	 Â classification	 Â error	 Â rate	 Â of	 Â 5Â±1%.	 Â 
In	 Â this	 Â project	 Â we	 Â will	 Â use	 Â data	 Â from	 Â the	 Â â€˜spellingâ€™	 Â task,	 Â which	 Â includes	 Â time	 Â series	 Â of	 Â 
neural	 Â activity,	 Â and	 Â labeling	 Â of	 Â the	 Â trials	 Â (Success	 Â /	 Â Failure).	 Â In	 Â this	 Â particular	 Â task	 Â 
the	 Â primates	 Â were	 Â required	 Â to	 Â bring	 Â the	 Â cursor	 Â above	 Â a	 Â highlighted	 Â letter	 Â and	 Â hold	 Â it	 Â 
there	 Â within	 Â a	 Â set	 Â timeframe	 Â (500ms).	 Â If	 Â successful	 Â the	 Â primates	 Â received	 Â a	 Â reward.	 Â 
In	 Â the	 Â case	 Â of	 Â failure,	 Â a	 Â specific	 Â sound	 Â is	 Â produced	 Â (Figure	 Â 1).	 Â 	 Â 
Hold	 Â 500ms 
Reward 
	 Â 
600ms 
	 Â  	 Â  
Success 
600ms 
Failure 

Target	 Â appears 
	 Â  

Cursor	 Â 
movement 

Experiment	 Â Setup	 Â 

	 Â  

	 Â  

	 Â  

being	 Â help.	 Â Feedback	 Â appears	 Â 600ms	 Â after	 Â selection.	 Â 

Figure	 Â 1:	 Â Experiment	 Â Setup.	 Â A	 Â Target	 Â appears	 Â and	 Â the	 Â primate	 Â moves	 Â the	 Â cursor	 Â using	 Â BCI.	 Â Selection	 Â is	 Â made	 Â when	 Â 
it	 Â holds	 Â the	 Â cursor	 Â on	 Â top	 Â of	 Â a	 Â circle	 Â for	 Â 500ms,	 Â than	 Â the	 Â target	 Â is	 Â vanishes.	 Â The	 Â target	 Â changes	 Â to	 Â blue	 Â in	 Â while	 Â it	 Â is	 Â 

For	 Â our	 Â initial	 Â (pre-Â­â€milestone)	 Â study,	 Â we	 Â used	 Â existing	 Â data	 Â from	 Â previous	 Â 
experiments,	 Â where	 Â the	 Â reward	 Â was	 Â given	 Â immediately	 Â after	 Â the	 Â success.	 Â For	 Â the	 Â 
sake	 Â of	 Â this	 Â project,	 Â we	 Â then	 Â conducted	 Â a	 Â new	 Â experiment,	 Â where	 Â feedback	 Â was	 Â 
delayed	 Â for	 Â 600ms,	 Â so	 Â that	 Â the	 Â reward	 Â doesnâ€™t	 Â mask	 Â the	 Â primateâ€™s	 Â internal	 Â â€œerror	 Â 
recognitionâ€.	 Â 
	 Â 

Introduction	 Â 	 Â 

Error	 Â Detection	 Â based	 Â on	 Â neural	 Â signals	 Â 

	 Â 
Nir	 Â Even-Â­â€Chen	 Â and	 Â Igor	 Â Berman,	 Â Electrical	 Â Engineering,	 Â Stanford	 Â 	 Â 
Brain	 Â computer	 Â interface	 Â (BCI)	 Â is	 Â a	 Â direct	 Â communication	 Â pathway	 Â between	 Â the	 Â 
brain	 Â and	 Â an	 Â external	 Â device.	 Â In	 Â Professor	 Â Shenoyâ€™s	 Â Lab	 Â primates	 Â are	 Â trained	 Â to	 Â use	 Â 
BCI	 Â to	 Â control	 Â 2D	 Â cursor	 Â to	 Â accomplish	 Â different	 Â tasks.	 Â The	 Â primate	 Â controls	 Â the	 Â 
cursor	 Â through	 Â its	 Â neural	 Â activity,	 Â which	 Â is	 Â being	 Â recorded	 Â by	 Â an	 Â array	 Â of	 Â electrodes	 Â 
implanted	 Â in	 Â the	 Â motor	 Â cortex.	 Â 
Decoding	 Â neural	 Â activity	 Â is	 Â a	 Â challenging	 Â task	 Â prone	 Â to	 Â error.	 Â In	 Â this	 Â work,	 Â we	 Â 
propose	 Â a	 Â binary	 Â classifier	 Â that	 Â uses	 Â the	 Â same	 Â neural	 Â inputs	 Â from	 Â the	 Â motor	 Â cortex	 Â 
to	 Â detect	 Â errors.	 Â 	 Â 
This	 Â classifier	 Â can	 Â alert	 Â the	 Â system	 Â when	 Â an	 Â error	 Â occurs	 Â and	 Â enable	 Â auto-Â­â€
correction,	 Â as	 Â well	 Â as	 Â provide	 Â feedback	 Â regarding	 Â the	 Â primateâ€™s	 Â understanding	 Â of	 Â 
failure	 Â and	 Â success.	 Â Eventually,	 Â this	 Â method	 Â can	 Â be	 Â used	 Â to	 Â enable	 Â paralyzed	 Â 
patients	 Â to	 Â communicate	 Â with	 Â their	 Â environment.	 Â 
In	 Â the	 Â first	 Â part	 Â of	 Â the	 Â project	 Â weâ€™ve	 Â focused	 Â on	 Â reducing	 Â the	 Â dataâ€™s	 Â dimensionality	 Â 
through	 Â preprocessing.	 Â In	 Â the	 Â second	 Â part,	 Â we	 Â compared	 Â various	 Â classification	 Â 
techniques	 Â on	 Â the	 Â data,	 Â eventually	 Â achieving	 Â a	 Â classification	 Â error	 Â rate	 Â of	 Â 5Â±1%.	 Â 
In	 Â this	 Â project	 Â we	 Â will	 Â use	 Â data	 Â from	 Â the	 Â â€˜spellingâ€™	 Â task,	 Â which	 Â includes	 Â time	 Â series	 Â of	 Â 
neural	 Â activity,	 Â and	 Â labeling	 Â of	 Â the	 Â trials	 Â (Success	 Â /	 Â Failure).	 Â In	 Â this	 Â particular	 Â task	 Â 
the	 Â primates	 Â were	 Â required	 Â to	 Â bring	 Â the	 Â cursor	 Â above	 Â a	 Â highlighted	 Â letter	 Â and	 Â hold	 Â it	 Â 
there	 Â within	 Â a	 Â set	 Â timeframe	 Â (500ms).	 Â If	 Â successful	 Â the	 Â primates	 Â received	 Â a	 Â reward.	 Â 
In	 Â the	 Â case	 Â of	 Â failure,	 Â a	 Â specific	 Â sound	 Â is	 Â produced	 Â (Figure	 Â 1).	 Â 	 Â 
Hold	 Â 500ms 
Reward 
	 Â 
600ms 
	 Â  	 Â  
Success 
600ms 
Failure 

Target	 Â appears 
	 Â  

Cursor	 Â 
movement 

Experiment	 Â Setup	 Â 

	 Â  

	 Â  

	 Â  

being	 Â help.	 Â Feedback	 Â appears	 Â 600ms	 Â after	 Â selection.	 Â 

Figure	 Â 1:	 Â Experiment	 Â Setup.	 Â A	 Â Target	 Â appears	 Â and	 Â the	 Â primate	 Â moves	 Â the	 Â cursor	 Â using	 Â BCI.	 Â Selection	 Â is	 Â made	 Â when	 Â 
it	 Â holds	 Â the	 Â cursor	 Â on	 Â top	 Â of	 Â a	 Â circle	 Â for	 Â 500ms,	 Â than	 Â the	 Â target	 Â is	 Â vanishes.	 Â The	 Â target	 Â changes	 Â to	 Â blue	 Â in	 Â while	 Â it	 Â is	 Â 

For	 Â our	 Â initial	 Â (pre-Â­â€milestone)	 Â study,	 Â we	 Â used	 Â existing	 Â data	 Â from	 Â previous	 Â 
experiments,	 Â where	 Â the	 Â reward	 Â was	 Â given	 Â immediately	 Â after	 Â the	 Â success.	 Â For	 Â the	 Â 
sake	 Â of	 Â this	 Â project,	 Â we	 Â then	 Â conducted	 Â a	 Â new	 Â experiment,	 Â where	 Â feedback	 Â was	 Â 
delayed	 Â for	 Â 600ms,	 Â so	 Â that	 Â the	 Â reward	 Â doesnâ€™t	 Â mask	 Â the	 Â primateâ€™s	 Â internal	 Â â€œerror	 Â 
recognitionâ€.	 Â 
	 Â 

Time	 Â Dimension:	 Â 

Number	 Â of	 Â Channels:	 Â 

The	 Â Data	 Â 

Preprocessing	 Â 
Dimensionality	 Â reduction	 Â 

	 Â 
The	 Â neural	 Â activity	 Â is	 Â recorded	 Â from	 Â the	 Â motor	 Â cortex	 Â (areas	 Â M1	 Â and	 Â PMd)	 Â with	 Â a	 Â 1	 Â 
kHz	 Â sampling	 Â rate	 Â from	 Â 192	 Â electrodes.	 Â In	 Â neuroscience	 Â it	 Â is	 Â common	 Â to	 Â analyze	 Â 
neuronâ€™s	 Â spikes,	 Â so	 Â our	 Â raw	 Â data	 Â includes	 Â is	 Â discrete	 Â in	 Â time	 Â and	 Â binary	 Â in	 Â values,	 Â 
with	 Â xt =1 Â if	 Â there	 Â was	 Â a	 Â spike	 Â during	 Â millisecond	 Â t.	 Â 
Based	 Â on	 Â previous	 Â studies,	 Â we	 Â expect	 Â the	 Â â€œfailure	 Â signalâ€	 Â to	 Â appear	 Â at	 Â most	 Â 200ms	 Â 
after	 Â the	 Â actual	 Â mistaken	 Â selection,	 Â so	 Â we	 Â use	 Â samples	 Â from	 Â 300ms	 Â before	 Â letter	 Â 
choice	 Â to	 Â 400ms	 Â after.	 Â The	 Â experiment	 Â was	 Â conducted	 Â on	 Â the	 Â same	 Â primate	 Â for	 Â two	 Â 
days;	 Â 1988	 Â trials	 Â were	 Â recorded	 Â with	 Â a	 Â 30%	 Â failure	 Â rate.	 Â 	 Â 
Each	 Â learning	 Â examples	 Â included	 Â 700	 Â time	 Â samples	 Â in	 Â 192	 Â channels,	 Â resulting	 Â in	 Â 
700Ã—192=134,400	 Â dimensions.	 Â Before	 Â proceeding	 Â to	 Â run	 Â learning	 Â algorithms	 Â on	 Â 
the	 Â data,	 Â weâ€™ll	 Â take	 Â some	 Â steps	 Â to	 Â reduce	 Â dimensionality:	 Â 
Based	 Â on	 Â prior	 Â studies,	 Â we	 Â expect	 Â the	 Â failure	 Â signal	 Â to	 Â last	 Â for	 Â tens	 Â of	 Â milliseconds.	 Â 
Furthermore,	 Â the	 Â usual	 Â firing	 Â rate	 Â for	 Â neurons	 Â in	 Â around	 Â 15-Â­â€20Hz,	 Â meaning	 Â that	 Â the	 Â 
1ms	 Â resolution	 Â produces	 Â very	 Â sparse	 Â data.	 Â Based	 Â on	 Â this	 Â evidence,	 Â and	 Â as	 Â is	 Â 
common	 Â in	 Â the	 Â field,	 Â we	 Â expect	 Â not	 Â to	 Â lose	 Â any	 Â critical	 Â information	 Â by	 Â downsampling	 Â 
the	 Â data	 Â and	 Â grouping	 Â the	 Â samples	 Â into	 Â 10ms	 Â bins	 Â of	 Â int	 Â values,	 Â resulting	 Â in	 Â only	 Â 70	 Â 
time	 Â samples	 Â per	 Â channel	 Â per	 Â trial.	 Â 
In	 Â the	 Â channel	 Â domain	 Â we	 Â applied	 Â per-Â­â€time-Â­â€frame	 Â PCA	 Â to	 Â reduce	 Â the	 Â number	 Â of	 Â 
channels.	 Â We	 Â decided	 Â not	 Â to	 Â normalize	 Â the	 Â variance,	 Â since	 Â all	 Â channels	 Â represented	 Â 
the	 Â same	 Â type	 Â of	 Â data	 Â (#spikes/10ms),	 Â and	 Â intuitively,	 Â active	 Â channels	 Â should	 Â have	 Â 
more	 Â influence	 Â than	 Â sparse	 Â ones.	 Â 	 Â 
Weâ€™ve	 Â tried	 Â two	 Â different	 Â approaches	 Â to	 Â performing	 Â the	 Â PCA,	 Â and	 Â we	 Â compared	 Â 
them	 Â by	 Â the	 Â %	 Â of	 Â variance	 Â represented	 Â in	 Â the	 Â first	 Â n	 Â components,	 Â using	 Â the	 Â rule	 Â of	 Â 
thumb	 Â that	 Â itâ€™s	 Â better	 Â to	 Â have	 Â fewer	 Â components	 Â that	 Â capture	 Â more	 Â variance:	 Â 
NaÃ¯ve	 Â  PCA:	 Â Our	 Â first	 Â approach	 Â was	 Â to	 Â concatenate	 Â all	 Â the	 Â timeframes	 Â of	 Â all	 Â trials	 Â 
(success	 Â and	 Â failure),	 Â and	 Â extract	 Â the	 Â primary	 Â components	 Â from	 Â the	 Â resulting	 Â 
covariance	 Â matrix.	 Â The	 Â result	 Â is	 Â shown	 Â in	 Â Figure	 Â 2	 Â below	 Â (red),	 Â demonstrating	 Â that	 Â 
~40	 Â components	 Â are	 Â required	 Â to	 Â capture	 Â 50%	 Â of	 Â the	 Â variance.	 Â 
Mean-Â­â€Diff	 Â PCA:	 Â Following	 Â these	 Â results,	 Â we	 Â came	 Â to	 Â assume	 Â that	 Â the	 Â first	 Â primary	 Â 
components	 Â we	 Â capture	 Â actually	 Â represent	 Â the	 Â kinematic	 Â signals	 Â produced	 Â by	 Â the	 Â 
brain,	 Â which	 Â are	 Â strong	 Â and	 Â common	 Â to	 Â both	 Â failure	 Â and	 Â success	 Â trials.	 Â This	 Â model	 Â 
can	 Â be	 Â represented	 Â a	 Â sum	 Â of	 Â three	 Â random	 Â processes:	 Â 
x!t =ğ‘£!t +ğ‘£!\!t +ğ‘£!t Â  Â  Â (1)	 Â 
Where	 Â ğ‘£!t 	 Â is	 Â kinematic	 Â component	 Â that	 Â controls	 Â the	 Â cursor,	 Â ğ‘£!\!t 	 Â is	 Â the	 Â 
component	 Â caused	 Â by	 Â success\failure	 Â and	 Â ğ‘£!t	 Â is	 Â noise.	 Â We	 Â assume	 Â that	 Â the	 Â three	 Â 
processes	 Â  are	 Â 
independent	 Â  and	 Â  have	 Â  zero	 Â  mean.	 Â  To	 Â  extract	 Â  the	 Â  second	 Â 
component Â ğ‘£!\!t,	 Â we	 Â applied	 Â the	 Â following	 Â method:	 Â 
	 Â 

Introduction	 Â 	 Â 

Error	 Â Detection	 Â based	 Â on	 Â neural	 Â signals	 Â 

	 Â 
Nir	 Â Even-Â­â€Chen	 Â and	 Â Igor	 Â Berman,	 Â Electrical	 Â Engineering,	 Â Stanford	 Â 	 Â 
Brain	 Â computer	 Â interface	 Â (BCI)	 Â is	 Â a	 Â direct	 Â communication	 Â pathway	 Â between	 Â the	 Â 
brain	 Â and	 Â an	 Â external	 Â device.	 Â In	 Â Professor	 Â Shenoyâ€™s	 Â Lab	 Â primates	 Â are	 Â trained	 Â to	 Â use	 Â 
BCI	 Â to	 Â control	 Â 2D	 Â cursor	 Â to	 Â accomplish	 Â different	 Â tasks.	 Â The	 Â primate	 Â controls	 Â the	 Â 
cursor	 Â through	 Â its	 Â neural	 Â activity,	 Â which	 Â is	 Â being	 Â recorded	 Â by	 Â an	 Â array	 Â of	 Â electrodes	 Â 
implanted	 Â in	 Â the	 Â motor	 Â cortex.	 Â 
Decoding	 Â neural	 Â activity	 Â is	 Â a	 Â challenging	 Â task	 Â prone	 Â to	 Â error.	 Â In	 Â this	 Â work,	 Â we	 Â 
propose	 Â a	 Â binary	 Â classifier	 Â that	 Â uses	 Â the	 Â same	 Â neural	 Â inputs	 Â from	 Â the	 Â motor	 Â cortex	 Â 
to	 Â detect	 Â errors.	 Â 	 Â 
This	 Â classifier	 Â can	 Â alert	 Â the	 Â system	 Â when	 Â an	 Â error	 Â occurs	 Â and	 Â enable	 Â auto-Â­â€
correction,	 Â as	 Â well	 Â as	 Â provide	 Â feedback	 Â regarding	 Â the	 Â primateâ€™s	 Â understanding	 Â of	 Â 
failure	 Â and	 Â success.	 Â Eventually,	 Â this	 Â method	 Â can	 Â be	 Â used	 Â to	 Â enable	 Â paralyzed	 Â 
patients	 Â to	 Â communicate	 Â with	 Â their	 Â environment.	 Â 
In	 Â the	 Â first	 Â part	 Â of	 Â the	 Â project	 Â weâ€™ve	 Â focused	 Â on	 Â reducing	 Â the	 Â dataâ€™s	 Â dimensionality	 Â 
through	 Â preprocessing.	 Â In	 Â the	 Â second	 Â part,	 Â we	 Â compared	 Â various	 Â classification	 Â 
techniques	 Â on	 Â the	 Â data,	 Â eventually	 Â achieving	 Â a	 Â classification	 Â error	 Â rate	 Â of	 Â 5Â±1%.	 Â 
In	 Â this	 Â project	 Â we	 Â will	 Â use	 Â data	 Â from	 Â the	 Â â€˜spellingâ€™	 Â task,	 Â which	 Â includes	 Â time	 Â series	 Â of	 Â 
neural	 Â activity,	 Â and	 Â labeling	 Â of	 Â the	 Â trials	 Â (Success	 Â /	 Â Failure).	 Â In	 Â this	 Â particular	 Â task	 Â 
the	 Â primates	 Â were	 Â required	 Â to	 Â bring	 Â the	 Â cursor	 Â above	 Â a	 Â highlighted	 Â letter	 Â and	 Â hold	 Â it	 Â 
there	 Â within	 Â a	 Â set	 Â timeframe	 Â (500ms).	 Â If	 Â successful	 Â the	 Â primates	 Â received	 Â a	 Â reward.	 Â 
In	 Â the	 Â case	 Â of	 Â failure,	 Â a	 Â specific	 Â sound	 Â is	 Â produced	 Â (Figure	 Â 1).	 Â 	 Â 
Hold	 Â 500ms 
Reward 
	 Â 
600ms 
	 Â  	 Â  
Success 
600ms 
Failure 

Target	 Â appears 
	 Â  

Cursor	 Â 
movement 

Experiment	 Â Setup	 Â 

	 Â  

	 Â  

	 Â  

being	 Â help.	 Â Feedback	 Â appears	 Â 600ms	 Â after	 Â selection.	 Â 

Figure	 Â 1:	 Â Experiment	 Â Setup.	 Â A	 Â Target	 Â appears	 Â and	 Â the	 Â primate	 Â moves	 Â the	 Â cursor	 Â using	 Â BCI.	 Â Selection	 Â is	 Â made	 Â when	 Â 
it	 Â holds	 Â the	 Â cursor	 Â on	 Â top	 Â of	 Â a	 Â circle	 Â for	 Â 500ms,	 Â than	 Â the	 Â target	 Â is	 Â vanishes.	 Â The	 Â target	 Â changes	 Â to	 Â blue	 Â in	 Â while	 Â it	 Â is	 Â 

For	 Â our	 Â initial	 Â (pre-Â­â€milestone)	 Â study,	 Â we	 Â used	 Â existing	 Â data	 Â from	 Â previous	 Â 
experiments,	 Â where	 Â the	 Â reward	 Â was	 Â given	 Â immediately	 Â after	 Â the	 Â success.	 Â For	 Â the	 Â 
sake	 Â of	 Â this	 Â project,	 Â we	 Â then	 Â conducted	 Â a	 Â new	 Â experiment,	 Â where	 Â feedback	 Â was	 Â 
delayed	 Â for	 Â 600ms,	 Â so	 Â that	 Â the	 Â reward	 Â doesnâ€™t	 Â mask	 Â the	 Â primateâ€™s	 Â internal	 Â â€œerror	 Â 
recognitionâ€.	 Â 
	 Â 

Time	 Â Dimension:	 Â 

Number	 Â of	 Â Channels:	 Â 

The	 Â Data	 Â 

Preprocessing	 Â 
Dimensionality	 Â reduction	 Â 

	 Â 
The	 Â neural	 Â activity	 Â is	 Â recorded	 Â from	 Â the	 Â motor	 Â cortex	 Â (areas	 Â M1	 Â and	 Â PMd)	 Â with	 Â a	 Â 1	 Â 
kHz	 Â sampling	 Â rate	 Â from	 Â 192	 Â electrodes.	 Â In	 Â neuroscience	 Â it	 Â is	 Â common	 Â to	 Â analyze	 Â 
neuronâ€™s	 Â spikes,	 Â so	 Â our	 Â raw	 Â data	 Â includes	 Â is	 Â discrete	 Â in	 Â time	 Â and	 Â binary	 Â in	 Â values,	 Â 
with	 Â xt =1 Â if	 Â there	 Â was	 Â a	 Â spike	 Â during	 Â millisecond	 Â t.	 Â 
Based	 Â on	 Â previous	 Â studies,	 Â we	 Â expect	 Â the	 Â â€œfailure	 Â signalâ€	 Â to	 Â appear	 Â at	 Â most	 Â 200ms	 Â 
after	 Â the	 Â actual	 Â mistaken	 Â selection,	 Â so	 Â we	 Â use	 Â samples	 Â from	 Â 300ms	 Â before	 Â letter	 Â 
choice	 Â to	 Â 400ms	 Â after.	 Â The	 Â experiment	 Â was	 Â conducted	 Â on	 Â the	 Â same	 Â primate	 Â for	 Â two	 Â 
days;	 Â 1988	 Â trials	 Â were	 Â recorded	 Â with	 Â a	 Â 30%	 Â failure	 Â rate.	 Â 	 Â 
Each	 Â learning	 Â examples	 Â included	 Â 700	 Â time	 Â samples	 Â in	 Â 192	 Â channels,	 Â resulting	 Â in	 Â 
700Ã—192=134,400	 Â dimensions.	 Â Before	 Â proceeding	 Â to	 Â run	 Â learning	 Â algorithms	 Â on	 Â 
the	 Â data,	 Â weâ€™ll	 Â take	 Â some	 Â steps	 Â to	 Â reduce	 Â dimensionality:	 Â 
Based	 Â on	 Â prior	 Â studies,	 Â we	 Â expect	 Â the	 Â failure	 Â signal	 Â to	 Â last	 Â for	 Â tens	 Â of	 Â milliseconds.	 Â 
Furthermore,	 Â the	 Â usual	 Â firing	 Â rate	 Â for	 Â neurons	 Â in	 Â around	 Â 15-Â­â€20Hz,	 Â meaning	 Â that	 Â the	 Â 
1ms	 Â resolution	 Â produces	 Â very	 Â sparse	 Â data.	 Â Based	 Â on	 Â this	 Â evidence,	 Â and	 Â as	 Â is	 Â 
common	 Â in	 Â the	 Â field,	 Â we	 Â expect	 Â not	 Â to	 Â lose	 Â any	 Â critical	 Â information	 Â by	 Â downsampling	 Â 
the	 Â data	 Â and	 Â grouping	 Â the	 Â samples	 Â into	 Â 10ms	 Â bins	 Â of	 Â int	 Â values,	 Â resulting	 Â in	 Â only	 Â 70	 Â 
time	 Â samples	 Â per	 Â channel	 Â per	 Â trial.	 Â 
In	 Â the	 Â channel	 Â domain	 Â we	 Â applied	 Â per-Â­â€time-Â­â€frame	 Â PCA	 Â to	 Â reduce	 Â the	 Â number	 Â of	 Â 
channels.	 Â We	 Â decided	 Â not	 Â to	 Â normalize	 Â the	 Â variance,	 Â since	 Â all	 Â channels	 Â represented	 Â 
the	 Â same	 Â type	 Â of	 Â data	 Â (#spikes/10ms),	 Â and	 Â intuitively,	 Â active	 Â channels	 Â should	 Â have	 Â 
more	 Â influence	 Â than	 Â sparse	 Â ones.	 Â 	 Â 
Weâ€™ve	 Â tried	 Â two	 Â different	 Â approaches	 Â to	 Â performing	 Â the	 Â PCA,	 Â and	 Â we	 Â compared	 Â 
them	 Â by	 Â the	 Â %	 Â of	 Â variance	 Â represented	 Â in	 Â the	 Â first	 Â n	 Â components,	 Â using	 Â the	 Â rule	 Â of	 Â 
thumb	 Â that	 Â itâ€™s	 Â better	 Â to	 Â have	 Â fewer	 Â components	 Â that	 Â capture	 Â more	 Â variance:	 Â 
NaÃ¯ve	 Â  PCA:	 Â Our	 Â first	 Â approach	 Â was	 Â to	 Â concatenate	 Â all	 Â the	 Â timeframes	 Â of	 Â all	 Â trials	 Â 
(success	 Â and	 Â failure),	 Â and	 Â extract	 Â the	 Â primary	 Â components	 Â from	 Â the	 Â resulting	 Â 
covariance	 Â matrix.	 Â The	 Â result	 Â is	 Â shown	 Â in	 Â Figure	 Â 2	 Â below	 Â (red),	 Â demonstrating	 Â that	 Â 
~40	 Â components	 Â are	 Â required	 Â to	 Â capture	 Â 50%	 Â of	 Â the	 Â variance.	 Â 
Mean-Â­â€Diff	 Â PCA:	 Â Following	 Â these	 Â results,	 Â we	 Â came	 Â to	 Â assume	 Â that	 Â the	 Â first	 Â primary	 Â 
components	 Â we	 Â capture	 Â actually	 Â represent	 Â the	 Â kinematic	 Â signals	 Â produced	 Â by	 Â the	 Â 
brain,	 Â which	 Â are	 Â strong	 Â and	 Â common	 Â to	 Â both	 Â failure	 Â and	 Â success	 Â trials.	 Â This	 Â model	 Â 
can	 Â be	 Â represented	 Â a	 Â sum	 Â of	 Â three	 Â random	 Â processes:	 Â 
x!t =ğ‘£!t +ğ‘£!\!t +ğ‘£!t Â  Â  Â (1)	 Â 
Where	 Â ğ‘£!t 	 Â is	 Â kinematic	 Â component	 Â that	 Â controls	 Â the	 Â cursor,	 Â ğ‘£!\!t 	 Â is	 Â the	 Â 
component	 Â caused	 Â by	 Â success\failure	 Â and	 Â ğ‘£!t	 Â is	 Â noise.	 Â We	 Â assume	 Â that	 Â the	 Â three	 Â 
processes	 Â  are	 Â 
independent	 Â  and	 Â  have	 Â  zero	 Â  mean.	 Â  To	 Â  extract	 Â  the	 Â  second	 Â 
component Â ğ‘£!\!t,	 Â we	 Â applied	 Â the	 Â following	 Â method:	 Â 
	 Â 

Î”ÂµÎ¼ğ‘¡ =1ğ‘!

	 Â 
â‰ˆğ‘£!ğ‘¡ âˆ’ğ‘£!ğ‘¡ Â  Â  Â  Â (2)	 Â 
The	 Â resulting	 Â mean-Â­â€diff	 Â Î”ÂµÎ¼âˆˆâ„›!"Ã—!"#	 Â captures	 Â the	 Â average	 Â differences	 Â between	 Â 
successful	 Â and	 Â failed	 Â trials.	 Â Since	 Â the	 Â kinematic	 Â component	 Â is	 Â generally	 Â orthogonal	 Â 
to	 Â the	 Â {fail/success}	 Â labelling,	 Â subtracting	 Â the	 Â two	 Â averages	 Â reduces	 Â its	 Â effect.	 Â 
Eventually,	 Â we	 Â ran	 Â a	 Â PCA	 Â on	 Â the	 Â matrix	 Â ğ›¥ğœ‡	 Â and	 Â examined	 Â the	 Â resulting	 Â vectors.	 Â As	 Â 
shown	 Â in	 Â Figure	 Â 2	 Â below,	 Â the	 Â 3	 Â first	 Â PCs	 Â together	 Â capture	 Â 90%	 Â of	 Â the	 Â variance:	 Â 

âˆ’1ğ‘!

ğ‘¥!ğ‘¡

ğ‘¥!ğ‘¡

!âˆˆ!!"#

!âˆˆ!"#

0.02

0.01

PCAmean
PCAall

e
c
n
a
i
r
a
V
 
d
e
n
a
p
x
E

i

l

30
30

40
40

50
50

10
10

20
20

0
 
0
0

Figure	 Â 2:	 Â Per-Â­â€vector(dotted	 Â line)	 Â 	 Â and	 Â Cumulative	 Â (solid	 Â line)	 Â Variance	 Â for	 Â two	 Â sets	 Â of	 Â PCs:	 Â the	 Â 

Based	 Â on	 Â the	 Â above,	 Â we	 Â choose	 Â to	 Â proceed	 Â with	 Â the	 Â second	 Â set	 Â of	 Â PCs,	 Â generated	 Â 
from	 Â the	 Â mean-Â­â€diff.	 Â To	 Â gauge	 Â their	 Â relevance	 Â to	 Â our	 Â classification	 Â problem,	 Â we	 Â 
project	 Â the	 Â successful	 Â and	 Â failed	 Â trials	 Â on	 Â the	 Â first	 Â three	 Â PCs	 Â we	 Â found	 Â (Figure	 Â 3),	 Â 
with	 Â the	 Â projection	 Â of	 Â failed	 Â results	 Â in	 Â blue	 Â and	 Â successful	 Â in	 Â red:	 Â 	 Â 

naÃ¯ve	 Â PCA	 Â (PCAall	 Â â€“	 Â red)	 Â and	 Â mean-Â­â€diff	 Â PCA	 Â (PCAmean	 Â â€“	 Â blue)	 Â 

#PCs

700
70

60
60

100

 

50

e
c
n
a
i
r
a
V
%
 
e
v
i
t
a
u
m
u
C

l

	 Â 

Figure	 Â 3:	 Â Projection	 Â of	 Â successes	 Â and	 Â failures	 Â on	 Â first	 Â three	 Â PCs.	 Â Failure	 Â in	 Â blue,	 Â success	 Â in	 Â red.	 Â Solid	 Â line	 Â represent	 Â 
the	 Â average	 Â projection	 Â for	 Â every	 Â timestamp,	 Â dotted	 Â line	 Â is	 Â the	 Â standard	 Â deviation	 Â of	 Â the	 Â mean.	 Â The	 Â circles	 Â represent	 Â 
the	 Â time	 Â evolvement	 Â where	 Â the	 Â black	 Â circle	 Â is	 Â target	 Â selection	 Â time	 Â (t=0).	 Â Top:	 Â 3D	 Â of	 Â 3	 Â leading	 Â PCs,	 Â Bottom:	 Â 2D	 Â view	 Â 
of	 Â PC1/3	 Â and	 Â PC1/2.	 Â 

We	 Â can	 Â see	 Â that	 Â the	 Â two	 Â processes	 Â are	 Â differentiating	 Â from	 Â target	 Â selection	 Â time	 Â 
(t=0,	 Â black	 Â circle).	 Â In	 Â the	 Â bottom	 Â figures	 Â it	 Â is	 Â evident	 Â that	 Â the	 Â mean	 Â projection	 Â on	 Â 
PC1	 Â and	 Â PC2	 Â are	 Â common	 Â for	 Â both	 Â scenarios.	 Â However,	 Â PC3	 Â contains	 Â the	 Â 
information	 Â that	 Â distinguishes	 Â between	 Â the	 Â two	 Â scenarios.	 Â 
	 Â 

Introduction	 Â 	 Â 

Error	 Â Detection	 Â based	 Â on	 Â neural	 Â signals	 Â 

	 Â 
Nir	 Â Even-Â­â€Chen	 Â and	 Â Igor	 Â Berman,	 Â Electrical	 Â Engineering,	 Â Stanford	 Â 	 Â 
Brain	 Â computer	 Â interface	 Â (BCI)	 Â is	 Â a	 Â direct	 Â communication	 Â pathway	 Â between	 Â the	 Â 
brain	 Â and	 Â an	 Â external	 Â device.	 Â In	 Â Professor	 Â Shenoyâ€™s	 Â Lab	 Â primates	 Â are	 Â trained	 Â to	 Â use	 Â 
BCI	 Â to	 Â control	 Â 2D	 Â cursor	 Â to	 Â accomplish	 Â different	 Â tasks.	 Â The	 Â primate	 Â controls	 Â the	 Â 
cursor	 Â through	 Â its	 Â neural	 Â activity,	 Â which	 Â is	 Â being	 Â recorded	 Â by	 Â an	 Â array	 Â of	 Â electrodes	 Â 
implanted	 Â in	 Â the	 Â motor	 Â cortex.	 Â 
Decoding	 Â neural	 Â activity	 Â is	 Â a	 Â challenging	 Â task	 Â prone	 Â to	 Â error.	 Â In	 Â this	 Â work,	 Â we	 Â 
propose	 Â a	 Â binary	 Â classifier	 Â that	 Â uses	 Â the	 Â same	 Â neural	 Â inputs	 Â from	 Â the	 Â motor	 Â cortex	 Â 
to	 Â detect	 Â errors.	 Â 	 Â 
This	 Â classifier	 Â can	 Â alert	 Â the	 Â system	 Â when	 Â an	 Â error	 Â occurs	 Â and	 Â enable	 Â auto-Â­â€
correction,	 Â as	 Â well	 Â as	 Â provide	 Â feedback	 Â regarding	 Â the	 Â primateâ€™s	 Â understanding	 Â of	 Â 
failure	 Â and	 Â success.	 Â Eventually,	 Â this	 Â method	 Â can	 Â be	 Â used	 Â to	 Â enable	 Â paralyzed	 Â 
patients	 Â to	 Â communicate	 Â with	 Â their	 Â environment.	 Â 
In	 Â the	 Â first	 Â part	 Â of	 Â the	 Â project	 Â weâ€™ve	 Â focused	 Â on	 Â reducing	 Â the	 Â dataâ€™s	 Â dimensionality	 Â 
through	 Â preprocessing.	 Â In	 Â the	 Â second	 Â part,	 Â we	 Â compared	 Â various	 Â classification	 Â 
techniques	 Â on	 Â the	 Â data,	 Â eventually	 Â achieving	 Â a	 Â classification	 Â error	 Â rate	 Â of	 Â 5Â±1%.	 Â 
In	 Â this	 Â project	 Â we	 Â will	 Â use	 Â data	 Â from	 Â the	 Â â€˜spellingâ€™	 Â task,	 Â which	 Â includes	 Â time	 Â series	 Â of	 Â 
neural	 Â activity,	 Â and	 Â labeling	 Â of	 Â the	 Â trials	 Â (Success	 Â /	 Â Failure).	 Â In	 Â this	 Â particular	 Â task	 Â 
the	 Â primates	 Â were	 Â required	 Â to	 Â bring	 Â the	 Â cursor	 Â above	 Â a	 Â highlighted	 Â letter	 Â and	 Â hold	 Â it	 Â 
there	 Â within	 Â a	 Â set	 Â timeframe	 Â (500ms).	 Â If	 Â successful	 Â the	 Â primates	 Â received	 Â a	 Â reward.	 Â 
In	 Â the	 Â case	 Â of	 Â failure,	 Â a	 Â specific	 Â sound	 Â is	 Â produced	 Â (Figure	 Â 1).	 Â 	 Â 
Hold	 Â 500ms 
Reward 
	 Â 
600ms 
	 Â  	 Â  
Success 
600ms 
Failure 

Target	 Â appears 
	 Â  

Cursor	 Â 
movement 

Experiment	 Â Setup	 Â 

	 Â  

	 Â  

	 Â  

being	 Â help.	 Â Feedback	 Â appears	 Â 600ms	 Â after	 Â selection.	 Â 

Figure	 Â 1:	 Â Experiment	 Â Setup.	 Â A	 Â Target	 Â appears	 Â and	 Â the	 Â primate	 Â moves	 Â the	 Â cursor	 Â using	 Â BCI.	 Â Selection	 Â is	 Â made	 Â when	 Â 
it	 Â holds	 Â the	 Â cursor	 Â on	 Â top	 Â of	 Â a	 Â circle	 Â for	 Â 500ms,	 Â than	 Â the	 Â target	 Â is	 Â vanishes.	 Â The	 Â target	 Â changes	 Â to	 Â blue	 Â in	 Â while	 Â it	 Â is	 Â 

For	 Â our	 Â initial	 Â (pre-Â­â€milestone)	 Â study,	 Â we	 Â used	 Â existing	 Â data	 Â from	 Â previous	 Â 
experiments,	 Â where	 Â the	 Â reward	 Â was	 Â given	 Â immediately	 Â after	 Â the	 Â success.	 Â For	 Â the	 Â 
sake	 Â of	 Â this	 Â project,	 Â we	 Â then	 Â conducted	 Â a	 Â new	 Â experiment,	 Â where	 Â feedback	 Â was	 Â 
delayed	 Â for	 Â 600ms,	 Â so	 Â that	 Â the	 Â reward	 Â doesnâ€™t	 Â mask	 Â the	 Â primateâ€™s	 Â internal	 Â â€œerror	 Â 
recognitionâ€.	 Â 
	 Â 

Time	 Â Dimension:	 Â 

Number	 Â of	 Â Channels:	 Â 

The	 Â Data	 Â 

Preprocessing	 Â 
Dimensionality	 Â reduction	 Â 

	 Â 
The	 Â neural	 Â activity	 Â is	 Â recorded	 Â from	 Â the	 Â motor	 Â cortex	 Â (areas	 Â M1	 Â and	 Â PMd)	 Â with	 Â a	 Â 1	 Â 
kHz	 Â sampling	 Â rate	 Â from	 Â 192	 Â electrodes.	 Â In	 Â neuroscience	 Â it	 Â is	 Â common	 Â to	 Â analyze	 Â 
neuronâ€™s	 Â spikes,	 Â so	 Â our	 Â raw	 Â data	 Â includes	 Â is	 Â discrete	 Â in	 Â time	 Â and	 Â binary	 Â in	 Â values,	 Â 
with	 Â xt =1 Â if	 Â there	 Â was	 Â a	 Â spike	 Â during	 Â millisecond	 Â t.	 Â 
Based	 Â on	 Â previous	 Â studies,	 Â we	 Â expect	 Â the	 Â â€œfailure	 Â signalâ€	 Â to	 Â appear	 Â at	 Â most	 Â 200ms	 Â 
after	 Â the	 Â actual	 Â mistaken	 Â selection,	 Â so	 Â we	 Â use	 Â samples	 Â from	 Â 300ms	 Â before	 Â letter	 Â 
choice	 Â to	 Â 400ms	 Â after.	 Â The	 Â experiment	 Â was	 Â conducted	 Â on	 Â the	 Â same	 Â primate	 Â for	 Â two	 Â 
days;	 Â 1988	 Â trials	 Â were	 Â recorded	 Â with	 Â a	 Â 30%	 Â failure	 Â rate.	 Â 	 Â 
Each	 Â learning	 Â examples	 Â included	 Â 700	 Â time	 Â samples	 Â in	 Â 192	 Â channels,	 Â resulting	 Â in	 Â 
700Ã—192=134,400	 Â dimensions.	 Â Before	 Â proceeding	 Â to	 Â run	 Â learning	 Â algorithms	 Â on	 Â 
the	 Â data,	 Â weâ€™ll	 Â take	 Â some	 Â steps	 Â to	 Â reduce	 Â dimensionality:	 Â 
Based	 Â on	 Â prior	 Â studies,	 Â we	 Â expect	 Â the	 Â failure	 Â signal	 Â to	 Â last	 Â for	 Â tens	 Â of	 Â milliseconds.	 Â 
Furthermore,	 Â the	 Â usual	 Â firing	 Â rate	 Â for	 Â neurons	 Â in	 Â around	 Â 15-Â­â€20Hz,	 Â meaning	 Â that	 Â the	 Â 
1ms	 Â resolution	 Â produces	 Â very	 Â sparse	 Â data.	 Â Based	 Â on	 Â this	 Â evidence,	 Â and	 Â as	 Â is	 Â 
common	 Â in	 Â the	 Â field,	 Â we	 Â expect	 Â not	 Â to	 Â lose	 Â any	 Â critical	 Â information	 Â by	 Â downsampling	 Â 
the	 Â data	 Â and	 Â grouping	 Â the	 Â samples	 Â into	 Â 10ms	 Â bins	 Â of	 Â int	 Â values,	 Â resulting	 Â in	 Â only	 Â 70	 Â 
time	 Â samples	 Â per	 Â channel	 Â per	 Â trial.	 Â 
In	 Â the	 Â channel	 Â domain	 Â we	 Â applied	 Â per-Â­â€time-Â­â€frame	 Â PCA	 Â to	 Â reduce	 Â the	 Â number	 Â of	 Â 
channels.	 Â We	 Â decided	 Â not	 Â to	 Â normalize	 Â the	 Â variance,	 Â since	 Â all	 Â channels	 Â represented	 Â 
the	 Â same	 Â type	 Â of	 Â data	 Â (#spikes/10ms),	 Â and	 Â intuitively,	 Â active	 Â channels	 Â should	 Â have	 Â 
more	 Â influence	 Â than	 Â sparse	 Â ones.	 Â 	 Â 
Weâ€™ve	 Â tried	 Â two	 Â different	 Â approaches	 Â to	 Â performing	 Â the	 Â PCA,	 Â and	 Â we	 Â compared	 Â 
them	 Â by	 Â the	 Â %	 Â of	 Â variance	 Â represented	 Â in	 Â the	 Â first	 Â n	 Â components,	 Â using	 Â the	 Â rule	 Â of	 Â 
thumb	 Â that	 Â itâ€™s	 Â better	 Â to	 Â have	 Â fewer	 Â components	 Â that	 Â capture	 Â more	 Â variance:	 Â 
NaÃ¯ve	 Â  PCA:	 Â Our	 Â first	 Â approach	 Â was	 Â to	 Â concatenate	 Â all	 Â the	 Â timeframes	 Â of	 Â all	 Â trials	 Â 
(success	 Â and	 Â failure),	 Â and	 Â extract	 Â the	 Â primary	 Â components	 Â from	 Â the	 Â resulting	 Â 
covariance	 Â matrix.	 Â The	 Â result	 Â is	 Â shown	 Â in	 Â Figure	 Â 2	 Â below	 Â (red),	 Â demonstrating	 Â that	 Â 
~40	 Â components	 Â are	 Â required	 Â to	 Â capture	 Â 50%	 Â of	 Â the	 Â variance.	 Â 
Mean-Â­â€Diff	 Â PCA:	 Â Following	 Â these	 Â results,	 Â we	 Â came	 Â to	 Â assume	 Â that	 Â the	 Â first	 Â primary	 Â 
components	 Â we	 Â capture	 Â actually	 Â represent	 Â the	 Â kinematic	 Â signals	 Â produced	 Â by	 Â the	 Â 
brain,	 Â which	 Â are	 Â strong	 Â and	 Â common	 Â to	 Â both	 Â failure	 Â and	 Â success	 Â trials.	 Â This	 Â model	 Â 
can	 Â be	 Â represented	 Â a	 Â sum	 Â of	 Â three	 Â random	 Â processes:	 Â 
x!t =ğ‘£!t +ğ‘£!\!t +ğ‘£!t Â  Â  Â (1)	 Â 
Where	 Â ğ‘£!t 	 Â is	 Â kinematic	 Â component	 Â that	 Â controls	 Â the	 Â cursor,	 Â ğ‘£!\!t 	 Â is	 Â the	 Â 
component	 Â caused	 Â by	 Â success\failure	 Â and	 Â ğ‘£!t	 Â is	 Â noise.	 Â We	 Â assume	 Â that	 Â the	 Â three	 Â 
processes	 Â  are	 Â 
independent	 Â  and	 Â  have	 Â  zero	 Â  mean.	 Â  To	 Â  extract	 Â  the	 Â  second	 Â 
component Â ğ‘£!\!t,	 Â we	 Â applied	 Â the	 Â following	 Â method:	 Â 
	 Â 

Î”ÂµÎ¼ğ‘¡ =1ğ‘!

	 Â 
â‰ˆğ‘£!ğ‘¡ âˆ’ğ‘£!ğ‘¡ Â  Â  Â  Â (2)	 Â 
The	 Â resulting	 Â mean-Â­â€diff	 Â Î”ÂµÎ¼âˆˆâ„›!"Ã—!"#	 Â captures	 Â the	 Â average	 Â differences	 Â between	 Â 
successful	 Â and	 Â failed	 Â trials.	 Â Since	 Â the	 Â kinematic	 Â component	 Â is	 Â generally	 Â orthogonal	 Â 
to	 Â the	 Â {fail/success}	 Â labelling,	 Â subtracting	 Â the	 Â two	 Â averages	 Â reduces	 Â its	 Â effect.	 Â 
Eventually,	 Â we	 Â ran	 Â a	 Â PCA	 Â on	 Â the	 Â matrix	 Â ğ›¥ğœ‡	 Â and	 Â examined	 Â the	 Â resulting	 Â vectors.	 Â As	 Â 
shown	 Â in	 Â Figure	 Â 2	 Â below,	 Â the	 Â 3	 Â first	 Â PCs	 Â together	 Â capture	 Â 90%	 Â of	 Â the	 Â variance:	 Â 

âˆ’1ğ‘!

ğ‘¥!ğ‘¡

ğ‘¥!ğ‘¡

!âˆˆ!!"#

!âˆˆ!"#

0.02

0.01

PCAmean
PCAall

e
c
n
a
i
r
a
V
 
d
e
n
a
p
x
E

i

l

30
30

40
40

50
50

10
10

20
20

0
 
0
0

Figure	 Â 2:	 Â Per-Â­â€vector(dotted	 Â line)	 Â 	 Â and	 Â Cumulative	 Â (solid	 Â line)	 Â Variance	 Â for	 Â two	 Â sets	 Â of	 Â PCs:	 Â the	 Â 

Based	 Â on	 Â the	 Â above,	 Â we	 Â choose	 Â to	 Â proceed	 Â with	 Â the	 Â second	 Â set	 Â of	 Â PCs,	 Â generated	 Â 
from	 Â the	 Â mean-Â­â€diff.	 Â To	 Â gauge	 Â their	 Â relevance	 Â to	 Â our	 Â classification	 Â problem,	 Â we	 Â 
project	 Â the	 Â successful	 Â and	 Â failed	 Â trials	 Â on	 Â the	 Â first	 Â three	 Â PCs	 Â we	 Â found	 Â (Figure	 Â 3),	 Â 
with	 Â the	 Â projection	 Â of	 Â failed	 Â results	 Â in	 Â blue	 Â and	 Â successful	 Â in	 Â red:	 Â 	 Â 

naÃ¯ve	 Â PCA	 Â (PCAall	 Â â€“	 Â red)	 Â and	 Â mean-Â­â€diff	 Â PCA	 Â (PCAmean	 Â â€“	 Â blue)	 Â 

#PCs

700
70

60
60

100

 

50

e
c
n
a
i
r
a
V
%
 
e
v
i
t
a
u
m
u
C

l

	 Â 

Figure	 Â 3:	 Â Projection	 Â of	 Â successes	 Â and	 Â failures	 Â on	 Â first	 Â three	 Â PCs.	 Â Failure	 Â in	 Â blue,	 Â success	 Â in	 Â red.	 Â Solid	 Â line	 Â represent	 Â 
the	 Â average	 Â projection	 Â for	 Â every	 Â timestamp,	 Â dotted	 Â line	 Â is	 Â the	 Â standard	 Â deviation	 Â of	 Â the	 Â mean.	 Â The	 Â circles	 Â represent	 Â 
the	 Â time	 Â evolvement	 Â where	 Â the	 Â black	 Â circle	 Â is	 Â target	 Â selection	 Â time	 Â (t=0).	 Â Top:	 Â 3D	 Â of	 Â 3	 Â leading	 Â PCs,	 Â Bottom:	 Â 2D	 Â view	 Â 
of	 Â PC1/3	 Â and	 Â PC1/2.	 Â 

We	 Â can	 Â see	 Â that	 Â the	 Â two	 Â processes	 Â are	 Â differentiating	 Â from	 Â target	 Â selection	 Â time	 Â 
(t=0,	 Â black	 Â circle).	 Â In	 Â the	 Â bottom	 Â figures	 Â it	 Â is	 Â evident	 Â that	 Â the	 Â mean	 Â projection	 Â on	 Â 
PC1	 Â and	 Â PC2	 Â are	 Â common	 Â for	 Â both	 Â scenarios.	 Â However,	 Â PC3	 Â contains	 Â the	 Â 
information	 Â that	 Â distinguishes	 Â between	 Â the	 Â two	 Â scenarios.	 Â 
	 Â 

Classification	 Â 

	 Â 
Using	 Â the	 Â preprocessed	 Â data	 Â (three	 Â channel	 Â with	 Â 70	 Â samples	 Â each,	 Â meaning	 Â ğ‘…!"Ã—!)	 Â 	 Â 
we	 Â compared	 Â several	 Â learning	 Â algorithms:	 Â SVM	 Â (C=0.1),	 Â Logistic	 Â regression,	 Â GDA	 Â 
and	 Â K-Â­â€NN	 Â (K=10)	 Â as	 Â shown	 Â in	 Â Figure	 Â 4.	 Â To	 Â compare	 Â the	 Â different	 Â methods	 Â we	 Â used	 Â 
10-Â­â€fold	 Â  cross	 Â  validation,	 Â  maintaining	 Â 
in	 Â  each	 Â  random	 Â  group	 Â  the	 Â  original	 Â 
success/failure	 Â rate.	 Â At	 Â this	 Â stage,	 Â we	 Â ignored	 Â the	 Â temporal	 Â information,	 Â treating	 Â our	 Â 
samples	 Â as	 Â vectors	 Â in	 Â 210-Â­â€dim	 Â space.	 Â 	 Â 

Algorithm	 Â Comparison	 Â 

	 Â 

	 Â 

Classifier comparison

25

20

15

10

5

%

 
r
o
r
r

E

0

SVM

LR

GDA

10-NN

number	 Â of	 Â PCs.	 Â 

using	 Â 10-Â­â€fold	 Â cross	 Â validation	 Â 

Figure	 Â 4:	 Â SVM	 Â performance	 Â as	 Â function	 Â of	 Â 

Figure	 Â 5:	 Â Four	 Â learning	 Â algorithms	 Â performance	 Â 

Effect	 Â of	 Â Dimensionality	 Â Reduction	 Â 

We	 Â can	 Â see	 Â that	 Â SVM	 Â achieves	 Â the	 Â best	 Â performance	 Â (5%	 Â classification	 Â error).	 Â This	 Â 
can	 Â be	 Â explained	 Â by	 Â the	 Â fact	 Â that	 Â our	 Â data	 Â is	 Â still	 Â in	 Â a	 Â high	 Â dimension	 Â space,	 Â and	 Â 
learning	 Â algorithms	 Â are	 Â prone	 Â to	 Â overfitting	 Â in	 Â this	 Â case,	 Â especially	 Â where	 Â the	 Â 
number	 Â of	 Â examples	 Â is	 Â low.	 Â The	 Â exception	 Â is	 Â the	 Â SVM	 Â algorithm,	 Â that	 Â has	 Â a	 Â low	 Â 
Effective	 Â VC	 Â DimensionÂ¸	 Â and	 Â can	 Â produce	 Â robust	 Â classifiers.	 Â The	 Â GDA	 Â algorithm	 Â 
performs	 Â especially	 Â poorly,	 Â indicating	 Â that	 Â the	 Â data	 Â distribution	 Â canâ€™t	 Â be	 Â modeled	 Â as	 Â 
Gaussian.	 Â 
Based	 Â on	 Â the	 Â above	 Â results,	 Â we	 Â selected	 Â the	 Â SVM	 Â method,	 Â and	 Â revisited	 Â our	 Â decision	 Â 
to	 Â choose	 Â the	 Â three	 Â leading	 Â PCs	 Â to	 Â project	 Â the	 Â data	 Â on,	 Â as	 Â shown	 Â in	 Â Figure	 Â 5.	 Â 
We	 Â can	 Â see	 Â that	 Â 3	 Â PCs	 Â capture	 Â almost	 Â all	 Â of	 Â the	 Â relevant	 Â information	 Â of	 Â all	 Â 192	 Â 
channels.	 Â Note	 Â that	 Â the	 Â other	 Â learning	 Â algorithms	 Â we	 Â applied	 Â would	 Â not	 Â be	 Â able	 Â to	 Â 
converge	 Â to	 Â a	 Â solution	 Â if	 Â run	 Â on	 Â the	 Â original	 Â data	 Â without	 Â preliminary	 Â PCA.	 Â 
Finally,	 Â we	 Â tried	 Â to	 Â answer	 Â this	 Â question	 Â -Â­â€	 Â how	 Â soon	 Â does	 Â the	 Â primate	 Â know	 Â it	 Â made	 Â 
a	 Â mistake?	 Â To	 Â asses	 Â this,	 Â we	 Â trained	 Â per-Â­â€time-Â­â€window	 Â classifiers	 Â (100ms	 Â windows,	 Â 
every	 Â 50ms)	 Â and	 Â measured	 Â their	 Â performance,	 Â results	 Â are	 Â in	 Â Figure	 Â 6.	 Â 	 Â 
First	 Â of	 Â all,	 Â we	 Â see	 Â that	 Â 300ms	 Â before	 Â selection,	 Â error	 Â rate	 Â is	 Â ~30%,	 Â which	 Â is	 Â the	 Â 
expected	 Â performance	 Â of	 Â a	 Â classifier	 Â without	 Â information,	 Â since	 Â the	 Â failure	 Â rate	 Â is	 Â 
30%.	 Â Progressing	 Â in	 Â time,	 Â we	 Â see	 Â that	 Â the	 Â trials	 Â are	 Â classified	 Â with	 Â better	 Â precision,	 Â 
and	 Â even	 Â before	 Â the	 Â selection,	 Â a	 Â prediction	 Â can	 Â be	 Â made	 Â with	 Â around	 Â 15%	 Â error	 Â rate.	 Â 
The	 Â minimum	 Â error	 Â of	 Â classification	 Â is	 Â around	 Â 200ms	 Â after	 Â the	 Â selection.	 Â 
	 Â 
	 Â 

Effect	 Â of	 Â Time	 Â Evolution	 Â 

Introduction	 Â 	 Â 

Error	 Â Detection	 Â based	 Â on	 Â neural	 Â signals	 Â 

	 Â 
Nir	 Â Even-Â­â€Chen	 Â and	 Â Igor	 Â Berman,	 Â Electrical	 Â Engineering,	 Â Stanford	 Â 	 Â 
Brain	 Â computer	 Â interface	 Â (BCI)	 Â is	 Â a	 Â direct	 Â communication	 Â pathway	 Â between	 Â the	 Â 
brain	 Â and	 Â an	 Â external	 Â device.	 Â In	 Â Professor	 Â Shenoyâ€™s	 Â Lab	 Â primates	 Â are	 Â trained	 Â to	 Â use	 Â 
BCI	 Â to	 Â control	 Â 2D	 Â cursor	 Â to	 Â accomplish	 Â different	 Â tasks.	 Â The	 Â primate	 Â controls	 Â the	 Â 
cursor	 Â through	 Â its	 Â neural	 Â activity,	 Â which	 Â is	 Â being	 Â recorded	 Â by	 Â an	 Â array	 Â of	 Â electrodes	 Â 
implanted	 Â in	 Â the	 Â motor	 Â cortex.	 Â 
Decoding	 Â neural	 Â activity	 Â is	 Â a	 Â challenging	 Â task	 Â prone	 Â to	 Â error.	 Â In	 Â this	 Â work,	 Â we	 Â 
propose	 Â a	 Â binary	 Â classifier	 Â that	 Â uses	 Â the	 Â same	 Â neural	 Â inputs	 Â from	 Â the	 Â motor	 Â cortex	 Â 
to	 Â detect	 Â errors.	 Â 	 Â 
This	 Â classifier	 Â can	 Â alert	 Â the	 Â system	 Â when	 Â an	 Â error	 Â occurs	 Â and	 Â enable	 Â auto-Â­â€
correction,	 Â as	 Â well	 Â as	 Â provide	 Â feedback	 Â regarding	 Â the	 Â primateâ€™s	 Â understanding	 Â of	 Â 
failure	 Â and	 Â success.	 Â Eventually,	 Â this	 Â method	 Â can	 Â be	 Â used	 Â to	 Â enable	 Â paralyzed	 Â 
patients	 Â to	 Â communicate	 Â with	 Â their	 Â environment.	 Â 
In	 Â the	 Â first	 Â part	 Â of	 Â the	 Â project	 Â weâ€™ve	 Â focused	 Â on	 Â reducing	 Â the	 Â dataâ€™s	 Â dimensionality	 Â 
through	 Â preprocessing.	 Â In	 Â the	 Â second	 Â part,	 Â we	 Â compared	 Â various	 Â classification	 Â 
techniques	 Â on	 Â the	 Â data,	 Â eventually	 Â achieving	 Â a	 Â classification	 Â error	 Â rate	 Â of	 Â 5Â±1%.	 Â 
In	 Â this	 Â project	 Â we	 Â will	 Â use	 Â data	 Â from	 Â the	 Â â€˜spellingâ€™	 Â task,	 Â which	 Â includes	 Â time	 Â series	 Â of	 Â 
neural	 Â activity,	 Â and	 Â labeling	 Â of	 Â the	 Â trials	 Â (Success	 Â /	 Â Failure).	 Â In	 Â this	 Â particular	 Â task	 Â 
the	 Â primates	 Â were	 Â required	 Â to	 Â bring	 Â the	 Â cursor	 Â above	 Â a	 Â highlighted	 Â letter	 Â and	 Â hold	 Â it	 Â 
there	 Â within	 Â a	 Â set	 Â timeframe	 Â (500ms).	 Â If	 Â successful	 Â the	 Â primates	 Â received	 Â a	 Â reward.	 Â 
In	 Â the	 Â case	 Â of	 Â failure,	 Â a	 Â specific	 Â sound	 Â is	 Â produced	 Â (Figure	 Â 1).	 Â 	 Â 
Hold	 Â 500ms 
Reward 
	 Â 
600ms 
	 Â  	 Â  
Success 
600ms 
Failure 

Target	 Â appears 
	 Â  

Cursor	 Â 
movement 

Experiment	 Â Setup	 Â 

	 Â  

	 Â  

	 Â  

being	 Â help.	 Â Feedback	 Â appears	 Â 600ms	 Â after	 Â selection.	 Â 

Figure	 Â 1:	 Â Experiment	 Â Setup.	 Â A	 Â Target	 Â appears	 Â and	 Â the	 Â primate	 Â moves	 Â the	 Â cursor	 Â using	 Â BCI.	 Â Selection	 Â is	 Â made	 Â when	 Â 
it	 Â holds	 Â the	 Â cursor	 Â on	 Â top	 Â of	 Â a	 Â circle	 Â for	 Â 500ms,	 Â than	 Â the	 Â target	 Â is	 Â vanishes.	 Â The	 Â target	 Â changes	 Â to	 Â blue	 Â in	 Â while	 Â it	 Â is	 Â 

For	 Â our	 Â initial	 Â (pre-Â­â€milestone)	 Â study,	 Â we	 Â used	 Â existing	 Â data	 Â from	 Â previous	 Â 
experiments,	 Â where	 Â the	 Â reward	 Â was	 Â given	 Â immediately	 Â after	 Â the	 Â success.	 Â For	 Â the	 Â 
sake	 Â of	 Â this	 Â project,	 Â we	 Â then	 Â conducted	 Â a	 Â new	 Â experiment,	 Â where	 Â feedback	 Â was	 Â 
delayed	 Â for	 Â 600ms,	 Â so	 Â that	 Â the	 Â reward	 Â doesnâ€™t	 Â mask	 Â the	 Â primateâ€™s	 Â internal	 Â â€œerror	 Â 
recognitionâ€.	 Â 
	 Â 

Time	 Â Dimension:	 Â 

Number	 Â of	 Â Channels:	 Â 

The	 Â Data	 Â 

Preprocessing	 Â 
Dimensionality	 Â reduction	 Â 

	 Â 
The	 Â neural	 Â activity	 Â is	 Â recorded	 Â from	 Â the	 Â motor	 Â cortex	 Â (areas	 Â M1	 Â and	 Â PMd)	 Â with	 Â a	 Â 1	 Â 
kHz	 Â sampling	 Â rate	 Â from	 Â 192	 Â electrodes.	 Â In	 Â neuroscience	 Â it	 Â is	 Â common	 Â to	 Â analyze	 Â 
neuronâ€™s	 Â spikes,	 Â so	 Â our	 Â raw	 Â data	 Â includes	 Â is	 Â discrete	 Â in	 Â time	 Â and	 Â binary	 Â in	 Â values,	 Â 
with	 Â xt =1 Â if	 Â there	 Â was	 Â a	 Â spike	 Â during	 Â millisecond	 Â t.	 Â 
Based	 Â on	 Â previous	 Â studies,	 Â we	 Â expect	 Â the	 Â â€œfailure	 Â signalâ€	 Â to	 Â appear	 Â at	 Â most	 Â 200ms	 Â 
after	 Â the	 Â actual	 Â mistaken	 Â selection,	 Â so	 Â we	 Â use	 Â samples	 Â from	 Â 300ms	 Â before	 Â letter	 Â 
choice	 Â to	 Â 400ms	 Â after.	 Â The	 Â experiment	 Â was	 Â conducted	 Â on	 Â the	 Â same	 Â primate	 Â for	 Â two	 Â 
days;	 Â 1988	 Â trials	 Â were	 Â recorded	 Â with	 Â a	 Â 30%	 Â failure	 Â rate.	 Â 	 Â 
Each	 Â learning	 Â examples	 Â included	 Â 700	 Â time	 Â samples	 Â in	 Â 192	 Â channels,	 Â resulting	 Â in	 Â 
700Ã—192=134,400	 Â dimensions.	 Â Before	 Â proceeding	 Â to	 Â run	 Â learning	 Â algorithms	 Â on	 Â 
the	 Â data,	 Â weâ€™ll	 Â take	 Â some	 Â steps	 Â to	 Â reduce	 Â dimensionality:	 Â 
Based	 Â on	 Â prior	 Â studies,	 Â we	 Â expect	 Â the	 Â failure	 Â signal	 Â to	 Â last	 Â for	 Â tens	 Â of	 Â milliseconds.	 Â 
Furthermore,	 Â the	 Â usual	 Â firing	 Â rate	 Â for	 Â neurons	 Â in	 Â around	 Â 15-Â­â€20Hz,	 Â meaning	 Â that	 Â the	 Â 
1ms	 Â resolution	 Â produces	 Â very	 Â sparse	 Â data.	 Â Based	 Â on	 Â this	 Â evidence,	 Â and	 Â as	 Â is	 Â 
common	 Â in	 Â the	 Â field,	 Â we	 Â expect	 Â not	 Â to	 Â lose	 Â any	 Â critical	 Â information	 Â by	 Â downsampling	 Â 
the	 Â data	 Â and	 Â grouping	 Â the	 Â samples	 Â into	 Â 10ms	 Â bins	 Â of	 Â int	 Â values,	 Â resulting	 Â in	 Â only	 Â 70	 Â 
time	 Â samples	 Â per	 Â channel	 Â per	 Â trial.	 Â 
In	 Â the	 Â channel	 Â domain	 Â we	 Â applied	 Â per-Â­â€time-Â­â€frame	 Â PCA	 Â to	 Â reduce	 Â the	 Â number	 Â of	 Â 
channels.	 Â We	 Â decided	 Â not	 Â to	 Â normalize	 Â the	 Â variance,	 Â since	 Â all	 Â channels	 Â represented	 Â 
the	 Â same	 Â type	 Â of	 Â data	 Â (#spikes/10ms),	 Â and	 Â intuitively,	 Â active	 Â channels	 Â should	 Â have	 Â 
more	 Â influence	 Â than	 Â sparse	 Â ones.	 Â 	 Â 
Weâ€™ve	 Â tried	 Â two	 Â different	 Â approaches	 Â to	 Â performing	 Â the	 Â PCA,	 Â and	 Â we	 Â compared	 Â 
them	 Â by	 Â the	 Â %	 Â of	 Â variance	 Â represented	 Â in	 Â the	 Â first	 Â n	 Â components,	 Â using	 Â the	 Â rule	 Â of	 Â 
thumb	 Â that	 Â itâ€™s	 Â better	 Â to	 Â have	 Â fewer	 Â components	 Â that	 Â capture	 Â more	 Â variance:	 Â 
NaÃ¯ve	 Â  PCA:	 Â Our	 Â first	 Â approach	 Â was	 Â to	 Â concatenate	 Â all	 Â the	 Â timeframes	 Â of	 Â all	 Â trials	 Â 
(success	 Â and	 Â failure),	 Â and	 Â extract	 Â the	 Â primary	 Â components	 Â from	 Â the	 Â resulting	 Â 
covariance	 Â matrix.	 Â The	 Â result	 Â is	 Â shown	 Â in	 Â Figure	 Â 2	 Â below	 Â (red),	 Â demonstrating	 Â that	 Â 
~40	 Â components	 Â are	 Â required	 Â to	 Â capture	 Â 50%	 Â of	 Â the	 Â variance.	 Â 
Mean-Â­â€Diff	 Â PCA:	 Â Following	 Â these	 Â results,	 Â we	 Â came	 Â to	 Â assume	 Â that	 Â the	 Â first	 Â primary	 Â 
components	 Â we	 Â capture	 Â actually	 Â represent	 Â the	 Â kinematic	 Â signals	 Â produced	 Â by	 Â the	 Â 
brain,	 Â which	 Â are	 Â strong	 Â and	 Â common	 Â to	 Â both	 Â failure	 Â and	 Â success	 Â trials.	 Â This	 Â model	 Â 
can	 Â be	 Â represented	 Â a	 Â sum	 Â of	 Â three	 Â random	 Â processes:	 Â 
x!t =ğ‘£!t +ğ‘£!\!t +ğ‘£!t Â  Â  Â (1)	 Â 
Where	 Â ğ‘£!t 	 Â is	 Â kinematic	 Â component	 Â that	 Â controls	 Â the	 Â cursor,	 Â ğ‘£!\!t 	 Â is	 Â the	 Â 
component	 Â caused	 Â by	 Â success\failure	 Â and	 Â ğ‘£!t	 Â is	 Â noise.	 Â We	 Â assume	 Â that	 Â the	 Â three	 Â 
processes	 Â  are	 Â 
independent	 Â  and	 Â  have	 Â  zero	 Â  mean.	 Â  To	 Â  extract	 Â  the	 Â  second	 Â 
component Â ğ‘£!\!t,	 Â we	 Â applied	 Â the	 Â following	 Â method:	 Â 
	 Â 

Î”ÂµÎ¼ğ‘¡ =1ğ‘!

	 Â 
â‰ˆğ‘£!ğ‘¡ âˆ’ğ‘£!ğ‘¡ Â  Â  Â  Â (2)	 Â 
The	 Â resulting	 Â mean-Â­â€diff	 Â Î”ÂµÎ¼âˆˆâ„›!"Ã—!"#	 Â captures	 Â the	 Â average	 Â differences	 Â between	 Â 
successful	 Â and	 Â failed	 Â trials.	 Â Since	 Â the	 Â kinematic	 Â component	 Â is	 Â generally	 Â orthogonal	 Â 
to	 Â the	 Â {fail/success}	 Â labelling,	 Â subtracting	 Â the	 Â two	 Â averages	 Â reduces	 Â its	 Â effect.	 Â 
Eventually,	 Â we	 Â ran	 Â a	 Â PCA	 Â on	 Â the	 Â matrix	 Â ğ›¥ğœ‡	 Â and	 Â examined	 Â the	 Â resulting	 Â vectors.	 Â As	 Â 
shown	 Â in	 Â Figure	 Â 2	 Â below,	 Â the	 Â 3	 Â first	 Â PCs	 Â together	 Â capture	 Â 90%	 Â of	 Â the	 Â variance:	 Â 

âˆ’1ğ‘!

ğ‘¥!ğ‘¡

ğ‘¥!ğ‘¡

!âˆˆ!!"#

!âˆˆ!"#

0.02

0.01

PCAmean
PCAall

e
c
n
a
i
r
a
V
 
d
e
n
a
p
x
E

i

l

30
30

40
40

50
50

10
10

20
20

0
 
0
0

Figure	 Â 2:	 Â Per-Â­â€vector(dotted	 Â line)	 Â 	 Â and	 Â Cumulative	 Â (solid	 Â line)	 Â Variance	 Â for	 Â two	 Â sets	 Â of	 Â PCs:	 Â the	 Â 

Based	 Â on	 Â the	 Â above,	 Â we	 Â choose	 Â to	 Â proceed	 Â with	 Â the	 Â second	 Â set	 Â of	 Â PCs,	 Â generated	 Â 
from	 Â the	 Â mean-Â­â€diff.	 Â To	 Â gauge	 Â their	 Â relevance	 Â to	 Â our	 Â classification	 Â problem,	 Â we	 Â 
project	 Â the	 Â successful	 Â and	 Â failed	 Â trials	 Â on	 Â the	 Â first	 Â three	 Â PCs	 Â we	 Â found	 Â (Figure	 Â 3),	 Â 
with	 Â the	 Â projection	 Â of	 Â failed	 Â results	 Â in	 Â blue	 Â and	 Â successful	 Â in	 Â red:	 Â 	 Â 

naÃ¯ve	 Â PCA	 Â (PCAall	 Â â€“	 Â red)	 Â and	 Â mean-Â­â€diff	 Â PCA	 Â (PCAmean	 Â â€“	 Â blue)	 Â 

#PCs

700
70

60
60

100

 

50

e
c
n
a
i
r
a
V
%
 
e
v
i
t
a
u
m
u
C

l

	 Â 

Figure	 Â 3:	 Â Projection	 Â of	 Â successes	 Â and	 Â failures	 Â on	 Â first	 Â three	 Â PCs.	 Â Failure	 Â in	 Â blue,	 Â success	 Â in	 Â red.	 Â Solid	 Â line	 Â represent	 Â 
the	 Â average	 Â projection	 Â for	 Â every	 Â timestamp,	 Â dotted	 Â line	 Â is	 Â the	 Â standard	 Â deviation	 Â of	 Â the	 Â mean.	 Â The	 Â circles	 Â represent	 Â 
the	 Â time	 Â evolvement	 Â where	 Â the	 Â black	 Â circle	 Â is	 Â target	 Â selection	 Â time	 Â (t=0).	 Â Top:	 Â 3D	 Â of	 Â 3	 Â leading	 Â PCs,	 Â Bottom:	 Â 2D	 Â view	 Â 
of	 Â PC1/3	 Â and	 Â PC1/2.	 Â 

We	 Â can	 Â see	 Â that	 Â the	 Â two	 Â processes	 Â are	 Â differentiating	 Â from	 Â target	 Â selection	 Â time	 Â 
(t=0,	 Â black	 Â circle).	 Â In	 Â the	 Â bottom	 Â figures	 Â it	 Â is	 Â evident	 Â that	 Â the	 Â mean	 Â projection	 Â on	 Â 
PC1	 Â and	 Â PC2	 Â are	 Â common	 Â for	 Â both	 Â scenarios.	 Â However,	 Â PC3	 Â contains	 Â the	 Â 
information	 Â that	 Â distinguishes	 Â between	 Â the	 Â two	 Â scenarios.	 Â 
	 Â 

Classification	 Â 

	 Â 
Using	 Â the	 Â preprocessed	 Â data	 Â (three	 Â channel	 Â with	 Â 70	 Â samples	 Â each,	 Â meaning	 Â ğ‘…!"Ã—!)	 Â 	 Â 
we	 Â compared	 Â several	 Â learning	 Â algorithms:	 Â SVM	 Â (C=0.1),	 Â Logistic	 Â regression,	 Â GDA	 Â 
and	 Â K-Â­â€NN	 Â (K=10)	 Â as	 Â shown	 Â in	 Â Figure	 Â 4.	 Â To	 Â compare	 Â the	 Â different	 Â methods	 Â we	 Â used	 Â 
10-Â­â€fold	 Â  cross	 Â  validation,	 Â  maintaining	 Â 
in	 Â  each	 Â  random	 Â  group	 Â  the	 Â  original	 Â 
success/failure	 Â rate.	 Â At	 Â this	 Â stage,	 Â we	 Â ignored	 Â the	 Â temporal	 Â information,	 Â treating	 Â our	 Â 
samples	 Â as	 Â vectors	 Â in	 Â 210-Â­â€dim	 Â space.	 Â 	 Â 

Algorithm	 Â Comparison	 Â 

	 Â 

	 Â 

Classifier comparison

25

20

15

10

5

%

 
r
o
r
r

E

0

SVM

LR

GDA

10-NN

number	 Â of	 Â PCs.	 Â 

using	 Â 10-Â­â€fold	 Â cross	 Â validation	 Â 

Figure	 Â 4:	 Â SVM	 Â performance	 Â as	 Â function	 Â of	 Â 

Figure	 Â 5:	 Â Four	 Â learning	 Â algorithms	 Â performance	 Â 

Effect	 Â of	 Â Dimensionality	 Â Reduction	 Â 

We	 Â can	 Â see	 Â that	 Â SVM	 Â achieves	 Â the	 Â best	 Â performance	 Â (5%	 Â classification	 Â error).	 Â This	 Â 
can	 Â be	 Â explained	 Â by	 Â the	 Â fact	 Â that	 Â our	 Â data	 Â is	 Â still	 Â in	 Â a	 Â high	 Â dimension	 Â space,	 Â and	 Â 
learning	 Â algorithms	 Â are	 Â prone	 Â to	 Â overfitting	 Â in	 Â this	 Â case,	 Â especially	 Â where	 Â the	 Â 
number	 Â of	 Â examples	 Â is	 Â low.	 Â The	 Â exception	 Â is	 Â the	 Â SVM	 Â algorithm,	 Â that	 Â has	 Â a	 Â low	 Â 
Effective	 Â VC	 Â DimensionÂ¸	 Â and	 Â can	 Â produce	 Â robust	 Â classifiers.	 Â The	 Â GDA	 Â algorithm	 Â 
performs	 Â especially	 Â poorly,	 Â indicating	 Â that	 Â the	 Â data	 Â distribution	 Â canâ€™t	 Â be	 Â modeled	 Â as	 Â 
Gaussian.	 Â 
Based	 Â on	 Â the	 Â above	 Â results,	 Â we	 Â selected	 Â the	 Â SVM	 Â method,	 Â and	 Â revisited	 Â our	 Â decision	 Â 
to	 Â choose	 Â the	 Â three	 Â leading	 Â PCs	 Â to	 Â project	 Â the	 Â data	 Â on,	 Â as	 Â shown	 Â in	 Â Figure	 Â 5.	 Â 
We	 Â can	 Â see	 Â that	 Â 3	 Â PCs	 Â capture	 Â almost	 Â all	 Â of	 Â the	 Â relevant	 Â information	 Â of	 Â all	 Â 192	 Â 
channels.	 Â Note	 Â that	 Â the	 Â other	 Â learning	 Â algorithms	 Â we	 Â applied	 Â would	 Â not	 Â be	 Â able	 Â to	 Â 
converge	 Â to	 Â a	 Â solution	 Â if	 Â run	 Â on	 Â the	 Â original	 Â data	 Â without	 Â preliminary	 Â PCA.	 Â 
Finally,	 Â we	 Â tried	 Â to	 Â answer	 Â this	 Â question	 Â -Â­â€	 Â how	 Â soon	 Â does	 Â the	 Â primate	 Â know	 Â it	 Â made	 Â 
a	 Â mistake?	 Â To	 Â asses	 Â this,	 Â we	 Â trained	 Â per-Â­â€time-Â­â€window	 Â classifiers	 Â (100ms	 Â windows,	 Â 
every	 Â 50ms)	 Â and	 Â measured	 Â their	 Â performance,	 Â results	 Â are	 Â in	 Â Figure	 Â 6.	 Â 	 Â 
First	 Â of	 Â all,	 Â we	 Â see	 Â that	 Â 300ms	 Â before	 Â selection,	 Â error	 Â rate	 Â is	 Â ~30%,	 Â which	 Â is	 Â the	 Â 
expected	 Â performance	 Â of	 Â a	 Â classifier	 Â without	 Â information,	 Â since	 Â the	 Â failure	 Â rate	 Â is	 Â 
30%.	 Â Progressing	 Â in	 Â time,	 Â we	 Â see	 Â that	 Â the	 Â trials	 Â are	 Â classified	 Â with	 Â better	 Â precision,	 Â 
and	 Â even	 Â before	 Â the	 Â selection,	 Â a	 Â prediction	 Â can	 Â be	 Â made	 Â with	 Â around	 Â 15%	 Â error	 Â rate.	 Â 
The	 Â minimum	 Â error	 Â of	 Â classification	 Â is	 Â around	 Â 200ms	 Â after	 Â the	 Â selection.	 Â 
	 Â 
	 Â 

Effect	 Â of	 Â Time	 Â Evolution	 Â 

Figure	 Â 6:	 Â Performance	 Â of	 Â time-Â­â€window	 Â classifiers,	 Â windows	 Â of	 Â 100ms	 Â (blue).	 Â 	 Â 

Classification	 Â based	 Â on	 Â all	 Â time	 Â samples	 Â (red)	 Â 

	 Â 

	 Â 

	 Â 
	 Â 

Summary	 Â 

In	 Â this	 Â project	 Â weâ€™ve	 Â shown	 Â that	 Â data	 Â related	 Â to	 Â the	 Â success	 Â or	 Â failure	 Â of	 Â a	 Â trial	 Â can	 Â 
be	 Â extracted	 Â and	 Â expressed	 Â in	 Â three	 Â primary	 Â components.	 Â Furthermore,	 Â an	 Â SVM	 Â 
classifier	 Â can	 Â be	 Â trained	 Â on	 Â this	 Â data	 Â to	 Â achieve	 Â and	 Â error	 Â rate	 Â of	 Â 5Â±1%.	 Â Indication	 Â 
about	 Â target	 Â selection	 Â error	 Â is	 Â available	 Â even	 Â before	 Â the	 Â error	 Â occurs,	 Â but	 Â is	 Â most	 Â 
evident	 Â 200ms	 Â after	 Â the	 Â selection.	 Â 
Error	 Â detection	 Â can	 Â improve	 Â the	 Â speed	 Â and	 Â precision	 Â of	 Â speller-Â­â€based	 Â BCI	 Â 
applications.	 Â 	 Â 
For	 Â future	 Â work,	 Â we	 Â would	 Â like	 Â to	 Â implement	 Â a	 Â real-Â­â€time	 Â error	 Â detector	 Â that	 Â would	 Â 
prevent	 Â wrong	 Â selections.	 Â In	 Â addition,	 Â we	 Â would	 Â try	 Â training	 Â separate	 Â classifiers	 Â for	 Â 
each	 Â time	 Â window,	 Â and	 Â then	 Â bagging	 Â their	 Â results	 Â into	 Â a	 Â single	 Â output.	 Â For	 Â example,	 Â 
by	 Â using	 Â 10	 Â SVMs	 Â aggregated	 Â by	 Â a	 Â logistic	 Â regression	 Â classifier.	 Â 	 Â 
Finally,	 Â given	 Â that	 Â false-Â­â€positive	 Â and	 Â false-Â­â€negative	 Â classifications	 Â can	 Â have	 Â different	 Â 
costs,	 Â we	 Â should	 Â train	 Â a	 Â weighted	 Â classifier,	 Â that	 Â takes	 Â these	 Â costs	 Â into	 Â account	 Â and	 Â 
minimizes	 Â the	 Â expected	 Â cost.	 Â 	 Â 
The	 Â experiments	 Â were	 Â done	 Â in	 Â prof.	 Â Shenoyâ€™s	 Â lab	 Â and	 Â with	 Â the	 Â help	 Â of	 Â Sergey	 Â 
Stavisky	 Â and	 Â Jonathan	 Â Kao.	 Â 
1.  Kao,  J.  C.,  Stavisky,  S.  D.,  Sussillo,  D.,  Nuyujukian,  P.,  &  Shenoy,  K.  V. 
Decoders.	 Â 

Information  Systems  Opportunities 

Acknowledgments	 Â 	 Â 

in  Brainâ€“Machine 

Interface 

Reference	 Â 

(2014). 

2.  Bottou, L., Cortes, C., & Vapnik, V. (1994). On the effective VC 

dimension. 

