Improving Taxi Revenue with Reinforcement Learning

Jingshu Wang1 and Benjamin Lampert2

Abstract— In recent news there has been controversy sur-
rounding Uber and Lyft taking business away from taxi drivers.
In light of this controversy, we explored how machine learning
could optimize a taxi drivers income by understanding the
ﬂow of demand throughout the day. We ﬁrst used historical
New York City (NYC) transaction data from 2013 to build
summary statistics of the demand, fare, duration, and transition
probabilities for different zipcodes and times across NYC.
We then applied a reinforcement learning model to maximize
the total revenue generated by an individual driver. Utilizing
dynamic programming we backtested our model on 10k drivers
and found that if they followed an optimal policy they would
earn on average ∼20% more per day than following a non-
optimal policy.

I. INTRODUCTION

With the emergence of traditional taxi competitors such
as Lyft and Uber, many New York Taxi cab drivers are con-
cerned with unfair competition. These companies leverage
analytics, mobile apps, and other data sources to adjust rates
based on a wide set of features. Uber, for example, has a team
dedicated to analyzing demand and usage across cities[1].
Providing similar services to normal drivers therefore would
be an important step to allow them to compete. Our ap-
proach to this problem was ﬁrst to analyze and preprocess
a collection of 2013 taxi records. We used a linear model
to understand which features best predict demand, and a
reinforcement
to maximize an individual
drivers income based on a set of summary statistics.

learning model

Fig. 1. Plot of each pickup location overlaid on a map of New York City.
Each dot represents a transaction during a single day in January. Points of
activity include Manhattan (left middle) and the JFK airport (lower right).

To handle the computational difﬁculties in training on
large collections, we subsampled our data. We only examined
transactions from January which resulted in a dataset of 14.5
million trips. For linear regression, we sampled and ﬁt only
0.1% of the data. For reinforcement learning, we trained our
model using the full dataset, but only backtested on sampled
drivers from Wednesday records.

II. DATA

B. Preprocessing

To improve our location features we decided to bin the raw
GPS coordinates of the pickup and drop-off location into a
zipcode. This was accomplished using the python package
Geodis[3]. We also applied a ﬁlter at this point to exclude
all data not within NYC (ie, not reported correctly or did not
successfully reverse geocode).

A. NYC Transactions

The dataset we used is from the 2013 NYC Taxi records
made available by a Freedom of Information act and ﬁrst
published by Chris Whong[2]. The collection contains all
173 million taxi transactions for 2013, where each entry
details the relevant information for each taxi trip. A summary
of these features can be found in Table I.

Label
pickup datetime
dropoff datetime
pickup longitude
pickup latitude
dropoff longitude
dropoff latitude
total amount

Example
2013/01/05 6:49:41 PM
2013/01/05 6:54:23 PM
-74.004707
40.73777
-74.009834
40.726002
$7

TABLE I

EXAMPLE OF SOME OF THE FEATURES CONTAINED IN THE RAW DATA.

1W. Jingshu jingshuw@stanford.edu
2B. Lampert lampertb@stanford.edu

The next goal was to build a feature set that would allow
us to train a policy on the data. We ﬁrst iterated through all
of the records to build a set of summary statistics related
to the fare information. These data are grouped by the key
(hour, pickupzipcode, dropoffzipcode). An example of one
ﬁeld from the summary data is as follows:

The next step was to get an estimate for the searching time
for the next customer. Because our dataset only observes
the time between pickup and drop-offs, we lack direct
information related to how long the driver searched in an
area to pick up a new customer.

To infer this data, we looked at drivers who picked up
new customers in the same zipcode as where they had just
dropped off a customer. This provided a time delta associated

Improving Taxi Revenue with Reinforcement Learning

Jingshu Wang1 and Benjamin Lampert2

Abstract— In recent news there has been controversy sur-
rounding Uber and Lyft taking business away from taxi drivers.
In light of this controversy, we explored how machine learning
could optimize a taxi drivers income by understanding the
ﬂow of demand throughout the day. We ﬁrst used historical
New York City (NYC) transaction data from 2013 to build
summary statistics of the demand, fare, duration, and transition
probabilities for different zipcodes and times across NYC.
We then applied a reinforcement learning model to maximize
the total revenue generated by an individual driver. Utilizing
dynamic programming we backtested our model on 10k drivers
and found that if they followed an optimal policy they would
earn on average ∼20% more per day than following a non-
optimal policy.

I. INTRODUCTION

With the emergence of traditional taxi competitors such
as Lyft and Uber, many New York Taxi cab drivers are con-
cerned with unfair competition. These companies leverage
analytics, mobile apps, and other data sources to adjust rates
based on a wide set of features. Uber, for example, has a team
dedicated to analyzing demand and usage across cities[1].
Providing similar services to normal drivers therefore would
be an important step to allow them to compete. Our ap-
proach to this problem was ﬁrst to analyze and preprocess
a collection of 2013 taxi records. We used a linear model
to understand which features best predict demand, and a
reinforcement
to maximize an individual
drivers income based on a set of summary statistics.

learning model

Fig. 1. Plot of each pickup location overlaid on a map of New York City.
Each dot represents a transaction during a single day in January. Points of
activity include Manhattan (left middle) and the JFK airport (lower right).

To handle the computational difﬁculties in training on
large collections, we subsampled our data. We only examined
transactions from January which resulted in a dataset of 14.5
million trips. For linear regression, we sampled and ﬁt only
0.1% of the data. For reinforcement learning, we trained our
model using the full dataset, but only backtested on sampled
drivers from Wednesday records.

II. DATA

B. Preprocessing

To improve our location features we decided to bin the raw
GPS coordinates of the pickup and drop-off location into a
zipcode. This was accomplished using the python package
Geodis[3]. We also applied a ﬁlter at this point to exclude
all data not within NYC (ie, not reported correctly or did not
successfully reverse geocode).

A. NYC Transactions

The dataset we used is from the 2013 NYC Taxi records
made available by a Freedom of Information act and ﬁrst
published by Chris Whong[2]. The collection contains all
173 million taxi transactions for 2013, where each entry
details the relevant information for each taxi trip. A summary
of these features can be found in Table I.

Label
pickup datetime
dropoff datetime
pickup longitude
pickup latitude
dropoff longitude
dropoff latitude
total amount

Example
2013/01/05 6:49:41 PM
2013/01/05 6:54:23 PM
-74.004707
40.73777
-74.009834
40.726002
$7

TABLE I

EXAMPLE OF SOME OF THE FEATURES CONTAINED IN THE RAW DATA.

1W. Jingshu jingshuw@stanford.edu
2B. Lampert lampertb@stanford.edu

The next goal was to build a feature set that would allow
us to train a policy on the data. We ﬁrst iterated through all
of the records to build a set of summary statistics related
to the fare information. These data are grouped by the key
(hour, pickupzipcode, dropoffzipcode). An example of one
ﬁeld from the summary data is as follows:

The next step was to get an estimate for the searching time
for the next customer. Because our dataset only observes
the time between pickup and drop-offs, we lack direct
information related to how long the driver searched in an
area to pick up a new customer.

To infer this data, we looked at drivers who picked up
new customers in the same zipcode as where they had just
dropped off a customer. This provided a time delta associated

In a traditional reinforcement learning model the intention
of the driver would be an important feature. In practice,
although a driver may intend to drive to a zipcode z, they can
pick up customer along the way. However, it’s impossible to
know a driver’s intention from historical data as we only
know where he ends up picking the next customer. Thus,
to reduce the difﬁculty in estimating the model parameters
from historical data, we consider a simpliﬁed policy:

• Policy: π(z, t) = z1 the next pickup zipcode from state
(z, t). The driver goes to z1 directly and will keep
searching inside z1 until he picks up the next customer
at z1

Then, we can write out the value function as:

(cid:35)

(cid:35)

III. MODEL

= E

R(z1, Tpick) +

P (z1, z(cid:48), Tpick)V π(z(cid:48), Tnext)

Statistic
Hour

PickupZipCode
DropOffZipCode

Number of transistions
Mean Trip Distance
Mean Trip Duration

Mean Fare

Transition Probability

Value
10am
10012
10011
302

1.19 Miles
7.9 Minutes

$8.58
7.36%

TABLE II

EXAMPLE ENTRY IN THE SUMMARY STATISTIC DATA FOR PICKING UP A

PASSENGER AT ZIPCODE 10012, WHO IS TRAVELING TO 10011,

BETWEEN 10-11AM ON WEDNESDAY.

with how long they were searching between customers.
We then modeled the searching time as an exponential
distribution to get the expected waiting time for each zipcode
and hour. More details will be found in Section III.

With these summary statistics we were then able to apply

our model.

A. Linear Regression

To ﬁnd the relationship between taxi demand and various
factors (geographical locations, time and date, weather etc.),
we use a linear regression model to decompose the sources
of variances. For the response, we aggregated the number of
trips every three hours for each zipcode. For predictors, we
consider zipcodes, the hours in a day, and the days in a week
as three major factors along with their two-term interactions.
We also include weather information during the 3 hour time
interval as additional predictors. The model is:

Yijk = µ + αi + βj + γk + (αβ)ij + (αγ)ik + (βγ)jk +

p(cid:88)

asXs + ijk

s=1

where αi are the main effects for zipcodes, βj are the main
effects for Monday, Tuesday, ··· , Sunday, and γk are the 3
hour intervals for a day. {X1, X2,··· , Xp} are the additional
predictors of weather information.
B. Reinforcement Learning

1) Framework: When a taxi driver drops off a customer,
and is looking for new business, there are two actions they
can make. Either they can stay in the area and wait for
passenger there, or travel to a new location. Travelling to a
new location has an associated cost, but if chosen correctly
the new location will impact future trips the revenue for
the day. We ﬁnd that a reinforcement learning model comes
naturally to quantify the above optimization procedure. Using
zipcodes as geographical units, we can deﬁne the states,
reward and value functions as:

• States: (z, t) a dropoff zipcode at a corresponding time
• Reward: R(z1, t1) average trip fair of one trip for a

pickup zipcode z1 at time t1

• Value function: V (z, t) expected total revenue to the

end of the day starting from state (s, t)

= E[f (π(z, t), Z(cid:48), Tpick) + V π(Z(cid:48), Tnext)]

= E

(P (z1, z(cid:48), Tpick)f (z1, z(cid:48), Tpick) + V π(z(cid:48), Tnext))

V π(z, t)

(cid:34)(cid:88)
(cid:34)

z(cid:48)

(cid:88)

z(cid:48)

where Z(cid:48), Tpick and Tnext represent the random zipcode
that the customer at z1 wants to go, the random pickup
time at z1 and the random dropoff time at Z(cid:48) respectively.
f (z1, z(cid:48), Tpick) is the trip fare from z1 to z(cid:48) at time t and
P (z1, z(cid:48), t) is a customers transition probability from z1 to
z(cid:48). The above equations hold after assuming that all the
random variables are independent. To further quantify Tpick
and Tnext, we have

Tpick = t + ∆travel(z, z1) + ∆search(z1)
Tnext = Tpick + ∆trip(z1, z(cid:48))

where ∆travel, ∆search and ∆trip are random time intervals
for the time cost traveling from z to z1 without a passenger,
searching for the next customer at z1, and driving a passen-
ger from z1 to z(cid:48) respectively. Finally, to approximate the
expectation in (1), we replace all the random time with their
expectations. As the value function is almost linear in t and
our estimates of R(z1, t1) and P (z1, z(cid:48), t) will be piecewise
constant (described later) in hour, this approximation should
be accurate enough most of the time. We then deﬁne the
optimal value function as:

V (cid:63)(z, t) = max

π

V π(z, t)

Then, it will satisfy:

V (cid:63)(z, t)

(cid:40)

= max

z1

R(z1, tpick) +

where

(cid:41)

P (z1, z(cid:48), tpick)V (cid:63)(z(cid:48), tnext)

(cid:88)

z(cid:48)

tpick = t + δtravel(z, z1, t) + δsearch(z1, t + δtravel(z, z1, t))
tnext = tpick + δtrip(z1, z(cid:48), tpick)

Improving Taxi Revenue with Reinforcement Learning

Jingshu Wang1 and Benjamin Lampert2

Abstract— In recent news there has been controversy sur-
rounding Uber and Lyft taking business away from taxi drivers.
In light of this controversy, we explored how machine learning
could optimize a taxi drivers income by understanding the
ﬂow of demand throughout the day. We ﬁrst used historical
New York City (NYC) transaction data from 2013 to build
summary statistics of the demand, fare, duration, and transition
probabilities for different zipcodes and times across NYC.
We then applied a reinforcement learning model to maximize
the total revenue generated by an individual driver. Utilizing
dynamic programming we backtested our model on 10k drivers
and found that if they followed an optimal policy they would
earn on average ∼20% more per day than following a non-
optimal policy.

I. INTRODUCTION

With the emergence of traditional taxi competitors such
as Lyft and Uber, many New York Taxi cab drivers are con-
cerned with unfair competition. These companies leverage
analytics, mobile apps, and other data sources to adjust rates
based on a wide set of features. Uber, for example, has a team
dedicated to analyzing demand and usage across cities[1].
Providing similar services to normal drivers therefore would
be an important step to allow them to compete. Our ap-
proach to this problem was ﬁrst to analyze and preprocess
a collection of 2013 taxi records. We used a linear model
to understand which features best predict demand, and a
reinforcement
to maximize an individual
drivers income based on a set of summary statistics.

learning model

Fig. 1. Plot of each pickup location overlaid on a map of New York City.
Each dot represents a transaction during a single day in January. Points of
activity include Manhattan (left middle) and the JFK airport (lower right).

To handle the computational difﬁculties in training on
large collections, we subsampled our data. We only examined
transactions from January which resulted in a dataset of 14.5
million trips. For linear regression, we sampled and ﬁt only
0.1% of the data. For reinforcement learning, we trained our
model using the full dataset, but only backtested on sampled
drivers from Wednesday records.

II. DATA

B. Preprocessing

To improve our location features we decided to bin the raw
GPS coordinates of the pickup and drop-off location into a
zipcode. This was accomplished using the python package
Geodis[3]. We also applied a ﬁlter at this point to exclude
all data not within NYC (ie, not reported correctly or did not
successfully reverse geocode).

A. NYC Transactions

The dataset we used is from the 2013 NYC Taxi records
made available by a Freedom of Information act and ﬁrst
published by Chris Whong[2]. The collection contains all
173 million taxi transactions for 2013, where each entry
details the relevant information for each taxi trip. A summary
of these features can be found in Table I.

Label
pickup datetime
dropoff datetime
pickup longitude
pickup latitude
dropoff longitude
dropoff latitude
total amount

Example
2013/01/05 6:49:41 PM
2013/01/05 6:54:23 PM
-74.004707
40.73777
-74.009834
40.726002
$7

TABLE I

EXAMPLE OF SOME OF THE FEATURES CONTAINED IN THE RAW DATA.

1W. Jingshu jingshuw@stanford.edu
2B. Lampert lampertb@stanford.edu

The next goal was to build a feature set that would allow
us to train a policy on the data. We ﬁrst iterated through all
of the records to build a set of summary statistics related
to the fare information. These data are grouped by the key
(hour, pickupzipcode, dropoffzipcode). An example of one
ﬁeld from the summary data is as follows:

The next step was to get an estimate for the searching time
for the next customer. Because our dataset only observes
the time between pickup and drop-offs, we lack direct
information related to how long the driver searched in an
area to pick up a new customer.

To infer this data, we looked at drivers who picked up
new customers in the same zipcode as where they had just
dropped off a customer. This provided a time delta associated

In a traditional reinforcement learning model the intention
of the driver would be an important feature. In practice,
although a driver may intend to drive to a zipcode z, they can
pick up customer along the way. However, it’s impossible to
know a driver’s intention from historical data as we only
know where he ends up picking the next customer. Thus,
to reduce the difﬁculty in estimating the model parameters
from historical data, we consider a simpliﬁed policy:

• Policy: π(z, t) = z1 the next pickup zipcode from state
(z, t). The driver goes to z1 directly and will keep
searching inside z1 until he picks up the next customer
at z1

Then, we can write out the value function as:

(cid:35)

(cid:35)

III. MODEL

= E

R(z1, Tpick) +

P (z1, z(cid:48), Tpick)V π(z(cid:48), Tnext)

Statistic
Hour

PickupZipCode
DropOffZipCode

Number of transistions
Mean Trip Distance
Mean Trip Duration

Mean Fare

Transition Probability

Value
10am
10012
10011
302

1.19 Miles
7.9 Minutes

$8.58
7.36%

TABLE II

EXAMPLE ENTRY IN THE SUMMARY STATISTIC DATA FOR PICKING UP A

PASSENGER AT ZIPCODE 10012, WHO IS TRAVELING TO 10011,

BETWEEN 10-11AM ON WEDNESDAY.

with how long they were searching between customers.
We then modeled the searching time as an exponential
distribution to get the expected waiting time for each zipcode
and hour. More details will be found in Section III.

With these summary statistics we were then able to apply

our model.

A. Linear Regression

To ﬁnd the relationship between taxi demand and various
factors (geographical locations, time and date, weather etc.),
we use a linear regression model to decompose the sources
of variances. For the response, we aggregated the number of
trips every three hours for each zipcode. For predictors, we
consider zipcodes, the hours in a day, and the days in a week
as three major factors along with their two-term interactions.
We also include weather information during the 3 hour time
interval as additional predictors. The model is:

Yijk = µ + αi + βj + γk + (αβ)ij + (αγ)ik + (βγ)jk +

p(cid:88)

asXs + ijk

s=1

where αi are the main effects for zipcodes, βj are the main
effects for Monday, Tuesday, ··· , Sunday, and γk are the 3
hour intervals for a day. {X1, X2,··· , Xp} are the additional
predictors of weather information.
B. Reinforcement Learning

1) Framework: When a taxi driver drops off a customer,
and is looking for new business, there are two actions they
can make. Either they can stay in the area and wait for
passenger there, or travel to a new location. Travelling to a
new location has an associated cost, but if chosen correctly
the new location will impact future trips the revenue for
the day. We ﬁnd that a reinforcement learning model comes
naturally to quantify the above optimization procedure. Using
zipcodes as geographical units, we can deﬁne the states,
reward and value functions as:

• States: (z, t) a dropoff zipcode at a corresponding time
• Reward: R(z1, t1) average trip fair of one trip for a

pickup zipcode z1 at time t1

• Value function: V (z, t) expected total revenue to the

end of the day starting from state (s, t)

= E[f (π(z, t), Z(cid:48), Tpick) + V π(Z(cid:48), Tnext)]

= E

(P (z1, z(cid:48), Tpick)f (z1, z(cid:48), Tpick) + V π(z(cid:48), Tnext))

V π(z, t)

(cid:34)(cid:88)
(cid:34)

z(cid:48)

(cid:88)

z(cid:48)

where Z(cid:48), Tpick and Tnext represent the random zipcode
that the customer at z1 wants to go, the random pickup
time at z1 and the random dropoff time at Z(cid:48) respectively.
f (z1, z(cid:48), Tpick) is the trip fare from z1 to z(cid:48) at time t and
P (z1, z(cid:48), t) is a customers transition probability from z1 to
z(cid:48). The above equations hold after assuming that all the
random variables are independent. To further quantify Tpick
and Tnext, we have

Tpick = t + ∆travel(z, z1) + ∆search(z1)
Tnext = Tpick + ∆trip(z1, z(cid:48))

where ∆travel, ∆search and ∆trip are random time intervals
for the time cost traveling from z to z1 without a passenger,
searching for the next customer at z1, and driving a passen-
ger from z1 to z(cid:48) respectively. Finally, to approximate the
expectation in (1), we replace all the random time with their
expectations. As the value function is almost linear in t and
our estimates of R(z1, t1) and P (z1, z(cid:48), t) will be piecewise
constant (described later) in hour, this approximation should
be accurate enough most of the time. We then deﬁne the
optimal value function as:

V (cid:63)(z, t) = max

π

V π(z, t)

Then, it will satisfy:

V (cid:63)(z, t)

(cid:40)

= max

z1

R(z1, tpick) +

where

(cid:41)

P (z1, z(cid:48), tpick)V (cid:63)(z(cid:48), tnext)

(cid:88)

z(cid:48)

tpick = t + δtravel(z, z1, t) + δsearch(z1, t + δtravel(z, z1, t))
tnext = tpick + δtrip(z1, z(cid:48), tpick)

2) Estimating model parameters from historical data:
To estimate the model parameters, we bin the data by days
in a week and hours in a day. For R(z, t), P (z1, z(cid:48), t)
and δtrip(z1, z(cid:48), t), we simply take from historical data the
average of the trip fare, the transit frequency and trip time
inside the bin that t falls into. For δtravel, we estimate it by:

δtravel(z, z1, t) = δtrip(z, z1, t) − δtrip(z1, z1, t)

as the driver will start searching as soon as he arrives at any
point in z1. Finally, we need to estimate δsearch. For this
we need to consider that drivers may drive away to other
zipcodes as they search for new passengers. Our prediction
of δsearch(z1, t1) is deﬁned as the expected searching time
if the driver keeps searching in z1 until he picks the next
customer. Thus, simpily taking the average time duration
of zdropoﬀ = znext
pickup to estimate δsearch will be a severally
downward biased estimate.

Assume instead that ∆search follows an exponential dis-
tribution exp(λ) which is a common distribution for waiting
time. Then

δsearch(z1, t1) =

1

λ(z1, t1)

The exponential distribution has a nice property. If we can
observe a lower quantile of ∆search: P (∆search < x) = p,
then

− ln(1 − p)

λ =

x

For the historical data, if the actual searching time was short,
it’s likely that the driver would still be in the same zipcode
when he picks up the next customer, so we roughly observe a
lower quantile of ∆search, and we can determine a reasonable
x as either the median or 75%(depending on how large they
pickup from data in the
are) of duration times for zdropoﬀ = znext
corresponding time bin. To estimate p, we assume that drivers
are randomly searching, so if x is not too large, the drivers
would still be in zipcodes close to z1 and the searching time
should be similar. Thus,
#{tnext

pickup − tdropoﬀ < x, zdropoﬀ = z1}

ˆp(z1, t) =

#{zdropoﬀ = z1}

3) Updating model parameters from future data: Updat-
ing our model parameters with online data can be accom-
plished by recalculating the summary statistics based on the
new information. Obtaining feedback on a drivers intention
would be an important new feature. Assuming that drivers
follow our policy to maximize their revenue, we will know
both where they intend to go and where they actually pick
up the next customer. For a policy π(z, t) = z1, instead of
saying that the driver must pick up the next customer at
z1, we can estimate the transitions probabilities from z to
a pickup zipcode z(cid:48) for policy π(z) , and the new value
function will be

V π(z, t)

(cid:40)(cid:88)

(cid:20)(cid:88)

= E

P π=z1(z, z(cid:48)(cid:48), t)

z(cid:48)(cid:48)

z(cid:48)

(cid:21)(cid:41)

(P (z(cid:48)(cid:48), z(cid:48), Tpick)f (z(cid:48)(cid:48), z(cid:48), Tpick) + V π(z(cid:48), Tnext))

Notice that P π is the transition probability for the drivers

while P is the transition probability for the customers.

IV. OUTCOME

A. Understanding the demand

The linear regression has R2 = 0.51. The ANOVA table

is:

PickupZip
day7
hour
tempF
windspeedMiles
weatherDesc
factor(winddir16point)
precipMM
humidity
cloudcover
HeatIndexF
PickupZip:day7
PickupZip:hour
day7:hour
Residuals

Df
94
6
7
1
1
10
15
1
1
1
1
564
658
42
22157

Sum Sq Mean Sq
105.03
9873.05
15.18
91.06
169.85
1188.95
0.48
0.48
0.01
0.01
2.87
28.75
2.91
43.72
4.16
4.16
0.03
0.03
0.01
0.01
1.71
1.71
1.34
757.07
4.07
2679.43
7.00
293.88
14625.13
0.66

TABLE III

F value
159.12
22.99
257.32
0.73
0.01
4.35
4.42
6.30
0.04
0.01
2.59
2.03
6.17
10.60

Pr(>F)
0.0000
0.0000
0.0000
0.3931
0.9274
0.0000
0.0000
0.0121
0.8417
0.9265
0.1075
0.0000
0.0000
0.0000

ANALYSIS OF VARIANCE: PICKUPZIP IS THE ZIPCODE FACTOR, DAY7 IS

THE DAYS IN A WEEK FACTOR AND HOUR IS THE FACTOR FOR THE
THREE-HOUR INTERVALS. OTHER FACTORS ARE INTERACTIONS AND

WEATHER INFORMATION.

Some of the main effects and interaction are shown in Fig.

2.

B. Backtesting the optimal policy

learning model

The result of the reinforcement

is an
optimal policy that details which zipcode mazimizes a drivers
earning potential for the day. If a driver is looking for a
new customer, the inputs of the policy would be his current
zipcode and the time of day. The output would be the zipcode
where he should look for a new passenger to maximize his
revenue. Fig. 3 illustrates what such a policy looks like.

To back test our model we used transaction records to
follow drivers for their shift. We compared their revenue for
the day versus what they would have earned if they had used
our policy. By using dynamic programming, we were able to
back test over 10k drivers and record their performance with
and without following our policy decision. Fig. 4 shows the
performance of 200 representative drivers.

Improving Taxi Revenue with Reinforcement Learning

Jingshu Wang1 and Benjamin Lampert2

Abstract— In recent news there has been controversy sur-
rounding Uber and Lyft taking business away from taxi drivers.
In light of this controversy, we explored how machine learning
could optimize a taxi drivers income by understanding the
ﬂow of demand throughout the day. We ﬁrst used historical
New York City (NYC) transaction data from 2013 to build
summary statistics of the demand, fare, duration, and transition
probabilities for different zipcodes and times across NYC.
We then applied a reinforcement learning model to maximize
the total revenue generated by an individual driver. Utilizing
dynamic programming we backtested our model on 10k drivers
and found that if they followed an optimal policy they would
earn on average ∼20% more per day than following a non-
optimal policy.

I. INTRODUCTION

With the emergence of traditional taxi competitors such
as Lyft and Uber, many New York Taxi cab drivers are con-
cerned with unfair competition. These companies leverage
analytics, mobile apps, and other data sources to adjust rates
based on a wide set of features. Uber, for example, has a team
dedicated to analyzing demand and usage across cities[1].
Providing similar services to normal drivers therefore would
be an important step to allow them to compete. Our ap-
proach to this problem was ﬁrst to analyze and preprocess
a collection of 2013 taxi records. We used a linear model
to understand which features best predict demand, and a
reinforcement
to maximize an individual
drivers income based on a set of summary statistics.

learning model

Fig. 1. Plot of each pickup location overlaid on a map of New York City.
Each dot represents a transaction during a single day in January. Points of
activity include Manhattan (left middle) and the JFK airport (lower right).

To handle the computational difﬁculties in training on
large collections, we subsampled our data. We only examined
transactions from January which resulted in a dataset of 14.5
million trips. For linear regression, we sampled and ﬁt only
0.1% of the data. For reinforcement learning, we trained our
model using the full dataset, but only backtested on sampled
drivers from Wednesday records.

II. DATA

B. Preprocessing

To improve our location features we decided to bin the raw
GPS coordinates of the pickup and drop-off location into a
zipcode. This was accomplished using the python package
Geodis[3]. We also applied a ﬁlter at this point to exclude
all data not within NYC (ie, not reported correctly or did not
successfully reverse geocode).

A. NYC Transactions

The dataset we used is from the 2013 NYC Taxi records
made available by a Freedom of Information act and ﬁrst
published by Chris Whong[2]. The collection contains all
173 million taxi transactions for 2013, where each entry
details the relevant information for each taxi trip. A summary
of these features can be found in Table I.

Label
pickup datetime
dropoff datetime
pickup longitude
pickup latitude
dropoff longitude
dropoff latitude
total amount

Example
2013/01/05 6:49:41 PM
2013/01/05 6:54:23 PM
-74.004707
40.73777
-74.009834
40.726002
$7

TABLE I

EXAMPLE OF SOME OF THE FEATURES CONTAINED IN THE RAW DATA.

1W. Jingshu jingshuw@stanford.edu
2B. Lampert lampertb@stanford.edu

The next goal was to build a feature set that would allow
us to train a policy on the data. We ﬁrst iterated through all
of the records to build a set of summary statistics related
to the fare information. These data are grouped by the key
(hour, pickupzipcode, dropoffzipcode). An example of one
ﬁeld from the summary data is as follows:

The next step was to get an estimate for the searching time
for the next customer. Because our dataset only observes
the time between pickup and drop-offs, we lack direct
information related to how long the driver searched in an
area to pick up a new customer.

To infer this data, we looked at drivers who picked up
new customers in the same zipcode as where they had just
dropped off a customer. This provided a time delta associated

In a traditional reinforcement learning model the intention
of the driver would be an important feature. In practice,
although a driver may intend to drive to a zipcode z, they can
pick up customer along the way. However, it’s impossible to
know a driver’s intention from historical data as we only
know where he ends up picking the next customer. Thus,
to reduce the difﬁculty in estimating the model parameters
from historical data, we consider a simpliﬁed policy:

• Policy: π(z, t) = z1 the next pickup zipcode from state
(z, t). The driver goes to z1 directly and will keep
searching inside z1 until he picks up the next customer
at z1

Then, we can write out the value function as:

(cid:35)

(cid:35)

III. MODEL

= E

R(z1, Tpick) +

P (z1, z(cid:48), Tpick)V π(z(cid:48), Tnext)

Statistic
Hour

PickupZipCode
DropOffZipCode

Number of transistions
Mean Trip Distance
Mean Trip Duration

Mean Fare

Transition Probability

Value
10am
10012
10011
302

1.19 Miles
7.9 Minutes

$8.58
7.36%

TABLE II

EXAMPLE ENTRY IN THE SUMMARY STATISTIC DATA FOR PICKING UP A

PASSENGER AT ZIPCODE 10012, WHO IS TRAVELING TO 10011,

BETWEEN 10-11AM ON WEDNESDAY.

with how long they were searching between customers.
We then modeled the searching time as an exponential
distribution to get the expected waiting time for each zipcode
and hour. More details will be found in Section III.

With these summary statistics we were then able to apply

our model.

A. Linear Regression

To ﬁnd the relationship between taxi demand and various
factors (geographical locations, time and date, weather etc.),
we use a linear regression model to decompose the sources
of variances. For the response, we aggregated the number of
trips every three hours for each zipcode. For predictors, we
consider zipcodes, the hours in a day, and the days in a week
as three major factors along with their two-term interactions.
We also include weather information during the 3 hour time
interval as additional predictors. The model is:

Yijk = µ + αi + βj + γk + (αβ)ij + (αγ)ik + (βγ)jk +

p(cid:88)

asXs + ijk

s=1

where αi are the main effects for zipcodes, βj are the main
effects for Monday, Tuesday, ··· , Sunday, and γk are the 3
hour intervals for a day. {X1, X2,··· , Xp} are the additional
predictors of weather information.
B. Reinforcement Learning

1) Framework: When a taxi driver drops off a customer,
and is looking for new business, there are two actions they
can make. Either they can stay in the area and wait for
passenger there, or travel to a new location. Travelling to a
new location has an associated cost, but if chosen correctly
the new location will impact future trips the revenue for
the day. We ﬁnd that a reinforcement learning model comes
naturally to quantify the above optimization procedure. Using
zipcodes as geographical units, we can deﬁne the states,
reward and value functions as:

• States: (z, t) a dropoff zipcode at a corresponding time
• Reward: R(z1, t1) average trip fair of one trip for a

pickup zipcode z1 at time t1

• Value function: V (z, t) expected total revenue to the

end of the day starting from state (s, t)

= E[f (π(z, t), Z(cid:48), Tpick) + V π(Z(cid:48), Tnext)]

= E

(P (z1, z(cid:48), Tpick)f (z1, z(cid:48), Tpick) + V π(z(cid:48), Tnext))

V π(z, t)

(cid:34)(cid:88)
(cid:34)

z(cid:48)

(cid:88)

z(cid:48)

where Z(cid:48), Tpick and Tnext represent the random zipcode
that the customer at z1 wants to go, the random pickup
time at z1 and the random dropoff time at Z(cid:48) respectively.
f (z1, z(cid:48), Tpick) is the trip fare from z1 to z(cid:48) at time t and
P (z1, z(cid:48), t) is a customers transition probability from z1 to
z(cid:48). The above equations hold after assuming that all the
random variables are independent. To further quantify Tpick
and Tnext, we have

Tpick = t + ∆travel(z, z1) + ∆search(z1)
Tnext = Tpick + ∆trip(z1, z(cid:48))

where ∆travel, ∆search and ∆trip are random time intervals
for the time cost traveling from z to z1 without a passenger,
searching for the next customer at z1, and driving a passen-
ger from z1 to z(cid:48) respectively. Finally, to approximate the
expectation in (1), we replace all the random time with their
expectations. As the value function is almost linear in t and
our estimates of R(z1, t1) and P (z1, z(cid:48), t) will be piecewise
constant (described later) in hour, this approximation should
be accurate enough most of the time. We then deﬁne the
optimal value function as:

V (cid:63)(z, t) = max

π

V π(z, t)

Then, it will satisfy:

V (cid:63)(z, t)

(cid:40)

= max

z1

R(z1, tpick) +

where

(cid:41)

P (z1, z(cid:48), tpick)V (cid:63)(z(cid:48), tnext)

(cid:88)

z(cid:48)

tpick = t + δtravel(z, z1, t) + δsearch(z1, t + δtravel(z, z1, t))
tnext = tpick + δtrip(z1, z(cid:48), tpick)

2) Estimating model parameters from historical data:
To estimate the model parameters, we bin the data by days
in a week and hours in a day. For R(z, t), P (z1, z(cid:48), t)
and δtrip(z1, z(cid:48), t), we simply take from historical data the
average of the trip fare, the transit frequency and trip time
inside the bin that t falls into. For δtravel, we estimate it by:

δtravel(z, z1, t) = δtrip(z, z1, t) − δtrip(z1, z1, t)

as the driver will start searching as soon as he arrives at any
point in z1. Finally, we need to estimate δsearch. For this
we need to consider that drivers may drive away to other
zipcodes as they search for new passengers. Our prediction
of δsearch(z1, t1) is deﬁned as the expected searching time
if the driver keeps searching in z1 until he picks the next
customer. Thus, simpily taking the average time duration
of zdropoﬀ = znext
pickup to estimate δsearch will be a severally
downward biased estimate.

Assume instead that ∆search follows an exponential dis-
tribution exp(λ) which is a common distribution for waiting
time. Then

δsearch(z1, t1) =

1

λ(z1, t1)

The exponential distribution has a nice property. If we can
observe a lower quantile of ∆search: P (∆search < x) = p,
then

− ln(1 − p)

λ =

x

For the historical data, if the actual searching time was short,
it’s likely that the driver would still be in the same zipcode
when he picks up the next customer, so we roughly observe a
lower quantile of ∆search, and we can determine a reasonable
x as either the median or 75%(depending on how large they
pickup from data in the
are) of duration times for zdropoﬀ = znext
corresponding time bin. To estimate p, we assume that drivers
are randomly searching, so if x is not too large, the drivers
would still be in zipcodes close to z1 and the searching time
should be similar. Thus,
#{tnext

pickup − tdropoﬀ < x, zdropoﬀ = z1}

ˆp(z1, t) =

#{zdropoﬀ = z1}

3) Updating model parameters from future data: Updat-
ing our model parameters with online data can be accom-
plished by recalculating the summary statistics based on the
new information. Obtaining feedback on a drivers intention
would be an important new feature. Assuming that drivers
follow our policy to maximize their revenue, we will know
both where they intend to go and where they actually pick
up the next customer. For a policy π(z, t) = z1, instead of
saying that the driver must pick up the next customer at
z1, we can estimate the transitions probabilities from z to
a pickup zipcode z(cid:48) for policy π(z) , and the new value
function will be

V π(z, t)

(cid:40)(cid:88)

(cid:20)(cid:88)

= E

P π=z1(z, z(cid:48)(cid:48), t)

z(cid:48)(cid:48)

z(cid:48)

(cid:21)(cid:41)

(P (z(cid:48)(cid:48), z(cid:48), Tpick)f (z(cid:48)(cid:48), z(cid:48), Tpick) + V π(z(cid:48), Tnext))

Notice that P π is the transition probability for the drivers

while P is the transition probability for the customers.

IV. OUTCOME

A. Understanding the demand

The linear regression has R2 = 0.51. The ANOVA table

is:

PickupZip
day7
hour
tempF
windspeedMiles
weatherDesc
factor(winddir16point)
precipMM
humidity
cloudcover
HeatIndexF
PickupZip:day7
PickupZip:hour
day7:hour
Residuals

Df
94
6
7
1
1
10
15
1
1
1
1
564
658
42
22157

Sum Sq Mean Sq
105.03
9873.05
15.18
91.06
169.85
1188.95
0.48
0.48
0.01
0.01
2.87
28.75
2.91
43.72
4.16
4.16
0.03
0.03
0.01
0.01
1.71
1.71
1.34
757.07
4.07
2679.43
7.00
293.88
14625.13
0.66

TABLE III

F value
159.12
22.99
257.32
0.73
0.01
4.35
4.42
6.30
0.04
0.01
2.59
2.03
6.17
10.60

Pr(>F)
0.0000
0.0000
0.0000
0.3931
0.9274
0.0000
0.0000
0.0121
0.8417
0.9265
0.1075
0.0000
0.0000
0.0000

ANALYSIS OF VARIANCE: PICKUPZIP IS THE ZIPCODE FACTOR, DAY7 IS

THE DAYS IN A WEEK FACTOR AND HOUR IS THE FACTOR FOR THE
THREE-HOUR INTERVALS. OTHER FACTORS ARE INTERACTIONS AND

WEATHER INFORMATION.

Some of the main effects and interaction are shown in Fig.

2.

B. Backtesting the optimal policy

learning model

The result of the reinforcement

is an
optimal policy that details which zipcode mazimizes a drivers
earning potential for the day. If a driver is looking for a
new customer, the inputs of the policy would be his current
zipcode and the time of day. The output would be the zipcode
where he should look for a new passenger to maximize his
revenue. Fig. 3 illustrates what such a policy looks like.

To back test our model we used transaction records to
follow drivers for their shift. We compared their revenue for
the day versus what they would have earned if they had used
our policy. By using dynamic programming, we were able to
back test over 10k drivers and record their performance with
and without following our policy decision. Fig. 4 shows the
performance of 200 representative drivers.

Fig. 4.
Following the optimal policy from our model produced higher
revenue than if a driver had used his own strategy. This result holds for a
variety of work durations.

V. CONCLUSIONS

Applying a reinforcement learning model to the NYC taxi
data we were able to show that an optimal policy can be
obtained that maximizes the revenue generated by a single
driver. Given a starting location and a time of the day our
model can direct drivers to the optimal zipcode to ﬁnd a
customer. Backtesting results show that if a drivers were to
follow our policy, they would earn ∼20% more revenue.

VI. FUTURE

There are ﬁnite number of passengers, and if all taxi
drivers used this policy then they would not all see the
same gains. Optimizing the entire system, and coordinating
between all drivers would be a better approach and could
ultimately beneﬁt all parties. Taxi companies could allocate
their medallions more efﬁciently, drivers would minimize
their vacancy time and working hours, and passengers could
see better Taxi coverage depending on peak hours/locations.
Another important piece of our model that is missing
from traditional reinforcement
learning algorithms is the
feedback from making a decision. Tracking the intention of
the driver to drive to a location, compared to where they
end up, is an important piece of information. If this were
to become a mobile app, then it might be possible to obtain
this information and improve our model.

REFERENCES

[1] ”http://blog.uber.com/uberdata/” #uberdata Uber Blog. Uber, n.d. Web.

12 Dec. 2014.

[2] Whong, Chris. ”FOILing NYCs Taxi Trip Data.” FOILing NYCs Taxi

Trip Data. N.p., 18 Mar. 2014. Web. 11 Dec. 2014.

[3] ”https://github.com/doat/geodis” Geodis. N.p., n.d. Web. 11 Dec. 2014.

Fig. 2.
Individual Effects:(a)-(c) are the main effects of variables weath-
erDesc, day7 and hour in Table III. For (b) and (c), the red dashed lines
show the signiﬁcance thresholds based on adjusted p-values. In (d) the color
represents the estimate of each interaction effect. A smile face represents a
signiﬁcant positive effect and a sad face is a signiﬁcant negative effect. All
pvalues have been adjusted using Bonferroni to accommodate for multiple
testing.

Fig. 3. Example policy from our model based on starting the day at 7am
and ending the day at 5pm. Each node represents a starting zipcode, and
the edges show the expected revenue for the day if the policy is taken.

lllllllllll−0.10.00.1Clear Cloudy FogIce pelletsLight drizzleMistModerate rainOvercast Partly Cloudy Patchy light snowSunny(a)Weatherlllllll−0.050.000.05MondayTuesdayWednesdayThursdayFridaySaturdaySunday(b)Dayllllllll−0.4−0.20.00.2[2am, 5am][5am, 8am][8am, 11am][11am, 2pm][2pm, 5pm][5pm, 8pm][8pm, 11pm][11pm, 2am](c)Hour  *−**−**−* ^_^ ^_^   *−*   ^_^  *−**−*                           *−**−**−**−* ^_^^_^ MondayTuesdayWednesdayThursdayFridaySaturdaySunday[2am, 5am][5am, 8am][8am, 11am][11am, 2pm][2pm, 5pm][5pm, 8pm][8pm, 11pm][11pm, 2am](d)−0.20.00.2EffectDay−Hour Interaction