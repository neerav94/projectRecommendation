Recognition and Classiﬁcation of Fast Food Images

shaoyu88@stanford.edu , sinalin@stanford.edu , beibeiw@stanford.edu

Shaoyu Lu, Sina Lin, Beibei Wang

Abstract

We aim to utilize learnt machine learning algo-
rithms to do fast food recognition. Our goal is to ﬁnd a
computational efﬁcient algorithm with high accuracy.
We use Bag of SIFT, color histogram and a combina-
tion of them as the features. k-NN and SVM method
(with and without kernel) are used to classify fast food
images to eight classes.

1. Introduction

Food recognition is of great importance nowadays
for multiple purposes. On one hand, for people who
want to get a better understanding of the food that they
are not familiar of or they haven’t even seen before,
they can simply took a picture and get to know more
details about it. On the other hand, the increasing de-
mand of dietary assessment tools to record the calorie
and nutrition has also been a driving force of the de-
velopment of food recognition technique. Therefore,
automatic food recognition is very important and has
great application potential.

However, food varies greatly in appearance (e.g.,
shape, colors) with tons of different ingredients and
assembling methods. This makes food recognition a
difﬁcult task for current state-of-the-art classiﬁcation
methods, and hence an important challenge for Com-
puter Vision researchers. Yoshiyuki Kawano and Keiji
Yanai [Kawano and Yanai, 2013] proposed a real-time
food recognition system which adopted a liner SVM
with a fast χ2 kernel, bounding box adjustment and
estimation of the expected direction of a food region.
Lukas Bossard et al. [Bossard et al., 2014] presented
a novel method based on Random Forests to mine dis-
criminative visual components and efﬁcient classiﬁca-
tion.

Figure 1. Examples from the food image dataset. Left:
“Original” dataset. Middle:
“ColorCorrected” dataset.
Right: “ColorCorrected+Segment” dataset.

In this effort, we intent to utilize learnt machine
learning algorithms to do fast food recognition and
classiﬁcation. Our goal is to develop a computational
efﬁcient algorithm with high accuracy. Different fea-
tures and models have been implemented and com-
pared.

2. Dataset

The dataset we used in this study was based on
the Pittsburgh Fast-food Image Dataset (PFID) im-
ages. This dataset was proposed by Chen et al.
[Chen et al., 2009] to properly evaluate the perfor-
mances of food recognition. This dataset is composed
by 1359 food images with RGB-color of fast-food
dishes mainly acquired in laboratory.

3. Pre-processing

Because the original dataset contains different light-
ing for the same food, we used white balance to
do color correction in order to minimize the within
class variance. Besides, the background is not re-
lated to the food, thus background segmentation was
used to enable only food features being extracted.
Figure.1 shows one set of example from the three
datasets (original, color corrected, color corrected plus
background segmentation). We labeled the food im-
ages into eight categories: Bread Sandwich (breakfast

Recognition and Classiﬁcation of Fast Food Images

shaoyu88@stanford.edu , sinalin@stanford.edu , beibeiw@stanford.edu

Shaoyu Lu, Sina Lin, Beibei Wang

Abstract

We aim to utilize learnt machine learning algo-
rithms to do fast food recognition. Our goal is to ﬁnd a
computational efﬁcient algorithm with high accuracy.
We use Bag of SIFT, color histogram and a combina-
tion of them as the features. k-NN and SVM method
(with and without kernel) are used to classify fast food
images to eight classes.

1. Introduction

Food recognition is of great importance nowadays
for multiple purposes. On one hand, for people who
want to get a better understanding of the food that they
are not familiar of or they haven’t even seen before,
they can simply took a picture and get to know more
details about it. On the other hand, the increasing de-
mand of dietary assessment tools to record the calorie
and nutrition has also been a driving force of the de-
velopment of food recognition technique. Therefore,
automatic food recognition is very important and has
great application potential.

However, food varies greatly in appearance (e.g.,
shape, colors) with tons of different ingredients and
assembling methods. This makes food recognition a
difﬁcult task for current state-of-the-art classiﬁcation
methods, and hence an important challenge for Com-
puter Vision researchers. Yoshiyuki Kawano and Keiji
Yanai [Kawano and Yanai, 2013] proposed a real-time
food recognition system which adopted a liner SVM
with a fast χ2 kernel, bounding box adjustment and
estimation of the expected direction of a food region.
Lukas Bossard et al. [Bossard et al., 2014] presented
a novel method based on Random Forests to mine dis-
criminative visual components and efﬁcient classiﬁca-
tion.

Figure 1. Examples from the food image dataset. Left:
“Original” dataset. Middle:
“ColorCorrected” dataset.
Right: “ColorCorrected+Segment” dataset.

In this effort, we intent to utilize learnt machine
learning algorithms to do fast food recognition and
classiﬁcation. Our goal is to develop a computational
efﬁcient algorithm with high accuracy. Different fea-
tures and models have been implemented and com-
pared.

2. Dataset

The dataset we used in this study was based on
the Pittsburgh Fast-food Image Dataset (PFID) im-
ages. This dataset was proposed by Chen et al.
[Chen et al., 2009] to properly evaluate the perfor-
mances of food recognition. This dataset is composed
by 1359 food images with RGB-color of fast-food
dishes mainly acquired in laboratory.

3. Pre-processing

Because the original dataset contains different light-
ing for the same food, we used white balance to
do color correction in order to minimize the within
class variance. Besides, the background is not re-
lated to the food, thus background segmentation was
used to enable only food features being extracted.
Figure.1 shows one set of example from the three
datasets (original, color corrected, color corrected plus
background segmentation). We labeled the food im-
ages into eight categories: Bread Sandwich (breakfast

sandwich), Burger, Chicken, Donut, Pancake, Pizza,
Salad and Sandwich. Classiﬁcation results compari-
son among these three datasets will be discussed later.

4. Features and Models
4.1. Features

In this study, two popular features in terms of im-
age processing including Bag of SIFT (Scale-Invariant
Feature Transform) [Lowe, 1999] and color histogram
were chosen to capture the image content in our fast
food images.

We chose SIFT to extract food image textures and
used bag of features since it’s invariant to spatial trans-
lation and rotation, and it can provide ﬁxed length fea-
ture vectors.We chose color histogram to extract color
distributions. To ensure both efﬁciency and accuracy,
we choose 16 bins per RGB color. Later on, we ex-
plored combining these two features together and ob-
tained a better result.

4.2. Models
k-Nearest

algorithm

Neighbors

(k-NN)
[Cover and Hart, 1967] and Support Vector Ma-
chine (SVM) [Suykens and Vandewalle, 1999] were
used as learning methods in our study. They are two
popular discriminative classiﬁers with no distribution
requirement.
k-NN is simple to implement with
usually good result. And the accuracy of this method
can be highly inﬂuenced by parameter k (Figure
SVM was chosen since it’s very robust and
3).
well developed for classiﬁcation.
[Kim et al., 2012]
It was originally designed for two-class problems.
Therefore, we utilized “one-versus-the-rest” method
in our multi-class problem.

Moreover, χ2 Kernel was utilized for both meth-
ods as some study shows that it’s good at image tex-
ture recognition. [Zhang et al., 2007] χ2 Kernel is of-
ten used with bag of feature since it is a more natu-
ral distance measure between histograms than the eu-
clidean distance.
[Yang et al., 2009] The χ2 Kernel
comes from the χ2 distribution:

(1)

k(cid:88)

(cid:18) Xi − µi

(cid:19)2

i=1

σi

where Xi ∼ N (µi, σ2

i ), i = 1, . . . , k.

Figure 2. Effects of dimensionality on classiﬁcation. The
data was “ColorCorrected”. Modeling used bag of SIFT
and SVM with χ2 kernel.

The bag of SIFT and χ2 Kernel were employed us-
ing code from VLFeat [Vedaldi and Fulkerson, 2012].
SVM code is from LIBLINEAR [Fan et al., 2008] and
k-NN is from Matlab Machine Leaning toolbox. We
developed our own code to compute the color his-
togram without counting the background color.

5. Performance Analysis

5.1. Effects of data dimensionality

Data dimensionality can affect the classiﬁcation ac-
curacy a lot. Thus, we need to ﬁnd an optimal dimen-
sion. From Figure.2, at the beginning, the accuracy
increases very fast as the dimension increases. How-
ever, when the dimension reaches 104, the accuracy
increases very slowly, and even decreases a little when
dimension reaches 105. This is because, when the di-
mension is too high, the volume of the data space con-
centrates on the surface, therefore available feature di-
mension won’t increase. Thus increasing dimensional-
ity won’t help classiﬁcation at this point. In this study,
the optimal dimension is 113375, which corresponds
to shrink image to 30% along both X- and Y-axis.

5.2. Effects of “K” in k-NN

From Figure.3, increasing number of neighbors im-
proves the accuracy because it helps reduce noise ef-
fects on k-NN classiﬁcation. But including too much
neighbors would ruin the classiﬁcation, as neighbors
from incorrect class would be involved. From the clas-
siﬁcation accuracy on test data, we choose the number
of nearest neighbors to be 4.

Pixel number of images103104105106107Classification accuracy (%)102030405060708090100Effect of dimensionality of data on classification accuracyShrink imageto ratio of 0.3Recognition and Classiﬁcation of Fast Food Images

shaoyu88@stanford.edu , sinalin@stanford.edu , beibeiw@stanford.edu

Shaoyu Lu, Sina Lin, Beibei Wang

Abstract

We aim to utilize learnt machine learning algo-
rithms to do fast food recognition. Our goal is to ﬁnd a
computational efﬁcient algorithm with high accuracy.
We use Bag of SIFT, color histogram and a combina-
tion of them as the features. k-NN and SVM method
(with and without kernel) are used to classify fast food
images to eight classes.

1. Introduction

Food recognition is of great importance nowadays
for multiple purposes. On one hand, for people who
want to get a better understanding of the food that they
are not familiar of or they haven’t even seen before,
they can simply took a picture and get to know more
details about it. On the other hand, the increasing de-
mand of dietary assessment tools to record the calorie
and nutrition has also been a driving force of the de-
velopment of food recognition technique. Therefore,
automatic food recognition is very important and has
great application potential.

However, food varies greatly in appearance (e.g.,
shape, colors) with tons of different ingredients and
assembling methods. This makes food recognition a
difﬁcult task for current state-of-the-art classiﬁcation
methods, and hence an important challenge for Com-
puter Vision researchers. Yoshiyuki Kawano and Keiji
Yanai [Kawano and Yanai, 2013] proposed a real-time
food recognition system which adopted a liner SVM
with a fast χ2 kernel, bounding box adjustment and
estimation of the expected direction of a food region.
Lukas Bossard et al. [Bossard et al., 2014] presented
a novel method based on Random Forests to mine dis-
criminative visual components and efﬁcient classiﬁca-
tion.

Figure 1. Examples from the food image dataset. Left:
“Original” dataset. Middle:
“ColorCorrected” dataset.
Right: “ColorCorrected+Segment” dataset.

In this effort, we intent to utilize learnt machine
learning algorithms to do fast food recognition and
classiﬁcation. Our goal is to develop a computational
efﬁcient algorithm with high accuracy. Different fea-
tures and models have been implemented and com-
pared.

2. Dataset

The dataset we used in this study was based on
the Pittsburgh Fast-food Image Dataset (PFID) im-
ages. This dataset was proposed by Chen et al.
[Chen et al., 2009] to properly evaluate the perfor-
mances of food recognition. This dataset is composed
by 1359 food images with RGB-color of fast-food
dishes mainly acquired in laboratory.

3. Pre-processing

Because the original dataset contains different light-
ing for the same food, we used white balance to
do color correction in order to minimize the within
class variance. Besides, the background is not re-
lated to the food, thus background segmentation was
used to enable only food features being extracted.
Figure.1 shows one set of example from the three
datasets (original, color corrected, color corrected plus
background segmentation). We labeled the food im-
ages into eight categories: Bread Sandwich (breakfast

sandwich), Burger, Chicken, Donut, Pancake, Pizza,
Salad and Sandwich. Classiﬁcation results compari-
son among these three datasets will be discussed later.

4. Features and Models
4.1. Features

In this study, two popular features in terms of im-
age processing including Bag of SIFT (Scale-Invariant
Feature Transform) [Lowe, 1999] and color histogram
were chosen to capture the image content in our fast
food images.

We chose SIFT to extract food image textures and
used bag of features since it’s invariant to spatial trans-
lation and rotation, and it can provide ﬁxed length fea-
ture vectors.We chose color histogram to extract color
distributions. To ensure both efﬁciency and accuracy,
we choose 16 bins per RGB color. Later on, we ex-
plored combining these two features together and ob-
tained a better result.

4.2. Models
k-Nearest

algorithm

Neighbors

(k-NN)
[Cover and Hart, 1967] and Support Vector Ma-
chine (SVM) [Suykens and Vandewalle, 1999] were
used as learning methods in our study. They are two
popular discriminative classiﬁers with no distribution
requirement.
k-NN is simple to implement with
usually good result. And the accuracy of this method
can be highly inﬂuenced by parameter k (Figure
SVM was chosen since it’s very robust and
3).
well developed for classiﬁcation.
[Kim et al., 2012]
It was originally designed for two-class problems.
Therefore, we utilized “one-versus-the-rest” method
in our multi-class problem.

Moreover, χ2 Kernel was utilized for both meth-
ods as some study shows that it’s good at image tex-
ture recognition. [Zhang et al., 2007] χ2 Kernel is of-
ten used with bag of feature since it is a more natu-
ral distance measure between histograms than the eu-
clidean distance.
[Yang et al., 2009] The χ2 Kernel
comes from the χ2 distribution:

(1)

k(cid:88)

(cid:18) Xi − µi

(cid:19)2

i=1

σi

where Xi ∼ N (µi, σ2

i ), i = 1, . . . , k.

Figure 2. Effects of dimensionality on classiﬁcation. The
data was “ColorCorrected”. Modeling used bag of SIFT
and SVM with χ2 kernel.

The bag of SIFT and χ2 Kernel were employed us-
ing code from VLFeat [Vedaldi and Fulkerson, 2012].
SVM code is from LIBLINEAR [Fan et al., 2008] and
k-NN is from Matlab Machine Leaning toolbox. We
developed our own code to compute the color his-
togram without counting the background color.

5. Performance Analysis

5.1. Effects of data dimensionality

Data dimensionality can affect the classiﬁcation ac-
curacy a lot. Thus, we need to ﬁnd an optimal dimen-
sion. From Figure.2, at the beginning, the accuracy
increases very fast as the dimension increases. How-
ever, when the dimension reaches 104, the accuracy
increases very slowly, and even decreases a little when
dimension reaches 105. This is because, when the di-
mension is too high, the volume of the data space con-
centrates on the surface, therefore available feature di-
mension won’t increase. Thus increasing dimensional-
ity won’t help classiﬁcation at this point. In this study,
the optimal dimension is 113375, which corresponds
to shrink image to 30% along both X- and Y-axis.

5.2. Effects of “K” in k-NN

From Figure.3, increasing number of neighbors im-
proves the accuracy because it helps reduce noise ef-
fects on k-NN classiﬁcation. But including too much
neighbors would ruin the classiﬁcation, as neighbors
from incorrect class would be involved. From the clas-
siﬁcation accuracy on test data, we choose the number
of nearest neighbors to be 4.

Pixel number of images103104105106107Classification accuracy (%)102030405060708090100Effect of dimensionality of data on classification accuracyShrink imageto ratio of 0.3Figure 3. Effects of number of nearest neighbors on k-NN.
The data here is the “ColorCorrected”, χ2 kernel is used.

Figure 4. Effects of data size on classiﬁcation. The data here
is the “ColorCorrected”, using bag of SIFT and SVM with
χ2 kernel.

5.3. Effects of training Data Size

In this test, we kept the ratio of training data size to
test data size to be 2 to 1 and increased the training data
size to see the effects. From Figure.4, the more train-
ing data we have, the better classiﬁcation accuracy we
can obtain. Notice that the accuracy increases slowly
when the training size reaches 30. Since the time com-
plexity would increase linearly with data matrix size,
to balance the accuracy with speed, we choose to ran-
domly pick 50 images from each class as training data
and 25 as test data.

6. Results and Discussion
6.1. The importance of kernel

Figure 5. Kernel effects on SVM and k-NN classiﬁcation
accuracy

6.2. Comparison of features and models

Confusion matrix was used to capture the classiﬁ-
cation accuracy on the test dataset. Figure.6 shows ex-
amples of confusion matrix of classiﬁcation results on
”ColorCorrected” dataset with Color Histogram (left)
and bag of SIFT (right), using k-NN (top) and SVM
(bottom). χ2 kernel was implemented in all cases.
Comparing top two with bottom two ﬁgures, we can
see SVM achieves much better results than k-NN.

Moreover, from the top two ﬁgures, color histogram
performs better on images with more distinct color fea-
tures such as salad (The accuracy by color histogram
is 100% on salad while the accuracy by Bag of SIFT is
84%). On the other hand, Bag of SIFT performs better
for images with distinct textures, such as donut (The
accuracy by color histogram is 72% on donut while
the accuracy by Bag of SIFT is 96%). The bottom two
matrixes indicate the same conclusion.

In order to take advantages of both color and texture
characters, we linearly combined Color histogram and
bag of SIFT to a new feature (named “ColorSIFT”):

ColorSIFT = α·ColorHist+(1−α)·BagOfSIFT (2)

As shown in Figure.5, χ2 kernel helps SVM gain
much better accuracy. This is because SVM is a lin-
ear classiﬁer who suffers from under-ﬁtting. Kernel
enables SVM to work on high-dimensional non-linear
problem, which greatly beneﬁts SVM. Thus we chose
to utilize kernel in following experiments.

where α is the weight parameter for color histogram
and 1 − α is the weight parameter for bag of SIFT.
From experiments, we ﬁnd the optimal of α is 0.4. As
shown in Figure.7, the accuracy increased to 97.5%
compared with the formal best (94%).

K (number of nearest neighbors)12345678910Classification accuracy (%)60626466687072Effects of K on k-NN accuracyWe choose K=4number of training data in each class(number of test data is the half)102030405060classification accuracy (%)707580859095Accuracy vs. size of training data0%	  10%	  20%	  30%	  40%	  50%	  60%	  70%	  80%	  90%	  100%	  Color	  Histogram	  +	  SVM	  Bag	  of	  SIFT	  +	  SVM	  Color	  Histogram	  +	  k-­‐NN	  Bag	  of	  SIFT	  +	  k-­‐NN	  Accuracy	  With	  Kernel	  Without	  Kernel	  Recognition and Classiﬁcation of Fast Food Images

shaoyu88@stanford.edu , sinalin@stanford.edu , beibeiw@stanford.edu

Shaoyu Lu, Sina Lin, Beibei Wang

Abstract

We aim to utilize learnt machine learning algo-
rithms to do fast food recognition. Our goal is to ﬁnd a
computational efﬁcient algorithm with high accuracy.
We use Bag of SIFT, color histogram and a combina-
tion of them as the features. k-NN and SVM method
(with and without kernel) are used to classify fast food
images to eight classes.

1. Introduction

Food recognition is of great importance nowadays
for multiple purposes. On one hand, for people who
want to get a better understanding of the food that they
are not familiar of or they haven’t even seen before,
they can simply took a picture and get to know more
details about it. On the other hand, the increasing de-
mand of dietary assessment tools to record the calorie
and nutrition has also been a driving force of the de-
velopment of food recognition technique. Therefore,
automatic food recognition is very important and has
great application potential.

However, food varies greatly in appearance (e.g.,
shape, colors) with tons of different ingredients and
assembling methods. This makes food recognition a
difﬁcult task for current state-of-the-art classiﬁcation
methods, and hence an important challenge for Com-
puter Vision researchers. Yoshiyuki Kawano and Keiji
Yanai [Kawano and Yanai, 2013] proposed a real-time
food recognition system which adopted a liner SVM
with a fast χ2 kernel, bounding box adjustment and
estimation of the expected direction of a food region.
Lukas Bossard et al. [Bossard et al., 2014] presented
a novel method based on Random Forests to mine dis-
criminative visual components and efﬁcient classiﬁca-
tion.

Figure 1. Examples from the food image dataset. Left:
“Original” dataset. Middle:
“ColorCorrected” dataset.
Right: “ColorCorrected+Segment” dataset.

In this effort, we intent to utilize learnt machine
learning algorithms to do fast food recognition and
classiﬁcation. Our goal is to develop a computational
efﬁcient algorithm with high accuracy. Different fea-
tures and models have been implemented and com-
pared.

2. Dataset

The dataset we used in this study was based on
the Pittsburgh Fast-food Image Dataset (PFID) im-
ages. This dataset was proposed by Chen et al.
[Chen et al., 2009] to properly evaluate the perfor-
mances of food recognition. This dataset is composed
by 1359 food images with RGB-color of fast-food
dishes mainly acquired in laboratory.

3. Pre-processing

Because the original dataset contains different light-
ing for the same food, we used white balance to
do color correction in order to minimize the within
class variance. Besides, the background is not re-
lated to the food, thus background segmentation was
used to enable only food features being extracted.
Figure.1 shows one set of example from the three
datasets (original, color corrected, color corrected plus
background segmentation). We labeled the food im-
ages into eight categories: Bread Sandwich (breakfast

sandwich), Burger, Chicken, Donut, Pancake, Pizza,
Salad and Sandwich. Classiﬁcation results compari-
son among these three datasets will be discussed later.

4. Features and Models
4.1. Features

In this study, two popular features in terms of im-
age processing including Bag of SIFT (Scale-Invariant
Feature Transform) [Lowe, 1999] and color histogram
were chosen to capture the image content in our fast
food images.

We chose SIFT to extract food image textures and
used bag of features since it’s invariant to spatial trans-
lation and rotation, and it can provide ﬁxed length fea-
ture vectors.We chose color histogram to extract color
distributions. To ensure both efﬁciency and accuracy,
we choose 16 bins per RGB color. Later on, we ex-
plored combining these two features together and ob-
tained a better result.

4.2. Models
k-Nearest

algorithm

Neighbors

(k-NN)
[Cover and Hart, 1967] and Support Vector Ma-
chine (SVM) [Suykens and Vandewalle, 1999] were
used as learning methods in our study. They are two
popular discriminative classiﬁers with no distribution
requirement.
k-NN is simple to implement with
usually good result. And the accuracy of this method
can be highly inﬂuenced by parameter k (Figure
SVM was chosen since it’s very robust and
3).
well developed for classiﬁcation.
[Kim et al., 2012]
It was originally designed for two-class problems.
Therefore, we utilized “one-versus-the-rest” method
in our multi-class problem.

Moreover, χ2 Kernel was utilized for both meth-
ods as some study shows that it’s good at image tex-
ture recognition. [Zhang et al., 2007] χ2 Kernel is of-
ten used with bag of feature since it is a more natu-
ral distance measure between histograms than the eu-
clidean distance.
[Yang et al., 2009] The χ2 Kernel
comes from the χ2 distribution:

(1)

k(cid:88)

(cid:18) Xi − µi

(cid:19)2

i=1

σi

where Xi ∼ N (µi, σ2

i ), i = 1, . . . , k.

Figure 2. Effects of dimensionality on classiﬁcation. The
data was “ColorCorrected”. Modeling used bag of SIFT
and SVM with χ2 kernel.

The bag of SIFT and χ2 Kernel were employed us-
ing code from VLFeat [Vedaldi and Fulkerson, 2012].
SVM code is from LIBLINEAR [Fan et al., 2008] and
k-NN is from Matlab Machine Leaning toolbox. We
developed our own code to compute the color his-
togram without counting the background color.

5. Performance Analysis

5.1. Effects of data dimensionality

Data dimensionality can affect the classiﬁcation ac-
curacy a lot. Thus, we need to ﬁnd an optimal dimen-
sion. From Figure.2, at the beginning, the accuracy
increases very fast as the dimension increases. How-
ever, when the dimension reaches 104, the accuracy
increases very slowly, and even decreases a little when
dimension reaches 105. This is because, when the di-
mension is too high, the volume of the data space con-
centrates on the surface, therefore available feature di-
mension won’t increase. Thus increasing dimensional-
ity won’t help classiﬁcation at this point. In this study,
the optimal dimension is 113375, which corresponds
to shrink image to 30% along both X- and Y-axis.

5.2. Effects of “K” in k-NN

From Figure.3, increasing number of neighbors im-
proves the accuracy because it helps reduce noise ef-
fects on k-NN classiﬁcation. But including too much
neighbors would ruin the classiﬁcation, as neighbors
from incorrect class would be involved. From the clas-
siﬁcation accuracy on test data, we choose the number
of nearest neighbors to be 4.

Pixel number of images103104105106107Classification accuracy (%)102030405060708090100Effect of dimensionality of data on classification accuracyShrink imageto ratio of 0.3Figure 3. Effects of number of nearest neighbors on k-NN.
The data here is the “ColorCorrected”, χ2 kernel is used.

Figure 4. Effects of data size on classiﬁcation. The data here
is the “ColorCorrected”, using bag of SIFT and SVM with
χ2 kernel.

5.3. Effects of training Data Size

In this test, we kept the ratio of training data size to
test data size to be 2 to 1 and increased the training data
size to see the effects. From Figure.4, the more train-
ing data we have, the better classiﬁcation accuracy we
can obtain. Notice that the accuracy increases slowly
when the training size reaches 30. Since the time com-
plexity would increase linearly with data matrix size,
to balance the accuracy with speed, we choose to ran-
domly pick 50 images from each class as training data
and 25 as test data.

6. Results and Discussion
6.1. The importance of kernel

Figure 5. Kernel effects on SVM and k-NN classiﬁcation
accuracy

6.2. Comparison of features and models

Confusion matrix was used to capture the classiﬁ-
cation accuracy on the test dataset. Figure.6 shows ex-
amples of confusion matrix of classiﬁcation results on
”ColorCorrected” dataset with Color Histogram (left)
and bag of SIFT (right), using k-NN (top) and SVM
(bottom). χ2 kernel was implemented in all cases.
Comparing top two with bottom two ﬁgures, we can
see SVM achieves much better results than k-NN.

Moreover, from the top two ﬁgures, color histogram
performs better on images with more distinct color fea-
tures such as salad (The accuracy by color histogram
is 100% on salad while the accuracy by Bag of SIFT is
84%). On the other hand, Bag of SIFT performs better
for images with distinct textures, such as donut (The
accuracy by color histogram is 72% on donut while
the accuracy by Bag of SIFT is 96%). The bottom two
matrixes indicate the same conclusion.

In order to take advantages of both color and texture
characters, we linearly combined Color histogram and
bag of SIFT to a new feature (named “ColorSIFT”):

ColorSIFT = α·ColorHist+(1−α)·BagOfSIFT (2)

As shown in Figure.5, χ2 kernel helps SVM gain
much better accuracy. This is because SVM is a lin-
ear classiﬁer who suffers from under-ﬁtting. Kernel
enables SVM to work on high-dimensional non-linear
problem, which greatly beneﬁts SVM. Thus we chose
to utilize kernel in following experiments.

where α is the weight parameter for color histogram
and 1 − α is the weight parameter for bag of SIFT.
From experiments, we ﬁnd the optimal of α is 0.4. As
shown in Figure.7, the accuracy increased to 97.5%
compared with the formal best (94%).

K (number of nearest neighbors)12345678910Classification accuracy (%)60626466687072Effects of K on k-NN accuracyWe choose K=4number of training data in each class(number of test data is the half)102030405060classification accuracy (%)707580859095Accuracy vs. size of training data0%	  10%	  20%	  30%	  40%	  50%	  60%	  70%	  80%	  90%	  100%	  Color	  Histogram	  +	  SVM	  Bag	  of	  SIFT	  +	  SVM	  Color	  Histogram	  +	  k-­‐NN	  Bag	  of	  SIFT	  +	  k-­‐NN	  Accuracy	  With	  Kernel	  Without	  Kernel	  Figure 6. Confusion Matrix on “ColorCorrected” dataset with Color Histogram (left) and bag of SIFT (right), using models
of k-NN (top) and SVM (bottom). χ2 kernel was utilized in all cases.

Model
SVM

Feature

Original Dataset Segmented Dataset Color Corrected Dataset

Bag of SIFT

(with kernel)

Color Histogram

SVM

Bag of SIFT

(without kernel) Color Histogram

k-NN

Bag of SIFT

(with kernel)

Color Histogram

k-NN

Bag of SIFT

(without kernel) Color Histogram

93.00%
85.93%
55.84%
68.43%
66.82%
78.93%
68.93%
73.50%

92.00%
78.73%
58.70%
65.50%
62.82%
68.86%
60.43%
65.00%

94.00%
84.93%
70.05%
73.00%
71.39%
74.23%
71.43%
69.50%

Table 1. Classiﬁcation accuracy comparison among the Original, “ColorCorrected” and “ColorCorrected+Segment” dataset.

6.3. Comparison between Different Dataset

7. Conclusion

From Table.1, we can see three datasets achieve al-

most the same accuracy via different algorithms.

To recognize different kinds of food with various
appearance, we extracted color and texture features

	  0.590.521.000.960.720.840.840.24Bag of SIFT and KNN Confusion matrix (71.39 % accuracy with kernel)Predicted Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwichTrue Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwich0.820.721.000.720.520.961.000.20Color Histogram and KNN Confusion matrix (74.23 % accuracy with kernel)Predicted Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwichTrue Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwich1.001.001.001.000.921.000.880.72Bag of SIFT and SVM Confusion matrix (94.00 % accuracy with kernel)Predicted Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwichTrue Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwich0.950.681.000.960.441.001.000.76Color Histogram and SVM Confusion matrix (84.93 % accuracy with kernel)Predicted Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwichTrue Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwich!!!!!!!!!!!!!!Color!Histogram!!!!!!!!!!!vs.!!!!!!!!!!!!Bag!of!SIFT!SVM!!!!!!!!!!!!!!!!!!!!!!vs.!!!!!!!!!!!!!!!!!!!k8NN!Recognition and Classiﬁcation of Fast Food Images

shaoyu88@stanford.edu , sinalin@stanford.edu , beibeiw@stanford.edu

Shaoyu Lu, Sina Lin, Beibei Wang

Abstract

We aim to utilize learnt machine learning algo-
rithms to do fast food recognition. Our goal is to ﬁnd a
computational efﬁcient algorithm with high accuracy.
We use Bag of SIFT, color histogram and a combina-
tion of them as the features. k-NN and SVM method
(with and without kernel) are used to classify fast food
images to eight classes.

1. Introduction

Food recognition is of great importance nowadays
for multiple purposes. On one hand, for people who
want to get a better understanding of the food that they
are not familiar of or they haven’t even seen before,
they can simply took a picture and get to know more
details about it. On the other hand, the increasing de-
mand of dietary assessment tools to record the calorie
and nutrition has also been a driving force of the de-
velopment of food recognition technique. Therefore,
automatic food recognition is very important and has
great application potential.

However, food varies greatly in appearance (e.g.,
shape, colors) with tons of different ingredients and
assembling methods. This makes food recognition a
difﬁcult task for current state-of-the-art classiﬁcation
methods, and hence an important challenge for Com-
puter Vision researchers. Yoshiyuki Kawano and Keiji
Yanai [Kawano and Yanai, 2013] proposed a real-time
food recognition system which adopted a liner SVM
with a fast χ2 kernel, bounding box adjustment and
estimation of the expected direction of a food region.
Lukas Bossard et al. [Bossard et al., 2014] presented
a novel method based on Random Forests to mine dis-
criminative visual components and efﬁcient classiﬁca-
tion.

Figure 1. Examples from the food image dataset. Left:
“Original” dataset. Middle:
“ColorCorrected” dataset.
Right: “ColorCorrected+Segment” dataset.

In this effort, we intent to utilize learnt machine
learning algorithms to do fast food recognition and
classiﬁcation. Our goal is to develop a computational
efﬁcient algorithm with high accuracy. Different fea-
tures and models have been implemented and com-
pared.

2. Dataset

The dataset we used in this study was based on
the Pittsburgh Fast-food Image Dataset (PFID) im-
ages. This dataset was proposed by Chen et al.
[Chen et al., 2009] to properly evaluate the perfor-
mances of food recognition. This dataset is composed
by 1359 food images with RGB-color of fast-food
dishes mainly acquired in laboratory.

3. Pre-processing

Because the original dataset contains different light-
ing for the same food, we used white balance to
do color correction in order to minimize the within
class variance. Besides, the background is not re-
lated to the food, thus background segmentation was
used to enable only food features being extracted.
Figure.1 shows one set of example from the three
datasets (original, color corrected, color corrected plus
background segmentation). We labeled the food im-
ages into eight categories: Bread Sandwich (breakfast

sandwich), Burger, Chicken, Donut, Pancake, Pizza,
Salad and Sandwich. Classiﬁcation results compari-
son among these three datasets will be discussed later.

4. Features and Models
4.1. Features

In this study, two popular features in terms of im-
age processing including Bag of SIFT (Scale-Invariant
Feature Transform) [Lowe, 1999] and color histogram
were chosen to capture the image content in our fast
food images.

We chose SIFT to extract food image textures and
used bag of features since it’s invariant to spatial trans-
lation and rotation, and it can provide ﬁxed length fea-
ture vectors.We chose color histogram to extract color
distributions. To ensure both efﬁciency and accuracy,
we choose 16 bins per RGB color. Later on, we ex-
plored combining these two features together and ob-
tained a better result.

4.2. Models
k-Nearest

algorithm

Neighbors

(k-NN)
[Cover and Hart, 1967] and Support Vector Ma-
chine (SVM) [Suykens and Vandewalle, 1999] were
used as learning methods in our study. They are two
popular discriminative classiﬁers with no distribution
requirement.
k-NN is simple to implement with
usually good result. And the accuracy of this method
can be highly inﬂuenced by parameter k (Figure
SVM was chosen since it’s very robust and
3).
well developed for classiﬁcation.
[Kim et al., 2012]
It was originally designed for two-class problems.
Therefore, we utilized “one-versus-the-rest” method
in our multi-class problem.

Moreover, χ2 Kernel was utilized for both meth-
ods as some study shows that it’s good at image tex-
ture recognition. [Zhang et al., 2007] χ2 Kernel is of-
ten used with bag of feature since it is a more natu-
ral distance measure between histograms than the eu-
clidean distance.
[Yang et al., 2009] The χ2 Kernel
comes from the χ2 distribution:

(1)

k(cid:88)

(cid:18) Xi − µi

(cid:19)2

i=1

σi

where Xi ∼ N (µi, σ2

i ), i = 1, . . . , k.

Figure 2. Effects of dimensionality on classiﬁcation. The
data was “ColorCorrected”. Modeling used bag of SIFT
and SVM with χ2 kernel.

The bag of SIFT and χ2 Kernel were employed us-
ing code from VLFeat [Vedaldi and Fulkerson, 2012].
SVM code is from LIBLINEAR [Fan et al., 2008] and
k-NN is from Matlab Machine Leaning toolbox. We
developed our own code to compute the color his-
togram without counting the background color.

5. Performance Analysis

5.1. Effects of data dimensionality

Data dimensionality can affect the classiﬁcation ac-
curacy a lot. Thus, we need to ﬁnd an optimal dimen-
sion. From Figure.2, at the beginning, the accuracy
increases very fast as the dimension increases. How-
ever, when the dimension reaches 104, the accuracy
increases very slowly, and even decreases a little when
dimension reaches 105. This is because, when the di-
mension is too high, the volume of the data space con-
centrates on the surface, therefore available feature di-
mension won’t increase. Thus increasing dimensional-
ity won’t help classiﬁcation at this point. In this study,
the optimal dimension is 113375, which corresponds
to shrink image to 30% along both X- and Y-axis.

5.2. Effects of “K” in k-NN

From Figure.3, increasing number of neighbors im-
proves the accuracy because it helps reduce noise ef-
fects on k-NN classiﬁcation. But including too much
neighbors would ruin the classiﬁcation, as neighbors
from incorrect class would be involved. From the clas-
siﬁcation accuracy on test data, we choose the number
of nearest neighbors to be 4.

Pixel number of images103104105106107Classification accuracy (%)102030405060708090100Effect of dimensionality of data on classification accuracyShrink imageto ratio of 0.3Figure 3. Effects of number of nearest neighbors on k-NN.
The data here is the “ColorCorrected”, χ2 kernel is used.

Figure 4. Effects of data size on classiﬁcation. The data here
is the “ColorCorrected”, using bag of SIFT and SVM with
χ2 kernel.

5.3. Effects of training Data Size

In this test, we kept the ratio of training data size to
test data size to be 2 to 1 and increased the training data
size to see the effects. From Figure.4, the more train-
ing data we have, the better classiﬁcation accuracy we
can obtain. Notice that the accuracy increases slowly
when the training size reaches 30. Since the time com-
plexity would increase linearly with data matrix size,
to balance the accuracy with speed, we choose to ran-
domly pick 50 images from each class as training data
and 25 as test data.

6. Results and Discussion
6.1. The importance of kernel

Figure 5. Kernel effects on SVM and k-NN classiﬁcation
accuracy

6.2. Comparison of features and models

Confusion matrix was used to capture the classiﬁ-
cation accuracy on the test dataset. Figure.6 shows ex-
amples of confusion matrix of classiﬁcation results on
”ColorCorrected” dataset with Color Histogram (left)
and bag of SIFT (right), using k-NN (top) and SVM
(bottom). χ2 kernel was implemented in all cases.
Comparing top two with bottom two ﬁgures, we can
see SVM achieves much better results than k-NN.

Moreover, from the top two ﬁgures, color histogram
performs better on images with more distinct color fea-
tures such as salad (The accuracy by color histogram
is 100% on salad while the accuracy by Bag of SIFT is
84%). On the other hand, Bag of SIFT performs better
for images with distinct textures, such as donut (The
accuracy by color histogram is 72% on donut while
the accuracy by Bag of SIFT is 96%). The bottom two
matrixes indicate the same conclusion.

In order to take advantages of both color and texture
characters, we linearly combined Color histogram and
bag of SIFT to a new feature (named “ColorSIFT”):

ColorSIFT = α·ColorHist+(1−α)·BagOfSIFT (2)

As shown in Figure.5, χ2 kernel helps SVM gain
much better accuracy. This is because SVM is a lin-
ear classiﬁer who suffers from under-ﬁtting. Kernel
enables SVM to work on high-dimensional non-linear
problem, which greatly beneﬁts SVM. Thus we chose
to utilize kernel in following experiments.

where α is the weight parameter for color histogram
and 1 − α is the weight parameter for bag of SIFT.
From experiments, we ﬁnd the optimal of α is 0.4. As
shown in Figure.7, the accuracy increased to 97.5%
compared with the formal best (94%).

K (number of nearest neighbors)12345678910Classification accuracy (%)60626466687072Effects of K on k-NN accuracyWe choose K=4number of training data in each class(number of test data is the half)102030405060classification accuracy (%)707580859095Accuracy vs. size of training data0%	  10%	  20%	  30%	  40%	  50%	  60%	  70%	  80%	  90%	  100%	  Color	  Histogram	  +	  SVM	  Bag	  of	  SIFT	  +	  SVM	  Color	  Histogram	  +	  k-­‐NN	  Bag	  of	  SIFT	  +	  k-­‐NN	  Accuracy	  With	  Kernel	  Without	  Kernel	  Figure 6. Confusion Matrix on “ColorCorrected” dataset with Color Histogram (left) and bag of SIFT (right), using models
of k-NN (top) and SVM (bottom). χ2 kernel was utilized in all cases.

Model
SVM

Feature

Original Dataset Segmented Dataset Color Corrected Dataset

Bag of SIFT

(with kernel)

Color Histogram

SVM

Bag of SIFT

(without kernel) Color Histogram

k-NN

Bag of SIFT

(with kernel)

Color Histogram

k-NN

Bag of SIFT

(without kernel) Color Histogram

93.00%
85.93%
55.84%
68.43%
66.82%
78.93%
68.93%
73.50%

92.00%
78.73%
58.70%
65.50%
62.82%
68.86%
60.43%
65.00%

94.00%
84.93%
70.05%
73.00%
71.39%
74.23%
71.43%
69.50%

Table 1. Classiﬁcation accuracy comparison among the Original, “ColorCorrected” and “ColorCorrected+Segment” dataset.

6.3. Comparison between Different Dataset

7. Conclusion

From Table.1, we can see three datasets achieve al-

most the same accuracy via different algorithms.

To recognize different kinds of food with various
appearance, we extracted color and texture features

	  0.590.521.000.960.720.840.840.24Bag of SIFT and KNN Confusion matrix (71.39 % accuracy with kernel)Predicted Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwichTrue Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwich0.820.721.000.720.520.961.000.20Color Histogram and KNN Confusion matrix (74.23 % accuracy with kernel)Predicted Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwichTrue Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwich1.001.001.001.000.921.000.880.72Bag of SIFT and SVM Confusion matrix (94.00 % accuracy with kernel)Predicted Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwichTrue Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwich0.950.681.000.960.441.001.000.76Color Histogram and SVM Confusion matrix (84.93 % accuracy with kernel)Predicted Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwichTrue Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwich!!!!!!!!!!!!!!Color!Histogram!!!!!!!!!!!vs.!!!!!!!!!!!!Bag!of!SIFT!SVM!!!!!!!!!!!!!!!!!!!!!!vs.!!!!!!!!!!!!!!!!!!!k8NN!Figure 7. Confusion Matrix on “ColorCorrected” dataset with combined feature of Color Histogram (left) and bag of SIFT
(right), using model of SVM with kernel.

from food images, and utilized k-NN and SVM to
do the classiﬁcation. To achieve a higher accuracy,
we linearly combined the two kind of features and
adopted the χ2 kernel. With performance analysis like
dimensionality and data size, we accomplished a high-
accuracy result with great computational efﬁciency.

8. Future Work

We plan to study for better background segmenta-
tion and color correction in image processing. We
are also going to explore more machine learning al-
gorithms and technique details. Moreover, we will
test algorithms with food images with real-life envi-
ronment.

References
[Bossard et al., 2014] Bossard, L., M. Guillaumin,

and L.
Van Gool, 2014, Food-101–mining discriminative compo-
nents with random forests, in Computer Vision–ECCV 2014:
Springer, 446–461.

[Chen et al., 2009] Chen, M., K. Dhingra, W. Wu, L. Yang, and
R. Sukthankar, 2009, Pﬁd: Pittsburgh fast-food image dataset:
Image Processing (ICIP), 2009 16th IEEE International Con-
ference on, IEEE, 289–292.

[Cover and Hart, 1967] Cover, T., and P. Hart, 1967, Nearest
Information Theory, IEEE

neighbor pattern classiﬁcation:
Transactions on, 13, 21–27.

[Fan et al., 2008] Fan, R.-E., K.-W. Chang, C.-J. Hsieh, X.-R.
Wang, and C.-J. Lin, 2008, Liblinear: A library for large linear

classiﬁcation: The Journal of Machine Learning Research, 9,
1871–1874.

[Kawano and Yanai, 2013] Kawano, Y., and K. Yanai, 2013,
Real-time mobile food recognition system: Computer Vision
and Pattern Recognition Workshops (CVPRW), 2013 IEEE
Conference on, IEEE, 1–7.

[Kim et al., 2012] Kim, J., B.-S. Kim, and S. Savarese, 2012,
Comparing image classiﬁcation methods: K-nearest-neighbor
and support-vector-machines: Proceedings of the 6th WSEAS
international conference on Computer Engineering and Appli-
cations, and Proceedings of the 2012 American conference
on Applied Mathematics, World Scientiﬁc and Engineering
Academy and Society (WSEAS), 133–138.

[Lowe, 1999] Lowe, D. G., 1999, Object recognition from lo-
cal scale-invariant features: Computer vision, 1999. The pro-
ceedings of the seventh IEEE international conference on, Ieee,
1150–1157.

[Suykens and Vandewalle, 1999] Suykens, J. A., and J. Vande-
walle, 1999, Least squares support vector machine classiﬁers:
Neural processing letters, 9, 293–300.

[Vedaldi and Fulkerson, 2012] Vedaldi, A., and B. Fulkerson,
2012, Vlfeat: An open and portable library of computer vision
algorithms (2008).

[Yang et al., 2009] Yang, J., K. Yu, Y. Gong, and T. Huang, 2009,
Linear spatial pyramid matching using sparse coding for image
classiﬁcation: Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, IEEE, 1794–1801.

[Zhang et al., 2007] Zhang, J., M. Marszałek, S. Lazebnik, and
C. Schmid, 2007, Local features and kernels for classiﬁcation
of texture and object categories: A comprehensive study: In-
ternational journal of computer vision, 73, 213–238.

1.001.001.001.000.881.001.000.92Bag of SIFT + Color Histogram and SVM Confusion matrix (97.50 % accuracy with kernel)Predicted Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwichTrue Class BreadSandBurgerChickenDonutPancakePizzaSaladSandwich