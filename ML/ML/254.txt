Equation to LaTeX

Abhinav Rastogi, Sevy Harris

{arastogi,sharris5}@stanford.edu

I.

Introduction

Copying equations from a pdf ﬁle to a LaTeX
document can be time consuming because
there is no easy way to convert the equations in
pdf form back to LaTeX code. This automatic
conversion requires segmentation of characters
from the image, identiﬁcation of these charac-
ters and conversion to LaTeX code. This project
deals with the ﬁrst two steps of this problem.
We have proposed a novel character segmen-
tation algorithm for printed text and have ex-
perimented with some features and classiﬁers
which classify the segmented characters.

II. Segmentation

We have made two assumptions for printed
text-
(a) All characters in the image are axis-aligned
(b) Pixels corresponding to a single character

are connected

The ﬁrst assumption is good because printed
text is well structured. If the image is not axis-
aligned, it can be corrected using convex-hull
detection if the rotation is less than 180◦ but
we have assumed that this rotation is absent
from our input images. The second assump-
tion holds true for the most of the characters in
printed text. Few characters like ‘i’, ‘j’, ‘=’ etc.
violate this assumption but they can be dealt
with separately.

Our segmentation algorithm is based on
the intuition that if we sum along the rows
of an axis-aligned image, the rows containing
characters will give a lower sum than the rows
containing blank pixels. Let us illustrate our
algorithm. Let I be an m × n image (m rows,
n columns) containing multiple equations.

Figure 1: The steps involved in segmentation of charac-
ters from an image of equation (order: top to
bottom)

Our segmentation algorithm proceeds in

the following steps -

1. Threshold the image Apply a threshold
of 0.9 to I to obtain a binary image B
containing 1 in blank pixels and 0 in char-
acter pixels. Remove or add blank pixels
on the border such that the minimum
distance of any character pixel from the
image border is 10 pixels.
2. Perform binary erosion We use a 1 × 10
horizontal mask to perform binary ero-
sion on I. This spreads the character
pixels along the rows so that when we

1

Equation to LaTeX

Abhinav Rastogi, Sevy Harris

{arastogi,sharris5}@stanford.edu

I.

Introduction

Copying equations from a pdf ﬁle to a LaTeX
document can be time consuming because
there is no easy way to convert the equations in
pdf form back to LaTeX code. This automatic
conversion requires segmentation of characters
from the image, identiﬁcation of these charac-
ters and conversion to LaTeX code. This project
deals with the ﬁrst two steps of this problem.
We have proposed a novel character segmen-
tation algorithm for printed text and have ex-
perimented with some features and classiﬁers
which classify the segmented characters.

II. Segmentation

We have made two assumptions for printed
text-
(a) All characters in the image are axis-aligned
(b) Pixels corresponding to a single character

are connected

The ﬁrst assumption is good because printed
text is well structured. If the image is not axis-
aligned, it can be corrected using convex-hull
detection if the rotation is less than 180◦ but
we have assumed that this rotation is absent
from our input images. The second assump-
tion holds true for the most of the characters in
printed text. Few characters like ‘i’, ‘j’, ‘=’ etc.
violate this assumption but they can be dealt
with separately.

Our segmentation algorithm is based on
the intuition that if we sum along the rows
of an axis-aligned image, the rows containing
characters will give a lower sum than the rows
containing blank pixels. Let us illustrate our
algorithm. Let I be an m × n image (m rows,
n columns) containing multiple equations.

Figure 1: The steps involved in segmentation of charac-
ters from an image of equation (order: top to
bottom)

Our segmentation algorithm proceeds in

the following steps -

1. Threshold the image Apply a threshold
of 0.9 to I to obtain a binary image B
containing 1 in blank pixels and 0 in char-
acter pixels. Remove or add blank pixels
on the border such that the minimum
distance of any character pixel from the
image border is 10 pixels.
2. Perform binary erosion We use a 1 × 10
horizontal mask to perform binary ero-
sion on I. This spreads the character
pixels along the rows so that when we

1

sum up the rows of the image, we get a
substantial difference between the sums
corresponding to blank rows and rows
containing characters. These images are
solely generated for the purpose of ob-
taining row split points.

3. Obtain blank rows Sum the horizontally
eroded image along rows to get a list
of m numbers {r1, r2, . . . rm} where ri =
j=1 I[i, j]. The set {i|ri > 0.99 max(ri)}
∑n
corresponds to the set of blank rows.
4. Split text lines As expected, many blank
rows occur next to each other. We take
the mid-point of each contiguous stretch
and split the image into many images,
each containing one row of text. Let
m1, m2, . . . mp be the mid-points of con-
tiguous stretches of blank columns. The
images I[mi : mi+1, :] are the images con-
taining a row of text for i = 1 to p − 1.
5. Split text vertically We perform steps 2-4
for the image of each line using a verti-
cal mask instead of horizontal and col-
umn sums instead of row sums to obtain
column splits and divide each text row
image further.

6. Obtain segmented characters Most of
the characters are segmented after step
5 but if the text contains characters like
‘i’ or subscripts in ∑, we might not have
segmented each character. So, if the im-
age obtained after step 5 has more than 1
connected component, steps 2-5 are again
repeated.

7. Scale and center Each obtained image is
then centered by addition or removal of
blank pixels on the border such that the
minimum distance of any character pixel
from the image border is 5 pixels.

III. Dataset

We wrote a python script which takes a txt ﬁle
as input and and substitutes its contents in a
tex ﬁle. A bash script then compiles the tex ﬁle
to generate the pdf document, converts the pdf
to an image, runs our segmentation algorithm
on the obtained image to segment the different

2

characters and labels each image according to
the its character name obtained from the txt
ﬁle. The above procedure can be used to obtain
as much data as we need with very little effort.
For the purpose of this project, we have fo-
cused on the identiﬁcation of upper and lower
case roman alphabets and digits. Our dataset
consists of 8 images of each of these characters
and contains 472 images in total.

IV. Features

Feature selection becomes a critical part of
this problem since it involves a large number
of classes and the size of each class is much
smaller than the number of classes. The fol-
lowing feature mappings map the image of
a character to a vector in a multidimensional
euclidean space. The learning algorithms are
then trained and tested on those vectors. We
tested different models on these three sets of
features-

(i) Average Pixel Intensity features (A) The
whole image is divided into a 16x16 grid
and the average pixel intensity in each
grid is calculated. The feature vector is of
size 256x1 and is obtained by concatenat-
ing the columns of the 16x16 image.

(ii) Neighborhood pattern features (B) The
pattern formed by the adjacent pixels
seems to be an important cue in the iden-
tiﬁcation of a digit. However, average
pixel intensity features do not take the
pattern formed by neighboring pixels into
account. If we consider a 3 × 3 neighbor-
hood of a pixel, there are 29 = 512 pos-
sible patterns that can be formed by the
pixels in this neighborhood. We assign
a number to each such pattern. Under
this feature mapping, we ﬁrst convert the
image into a 16 × 16 image as described
above. Then each non-border pixel, which
are 14 × 14 = 196 in number are assigned
the number corresponding to the pattern
formed by its 3 × 3 neighborhood. The
feature vector of size 196 × 1 is obtained
by concatenating the numbers assigned
to each of the non-border pixels in the

Equation to LaTeX

Abhinav Rastogi, Sevy Harris

{arastogi,sharris5}@stanford.edu

I.

Introduction

Copying equations from a pdf ﬁle to a LaTeX
document can be time consuming because
there is no easy way to convert the equations in
pdf form back to LaTeX code. This automatic
conversion requires segmentation of characters
from the image, identiﬁcation of these charac-
ters and conversion to LaTeX code. This project
deals with the ﬁrst two steps of this problem.
We have proposed a novel character segmen-
tation algorithm for printed text and have ex-
perimented with some features and classiﬁers
which classify the segmented characters.

II. Segmentation

We have made two assumptions for printed
text-
(a) All characters in the image are axis-aligned
(b) Pixels corresponding to a single character

are connected

The ﬁrst assumption is good because printed
text is well structured. If the image is not axis-
aligned, it can be corrected using convex-hull
detection if the rotation is less than 180◦ but
we have assumed that this rotation is absent
from our input images. The second assump-
tion holds true for the most of the characters in
printed text. Few characters like ‘i’, ‘j’, ‘=’ etc.
violate this assumption but they can be dealt
with separately.

Our segmentation algorithm is based on
the intuition that if we sum along the rows
of an axis-aligned image, the rows containing
characters will give a lower sum than the rows
containing blank pixels. Let us illustrate our
algorithm. Let I be an m × n image (m rows,
n columns) containing multiple equations.

Figure 1: The steps involved in segmentation of charac-
ters from an image of equation (order: top to
bottom)

Our segmentation algorithm proceeds in

the following steps -

1. Threshold the image Apply a threshold
of 0.9 to I to obtain a binary image B
containing 1 in blank pixels and 0 in char-
acter pixels. Remove or add blank pixels
on the border such that the minimum
distance of any character pixel from the
image border is 10 pixels.
2. Perform binary erosion We use a 1 × 10
horizontal mask to perform binary ero-
sion on I. This spreads the character
pixels along the rows so that when we

1

sum up the rows of the image, we get a
substantial difference between the sums
corresponding to blank rows and rows
containing characters. These images are
solely generated for the purpose of ob-
taining row split points.

3. Obtain blank rows Sum the horizontally
eroded image along rows to get a list
of m numbers {r1, r2, . . . rm} where ri =
j=1 I[i, j]. The set {i|ri > 0.99 max(ri)}
∑n
corresponds to the set of blank rows.
4. Split text lines As expected, many blank
rows occur next to each other. We take
the mid-point of each contiguous stretch
and split the image into many images,
each containing one row of text. Let
m1, m2, . . . mp be the mid-points of con-
tiguous stretches of blank columns. The
images I[mi : mi+1, :] are the images con-
taining a row of text for i = 1 to p − 1.
5. Split text vertically We perform steps 2-4
for the image of each line using a verti-
cal mask instead of horizontal and col-
umn sums instead of row sums to obtain
column splits and divide each text row
image further.

6. Obtain segmented characters Most of
the characters are segmented after step
5 but if the text contains characters like
‘i’ or subscripts in ∑, we might not have
segmented each character. So, if the im-
age obtained after step 5 has more than 1
connected component, steps 2-5 are again
repeated.

7. Scale and center Each obtained image is
then centered by addition or removal of
blank pixels on the border such that the
minimum distance of any character pixel
from the image border is 5 pixels.

III. Dataset

We wrote a python script which takes a txt ﬁle
as input and and substitutes its contents in a
tex ﬁle. A bash script then compiles the tex ﬁle
to generate the pdf document, converts the pdf
to an image, runs our segmentation algorithm
on the obtained image to segment the different

2

characters and labels each image according to
the its character name obtained from the txt
ﬁle. The above procedure can be used to obtain
as much data as we need with very little effort.
For the purpose of this project, we have fo-
cused on the identiﬁcation of upper and lower
case roman alphabets and digits. Our dataset
consists of 8 images of each of these characters
and contains 472 images in total.

IV. Features

Feature selection becomes a critical part of
this problem since it involves a large number
of classes and the size of each class is much
smaller than the number of classes. The fol-
lowing feature mappings map the image of
a character to a vector in a multidimensional
euclidean space. The learning algorithms are
then trained and tested on those vectors. We
tested different models on these three sets of
features-

(i) Average Pixel Intensity features (A) The
whole image is divided into a 16x16 grid
and the average pixel intensity in each
grid is calculated. The feature vector is of
size 256x1 and is obtained by concatenat-
ing the columns of the 16x16 image.

(ii) Neighborhood pattern features (B) The
pattern formed by the adjacent pixels
seems to be an important cue in the iden-
tiﬁcation of a digit. However, average
pixel intensity features do not take the
pattern formed by neighboring pixels into
account. If we consider a 3 × 3 neighbor-
hood of a pixel, there are 29 = 512 pos-
sible patterns that can be formed by the
pixels in this neighborhood. We assign
a number to each such pattern. Under
this feature mapping, we ﬁrst convert the
image into a 16 × 16 image as described
above. Then each non-border pixel, which
are 14 × 14 = 196 in number are assigned
the number corresponding to the pattern
formed by its 3 × 3 neighborhood. The
feature vector of size 196 × 1 is obtained
by concatenating the numbers assigned
to each of the non-border pixels in the

16 × 16 image.

(iii) Discrete Fourier Transform features (C)
The fourier transform captures the trends
in relative patterns and is not as suscepti-
ble to noise as the neighborhood pattern
features. The feature vector is obtained by
taking the DFT of the 16x16 average pixel
intensity image. The real and imaginary
parts of the DFT coefﬁcients are then con-
catenated to form a 512 × 1 feature vector.

Figure 2: (i) Original image extracted from text (ii)
16 × 16 image with average pixel intensities
(iii) Absolute value of real part DFT coefﬁ-
cients and (iv) Absolute value of imaginary
part DFT coefﬁcients (order: left to right)

V. Models

We employed the multiclass version of the fol-
lowing models to classify the characters. The
training and test error results for these models
are shown in the results section.

(i) Naive Bayes We chose Naive Bayes as a
baseline model so that we could get a
rough idea of the difﬁculty of the task.
We trained a Naive Bayes model using
the average pixel intensity features and
neighborhood pattern features.

(ii) Linear Discriminant Analysis This
model assumes that the conditional dis-
tribution of the features given the class is
gaussian. This assumption is not explic-
itly true for our dataset and all classes
having the same covariance matrix does
not seem to be a good assumption. Nev-
ertheless, we expected a fairly good result
because much variability is not present in
our class. For the multiclass formulation
of LDA, we used a one-vs-one approach.
This involves training a classiﬁer for each
pairwise combination of classes. So, if
the number of classes is K then we train
K(K − 1)/2 classiﬁers. During prediction,

3

we classify the input using all of these
classiﬁers and the class which gets the ma-
jority vote is picked to be our predicted
class.

(iii) Support Vector Machine SVM is a fairly
good choice if we assume that the feature
space is linearly separable. This is not
intuitively clear as our feature space has
a high dimension. Nevertheless, as previ-
ously mentioned, our classes do not have
a large variability and hence we expect
classes to be linearly separable pairwise.
However, another issue is that our fea-
ture space has far more dimensions than
the number of data points and our data
points are close to each other. SVM (or
any linear classiﬁer) may pose the danger
of overﬁtting. For this reason, we did not
use polynomial or RBF kernels for SVM
because it further increases the effective
dimensionality for the feature space. The
objective function of or algorithm is

min
w,b,ζ

1
2

wTw + C

n∑

i=1

ζi

subject to yi(wTφ(xi) + b) ≥ 1 − ζi

where ζi ≥ 0 for i = 1, . . . n

Our model uses a one-vs-one approach
for the multiclass formulation of SVM as
described previously.

(iv) Random forest This model can perform
well given large feature sets because it
combines the predictions of various de-
cision trees to build a more robust clas-
siﬁer. While constructing new decision
trees, this method uses a random subset
of features. This helps us to get rid of
spurious features and might improve the
robustness of our estimate. Our random
forest model makes use of ID3 algorithm
for training decision trees and uses the
Gini measure for calculating the goodness
of a split criteria. The Gini impurity mea-
sure is deﬁned as

Gini(Xm) = ∑
k

pmk(1 − pmk)

Equation to LaTeX

Abhinav Rastogi, Sevy Harris

{arastogi,sharris5}@stanford.edu

I.

Introduction

Copying equations from a pdf ﬁle to a LaTeX
document can be time consuming because
there is no easy way to convert the equations in
pdf form back to LaTeX code. This automatic
conversion requires segmentation of characters
from the image, identiﬁcation of these charac-
ters and conversion to LaTeX code. This project
deals with the ﬁrst two steps of this problem.
We have proposed a novel character segmen-
tation algorithm for printed text and have ex-
perimented with some features and classiﬁers
which classify the segmented characters.

II. Segmentation

We have made two assumptions for printed
text-
(a) All characters in the image are axis-aligned
(b) Pixels corresponding to a single character

are connected

The ﬁrst assumption is good because printed
text is well structured. If the image is not axis-
aligned, it can be corrected using convex-hull
detection if the rotation is less than 180◦ but
we have assumed that this rotation is absent
from our input images. The second assump-
tion holds true for the most of the characters in
printed text. Few characters like ‘i’, ‘j’, ‘=’ etc.
violate this assumption but they can be dealt
with separately.

Our segmentation algorithm is based on
the intuition that if we sum along the rows
of an axis-aligned image, the rows containing
characters will give a lower sum than the rows
containing blank pixels. Let us illustrate our
algorithm. Let I be an m × n image (m rows,
n columns) containing multiple equations.

Figure 1: The steps involved in segmentation of charac-
ters from an image of equation (order: top to
bottom)

Our segmentation algorithm proceeds in

the following steps -

1. Threshold the image Apply a threshold
of 0.9 to I to obtain a binary image B
containing 1 in blank pixels and 0 in char-
acter pixels. Remove or add blank pixels
on the border such that the minimum
distance of any character pixel from the
image border is 10 pixels.
2. Perform binary erosion We use a 1 × 10
horizontal mask to perform binary ero-
sion on I. This spreads the character
pixels along the rows so that when we

1

sum up the rows of the image, we get a
substantial difference between the sums
corresponding to blank rows and rows
containing characters. These images are
solely generated for the purpose of ob-
taining row split points.

3. Obtain blank rows Sum the horizontally
eroded image along rows to get a list
of m numbers {r1, r2, . . . rm} where ri =
j=1 I[i, j]. The set {i|ri > 0.99 max(ri)}
∑n
corresponds to the set of blank rows.
4. Split text lines As expected, many blank
rows occur next to each other. We take
the mid-point of each contiguous stretch
and split the image into many images,
each containing one row of text. Let
m1, m2, . . . mp be the mid-points of con-
tiguous stretches of blank columns. The
images I[mi : mi+1, :] are the images con-
taining a row of text for i = 1 to p − 1.
5. Split text vertically We perform steps 2-4
for the image of each line using a verti-
cal mask instead of horizontal and col-
umn sums instead of row sums to obtain
column splits and divide each text row
image further.

6. Obtain segmented characters Most of
the characters are segmented after step
5 but if the text contains characters like
‘i’ or subscripts in ∑, we might not have
segmented each character. So, if the im-
age obtained after step 5 has more than 1
connected component, steps 2-5 are again
repeated.

7. Scale and center Each obtained image is
then centered by addition or removal of
blank pixels on the border such that the
minimum distance of any character pixel
from the image border is 5 pixels.

III. Dataset

We wrote a python script which takes a txt ﬁle
as input and and substitutes its contents in a
tex ﬁle. A bash script then compiles the tex ﬁle
to generate the pdf document, converts the pdf
to an image, runs our segmentation algorithm
on the obtained image to segment the different

2

characters and labels each image according to
the its character name obtained from the txt
ﬁle. The above procedure can be used to obtain
as much data as we need with very little effort.
For the purpose of this project, we have fo-
cused on the identiﬁcation of upper and lower
case roman alphabets and digits. Our dataset
consists of 8 images of each of these characters
and contains 472 images in total.

IV. Features

Feature selection becomes a critical part of
this problem since it involves a large number
of classes and the size of each class is much
smaller than the number of classes. The fol-
lowing feature mappings map the image of
a character to a vector in a multidimensional
euclidean space. The learning algorithms are
then trained and tested on those vectors. We
tested different models on these three sets of
features-

(i) Average Pixel Intensity features (A) The
whole image is divided into a 16x16 grid
and the average pixel intensity in each
grid is calculated. The feature vector is of
size 256x1 and is obtained by concatenat-
ing the columns of the 16x16 image.

(ii) Neighborhood pattern features (B) The
pattern formed by the adjacent pixels
seems to be an important cue in the iden-
tiﬁcation of a digit. However, average
pixel intensity features do not take the
pattern formed by neighboring pixels into
account. If we consider a 3 × 3 neighbor-
hood of a pixel, there are 29 = 512 pos-
sible patterns that can be formed by the
pixels in this neighborhood. We assign
a number to each such pattern. Under
this feature mapping, we ﬁrst convert the
image into a 16 × 16 image as described
above. Then each non-border pixel, which
are 14 × 14 = 196 in number are assigned
the number corresponding to the pattern
formed by its 3 × 3 neighborhood. The
feature vector of size 196 × 1 is obtained
by concatenating the numbers assigned
to each of the non-border pixels in the

16 × 16 image.

(iii) Discrete Fourier Transform features (C)
The fourier transform captures the trends
in relative patterns and is not as suscepti-
ble to noise as the neighborhood pattern
features. The feature vector is obtained by
taking the DFT of the 16x16 average pixel
intensity image. The real and imaginary
parts of the DFT coefﬁcients are then con-
catenated to form a 512 × 1 feature vector.

Figure 2: (i) Original image extracted from text (ii)
16 × 16 image with average pixel intensities
(iii) Absolute value of real part DFT coefﬁ-
cients and (iv) Absolute value of imaginary
part DFT coefﬁcients (order: left to right)

V. Models

We employed the multiclass version of the fol-
lowing models to classify the characters. The
training and test error results for these models
are shown in the results section.

(i) Naive Bayes We chose Naive Bayes as a
baseline model so that we could get a
rough idea of the difﬁculty of the task.
We trained a Naive Bayes model using
the average pixel intensity features and
neighborhood pattern features.

(ii) Linear Discriminant Analysis This
model assumes that the conditional dis-
tribution of the features given the class is
gaussian. This assumption is not explic-
itly true for our dataset and all classes
having the same covariance matrix does
not seem to be a good assumption. Nev-
ertheless, we expected a fairly good result
because much variability is not present in
our class. For the multiclass formulation
of LDA, we used a one-vs-one approach.
This involves training a classiﬁer for each
pairwise combination of classes. So, if
the number of classes is K then we train
K(K − 1)/2 classiﬁers. During prediction,

3

we classify the input using all of these
classiﬁers and the class which gets the ma-
jority vote is picked to be our predicted
class.

(iii) Support Vector Machine SVM is a fairly
good choice if we assume that the feature
space is linearly separable. This is not
intuitively clear as our feature space has
a high dimension. Nevertheless, as previ-
ously mentioned, our classes do not have
a large variability and hence we expect
classes to be linearly separable pairwise.
However, another issue is that our fea-
ture space has far more dimensions than
the number of data points and our data
points are close to each other. SVM (or
any linear classiﬁer) may pose the danger
of overﬁtting. For this reason, we did not
use polynomial or RBF kernels for SVM
because it further increases the effective
dimensionality for the feature space. The
objective function of or algorithm is

min
w,b,ζ

1
2

wTw + C

n∑

i=1

ζi

subject to yi(wTφ(xi) + b) ≥ 1 − ζi

where ζi ≥ 0 for i = 1, . . . n

Our model uses a one-vs-one approach
for the multiclass formulation of SVM as
described previously.

(iv) Random forest This model can perform
well given large feature sets because it
combines the predictions of various de-
cision trees to build a more robust clas-
siﬁer. While constructing new decision
trees, this method uses a random subset
of features. This helps us to get rid of
spurious features and might improve the
robustness of our estimate. Our random
forest model makes use of ID3 algorithm
for training decision trees and uses the
Gini measure for calculating the goodness
of a split criteria. The Gini impurity mea-
sure is deﬁned as

Gini(Xm) = ∑
k

pmk(1 − pmk)

where pmk is the fraction of times an ele-
ment of class k occurs in a split Xm of set
X. Gini approximates the entropy func-
tion and hence our decision tree approx-
imates a sequence of split criteria that
decrease the entropy of the ﬁnal parti-
tion. After trainig numerous decision
trees, our model combines them using
the AdaBoost algorithm.

VI. Results

We experimented with various combinations
of features and models described above. The
training and test results are as follows. The ab-
breviations for feature names have been given
in the section on features. The errors are re-
ported as the percentage of data misclassiﬁed.
The training data consists for 354 images and
the test data consists of 118 images.

Model A
0.1638
NB
LDA
0.1299
SVM 0.0734
RF
0.1582

Feature
B
0.3842
0.3729
0.3672
0.3616

C
-
0.0
0.0
0.0

Table 1: Training Error of feature model combination

Model A
0.0339
NB
0.0339
LDA
SVM 0.0169
RF
0.1017

Feature
B
0.3729
0.3559
0.3559
0.3559

C
-
0.0678
0.0508
0.0847

Table 2: Test Error of feature model combination

VII. Discussion

As expected, the average pixel intensities fail to
give a good result because a slight shift in the
image might lead to a different feature vector
altogether. We expected neighborhood pixel
pattern features to work better than average
pixel features but it was surprising to ﬁnd that

they perform worse. On further deliberation,
we feel that these features fail due to a simi-
lar reason. A slight shift in the image might
severely affect the feature vector. Moreover,
the effect of noise is more pronounced in this
feature because a noisy pixel will considerably
change up to 9 components of the feature vec-
tor. These components correspond to the noisy
pixel and its neighbors.

It is tough to visualize the exact correspon-
dence between a DFT feature and the image.
Hence, we had a neutral expectation from this
feature mapping before we began working on
it. As can be seen, these features give a con-
siderably better result than other features. Our
explanation for this observation is that DFT
features capture the spatial periodicities of var-
ious pixels. DFT can be thought of as averaging
after multiplication by sinusoids of different an-
gular frequencies which are integral fractions
of 2π. Due to the averaging operation, the DFT
features are robust to noise. Further, a shift in
the image corresponds to the change in phase
of the DFT coefﬁcients and hence the feature
vectors are not affected much.

As far as models are concerned, we didn’t
expect a good result from our baseline model,
which was a multiclass Naive Bayes classiﬁer.
This is because NB assumes that the pixel in-
tensities are independent, which doesn’t seem
to be a good assumption. The LDA classiﬁer
works for our data because we have very lit-
tle variability within each class and hence the
assumption that all classes share the same co-
variance matrix doesn’t have much effect on
the accuracy of our algorithm if the classes are
far apart in the feature space, which is true for
most of the classes.

The random forest model is more robust
and helps in decreasing the variance of the clas-
siﬁer because it uses a set of decision trees to
make its decision. Since we pick up a random
subset of features in the process of training
the decision trees, we automatically ﬁlter out
good features which affect our decision. Our
intuition is validated by the fact that RF does
better than SVM when we use an inferior set
of features (features A and B) because of its

4

Equation to LaTeX

Abhinav Rastogi, Sevy Harris

{arastogi,sharris5}@stanford.edu

I.

Introduction

Copying equations from a pdf ﬁle to a LaTeX
document can be time consuming because
there is no easy way to convert the equations in
pdf form back to LaTeX code. This automatic
conversion requires segmentation of characters
from the image, identiﬁcation of these charac-
ters and conversion to LaTeX code. This project
deals with the ﬁrst two steps of this problem.
We have proposed a novel character segmen-
tation algorithm for printed text and have ex-
perimented with some features and classiﬁers
which classify the segmented characters.

II. Segmentation

We have made two assumptions for printed
text-
(a) All characters in the image are axis-aligned
(b) Pixels corresponding to a single character

are connected

The ﬁrst assumption is good because printed
text is well structured. If the image is not axis-
aligned, it can be corrected using convex-hull
detection if the rotation is less than 180◦ but
we have assumed that this rotation is absent
from our input images. The second assump-
tion holds true for the most of the characters in
printed text. Few characters like ‘i’, ‘j’, ‘=’ etc.
violate this assumption but they can be dealt
with separately.

Our segmentation algorithm is based on
the intuition that if we sum along the rows
of an axis-aligned image, the rows containing
characters will give a lower sum than the rows
containing blank pixels. Let us illustrate our
algorithm. Let I be an m × n image (m rows,
n columns) containing multiple equations.

Figure 1: The steps involved in segmentation of charac-
ters from an image of equation (order: top to
bottom)

Our segmentation algorithm proceeds in

the following steps -

1. Threshold the image Apply a threshold
of 0.9 to I to obtain a binary image B
containing 1 in blank pixels and 0 in char-
acter pixels. Remove or add blank pixels
on the border such that the minimum
distance of any character pixel from the
image border is 10 pixels.
2. Perform binary erosion We use a 1 × 10
horizontal mask to perform binary ero-
sion on I. This spreads the character
pixels along the rows so that when we

1

sum up the rows of the image, we get a
substantial difference between the sums
corresponding to blank rows and rows
containing characters. These images are
solely generated for the purpose of ob-
taining row split points.

3. Obtain blank rows Sum the horizontally
eroded image along rows to get a list
of m numbers {r1, r2, . . . rm} where ri =
j=1 I[i, j]. The set {i|ri > 0.99 max(ri)}
∑n
corresponds to the set of blank rows.
4. Split text lines As expected, many blank
rows occur next to each other. We take
the mid-point of each contiguous stretch
and split the image into many images,
each containing one row of text. Let
m1, m2, . . . mp be the mid-points of con-
tiguous stretches of blank columns. The
images I[mi : mi+1, :] are the images con-
taining a row of text for i = 1 to p − 1.
5. Split text vertically We perform steps 2-4
for the image of each line using a verti-
cal mask instead of horizontal and col-
umn sums instead of row sums to obtain
column splits and divide each text row
image further.

6. Obtain segmented characters Most of
the characters are segmented after step
5 but if the text contains characters like
‘i’ or subscripts in ∑, we might not have
segmented each character. So, if the im-
age obtained after step 5 has more than 1
connected component, steps 2-5 are again
repeated.

7. Scale and center Each obtained image is
then centered by addition or removal of
blank pixels on the border such that the
minimum distance of any character pixel
from the image border is 5 pixels.

III. Dataset

We wrote a python script which takes a txt ﬁle
as input and and substitutes its contents in a
tex ﬁle. A bash script then compiles the tex ﬁle
to generate the pdf document, converts the pdf
to an image, runs our segmentation algorithm
on the obtained image to segment the different

2

characters and labels each image according to
the its character name obtained from the txt
ﬁle. The above procedure can be used to obtain
as much data as we need with very little effort.
For the purpose of this project, we have fo-
cused on the identiﬁcation of upper and lower
case roman alphabets and digits. Our dataset
consists of 8 images of each of these characters
and contains 472 images in total.

IV. Features

Feature selection becomes a critical part of
this problem since it involves a large number
of classes and the size of each class is much
smaller than the number of classes. The fol-
lowing feature mappings map the image of
a character to a vector in a multidimensional
euclidean space. The learning algorithms are
then trained and tested on those vectors. We
tested different models on these three sets of
features-

(i) Average Pixel Intensity features (A) The
whole image is divided into a 16x16 grid
and the average pixel intensity in each
grid is calculated. The feature vector is of
size 256x1 and is obtained by concatenat-
ing the columns of the 16x16 image.

(ii) Neighborhood pattern features (B) The
pattern formed by the adjacent pixels
seems to be an important cue in the iden-
tiﬁcation of a digit. However, average
pixel intensity features do not take the
pattern formed by neighboring pixels into
account. If we consider a 3 × 3 neighbor-
hood of a pixel, there are 29 = 512 pos-
sible patterns that can be formed by the
pixels in this neighborhood. We assign
a number to each such pattern. Under
this feature mapping, we ﬁrst convert the
image into a 16 × 16 image as described
above. Then each non-border pixel, which
are 14 × 14 = 196 in number are assigned
the number corresponding to the pattern
formed by its 3 × 3 neighborhood. The
feature vector of size 196 × 1 is obtained
by concatenating the numbers assigned
to each of the non-border pixels in the

16 × 16 image.

(iii) Discrete Fourier Transform features (C)
The fourier transform captures the trends
in relative patterns and is not as suscepti-
ble to noise as the neighborhood pattern
features. The feature vector is obtained by
taking the DFT of the 16x16 average pixel
intensity image. The real and imaginary
parts of the DFT coefﬁcients are then con-
catenated to form a 512 × 1 feature vector.

Figure 2: (i) Original image extracted from text (ii)
16 × 16 image with average pixel intensities
(iii) Absolute value of real part DFT coefﬁ-
cients and (iv) Absolute value of imaginary
part DFT coefﬁcients (order: left to right)

V. Models

We employed the multiclass version of the fol-
lowing models to classify the characters. The
training and test error results for these models
are shown in the results section.

(i) Naive Bayes We chose Naive Bayes as a
baseline model so that we could get a
rough idea of the difﬁculty of the task.
We trained a Naive Bayes model using
the average pixel intensity features and
neighborhood pattern features.

(ii) Linear Discriminant Analysis This
model assumes that the conditional dis-
tribution of the features given the class is
gaussian. This assumption is not explic-
itly true for our dataset and all classes
having the same covariance matrix does
not seem to be a good assumption. Nev-
ertheless, we expected a fairly good result
because much variability is not present in
our class. For the multiclass formulation
of LDA, we used a one-vs-one approach.
This involves training a classiﬁer for each
pairwise combination of classes. So, if
the number of classes is K then we train
K(K − 1)/2 classiﬁers. During prediction,

3

we classify the input using all of these
classiﬁers and the class which gets the ma-
jority vote is picked to be our predicted
class.

(iii) Support Vector Machine SVM is a fairly
good choice if we assume that the feature
space is linearly separable. This is not
intuitively clear as our feature space has
a high dimension. Nevertheless, as previ-
ously mentioned, our classes do not have
a large variability and hence we expect
classes to be linearly separable pairwise.
However, another issue is that our fea-
ture space has far more dimensions than
the number of data points and our data
points are close to each other. SVM (or
any linear classiﬁer) may pose the danger
of overﬁtting. For this reason, we did not
use polynomial or RBF kernels for SVM
because it further increases the effective
dimensionality for the feature space. The
objective function of or algorithm is

min
w,b,ζ

1
2

wTw + C

n∑

i=1

ζi

subject to yi(wTφ(xi) + b) ≥ 1 − ζi

where ζi ≥ 0 for i = 1, . . . n

Our model uses a one-vs-one approach
for the multiclass formulation of SVM as
described previously.

(iv) Random forest This model can perform
well given large feature sets because it
combines the predictions of various de-
cision trees to build a more robust clas-
siﬁer. While constructing new decision
trees, this method uses a random subset
of features. This helps us to get rid of
spurious features and might improve the
robustness of our estimate. Our random
forest model makes use of ID3 algorithm
for training decision trees and uses the
Gini measure for calculating the goodness
of a split criteria. The Gini impurity mea-
sure is deﬁned as

Gini(Xm) = ∑
k

pmk(1 − pmk)

where pmk is the fraction of times an ele-
ment of class k occurs in a split Xm of set
X. Gini approximates the entropy func-
tion and hence our decision tree approx-
imates a sequence of split criteria that
decrease the entropy of the ﬁnal parti-
tion. After trainig numerous decision
trees, our model combines them using
the AdaBoost algorithm.

VI. Results

We experimented with various combinations
of features and models described above. The
training and test results are as follows. The ab-
breviations for feature names have been given
in the section on features. The errors are re-
ported as the percentage of data misclassiﬁed.
The training data consists for 354 images and
the test data consists of 118 images.

Model A
0.1638
NB
LDA
0.1299
SVM 0.0734
RF
0.1582

Feature
B
0.3842
0.3729
0.3672
0.3616

C
-
0.0
0.0
0.0

Table 1: Training Error of feature model combination

Model A
0.0339
NB
0.0339
LDA
SVM 0.0169
RF
0.1017

Feature
B
0.3729
0.3559
0.3559
0.3559

C
-
0.0678
0.0508
0.0847

Table 2: Test Error of feature model combination

VII. Discussion

As expected, the average pixel intensities fail to
give a good result because a slight shift in the
image might lead to a different feature vector
altogether. We expected neighborhood pixel
pattern features to work better than average
pixel features but it was surprising to ﬁnd that

they perform worse. On further deliberation,
we feel that these features fail due to a simi-
lar reason. A slight shift in the image might
severely affect the feature vector. Moreover,
the effect of noise is more pronounced in this
feature because a noisy pixel will considerably
change up to 9 components of the feature vec-
tor. These components correspond to the noisy
pixel and its neighbors.

It is tough to visualize the exact correspon-
dence between a DFT feature and the image.
Hence, we had a neutral expectation from this
feature mapping before we began working on
it. As can be seen, these features give a con-
siderably better result than other features. Our
explanation for this observation is that DFT
features capture the spatial periodicities of var-
ious pixels. DFT can be thought of as averaging
after multiplication by sinusoids of different an-
gular frequencies which are integral fractions
of 2π. Due to the averaging operation, the DFT
features are robust to noise. Further, a shift in
the image corresponds to the change in phase
of the DFT coefﬁcients and hence the feature
vectors are not affected much.

As far as models are concerned, we didn’t
expect a good result from our baseline model,
which was a multiclass Naive Bayes classiﬁer.
This is because NB assumes that the pixel in-
tensities are independent, which doesn’t seem
to be a good assumption. The LDA classiﬁer
works for our data because we have very lit-
tle variability within each class and hence the
assumption that all classes share the same co-
variance matrix doesn’t have much effect on
the accuracy of our algorithm if the classes are
far apart in the feature space, which is true for
most of the classes.

The random forest model is more robust
and helps in decreasing the variance of the clas-
siﬁer because it uses a set of decision trees to
make its decision. Since we pick up a random
subset of features in the process of training
the decision trees, we automatically ﬁlter out
good features which affect our decision. Our
intuition is validated by the fact that RF does
better than SVM when we use an inferior set
of features (features A and B) because of its

4

ability to discard irrelevant features. However,
when we have a better set of features (feature
C), its performance is close to that of SVM.

SVM and DFT features was the best model-
feature combination we found. On test data,
our model misclassiﬁes 6 out of 118 images.
The errors involve misclassiﬁcation of ‘0’, ‘1’
and ‘2’ as ‘o’, ‘t’ and ‘a’ respectively. The con-
fused characters are structurally similar and
hence the confusion is plausible.

VIII. Future Work

Classiﬁcation of printed characters is a tough
task. Close to perfect accuracy is desirable in
order to build a system for converting equa-
tions to LaTeX commands. We have only
worked on a subset of characters in this project.
If one were to include greek alphabets too, the
error would increase because of presence of
similar characters like ‘a’ and ‘α’, ‘B’ and ‘β’
etc.

While doing a literature survey, we found
that neural networks give a good result for
handwritten digits. It would be interesting to
explore how they work on printed text when

lots of different classes are present.

Another interesting but non-machine learn-
ing aspect of this problem which we have not
considered in this project is the generation of
LaTeX commands after segmentation and clas-
siﬁcation of text.

References

[1] T. Mitchell, Machine Learning: A Guide to
Current Research: Springer, 1986. pp52-78

[2] A. Ng, "CS 229 Lecture Notes: Sup-
[online]

Vector Machines,"

port
cs229.stanford.edu/notes

[3] E. Brown,

"Character Recognition by
[online],

Feature Point Extraction,"
www.ccs.neu.edu/home/feneric

[4] Mahmoud, S.A.; Mahmoud, A.S., "Ara-
bic Character Recognition using Modiﬁed
Fourier Spectrum (MFS)," Geometric Mod-
eling and Imaging - New Trends, 2006 ,
vol., no., pp.155,159, 16-18 Aug. 1993

[5] Scikit Learn- A machine learning library

for Python [online] scikitlearn.org

5

