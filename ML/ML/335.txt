CS229 Final Project: Multi-class motif discovery in

keratinocyte diﬀerentiation

Daniel Kim

December 12, 2014

Introduction

Enhancer elements are short segments of regulatory DNA that play important roles in activating gene ex-
pression within dynamic biological processes like cell diﬀerentiation. These enhancer regions are often bound
by activating proteins known as transcription factors (TFs), and the DNA sites bound by TFs often have a
sequence pattern, known as a ’motif’, that can be matched to individual TFs. With over 2 million putative
enhancer regions in the human genome, understanding enhancer function is essential to understanding dy-
namic gene regulation. As such, discovering regulatory sequence motifs within enhancer regions is an area of
continuing research. Previous methods have included the use of discovered motifs and looking for matches
within enhancer regions, as well as machine learning techniques (logistic regression and SVMs) among others
[2]. However, such methods have only compared positive examples to negative examples, in a binary-class
format. Here, we extend machine learning techniques to discover sequence features in a multi- class format,
which is useful in situations where enhancers have varying activity patterns and we are interested in the
underlying sequence features that are leading to these multiple patterns. We use the skin diﬀerentiation
process as our test case.

Skin diﬀerentiation, the transition from skin progenitor cell to diﬀerentiated keratinocyte cell, is a process
disrupted in skin cancer (squamous cell carcinoma and basal cell carcinoma), psoriasis, chronic wounds, and
nearly 100 inherited human skin disorders. Disruption of skin diﬀerentiation is most often caused by changes
in gene regulation, as shown in studies on transcription factors and their roles in skin function and develop-
ment. Additionally, accurate genetic models of primary keratinocytes (both in vitro and in vivo) have proven
to be powerful tools in understanding regulation by transcription factors and non-coding RNAs. Further-
more, numerous genomic time series datasets have been generated with primary keratinocytes. The dynamic
nature of epidermal homeostasis, biologically relevant model systems, and availability of genomic datasets
makes keratinocyte diﬀerentiation an ideal process to understand enhancer elements and the underlying
sequence diﬀerences.

Materials and Methods

Datasets We collected histone ChIP-seq (H3K4me1, H3K4me3, H3K27me3, H3K27ac, H3K9ac) and
ATAC-seq for 3 time points in skin diﬀerentiation (days 0, 3, and 6). We pre-process the data so that
we determine enhancers and their activity levels at each time point. Then, we convert the sequence in
each enhancer region into counts of k-mers, which are short DNA sequences of length k (for k=2, sequences
include AA, AC, AG, AT...TT). After pre-processing, the rows are examples of enhancers and the columns
are the k-mer features (depending on k, there are 4k features, since there are 4 bases). The examples are
labeled with which enhancer activity pattern it belongs to.

Pre-processing: bifurcation analysis for labeling To get enhancer elements, we ﬁrst utilize a hidden
Markov Model to combine histone mark data and output chromatin ’states’ across the genome (ChromHMM[1]).
We also call peaks from ATAC-seq data, which marks regions of open chromatin. We then intersect the two
set of regions obtained to get a consensus set of regions. We then utilize the ATAC-seq sequencing reads
found in those regions and look for diﬀerential peaks utilizing DESeq. Thus, peaks from one time point
to another can increase (1), decrease (-1), or stay the same (0). With three timepoints and two pairs of
comparisons that can be made across time, this allows us to segment the peaks into 9 diﬀerent peak patterns,
which constitute our 9 classes for prediction. We then label each group in order (increase/increase is group
1, increase/same is group 2, increase/decrease is group 3, and so on, as shown in Table 1).

1

CS229 Final Project: Multi-class motif discovery in

keratinocyte diﬀerentiation

Daniel Kim

December 12, 2014

Introduction

Enhancer elements are short segments of regulatory DNA that play important roles in activating gene ex-
pression within dynamic biological processes like cell diﬀerentiation. These enhancer regions are often bound
by activating proteins known as transcription factors (TFs), and the DNA sites bound by TFs often have a
sequence pattern, known as a ’motif’, that can be matched to individual TFs. With over 2 million putative
enhancer regions in the human genome, understanding enhancer function is essential to understanding dy-
namic gene regulation. As such, discovering regulatory sequence motifs within enhancer regions is an area of
continuing research. Previous methods have included the use of discovered motifs and looking for matches
within enhancer regions, as well as machine learning techniques (logistic regression and SVMs) among others
[2]. However, such methods have only compared positive examples to negative examples, in a binary-class
format. Here, we extend machine learning techniques to discover sequence features in a multi- class format,
which is useful in situations where enhancers have varying activity patterns and we are interested in the
underlying sequence features that are leading to these multiple patterns. We use the skin diﬀerentiation
process as our test case.

Skin diﬀerentiation, the transition from skin progenitor cell to diﬀerentiated keratinocyte cell, is a process
disrupted in skin cancer (squamous cell carcinoma and basal cell carcinoma), psoriasis, chronic wounds, and
nearly 100 inherited human skin disorders. Disruption of skin diﬀerentiation is most often caused by changes
in gene regulation, as shown in studies on transcription factors and their roles in skin function and develop-
ment. Additionally, accurate genetic models of primary keratinocytes (both in vitro and in vivo) have proven
to be powerful tools in understanding regulation by transcription factors and non-coding RNAs. Further-
more, numerous genomic time series datasets have been generated with primary keratinocytes. The dynamic
nature of epidermal homeostasis, biologically relevant model systems, and availability of genomic datasets
makes keratinocyte diﬀerentiation an ideal process to understand enhancer elements and the underlying
sequence diﬀerences.

Materials and Methods

Datasets We collected histone ChIP-seq (H3K4me1, H3K4me3, H3K27me3, H3K27ac, H3K9ac) and
ATAC-seq for 3 time points in skin diﬀerentiation (days 0, 3, and 6). We pre-process the data so that
we determine enhancers and their activity levels at each time point. Then, we convert the sequence in
each enhancer region into counts of k-mers, which are short DNA sequences of length k (for k=2, sequences
include AA, AC, AG, AT...TT). After pre-processing, the rows are examples of enhancers and the columns
are the k-mer features (depending on k, there are 4k features, since there are 4 bases). The examples are
labeled with which enhancer activity pattern it belongs to.

Pre-processing: bifurcation analysis for labeling To get enhancer elements, we ﬁrst utilize a hidden
Markov Model to combine histone mark data and output chromatin ’states’ across the genome (ChromHMM[1]).
We also call peaks from ATAC-seq data, which marks regions of open chromatin. We then intersect the two
set of regions obtained to get a consensus set of regions. We then utilize the ATAC-seq sequencing reads
found in those regions and look for diﬀerential peaks utilizing DESeq. Thus, peaks from one time point
to another can increase (1), decrease (-1), or stay the same (0). With three timepoints and two pairs of
comparisons that can be made across time, this allows us to segment the peaks into 9 diﬀerent peak patterns,
which constitute our 9 classes for prediction. We then label each group in order (increase/increase is group
1, increase/same is group 2, increase/decrease is group 3, and so on, as shown in Table 1).

1

Table 1: Number of samples in each class

Group Day 0 to Day 3 Day 3 to Day 6 Number of samples
1
2
3
4
5
6
7
8
9

Up
No change
Down
Up
No change
Down
Up
No change
Down

Up
Up
Up
No change
No change
No change
Down
Down
Down

704
5078
996
5592
41725
5265
1002
5158
738

Multinomial logistic regression We take the sequence data and convert into k-mers (lengths of 2-4)
and track the counts of each kmer found in the sequence. For each length k, we have 4k features. We then
run logistic regression with lasso penalty (R package: glmnet[3]), which runs multinomial logistic regression
based on the equations below:

P r(G = (cid:96)|x) =

(cid:34)

1
N

{β0(cid:96),β(cid:96)}K

max
1 RK(p+1)

eβ0(cid:96)+xT β(cid:96)
k=1 eβ0k+xT βk

(cid:80)K
log pgi(xi) − λ

K(cid:88)

N(cid:88)

i=1

(cid:96)=1

(cid:35)

Pα(β(cid:96))

(1)

(2)

Succinctly, the model learns a logistic regression classiﬁer for each class, and then takes the highest
probability outcome as the predicted class. The lasso penalty is used to reduce weights on k-mer features
that are relatively insigniﬁcant.

Multi-class SVM We use the spectrum kernel, which converts sequence into k-mers (lengths of 2-10). For
each length k, we have 4k features. We then run multi-class SVM with L2 regularization and L1 loss with
the one-vs-the-rest strategy (R package: KeBABS[4]), which runs the SVM based on the equation below:

min

w

1
2

wT w + C

l(cid:88)

i=1

ξ(w; xi, yi)

(3)

Succinctly, the SVM ﬁrst uses the kernel to quickly compute the k-mer feature values without having to
maintain the k-mer feature space, then it runs an SVM that takes a single class as a positive class and the
rest as negative classes.

Results

Multinomial logistic regression Running multinomial logistic regression, we ﬁnd that error rates (de-
ﬁned as all misclassiﬁed examples) are high and remain very high, despite increasing the training set size and
the k-mer length (Figures 1-3). However, there is enough of a downward trend that it is possible that more
training data could be useful in decreasing the error rate. On average, we get errors around 0.8-0.9. However,
we do ﬁnd that the weights learned in logistic regression for the diﬀerent k-mer features are heterogenous
across the nine classes, suggesting that there are sequence diﬀerences across the classes that were learned by
multinomial regression (Figure 4).

2

CS229 Final Project: Multi-class motif discovery in

keratinocyte diﬀerentiation

Daniel Kim

December 12, 2014

Introduction

Enhancer elements are short segments of regulatory DNA that play important roles in activating gene ex-
pression within dynamic biological processes like cell diﬀerentiation. These enhancer regions are often bound
by activating proteins known as transcription factors (TFs), and the DNA sites bound by TFs often have a
sequence pattern, known as a ’motif’, that can be matched to individual TFs. With over 2 million putative
enhancer regions in the human genome, understanding enhancer function is essential to understanding dy-
namic gene regulation. As such, discovering regulatory sequence motifs within enhancer regions is an area of
continuing research. Previous methods have included the use of discovered motifs and looking for matches
within enhancer regions, as well as machine learning techniques (logistic regression and SVMs) among others
[2]. However, such methods have only compared positive examples to negative examples, in a binary-class
format. Here, we extend machine learning techniques to discover sequence features in a multi- class format,
which is useful in situations where enhancers have varying activity patterns and we are interested in the
underlying sequence features that are leading to these multiple patterns. We use the skin diﬀerentiation
process as our test case.

Skin diﬀerentiation, the transition from skin progenitor cell to diﬀerentiated keratinocyte cell, is a process
disrupted in skin cancer (squamous cell carcinoma and basal cell carcinoma), psoriasis, chronic wounds, and
nearly 100 inherited human skin disorders. Disruption of skin diﬀerentiation is most often caused by changes
in gene regulation, as shown in studies on transcription factors and their roles in skin function and develop-
ment. Additionally, accurate genetic models of primary keratinocytes (both in vitro and in vivo) have proven
to be powerful tools in understanding regulation by transcription factors and non-coding RNAs. Further-
more, numerous genomic time series datasets have been generated with primary keratinocytes. The dynamic
nature of epidermal homeostasis, biologically relevant model systems, and availability of genomic datasets
makes keratinocyte diﬀerentiation an ideal process to understand enhancer elements and the underlying
sequence diﬀerences.

Materials and Methods

Datasets We collected histone ChIP-seq (H3K4me1, H3K4me3, H3K27me3, H3K27ac, H3K9ac) and
ATAC-seq for 3 time points in skin diﬀerentiation (days 0, 3, and 6). We pre-process the data so that
we determine enhancers and their activity levels at each time point. Then, we convert the sequence in
each enhancer region into counts of k-mers, which are short DNA sequences of length k (for k=2, sequences
include AA, AC, AG, AT...TT). After pre-processing, the rows are examples of enhancers and the columns
are the k-mer features (depending on k, there are 4k features, since there are 4 bases). The examples are
labeled with which enhancer activity pattern it belongs to.

Pre-processing: bifurcation analysis for labeling To get enhancer elements, we ﬁrst utilize a hidden
Markov Model to combine histone mark data and output chromatin ’states’ across the genome (ChromHMM[1]).
We also call peaks from ATAC-seq data, which marks regions of open chromatin. We then intersect the two
set of regions obtained to get a consensus set of regions. We then utilize the ATAC-seq sequencing reads
found in those regions and look for diﬀerential peaks utilizing DESeq. Thus, peaks from one time point
to another can increase (1), decrease (-1), or stay the same (0). With three timepoints and two pairs of
comparisons that can be made across time, this allows us to segment the peaks into 9 diﬀerent peak patterns,
which constitute our 9 classes for prediction. We then label each group in order (increase/increase is group
1, increase/same is group 2, increase/decrease is group 3, and so on, as shown in Table 1).

1

Table 1: Number of samples in each class

Group Day 0 to Day 3 Day 3 to Day 6 Number of samples
1
2
3
4
5
6
7
8
9

Up
No change
Down
Up
No change
Down
Up
No change
Down

Up
Up
Up
No change
No change
No change
Down
Down
Down

704
5078
996
5592
41725
5265
1002
5158
738

Multinomial logistic regression We take the sequence data and convert into k-mers (lengths of 2-4)
and track the counts of each kmer found in the sequence. For each length k, we have 4k features. We then
run logistic regression with lasso penalty (R package: glmnet[3]), which runs multinomial logistic regression
based on the equations below:

P r(G = (cid:96)|x) =

(cid:34)

1
N

{β0(cid:96),β(cid:96)}K

max
1 RK(p+1)

eβ0(cid:96)+xT β(cid:96)
k=1 eβ0k+xT βk

(cid:80)K
log pgi(xi) − λ

K(cid:88)

N(cid:88)

i=1

(cid:96)=1

(cid:35)

Pα(β(cid:96))

(1)

(2)

Succinctly, the model learns a logistic regression classiﬁer for each class, and then takes the highest
probability outcome as the predicted class. The lasso penalty is used to reduce weights on k-mer features
that are relatively insigniﬁcant.

Multi-class SVM We use the spectrum kernel, which converts sequence into k-mers (lengths of 2-10). For
each length k, we have 4k features. We then run multi-class SVM with L2 regularization and L1 loss with
the one-vs-the-rest strategy (R package: KeBABS[4]), which runs the SVM based on the equation below:

min

w

1
2

wT w + C

l(cid:88)

i=1

ξ(w; xi, yi)

(3)

Succinctly, the SVM ﬁrst uses the kernel to quickly compute the k-mer feature values without having to
maintain the k-mer feature space, then it runs an SVM that takes a single class as a positive class and the
rest as negative classes.

Results

Multinomial logistic regression Running multinomial logistic regression, we ﬁnd that error rates (de-
ﬁned as all misclassiﬁed examples) are high and remain very high, despite increasing the training set size and
the k-mer length (Figures 1-3). However, there is enough of a downward trend that it is possible that more
training data could be useful in decreasing the error rate. On average, we get errors around 0.8-0.9. However,
we do ﬁnd that the weights learned in logistic regression for the diﬀerent k-mer features are heterogenous
across the nine classes, suggesting that there are sequence diﬀerences across the classes that were learned by
multinomial regression (Figure 4).

2

Figure 1:
multinomial
(kmer=2)

Error analysis of
logistic regression

Figure 2:
multinomial
(kmer=3)

Error analysis of
logistic regression

Figure 3:
multinomial
(kmer=4)

Error analysis of
logistic regression

Figure 4: Heatmap of learned weights for the multi-class logistic regression using kmer=2.

Multi-class support vector machines Running multi-class SVMs, we ﬁnd that the error rates are high
and remain high, despite changing the k-mer length used and increasing the training set size (Figures 5-10).
However, we again ﬁnd that the weights learned by the SVM are heterogenous across the classes (Figure 11).

Figure 5: Error analysis of
multi-class SVM (kmer=2)

Figure 6: Error analysis of
multi-class SVM (kmer=3)

Figure 7: Error analysis of
multi-class SVM (kmer=4)

3

0.810.830.8501000020000Training Set Size (m)ErrorgrouptrainingtestLogistic Regression Error Analysis (k−mer: 2)0.820.840.8601000020000Training Set Size (m)ErrorgrouptrainingtestLogistic Regression Error Analysis (k−mer: 3)0.8250.8500.8750.90001000020000Training Set Size (m)ErrorgrouptrainingtestLogistic Regression Error Analysis (k−mer: 4)AAACAGATCACCCGCTGAGCGGGTTATCTGTT987654321−0.0200.010.02Value0Color Keyand HistogramCount0.840.860.880.90500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 2)0.8000.8250.8500.8750.900500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 3)0.40.60.8500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 4)CS229 Final Project: Multi-class motif discovery in

keratinocyte diﬀerentiation

Daniel Kim

December 12, 2014

Introduction

Enhancer elements are short segments of regulatory DNA that play important roles in activating gene ex-
pression within dynamic biological processes like cell diﬀerentiation. These enhancer regions are often bound
by activating proteins known as transcription factors (TFs), and the DNA sites bound by TFs often have a
sequence pattern, known as a ’motif’, that can be matched to individual TFs. With over 2 million putative
enhancer regions in the human genome, understanding enhancer function is essential to understanding dy-
namic gene regulation. As such, discovering regulatory sequence motifs within enhancer regions is an area of
continuing research. Previous methods have included the use of discovered motifs and looking for matches
within enhancer regions, as well as machine learning techniques (logistic regression and SVMs) among others
[2]. However, such methods have only compared positive examples to negative examples, in a binary-class
format. Here, we extend machine learning techniques to discover sequence features in a multi- class format,
which is useful in situations where enhancers have varying activity patterns and we are interested in the
underlying sequence features that are leading to these multiple patterns. We use the skin diﬀerentiation
process as our test case.

Skin diﬀerentiation, the transition from skin progenitor cell to diﬀerentiated keratinocyte cell, is a process
disrupted in skin cancer (squamous cell carcinoma and basal cell carcinoma), psoriasis, chronic wounds, and
nearly 100 inherited human skin disorders. Disruption of skin diﬀerentiation is most often caused by changes
in gene regulation, as shown in studies on transcription factors and their roles in skin function and develop-
ment. Additionally, accurate genetic models of primary keratinocytes (both in vitro and in vivo) have proven
to be powerful tools in understanding regulation by transcription factors and non-coding RNAs. Further-
more, numerous genomic time series datasets have been generated with primary keratinocytes. The dynamic
nature of epidermal homeostasis, biologically relevant model systems, and availability of genomic datasets
makes keratinocyte diﬀerentiation an ideal process to understand enhancer elements and the underlying
sequence diﬀerences.

Materials and Methods

Datasets We collected histone ChIP-seq (H3K4me1, H3K4me3, H3K27me3, H3K27ac, H3K9ac) and
ATAC-seq for 3 time points in skin diﬀerentiation (days 0, 3, and 6). We pre-process the data so that
we determine enhancers and their activity levels at each time point. Then, we convert the sequence in
each enhancer region into counts of k-mers, which are short DNA sequences of length k (for k=2, sequences
include AA, AC, AG, AT...TT). After pre-processing, the rows are examples of enhancers and the columns
are the k-mer features (depending on k, there are 4k features, since there are 4 bases). The examples are
labeled with which enhancer activity pattern it belongs to.

Pre-processing: bifurcation analysis for labeling To get enhancer elements, we ﬁrst utilize a hidden
Markov Model to combine histone mark data and output chromatin ’states’ across the genome (ChromHMM[1]).
We also call peaks from ATAC-seq data, which marks regions of open chromatin. We then intersect the two
set of regions obtained to get a consensus set of regions. We then utilize the ATAC-seq sequencing reads
found in those regions and look for diﬀerential peaks utilizing DESeq. Thus, peaks from one time point
to another can increase (1), decrease (-1), or stay the same (0). With three timepoints and two pairs of
comparisons that can be made across time, this allows us to segment the peaks into 9 diﬀerent peak patterns,
which constitute our 9 classes for prediction. We then label each group in order (increase/increase is group
1, increase/same is group 2, increase/decrease is group 3, and so on, as shown in Table 1).

1

Table 1: Number of samples in each class

Group Day 0 to Day 3 Day 3 to Day 6 Number of samples
1
2
3
4
5
6
7
8
9

Up
No change
Down
Up
No change
Down
Up
No change
Down

Up
Up
Up
No change
No change
No change
Down
Down
Down

704
5078
996
5592
41725
5265
1002
5158
738

Multinomial logistic regression We take the sequence data and convert into k-mers (lengths of 2-4)
and track the counts of each kmer found in the sequence. For each length k, we have 4k features. We then
run logistic regression with lasso penalty (R package: glmnet[3]), which runs multinomial logistic regression
based on the equations below:

P r(G = (cid:96)|x) =

(cid:34)

1
N

{β0(cid:96),β(cid:96)}K

max
1 RK(p+1)

eβ0(cid:96)+xT β(cid:96)
k=1 eβ0k+xT βk

(cid:80)K
log pgi(xi) − λ

K(cid:88)

N(cid:88)

i=1

(cid:96)=1

(cid:35)

Pα(β(cid:96))

(1)

(2)

Succinctly, the model learns a logistic regression classiﬁer for each class, and then takes the highest
probability outcome as the predicted class. The lasso penalty is used to reduce weights on k-mer features
that are relatively insigniﬁcant.

Multi-class SVM We use the spectrum kernel, which converts sequence into k-mers (lengths of 2-10). For
each length k, we have 4k features. We then run multi-class SVM with L2 regularization and L1 loss with
the one-vs-the-rest strategy (R package: KeBABS[4]), which runs the SVM based on the equation below:

min

w

1
2

wT w + C

l(cid:88)

i=1

ξ(w; xi, yi)

(3)

Succinctly, the SVM ﬁrst uses the kernel to quickly compute the k-mer feature values without having to
maintain the k-mer feature space, then it runs an SVM that takes a single class as a positive class and the
rest as negative classes.

Results

Multinomial logistic regression Running multinomial logistic regression, we ﬁnd that error rates (de-
ﬁned as all misclassiﬁed examples) are high and remain very high, despite increasing the training set size and
the k-mer length (Figures 1-3). However, there is enough of a downward trend that it is possible that more
training data could be useful in decreasing the error rate. On average, we get errors around 0.8-0.9. However,
we do ﬁnd that the weights learned in logistic regression for the diﬀerent k-mer features are heterogenous
across the nine classes, suggesting that there are sequence diﬀerences across the classes that were learned by
multinomial regression (Figure 4).

2

Figure 1:
multinomial
(kmer=2)

Error analysis of
logistic regression

Figure 2:
multinomial
(kmer=3)

Error analysis of
logistic regression

Figure 3:
multinomial
(kmer=4)

Error analysis of
logistic regression

Figure 4: Heatmap of learned weights for the multi-class logistic regression using kmer=2.

Multi-class support vector machines Running multi-class SVMs, we ﬁnd that the error rates are high
and remain high, despite changing the k-mer length used and increasing the training set size (Figures 5-10).
However, we again ﬁnd that the weights learned by the SVM are heterogenous across the classes (Figure 11).

Figure 5: Error analysis of
multi-class SVM (kmer=2)

Figure 6: Error analysis of
multi-class SVM (kmer=3)

Figure 7: Error analysis of
multi-class SVM (kmer=4)

3

0.810.830.8501000020000Training Set Size (m)ErrorgrouptrainingtestLogistic Regression Error Analysis (k−mer: 2)0.820.840.8601000020000Training Set Size (m)ErrorgrouptrainingtestLogistic Regression Error Analysis (k−mer: 3)0.8250.8500.8750.90001000020000Training Set Size (m)ErrorgrouptrainingtestLogistic Regression Error Analysis (k−mer: 4)AAACAGATCACCCGCTGAGCGGGTTATCTGTT987654321−0.0200.010.02Value0Color Keyand HistogramCount0.840.860.880.90500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 2)0.8000.8250.8500.8750.900500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 3)0.40.60.8500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 4)Figure 8: Error analysis of
multi-class SVM (kmer=5)

Figure 9: Error analysis of
multi-class SVM (kmer=6)

Figure 10: Error analysis of
multi-class SVM (kmer=7)

Figure 11: Heatmap of learned weights for the multi-class logistic regression using kmer=2.

Discussion

Error analysis suggests that more data could be useful for multinomial logistic regression but not for multi-
class SVM. Notably, as the k-mer length increased for SVMs, we ﬁnd signiﬁcant over ﬁtting - the training
error drops to 0, while the test error stays around 0.80. However, there are only so many enhancers that
are biologically active in keratinocyte diﬀerentiation, and so generating more examples of enhancers is not
possible. We do note that there could be quality issues in the data, as the read depths of sequencing of the
ATAC-seq datasets were low - increasing the sequencing levels could lead to better classes of data that lead
to better classiﬁcation and k-mer weights.

We do ﬁnd that there are diﬀerent weights on the k-mers for the diﬀerent classes. This suggests that
there are underlying sequence features that do lead to diﬀerent enhancer activity patterns. However, the
high classiﬁcation error rate suggests that there are very few sequences in each group that actually have
these sequence features (and thus get classiﬁed correctly in the test set). As such, further segmentation of
the classes is likely necessary to actually have better predictions.

This problem of signiﬁcant heterogeneity within the classes is demonstrated in Figure 12, where k-means
clustering is performed on one class of enhancers. Using a k of 10, we see signiﬁcant heterogeneity leading
to at least 10 distinct clusters of sequence with diﬀering numbers of each k-mer. Seeing this heterogeneity,
further segmentation is necessary in each class to produce classiﬁcation that is representative of the entire
group of sequences in a class.

The high error rates seen in both multi-class learning techniques is somewhat expected, given the biolog-
ical context. When considering the original segmentation of the classes (based on enhancer activity in three
time points), it is highly likely that there are many diﬀerent mechanisms at work in causing various enhancers
to change activity patterns. In other words, there are many transcription factors and many combinations of
transcription factors that are acting on groups of enhancers to change their behavior. As such, it is likely
that many enhancers in a group may have very diﬀerent motifs, despite acting in the same manner across
time.

4

0.000.250.500.75500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 5)0.000.250.500.75500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 6)0.000.250.500.75500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 7)AAACAGATCACCCGCTGAGCGGGTTATCTGTT987654321−0.0100.01Value0Color Keyand HistogramCountCS229 Final Project: Multi-class motif discovery in

keratinocyte diﬀerentiation

Daniel Kim

December 12, 2014

Introduction

Enhancer elements are short segments of regulatory DNA that play important roles in activating gene ex-
pression within dynamic biological processes like cell diﬀerentiation. These enhancer regions are often bound
by activating proteins known as transcription factors (TFs), and the DNA sites bound by TFs often have a
sequence pattern, known as a ’motif’, that can be matched to individual TFs. With over 2 million putative
enhancer regions in the human genome, understanding enhancer function is essential to understanding dy-
namic gene regulation. As such, discovering regulatory sequence motifs within enhancer regions is an area of
continuing research. Previous methods have included the use of discovered motifs and looking for matches
within enhancer regions, as well as machine learning techniques (logistic regression and SVMs) among others
[2]. However, such methods have only compared positive examples to negative examples, in a binary-class
format. Here, we extend machine learning techniques to discover sequence features in a multi- class format,
which is useful in situations where enhancers have varying activity patterns and we are interested in the
underlying sequence features that are leading to these multiple patterns. We use the skin diﬀerentiation
process as our test case.

Skin diﬀerentiation, the transition from skin progenitor cell to diﬀerentiated keratinocyte cell, is a process
disrupted in skin cancer (squamous cell carcinoma and basal cell carcinoma), psoriasis, chronic wounds, and
nearly 100 inherited human skin disorders. Disruption of skin diﬀerentiation is most often caused by changes
in gene regulation, as shown in studies on transcription factors and their roles in skin function and develop-
ment. Additionally, accurate genetic models of primary keratinocytes (both in vitro and in vivo) have proven
to be powerful tools in understanding regulation by transcription factors and non-coding RNAs. Further-
more, numerous genomic time series datasets have been generated with primary keratinocytes. The dynamic
nature of epidermal homeostasis, biologically relevant model systems, and availability of genomic datasets
makes keratinocyte diﬀerentiation an ideal process to understand enhancer elements and the underlying
sequence diﬀerences.

Materials and Methods

Datasets We collected histone ChIP-seq (H3K4me1, H3K4me3, H3K27me3, H3K27ac, H3K9ac) and
ATAC-seq for 3 time points in skin diﬀerentiation (days 0, 3, and 6). We pre-process the data so that
we determine enhancers and their activity levels at each time point. Then, we convert the sequence in
each enhancer region into counts of k-mers, which are short DNA sequences of length k (for k=2, sequences
include AA, AC, AG, AT...TT). After pre-processing, the rows are examples of enhancers and the columns
are the k-mer features (depending on k, there are 4k features, since there are 4 bases). The examples are
labeled with which enhancer activity pattern it belongs to.

Pre-processing: bifurcation analysis for labeling To get enhancer elements, we ﬁrst utilize a hidden
Markov Model to combine histone mark data and output chromatin ’states’ across the genome (ChromHMM[1]).
We also call peaks from ATAC-seq data, which marks regions of open chromatin. We then intersect the two
set of regions obtained to get a consensus set of regions. We then utilize the ATAC-seq sequencing reads
found in those regions and look for diﬀerential peaks utilizing DESeq. Thus, peaks from one time point
to another can increase (1), decrease (-1), or stay the same (0). With three timepoints and two pairs of
comparisons that can be made across time, this allows us to segment the peaks into 9 diﬀerent peak patterns,
which constitute our 9 classes for prediction. We then label each group in order (increase/increase is group
1, increase/same is group 2, increase/decrease is group 3, and so on, as shown in Table 1).

1

Table 1: Number of samples in each class

Group Day 0 to Day 3 Day 3 to Day 6 Number of samples
1
2
3
4
5
6
7
8
9

Up
No change
Down
Up
No change
Down
Up
No change
Down

Up
Up
Up
No change
No change
No change
Down
Down
Down

704
5078
996
5592
41725
5265
1002
5158
738

Multinomial logistic regression We take the sequence data and convert into k-mers (lengths of 2-4)
and track the counts of each kmer found in the sequence. For each length k, we have 4k features. We then
run logistic regression with lasso penalty (R package: glmnet[3]), which runs multinomial logistic regression
based on the equations below:

P r(G = (cid:96)|x) =

(cid:34)

1
N

{β0(cid:96),β(cid:96)}K

max
1 RK(p+1)

eβ0(cid:96)+xT β(cid:96)
k=1 eβ0k+xT βk

(cid:80)K
log pgi(xi) − λ

K(cid:88)

N(cid:88)

i=1

(cid:96)=1

(cid:35)

Pα(β(cid:96))

(1)

(2)

Succinctly, the model learns a logistic regression classiﬁer for each class, and then takes the highest
probability outcome as the predicted class. The lasso penalty is used to reduce weights on k-mer features
that are relatively insigniﬁcant.

Multi-class SVM We use the spectrum kernel, which converts sequence into k-mers (lengths of 2-10). For
each length k, we have 4k features. We then run multi-class SVM with L2 regularization and L1 loss with
the one-vs-the-rest strategy (R package: KeBABS[4]), which runs the SVM based on the equation below:

min

w

1
2

wT w + C

l(cid:88)

i=1

ξ(w; xi, yi)

(3)

Succinctly, the SVM ﬁrst uses the kernel to quickly compute the k-mer feature values without having to
maintain the k-mer feature space, then it runs an SVM that takes a single class as a positive class and the
rest as negative classes.

Results

Multinomial logistic regression Running multinomial logistic regression, we ﬁnd that error rates (de-
ﬁned as all misclassiﬁed examples) are high and remain very high, despite increasing the training set size and
the k-mer length (Figures 1-3). However, there is enough of a downward trend that it is possible that more
training data could be useful in decreasing the error rate. On average, we get errors around 0.8-0.9. However,
we do ﬁnd that the weights learned in logistic regression for the diﬀerent k-mer features are heterogenous
across the nine classes, suggesting that there are sequence diﬀerences across the classes that were learned by
multinomial regression (Figure 4).

2

Figure 1:
multinomial
(kmer=2)

Error analysis of
logistic regression

Figure 2:
multinomial
(kmer=3)

Error analysis of
logistic regression

Figure 3:
multinomial
(kmer=4)

Error analysis of
logistic regression

Figure 4: Heatmap of learned weights for the multi-class logistic regression using kmer=2.

Multi-class support vector machines Running multi-class SVMs, we ﬁnd that the error rates are high
and remain high, despite changing the k-mer length used and increasing the training set size (Figures 5-10).
However, we again ﬁnd that the weights learned by the SVM are heterogenous across the classes (Figure 11).

Figure 5: Error analysis of
multi-class SVM (kmer=2)

Figure 6: Error analysis of
multi-class SVM (kmer=3)

Figure 7: Error analysis of
multi-class SVM (kmer=4)

3

0.810.830.8501000020000Training Set Size (m)ErrorgrouptrainingtestLogistic Regression Error Analysis (k−mer: 2)0.820.840.8601000020000Training Set Size (m)ErrorgrouptrainingtestLogistic Regression Error Analysis (k−mer: 3)0.8250.8500.8750.90001000020000Training Set Size (m)ErrorgrouptrainingtestLogistic Regression Error Analysis (k−mer: 4)AAACAGATCACCCGCTGAGCGGGTTATCTGTT987654321−0.0200.010.02Value0Color Keyand HistogramCount0.840.860.880.90500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 2)0.8000.8250.8500.8750.900500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 3)0.40.60.8500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 4)Figure 8: Error analysis of
multi-class SVM (kmer=5)

Figure 9: Error analysis of
multi-class SVM (kmer=6)

Figure 10: Error analysis of
multi-class SVM (kmer=7)

Figure 11: Heatmap of learned weights for the multi-class logistic regression using kmer=2.

Discussion

Error analysis suggests that more data could be useful for multinomial logistic regression but not for multi-
class SVM. Notably, as the k-mer length increased for SVMs, we ﬁnd signiﬁcant over ﬁtting - the training
error drops to 0, while the test error stays around 0.80. However, there are only so many enhancers that
are biologically active in keratinocyte diﬀerentiation, and so generating more examples of enhancers is not
possible. We do note that there could be quality issues in the data, as the read depths of sequencing of the
ATAC-seq datasets were low - increasing the sequencing levels could lead to better classes of data that lead
to better classiﬁcation and k-mer weights.

We do ﬁnd that there are diﬀerent weights on the k-mers for the diﬀerent classes. This suggests that
there are underlying sequence features that do lead to diﬀerent enhancer activity patterns. However, the
high classiﬁcation error rate suggests that there are very few sequences in each group that actually have
these sequence features (and thus get classiﬁed correctly in the test set). As such, further segmentation of
the classes is likely necessary to actually have better predictions.

This problem of signiﬁcant heterogeneity within the classes is demonstrated in Figure 12, where k-means
clustering is performed on one class of enhancers. Using a k of 10, we see signiﬁcant heterogeneity leading
to at least 10 distinct clusters of sequence with diﬀering numbers of each k-mer. Seeing this heterogeneity,
further segmentation is necessary in each class to produce classiﬁcation that is representative of the entire
group of sequences in a class.

The high error rates seen in both multi-class learning techniques is somewhat expected, given the biolog-
ical context. When considering the original segmentation of the classes (based on enhancer activity in three
time points), it is highly likely that there are many diﬀerent mechanisms at work in causing various enhancers
to change activity patterns. In other words, there are many transcription factors and many combinations of
transcription factors that are acting on groups of enhancers to change their behavior. As such, it is likely
that many enhancers in a group may have very diﬀerent motifs, despite acting in the same manner across
time.

4

0.000.250.500.75500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 5)0.000.250.500.75500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 6)0.000.250.500.75500010000Training Set Size (m)ErrorgrouptrainingtestSVM Error Analysis (k−mer: 7)AAACAGATCACCCGCTGAGCGGGTTATCTGTT987654321−0.0100.01Value0Color Keyand HistogramCountFigure 12: Enhancers from group 1 clustered into 10 groups using k-means clustering. There is notable
sequence heterogeneity in the class.

Future Directions

Logistic regression was not performed on higher order k-mers due to computational constraints. Utilizing a
kernel within logistic regression could be valuable and very interpretable. Additionally, a variety of other
sequence-based kernels exist that consider mismatches and gaps. These are more biologically accurate kernels
and may ﬁt a better model. Noting the high error rate and the likelihood that biological heterogeneity plays a
role in the error rate, segmenting the sequences further may also act as a ﬁlter for creating more homogenous
classes. Finally, directly checking for motif instances within the regions may also provide important features.

Acknowledgments

Thank you to Andrew Ng and the TA’s who put on a wonderful class!

References

[1] Ernst, J., & Kellis, M. (2012). ChromHMM: automating chromatin-state discovery and characterization.

Nature Methods, 9(3), 215-6. doi:10.1038/nmeth.1906

[2] Heinz, S., Benner, C., Spann, N., Bertolino, E., Lin, Y. C., Laslo, P., ... Glass, C. K. (2010). Simple
combinations of lineage-determining transcription factors prime cis- regulatory elements required for
macrophage and B cell identities. Molecular Cell, 38(4), 576-89. doi:10.1016/j.molcel.2010.05.004

[3] Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for General-
ized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL
http://www.jstatsoft.org/v33/i01/

[4] J. Palme and U.Bodenhofer (2014). KeBABS - An R Package for Kernel Based Analysis of Biological

Sequences.Unpublished. http://www.bioconductor.org/packages/ release/html/kebabs.html

[5] The Spectrum Kernel: A String Kernel for SVM Protein Classiﬁcation. C. Leslie, E., Eskin, W. S.

Noble. Proceedings of the Paciﬁc Symposium on Biocomputing (PSB 2002), 2002.

5

CGTTAAGTACTAATCCCATCCTGCGGTGGAAG00.20.40.6Value010002000Color Keyand HistogramCount