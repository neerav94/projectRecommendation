Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	 Â 	 Â Triage	 Â Task:	 Â Based	 Â on	 Â its	 Â abstract,	 Â can	 Â we	 Â determine	 Â if	 Â a	 Â new	 Â paper	 Â contains	 Â 	 Â 
relevant	 Â information?	 Â This	 Â is	 Â a	 Â binary	 Â classification	 Â task	 Â where	 Â a	 Â label	 Â of	 Â 1	 Â 
indicates	 Â that	 Â a	 Â paper	 Â contains	 Â relevant	 Â information	 Â and	 Â a	 Â label	 Â of	 Â -Â­â€1	 Â indicates	 Â 
that	 Â a	 Â paper	 Â does	 Â not	 Â contain	 Â relevant	 Â information.	 Â 
2)	 Â 	 Â Multi-Â­â€label	 Â Classification	 Â Task:	 Â Assuming	 Â a	 Â paper	 Â has	 Â relevant	 Â content,	 Â 
based	 Â on	 Â its	 Â fulltext,	 Â can	 Â we	 Â determine	 Â which	 Â biological	 Â category	 Â (or	 Â categories)	 Â 
can	 Â be	 Â associated	 Â with	 Â it?	 Â Since	 Â each	 Â paper	 Â can	 Â be	 Â associated	 Â with	 Â multiple	 Â 
biological	 Â categories,	 Â this	 Â is	 Â not	 Â a	 Â multi-Â­â€class	 Â classification	 Â task,	 Â but	 Â rather	 Â a	 Â 
multi-Â­â€label	 Â classification	 Â task.	 Â Given	 Â k	 Â categories,	 Â for	 Â each	 Â paper	 Â we	 Â would	 Â like	 Â 
to	 Â predict	 Â a	 Â binary	 Â vector	 Â of	 Â length	 Â k	 Â indicating	 Â which	 Â biological	 Â categories	 Â the	 Â 
paper	 Â is	 Â associated	 Â with.	 Â 

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	 Â 

	 Â 
f
o
s
n
o
i
l
l
i

M

 

	 Â 

s
d
r
o
W

Features 

Dictionary	 Â Size	 Â 

Abstract	 Â 
Fulltext	 Â 

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	 Â 1	 Â 2	 Â 3	 Â 4	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â bigram	 Â feature	 Â set	 Â contains	 Â both	 Â unigrams	 Â and	 Â bigrams,	 Â while	 Â the	 Â trigram	 Â feature	 Â set	 Â contains	 Â 

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	 Â 1:	 Â Feature	 Â set	 Â size.	 Â 

Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	 Â 	 Â Triage	 Â Task:	 Â Based	 Â on	 Â its	 Â abstract,	 Â can	 Â we	 Â determine	 Â if	 Â a	 Â new	 Â paper	 Â contains	 Â 	 Â 
relevant	 Â information?	 Â This	 Â is	 Â a	 Â binary	 Â classification	 Â task	 Â where	 Â a	 Â label	 Â of	 Â 1	 Â 
indicates	 Â that	 Â a	 Â paper	 Â contains	 Â relevant	 Â information	 Â and	 Â a	 Â label	 Â of	 Â -Â­â€1	 Â indicates	 Â 
that	 Â a	 Â paper	 Â does	 Â not	 Â contain	 Â relevant	 Â information.	 Â 
2)	 Â 	 Â Multi-Â­â€label	 Â Classification	 Â Task:	 Â Assuming	 Â a	 Â paper	 Â has	 Â relevant	 Â content,	 Â 
based	 Â on	 Â its	 Â fulltext,	 Â can	 Â we	 Â determine	 Â which	 Â biological	 Â category	 Â (or	 Â categories)	 Â 
can	 Â be	 Â associated	 Â with	 Â it?	 Â Since	 Â each	 Â paper	 Â can	 Â be	 Â associated	 Â with	 Â multiple	 Â 
biological	 Â categories,	 Â this	 Â is	 Â not	 Â a	 Â multi-Â­â€class	 Â classification	 Â task,	 Â but	 Â rather	 Â a	 Â 
multi-Â­â€label	 Â classification	 Â task.	 Â Given	 Â k	 Â categories,	 Â for	 Â each	 Â paper	 Â we	 Â would	 Â like	 Â 
to	 Â predict	 Â a	 Â binary	 Â vector	 Â of	 Â length	 Â k	 Â indicating	 Â which	 Â biological	 Â categories	 Â the	 Â 
paper	 Â is	 Â associated	 Â with.	 Â 

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	 Â 

	 Â 
f
o
s
n
o
i
l
l
i

M

 

	 Â 

s
d
r
o
W

Features 

Dictionary	 Â Size	 Â 

Abstract	 Â 
Fulltext	 Â 

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	 Â 1	 Â 2	 Â 3	 Â 4	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â bigram	 Â feature	 Â set	 Â contains	 Â both	 Â unigrams	 Â and	 Â bigrams,	 Â while	 Â the	 Â trigram	 Â feature	 Â set	 Â contains	 Â 

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	 Â 1:	 Â Feature	 Â set	 Â size.	 Â 

0.6 

0.4 

0.2 

 

 
 

 

n

i
 
t
n
e
m
e
v
o
r
p
m

y
c
a
r
u
c
c
A
n
o
i
t
a
d

 

I
 

%

i
l
a
V

Effect of Preprocessing on  

Prediction Accuracy 

Aliased and Stemmed 
Stemmed 
Aliased 

0 

-0.2 

NaÃ¯ve 
Bayes 

SVM 

Logistic 
Regression 

computational constraints. Figure 1 shows 
the distribution of dictionary sizes for each 
of our feature sets. 

We experimented with two data 
preprocessing steps: stemming and alias 
replacement. We used the porter22 stemming 
algorithm to stem all of the words in the 
corpus. We also wrote code to replace any 
biological terms or aliases with standardized 
names, including gene names and Gene 
Ontology3, Yeast Phenotype Ontology4, and 
CHEBI5 chemical terms. To determine 
whether these preprocessing steps increase 

Figure	 Â 2:	 Â Effect	 Â of	 Â preprocessing	 Â on	 Â validation	 Â set	 Â 
accuracy	 Â relative	 Â to	 Â no	 Â preprocessing	 Â .	 Â 

We started by training four types of classifiers6 on the triage task: logistic 

the accuracy of our learning algorithms, we compared performance on three baseline 

both aliasing and stemming words has a positive effect on performance. 
 
Models 

unigram models to	 Â performance	 Â with	 Â no	 Â preprocessing (see Figure 2) and found that 
regression, SVM, NaÃ¯ve Bayes, and KNN.	 Â Logistic	 Â regression	 Â and	 Â SVM	 Â outperform	 Â the	 Â 
other	 Â two	 Â models,	 Â and	 Â in	 Â fact	 Â perform	 Â nearly	 Â identically.	 Â We	 Â chose	 Â to	 Â continue	 Â our	 Â 
analysis	 Â of	 Â the	 Â multi-Â­â€label	 Â classification	 Â task	 Â with	 Â logistic	 Â regression	 Â because	 Â logistic	 Â 
regression	 Â models	 Â the	 Â probability	 Â that	 Â a	 Â datapoint	 Â has	 Â a	 Â label	 Â of	 Â 1.	 Â This	 Â can	 Â be	 Â used	 Â 
as	 Â a	 Â confidence	 Â score,	 Â allowing	 Â us	 Â determine	 Â how	 Â confident	 Â we	 Â are	 Â in	 Â a	 Â given	 Â 
prediction.	 Â Because	 Â an	 Â SVM	 Â only	 Â takes	 Â into	 Â account	 Â the	 Â support	 Â vectors,	 Â a	 Â small	 Â 
subset	 Â of	 Â the	 Â training	 Â data,	 Â it	 Â is	 Â difficult	 Â to	 Â generate	 Â a	 Â meaningful	 Â confidence	 Â score	 Â 
for	 Â points	 Â that	 Â arenâ€™t	 Â support	 Â vectors.	 Â 	 Â Furthermore,	 Â the	 Â logistic	 Â regression	 Â model	 Â 
allows	 Â us	 Â to	 Â easily	 Â tune	 Â the	 Â false	 Â positive/false	 Â negative	 Â tradeoff,	 Â an	 Â important	 Â 
factor	 Â for	 Â this	 Â task. 
the logistic regression model. According to the logistic regression model, the probability	 Â 
that	 Â ğ‘¥!	 Â has	 Â label	 Â ğ‘¦! =1	 Â is	 Â modelled	 Â as 
ğ‘ƒğ‘¦! =1ğ‘¥!;ğœƒ,ğ‘ =â„!ğœƒ,ğ‘ =
1+expâˆ’ğœƒ!ğ‘¥! âˆ’ğ‘	 Â 
All	 Â features	 Â are	 Â normalized	 Â by	 Â their	 Â ğ¿!	 Â norm,	 Â i.e.	 Â we	 Â scale	 Â each	 Â feature	 Â so	 Â that	 Â 
=1	 Â for	 Â ğ‘—=1,â€¦,ğ‘‘.	 Â We	 Â did	 Â not	 Â normalize	 Â individual	 Â documents	 Â because	 Â 
ğ‘¥!! !
!!!!
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
2	 Â https://pypi.python.org/pypi/stemming/1.0	 Â 
3	 Â http://geneontology.org/	 Â 
4	 Â http://www.yeastgenome.org/ontology/phenotype/ypo/overview	 Â 
5	 Â http://www.ebi.ac.uk/chebi	 Â 
6	 Â Scikit-Â­â€learn:	 Â Machine	 Â Learning	 Â in	 Â Python,	 Â Pedregosa	 Â et	 Â al.,	 Â JMLR	 Â 12,	 Â pp.	 Â 2825-Â­â€2830,	 Â 2011.	 Â 
	 Â 

 
false negatives. False negatives correspond to missed information, which we would like 
to avoid. In order to drive the number of false negatives down, we made a modification to 

For our triage task, itâ€™s extremely important for our classifier to generate very few 

1

Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	 Â 	 Â Triage	 Â Task:	 Â Based	 Â on	 Â its	 Â abstract,	 Â can	 Â we	 Â determine	 Â if	 Â a	 Â new	 Â paper	 Â contains	 Â 	 Â 
relevant	 Â information?	 Â This	 Â is	 Â a	 Â binary	 Â classification	 Â task	 Â where	 Â a	 Â label	 Â of	 Â 1	 Â 
indicates	 Â that	 Â a	 Â paper	 Â contains	 Â relevant	 Â information	 Â and	 Â a	 Â label	 Â of	 Â -Â­â€1	 Â indicates	 Â 
that	 Â a	 Â paper	 Â does	 Â not	 Â contain	 Â relevant	 Â information.	 Â 
2)	 Â 	 Â Multi-Â­â€label	 Â Classification	 Â Task:	 Â Assuming	 Â a	 Â paper	 Â has	 Â relevant	 Â content,	 Â 
based	 Â on	 Â its	 Â fulltext,	 Â can	 Â we	 Â determine	 Â which	 Â biological	 Â category	 Â (or	 Â categories)	 Â 
can	 Â be	 Â associated	 Â with	 Â it?	 Â Since	 Â each	 Â paper	 Â can	 Â be	 Â associated	 Â with	 Â multiple	 Â 
biological	 Â categories,	 Â this	 Â is	 Â not	 Â a	 Â multi-Â­â€class	 Â classification	 Â task,	 Â but	 Â rather	 Â a	 Â 
multi-Â­â€label	 Â classification	 Â task.	 Â Given	 Â k	 Â categories,	 Â for	 Â each	 Â paper	 Â we	 Â would	 Â like	 Â 
to	 Â predict	 Â a	 Â binary	 Â vector	 Â of	 Â length	 Â k	 Â indicating	 Â which	 Â biological	 Â categories	 Â the	 Â 
paper	 Â is	 Â associated	 Â with.	 Â 

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	 Â 

	 Â 
f
o
s
n
o
i
l
l
i

M

 

	 Â 

s
d
r
o
W

Features 

Dictionary	 Â Size	 Â 

Abstract	 Â 
Fulltext	 Â 

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	 Â 1	 Â 2	 Â 3	 Â 4	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â bigram	 Â feature	 Â set	 Â contains	 Â both	 Â unigrams	 Â and	 Â bigrams,	 Â while	 Â the	 Â trigram	 Â feature	 Â set	 Â contains	 Â 

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	 Â 1:	 Â Feature	 Â set	 Â size.	 Â 

0.6 

0.4 

0.2 

 

 
 

 

n

i
 
t
n
e
m
e
v
o
r
p
m

y
c
a
r
u
c
c
A
n
o
i
t
a
d

 

I
 

%

i
l
a
V

Effect of Preprocessing on  

Prediction Accuracy 

Aliased and Stemmed 
Stemmed 
Aliased 

0 

-0.2 

NaÃ¯ve 
Bayes 

SVM 

Logistic 
Regression 

computational constraints. Figure 1 shows 
the distribution of dictionary sizes for each 
of our feature sets. 

We experimented with two data 
preprocessing steps: stemming and alias 
replacement. We used the porter22 stemming 
algorithm to stem all of the words in the 
corpus. We also wrote code to replace any 
biological terms or aliases with standardized 
names, including gene names and Gene 
Ontology3, Yeast Phenotype Ontology4, and 
CHEBI5 chemical terms. To determine 
whether these preprocessing steps increase 

Figure	 Â 2:	 Â Effect	 Â of	 Â preprocessing	 Â on	 Â validation	 Â set	 Â 
accuracy	 Â relative	 Â to	 Â no	 Â preprocessing	 Â .	 Â 

We started by training four types of classifiers6 on the triage task: logistic 

the accuracy of our learning algorithms, we compared performance on three baseline 

both aliasing and stemming words has a positive effect on performance. 
 
Models 

unigram models to	 Â performance	 Â with	 Â no	 Â preprocessing (see Figure 2) and found that 
regression, SVM, NaÃ¯ve Bayes, and KNN.	 Â Logistic	 Â regression	 Â and	 Â SVM	 Â outperform	 Â the	 Â 
other	 Â two	 Â models,	 Â and	 Â in	 Â fact	 Â perform	 Â nearly	 Â identically.	 Â We	 Â chose	 Â to	 Â continue	 Â our	 Â 
analysis	 Â of	 Â the	 Â multi-Â­â€label	 Â classification	 Â task	 Â with	 Â logistic	 Â regression	 Â because	 Â logistic	 Â 
regression	 Â models	 Â the	 Â probability	 Â that	 Â a	 Â datapoint	 Â has	 Â a	 Â label	 Â of	 Â 1.	 Â This	 Â can	 Â be	 Â used	 Â 
as	 Â a	 Â confidence	 Â score,	 Â allowing	 Â us	 Â determine	 Â how	 Â confident	 Â we	 Â are	 Â in	 Â a	 Â given	 Â 
prediction.	 Â Because	 Â an	 Â SVM	 Â only	 Â takes	 Â into	 Â account	 Â the	 Â support	 Â vectors,	 Â a	 Â small	 Â 
subset	 Â of	 Â the	 Â training	 Â data,	 Â it	 Â is	 Â difficult	 Â to	 Â generate	 Â a	 Â meaningful	 Â confidence	 Â score	 Â 
for	 Â points	 Â that	 Â arenâ€™t	 Â support	 Â vectors.	 Â 	 Â Furthermore,	 Â the	 Â logistic	 Â regression	 Â model	 Â 
allows	 Â us	 Â to	 Â easily	 Â tune	 Â the	 Â false	 Â positive/false	 Â negative	 Â tradeoff,	 Â an	 Â important	 Â 
factor	 Â for	 Â this	 Â task. 
the logistic regression model. According to the logistic regression model, the probability	 Â 
that	 Â ğ‘¥!	 Â has	 Â label	 Â ğ‘¦! =1	 Â is	 Â modelled	 Â as 
ğ‘ƒğ‘¦! =1ğ‘¥!;ğœƒ,ğ‘ =â„!ğœƒ,ğ‘ =
1+expâˆ’ğœƒ!ğ‘¥! âˆ’ğ‘	 Â 
All	 Â features	 Â are	 Â normalized	 Â by	 Â their	 Â ğ¿!	 Â norm,	 Â i.e.	 Â we	 Â scale	 Â each	 Â feature	 Â so	 Â that	 Â 
=1	 Â for	 Â ğ‘—=1,â€¦,ğ‘‘.	 Â We	 Â did	 Â not	 Â normalize	 Â individual	 Â documents	 Â because	 Â 
ğ‘¥!! !
!!!!
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
2	 Â https://pypi.python.org/pypi/stemming/1.0	 Â 
3	 Â http://geneontology.org/	 Â 
4	 Â http://www.yeastgenome.org/ontology/phenotype/ypo/overview	 Â 
5	 Â http://www.ebi.ac.uk/chebi	 Â 
6	 Â Scikit-Â­â€learn:	 Â Machine	 Â Learning	 Â in	 Â Python,	 Â Pedregosa	 Â et	 Â al.,	 Â JMLR	 Â 12,	 Â pp.	 Â 2825-Â­â€2830,	 Â 2011.	 Â 
	 Â 

 
false negatives. False negatives correspond to missed information, which we would like 
to avoid. In order to drive the number of false negatives down, we made a modification to 

For our triage task, itâ€™s extremely important for our classifier to generate very few 

1

they	 Â are	 Â all	 Â abstracts	 Â and	 Â have	 Â approximately	 Â the	 Â same	 Â length.	 Â We	 Â used	 Â ğ¿!	 Â 
regularization,	 Â so	 Â the	 Â classical	 Â ğ¿!-Â­â€regularized	 Â logistic	 Â regression	 Â negative	 Â log-Â­â€
likelihood	 Â is	 Â â„“ğ“ğœƒ,ğ‘ =âˆ’
+ğœ†2 ğœƒ !!	 Â 
ğ‘¦!logâ„!ğœƒ,ğ‘ + 1âˆ’ğ‘¦!
!
However,	 Â we	 Â are	 Â interested	 Â in	 Â a	 Â slightly	 Â different	 Â problem	 Â where	 Â we	 Â minimize	 Â the	 Â 
error	 Â on	 Â negative	 Â examples	 Â ğ’©= 1â‰¤ğ‘–â‰¤ğ‘›ğ‘¦! =0	 Â subject	 Â to	 Â the	 Â error	 Â on	 Â 
!!!
positive	 Â examples	 Â ğ’«= 1â‰¤ğ‘–â‰¤ğ‘›ğ‘¦! =1	 Â being	 Â below	 Â an	 Â acceptable	 Â threshold.	 Â 
This	 Â is	 Â given	 Â by	 Â the	 Â problem	 Â 
	 Â  min!,! Â 	 Â 
âˆ’
	 Â 
!âˆˆğ’©
	 Â subject Â to	 Â 
âˆ’
It	 Â can	 Â be	 Â shown	 Â (through	 Â the	 Â Lagrangian)	 Â that,	 Â for	 Â an	 Â appropriately	 Â chosen	 Â ğ›¼â‰¥0,	 Â 
the	 Â above	 Â is	 Â equivalent	 Â to	 Â minimizing	 Â 
!âˆˆğ’«
âˆ’ğ›¼
âˆ’
logâ„!ğœƒ,ğ‘
!âˆˆğ’©
!âˆˆğ’«

log1âˆ’â„!ğœƒ,ğ‘
+ğœ†2 ğœƒ !!	 Â 
+ğœ†2 ğœƒ !!	 Â 

log1âˆ’â„!ğœƒ,ğ‘
â‰¤ğœ	 Â 
logâ„!ğœƒ,ğ‘
log1âˆ’â„!ğœƒ,ğ‘

Therefore, we experimented with weighting our positive and negative classes in order to 
tune the number of false negatives on our validation set. 
 
Triage Task 

For the triage task, we separated our corpus of abstracts into training, validation, 
and testing sets. We used the validation set to select a model and tune parameters and the 
testing set to report on the predicted performance of our optimal classifier.  

We experimented with four different classifiers: naÃ¯ve bayes, KNN, logistic 

regression, and SVM, our results are given in Table 1. Both logistic regression and SVM 
produced extremely similar models. This likely occurred because they both produce a 
linear decision boundary that takes into account each training example according to how 

well it is classified and can handle many correlated features because of ğ¿! regularization. 

Both KNN and NaÃ¯ve Bayes seemed to suffer from spurious and correlated features. This 
trend is most clear for KNN; as more features were added with the bigram and trigram 
datasets, performance steeply dropped off.  

Table 1: Triage Task Results 

Model 

(120,000 training samples; 15,000 validation samples; 15,000 testing samples): 
Validation 
Accuracy 
81.69% 
82.08% 
82.17% 
72.20% 
56.00% 
59.30% 
86.67% 
87.08% 
87.22% 
86.51% 

Baseline  Training 
Accuracy 
84.28% 
53% 
86.90% 
53% 
53% 
89.65% 
53% 
53% 
53% 
53% 
53% 
53% 
53% 

Features 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 

Corpus 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 

NB 
NB 
NB 
KNN 
KNN 
KNN 
SVM 
SVM 
SVM 
LogReg 

89.57% 
94.19% 
96.41% 
89.35% 

- 
- 
- 

Validation 
Sensitivity 

Validation 
Specificity 

.730 
.734 
.734 

- 
- 
- 

.832 
.841 
.842 
.833 

.891 
.895 
.897 

- 
- 
- 

.896 
.896 
.898 
.893 

Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	 Â 	 Â Triage	 Â Task:	 Â Based	 Â on	 Â its	 Â abstract,	 Â can	 Â we	 Â determine	 Â if	 Â a	 Â new	 Â paper	 Â contains	 Â 	 Â 
relevant	 Â information?	 Â This	 Â is	 Â a	 Â binary	 Â classification	 Â task	 Â where	 Â a	 Â label	 Â of	 Â 1	 Â 
indicates	 Â that	 Â a	 Â paper	 Â contains	 Â relevant	 Â information	 Â and	 Â a	 Â label	 Â of	 Â -Â­â€1	 Â indicates	 Â 
that	 Â a	 Â paper	 Â does	 Â not	 Â contain	 Â relevant	 Â information.	 Â 
2)	 Â 	 Â Multi-Â­â€label	 Â Classification	 Â Task:	 Â Assuming	 Â a	 Â paper	 Â has	 Â relevant	 Â content,	 Â 
based	 Â on	 Â its	 Â fulltext,	 Â can	 Â we	 Â determine	 Â which	 Â biological	 Â category	 Â (or	 Â categories)	 Â 
can	 Â be	 Â associated	 Â with	 Â it?	 Â Since	 Â each	 Â paper	 Â can	 Â be	 Â associated	 Â with	 Â multiple	 Â 
biological	 Â categories,	 Â this	 Â is	 Â not	 Â a	 Â multi-Â­â€class	 Â classification	 Â task,	 Â but	 Â rather	 Â a	 Â 
multi-Â­â€label	 Â classification	 Â task.	 Â Given	 Â k	 Â categories,	 Â for	 Â each	 Â paper	 Â we	 Â would	 Â like	 Â 
to	 Â predict	 Â a	 Â binary	 Â vector	 Â of	 Â length	 Â k	 Â indicating	 Â which	 Â biological	 Â categories	 Â the	 Â 
paper	 Â is	 Â associated	 Â with.	 Â 

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	 Â 

	 Â 
f
o
s
n
o
i
l
l
i

M

 

	 Â 

s
d
r
o
W

Features 

Dictionary	 Â Size	 Â 

Abstract	 Â 
Fulltext	 Â 

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	 Â 1	 Â 2	 Â 3	 Â 4	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â bigram	 Â feature	 Â set	 Â contains	 Â both	 Â unigrams	 Â and	 Â bigrams,	 Â while	 Â the	 Â trigram	 Â feature	 Â set	 Â contains	 Â 

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	 Â 1:	 Â Feature	 Â set	 Â size.	 Â 

0.6 

0.4 

0.2 

 

 
 

 

n

i
 
t
n
e
m
e
v
o
r
p
m

y
c
a
r
u
c
c
A
n
o
i
t
a
d

 

I
 

%

i
l
a
V

Effect of Preprocessing on  

Prediction Accuracy 

Aliased and Stemmed 
Stemmed 
Aliased 

0 

-0.2 

NaÃ¯ve 
Bayes 

SVM 

Logistic 
Regression 

computational constraints. Figure 1 shows 
the distribution of dictionary sizes for each 
of our feature sets. 

We experimented with two data 
preprocessing steps: stemming and alias 
replacement. We used the porter22 stemming 
algorithm to stem all of the words in the 
corpus. We also wrote code to replace any 
biological terms or aliases with standardized 
names, including gene names and Gene 
Ontology3, Yeast Phenotype Ontology4, and 
CHEBI5 chemical terms. To determine 
whether these preprocessing steps increase 

Figure	 Â 2:	 Â Effect	 Â of	 Â preprocessing	 Â on	 Â validation	 Â set	 Â 
accuracy	 Â relative	 Â to	 Â no	 Â preprocessing	 Â .	 Â 

We started by training four types of classifiers6 on the triage task: logistic 

the accuracy of our learning algorithms, we compared performance on three baseline 

both aliasing and stemming words has a positive effect on performance. 
 
Models 

unigram models to	 Â performance	 Â with	 Â no	 Â preprocessing (see Figure 2) and found that 
regression, SVM, NaÃ¯ve Bayes, and KNN.	 Â Logistic	 Â regression	 Â and	 Â SVM	 Â outperform	 Â the	 Â 
other	 Â two	 Â models,	 Â and	 Â in	 Â fact	 Â perform	 Â nearly	 Â identically.	 Â We	 Â chose	 Â to	 Â continue	 Â our	 Â 
analysis	 Â of	 Â the	 Â multi-Â­â€label	 Â classification	 Â task	 Â with	 Â logistic	 Â regression	 Â because	 Â logistic	 Â 
regression	 Â models	 Â the	 Â probability	 Â that	 Â a	 Â datapoint	 Â has	 Â a	 Â label	 Â of	 Â 1.	 Â This	 Â can	 Â be	 Â used	 Â 
as	 Â a	 Â confidence	 Â score,	 Â allowing	 Â us	 Â determine	 Â how	 Â confident	 Â we	 Â are	 Â in	 Â a	 Â given	 Â 
prediction.	 Â Because	 Â an	 Â SVM	 Â only	 Â takes	 Â into	 Â account	 Â the	 Â support	 Â vectors,	 Â a	 Â small	 Â 
subset	 Â of	 Â the	 Â training	 Â data,	 Â it	 Â is	 Â difficult	 Â to	 Â generate	 Â a	 Â meaningful	 Â confidence	 Â score	 Â 
for	 Â points	 Â that	 Â arenâ€™t	 Â support	 Â vectors.	 Â 	 Â Furthermore,	 Â the	 Â logistic	 Â regression	 Â model	 Â 
allows	 Â us	 Â to	 Â easily	 Â tune	 Â the	 Â false	 Â positive/false	 Â negative	 Â tradeoff,	 Â an	 Â important	 Â 
factor	 Â for	 Â this	 Â task. 
the logistic regression model. According to the logistic regression model, the probability	 Â 
that	 Â ğ‘¥!	 Â has	 Â label	 Â ğ‘¦! =1	 Â is	 Â modelled	 Â as 
ğ‘ƒğ‘¦! =1ğ‘¥!;ğœƒ,ğ‘ =â„!ğœƒ,ğ‘ =
1+expâˆ’ğœƒ!ğ‘¥! âˆ’ğ‘	 Â 
All	 Â features	 Â are	 Â normalized	 Â by	 Â their	 Â ğ¿!	 Â norm,	 Â i.e.	 Â we	 Â scale	 Â each	 Â feature	 Â so	 Â that	 Â 
=1	 Â for	 Â ğ‘—=1,â€¦,ğ‘‘.	 Â We	 Â did	 Â not	 Â normalize	 Â individual	 Â documents	 Â because	 Â 
ğ‘¥!! !
!!!!
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
2	 Â https://pypi.python.org/pypi/stemming/1.0	 Â 
3	 Â http://geneontology.org/	 Â 
4	 Â http://www.yeastgenome.org/ontology/phenotype/ypo/overview	 Â 
5	 Â http://www.ebi.ac.uk/chebi	 Â 
6	 Â Scikit-Â­â€learn:	 Â Machine	 Â Learning	 Â in	 Â Python,	 Â Pedregosa	 Â et	 Â al.,	 Â JMLR	 Â 12,	 Â pp.	 Â 2825-Â­â€2830,	 Â 2011.	 Â 
	 Â 

 
false negatives. False negatives correspond to missed information, which we would like 
to avoid. In order to drive the number of false negatives down, we made a modification to 

For our triage task, itâ€™s extremely important for our classifier to generate very few 

1

they	 Â are	 Â all	 Â abstracts	 Â and	 Â have	 Â approximately	 Â the	 Â same	 Â length.	 Â We	 Â used	 Â ğ¿!	 Â 
regularization,	 Â so	 Â the	 Â classical	 Â ğ¿!-Â­â€regularized	 Â logistic	 Â regression	 Â negative	 Â log-Â­â€
likelihood	 Â is	 Â â„“ğ“ğœƒ,ğ‘ =âˆ’
+ğœ†2 ğœƒ !!	 Â 
ğ‘¦!logâ„!ğœƒ,ğ‘ + 1âˆ’ğ‘¦!
!
However,	 Â we	 Â are	 Â interested	 Â in	 Â a	 Â slightly	 Â different	 Â problem	 Â where	 Â we	 Â minimize	 Â the	 Â 
error	 Â on	 Â negative	 Â examples	 Â ğ’©= 1â‰¤ğ‘–â‰¤ğ‘›ğ‘¦! =0	 Â subject	 Â to	 Â the	 Â error	 Â on	 Â 
!!!
positive	 Â examples	 Â ğ’«= 1â‰¤ğ‘–â‰¤ğ‘›ğ‘¦! =1	 Â being	 Â below	 Â an	 Â acceptable	 Â threshold.	 Â 
This	 Â is	 Â given	 Â by	 Â the	 Â problem	 Â 
	 Â  min!,! Â 	 Â 
âˆ’
	 Â 
!âˆˆğ’©
	 Â subject Â to	 Â 
âˆ’
It	 Â can	 Â be	 Â shown	 Â (through	 Â the	 Â Lagrangian)	 Â that,	 Â for	 Â an	 Â appropriately	 Â chosen	 Â ğ›¼â‰¥0,	 Â 
the	 Â above	 Â is	 Â equivalent	 Â to	 Â minimizing	 Â 
!âˆˆğ’«
âˆ’ğ›¼
âˆ’
logâ„!ğœƒ,ğ‘
!âˆˆğ’©
!âˆˆğ’«

log1âˆ’â„!ğœƒ,ğ‘
+ğœ†2 ğœƒ !!	 Â 
+ğœ†2 ğœƒ !!	 Â 

log1âˆ’â„!ğœƒ,ğ‘
â‰¤ğœ	 Â 
logâ„!ğœƒ,ğ‘
log1âˆ’â„!ğœƒ,ğ‘

Therefore, we experimented with weighting our positive and negative classes in order to 
tune the number of false negatives on our validation set. 
 
Triage Task 

For the triage task, we separated our corpus of abstracts into training, validation, 
and testing sets. We used the validation set to select a model and tune parameters and the 
testing set to report on the predicted performance of our optimal classifier.  

We experimented with four different classifiers: naÃ¯ve bayes, KNN, logistic 

regression, and SVM, our results are given in Table 1. Both logistic regression and SVM 
produced extremely similar models. This likely occurred because they both produce a 
linear decision boundary that takes into account each training example according to how 

well it is classified and can handle many correlated features because of ğ¿! regularization. 

Both KNN and NaÃ¯ve Bayes seemed to suffer from spurious and correlated features. This 
trend is most clear for KNN; as more features were added with the bigram and trigram 
datasets, performance steeply dropped off.  

Table 1: Triage Task Results 

Model 

(120,000 training samples; 15,000 validation samples; 15,000 testing samples): 
Validation 
Accuracy 
81.69% 
82.08% 
82.17% 
72.20% 
56.00% 
59.30% 
86.67% 
87.08% 
87.22% 
86.51% 

Baseline  Training 
Accuracy 
84.28% 
53% 
86.90% 
53% 
53% 
89.65% 
53% 
53% 
53% 
53% 
53% 
53% 
53% 

Features 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 

Corpus 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 

NB 
NB 
NB 
KNN 
KNN 
KNN 
SVM 
SVM 
SVM 
LogReg 

89.57% 
94.19% 
96.41% 
89.35% 

- 
- 
- 

Validation 
Sensitivity 

Validation 
Specificity 

.730 
.734 
.734 

- 
- 
- 

.832 
.841 
.842 
.833 

.891 
.895 
.897 

- 
- 
- 

.896 
.896 
.898 
.893 

.841 
.842 

53% 
53% 

93.56% 
95.66% 

87.01% 
87.17% 

Abstract 
Abstract 

Bigrams 
Trigrams 

.895 
LogReg 
LogReg 
.897 
* Baseline shows the accuracy of classifying each sample to belong to the most common training class. 
Since curators must read all positively triaged papers, any false positives can 
 
easily be corrected during the annotation process. This means we are primarily interested 
in our classifierâ€™s negative predictions. False negatives correspond to papers whose 
information is missed, which we want to avoid. Furthermore, since curators no longer 
need to read papers that are triaged as negative, the rate of negative predictions measures 

the time savings of our classifier. By changing the class weight parameter ğ›¼ given in the 

equations above, we were able to tune the number of false negative samples on our 
validation set. Table 2 shows validation statistics using trigram features on the abstract 
corpus for the triage task. Although validation accuracy may decrease, tuning alpha 
allows us to decrease the expected number of false negative predictions, which results in 
a better classifier for the task.  

Table 2: Alpha Tuning Results 

Alpha 

1 
0.1 
0.001 

Validation 
Accuracy 
87.17% 
85.99% 
76.51% 

TP 
FN 
5839  1092 
877 
6054 
6691 
240 

TN 
FP 
833 
7236 
1224  6845 
3283  4786 

False Negative/ 

Predicted Negative 

Predicted Negative/ 

All 

7.28% 
5.84% 
1.6% 

55.52% 
51.48% 
33.51% 

Based on these results, we chose a logistic regression classifier using trigrams and an 
alpha parameter of 0.1 as our optimal classifier. Table 3 shows the predicted performance 
of this classifier based on the testing set. 

Table 3: Testing Results on Triage Task 

Testing 
Accuracy 

85.8% 

TP 
6330 

FN 
687 

FP 
TN 
1443  6540 

False Negative/ 

Predicted Negative 

4.58% 

Predicted Negative/ 

All 
43.6% 

We split the fulltext corpus in a similar manner as the abstract corpus, creating 

 
Multi-label Classification Task 
 
training, validation, and testing sets. We chose to approach the multi-label classification 
problem by training 5 classifiers, one for each biological category. Results are shown in 
Table 4. Overall, this task seemed to be much more difficult than the triage task. This 
may have been due to the fact that our dataset had relatively few positive examples for 
each label. Performance for one of the five categories was fairly poor, with our logistic 
regression classifier performing less accurately than a simple baseline classifier that 
assigns every sample to the most common training class. Performance on the remaining 
three categories was adequate, but low. This task requires further work.  

(70,000 training samples; 7,000 validation samples; 7,000 testing samples) 

Table 4: Multi-label Classification Task Results 

Model 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 

Category Corpus  Features  Baseline 

GO  Abstract Unigrams  88% 
GO  Abstract Trigrams  88% 
GO 
Fulltext  Unigrams  88% 
Phen.  Abstract Unigrams  79% 
Phen.  Abstract Trigrams  79% 
Phen  Fulltext  Unigrams  79% 
Phys  Abstract Unigrams  91% 

Training  
Accuracy 
96.41% 
100.00% 
99.98% 
97.23% 
100.00% 
100.00% 
97.80% 

Validation  
Accuracy 
82.67% 
86.54% 
85.23% 
87.82% 
91.58% 
90.55% 
91.54% 

Validation  
Sensitivity 
.867 
.970 
.906 
.913 
.987 
.952 
.928 

Validation  
Specificity 
.575 
.395 
.507 
.440 
.196 
.300 
.654 

Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	 Â 	 Â Triage	 Â Task:	 Â Based	 Â on	 Â its	 Â abstract,	 Â can	 Â we	 Â determine	 Â if	 Â a	 Â new	 Â paper	 Â contains	 Â 	 Â 
relevant	 Â information?	 Â This	 Â is	 Â a	 Â binary	 Â classification	 Â task	 Â where	 Â a	 Â label	 Â of	 Â 1	 Â 
indicates	 Â that	 Â a	 Â paper	 Â contains	 Â relevant	 Â information	 Â and	 Â a	 Â label	 Â of	 Â -Â­â€1	 Â indicates	 Â 
that	 Â a	 Â paper	 Â does	 Â not	 Â contain	 Â relevant	 Â information.	 Â 
2)	 Â 	 Â Multi-Â­â€label	 Â Classification	 Â Task:	 Â Assuming	 Â a	 Â paper	 Â has	 Â relevant	 Â content,	 Â 
based	 Â on	 Â its	 Â fulltext,	 Â can	 Â we	 Â determine	 Â which	 Â biological	 Â category	 Â (or	 Â categories)	 Â 
can	 Â be	 Â associated	 Â with	 Â it?	 Â Since	 Â each	 Â paper	 Â can	 Â be	 Â associated	 Â with	 Â multiple	 Â 
biological	 Â categories,	 Â this	 Â is	 Â not	 Â a	 Â multi-Â­â€class	 Â classification	 Â task,	 Â but	 Â rather	 Â a	 Â 
multi-Â­â€label	 Â classification	 Â task.	 Â Given	 Â k	 Â categories,	 Â for	 Â each	 Â paper	 Â we	 Â would	 Â like	 Â 
to	 Â predict	 Â a	 Â binary	 Â vector	 Â of	 Â length	 Â k	 Â indicating	 Â which	 Â biological	 Â categories	 Â the	 Â 
paper	 Â is	 Â associated	 Â with.	 Â 

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	 Â 

	 Â 
f
o
s
n
o
i
l
l
i

M

 

	 Â 

s
d
r
o
W

Features 

Dictionary	 Â Size	 Â 

Abstract	 Â 
Fulltext	 Â 

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	 Â 1	 Â 2	 Â 3	 Â 4	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â bigram	 Â feature	 Â set	 Â contains	 Â both	 Â unigrams	 Â and	 Â bigrams,	 Â while	 Â the	 Â trigram	 Â feature	 Â set	 Â contains	 Â 

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	 Â 1:	 Â Feature	 Â set	 Â size.	 Â 

0.6 

0.4 

0.2 

 

 
 

 

n

i
 
t
n
e
m
e
v
o
r
p
m

y
c
a
r
u
c
c
A
n
o
i
t
a
d

 

I
 

%

i
l
a
V

Effect of Preprocessing on  

Prediction Accuracy 

Aliased and Stemmed 
Stemmed 
Aliased 

0 

-0.2 

NaÃ¯ve 
Bayes 

SVM 

Logistic 
Regression 

computational constraints. Figure 1 shows 
the distribution of dictionary sizes for each 
of our feature sets. 

We experimented with two data 
preprocessing steps: stemming and alias 
replacement. We used the porter22 stemming 
algorithm to stem all of the words in the 
corpus. We also wrote code to replace any 
biological terms or aliases with standardized 
names, including gene names and Gene 
Ontology3, Yeast Phenotype Ontology4, and 
CHEBI5 chemical terms. To determine 
whether these preprocessing steps increase 

Figure	 Â 2:	 Â Effect	 Â of	 Â preprocessing	 Â on	 Â validation	 Â set	 Â 
accuracy	 Â relative	 Â to	 Â no	 Â preprocessing	 Â .	 Â 

We started by training four types of classifiers6 on the triage task: logistic 

the accuracy of our learning algorithms, we compared performance on three baseline 

both aliasing and stemming words has a positive effect on performance. 
 
Models 

unigram models to	 Â performance	 Â with	 Â no	 Â preprocessing (see Figure 2) and found that 
regression, SVM, NaÃ¯ve Bayes, and KNN.	 Â Logistic	 Â regression	 Â and	 Â SVM	 Â outperform	 Â the	 Â 
other	 Â two	 Â models,	 Â and	 Â in	 Â fact	 Â perform	 Â nearly	 Â identically.	 Â We	 Â chose	 Â to	 Â continue	 Â our	 Â 
analysis	 Â of	 Â the	 Â multi-Â­â€label	 Â classification	 Â task	 Â with	 Â logistic	 Â regression	 Â because	 Â logistic	 Â 
regression	 Â models	 Â the	 Â probability	 Â that	 Â a	 Â datapoint	 Â has	 Â a	 Â label	 Â of	 Â 1.	 Â This	 Â can	 Â be	 Â used	 Â 
as	 Â a	 Â confidence	 Â score,	 Â allowing	 Â us	 Â determine	 Â how	 Â confident	 Â we	 Â are	 Â in	 Â a	 Â given	 Â 
prediction.	 Â Because	 Â an	 Â SVM	 Â only	 Â takes	 Â into	 Â account	 Â the	 Â support	 Â vectors,	 Â a	 Â small	 Â 
subset	 Â of	 Â the	 Â training	 Â data,	 Â it	 Â is	 Â difficult	 Â to	 Â generate	 Â a	 Â meaningful	 Â confidence	 Â score	 Â 
for	 Â points	 Â that	 Â arenâ€™t	 Â support	 Â vectors.	 Â 	 Â Furthermore,	 Â the	 Â logistic	 Â regression	 Â model	 Â 
allows	 Â us	 Â to	 Â easily	 Â tune	 Â the	 Â false	 Â positive/false	 Â negative	 Â tradeoff,	 Â an	 Â important	 Â 
factor	 Â for	 Â this	 Â task. 
the logistic regression model. According to the logistic regression model, the probability	 Â 
that	 Â ğ‘¥!	 Â has	 Â label	 Â ğ‘¦! =1	 Â is	 Â modelled	 Â as 
ğ‘ƒğ‘¦! =1ğ‘¥!;ğœƒ,ğ‘ =â„!ğœƒ,ğ‘ =
1+expâˆ’ğœƒ!ğ‘¥! âˆ’ğ‘	 Â 
All	 Â features	 Â are	 Â normalized	 Â by	 Â their	 Â ğ¿!	 Â norm,	 Â i.e.	 Â we	 Â scale	 Â each	 Â feature	 Â so	 Â that	 Â 
=1	 Â for	 Â ğ‘—=1,â€¦,ğ‘‘.	 Â We	 Â did	 Â not	 Â normalize	 Â individual	 Â documents	 Â because	 Â 
ğ‘¥!! !
!!!!
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
2	 Â https://pypi.python.org/pypi/stemming/1.0	 Â 
3	 Â http://geneontology.org/	 Â 
4	 Â http://www.yeastgenome.org/ontology/phenotype/ypo/overview	 Â 
5	 Â http://www.ebi.ac.uk/chebi	 Â 
6	 Â Scikit-Â­â€learn:	 Â Machine	 Â Learning	 Â in	 Â Python,	 Â Pedregosa	 Â et	 Â al.,	 Â JMLR	 Â 12,	 Â pp.	 Â 2825-Â­â€2830,	 Â 2011.	 Â 
	 Â 

 
false negatives. False negatives correspond to missed information, which we would like 
to avoid. In order to drive the number of false negatives down, we made a modification to 

For our triage task, itâ€™s extremely important for our classifier to generate very few 

1

they	 Â are	 Â all	 Â abstracts	 Â and	 Â have	 Â approximately	 Â the	 Â same	 Â length.	 Â We	 Â used	 Â ğ¿!	 Â 
regularization,	 Â so	 Â the	 Â classical	 Â ğ¿!-Â­â€regularized	 Â logistic	 Â regression	 Â negative	 Â log-Â­â€
likelihood	 Â is	 Â â„“ğ“ğœƒ,ğ‘ =âˆ’
+ğœ†2 ğœƒ !!	 Â 
ğ‘¦!logâ„!ğœƒ,ğ‘ + 1âˆ’ğ‘¦!
!
However,	 Â we	 Â are	 Â interested	 Â in	 Â a	 Â slightly	 Â different	 Â problem	 Â where	 Â we	 Â minimize	 Â the	 Â 
error	 Â on	 Â negative	 Â examples	 Â ğ’©= 1â‰¤ğ‘–â‰¤ğ‘›ğ‘¦! =0	 Â subject	 Â to	 Â the	 Â error	 Â on	 Â 
!!!
positive	 Â examples	 Â ğ’«= 1â‰¤ğ‘–â‰¤ğ‘›ğ‘¦! =1	 Â being	 Â below	 Â an	 Â acceptable	 Â threshold.	 Â 
This	 Â is	 Â given	 Â by	 Â the	 Â problem	 Â 
	 Â  min!,! Â 	 Â 
âˆ’
	 Â 
!âˆˆğ’©
	 Â subject Â to	 Â 
âˆ’
It	 Â can	 Â be	 Â shown	 Â (through	 Â the	 Â Lagrangian)	 Â that,	 Â for	 Â an	 Â appropriately	 Â chosen	 Â ğ›¼â‰¥0,	 Â 
the	 Â above	 Â is	 Â equivalent	 Â to	 Â minimizing	 Â 
!âˆˆğ’«
âˆ’ğ›¼
âˆ’
logâ„!ğœƒ,ğ‘
!âˆˆğ’©
!âˆˆğ’«

log1âˆ’â„!ğœƒ,ğ‘
+ğœ†2 ğœƒ !!	 Â 
+ğœ†2 ğœƒ !!	 Â 

log1âˆ’â„!ğœƒ,ğ‘
â‰¤ğœ	 Â 
logâ„!ğœƒ,ğ‘
log1âˆ’â„!ğœƒ,ğ‘

Therefore, we experimented with weighting our positive and negative classes in order to 
tune the number of false negatives on our validation set. 
 
Triage Task 

For the triage task, we separated our corpus of abstracts into training, validation, 
and testing sets. We used the validation set to select a model and tune parameters and the 
testing set to report on the predicted performance of our optimal classifier.  

We experimented with four different classifiers: naÃ¯ve bayes, KNN, logistic 

regression, and SVM, our results are given in Table 1. Both logistic regression and SVM 
produced extremely similar models. This likely occurred because they both produce a 
linear decision boundary that takes into account each training example according to how 

well it is classified and can handle many correlated features because of ğ¿! regularization. 

Both KNN and NaÃ¯ve Bayes seemed to suffer from spurious and correlated features. This 
trend is most clear for KNN; as more features were added with the bigram and trigram 
datasets, performance steeply dropped off.  

Table 1: Triage Task Results 

Model 

(120,000 training samples; 15,000 validation samples; 15,000 testing samples): 
Validation 
Accuracy 
81.69% 
82.08% 
82.17% 
72.20% 
56.00% 
59.30% 
86.67% 
87.08% 
87.22% 
86.51% 

Baseline  Training 
Accuracy 
84.28% 
53% 
86.90% 
53% 
53% 
89.65% 
53% 
53% 
53% 
53% 
53% 
53% 
53% 

Features 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 

Corpus 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 

NB 
NB 
NB 
KNN 
KNN 
KNN 
SVM 
SVM 
SVM 
LogReg 

89.57% 
94.19% 
96.41% 
89.35% 

- 
- 
- 

Validation 
Sensitivity 

Validation 
Specificity 

.730 
.734 
.734 

- 
- 
- 

.832 
.841 
.842 
.833 

.891 
.895 
.897 

- 
- 
- 

.896 
.896 
.898 
.893 

.841 
.842 

53% 
53% 

93.56% 
95.66% 

87.01% 
87.17% 

Abstract 
Abstract 

Bigrams 
Trigrams 

.895 
LogReg 
LogReg 
.897 
* Baseline shows the accuracy of classifying each sample to belong to the most common training class. 
Since curators must read all positively triaged papers, any false positives can 
 
easily be corrected during the annotation process. This means we are primarily interested 
in our classifierâ€™s negative predictions. False negatives correspond to papers whose 
information is missed, which we want to avoid. Furthermore, since curators no longer 
need to read papers that are triaged as negative, the rate of negative predictions measures 

the time savings of our classifier. By changing the class weight parameter ğ›¼ given in the 

equations above, we were able to tune the number of false negative samples on our 
validation set. Table 2 shows validation statistics using trigram features on the abstract 
corpus for the triage task. Although validation accuracy may decrease, tuning alpha 
allows us to decrease the expected number of false negative predictions, which results in 
a better classifier for the task.  

Table 2: Alpha Tuning Results 

Alpha 

1 
0.1 
0.001 

Validation 
Accuracy 
87.17% 
85.99% 
76.51% 

TP 
FN 
5839  1092 
877 
6054 
6691 
240 

TN 
FP 
833 
7236 
1224  6845 
3283  4786 

False Negative/ 

Predicted Negative 

Predicted Negative/ 

All 

7.28% 
5.84% 
1.6% 

55.52% 
51.48% 
33.51% 

Based on these results, we chose a logistic regression classifier using trigrams and an 
alpha parameter of 0.1 as our optimal classifier. Table 3 shows the predicted performance 
of this classifier based on the testing set. 

Table 3: Testing Results on Triage Task 

Testing 
Accuracy 

85.8% 

TP 
6330 

FN 
687 

FP 
TN 
1443  6540 

False Negative/ 

Predicted Negative 

4.58% 

Predicted Negative/ 

All 
43.6% 

We split the fulltext corpus in a similar manner as the abstract corpus, creating 

 
Multi-label Classification Task 
 
training, validation, and testing sets. We chose to approach the multi-label classification 
problem by training 5 classifiers, one for each biological category. Results are shown in 
Table 4. Overall, this task seemed to be much more difficult than the triage task. This 
may have been due to the fact that our dataset had relatively few positive examples for 
each label. Performance for one of the five categories was fairly poor, with our logistic 
regression classifier performing less accurately than a simple baseline classifier that 
assigns every sample to the most common training class. Performance on the remaining 
three categories was adequate, but low. This task requires further work.  

(70,000 training samples; 7,000 validation samples; 7,000 testing samples) 

Table 4: Multi-label Classification Task Results 

Model 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 

Category Corpus  Features  Baseline 

GO  Abstract Unigrams  88% 
GO  Abstract Trigrams  88% 
GO 
Fulltext  Unigrams  88% 
Phen.  Abstract Unigrams  79% 
Phen.  Abstract Trigrams  79% 
Phen  Fulltext  Unigrams  79% 
Phys  Abstract Unigrams  91% 

Training  
Accuracy 
96.41% 
100.00% 
99.98% 
97.23% 
100.00% 
100.00% 
97.80% 

Validation  
Accuracy 
82.67% 
86.54% 
85.23% 
87.82% 
91.58% 
90.55% 
91.54% 

Validation  
Sensitivity 
.867 
.970 
.906 
.913 
.987 
.952 
.928 

Validation  
Specificity 
.575 
.395 
.507 
.440 
.196 
.300 
.654 

Phys  Abstract Trigrams  91% 
Phys 
Fulltext  Unigrams  91% 
Gen.  Abstract Unigrams  91% 
Gen.  Abstract Trigrams  91% 
Gen. 
Fulltext  Unigrams  91% 
Lit.  Abstract Unigram  77% 
Lit.  Abstract Trigrams  77% 
Lit. 
Fulltext  Unigrams  77% 

100.00% 
99.78% 
100.0% 
100.00% 
99.79% 
97.75% 
100.00% 
99.45% 

92.27% 
92.68% 
89.11% 
92.38% 
91.67% 
90.33% 
90.85% 
89.66% 

.983 
.945 
.921 
.983 
.946 
.730 
.871 
.760 

.567 
.707 
.625 
.531 
.644 
.941 
.854 
.923 

* Baseline shows the accuracy of classifying each sample to belong to the most common training class. 

ROC	 Â Curve	 Â for	 Â GO	 Â Category	 Â 

Unigrams	 Â 
Bigrams	 Â 
Trigrams	 Â 
0.6	 Â 
0.8	 Â 

1	 Â 

0	 Â 

0.2	 Â 

0.4	 Â 

1-Â­â€Speci?icity	 Â 

Figure	 Â 3:	 Â Sensitivity	 Â as	 Â a	 Â function	 Â of	 Â 
specificity	 Â for	 Â unigram,	 Â bigram,	 Â and	 Â trigram	 Â 
features	 Â for	 Â the	 Â GO	 Â multi-Â­â€label	 Â category.	 Â  

In Table 4, when comparing the 

performance of the abstract unigrams to the 
abstract trigrams, it appears that sensitivity 
increases while specificity decreases. However, 
this is because we are sampling different points 
along the ROC curve. When we plot the ROC 
curves for unigrams, bigrams, and trigrams (see 
Figure 3), we see that as we move to more 
complicated feature sets, the ROC curve 
improves. This is most likely because bigrams 
and trigrams have the power to capture 
informative phrases and multi-word biological 
terms that canâ€™t be captured by unigrams alone.  
Based on these results, we chose to use 

the abstract trigrams dataset with a logistic 

LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 

0	 Â 0.2	 Â 0.4	 Â 0.6	 Â 0.8	 Â 1	 Â 

	 Â 

y
t
i
v
i
t
i
s
n
e
S

regression model as our optimal classifier for this task. Table 5 shows predicted 
performance for the classifier for each category based on the testing set. 
Table 5: Testing Results on Multi-label Classification Task 

Category 

Baseline 

Testing Accuracy 

Testing Sensitivity  Testing Specificity 

GO 
Phen. 
Phys. 
Gen. 
Lit. 

88% 
79% 
91% 
91% 
77% 

85.52% 
91.58% 
92.33% 
91.95% 
90.45% 

.934 
.974 
.966 
.965 
.701 

.386 
.192 
.640 
.511 
.946 

 
Future Work 

There are several areas of this project that would benefit from future work. Due to 

the size of our datasets, we were only able to do parameter tuning at an order of 
magnitude scale. By tuning parameters at a more fine-grained level, we believe we could 
increase performance on both tasks. Our results on the multi-label classification task 
show that working with trigrams from the abstract corpus offers significant gains over 
working with unigrams. We believe that working with bigrams or trigrams from the 
fulltext corpus has the potential to dramatically increase performance by capturing 
information found in phrases and multi-word biological terms. However, this would 
require custom software because it would increase our feature size by at least an order of 
magnitude. Finally, the ultimate goal is to integrate our classifier into an active curation 
pipeline. We plan to present our results to the Saccharomyces Genome Database, and 
hope to eventually use these techniques to automate part of the biological data curation 
process. 

