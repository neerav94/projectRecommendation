Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	  	  Triage	  Task:	  Based	  on	  its	  abstract,	  can	  we	  determine	  if	  a	  new	  paper	  contains	  	  
relevant	  information?	  This	  is	  a	  binary	  classification	  task	  where	  a	  label	  of	  1	  
indicates	  that	  a	  paper	  contains	  relevant	  information	  and	  a	  label	  of	  -­‐1	  indicates	  
that	  a	  paper	  does	  not	  contain	  relevant	  information.	  
2)	  	  Multi-­‐label	  Classification	  Task:	  Assuming	  a	  paper	  has	  relevant	  content,	  
based	  on	  its	  fulltext,	  can	  we	  determine	  which	  biological	  category	  (or	  categories)	  
can	  be	  associated	  with	  it?	  Since	  each	  paper	  can	  be	  associated	  with	  multiple	  
biological	  categories,	  this	  is	  not	  a	  multi-­‐class	  classification	  task,	  but	  rather	  a	  
multi-­‐label	  classification	  task.	  Given	  k	  categories,	  for	  each	  paper	  we	  would	  like	  
to	  predict	  a	  binary	  vector	  of	  length	  k	  indicating	  which	  biological	  categories	  the	  
paper	  is	  associated	  with.	  

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	  

	  
f
o
s
n
o
i
l
l
i

M

 

	  

s
d
r
o
W

Features 

Dictionary	  Size	  

Abstract	  
Fulltext	  

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	  1	  2	  3	  4	  
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
1	  The	  bigram	  feature	  set	  contains	  both	  unigrams	  and	  bigrams,	  while	  the	  trigram	  feature	  set	  contains	  

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	  1:	  Feature	  set	  size.	  

Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	  	  Triage	  Task:	  Based	  on	  its	  abstract,	  can	  we	  determine	  if	  a	  new	  paper	  contains	  	  
relevant	  information?	  This	  is	  a	  binary	  classification	  task	  where	  a	  label	  of	  1	  
indicates	  that	  a	  paper	  contains	  relevant	  information	  and	  a	  label	  of	  -­‐1	  indicates	  
that	  a	  paper	  does	  not	  contain	  relevant	  information.	  
2)	  	  Multi-­‐label	  Classification	  Task:	  Assuming	  a	  paper	  has	  relevant	  content,	  
based	  on	  its	  fulltext,	  can	  we	  determine	  which	  biological	  category	  (or	  categories)	  
can	  be	  associated	  with	  it?	  Since	  each	  paper	  can	  be	  associated	  with	  multiple	  
biological	  categories,	  this	  is	  not	  a	  multi-­‐class	  classification	  task,	  but	  rather	  a	  
multi-­‐label	  classification	  task.	  Given	  k	  categories,	  for	  each	  paper	  we	  would	  like	  
to	  predict	  a	  binary	  vector	  of	  length	  k	  indicating	  which	  biological	  categories	  the	  
paper	  is	  associated	  with.	  

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	  

	  
f
o
s
n
o
i
l
l
i

M

 

	  

s
d
r
o
W

Features 

Dictionary	  Size	  

Abstract	  
Fulltext	  

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	  1	  2	  3	  4	  
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
1	  The	  bigram	  feature	  set	  contains	  both	  unigrams	  and	  bigrams,	  while	  the	  trigram	  feature	  set	  contains	  

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	  1:	  Feature	  set	  size.	  

0.6 

0.4 

0.2 

 

 
 

 

n

i
 
t
n
e
m
e
v
o
r
p
m

y
c
a
r
u
c
c
A
n
o
i
t
a
d

 

I
 

%

i
l
a
V

Effect of Preprocessing on  

Prediction Accuracy 

Aliased and Stemmed 
Stemmed 
Aliased 

0 

-0.2 

Naïve 
Bayes 

SVM 

Logistic 
Regression 

computational constraints. Figure 1 shows 
the distribution of dictionary sizes for each 
of our feature sets. 

We experimented with two data 
preprocessing steps: stemming and alias 
replacement. We used the porter22 stemming 
algorithm to stem all of the words in the 
corpus. We also wrote code to replace any 
biological terms or aliases with standardized 
names, including gene names and Gene 
Ontology3, Yeast Phenotype Ontology4, and 
CHEBI5 chemical terms. To determine 
whether these preprocessing steps increase 

Figure	  2:	  Effect	  of	  preprocessing	  on	  validation	  set	  
accuracy	  relative	  to	  no	  preprocessing	  .	  

We started by training four types of classifiers6 on the triage task: logistic 

the accuracy of our learning algorithms, we compared performance on three baseline 

both aliasing and stemming words has a positive effect on performance. 
 
Models 

unigram models to	  performance	  with	  no	  preprocessing (see Figure 2) and found that 
regression, SVM, Naïve Bayes, and KNN.	  Logistic	  regression	  and	  SVM	  outperform	  the	  
other	  two	  models,	  and	  in	  fact	  perform	  nearly	  identically.	  We	  chose	  to	  continue	  our	  
analysis	  of	  the	  multi-­‐label	  classification	  task	  with	  logistic	  regression	  because	  logistic	  
regression	  models	  the	  probability	  that	  a	  datapoint	  has	  a	  label	  of	  1.	  This	  can	  be	  used	  
as	  a	  confidence	  score,	  allowing	  us	  determine	  how	  confident	  we	  are	  in	  a	  given	  
prediction.	  Because	  an	  SVM	  only	  takes	  into	  account	  the	  support	  vectors,	  a	  small	  
subset	  of	  the	  training	  data,	  it	  is	  difficult	  to	  generate	  a	  meaningful	  confidence	  score	  
for	  points	  that	  aren’t	  support	  vectors.	  	  Furthermore,	  the	  logistic	  regression	  model	  
allows	  us	  to	  easily	  tune	  the	  false	  positive/false	  negative	  tradeoff,	  an	  important	  
factor	  for	  this	  task. 
the logistic regression model. According to the logistic regression model, the probability	  
that	  𝑥!	  has	  label	  𝑦! =1	  is	  modelled	  as 
𝑃𝑦! =1𝑥!;𝜃,𝑏 =ℎ!𝜃,𝑏 =
1+exp−𝜃!𝑥! −𝑏	  
All	  features	  are	  normalized	  by	  their	  𝐿!	  norm,	  i.e.	  we	  scale	  each	  feature	  so	  that	  
=1	  for	  𝑗=1,…,𝑑.	  We	  did	  not	  normalize	  individual	  documents	  because	  
𝑥!! !
!!!!
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
2	  https://pypi.python.org/pypi/stemming/1.0	  
3	  http://geneontology.org/	  
4	  http://www.yeastgenome.org/ontology/phenotype/ypo/overview	  
5	  http://www.ebi.ac.uk/chebi	  
6	  Scikit-­‐learn:	  Machine	  Learning	  in	  Python,	  Pedregosa	  et	  al.,	  JMLR	  12,	  pp.	  2825-­‐2830,	  2011.	  
	  

 
false negatives. False negatives correspond to missed information, which we would like 
to avoid. In order to drive the number of false negatives down, we made a modification to 

For our triage task, it’s extremely important for our classifier to generate very few 

1

Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	  	  Triage	  Task:	  Based	  on	  its	  abstract,	  can	  we	  determine	  if	  a	  new	  paper	  contains	  	  
relevant	  information?	  This	  is	  a	  binary	  classification	  task	  where	  a	  label	  of	  1	  
indicates	  that	  a	  paper	  contains	  relevant	  information	  and	  a	  label	  of	  -­‐1	  indicates	  
that	  a	  paper	  does	  not	  contain	  relevant	  information.	  
2)	  	  Multi-­‐label	  Classification	  Task:	  Assuming	  a	  paper	  has	  relevant	  content,	  
based	  on	  its	  fulltext,	  can	  we	  determine	  which	  biological	  category	  (or	  categories)	  
can	  be	  associated	  with	  it?	  Since	  each	  paper	  can	  be	  associated	  with	  multiple	  
biological	  categories,	  this	  is	  not	  a	  multi-­‐class	  classification	  task,	  but	  rather	  a	  
multi-­‐label	  classification	  task.	  Given	  k	  categories,	  for	  each	  paper	  we	  would	  like	  
to	  predict	  a	  binary	  vector	  of	  length	  k	  indicating	  which	  biological	  categories	  the	  
paper	  is	  associated	  with.	  

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	  

	  
f
o
s
n
o
i
l
l
i

M

 

	  

s
d
r
o
W

Features 

Dictionary	  Size	  

Abstract	  
Fulltext	  

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	  1	  2	  3	  4	  
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
1	  The	  bigram	  feature	  set	  contains	  both	  unigrams	  and	  bigrams,	  while	  the	  trigram	  feature	  set	  contains	  

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	  1:	  Feature	  set	  size.	  

0.6 

0.4 

0.2 

 

 
 

 

n

i
 
t
n
e
m
e
v
o
r
p
m

y
c
a
r
u
c
c
A
n
o
i
t
a
d

 

I
 

%

i
l
a
V

Effect of Preprocessing on  

Prediction Accuracy 

Aliased and Stemmed 
Stemmed 
Aliased 

0 

-0.2 

Naïve 
Bayes 

SVM 

Logistic 
Regression 

computational constraints. Figure 1 shows 
the distribution of dictionary sizes for each 
of our feature sets. 

We experimented with two data 
preprocessing steps: stemming and alias 
replacement. We used the porter22 stemming 
algorithm to stem all of the words in the 
corpus. We also wrote code to replace any 
biological terms or aliases with standardized 
names, including gene names and Gene 
Ontology3, Yeast Phenotype Ontology4, and 
CHEBI5 chemical terms. To determine 
whether these preprocessing steps increase 

Figure	  2:	  Effect	  of	  preprocessing	  on	  validation	  set	  
accuracy	  relative	  to	  no	  preprocessing	  .	  

We started by training four types of classifiers6 on the triage task: logistic 

the accuracy of our learning algorithms, we compared performance on three baseline 

both aliasing and stemming words has a positive effect on performance. 
 
Models 

unigram models to	  performance	  with	  no	  preprocessing (see Figure 2) and found that 
regression, SVM, Naïve Bayes, and KNN.	  Logistic	  regression	  and	  SVM	  outperform	  the	  
other	  two	  models,	  and	  in	  fact	  perform	  nearly	  identically.	  We	  chose	  to	  continue	  our	  
analysis	  of	  the	  multi-­‐label	  classification	  task	  with	  logistic	  regression	  because	  logistic	  
regression	  models	  the	  probability	  that	  a	  datapoint	  has	  a	  label	  of	  1.	  This	  can	  be	  used	  
as	  a	  confidence	  score,	  allowing	  us	  determine	  how	  confident	  we	  are	  in	  a	  given	  
prediction.	  Because	  an	  SVM	  only	  takes	  into	  account	  the	  support	  vectors,	  a	  small	  
subset	  of	  the	  training	  data,	  it	  is	  difficult	  to	  generate	  a	  meaningful	  confidence	  score	  
for	  points	  that	  aren’t	  support	  vectors.	  	  Furthermore,	  the	  logistic	  regression	  model	  
allows	  us	  to	  easily	  tune	  the	  false	  positive/false	  negative	  tradeoff,	  an	  important	  
factor	  for	  this	  task. 
the logistic regression model. According to the logistic regression model, the probability	  
that	  𝑥!	  has	  label	  𝑦! =1	  is	  modelled	  as 
𝑃𝑦! =1𝑥!;𝜃,𝑏 =ℎ!𝜃,𝑏 =
1+exp−𝜃!𝑥! −𝑏	  
All	  features	  are	  normalized	  by	  their	  𝐿!	  norm,	  i.e.	  we	  scale	  each	  feature	  so	  that	  
=1	  for	  𝑗=1,…,𝑑.	  We	  did	  not	  normalize	  individual	  documents	  because	  
𝑥!! !
!!!!
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
2	  https://pypi.python.org/pypi/stemming/1.0	  
3	  http://geneontology.org/	  
4	  http://www.yeastgenome.org/ontology/phenotype/ypo/overview	  
5	  http://www.ebi.ac.uk/chebi	  
6	  Scikit-­‐learn:	  Machine	  Learning	  in	  Python,	  Pedregosa	  et	  al.,	  JMLR	  12,	  pp.	  2825-­‐2830,	  2011.	  
	  

 
false negatives. False negatives correspond to missed information, which we would like 
to avoid. In order to drive the number of false negatives down, we made a modification to 

For our triage task, it’s extremely important for our classifier to generate very few 

1

they	  are	  all	  abstracts	  and	  have	  approximately	  the	  same	  length.	  We	  used	  𝐿!	  
regularization,	  so	  the	  classical	  𝐿!-­‐regularized	  logistic	  regression	  negative	  log-­‐
likelihood	  is	  ℓ𝓁𝜃,𝑏 =−
+𝜆2 𝜃 !!	  
𝑦!logℎ!𝜃,𝑏 + 1−𝑦!
!
However,	  we	  are	  interested	  in	  a	  slightly	  different	  problem	  where	  we	  minimize	  the	  
error	  on	  negative	  examples	  𝒩= 1≤𝑖≤𝑛𝑦! =0	  subject	  to	  the	  error	  on	  
!!!
positive	  examples	  𝒫= 1≤𝑖≤𝑛𝑦! =1	  being	  below	  an	  acceptable	  threshold.	  
This	  is	  given	  by	  the	  problem	  
	   min!,!  	  
−
	  
!∈𝒩
	  subject  to	  
−
It	  can	  be	  shown	  (through	  the	  Lagrangian)	  that,	  for	  an	  appropriately	  chosen	  𝛼≥0,	  
the	  above	  is	  equivalent	  to	  minimizing	  
!∈𝒫
−𝛼
−
logℎ!𝜃,𝑏
!∈𝒩
!∈𝒫

log1−ℎ!𝜃,𝑏
+𝜆2 𝜃 !!	  
+𝜆2 𝜃 !!	  

log1−ℎ!𝜃,𝑏
≤𝜏	  
logℎ!𝜃,𝑏
log1−ℎ!𝜃,𝑏

Therefore, we experimented with weighting our positive and negative classes in order to 
tune the number of false negatives on our validation set. 
 
Triage Task 

For the triage task, we separated our corpus of abstracts into training, validation, 
and testing sets. We used the validation set to select a model and tune parameters and the 
testing set to report on the predicted performance of our optimal classifier.  

We experimented with four different classifiers: naïve bayes, KNN, logistic 

regression, and SVM, our results are given in Table 1. Both logistic regression and SVM 
produced extremely similar models. This likely occurred because they both produce a 
linear decision boundary that takes into account each training example according to how 

well it is classified and can handle many correlated features because of 𝐿! regularization. 

Both KNN and Naïve Bayes seemed to suffer from spurious and correlated features. This 
trend is most clear for KNN; as more features were added with the bigram and trigram 
datasets, performance steeply dropped off.  

Table 1: Triage Task Results 

Model 

(120,000 training samples; 15,000 validation samples; 15,000 testing samples): 
Validation 
Accuracy 
81.69% 
82.08% 
82.17% 
72.20% 
56.00% 
59.30% 
86.67% 
87.08% 
87.22% 
86.51% 

Baseline  Training 
Accuracy 
84.28% 
53% 
86.90% 
53% 
53% 
89.65% 
53% 
53% 
53% 
53% 
53% 
53% 
53% 

Features 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 

Corpus 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 

NB 
NB 
NB 
KNN 
KNN 
KNN 
SVM 
SVM 
SVM 
LogReg 

89.57% 
94.19% 
96.41% 
89.35% 

- 
- 
- 

Validation 
Sensitivity 

Validation 
Specificity 

.730 
.734 
.734 

- 
- 
- 

.832 
.841 
.842 
.833 

.891 
.895 
.897 

- 
- 
- 

.896 
.896 
.898 
.893 

Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	  	  Triage	  Task:	  Based	  on	  its	  abstract,	  can	  we	  determine	  if	  a	  new	  paper	  contains	  	  
relevant	  information?	  This	  is	  a	  binary	  classification	  task	  where	  a	  label	  of	  1	  
indicates	  that	  a	  paper	  contains	  relevant	  information	  and	  a	  label	  of	  -­‐1	  indicates	  
that	  a	  paper	  does	  not	  contain	  relevant	  information.	  
2)	  	  Multi-­‐label	  Classification	  Task:	  Assuming	  a	  paper	  has	  relevant	  content,	  
based	  on	  its	  fulltext,	  can	  we	  determine	  which	  biological	  category	  (or	  categories)	  
can	  be	  associated	  with	  it?	  Since	  each	  paper	  can	  be	  associated	  with	  multiple	  
biological	  categories,	  this	  is	  not	  a	  multi-­‐class	  classification	  task,	  but	  rather	  a	  
multi-­‐label	  classification	  task.	  Given	  k	  categories,	  for	  each	  paper	  we	  would	  like	  
to	  predict	  a	  binary	  vector	  of	  length	  k	  indicating	  which	  biological	  categories	  the	  
paper	  is	  associated	  with.	  

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	  

	  
f
o
s
n
o
i
l
l
i

M

 

	  

s
d
r
o
W

Features 

Dictionary	  Size	  

Abstract	  
Fulltext	  

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	  1	  2	  3	  4	  
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
1	  The	  bigram	  feature	  set	  contains	  both	  unigrams	  and	  bigrams,	  while	  the	  trigram	  feature	  set	  contains	  

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	  1:	  Feature	  set	  size.	  

0.6 

0.4 

0.2 

 

 
 

 

n

i
 
t
n
e
m
e
v
o
r
p
m

y
c
a
r
u
c
c
A
n
o
i
t
a
d

 

I
 

%

i
l
a
V

Effect of Preprocessing on  

Prediction Accuracy 

Aliased and Stemmed 
Stemmed 
Aliased 

0 

-0.2 

Naïve 
Bayes 

SVM 

Logistic 
Regression 

computational constraints. Figure 1 shows 
the distribution of dictionary sizes for each 
of our feature sets. 

We experimented with two data 
preprocessing steps: stemming and alias 
replacement. We used the porter22 stemming 
algorithm to stem all of the words in the 
corpus. We also wrote code to replace any 
biological terms or aliases with standardized 
names, including gene names and Gene 
Ontology3, Yeast Phenotype Ontology4, and 
CHEBI5 chemical terms. To determine 
whether these preprocessing steps increase 

Figure	  2:	  Effect	  of	  preprocessing	  on	  validation	  set	  
accuracy	  relative	  to	  no	  preprocessing	  .	  

We started by training four types of classifiers6 on the triage task: logistic 

the accuracy of our learning algorithms, we compared performance on three baseline 

both aliasing and stemming words has a positive effect on performance. 
 
Models 

unigram models to	  performance	  with	  no	  preprocessing (see Figure 2) and found that 
regression, SVM, Naïve Bayes, and KNN.	  Logistic	  regression	  and	  SVM	  outperform	  the	  
other	  two	  models,	  and	  in	  fact	  perform	  nearly	  identically.	  We	  chose	  to	  continue	  our	  
analysis	  of	  the	  multi-­‐label	  classification	  task	  with	  logistic	  regression	  because	  logistic	  
regression	  models	  the	  probability	  that	  a	  datapoint	  has	  a	  label	  of	  1.	  This	  can	  be	  used	  
as	  a	  confidence	  score,	  allowing	  us	  determine	  how	  confident	  we	  are	  in	  a	  given	  
prediction.	  Because	  an	  SVM	  only	  takes	  into	  account	  the	  support	  vectors,	  a	  small	  
subset	  of	  the	  training	  data,	  it	  is	  difficult	  to	  generate	  a	  meaningful	  confidence	  score	  
for	  points	  that	  aren’t	  support	  vectors.	  	  Furthermore,	  the	  logistic	  regression	  model	  
allows	  us	  to	  easily	  tune	  the	  false	  positive/false	  negative	  tradeoff,	  an	  important	  
factor	  for	  this	  task. 
the logistic regression model. According to the logistic regression model, the probability	  
that	  𝑥!	  has	  label	  𝑦! =1	  is	  modelled	  as 
𝑃𝑦! =1𝑥!;𝜃,𝑏 =ℎ!𝜃,𝑏 =
1+exp−𝜃!𝑥! −𝑏	  
All	  features	  are	  normalized	  by	  their	  𝐿!	  norm,	  i.e.	  we	  scale	  each	  feature	  so	  that	  
=1	  for	  𝑗=1,…,𝑑.	  We	  did	  not	  normalize	  individual	  documents	  because	  
𝑥!! !
!!!!
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
2	  https://pypi.python.org/pypi/stemming/1.0	  
3	  http://geneontology.org/	  
4	  http://www.yeastgenome.org/ontology/phenotype/ypo/overview	  
5	  http://www.ebi.ac.uk/chebi	  
6	  Scikit-­‐learn:	  Machine	  Learning	  in	  Python,	  Pedregosa	  et	  al.,	  JMLR	  12,	  pp.	  2825-­‐2830,	  2011.	  
	  

 
false negatives. False negatives correspond to missed information, which we would like 
to avoid. In order to drive the number of false negatives down, we made a modification to 

For our triage task, it’s extremely important for our classifier to generate very few 

1

they	  are	  all	  abstracts	  and	  have	  approximately	  the	  same	  length.	  We	  used	  𝐿!	  
regularization,	  so	  the	  classical	  𝐿!-­‐regularized	  logistic	  regression	  negative	  log-­‐
likelihood	  is	  ℓ𝓁𝜃,𝑏 =−
+𝜆2 𝜃 !!	  
𝑦!logℎ!𝜃,𝑏 + 1−𝑦!
!
However,	  we	  are	  interested	  in	  a	  slightly	  different	  problem	  where	  we	  minimize	  the	  
error	  on	  negative	  examples	  𝒩= 1≤𝑖≤𝑛𝑦! =0	  subject	  to	  the	  error	  on	  
!!!
positive	  examples	  𝒫= 1≤𝑖≤𝑛𝑦! =1	  being	  below	  an	  acceptable	  threshold.	  
This	  is	  given	  by	  the	  problem	  
	   min!,!  	  
−
	  
!∈𝒩
	  subject  to	  
−
It	  can	  be	  shown	  (through	  the	  Lagrangian)	  that,	  for	  an	  appropriately	  chosen	  𝛼≥0,	  
the	  above	  is	  equivalent	  to	  minimizing	  
!∈𝒫
−𝛼
−
logℎ!𝜃,𝑏
!∈𝒩
!∈𝒫

log1−ℎ!𝜃,𝑏
+𝜆2 𝜃 !!	  
+𝜆2 𝜃 !!	  

log1−ℎ!𝜃,𝑏
≤𝜏	  
logℎ!𝜃,𝑏
log1−ℎ!𝜃,𝑏

Therefore, we experimented with weighting our positive and negative classes in order to 
tune the number of false negatives on our validation set. 
 
Triage Task 

For the triage task, we separated our corpus of abstracts into training, validation, 
and testing sets. We used the validation set to select a model and tune parameters and the 
testing set to report on the predicted performance of our optimal classifier.  

We experimented with four different classifiers: naïve bayes, KNN, logistic 

regression, and SVM, our results are given in Table 1. Both logistic regression and SVM 
produced extremely similar models. This likely occurred because they both produce a 
linear decision boundary that takes into account each training example according to how 

well it is classified and can handle many correlated features because of 𝐿! regularization. 

Both KNN and Naïve Bayes seemed to suffer from spurious and correlated features. This 
trend is most clear for KNN; as more features were added with the bigram and trigram 
datasets, performance steeply dropped off.  

Table 1: Triage Task Results 

Model 

(120,000 training samples; 15,000 validation samples; 15,000 testing samples): 
Validation 
Accuracy 
81.69% 
82.08% 
82.17% 
72.20% 
56.00% 
59.30% 
86.67% 
87.08% 
87.22% 
86.51% 

Baseline  Training 
Accuracy 
84.28% 
53% 
86.90% 
53% 
53% 
89.65% 
53% 
53% 
53% 
53% 
53% 
53% 
53% 

Features 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 

Corpus 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 

NB 
NB 
NB 
KNN 
KNN 
KNN 
SVM 
SVM 
SVM 
LogReg 

89.57% 
94.19% 
96.41% 
89.35% 

- 
- 
- 

Validation 
Sensitivity 

Validation 
Specificity 

.730 
.734 
.734 

- 
- 
- 

.832 
.841 
.842 
.833 

.891 
.895 
.897 

- 
- 
- 

.896 
.896 
.898 
.893 

.841 
.842 

53% 
53% 

93.56% 
95.66% 

87.01% 
87.17% 

Abstract 
Abstract 

Bigrams 
Trigrams 

.895 
LogReg 
LogReg 
.897 
* Baseline shows the accuracy of classifying each sample to belong to the most common training class. 
Since curators must read all positively triaged papers, any false positives can 
 
easily be corrected during the annotation process. This means we are primarily interested 
in our classifier’s negative predictions. False negatives correspond to papers whose 
information is missed, which we want to avoid. Furthermore, since curators no longer 
need to read papers that are triaged as negative, the rate of negative predictions measures 

the time savings of our classifier. By changing the class weight parameter 𝛼 given in the 

equations above, we were able to tune the number of false negative samples on our 
validation set. Table 2 shows validation statistics using trigram features on the abstract 
corpus for the triage task. Although validation accuracy may decrease, tuning alpha 
allows us to decrease the expected number of false negative predictions, which results in 
a better classifier for the task.  

Table 2: Alpha Tuning Results 

Alpha 

1 
0.1 
0.001 

Validation 
Accuracy 
87.17% 
85.99% 
76.51% 

TP 
FN 
5839  1092 
877 
6054 
6691 
240 

TN 
FP 
833 
7236 
1224  6845 
3283  4786 

False Negative/ 

Predicted Negative 

Predicted Negative/ 

All 

7.28% 
5.84% 
1.6% 

55.52% 
51.48% 
33.51% 

Based on these results, we chose a logistic regression classifier using trigrams and an 
alpha parameter of 0.1 as our optimal classifier. Table 3 shows the predicted performance 
of this classifier based on the testing set. 

Table 3: Testing Results on Triage Task 

Testing 
Accuracy 

85.8% 

TP 
6330 

FN 
687 

FP 
TN 
1443  6540 

False Negative/ 

Predicted Negative 

4.58% 

Predicted Negative/ 

All 
43.6% 

We split the fulltext corpus in a similar manner as the abstract corpus, creating 

 
Multi-label Classification Task 
 
training, validation, and testing sets. We chose to approach the multi-label classification 
problem by training 5 classifiers, one for each biological category. Results are shown in 
Table 4. Overall, this task seemed to be much more difficult than the triage task. This 
may have been due to the fact that our dataset had relatively few positive examples for 
each label. Performance for one of the five categories was fairly poor, with our logistic 
regression classifier performing less accurately than a simple baseline classifier that 
assigns every sample to the most common training class. Performance on the remaining 
three categories was adequate, but low. This task requires further work.  

(70,000 training samples; 7,000 validation samples; 7,000 testing samples) 

Table 4: Multi-label Classification Task Results 

Model 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 

Category Corpus  Features  Baseline 

GO  Abstract Unigrams  88% 
GO  Abstract Trigrams  88% 
GO 
Fulltext  Unigrams  88% 
Phen.  Abstract Unigrams  79% 
Phen.  Abstract Trigrams  79% 
Phen  Fulltext  Unigrams  79% 
Phys  Abstract Unigrams  91% 

Training  
Accuracy 
96.41% 
100.00% 
99.98% 
97.23% 
100.00% 
100.00% 
97.80% 

Validation  
Accuracy 
82.67% 
86.54% 
85.23% 
87.82% 
91.58% 
90.55% 
91.54% 

Validation  
Sensitivity 
.867 
.970 
.906 
.913 
.987 
.952 
.928 

Validation  
Specificity 
.575 
.395 
.507 
.440 
.196 
.300 
.654 

Machine Learning Methods for Biological Data Curation 

Kelley Paskov (kpaskov@stanford.edu) 

Background 

Biological publications are being produced more rapidly than any human could 
possibly read them, so modern biologists rely heavily on the manually curated data in 
biological databases. This data is collected by expert biocurators who read literature 
focused on a particular organism, disease, or process and extract key information in a 
structured format using ontologies and controlled vocabularies. We would like to 
automate parts of this process and focus on two prediction tasks:  

1)	  	  Triage	  Task:	  Based	  on	  its	  abstract,	  can	  we	  determine	  if	  a	  new	  paper	  contains	  	  
relevant	  information?	  This	  is	  a	  binary	  classification	  task	  where	  a	  label	  of	  1	  
indicates	  that	  a	  paper	  contains	  relevant	  information	  and	  a	  label	  of	  -­‐1	  indicates	  
that	  a	  paper	  does	  not	  contain	  relevant	  information.	  
2)	  	  Multi-­‐label	  Classification	  Task:	  Assuming	  a	  paper	  has	  relevant	  content,	  
based	  on	  its	  fulltext,	  can	  we	  determine	  which	  biological	  category	  (or	  categories)	  
can	  be	  associated	  with	  it?	  Since	  each	  paper	  can	  be	  associated	  with	  multiple	  
biological	  categories,	  this	  is	  not	  a	  multi-­‐class	  classification	  task,	  but	  rather	  a	  
multi-­‐label	  classification	  task.	  Given	  k	  categories,	  for	  each	  paper	  we	  would	  like	  
to	  predict	  a	  binary	  vector	  of	  length	  k	  indicating	  which	  biological	  categories	  the	  
paper	  is	  associated	  with.	  

Some promising work has been done on automated paper triage and gene detection 

within text, particularly related to the BioCreative and BioNLP competitions aimed at 
evaluating the state of the art in biomedical text mining. But tools produced from these 
competitions often have too narrow a focus or are unable to handle datasets on the scale 
of well-established model organism databases. 

 
Data 

	  

	  
f
o
s
n
o
i
l
l
i

M

 

	  

s
d
r
o
W

Features 

Dictionary	  Size	  

Abstract	  
Fulltext	  

We have collected the full text of 70,000 articles curated by the Saccharomyces 
Genome Database (SGD) over the past 20 years, and the multi-category label for each 
article along 5 biological categories (ie we have a multi-label classification task with k=5 
categories). We have also collected the abstracts from 150,000 articles, including the 
70,000 above and an additional 80,000 articles that do not contain annotatable 
information. 

0	  1	  2	  3	  4	  
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
1	  The	  bigram	  feature	  set	  contains	  both	  unigrams	  and	  bigrams,	  while	  the	  trigram	  feature	  set	  contains	  

We chose to represent each paper as a bag of n-
grams, capturing the number of occurrences of each n-
gram in the text of the paper. This representation is 
effective and is easy to compute over the large amounts of 
text in our two datasets - currently over 4 gigabytes. For 
our abstract corpus, we experimented with unigram, 
bigram and trigram1 representations. For our fulltext 
corpus, we used only the unigram representation due to 

Figure	  1:	  Feature	  set	  size.	  

0.6 

0.4 

0.2 

 

 
 

 

n

i
 
t
n
e
m
e
v
o
r
p
m

y
c
a
r
u
c
c
A
n
o
i
t
a
d

 

I
 

%

i
l
a
V

Effect of Preprocessing on  

Prediction Accuracy 

Aliased and Stemmed 
Stemmed 
Aliased 

0 

-0.2 

Naïve 
Bayes 

SVM 

Logistic 
Regression 

computational constraints. Figure 1 shows 
the distribution of dictionary sizes for each 
of our feature sets. 

We experimented with two data 
preprocessing steps: stemming and alias 
replacement. We used the porter22 stemming 
algorithm to stem all of the words in the 
corpus. We also wrote code to replace any 
biological terms or aliases with standardized 
names, including gene names and Gene 
Ontology3, Yeast Phenotype Ontology4, and 
CHEBI5 chemical terms. To determine 
whether these preprocessing steps increase 

Figure	  2:	  Effect	  of	  preprocessing	  on	  validation	  set	  
accuracy	  relative	  to	  no	  preprocessing	  .	  

We started by training four types of classifiers6 on the triage task: logistic 

the accuracy of our learning algorithms, we compared performance on three baseline 

both aliasing and stemming words has a positive effect on performance. 
 
Models 

unigram models to	  performance	  with	  no	  preprocessing (see Figure 2) and found that 
regression, SVM, Naïve Bayes, and KNN.	  Logistic	  regression	  and	  SVM	  outperform	  the	  
other	  two	  models,	  and	  in	  fact	  perform	  nearly	  identically.	  We	  chose	  to	  continue	  our	  
analysis	  of	  the	  multi-­‐label	  classification	  task	  with	  logistic	  regression	  because	  logistic	  
regression	  models	  the	  probability	  that	  a	  datapoint	  has	  a	  label	  of	  1.	  This	  can	  be	  used	  
as	  a	  confidence	  score,	  allowing	  us	  determine	  how	  confident	  we	  are	  in	  a	  given	  
prediction.	  Because	  an	  SVM	  only	  takes	  into	  account	  the	  support	  vectors,	  a	  small	  
subset	  of	  the	  training	  data,	  it	  is	  difficult	  to	  generate	  a	  meaningful	  confidence	  score	  
for	  points	  that	  aren’t	  support	  vectors.	  	  Furthermore,	  the	  logistic	  regression	  model	  
allows	  us	  to	  easily	  tune	  the	  false	  positive/false	  negative	  tradeoff,	  an	  important	  
factor	  for	  this	  task. 
the logistic regression model. According to the logistic regression model, the probability	  
that	  𝑥!	  has	  label	  𝑦! =1	  is	  modelled	  as 
𝑃𝑦! =1𝑥!;𝜃,𝑏 =ℎ!𝜃,𝑏 =
1+exp−𝜃!𝑥! −𝑏	  
All	  features	  are	  normalized	  by	  their	  𝐿!	  norm,	  i.e.	  we	  scale	  each	  feature	  so	  that	  
=1	  for	  𝑗=1,…,𝑑.	  We	  did	  not	  normalize	  individual	  documents	  because	  
𝑥!! !
!!!!
	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
2	  https://pypi.python.org/pypi/stemming/1.0	  
3	  http://geneontology.org/	  
4	  http://www.yeastgenome.org/ontology/phenotype/ypo/overview	  
5	  http://www.ebi.ac.uk/chebi	  
6	  Scikit-­‐learn:	  Machine	  Learning	  in	  Python,	  Pedregosa	  et	  al.,	  JMLR	  12,	  pp.	  2825-­‐2830,	  2011.	  
	  

 
false negatives. False negatives correspond to missed information, which we would like 
to avoid. In order to drive the number of false negatives down, we made a modification to 

For our triage task, it’s extremely important for our classifier to generate very few 

1

they	  are	  all	  abstracts	  and	  have	  approximately	  the	  same	  length.	  We	  used	  𝐿!	  
regularization,	  so	  the	  classical	  𝐿!-­‐regularized	  logistic	  regression	  negative	  log-­‐
likelihood	  is	  ℓ𝓁𝜃,𝑏 =−
+𝜆2 𝜃 !!	  
𝑦!logℎ!𝜃,𝑏 + 1−𝑦!
!
However,	  we	  are	  interested	  in	  a	  slightly	  different	  problem	  where	  we	  minimize	  the	  
error	  on	  negative	  examples	  𝒩= 1≤𝑖≤𝑛𝑦! =0	  subject	  to	  the	  error	  on	  
!!!
positive	  examples	  𝒫= 1≤𝑖≤𝑛𝑦! =1	  being	  below	  an	  acceptable	  threshold.	  
This	  is	  given	  by	  the	  problem	  
	   min!,!  	  
−
	  
!∈𝒩
	  subject  to	  
−
It	  can	  be	  shown	  (through	  the	  Lagrangian)	  that,	  for	  an	  appropriately	  chosen	  𝛼≥0,	  
the	  above	  is	  equivalent	  to	  minimizing	  
!∈𝒫
−𝛼
−
logℎ!𝜃,𝑏
!∈𝒩
!∈𝒫

log1−ℎ!𝜃,𝑏
+𝜆2 𝜃 !!	  
+𝜆2 𝜃 !!	  

log1−ℎ!𝜃,𝑏
≤𝜏	  
logℎ!𝜃,𝑏
log1−ℎ!𝜃,𝑏

Therefore, we experimented with weighting our positive and negative classes in order to 
tune the number of false negatives on our validation set. 
 
Triage Task 

For the triage task, we separated our corpus of abstracts into training, validation, 
and testing sets. We used the validation set to select a model and tune parameters and the 
testing set to report on the predicted performance of our optimal classifier.  

We experimented with four different classifiers: naïve bayes, KNN, logistic 

regression, and SVM, our results are given in Table 1. Both logistic regression and SVM 
produced extremely similar models. This likely occurred because they both produce a 
linear decision boundary that takes into account each training example according to how 

well it is classified and can handle many correlated features because of 𝐿! regularization. 

Both KNN and Naïve Bayes seemed to suffer from spurious and correlated features. This 
trend is most clear for KNN; as more features were added with the bigram and trigram 
datasets, performance steeply dropped off.  

Table 1: Triage Task Results 

Model 

(120,000 training samples; 15,000 validation samples; 15,000 testing samples): 
Validation 
Accuracy 
81.69% 
82.08% 
82.17% 
72.20% 
56.00% 
59.30% 
86.67% 
87.08% 
87.22% 
86.51% 

Baseline  Training 
Accuracy 
84.28% 
53% 
86.90% 
53% 
53% 
89.65% 
53% 
53% 
53% 
53% 
53% 
53% 
53% 

Features 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 
Bigrams 
Trigrams 
Unigrams 

Corpus 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 
Abstract 

NB 
NB 
NB 
KNN 
KNN 
KNN 
SVM 
SVM 
SVM 
LogReg 

89.57% 
94.19% 
96.41% 
89.35% 

- 
- 
- 

Validation 
Sensitivity 

Validation 
Specificity 

.730 
.734 
.734 

- 
- 
- 

.832 
.841 
.842 
.833 

.891 
.895 
.897 

- 
- 
- 

.896 
.896 
.898 
.893 

.841 
.842 

53% 
53% 

93.56% 
95.66% 

87.01% 
87.17% 

Abstract 
Abstract 

Bigrams 
Trigrams 

.895 
LogReg 
LogReg 
.897 
* Baseline shows the accuracy of classifying each sample to belong to the most common training class. 
Since curators must read all positively triaged papers, any false positives can 
 
easily be corrected during the annotation process. This means we are primarily interested 
in our classifier’s negative predictions. False negatives correspond to papers whose 
information is missed, which we want to avoid. Furthermore, since curators no longer 
need to read papers that are triaged as negative, the rate of negative predictions measures 

the time savings of our classifier. By changing the class weight parameter 𝛼 given in the 

equations above, we were able to tune the number of false negative samples on our 
validation set. Table 2 shows validation statistics using trigram features on the abstract 
corpus for the triage task. Although validation accuracy may decrease, tuning alpha 
allows us to decrease the expected number of false negative predictions, which results in 
a better classifier for the task.  

Table 2: Alpha Tuning Results 

Alpha 

1 
0.1 
0.001 

Validation 
Accuracy 
87.17% 
85.99% 
76.51% 

TP 
FN 
5839  1092 
877 
6054 
6691 
240 

TN 
FP 
833 
7236 
1224  6845 
3283  4786 

False Negative/ 

Predicted Negative 

Predicted Negative/ 

All 

7.28% 
5.84% 
1.6% 

55.52% 
51.48% 
33.51% 

Based on these results, we chose a logistic regression classifier using trigrams and an 
alpha parameter of 0.1 as our optimal classifier. Table 3 shows the predicted performance 
of this classifier based on the testing set. 

Table 3: Testing Results on Triage Task 

Testing 
Accuracy 

85.8% 

TP 
6330 

FN 
687 

FP 
TN 
1443  6540 

False Negative/ 

Predicted Negative 

4.58% 

Predicted Negative/ 

All 
43.6% 

We split the fulltext corpus in a similar manner as the abstract corpus, creating 

 
Multi-label Classification Task 
 
training, validation, and testing sets. We chose to approach the multi-label classification 
problem by training 5 classifiers, one for each biological category. Results are shown in 
Table 4. Overall, this task seemed to be much more difficult than the triage task. This 
may have been due to the fact that our dataset had relatively few positive examples for 
each label. Performance for one of the five categories was fairly poor, with our logistic 
regression classifier performing less accurately than a simple baseline classifier that 
assigns every sample to the most common training class. Performance on the remaining 
three categories was adequate, but low. This task requires further work.  

(70,000 training samples; 7,000 validation samples; 7,000 testing samples) 

Table 4: Multi-label Classification Task Results 

Model 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 

Category Corpus  Features  Baseline 

GO  Abstract Unigrams  88% 
GO  Abstract Trigrams  88% 
GO 
Fulltext  Unigrams  88% 
Phen.  Abstract Unigrams  79% 
Phen.  Abstract Trigrams  79% 
Phen  Fulltext  Unigrams  79% 
Phys  Abstract Unigrams  91% 

Training  
Accuracy 
96.41% 
100.00% 
99.98% 
97.23% 
100.00% 
100.00% 
97.80% 

Validation  
Accuracy 
82.67% 
86.54% 
85.23% 
87.82% 
91.58% 
90.55% 
91.54% 

Validation  
Sensitivity 
.867 
.970 
.906 
.913 
.987 
.952 
.928 

Validation  
Specificity 
.575 
.395 
.507 
.440 
.196 
.300 
.654 

Phys  Abstract Trigrams  91% 
Phys 
Fulltext  Unigrams  91% 
Gen.  Abstract Unigrams  91% 
Gen.  Abstract Trigrams  91% 
Gen. 
Fulltext  Unigrams  91% 
Lit.  Abstract Unigram  77% 
Lit.  Abstract Trigrams  77% 
Lit. 
Fulltext  Unigrams  77% 

100.00% 
99.78% 
100.0% 
100.00% 
99.79% 
97.75% 
100.00% 
99.45% 

92.27% 
92.68% 
89.11% 
92.38% 
91.67% 
90.33% 
90.85% 
89.66% 

.983 
.945 
.921 
.983 
.946 
.730 
.871 
.760 

.567 
.707 
.625 
.531 
.644 
.941 
.854 
.923 

* Baseline shows the accuracy of classifying each sample to belong to the most common training class. 

ROC	  Curve	  for	  GO	  Category	  

Unigrams	  
Bigrams	  
Trigrams	  
0.6	  
0.8	  

1	  

0	  

0.2	  

0.4	  

1-­‐Speci?icity	  

Figure	  3:	  Sensitivity	  as	  a	  function	  of	  
specificity	  for	  unigram,	  bigram,	  and	  trigram	  
features	  for	  the	  GO	  multi-­‐label	  category.	   

In Table 4, when comparing the 

performance of the abstract unigrams to the 
abstract trigrams, it appears that sensitivity 
increases while specificity decreases. However, 
this is because we are sampling different points 
along the ROC curve. When we plot the ROC 
curves for unigrams, bigrams, and trigrams (see 
Figure 3), we see that as we move to more 
complicated feature sets, the ROC curve 
improves. This is most likely because bigrams 
and trigrams have the power to capture 
informative phrases and multi-word biological 
terms that can’t be captured by unigrams alone.  
Based on these results, we chose to use 

the abstract trigrams dataset with a logistic 

LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 
LogReg 

0	  0.2	  0.4	  0.6	  0.8	  1	  

	  

y
t
i
v
i
t
i
s
n
e
S

regression model as our optimal classifier for this task. Table 5 shows predicted 
performance for the classifier for each category based on the testing set. 
Table 5: Testing Results on Multi-label Classification Task 

Category 

Baseline 

Testing Accuracy 

Testing Sensitivity  Testing Specificity 

GO 
Phen. 
Phys. 
Gen. 
Lit. 

88% 
79% 
91% 
91% 
77% 

85.52% 
91.58% 
92.33% 
91.95% 
90.45% 

.934 
.974 
.966 
.965 
.701 

.386 
.192 
.640 
.511 
.946 

 
Future Work 

There are several areas of this project that would benefit from future work. Due to 

the size of our datasets, we were only able to do parameter tuning at an order of 
magnitude scale. By tuning parameters at a more fine-grained level, we believe we could 
increase performance on both tasks. Our results on the multi-label classification task 
show that working with trigrams from the abstract corpus offers significant gains over 
working with unigrams. We believe that working with bigrams or trigrams from the 
fulltext corpus has the potential to dramatically increase performance by capturing 
information found in phrases and multi-word biological terms. However, this would 
require custom software because it would increase our feature size by at least an order of 
magnitude. Finally, the ultimate goal is to integrate our classifier into an active curation 
pipeline. We plan to present our results to the Saccharomyces Genome Database, and 
hope to eventually use these techniques to automate part of the biological data curation 
process. 

