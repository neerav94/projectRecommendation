Predicting Africa Soil Properties Using Machine

Learning Techniques

Iretiayo Akinola ∗, Thomas Dowd †,

Electrical Engineering

Stanford University, Stanford, CA 94305

Email: ∗iakinola@stanford.edu, †tjdowd@stanford.edu

Abstract—Different machine learning algorithms were as-
sessed for estimating ﬁve functional soil parameters (SOC content,
Calcium content, Phosphorous content, sand content, and pH
value). The algorithms used include variants of linear regression
and support vector regression. A closer look at the prediction
performance for each target revealed that apart from pH, which
consistently had worse performance, prediction for the other
soil properties was quite satisfactory (RMSE < 0.4). Applying
machine learning techniques to soil properties prediction has
shown a lot of promising and encouraging results. Getting more
data, domain knowledge and intuition, possibly from soil scien-
tist/experts, would surely maximize this potential for accurate soil
property prediction.

I.

INTRODUCTION

Soil functional properties (such as primary productivity,
nutrient and water retention, and resistance to erosion) in-
dicate a location’s ability to perform important ecological
services. Traditional methods of measuring and characterizing
soil properties require expensive and time-consuming scientiﬁc
procedures. Low cost measurements obtained through diffuse
reﬂectance infrared spectroscopy and remotely collected data
have the potential to quickly estimate these same characteris-
tics without the use of costly chemical resources. Inexpensive
characterization methods would allow large, data-sparse re-
gions to plan sustainable agricultural development and manage
local natural resources.

Our objective was to accurately model the relationship
between these inexpensive measurements and ﬁve soil char-
acteristics properties: Soil Organic Carbon (SOC) content,
Calcium content, Phosphorus content, sand content, and pH
value.

II. LITERATURE-REVIEW

Since the mid 1900s, pedotransfer functions (PTF), which
are predictive functions of certain soil properties using data
from soil surveys, have been used to predict soils in temperate
regions. However, soil prediction methods can be adapted
across climates. According to Minasnya (2011) [1], methods
developed in temperate regions can be applied for the soils
in the tropical regions albeit with adjustments to calibration
and choice of relevant available predictors. Prior to model
selection, a literature review was conducted on past work
modeling spectroscopy data to soil characteristic qualities.

et al. (2006) performed an analysis on using visible, near-
infrared, and mid-infrared absorbance data to determine soil
characteristic properties. Their method used partial
least-
square regression (PLSR) to model the system, and they were
most successful in developing predictions for pH, Organic
Carbon, Phosphorous, and sand content using mid-infrared
absorbance values. [2]

Rossel and Behrens (2009) experimented with a number
of machine learning methods to map full-range spectroscopy
data to similar soil characteristics. Support Vector Regression
provided the most accurate results for the three target variables
in question (SOC content, clay content and pH levels). [3]

More recent prediction of soil properties such as [1] and [4]
still embrace the linear model assumption for their research.
In this project, we explore how recent improved machine
learning techniques can be applied to the soil science domain
to improve the overall prediction performance.

III. DATASET

The dataset consisted of a collection of 1,157 soil sample
measures. Soil was collected from a variety of locations
in Africa. Each data point contained 3,594 features which
represent the following low-cost measurements:

•

•
•

3,578 mid-infrared absorbance measurements (wave-
lengths ranging from 7497.96 cm−1 - 599.76 cm−1).
Originally obtained via Diffuse Reﬂectance Infrared
Fourier transform Spectroscopy.
Depth of the soil sample (topsoil or subsoil)
Remotely-collected data including climate, topograph-
ical, vegetation index, surface temperature, and other
information about the sample collection site obtained
via satellite. All satellite data was mean centered and
scaled.

The data set contains monotonically adjusted values for the
ﬁve target variables (SOC content, Calcium content, Phospho-
rous content, sand content, and pH value) such that all target
variables take both positive and negative values.

The dataset was provided by the ”Africa Soil Property”

competition hosted on Kaggle.com.

IV. MODEL SELECTION

Pedotransfer functions and most other models used in
(predictive) soil science are linear regression based. Rossel

Noting the success of linear ﬁtting in other datasets map-
ping spectroscopy data to soil content characteristics, a number

Predicting Africa Soil Properties Using Machine

Learning Techniques

Iretiayo Akinola ∗, Thomas Dowd †,

Electrical Engineering

Stanford University, Stanford, CA 94305

Email: ∗iakinola@stanford.edu, †tjdowd@stanford.edu

Abstract—Different machine learning algorithms were as-
sessed for estimating ﬁve functional soil parameters (SOC content,
Calcium content, Phosphorous content, sand content, and pH
value). The algorithms used include variants of linear regression
and support vector regression. A closer look at the prediction
performance for each target revealed that apart from pH, which
consistently had worse performance, prediction for the other
soil properties was quite satisfactory (RMSE < 0.4). Applying
machine learning techniques to soil properties prediction has
shown a lot of promising and encouraging results. Getting more
data, domain knowledge and intuition, possibly from soil scien-
tist/experts, would surely maximize this potential for accurate soil
property prediction.

I.

INTRODUCTION

Soil functional properties (such as primary productivity,
nutrient and water retention, and resistance to erosion) in-
dicate a location’s ability to perform important ecological
services. Traditional methods of measuring and characterizing
soil properties require expensive and time-consuming scientiﬁc
procedures. Low cost measurements obtained through diffuse
reﬂectance infrared spectroscopy and remotely collected data
have the potential to quickly estimate these same characteris-
tics without the use of costly chemical resources. Inexpensive
characterization methods would allow large, data-sparse re-
gions to plan sustainable agricultural development and manage
local natural resources.

Our objective was to accurately model the relationship
between these inexpensive measurements and ﬁve soil char-
acteristics properties: Soil Organic Carbon (SOC) content,
Calcium content, Phosphorus content, sand content, and pH
value.

II. LITERATURE-REVIEW

Since the mid 1900s, pedotransfer functions (PTF), which
are predictive functions of certain soil properties using data
from soil surveys, have been used to predict soils in temperate
regions. However, soil prediction methods can be adapted
across climates. According to Minasnya (2011) [1], methods
developed in temperate regions can be applied for the soils
in the tropical regions albeit with adjustments to calibration
and choice of relevant available predictors. Prior to model
selection, a literature review was conducted on past work
modeling spectroscopy data to soil characteristic qualities.

et al. (2006) performed an analysis on using visible, near-
infrared, and mid-infrared absorbance data to determine soil
characteristic properties. Their method used partial
least-
square regression (PLSR) to model the system, and they were
most successful in developing predictions for pH, Organic
Carbon, Phosphorous, and sand content using mid-infrared
absorbance values. [2]

Rossel and Behrens (2009) experimented with a number
of machine learning methods to map full-range spectroscopy
data to similar soil characteristics. Support Vector Regression
provided the most accurate results for the three target variables
in question (SOC content, clay content and pH levels). [3]

More recent prediction of soil properties such as [1] and [4]
still embrace the linear model assumption for their research.
In this project, we explore how recent improved machine
learning techniques can be applied to the soil science domain
to improve the overall prediction performance.

III. DATASET

The dataset consisted of a collection of 1,157 soil sample
measures. Soil was collected from a variety of locations
in Africa. Each data point contained 3,594 features which
represent the following low-cost measurements:

•

•
•

3,578 mid-infrared absorbance measurements (wave-
lengths ranging from 7497.96 cm−1 - 599.76 cm−1).
Originally obtained via Diffuse Reﬂectance Infrared
Fourier transform Spectroscopy.
Depth of the soil sample (topsoil or subsoil)
Remotely-collected data including climate, topograph-
ical, vegetation index, surface temperature, and other
information about the sample collection site obtained
via satellite. All satellite data was mean centered and
scaled.

The data set contains monotonically adjusted values for the
ﬁve target variables (SOC content, Calcium content, Phospho-
rous content, sand content, and pH value) such that all target
variables take both positive and negative values.

The dataset was provided by the ”Africa Soil Property”

competition hosted on Kaggle.com.

IV. MODEL SELECTION

Pedotransfer functions and most other models used in
(predictive) soil science are linear regression based. Rossel

Noting the success of linear ﬁtting in other datasets map-
ping spectroscopy data to soil content characteristics, a number

Linear regression is then conducted on the transformed sets.
[5]

Support Vector Regression:

Developed by Vapnik (1998). [6] For a feature space of
xi ∈ Rn and target variable yi ∈ R with a total of m samples,
parameters C > 0 and  > 0, and a valid kernel function φ(x),
we can develop the following problem that would best ﬁt the
model estimating y as wT φ(x) + b:

1

2 wT w + C((cid:80)m

i=1 ξi +(cid:80)m

i=1 ξ∗
i )
wT φ(xi) + b − yi ≤  + ξi,
yi − wT φ(xi) − b ≤  + ξi,

ξi, ξ∗

i ≥ 0, i = 1, ..., m

(cid:80)m

i ) +(cid:80)m

1

2 (α − α∗)T Q(α − α∗)+

i=1(αi − α∗
0 ≤ αi, α∗

eT (α − α∗) = 0

i ≤ C, i = 1, ..., m

i=1 yi(αi − α∗
i )

minw,b,ξ,ξ∗
subject to

minα,α∗

subject to

(cid:80)m
i=1(−αi + α∗

of linear regression variants were attempted. In an effort to be
thorough, other methods such as support vector regression and
clustering were also attempted. The choices of models tried
were also guided by the fact that the number of features is
far more than the sample size which eliminated options like
SGD Regressor (Stochastic Gradient Descent Regressor) that
requires relatively large data-points.

Linear Regression:

Determines linear coefﬁcients to map feature set to target
variables. Linear coefﬁcients, β, for the feature set data, x,
are determined for each target variable, y, across the entirety

of the test set (of size m) such that (cid:80)m

(cid:0)yi − βT xi

(cid:1)2 is

i=1

minimized.

Ridge Regression:

Ridge regression is similar to linear regression in that it
determines linear coefﬁcients to map the feature set to target
variables. However, ridge regression also attempts to limit the
magnitude of its linear coefﬁcients by adding an additional
term that penalizes the l2-norm of the β term. β is chosen as
the following:

m(cid:88)

(cid:0)yi − βT xi

(cid:1)2

+ γ(cid:107)β(cid:107)2

2

βridge = argminβ

i=1
where γ is a tuning parameter.

Lasso Regression:

Lasso regression takes the same approach as ridge regres-
sion only it penalizes the l1-norm of the β term, which results
in the following deﬁnition:

m(cid:88)

(cid:0)yi − βT xi

(cid:1)2

+ γ(cid:107)β(cid:107)1

βlasso = argminβ

The dual of this problem is the following:

where Q = K(xi, x). The approximate solution of the dual is
i )K(xi, x) + b. [7] This project attempted to

ﬁt the data using the gaussian kernel.

V.

IMPLEMENTATION DETAILS

The error metric used in this project was the mean column-
wise root mean square error (MCRMSE) between the actual
target variables values and predicted target variable values.

MCRMSE =

1
5

i − ˆyi
(y(j)

(j))

Root-mean square error (RMSE) was used to compare model-
ﬁtting performance of individual target variables.

m(cid:88)

i=1

n

(cid:118)(cid:117)(cid:117)(cid:116) 1
m(cid:88)

i=1

j=1

5(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 1

n

where γ is again a tuning parameter.

i=1

Principal Component Regression:

RMSEj =

i − ˆyi
(y(j)

(j))

PCR uses principal component analysis (PCA) to ﬁrst
reduce the dimensions of the feature space. Linear regression
is then used to determine coefﬁcients mapping the new feature
space to the target variables. PCA transforms the feature space
by using each sample’s feature measurements to determine cor-
relation in the feature space variables, converts the feature data
into a set of orthogonal and linearly uncorrelated components,
and uses the ﬁrst n components to limit the original feature
space to Rn. PCA work in this project was conducted through
packages using the singular value decomposition of X T X,
where X is the feature space data.

Partial Least-squares Regression:

PLSR is a similar procedure to PCR, only the objective is
to identify and remove cross-correlation between feature set
and the target variables. While PCR utilities the singular value
decomposition of X T X, PLSR uses the singular value decom-
position of the product of the transposed feature space and the
target variable space, X T Y , to determine common orthogonal
factors and transform both feature and target variable space.

K-fold cross validation (K = 10) was used to develop
an accurate measurement of
these error metrics. Model
training and prediction was conducted in Python using the
scikit-learn package.

VI. RESULTS

Feature Reduction

As the dimension of the feature set exceeds the number
of samples, dimensionality reduction through principal com-
ponent analysis distilled the feature set to its most critical
components. Most of the models used did not beneﬁt signiﬁ-
cantly from the reduced feature space, likely because the other
algorithms’ constraints performed a similar action of reducing
highly related features.

The model with most improved performance was linear
regression, which, when conducted after PCA became PCR. To
determine the optimal number of features, PCR was conducted
a 100 times with a range of feature reduction factors (from 15
to the number of samples available in the training set). The

Predicting Africa Soil Properties Using Machine

Learning Techniques

Iretiayo Akinola ∗, Thomas Dowd †,

Electrical Engineering

Stanford University, Stanford, CA 94305

Email: ∗iakinola@stanford.edu, †tjdowd@stanford.edu

Abstract—Different machine learning algorithms were as-
sessed for estimating ﬁve functional soil parameters (SOC content,
Calcium content, Phosphorous content, sand content, and pH
value). The algorithms used include variants of linear regression
and support vector regression. A closer look at the prediction
performance for each target revealed that apart from pH, which
consistently had worse performance, prediction for the other
soil properties was quite satisfactory (RMSE < 0.4). Applying
machine learning techniques to soil properties prediction has
shown a lot of promising and encouraging results. Getting more
data, domain knowledge and intuition, possibly from soil scien-
tist/experts, would surely maximize this potential for accurate soil
property prediction.

I.

INTRODUCTION

Soil functional properties (such as primary productivity,
nutrient and water retention, and resistance to erosion) in-
dicate a location’s ability to perform important ecological
services. Traditional methods of measuring and characterizing
soil properties require expensive and time-consuming scientiﬁc
procedures. Low cost measurements obtained through diffuse
reﬂectance infrared spectroscopy and remotely collected data
have the potential to quickly estimate these same characteris-
tics without the use of costly chemical resources. Inexpensive
characterization methods would allow large, data-sparse re-
gions to plan sustainable agricultural development and manage
local natural resources.

Our objective was to accurately model the relationship
between these inexpensive measurements and ﬁve soil char-
acteristics properties: Soil Organic Carbon (SOC) content,
Calcium content, Phosphorus content, sand content, and pH
value.

II. LITERATURE-REVIEW

Since the mid 1900s, pedotransfer functions (PTF), which
are predictive functions of certain soil properties using data
from soil surveys, have been used to predict soils in temperate
regions. However, soil prediction methods can be adapted
across climates. According to Minasnya (2011) [1], methods
developed in temperate regions can be applied for the soils
in the tropical regions albeit with adjustments to calibration
and choice of relevant available predictors. Prior to model
selection, a literature review was conducted on past work
modeling spectroscopy data to soil characteristic qualities.

et al. (2006) performed an analysis on using visible, near-
infrared, and mid-infrared absorbance data to determine soil
characteristic properties. Their method used partial
least-
square regression (PLSR) to model the system, and they were
most successful in developing predictions for pH, Organic
Carbon, Phosphorous, and sand content using mid-infrared
absorbance values. [2]

Rossel and Behrens (2009) experimented with a number
of machine learning methods to map full-range spectroscopy
data to similar soil characteristics. Support Vector Regression
provided the most accurate results for the three target variables
in question (SOC content, clay content and pH levels). [3]

More recent prediction of soil properties such as [1] and [4]
still embrace the linear model assumption for their research.
In this project, we explore how recent improved machine
learning techniques can be applied to the soil science domain
to improve the overall prediction performance.

III. DATASET

The dataset consisted of a collection of 1,157 soil sample
measures. Soil was collected from a variety of locations
in Africa. Each data point contained 3,594 features which
represent the following low-cost measurements:

•

•
•

3,578 mid-infrared absorbance measurements (wave-
lengths ranging from 7497.96 cm−1 - 599.76 cm−1).
Originally obtained via Diffuse Reﬂectance Infrared
Fourier transform Spectroscopy.
Depth of the soil sample (topsoil or subsoil)
Remotely-collected data including climate, topograph-
ical, vegetation index, surface temperature, and other
information about the sample collection site obtained
via satellite. All satellite data was mean centered and
scaled.

The data set contains monotonically adjusted values for the
ﬁve target variables (SOC content, Calcium content, Phospho-
rous content, sand content, and pH value) such that all target
variables take both positive and negative values.

The dataset was provided by the ”Africa Soil Property”

competition hosted on Kaggle.com.

IV. MODEL SELECTION

Pedotransfer functions and most other models used in
(predictive) soil science are linear regression based. Rossel

Noting the success of linear ﬁtting in other datasets map-
ping spectroscopy data to soil content characteristics, a number

Linear regression is then conducted on the transformed sets.
[5]

Support Vector Regression:

Developed by Vapnik (1998). [6] For a feature space of
xi ∈ Rn and target variable yi ∈ R with a total of m samples,
parameters C > 0 and  > 0, and a valid kernel function φ(x),
we can develop the following problem that would best ﬁt the
model estimating y as wT φ(x) + b:

1

2 wT w + C((cid:80)m

i=1 ξi +(cid:80)m

i=1 ξ∗
i )
wT φ(xi) + b − yi ≤  + ξi,
yi − wT φ(xi) − b ≤  + ξi,

ξi, ξ∗

i ≥ 0, i = 1, ..., m

(cid:80)m

i ) +(cid:80)m

1

2 (α − α∗)T Q(α − α∗)+

i=1(αi − α∗
0 ≤ αi, α∗

eT (α − α∗) = 0

i ≤ C, i = 1, ..., m

i=1 yi(αi − α∗
i )

minw,b,ξ,ξ∗
subject to

minα,α∗

subject to

(cid:80)m
i=1(−αi + α∗

of linear regression variants were attempted. In an effort to be
thorough, other methods such as support vector regression and
clustering were also attempted. The choices of models tried
were also guided by the fact that the number of features is
far more than the sample size which eliminated options like
SGD Regressor (Stochastic Gradient Descent Regressor) that
requires relatively large data-points.

Linear Regression:

Determines linear coefﬁcients to map feature set to target
variables. Linear coefﬁcients, β, for the feature set data, x,
are determined for each target variable, y, across the entirety

of the test set (of size m) such that (cid:80)m

(cid:0)yi − βT xi

(cid:1)2 is

i=1

minimized.

Ridge Regression:

Ridge regression is similar to linear regression in that it
determines linear coefﬁcients to map the feature set to target
variables. However, ridge regression also attempts to limit the
magnitude of its linear coefﬁcients by adding an additional
term that penalizes the l2-norm of the β term. β is chosen as
the following:

m(cid:88)

(cid:0)yi − βT xi

(cid:1)2

+ γ(cid:107)β(cid:107)2

2

βridge = argminβ

i=1
where γ is a tuning parameter.

Lasso Regression:

Lasso regression takes the same approach as ridge regres-
sion only it penalizes the l1-norm of the β term, which results
in the following deﬁnition:

m(cid:88)

(cid:0)yi − βT xi

(cid:1)2

+ γ(cid:107)β(cid:107)1

βlasso = argminβ

The dual of this problem is the following:

where Q = K(xi, x). The approximate solution of the dual is
i )K(xi, x) + b. [7] This project attempted to

ﬁt the data using the gaussian kernel.

V.

IMPLEMENTATION DETAILS

The error metric used in this project was the mean column-
wise root mean square error (MCRMSE) between the actual
target variables values and predicted target variable values.

MCRMSE =

1
5

i − ˆyi
(y(j)

(j))

Root-mean square error (RMSE) was used to compare model-
ﬁtting performance of individual target variables.

m(cid:88)

i=1

n

(cid:118)(cid:117)(cid:117)(cid:116) 1
m(cid:88)

i=1

j=1

5(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 1

n

where γ is again a tuning parameter.

i=1

Principal Component Regression:

RMSEj =

i − ˆyi
(y(j)

(j))

PCR uses principal component analysis (PCA) to ﬁrst
reduce the dimensions of the feature space. Linear regression
is then used to determine coefﬁcients mapping the new feature
space to the target variables. PCA transforms the feature space
by using each sample’s feature measurements to determine cor-
relation in the feature space variables, converts the feature data
into a set of orthogonal and linearly uncorrelated components,
and uses the ﬁrst n components to limit the original feature
space to Rn. PCA work in this project was conducted through
packages using the singular value decomposition of X T X,
where X is the feature space data.

Partial Least-squares Regression:

PLSR is a similar procedure to PCR, only the objective is
to identify and remove cross-correlation between feature set
and the target variables. While PCR utilities the singular value
decomposition of X T X, PLSR uses the singular value decom-
position of the product of the transposed feature space and the
target variable space, X T Y , to determine common orthogonal
factors and transform both feature and target variable space.

K-fold cross validation (K = 10) was used to develop
an accurate measurement of
these error metrics. Model
training and prediction was conducted in Python using the
scikit-learn package.

VI. RESULTS

Feature Reduction

As the dimension of the feature set exceeds the number
of samples, dimensionality reduction through principal com-
ponent analysis distilled the feature set to its most critical
components. Most of the models used did not beneﬁt signiﬁ-
cantly from the reduced feature space, likely because the other
algorithms’ constraints performed a similar action of reducing
highly related features.

The model with most improved performance was linear
regression, which, when conducted after PCA became PCR. To
determine the optimal number of features, PCR was conducted
a 100 times with a range of feature reduction factors (from 15
to the number of samples available in the training set). The

training and test MCRMSE of these models were calculated
for each reduced feature set. The results can been seen in the
plot below:

The soil data could be divided two classes of similar size:
topsoil and subsoil. After getting the performance for the entire
dataset, we repeated the whole prediction procedure for each
of the classes and found that training and testing errors were
very similar to that of the overall performance.

Fig. 1: MCRMSE vs. Total Features

Using these results, the optimal number of features was
determined to be 65. To ensure the validity of this conclusion,
singular value decomposition was conducted on the feature set,
which determined that a reduced feature set in R65 preserved
99.5% of the data.

Model Performance

The ﬁgure below shows the prediction performance by six
best performing algorithms on the ﬁve target variables. The
overall prediction performance showed that the linear regres-
sion based models had comparable and better performances
compared to the others. Ridge regression and PCR, the two
top-performing models, both perform the same underlying
function of penalizing the non-relevant features. While ridge
regression does this by constraining the regression coefﬁcients,
PCA transforms the feature set by removing redundancies.
Lasso regression also had similar performance.

Fig. 2: Target Variable RMSE by Model

Table 1: Model Prediction Performance (MCRMSE)

The table above contains only the results from the
most successful models. Some other methods attempted that
achieved poorer results are summarized below.

Other Models

One model attempted was a double-layered estimation
model was considered where an initial a classiﬁcation stage
on quantized data is followed by a linear regression prediction
of data in each of the quanta-bins. The results of the ﬁrst
SVM-based classiﬁcation step were not as impressive enough.
While the linear kernel performed best (compared to gaussian,
polynomial, sigmoid kernel) which conﬁrmed the notion that
linear models are best for soil prediction, the highest prediction
performance achieved for just two class quantization was less
88% for all the target variables. This forms a poor basis for
the following regression step on as already misclassiﬁed data-
points would get poor estimations results in the end; besides
the misclassiﬁed datapoints from the ﬁrst stage might corrupt
the regression model learning/ training process.

Bagged Support Vector Regression was also conducted in
an effort to prevent overﬁtting for training data. However, the
bagged results were not signiﬁcantly different from the regular
SVR output.

VII. DISCUSSIONS

For the majority of the target variables, linear regression
and other linear variations achieved high performance. The
exception was pH, which remained the most difﬁcult to predict
for all models. Figure [3] shows that pH was less correlated
with other target variables. This could be because the set
of features in the data does not capture all the factors that
contribute to the pH of soils. While other target variables might
have intersection of features that are commonly indicative
of them in varying degrees/weights present in the dataset,
capturing indicators of some of the many different component
minerals that contribute to the pH of a soil might require other
types of features.

Predicting Africa Soil Properties Using Machine

Learning Techniques

Iretiayo Akinola ∗, Thomas Dowd †,

Electrical Engineering

Stanford University, Stanford, CA 94305

Email: ∗iakinola@stanford.edu, †tjdowd@stanford.edu

Abstract—Different machine learning algorithms were as-
sessed for estimating ﬁve functional soil parameters (SOC content,
Calcium content, Phosphorous content, sand content, and pH
value). The algorithms used include variants of linear regression
and support vector regression. A closer look at the prediction
performance for each target revealed that apart from pH, which
consistently had worse performance, prediction for the other
soil properties was quite satisfactory (RMSE < 0.4). Applying
machine learning techniques to soil properties prediction has
shown a lot of promising and encouraging results. Getting more
data, domain knowledge and intuition, possibly from soil scien-
tist/experts, would surely maximize this potential for accurate soil
property prediction.

I.

INTRODUCTION

Soil functional properties (such as primary productivity,
nutrient and water retention, and resistance to erosion) in-
dicate a location’s ability to perform important ecological
services. Traditional methods of measuring and characterizing
soil properties require expensive and time-consuming scientiﬁc
procedures. Low cost measurements obtained through diffuse
reﬂectance infrared spectroscopy and remotely collected data
have the potential to quickly estimate these same characteris-
tics without the use of costly chemical resources. Inexpensive
characterization methods would allow large, data-sparse re-
gions to plan sustainable agricultural development and manage
local natural resources.

Our objective was to accurately model the relationship
between these inexpensive measurements and ﬁve soil char-
acteristics properties: Soil Organic Carbon (SOC) content,
Calcium content, Phosphorus content, sand content, and pH
value.

II. LITERATURE-REVIEW

Since the mid 1900s, pedotransfer functions (PTF), which
are predictive functions of certain soil properties using data
from soil surveys, have been used to predict soils in temperate
regions. However, soil prediction methods can be adapted
across climates. According to Minasnya (2011) [1], methods
developed in temperate regions can be applied for the soils
in the tropical regions albeit with adjustments to calibration
and choice of relevant available predictors. Prior to model
selection, a literature review was conducted on past work
modeling spectroscopy data to soil characteristic qualities.

et al. (2006) performed an analysis on using visible, near-
infrared, and mid-infrared absorbance data to determine soil
characteristic properties. Their method used partial
least-
square regression (PLSR) to model the system, and they were
most successful in developing predictions for pH, Organic
Carbon, Phosphorous, and sand content using mid-infrared
absorbance values. [2]

Rossel and Behrens (2009) experimented with a number
of machine learning methods to map full-range spectroscopy
data to similar soil characteristics. Support Vector Regression
provided the most accurate results for the three target variables
in question (SOC content, clay content and pH levels). [3]

More recent prediction of soil properties such as [1] and [4]
still embrace the linear model assumption for their research.
In this project, we explore how recent improved machine
learning techniques can be applied to the soil science domain
to improve the overall prediction performance.

III. DATASET

The dataset consisted of a collection of 1,157 soil sample
measures. Soil was collected from a variety of locations
in Africa. Each data point contained 3,594 features which
represent the following low-cost measurements:

•

•
•

3,578 mid-infrared absorbance measurements (wave-
lengths ranging from 7497.96 cm−1 - 599.76 cm−1).
Originally obtained via Diffuse Reﬂectance Infrared
Fourier transform Spectroscopy.
Depth of the soil sample (topsoil or subsoil)
Remotely-collected data including climate, topograph-
ical, vegetation index, surface temperature, and other
information about the sample collection site obtained
via satellite. All satellite data was mean centered and
scaled.

The data set contains monotonically adjusted values for the
ﬁve target variables (SOC content, Calcium content, Phospho-
rous content, sand content, and pH value) such that all target
variables take both positive and negative values.

The dataset was provided by the ”Africa Soil Property”

competition hosted on Kaggle.com.

IV. MODEL SELECTION

Pedotransfer functions and most other models used in
(predictive) soil science are linear regression based. Rossel

Noting the success of linear ﬁtting in other datasets map-
ping spectroscopy data to soil content characteristics, a number

Linear regression is then conducted on the transformed sets.
[5]

Support Vector Regression:

Developed by Vapnik (1998). [6] For a feature space of
xi ∈ Rn and target variable yi ∈ R with a total of m samples,
parameters C > 0 and  > 0, and a valid kernel function φ(x),
we can develop the following problem that would best ﬁt the
model estimating y as wT φ(x) + b:

1

2 wT w + C((cid:80)m

i=1 ξi +(cid:80)m

i=1 ξ∗
i )
wT φ(xi) + b − yi ≤  + ξi,
yi − wT φ(xi) − b ≤  + ξi,

ξi, ξ∗

i ≥ 0, i = 1, ..., m

(cid:80)m

i ) +(cid:80)m

1

2 (α − α∗)T Q(α − α∗)+

i=1(αi − α∗
0 ≤ αi, α∗

eT (α − α∗) = 0

i ≤ C, i = 1, ..., m

i=1 yi(αi − α∗
i )

minw,b,ξ,ξ∗
subject to

minα,α∗

subject to

(cid:80)m
i=1(−αi + α∗

of linear regression variants were attempted. In an effort to be
thorough, other methods such as support vector regression and
clustering were also attempted. The choices of models tried
were also guided by the fact that the number of features is
far more than the sample size which eliminated options like
SGD Regressor (Stochastic Gradient Descent Regressor) that
requires relatively large data-points.

Linear Regression:

Determines linear coefﬁcients to map feature set to target
variables. Linear coefﬁcients, β, for the feature set data, x,
are determined for each target variable, y, across the entirety

of the test set (of size m) such that (cid:80)m

(cid:0)yi − βT xi

(cid:1)2 is

i=1

minimized.

Ridge Regression:

Ridge regression is similar to linear regression in that it
determines linear coefﬁcients to map the feature set to target
variables. However, ridge regression also attempts to limit the
magnitude of its linear coefﬁcients by adding an additional
term that penalizes the l2-norm of the β term. β is chosen as
the following:

m(cid:88)

(cid:0)yi − βT xi

(cid:1)2

+ γ(cid:107)β(cid:107)2

2

βridge = argminβ

i=1
where γ is a tuning parameter.

Lasso Regression:

Lasso regression takes the same approach as ridge regres-
sion only it penalizes the l1-norm of the β term, which results
in the following deﬁnition:

m(cid:88)

(cid:0)yi − βT xi

(cid:1)2

+ γ(cid:107)β(cid:107)1

βlasso = argminβ

The dual of this problem is the following:

where Q = K(xi, x). The approximate solution of the dual is
i )K(xi, x) + b. [7] This project attempted to

ﬁt the data using the gaussian kernel.

V.

IMPLEMENTATION DETAILS

The error metric used in this project was the mean column-
wise root mean square error (MCRMSE) between the actual
target variables values and predicted target variable values.

MCRMSE =

1
5

i − ˆyi
(y(j)

(j))

Root-mean square error (RMSE) was used to compare model-
ﬁtting performance of individual target variables.

m(cid:88)

i=1

n

(cid:118)(cid:117)(cid:117)(cid:116) 1
m(cid:88)

i=1

j=1

5(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 1

n

where γ is again a tuning parameter.

i=1

Principal Component Regression:

RMSEj =

i − ˆyi
(y(j)

(j))

PCR uses principal component analysis (PCA) to ﬁrst
reduce the dimensions of the feature space. Linear regression
is then used to determine coefﬁcients mapping the new feature
space to the target variables. PCA transforms the feature space
by using each sample’s feature measurements to determine cor-
relation in the feature space variables, converts the feature data
into a set of orthogonal and linearly uncorrelated components,
and uses the ﬁrst n components to limit the original feature
space to Rn. PCA work in this project was conducted through
packages using the singular value decomposition of X T X,
where X is the feature space data.

Partial Least-squares Regression:

PLSR is a similar procedure to PCR, only the objective is
to identify and remove cross-correlation between feature set
and the target variables. While PCR utilities the singular value
decomposition of X T X, PLSR uses the singular value decom-
position of the product of the transposed feature space and the
target variable space, X T Y , to determine common orthogonal
factors and transform both feature and target variable space.

K-fold cross validation (K = 10) was used to develop
an accurate measurement of
these error metrics. Model
training and prediction was conducted in Python using the
scikit-learn package.

VI. RESULTS

Feature Reduction

As the dimension of the feature set exceeds the number
of samples, dimensionality reduction through principal com-
ponent analysis distilled the feature set to its most critical
components. Most of the models used did not beneﬁt signiﬁ-
cantly from the reduced feature space, likely because the other
algorithms’ constraints performed a similar action of reducing
highly related features.

The model with most improved performance was linear
regression, which, when conducted after PCA became PCR. To
determine the optimal number of features, PCR was conducted
a 100 times with a range of feature reduction factors (from 15
to the number of samples available in the training set). The

training and test MCRMSE of these models were calculated
for each reduced feature set. The results can been seen in the
plot below:

The soil data could be divided two classes of similar size:
topsoil and subsoil. After getting the performance for the entire
dataset, we repeated the whole prediction procedure for each
of the classes and found that training and testing errors were
very similar to that of the overall performance.

Fig. 1: MCRMSE vs. Total Features

Using these results, the optimal number of features was
determined to be 65. To ensure the validity of this conclusion,
singular value decomposition was conducted on the feature set,
which determined that a reduced feature set in R65 preserved
99.5% of the data.

Model Performance

The ﬁgure below shows the prediction performance by six
best performing algorithms on the ﬁve target variables. The
overall prediction performance showed that the linear regres-
sion based models had comparable and better performances
compared to the others. Ridge regression and PCR, the two
top-performing models, both perform the same underlying
function of penalizing the non-relevant features. While ridge
regression does this by constraining the regression coefﬁcients,
PCA transforms the feature set by removing redundancies.
Lasso regression also had similar performance.

Fig. 2: Target Variable RMSE by Model

Table 1: Model Prediction Performance (MCRMSE)

The table above contains only the results from the
most successful models. Some other methods attempted that
achieved poorer results are summarized below.

Other Models

One model attempted was a double-layered estimation
model was considered where an initial a classiﬁcation stage
on quantized data is followed by a linear regression prediction
of data in each of the quanta-bins. The results of the ﬁrst
SVM-based classiﬁcation step were not as impressive enough.
While the linear kernel performed best (compared to gaussian,
polynomial, sigmoid kernel) which conﬁrmed the notion that
linear models are best for soil prediction, the highest prediction
performance achieved for just two class quantization was less
88% for all the target variables. This forms a poor basis for
the following regression step on as already misclassiﬁed data-
points would get poor estimations results in the end; besides
the misclassiﬁed datapoints from the ﬁrst stage might corrupt
the regression model learning/ training process.

Bagged Support Vector Regression was also conducted in
an effort to prevent overﬁtting for training data. However, the
bagged results were not signiﬁcantly different from the regular
SVR output.

VII. DISCUSSIONS

For the majority of the target variables, linear regression
and other linear variations achieved high performance. The
exception was pH, which remained the most difﬁcult to predict
for all models. Figure [3] shows that pH was less correlated
with other target variables. This could be because the set
of features in the data does not capture all the factors that
contribute to the pH of soils. While other target variables might
have intersection of features that are commonly indicative
of them in varying degrees/weights present in the dataset,
capturing indicators of some of the many different component
minerals that contribute to the pH of a soil might require other
types of features.

Unlike the other linear methods, the reduction of common
correlations between features and target variables allowed for
a linear model that achieved similar RMSE across all the target
variables.

Support Vector Regression

Support Vector Regression was used in an attempt to obtain
a better generalized ﬁt for the data. SVR was conducted with
a gaussian kernel. The result had a very low training error
metric, but suffered signiﬁcant drop in performance for the
test data, suggesting an overﬁtting problem. Further attempts
were made to improve the gaussian kernel SVR performance
through bagging methods, but these did not yield better results.

VIII. CONCLUSION

Asides from pH which performed poorly, the prediction
performance of our study shows that some soil functional
properties can estimated reasonably well (e.g. Ca with rmse¡.4)
using carefully selected cheaper soil characteristics as pre-
dictors and smart machine learning algorithms. Our results
show that Machine Learning techniques applied soil proper-
ties prediction holds a lot of promise. With more data and
soil science domain-speciﬁc tricks, the potential for applying
machine learning to soil property prediction would surely be
maximized.

IX. FUTURE WORK

Use of probabilistic graphical models for capturing corre-
lations between target variables. The correlation map above
shows that some target variables are quite correlated. Joint
probability models might be useful
to incorporated these
correlation information into the prediction process to enhance
the overall prediction performance.

Fig. 3: Target Variable Correlation

Linear Regression

Given that linear regression models seek to minimize least-
squares error, linear regression performed very well on the
training set. However,
the higher test error suggested that
the linear relationship between the training features set and
target variables could not be directly extended to test data.
Variations on linear regression were explored to compensate
for this deﬁciency without adjusting the feature set, while
other methods sought to increase accuracy by adapting and
increasing the feature set.

Ridge Regression

The estimation performance expectedly depends on the
choice of regularizer. Since there are ﬁve different
target
variables, different regularization values optimizes each of
the targets. In particular, as the regularization parameter was
varied in one direction, performance of the targets (except P)
improved; a compromise had to be made on the regularization
value chosen. Performing PCA before ridge regression did not
improve performance.

Lasso Regression

This is another regularized regression model and the per-
formance was very comparable to that of ridge regression as
expected. However, it was observed that Lasso did consider-
ably better than ridge and and slightly better than PCR on the
prediction of pH. This suggests that the l1 norm regularization
might be more suited for speciﬁc target variables than others.

Get access to raw data and try out other transformation
techniques to extract features. The dataset obtained for this
project had been cured and conditioned by the source. It
would be interesting to try out different feature transformation
techniques on the raw spectrophotometer measurements. This
would help build more accurate models about the data.

Principal Component Regression

Based on the MCRMSE metric,

the PCR model had
the best performance of all models tested. A quick analysis
of the results for individual target variables reveal that the
PCR method achieved very similar results to lasso regression.
PCR served as a signiﬁcantly better estimator for pH than
standard linear regression, demonstrating that removing the
highly correlated features contributed to a more generalized
model for pH.

Implementation of ensemble methods to combine success-
ful estimation models. In addition to exploring other possible
prediction algorithms, ensemble learning technique could be
employed to combine the top performing algorithms to im-
prove overall performance.

Cluster analysis of the data might reveal and pull together
samples with similar soil characteristics. Target variables can
then be separately predicted for each of the clusters.

Partial Least-squares Regression

X. ACKNOWLEDGEMENT

PLSR performed signiﬁcantly worse than the other linear
regression variants for almost all the target variables. How-
ever, its higher performance for pH estimation was notable.

We would like to thank Professor Ng for his excellent
instruction, and the CS 229 TAs for their help and advice on
this project.

Predicting Africa Soil Properties Using Machine

Learning Techniques

Iretiayo Akinola ∗, Thomas Dowd †,

Electrical Engineering

Stanford University, Stanford, CA 94305

Email: ∗iakinola@stanford.edu, †tjdowd@stanford.edu

Abstract—Different machine learning algorithms were as-
sessed for estimating ﬁve functional soil parameters (SOC content,
Calcium content, Phosphorous content, sand content, and pH
value). The algorithms used include variants of linear regression
and support vector regression. A closer look at the prediction
performance for each target revealed that apart from pH, which
consistently had worse performance, prediction for the other
soil properties was quite satisfactory (RMSE < 0.4). Applying
machine learning techniques to soil properties prediction has
shown a lot of promising and encouraging results. Getting more
data, domain knowledge and intuition, possibly from soil scien-
tist/experts, would surely maximize this potential for accurate soil
property prediction.

I.

INTRODUCTION

Soil functional properties (such as primary productivity,
nutrient and water retention, and resistance to erosion) in-
dicate a location’s ability to perform important ecological
services. Traditional methods of measuring and characterizing
soil properties require expensive and time-consuming scientiﬁc
procedures. Low cost measurements obtained through diffuse
reﬂectance infrared spectroscopy and remotely collected data
have the potential to quickly estimate these same characteris-
tics without the use of costly chemical resources. Inexpensive
characterization methods would allow large, data-sparse re-
gions to plan sustainable agricultural development and manage
local natural resources.

Our objective was to accurately model the relationship
between these inexpensive measurements and ﬁve soil char-
acteristics properties: Soil Organic Carbon (SOC) content,
Calcium content, Phosphorus content, sand content, and pH
value.

II. LITERATURE-REVIEW

Since the mid 1900s, pedotransfer functions (PTF), which
are predictive functions of certain soil properties using data
from soil surveys, have been used to predict soils in temperate
regions. However, soil prediction methods can be adapted
across climates. According to Minasnya (2011) [1], methods
developed in temperate regions can be applied for the soils
in the tropical regions albeit with adjustments to calibration
and choice of relevant available predictors. Prior to model
selection, a literature review was conducted on past work
modeling spectroscopy data to soil characteristic qualities.

et al. (2006) performed an analysis on using visible, near-
infrared, and mid-infrared absorbance data to determine soil
characteristic properties. Their method used partial
least-
square regression (PLSR) to model the system, and they were
most successful in developing predictions for pH, Organic
Carbon, Phosphorous, and sand content using mid-infrared
absorbance values. [2]

Rossel and Behrens (2009) experimented with a number
of machine learning methods to map full-range spectroscopy
data to similar soil characteristics. Support Vector Regression
provided the most accurate results for the three target variables
in question (SOC content, clay content and pH levels). [3]

More recent prediction of soil properties such as [1] and [4]
still embrace the linear model assumption for their research.
In this project, we explore how recent improved machine
learning techniques can be applied to the soil science domain
to improve the overall prediction performance.

III. DATASET

The dataset consisted of a collection of 1,157 soil sample
measures. Soil was collected from a variety of locations
in Africa. Each data point contained 3,594 features which
represent the following low-cost measurements:

•

•
•

3,578 mid-infrared absorbance measurements (wave-
lengths ranging from 7497.96 cm−1 - 599.76 cm−1).
Originally obtained via Diffuse Reﬂectance Infrared
Fourier transform Spectroscopy.
Depth of the soil sample (topsoil or subsoil)
Remotely-collected data including climate, topograph-
ical, vegetation index, surface temperature, and other
information about the sample collection site obtained
via satellite. All satellite data was mean centered and
scaled.

The data set contains monotonically adjusted values for the
ﬁve target variables (SOC content, Calcium content, Phospho-
rous content, sand content, and pH value) such that all target
variables take both positive and negative values.

The dataset was provided by the ”Africa Soil Property”

competition hosted on Kaggle.com.

IV. MODEL SELECTION

Pedotransfer functions and most other models used in
(predictive) soil science are linear regression based. Rossel

Noting the success of linear ﬁtting in other datasets map-
ping spectroscopy data to soil content characteristics, a number

Linear regression is then conducted on the transformed sets.
[5]

Support Vector Regression:

Developed by Vapnik (1998). [6] For a feature space of
xi ∈ Rn and target variable yi ∈ R with a total of m samples,
parameters C > 0 and  > 0, and a valid kernel function φ(x),
we can develop the following problem that would best ﬁt the
model estimating y as wT φ(x) + b:

1

2 wT w + C((cid:80)m

i=1 ξi +(cid:80)m

i=1 ξ∗
i )
wT φ(xi) + b − yi ≤  + ξi,
yi − wT φ(xi) − b ≤  + ξi,

ξi, ξ∗

i ≥ 0, i = 1, ..., m

(cid:80)m

i ) +(cid:80)m

1

2 (α − α∗)T Q(α − α∗)+

i=1(αi − α∗
0 ≤ αi, α∗

eT (α − α∗) = 0

i ≤ C, i = 1, ..., m

i=1 yi(αi − α∗
i )

minw,b,ξ,ξ∗
subject to

minα,α∗

subject to

(cid:80)m
i=1(−αi + α∗

of linear regression variants were attempted. In an effort to be
thorough, other methods such as support vector regression and
clustering were also attempted. The choices of models tried
were also guided by the fact that the number of features is
far more than the sample size which eliminated options like
SGD Regressor (Stochastic Gradient Descent Regressor) that
requires relatively large data-points.

Linear Regression:

Determines linear coefﬁcients to map feature set to target
variables. Linear coefﬁcients, β, for the feature set data, x,
are determined for each target variable, y, across the entirety

of the test set (of size m) such that (cid:80)m

(cid:0)yi − βT xi

(cid:1)2 is

i=1

minimized.

Ridge Regression:

Ridge regression is similar to linear regression in that it
determines linear coefﬁcients to map the feature set to target
variables. However, ridge regression also attempts to limit the
magnitude of its linear coefﬁcients by adding an additional
term that penalizes the l2-norm of the β term. β is chosen as
the following:

m(cid:88)

(cid:0)yi − βT xi

(cid:1)2

+ γ(cid:107)β(cid:107)2

2

βridge = argminβ

i=1
where γ is a tuning parameter.

Lasso Regression:

Lasso regression takes the same approach as ridge regres-
sion only it penalizes the l1-norm of the β term, which results
in the following deﬁnition:

m(cid:88)

(cid:0)yi − βT xi

(cid:1)2

+ γ(cid:107)β(cid:107)1

βlasso = argminβ

The dual of this problem is the following:

where Q = K(xi, x). The approximate solution of the dual is
i )K(xi, x) + b. [7] This project attempted to

ﬁt the data using the gaussian kernel.

V.

IMPLEMENTATION DETAILS

The error metric used in this project was the mean column-
wise root mean square error (MCRMSE) between the actual
target variables values and predicted target variable values.

MCRMSE =

1
5

i − ˆyi
(y(j)

(j))

Root-mean square error (RMSE) was used to compare model-
ﬁtting performance of individual target variables.

m(cid:88)

i=1

n

(cid:118)(cid:117)(cid:117)(cid:116) 1
m(cid:88)

i=1

j=1

5(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 1

n

where γ is again a tuning parameter.

i=1

Principal Component Regression:

RMSEj =

i − ˆyi
(y(j)

(j))

PCR uses principal component analysis (PCA) to ﬁrst
reduce the dimensions of the feature space. Linear regression
is then used to determine coefﬁcients mapping the new feature
space to the target variables. PCA transforms the feature space
by using each sample’s feature measurements to determine cor-
relation in the feature space variables, converts the feature data
into a set of orthogonal and linearly uncorrelated components,
and uses the ﬁrst n components to limit the original feature
space to Rn. PCA work in this project was conducted through
packages using the singular value decomposition of X T X,
where X is the feature space data.

Partial Least-squares Regression:

PLSR is a similar procedure to PCR, only the objective is
to identify and remove cross-correlation between feature set
and the target variables. While PCR utilities the singular value
decomposition of X T X, PLSR uses the singular value decom-
position of the product of the transposed feature space and the
target variable space, X T Y , to determine common orthogonal
factors and transform both feature and target variable space.

K-fold cross validation (K = 10) was used to develop
an accurate measurement of
these error metrics. Model
training and prediction was conducted in Python using the
scikit-learn package.

VI. RESULTS

Feature Reduction

As the dimension of the feature set exceeds the number
of samples, dimensionality reduction through principal com-
ponent analysis distilled the feature set to its most critical
components. Most of the models used did not beneﬁt signiﬁ-
cantly from the reduced feature space, likely because the other
algorithms’ constraints performed a similar action of reducing
highly related features.

The model with most improved performance was linear
regression, which, when conducted after PCA became PCR. To
determine the optimal number of features, PCR was conducted
a 100 times with a range of feature reduction factors (from 15
to the number of samples available in the training set). The

training and test MCRMSE of these models were calculated
for each reduced feature set. The results can been seen in the
plot below:

The soil data could be divided two classes of similar size:
topsoil and subsoil. After getting the performance for the entire
dataset, we repeated the whole prediction procedure for each
of the classes and found that training and testing errors were
very similar to that of the overall performance.

Fig. 1: MCRMSE vs. Total Features

Using these results, the optimal number of features was
determined to be 65. To ensure the validity of this conclusion,
singular value decomposition was conducted on the feature set,
which determined that a reduced feature set in R65 preserved
99.5% of the data.

Model Performance

The ﬁgure below shows the prediction performance by six
best performing algorithms on the ﬁve target variables. The
overall prediction performance showed that the linear regres-
sion based models had comparable and better performances
compared to the others. Ridge regression and PCR, the two
top-performing models, both perform the same underlying
function of penalizing the non-relevant features. While ridge
regression does this by constraining the regression coefﬁcients,
PCA transforms the feature set by removing redundancies.
Lasso regression also had similar performance.

Fig. 2: Target Variable RMSE by Model

Table 1: Model Prediction Performance (MCRMSE)

The table above contains only the results from the
most successful models. Some other methods attempted that
achieved poorer results are summarized below.

Other Models

One model attempted was a double-layered estimation
model was considered where an initial a classiﬁcation stage
on quantized data is followed by a linear regression prediction
of data in each of the quanta-bins. The results of the ﬁrst
SVM-based classiﬁcation step were not as impressive enough.
While the linear kernel performed best (compared to gaussian,
polynomial, sigmoid kernel) which conﬁrmed the notion that
linear models are best for soil prediction, the highest prediction
performance achieved for just two class quantization was less
88% for all the target variables. This forms a poor basis for
the following regression step on as already misclassiﬁed data-
points would get poor estimations results in the end; besides
the misclassiﬁed datapoints from the ﬁrst stage might corrupt
the regression model learning/ training process.

Bagged Support Vector Regression was also conducted in
an effort to prevent overﬁtting for training data. However, the
bagged results were not signiﬁcantly different from the regular
SVR output.

VII. DISCUSSIONS

For the majority of the target variables, linear regression
and other linear variations achieved high performance. The
exception was pH, which remained the most difﬁcult to predict
for all models. Figure [3] shows that pH was less correlated
with other target variables. This could be because the set
of features in the data does not capture all the factors that
contribute to the pH of soils. While other target variables might
have intersection of features that are commonly indicative
of them in varying degrees/weights present in the dataset,
capturing indicators of some of the many different component
minerals that contribute to the pH of a soil might require other
types of features.

Unlike the other linear methods, the reduction of common
correlations between features and target variables allowed for
a linear model that achieved similar RMSE across all the target
variables.

Support Vector Regression

Support Vector Regression was used in an attempt to obtain
a better generalized ﬁt for the data. SVR was conducted with
a gaussian kernel. The result had a very low training error
metric, but suffered signiﬁcant drop in performance for the
test data, suggesting an overﬁtting problem. Further attempts
were made to improve the gaussian kernel SVR performance
through bagging methods, but these did not yield better results.

VIII. CONCLUSION

Asides from pH which performed poorly, the prediction
performance of our study shows that some soil functional
properties can estimated reasonably well (e.g. Ca with rmse¡.4)
using carefully selected cheaper soil characteristics as pre-
dictors and smart machine learning algorithms. Our results
show that Machine Learning techniques applied soil proper-
ties prediction holds a lot of promise. With more data and
soil science domain-speciﬁc tricks, the potential for applying
machine learning to soil property prediction would surely be
maximized.

IX. FUTURE WORK

Use of probabilistic graphical models for capturing corre-
lations between target variables. The correlation map above
shows that some target variables are quite correlated. Joint
probability models might be useful
to incorporated these
correlation information into the prediction process to enhance
the overall prediction performance.

Fig. 3: Target Variable Correlation

Linear Regression

Given that linear regression models seek to minimize least-
squares error, linear regression performed very well on the
training set. However,
the higher test error suggested that
the linear relationship between the training features set and
target variables could not be directly extended to test data.
Variations on linear regression were explored to compensate
for this deﬁciency without adjusting the feature set, while
other methods sought to increase accuracy by adapting and
increasing the feature set.

Ridge Regression

The estimation performance expectedly depends on the
choice of regularizer. Since there are ﬁve different
target
variables, different regularization values optimizes each of
the targets. In particular, as the regularization parameter was
varied in one direction, performance of the targets (except P)
improved; a compromise had to be made on the regularization
value chosen. Performing PCA before ridge regression did not
improve performance.

Lasso Regression

This is another regularized regression model and the per-
formance was very comparable to that of ridge regression as
expected. However, it was observed that Lasso did consider-
ably better than ridge and and slightly better than PCR on the
prediction of pH. This suggests that the l1 norm regularization
might be more suited for speciﬁc target variables than others.

Get access to raw data and try out other transformation
techniques to extract features. The dataset obtained for this
project had been cured and conditioned by the source. It
would be interesting to try out different feature transformation
techniques on the raw spectrophotometer measurements. This
would help build more accurate models about the data.

Principal Component Regression

Based on the MCRMSE metric,

the PCR model had
the best performance of all models tested. A quick analysis
of the results for individual target variables reveal that the
PCR method achieved very similar results to lasso regression.
PCR served as a signiﬁcantly better estimator for pH than
standard linear regression, demonstrating that removing the
highly correlated features contributed to a more generalized
model for pH.

Implementation of ensemble methods to combine success-
ful estimation models. In addition to exploring other possible
prediction algorithms, ensemble learning technique could be
employed to combine the top performing algorithms to im-
prove overall performance.

Cluster analysis of the data might reveal and pull together
samples with similar soil characteristics. Target variables can
then be separately predicted for each of the clusters.

Partial Least-squares Regression

X. ACKNOWLEDGEMENT

PLSR performed signiﬁcantly worse than the other linear
regression variants for almost all the target variables. How-
ever, its higher performance for pH estimation was notable.

We would like to thank Professor Ng for his excellent
instruction, and the CS 229 TAs for their help and advice on
this project.

REFERENCES

[1] B. Minasny and A. E. Hartemink, “Predicting soil properties in the

tropics,” Earth-Science Reviews, vol. 106, no. 12, pp. 52 – 62, 2011.

[2] R. V. Rossel, D. Walvoort, A. McBratney, L. Janik, and J. Skjemstad,
“Visible, near infrared, mid infrared or combined diffuse reﬂectance
spectroscopy for simultaneous assessment of various soil properties,”
Geoderma, vol. 131, no. 1–2, pp. 59 – 75, 2006.

[3] R. V. Rossel and T. Behrens, “Using data mining to model and interpret
soil diffuse reﬂectance spectra,” Geoderma, vol. 158, no. 1–2, pp. 46
– 54, 2010. Diffuse reﬂectance spectroscopy in soil science and land
resource assessment.
J. A. C. Medeiros, M. Cooper, J. Dalla Rosa, M. Grimaldi, and Y. Coquet,
“Assessment of pedotransfer functions for estimating soil water retention
curves for the amazon region,” Revista Brasileira de Ci ˜A do Solo, vol. 38,
pp. 730 – 743, 06 2014.

[4]

[5] H. Abdi, “Partial

least squares regression and projection on latent
structure regression (pls regression),” Wiley Interdisciplinary Reviews:
Computational Statistics, vol. 2, no. 1, pp. 97–106, 2010.

[6] V. N. Vapnik, The Nature of Statistical Learning Theory. New York, NY,

USA: Springer-Verlag New York, Inc., 1995.

[7] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector
machines,” ACM Transactions on Intelligent Systems and Technology,
vol. 2, pp. 27:1–27:27, 2011.

