Star-Galaxy Separation in the Era of Precision Cosmology

Michael Baumer, Noah Kurinsky, and Max Zimet

Stanford University, Department of Physics, Stanford, CA 94305

November 14, 2014

Introduction

To anyone who has seen the beautiful images of the
intricate structure in nearby galaxies, distinguish-
ing between stars and galaxies might seem like an
easy problem. However, modern cosmological surveys
such as the Dark Energy Survey (DES) [3] are pri-
marily interested in observing as many distant galax-
ies as possible, as these provide the most useful data
for constraining cosmology (the history of structure
formation in the universe). However, at such vast
distances, both stars and galaxies begin to look like
low-resolution point sources, making it diﬃcult to iso-
late a sample of galaxies (containing interesting cos-
mological information) from intervening dim stars in
our own galaxy.

This challenge, known as “star-galaxy separation”
is a crucial step in any cosmological survey. Perform-
ing this classiﬁcation more accurately can greatly
improve the precision of scientiﬁc insights extracted
from massive galaxy surveys like DES. The most
widely-used methods for star-galaxy separation in-
clude class_star, which is a legacy classiﬁer with a
limited feature set, and spread_model, a linear dis-
criminant based on weighted object size [1]. The per-
formance of both of these methods is insuﬃcient for
the demands of modern precision cosmology, which is
why the application of machine learning techniques
to this problem has attracted recent interest in the
cosmology community [6]. Since data on the real-
world performance of such methods has yet to be
published in the literature, we will use these two clas-
siﬁers as benchmarks for assessing the performance of
our models in this paper.

Data and Feature Selection

We use a matched catalog of objects observed by
both DES, a ground-based observatory, and the Hub-
ble Space Telescope (HST) [4] over 1 square degree
of the sky. The true classiﬁcations (star or galaxy)
are binary labels computed from the more-detailed

Figure 1: In the top false-color image from DES, it is
easy to tell that the large object in the foreground, NCG
1398, is a galaxy, but what about all the point sources
behind it? In the bottom images of our actual data, we
see sky maps of stars (right) and galaxies (left). The voids
seen in the map of galaxies are regions of the sky which
are blocked by bright nearby stars.

HST data. The catalog contains 222,000 sources,
of which 70% (∼150,000) are used for training and
30% (∼70,000) are (randomly) reserved for cross-
validation.

From the 226 variables available in the catalog,
we selected 45 variables which appeared to have dis-
criminating power based on plots like those shown in
Figure 2. Features include the surface brightnesses,
astronomical magnitudes, and sizes of all objects as
observed through 5 diﬀerent color ﬁlters (near ultra-
violet to near infrared). We also use goodness-of-ﬁt
statistics for stellar and galactic luminosity proﬁles

1

Star-Galaxy Separation in the Era of Precision Cosmology

Michael Baumer, Noah Kurinsky, and Max Zimet

Stanford University, Department of Physics, Stanford, CA 94305

November 14, 2014

Introduction

To anyone who has seen the beautiful images of the
intricate structure in nearby galaxies, distinguish-
ing between stars and galaxies might seem like an
easy problem. However, modern cosmological surveys
such as the Dark Energy Survey (DES) [3] are pri-
marily interested in observing as many distant galax-
ies as possible, as these provide the most useful data
for constraining cosmology (the history of structure
formation in the universe). However, at such vast
distances, both stars and galaxies begin to look like
low-resolution point sources, making it diﬃcult to iso-
late a sample of galaxies (containing interesting cos-
mological information) from intervening dim stars in
our own galaxy.

This challenge, known as “star-galaxy separation”
is a crucial step in any cosmological survey. Perform-
ing this classiﬁcation more accurately can greatly
improve the precision of scientiﬁc insights extracted
from massive galaxy surveys like DES. The most
widely-used methods for star-galaxy separation in-
clude class_star, which is a legacy classiﬁer with a
limited feature set, and spread_model, a linear dis-
criminant based on weighted object size [1]. The per-
formance of both of these methods is insuﬃcient for
the demands of modern precision cosmology, which is
why the application of machine learning techniques
to this problem has attracted recent interest in the
cosmology community [6]. Since data on the real-
world performance of such methods has yet to be
published in the literature, we will use these two clas-
siﬁers as benchmarks for assessing the performance of
our models in this paper.

Data and Feature Selection

We use a matched catalog of objects observed by
both DES, a ground-based observatory, and the Hub-
ble Space Telescope (HST) [4] over 1 square degree
of the sky. The true classiﬁcations (star or galaxy)
are binary labels computed from the more-detailed

Figure 1: In the top false-color image from DES, it is
easy to tell that the large object in the foreground, NCG
1398, is a galaxy, but what about all the point sources
behind it? In the bottom images of our actual data, we
see sky maps of stars (right) and galaxies (left). The voids
seen in the map of galaxies are regions of the sky which
are blocked by bright nearby stars.

HST data. The catalog contains 222,000 sources,
of which 70% (∼150,000) are used for training and
30% (∼70,000) are (randomly) reserved for cross-
validation.

From the 226 variables available in the catalog,
we selected 45 variables which appeared to have dis-
criminating power based on plots like those shown in
Figure 2. Features include the surface brightnesses,
astronomical magnitudes, and sizes of all objects as
observed through 5 diﬀerent color ﬁlters (near ultra-
violet to near infrared). We also use goodness-of-ﬁt
statistics for stellar and galactic luminosity proﬁles

1

(a) log χ2 for a star-like luminosity pro-
ﬁle vs. a galaxy-like proﬁle.

log (size)

(b)
maximum) vs. infrared magnitude

(full-width

at

half-

(c) Histograms of mean surface bright-
ness

Figure 2: A selection of catalog variables (before preprocessing) selected as input features to our models.

(which describe the fall-oﬀ of brightness from centers
of objects).

As a preprocessing step, we transformed all vari-
ables to have a mean of 0 and variance of 1. For cer-
tain variables - namely chi-squares and object sizes -
we decided to use their logarithm rather than their
raw value, to increase their discriminating power and
make each feature more Gaussian (see panels (a) and
(b) in Figure 2).

In performing feature selection,

it was impor-
tant to us that features were included in physically-
motivated groups. After making the plot of learning
vs.
included features shown in Figure 3, we imple-
mented a backwards search, using a linear SVM, to
determine the relative power of each of the variables.
We were surprised to ﬁnd that our less promising pa-
rameters (χ2 and surface brightness) were often more
powerful than magnitudes in certain color bands, and
considered removing some magnitudes from our fea-
tures. We ultimately decided against this, however,
given that we wanted to select features in a uniform
manner for all color bands to maintain a physically-
motivated feature set.

Figure 3: A plot of training success vs. features used
for four of our models.

Method

GDA + SMOTE
GNB + SMOTE

LR

LinSVM

GaussianSVM

1 − ˆtrain
66.8%
70.5%
86.8%
89.4%
95.4%

1 − ˆtest
91.4%
91.4%
86.4%
89.0%
94.2%

Methods and Results

Table 1: Training and test error for multiple super-
vised learning methods.

To determine which method would obtain the best
discrimination, we initially ran a handful of basic
algorithms with default parameters and compared
results between them. Given that we had contin-
uous inputs and a binary output, we employed lo-
gistic regression (LR), Gaussian discriminant analy-
sis (GDA), linear- and Gaussian-kernel SVMs, and
Gaussian Naive Bayes (GNB) as classiﬁers. We im-
plemented our models in Python, using the sklearn
module [5].

We opted to employ the (cid:96)1

regularized lin-
ear SVM classiﬁer LinearSVC ((cid:96)2
regularization
gave comparable results), the Naive Bayes algo-
rithm GaussianNB, the logistic regression method
SGDClassifier(loss="log"), and the GDA algo-
rithm, using class weighting where implemented
(namely, for SVM and LR) to compensate for the
much larger number of galaxies than stars in our
training sample. The results of these initial classi-

2

Star-Galaxy Separation in the Era of Precision Cosmology

Michael Baumer, Noah Kurinsky, and Max Zimet

Stanford University, Department of Physics, Stanford, CA 94305

November 14, 2014

Introduction

To anyone who has seen the beautiful images of the
intricate structure in nearby galaxies, distinguish-
ing between stars and galaxies might seem like an
easy problem. However, modern cosmological surveys
such as the Dark Energy Survey (DES) [3] are pri-
marily interested in observing as many distant galax-
ies as possible, as these provide the most useful data
for constraining cosmology (the history of structure
formation in the universe). However, at such vast
distances, both stars and galaxies begin to look like
low-resolution point sources, making it diﬃcult to iso-
late a sample of galaxies (containing interesting cos-
mological information) from intervening dim stars in
our own galaxy.

This challenge, known as “star-galaxy separation”
is a crucial step in any cosmological survey. Perform-
ing this classiﬁcation more accurately can greatly
improve the precision of scientiﬁc insights extracted
from massive galaxy surveys like DES. The most
widely-used methods for star-galaxy separation in-
clude class_star, which is a legacy classiﬁer with a
limited feature set, and spread_model, a linear dis-
criminant based on weighted object size [1]. The per-
formance of both of these methods is insuﬃcient for
the demands of modern precision cosmology, which is
why the application of machine learning techniques
to this problem has attracted recent interest in the
cosmology community [6]. Since data on the real-
world performance of such methods has yet to be
published in the literature, we will use these two clas-
siﬁers as benchmarks for assessing the performance of
our models in this paper.

Data and Feature Selection

We use a matched catalog of objects observed by
both DES, a ground-based observatory, and the Hub-
ble Space Telescope (HST) [4] over 1 square degree
of the sky. The true classiﬁcations (star or galaxy)
are binary labels computed from the more-detailed

Figure 1: In the top false-color image from DES, it is
easy to tell that the large object in the foreground, NCG
1398, is a galaxy, but what about all the point sources
behind it? In the bottom images of our actual data, we
see sky maps of stars (right) and galaxies (left). The voids
seen in the map of galaxies are regions of the sky which
are blocked by bright nearby stars.

HST data. The catalog contains 222,000 sources,
of which 70% (∼150,000) are used for training and
30% (∼70,000) are (randomly) reserved for cross-
validation.

From the 226 variables available in the catalog,
we selected 45 variables which appeared to have dis-
criminating power based on plots like those shown in
Figure 2. Features include the surface brightnesses,
astronomical magnitudes, and sizes of all objects as
observed through 5 diﬀerent color ﬁlters (near ultra-
violet to near infrared). We also use goodness-of-ﬁt
statistics for stellar and galactic luminosity proﬁles

1

(a) log χ2 for a star-like luminosity pro-
ﬁle vs. a galaxy-like proﬁle.

log (size)

(b)
maximum) vs. infrared magnitude

(full-width

at

half-

(c) Histograms of mean surface bright-
ness

Figure 2: A selection of catalog variables (before preprocessing) selected as input features to our models.

(which describe the fall-oﬀ of brightness from centers
of objects).

As a preprocessing step, we transformed all vari-
ables to have a mean of 0 and variance of 1. For cer-
tain variables - namely chi-squares and object sizes -
we decided to use their logarithm rather than their
raw value, to increase their discriminating power and
make each feature more Gaussian (see panels (a) and
(b) in Figure 2).

In performing feature selection,

it was impor-
tant to us that features were included in physically-
motivated groups. After making the plot of learning
vs.
included features shown in Figure 3, we imple-
mented a backwards search, using a linear SVM, to
determine the relative power of each of the variables.
We were surprised to ﬁnd that our less promising pa-
rameters (χ2 and surface brightness) were often more
powerful than magnitudes in certain color bands, and
considered removing some magnitudes from our fea-
tures. We ultimately decided against this, however,
given that we wanted to select features in a uniform
manner for all color bands to maintain a physically-
motivated feature set.

Figure 3: A plot of training success vs. features used
for four of our models.

Method

GDA + SMOTE
GNB + SMOTE

LR

LinSVM

GaussianSVM

1 − ˆtrain
66.8%
70.5%
86.8%
89.4%
95.4%

1 − ˆtest
91.4%
91.4%
86.4%
89.0%
94.2%

Methods and Results

Table 1: Training and test error for multiple super-
vised learning methods.

To determine which method would obtain the best
discrimination, we initially ran a handful of basic
algorithms with default parameters and compared
results between them. Given that we had contin-
uous inputs and a binary output, we employed lo-
gistic regression (LR), Gaussian discriminant analy-
sis (GDA), linear- and Gaussian-kernel SVMs, and
Gaussian Naive Bayes (GNB) as classiﬁers. We im-
plemented our models in Python, using the sklearn
module [5].

We opted to employ the (cid:96)1

regularized lin-
ear SVM classiﬁer LinearSVC ((cid:96)2
regularization
gave comparable results), the Naive Bayes algo-
rithm GaussianNB, the logistic regression method
SGDClassifier(loss="log"), and the GDA algo-
rithm, using class weighting where implemented
(namely, for SVM and LR) to compensate for the
much larger number of galaxies than stars in our
training sample. The results of these initial classi-

2

Method GDA + SMOTE GNB + SMOTE

LR

G

S

G

S

G

S

LinSVM
S
G

GaussianSVM

G

S

True G 95.5%
True S

4.5%
59.8% 40.2 %

95.0%
5.0%
52.3% 47.7 %

87.3% 12.7% 90.2% 9.8% 95.2% 4.8%
19.7% 80.3% 19.8% 80.2% 17.8% 82.2%

Table 2: Confusion matrices produced on our test set for diﬀerent supervised learning techniques, showing
the true/false positive rates for galaxies in the top row and the true/false negative rates for stars in the
bottom row of each entry.

ﬁcations can be seen in table 1, and the confusion
matrices from these runs can be seen in table 2.

The statistics in Table 1 show that we have suﬃ-
cient training data for the LR, GDA, and SVM meth-
ods, since our test error is very similar to our training
error. We decided to proceed further from here with
Naive Bayes/GDA and SVM, in order to continue our
search for a successful generative model and optimize
our best discriminative algorithm.

Gaussian Naive Bayes and GDA with Boot-
strapping and SMOTE We ﬁrst describe the
generative algorithms we applied. Eq. (1) shows the
probability distribution assumption for the Gaussian
Naive Bayes classiﬁer, where xi is the value of the
i-th feature and y is the class (star or galaxy) of the
object under consideration.

n(cid:89)

p(x|y) =

p(xi|y),

xi|y ∼ N (µi,y, σ2

i,y)

(1)

i=1

This makes stronger assumptions than the quadratic
Gaussian Discriminant Analysis model, whose as-
sumed probability distributions are described in Eq.
(2):

x|y ∼ N (µy, Σy).

(2)

In fact, this shows that the GNB model is equivalent
to the quadratic GDA model, if we require in the
latter that the covariance matrices Σy be diagonal.

To correct for the eﬀects of our imbalanced data
set, we applied the bootstrapping technique, whereby
we sampled from our set of galaxies, and trained on
this sample, plus the set of all of our stars; the in-
tent was to take 50 such samples and average the
results of these diﬀerent classiﬁers. However, each
individual classiﬁer still performed poorly, misclas-
sifying most stars, unless we heavily undersampled
from our galaxies (that is, we chose far fewer galaxies
than stars to be in our new data set), in which case
the classiﬁers misclassiﬁed most galaxies.

3

Figure 4: Training success vs. features used for data
with equal numbers of stars and galaxies, produced
using the SMOTE algorithm.

We also performed a synthetic oversampling of
stars using the Synthetic Minority Over-sampling
TechniquE (SMOTE) algorithm [2]. The idea of
SMOTE is to create new training examples which
look like the stars in our data set. Speciﬁcally, given
a training example with features x that is a star, we
choose, uniformly at random, one of the 5 nearest
stars in the training set (where “nearest” means we
use the Euclidean (cid:96)2 norm on the space of our nor-
malized features). Denoting this chosen neighbor by
x(cid:48), we then create a new training example at random
location along the line segment connecting x and x(cid:48).
After oversampling our population of stars using
the SMOTE algorithm to have equal numbers of stars
and galaxies, the training errors for Gaussian Naive
Bayes and GDA suﬀer, as they are no longer able to
succeed by classifying almost everything as a galaxy.
Given that these generative models fail to classify a
balanced dataset well, we conclude that our data does
not satisfy their respective assumptions suﬃciently to
warrant their use. This is not very surprising, as gen-
erative learning models make signiﬁcant assumptions.
As we will see next, the discriminative algorithms we

Star-Galaxy Separation in the Era of Precision Cosmology

Michael Baumer, Noah Kurinsky, and Max Zimet

Stanford University, Department of Physics, Stanford, CA 94305

November 14, 2014

Introduction

To anyone who has seen the beautiful images of the
intricate structure in nearby galaxies, distinguish-
ing between stars and galaxies might seem like an
easy problem. However, modern cosmological surveys
such as the Dark Energy Survey (DES) [3] are pri-
marily interested in observing as many distant galax-
ies as possible, as these provide the most useful data
for constraining cosmology (the history of structure
formation in the universe). However, at such vast
distances, both stars and galaxies begin to look like
low-resolution point sources, making it diﬃcult to iso-
late a sample of galaxies (containing interesting cos-
mological information) from intervening dim stars in
our own galaxy.

This challenge, known as “star-galaxy separation”
is a crucial step in any cosmological survey. Perform-
ing this classiﬁcation more accurately can greatly
improve the precision of scientiﬁc insights extracted
from massive galaxy surveys like DES. The most
widely-used methods for star-galaxy separation in-
clude class_star, which is a legacy classiﬁer with a
limited feature set, and spread_model, a linear dis-
criminant based on weighted object size [1]. The per-
formance of both of these methods is insuﬃcient for
the demands of modern precision cosmology, which is
why the application of machine learning techniques
to this problem has attracted recent interest in the
cosmology community [6]. Since data on the real-
world performance of such methods has yet to be
published in the literature, we will use these two clas-
siﬁers as benchmarks for assessing the performance of
our models in this paper.

Data and Feature Selection

We use a matched catalog of objects observed by
both DES, a ground-based observatory, and the Hub-
ble Space Telescope (HST) [4] over 1 square degree
of the sky. The true classiﬁcations (star or galaxy)
are binary labels computed from the more-detailed

Figure 1: In the top false-color image from DES, it is
easy to tell that the large object in the foreground, NCG
1398, is a galaxy, but what about all the point sources
behind it? In the bottom images of our actual data, we
see sky maps of stars (right) and galaxies (left). The voids
seen in the map of galaxies are regions of the sky which
are blocked by bright nearby stars.

HST data. The catalog contains 222,000 sources,
of which 70% (∼150,000) are used for training and
30% (∼70,000) are (randomly) reserved for cross-
validation.

From the 226 variables available in the catalog,
we selected 45 variables which appeared to have dis-
criminating power based on plots like those shown in
Figure 2. Features include the surface brightnesses,
astronomical magnitudes, and sizes of all objects as
observed through 5 diﬀerent color ﬁlters (near ultra-
violet to near infrared). We also use goodness-of-ﬁt
statistics for stellar and galactic luminosity proﬁles

1

(a) log χ2 for a star-like luminosity pro-
ﬁle vs. a galaxy-like proﬁle.

log (size)

(b)
maximum) vs. infrared magnitude

(full-width

at

half-

(c) Histograms of mean surface bright-
ness

Figure 2: A selection of catalog variables (before preprocessing) selected as input features to our models.

(which describe the fall-oﬀ of brightness from centers
of objects).

As a preprocessing step, we transformed all vari-
ables to have a mean of 0 and variance of 1. For cer-
tain variables - namely chi-squares and object sizes -
we decided to use their logarithm rather than their
raw value, to increase their discriminating power and
make each feature more Gaussian (see panels (a) and
(b) in Figure 2).

In performing feature selection,

it was impor-
tant to us that features were included in physically-
motivated groups. After making the plot of learning
vs.
included features shown in Figure 3, we imple-
mented a backwards search, using a linear SVM, to
determine the relative power of each of the variables.
We were surprised to ﬁnd that our less promising pa-
rameters (χ2 and surface brightness) were often more
powerful than magnitudes in certain color bands, and
considered removing some magnitudes from our fea-
tures. We ultimately decided against this, however,
given that we wanted to select features in a uniform
manner for all color bands to maintain a physically-
motivated feature set.

Figure 3: A plot of training success vs. features used
for four of our models.

Method

GDA + SMOTE
GNB + SMOTE

LR

LinSVM

GaussianSVM

1 − ˆtrain
66.8%
70.5%
86.8%
89.4%
95.4%

1 − ˆtest
91.4%
91.4%
86.4%
89.0%
94.2%

Methods and Results

Table 1: Training and test error for multiple super-
vised learning methods.

To determine which method would obtain the best
discrimination, we initially ran a handful of basic
algorithms with default parameters and compared
results between them. Given that we had contin-
uous inputs and a binary output, we employed lo-
gistic regression (LR), Gaussian discriminant analy-
sis (GDA), linear- and Gaussian-kernel SVMs, and
Gaussian Naive Bayes (GNB) as classiﬁers. We im-
plemented our models in Python, using the sklearn
module [5].

We opted to employ the (cid:96)1

regularized lin-
ear SVM classiﬁer LinearSVC ((cid:96)2
regularization
gave comparable results), the Naive Bayes algo-
rithm GaussianNB, the logistic regression method
SGDClassifier(loss="log"), and the GDA algo-
rithm, using class weighting where implemented
(namely, for SVM and LR) to compensate for the
much larger number of galaxies than stars in our
training sample. The results of these initial classi-

2

Method GDA + SMOTE GNB + SMOTE

LR

G

S

G

S

G

S

LinSVM
S
G

GaussianSVM

G

S

True G 95.5%
True S

4.5%
59.8% 40.2 %

95.0%
5.0%
52.3% 47.7 %

87.3% 12.7% 90.2% 9.8% 95.2% 4.8%
19.7% 80.3% 19.8% 80.2% 17.8% 82.2%

Table 2: Confusion matrices produced on our test set for diﬀerent supervised learning techniques, showing
the true/false positive rates for galaxies in the top row and the true/false negative rates for stars in the
bottom row of each entry.

ﬁcations can be seen in table 1, and the confusion
matrices from these runs can be seen in table 2.

The statistics in Table 1 show that we have suﬃ-
cient training data for the LR, GDA, and SVM meth-
ods, since our test error is very similar to our training
error. We decided to proceed further from here with
Naive Bayes/GDA and SVM, in order to continue our
search for a successful generative model and optimize
our best discriminative algorithm.

Gaussian Naive Bayes and GDA with Boot-
strapping and SMOTE We ﬁrst describe the
generative algorithms we applied. Eq. (1) shows the
probability distribution assumption for the Gaussian
Naive Bayes classiﬁer, where xi is the value of the
i-th feature and y is the class (star or galaxy) of the
object under consideration.

n(cid:89)

p(x|y) =

p(xi|y),

xi|y ∼ N (µi,y, σ2

i,y)

(1)

i=1

This makes stronger assumptions than the quadratic
Gaussian Discriminant Analysis model, whose as-
sumed probability distributions are described in Eq.
(2):

x|y ∼ N (µy, Σy).

(2)

In fact, this shows that the GNB model is equivalent
to the quadratic GDA model, if we require in the
latter that the covariance matrices Σy be diagonal.

To correct for the eﬀects of our imbalanced data
set, we applied the bootstrapping technique, whereby
we sampled from our set of galaxies, and trained on
this sample, plus the set of all of our stars; the in-
tent was to take 50 such samples and average the
results of these diﬀerent classiﬁers. However, each
individual classiﬁer still performed poorly, misclas-
sifying most stars, unless we heavily undersampled
from our galaxies (that is, we chose far fewer galaxies
than stars to be in our new data set), in which case
the classiﬁers misclassiﬁed most galaxies.

3

Figure 4: Training success vs. features used for data
with equal numbers of stars and galaxies, produced
using the SMOTE algorithm.

We also performed a synthetic oversampling of
stars using the Synthetic Minority Over-sampling
TechniquE (SMOTE) algorithm [2]. The idea of
SMOTE is to create new training examples which
look like the stars in our data set. Speciﬁcally, given
a training example with features x that is a star, we
choose, uniformly at random, one of the 5 nearest
stars in the training set (where “nearest” means we
use the Euclidean (cid:96)2 norm on the space of our nor-
malized features). Denoting this chosen neighbor by
x(cid:48), we then create a new training example at random
location along the line segment connecting x and x(cid:48).
After oversampling our population of stars using
the SMOTE algorithm to have equal numbers of stars
and galaxies, the training errors for Gaussian Naive
Bayes and GDA suﬀer, as they are no longer able to
succeed by classifying almost everything as a galaxy.
Given that these generative models fail to classify a
balanced dataset well, we conclude that our data does
not satisfy their respective assumptions suﬃciently to
warrant their use. This is not very surprising, as gen-
erative learning models make signiﬁcant assumptions.
As we will see next, the discriminative algorithms we

Figure 5: Results of two rounds of Gaussian SVM optimizations, varying the parameters C and γ logarith-
mically. The leftmost plot shows C ∈ {0.1, 10.0} and γ ∈ {0, 10−3}, while the middle and right plots show
C ∈ {10.0, 1000.0} and γ ∈ {0.01, 10−5}. The left two plots show training versus test success percentages,
while the rightmost plot compares success for stars and galaxies over the range of our better performing grid
search.

used performed much better.

Grid-Search Optimized SVM with Gaussian
Kernel, L1-Regularization Given the good per-
formance of the Linear SVM and a preliminary run of
the SVM with Gaussian Kernel, we decided to try to
optimize the (cid:96)1-regularized SVM by performing a grid
search across the parameter space C ∈ {10−3, 103}
and γ ∈ {10−5, 1} spaced logarithmically in powers
of 101. We employed the SVC method of sklearn,
a python wrapper for libSVM, which provides auto-
matic class weighting to counteract our unbalanced
training sample. We used the barley batch server to
run these grid samples in parallel, and had SVC record
the training error, test error, and confusion matrices
for each model. The resulting scores from these opti-
mizations can be seen in Figure 5.

Our model choice was based on of the requirement
for lowest generalization error, highest test success,
and best separation for stars, as a classiﬁer which
classiﬁed every point as a galaxy would have a good
overall test performance but perform no source sep-
aration, and thus perform poorly on stars.
In Fig-
ure 5, this was performed functionally by selecting,
ﬁrst, the model with the highest test success, then
among those the models closest to their correspond-

1At the poster session, our TAs suggested importance sam-
pling or random sampling as alternatives to a grid search.
Given the average run time of between 1 to 10 hours per opti-
mization, and the fact that importance sampling is an iterative
process, we opted for grid search to reduce computing time. It
can be seen in our ﬁgures that our optimization is fairly con-
vex, so the grid search appears to be suﬃciently accurate. See
also csie.ntu.edu.tw/∼cjlin/papers/guide/guide.pdf.

Figure 6: Histogram of the margin from the opti-
mal separation plane (as determined by our Gaussian
SVM) of each training example; a negative margin
corresponds to a galaxy, positive to a star.

ing training success rates, to minimize overﬁtting.
Then, among this subset of models, we chose the
model which performed best separating stars while
not compromising galaxy separation performance.

The sum total of these considerations leads us to
select the model with C = 100.0, γ = 0.01, which has
the confusion matrix shown in table 2 and an over-
all test success rate of 94.2%. Out of all methods
tested, this SVM had the highest generalized success
rate and best performance separating stars, at 82%
success. The distance of each example from the sepa-
rating hyperplane of the SVM can be seen plotted as a
histogram separately for stars and galaxies in Figure

4

Star-Galaxy Separation in the Era of Precision Cosmology

Michael Baumer, Noah Kurinsky, and Max Zimet

Stanford University, Department of Physics, Stanford, CA 94305

November 14, 2014

Introduction

To anyone who has seen the beautiful images of the
intricate structure in nearby galaxies, distinguish-
ing between stars and galaxies might seem like an
easy problem. However, modern cosmological surveys
such as the Dark Energy Survey (DES) [3] are pri-
marily interested in observing as many distant galax-
ies as possible, as these provide the most useful data
for constraining cosmology (the history of structure
formation in the universe). However, at such vast
distances, both stars and galaxies begin to look like
low-resolution point sources, making it diﬃcult to iso-
late a sample of galaxies (containing interesting cos-
mological information) from intervening dim stars in
our own galaxy.

This challenge, known as “star-galaxy separation”
is a crucial step in any cosmological survey. Perform-
ing this classiﬁcation more accurately can greatly
improve the precision of scientiﬁc insights extracted
from massive galaxy surveys like DES. The most
widely-used methods for star-galaxy separation in-
clude class_star, which is a legacy classiﬁer with a
limited feature set, and spread_model, a linear dis-
criminant based on weighted object size [1]. The per-
formance of both of these methods is insuﬃcient for
the demands of modern precision cosmology, which is
why the application of machine learning techniques
to this problem has attracted recent interest in the
cosmology community [6]. Since data on the real-
world performance of such methods has yet to be
published in the literature, we will use these two clas-
siﬁers as benchmarks for assessing the performance of
our models in this paper.

Data and Feature Selection

We use a matched catalog of objects observed by
both DES, a ground-based observatory, and the Hub-
ble Space Telescope (HST) [4] over 1 square degree
of the sky. The true classiﬁcations (star or galaxy)
are binary labels computed from the more-detailed

Figure 1: In the top false-color image from DES, it is
easy to tell that the large object in the foreground, NCG
1398, is a galaxy, but what about all the point sources
behind it? In the bottom images of our actual data, we
see sky maps of stars (right) and galaxies (left). The voids
seen in the map of galaxies are regions of the sky which
are blocked by bright nearby stars.

HST data. The catalog contains 222,000 sources,
of which 70% (∼150,000) are used for training and
30% (∼70,000) are (randomly) reserved for cross-
validation.

From the 226 variables available in the catalog,
we selected 45 variables which appeared to have dis-
criminating power based on plots like those shown in
Figure 2. Features include the surface brightnesses,
astronomical magnitudes, and sizes of all objects as
observed through 5 diﬀerent color ﬁlters (near ultra-
violet to near infrared). We also use goodness-of-ﬁt
statistics for stellar and galactic luminosity proﬁles

1

(a) log χ2 for a star-like luminosity pro-
ﬁle vs. a galaxy-like proﬁle.

log (size)

(b)
maximum) vs. infrared magnitude

(full-width

at

half-

(c) Histograms of mean surface bright-
ness

Figure 2: A selection of catalog variables (before preprocessing) selected as input features to our models.

(which describe the fall-oﬀ of brightness from centers
of objects).

As a preprocessing step, we transformed all vari-
ables to have a mean of 0 and variance of 1. For cer-
tain variables - namely chi-squares and object sizes -
we decided to use their logarithm rather than their
raw value, to increase their discriminating power and
make each feature more Gaussian (see panels (a) and
(b) in Figure 2).

In performing feature selection,

it was impor-
tant to us that features were included in physically-
motivated groups. After making the plot of learning
vs.
included features shown in Figure 3, we imple-
mented a backwards search, using a linear SVM, to
determine the relative power of each of the variables.
We were surprised to ﬁnd that our less promising pa-
rameters (χ2 and surface brightness) were often more
powerful than magnitudes in certain color bands, and
considered removing some magnitudes from our fea-
tures. We ultimately decided against this, however,
given that we wanted to select features in a uniform
manner for all color bands to maintain a physically-
motivated feature set.

Figure 3: A plot of training success vs. features used
for four of our models.

Method

GDA + SMOTE
GNB + SMOTE

LR

LinSVM

GaussianSVM

1 − ˆtrain
66.8%
70.5%
86.8%
89.4%
95.4%

1 − ˆtest
91.4%
91.4%
86.4%
89.0%
94.2%

Methods and Results

Table 1: Training and test error for multiple super-
vised learning methods.

To determine which method would obtain the best
discrimination, we initially ran a handful of basic
algorithms with default parameters and compared
results between them. Given that we had contin-
uous inputs and a binary output, we employed lo-
gistic regression (LR), Gaussian discriminant analy-
sis (GDA), linear- and Gaussian-kernel SVMs, and
Gaussian Naive Bayes (GNB) as classiﬁers. We im-
plemented our models in Python, using the sklearn
module [5].

We opted to employ the (cid:96)1

regularized lin-
ear SVM classiﬁer LinearSVC ((cid:96)2
regularization
gave comparable results), the Naive Bayes algo-
rithm GaussianNB, the logistic regression method
SGDClassifier(loss="log"), and the GDA algo-
rithm, using class weighting where implemented
(namely, for SVM and LR) to compensate for the
much larger number of galaxies than stars in our
training sample. The results of these initial classi-

2

Method GDA + SMOTE GNB + SMOTE

LR

G

S

G

S

G

S

LinSVM
S
G

GaussianSVM

G

S

True G 95.5%
True S

4.5%
59.8% 40.2 %

95.0%
5.0%
52.3% 47.7 %

87.3% 12.7% 90.2% 9.8% 95.2% 4.8%
19.7% 80.3% 19.8% 80.2% 17.8% 82.2%

Table 2: Confusion matrices produced on our test set for diﬀerent supervised learning techniques, showing
the true/false positive rates for galaxies in the top row and the true/false negative rates for stars in the
bottom row of each entry.

ﬁcations can be seen in table 1, and the confusion
matrices from these runs can be seen in table 2.

The statistics in Table 1 show that we have suﬃ-
cient training data for the LR, GDA, and SVM meth-
ods, since our test error is very similar to our training
error. We decided to proceed further from here with
Naive Bayes/GDA and SVM, in order to continue our
search for a successful generative model and optimize
our best discriminative algorithm.

Gaussian Naive Bayes and GDA with Boot-
strapping and SMOTE We ﬁrst describe the
generative algorithms we applied. Eq. (1) shows the
probability distribution assumption for the Gaussian
Naive Bayes classiﬁer, where xi is the value of the
i-th feature and y is the class (star or galaxy) of the
object under consideration.

n(cid:89)

p(x|y) =

p(xi|y),

xi|y ∼ N (µi,y, σ2

i,y)

(1)

i=1

This makes stronger assumptions than the quadratic
Gaussian Discriminant Analysis model, whose as-
sumed probability distributions are described in Eq.
(2):

x|y ∼ N (µy, Σy).

(2)

In fact, this shows that the GNB model is equivalent
to the quadratic GDA model, if we require in the
latter that the covariance matrices Σy be diagonal.

To correct for the eﬀects of our imbalanced data
set, we applied the bootstrapping technique, whereby
we sampled from our set of galaxies, and trained on
this sample, plus the set of all of our stars; the in-
tent was to take 50 such samples and average the
results of these diﬀerent classiﬁers. However, each
individual classiﬁer still performed poorly, misclas-
sifying most stars, unless we heavily undersampled
from our galaxies (that is, we chose far fewer galaxies
than stars to be in our new data set), in which case
the classiﬁers misclassiﬁed most galaxies.

3

Figure 4: Training success vs. features used for data
with equal numbers of stars and galaxies, produced
using the SMOTE algorithm.

We also performed a synthetic oversampling of
stars using the Synthetic Minority Over-sampling
TechniquE (SMOTE) algorithm [2]. The idea of
SMOTE is to create new training examples which
look like the stars in our data set. Speciﬁcally, given
a training example with features x that is a star, we
choose, uniformly at random, one of the 5 nearest
stars in the training set (where “nearest” means we
use the Euclidean (cid:96)2 norm on the space of our nor-
malized features). Denoting this chosen neighbor by
x(cid:48), we then create a new training example at random
location along the line segment connecting x and x(cid:48).
After oversampling our population of stars using
the SMOTE algorithm to have equal numbers of stars
and galaxies, the training errors for Gaussian Naive
Bayes and GDA suﬀer, as they are no longer able to
succeed by classifying almost everything as a galaxy.
Given that these generative models fail to classify a
balanced dataset well, we conclude that our data does
not satisfy their respective assumptions suﬃciently to
warrant their use. This is not very surprising, as gen-
erative learning models make signiﬁcant assumptions.
As we will see next, the discriminative algorithms we

Figure 5: Results of two rounds of Gaussian SVM optimizations, varying the parameters C and γ logarith-
mically. The leftmost plot shows C ∈ {0.1, 10.0} and γ ∈ {0, 10−3}, while the middle and right plots show
C ∈ {10.0, 1000.0} and γ ∈ {0.01, 10−5}. The left two plots show training versus test success percentages,
while the rightmost plot compares success for stars and galaxies over the range of our better performing grid
search.

used performed much better.

Grid-Search Optimized SVM with Gaussian
Kernel, L1-Regularization Given the good per-
formance of the Linear SVM and a preliminary run of
the SVM with Gaussian Kernel, we decided to try to
optimize the (cid:96)1-regularized SVM by performing a grid
search across the parameter space C ∈ {10−3, 103}
and γ ∈ {10−5, 1} spaced logarithmically in powers
of 101. We employed the SVC method of sklearn,
a python wrapper for libSVM, which provides auto-
matic class weighting to counteract our unbalanced
training sample. We used the barley batch server to
run these grid samples in parallel, and had SVC record
the training error, test error, and confusion matrices
for each model. The resulting scores from these opti-
mizations can be seen in Figure 5.

Our model choice was based on of the requirement
for lowest generalization error, highest test success,
and best separation for stars, as a classiﬁer which
classiﬁed every point as a galaxy would have a good
overall test performance but perform no source sep-
aration, and thus perform poorly on stars.
In Fig-
ure 5, this was performed functionally by selecting,
ﬁrst, the model with the highest test success, then
among those the models closest to their correspond-

1At the poster session, our TAs suggested importance sam-
pling or random sampling as alternatives to a grid search.
Given the average run time of between 1 to 10 hours per opti-
mization, and the fact that importance sampling is an iterative
process, we opted for grid search to reduce computing time. It
can be seen in our ﬁgures that our optimization is fairly con-
vex, so the grid search appears to be suﬃciently accurate. See
also csie.ntu.edu.tw/∼cjlin/papers/guide/guide.pdf.

Figure 6: Histogram of the margin from the opti-
mal separation plane (as determined by our Gaussian
SVM) of each training example; a negative margin
corresponds to a galaxy, positive to a star.

ing training success rates, to minimize overﬁtting.
Then, among this subset of models, we chose the
model which performed best separating stars while
not compromising galaxy separation performance.

The sum total of these considerations leads us to
select the model with C = 100.0, γ = 0.01, which has
the confusion matrix shown in table 2 and an over-
all test success rate of 94.2%. Out of all methods
tested, this SVM had the highest generalized success
rate and best performance separating stars, at 82%
success. The distance of each example from the sepa-
rating hyperplane of the SVM can be seen plotted as a
histogram separately for stars and galaxies in Figure

4

6. The factor which most limits the star separation
performance seems to be the multimodal distribution
of stars in our feature space, one of which is highly
overlapping with galaxies. It is interesting to note the
highly gaussian distribution of galaxy margins from
the plane, indicating that these were better modeled,
and overall had more continuous properties.

Discussion

Since we determined that the generative models
we attempted to use were not appropriate for our
dataset, we are left with three successful discrimi-
native models: logistic regression, Linear SVM, and
Gaussian SVM. We used a Receiver Operating Char-
acteristic (ROC) curve to characterize the perfor-
mance of our benchmark models, class_star and
spread_model, since they both give continuous out-
put scores. Since our three models all give binary
outputs, they appear as points on the ROC curve,
as shown in Figure 7. We see that all three of our
models signiﬁcantly outperform the two benchmarks!
As noted in the previous section, the multimodal
distribution of stars was the limiting factor in our
ability to separate these object classes. Regardless of
the model, we rarely saw a “true star rate” greater
than about 80%, suggesting this class of stars com-
prises on average 20% of observed objects, and may
be worth modeling. Given the high dimensionality
of our features, it was not obvious what feature may
have set these stars apart, but it is clear that they are
confounded with similar galaxies, though galaxies do
not show similar substructure.

Figure 7: An ROC curve for our best three supervised
learning methods compared to benchmark classiﬁers
from the literature.

boundary indicates that there might be a class of
stars that is not properly modeled by our current fea-
ture set. This motivates the future work of seeking
out other astronomical catalogs with more or diﬀer-
ent features to enable better modeling of our stellar
population.
In addition, though we chose to focus
on algorithms discussed in this course, deep learn-
ing also has great potential for improving star-galaxy
separation. Such algorithms are the focus of some
bleeding-edge cosmology research [6], though their
performance on current-generation survey data has
yet to be published.

References

Conclusions and Future Work

We have shown that machine learning techniques
are remarkably successful
in addressing the chal-
lenges of star-galaxy separation for modern cosmol-
ogy. Though the assumptions of our generative
models–GDA and Naive Bayes–were not borne out
in the data, causing them to perform poorly, we
had success with logistic regression and SVMs, and
the largest challenge was in feature standardization
and SVM optimization. Our best model, the Gaus-
sian SVM, achieved very good performance, classify-
ing 95.2% of true galaxies correctly, while achieving
82.2% accuracy in classifying true stars, surpassing
both of our benchmark classiﬁers.

The distribution of stars near the SVM decision

[1] Bertin & Arnouts (1996). SExtractor: Software

for Source Extraction. A&AS : 117, 393.

[2] Chawla et al (2002). SMOTE: Synthetic Minor-
ity Over-sampling Technique. JAIR: 16, pp. 321-
357.

[3] DES Collaboration (2005). The Dark Energy

Survey. arXiv:astro-ph/0510346.

[4] Koekemoer et al (2007). The COSMOS Sur-
vey: Hubble Space Telescope Advanced Camera
for Surveys Observations and Data Processing.
ApJS : 172, 196.

[5] Pedregosa et al (2011). Scikit-learn: Machine
Learning in Python. JMLR: 12, pp. 2825-2830.
[6] Soumagnac et al (2013). Star/galaxy separation
at faint magnitudes: Application to a simulated
Dark Energy Survey. arXiv:1306.5236

5

