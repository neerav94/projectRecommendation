Bike Share Usage Prediction in London

Ford Rylander
fordr@stanford.edu

Bo Peng

bpeng@stanford.edu

Jeff Wheeler

jeffsw@stanford.edu

Abstract—The rapid growth in recent years of bike shar-
ing around the world has led to real challenges balancing
supply and demand. To provide a possible solution for this
situation, bike sharing usage prediction is critical. In this
study, a model based on K-mean clustering and polynomial
regression is applied to predict the bike sharing usage in
a given time period and station. London bike sharing data
are used for training and testing our model.

I. INTRODUCTION

Bike sharing is a short-distance bike rental ser-
vice that allows customers to pick up a bike at
one station, use it for transportation as needed,
and return it to another station. This transportation
model delivers customers the conveniences of a bike
without the headaches of bike ownership including
theft, parking, storage and routine maintenance.

Demand for bike sharing services has exploded
times; as of June 2014, public bike-
in recent
sharing systems can be found in 712 cities on ﬁve
continents, with approximately 806,200 bicycles in
circulation at a total of 37,500 stations [1].

Demand prediction at bicycle sharing stations is
crucial to ensure that there are neither too many
bikes (and hence not enough bikes at some other
station that needs them) nor too few bikes (and
thus lost proﬁts). This paper presents the ﬁndings
of an algorithm that utilizes historical usage and
weather data to accurately predict the number of
bikes required at a given station in London.

An examination of prior arts reveals several pro-
posed methods to predict bike demand including an
autoregressive moving average (ARMA) algorithm
[2], linear regression, and support vector machine
(SVM) regression [3]. The ARMA model leverages
time-domain autocorrelation to highlight underlying
periodicity in the bike demand data, linear regres-
sion uses a computationally efﬁcient yet powerful
model to ﬁt the time-series data, and SVM regres-
sion is more computationally intensive. Taking the

pros and cons of these prior art approaches into
consideration, this paper presents the performance
of a 2nd-order polynomial model that is ﬁt after
clustering the training set using the geographical
coordinates of the start stations.

II. DATA

Historical bike share station demand and weather
data are publicly available for download as a list
of rides and weather data in a given day. The
bike rental raw data includes time and location
information for each rental. These raw data are then
processed with a binning algorithm to separate the
data in the required time-segment granularity and
then time-correlating this with the weather data. The
ride counts are then shifted by several time intervals
and joined back with the original data to generate
the historic features.

The list of all bike rides is available directly from

Transport for London [4].

III. FEATURES

We designed a pipeline structure that transforms
our basic feature set into a 2nd-order polynomial
feature set, and then reduces it to only the best
features.

X — Original Feature Set

We have aggregated these data into a single
design matrix, where each row represents an hour-
long segment of time for any given station. Each
row in X is given by

x(i) = [d1 d2 d3 d7 weather mean temp . . .

weekday is weekend . . .
h1 . . . h5]T ∈ R14.

The values in this vector are given by

Bike Share Usage Prediction in London

Ford Rylander
fordr@stanford.edu

Bo Peng

bpeng@stanford.edu

Jeff Wheeler

jeffsw@stanford.edu

Abstract—The rapid growth in recent years of bike shar-
ing around the world has led to real challenges balancing
supply and demand. To provide a possible solution for this
situation, bike sharing usage prediction is critical. In this
study, a model based on K-mean clustering and polynomial
regression is applied to predict the bike sharing usage in
a given time period and station. London bike sharing data
are used for training and testing our model.

I. INTRODUCTION

Bike sharing is a short-distance bike rental ser-
vice that allows customers to pick up a bike at
one station, use it for transportation as needed,
and return it to another station. This transportation
model delivers customers the conveniences of a bike
without the headaches of bike ownership including
theft, parking, storage and routine maintenance.

Demand for bike sharing services has exploded
times; as of June 2014, public bike-
in recent
sharing systems can be found in 712 cities on ﬁve
continents, with approximately 806,200 bicycles in
circulation at a total of 37,500 stations [1].

Demand prediction at bicycle sharing stations is
crucial to ensure that there are neither too many
bikes (and hence not enough bikes at some other
station that needs them) nor too few bikes (and
thus lost proﬁts). This paper presents the ﬁndings
of an algorithm that utilizes historical usage and
weather data to accurately predict the number of
bikes required at a given station in London.

An examination of prior arts reveals several pro-
posed methods to predict bike demand including an
autoregressive moving average (ARMA) algorithm
[2], linear regression, and support vector machine
(SVM) regression [3]. The ARMA model leverages
time-domain autocorrelation to highlight underlying
periodicity in the bike demand data, linear regres-
sion uses a computationally efﬁcient yet powerful
model to ﬁt the time-series data, and SVM regres-
sion is more computationally intensive. Taking the

pros and cons of these prior art approaches into
consideration, this paper presents the performance
of a 2nd-order polynomial model that is ﬁt after
clustering the training set using the geographical
coordinates of the start stations.

II. DATA

Historical bike share station demand and weather
data are publicly available for download as a list
of rides and weather data in a given day. The
bike rental raw data includes time and location
information for each rental. These raw data are then
processed with a binning algorithm to separate the
data in the required time-segment granularity and
then time-correlating this with the weather data. The
ride counts are then shifted by several time intervals
and joined back with the original data to generate
the historic features.

The list of all bike rides is available directly from

Transport for London [4].

III. FEATURES

We designed a pipeline structure that transforms
our basic feature set into a 2nd-order polynomial
feature set, and then reduces it to only the best
features.

X — Original Feature Set

We have aggregated these data into a single
design matrix, where each row represents an hour-
long segment of time for any given station. Each
row in X is given by

x(i) = [d1 d2 d3 d7 weather mean temp . . .

weekday is weekend . . .
h1 . . . h5]T ∈ R14.

The values in this vector are given by

1-15

16-30

31-50

Cluster11 → LR Cluster12 → LR Cluster13 → LR Cluster14 → LR
Cluster21 → LR Cluster22 → LR Cluster23 → LR Cluster24 → LR
Cluster31 → LR Cluster32 → LR Cluster33 → LR Cluster34 → LR Cluster35 → LR Cluster36 → LR

Figure 1. Geographic-Based K-Means on Stations Grouped by Overall Popularity

d1, d2, d3, d7

dn is the number of rides during the same
hour n days prior.

weather

is a binary representation of the weather
events from the day of x(i).

mean temp

is the mean temperature on the day of x(i).

weekday

is an integer representation of the day of
the week, with weekday ∈ [0, 6].

is a single bit that encodes whether x(i) is
on a weekend or not.

hn is the demand n hours prior to the hour
of x(i).

is weekend

h1, . . . ,h5

⇓

X(cid:48) — Polynomial Feature Set

We create X(cid:48) by applying a 2nd-order polynomial

transformation to X. Each row is given by

x(cid:48)(i) = [1 x(i)
x(i)
1 x(i)

1

1

x(i)
2
x(i)
1 x(i)

2

. . . x(i)

14 . . .
. . . x(i)

14 ]T ∈ R120.

14 x(i)

⇓

X(cid:48)(cid:48) — Reduced to Best Features

Finally, we perform feature selection to limit the
variance problems and reduce regression complexity
by selecting the best 75 features:

x(cid:48)(cid:48)(i) = f eature selection(x(cid:48)(i)) ∈ R75.

We chose 75 features experimentally by optimiz-
ing the test error, as shown in Figure 2. With large
numbers of features, the test error degrades because
we overﬁt the training samples.

Figure 2. Selecting number of features

IV. MODELS

We divide the dataset into stations by overall
popularity, and then cluster these divided data based
on GPS coordinates of each starting station. In
each cluster (denoted as Clusterrs in Figure 1),
we ﬁt a 2nd-degree polynomial model using linear
regression. When predicting, we ﬁnd the cluster
closest to the station whose demand we’re trying to
predict and use the ﬁtted polynomial for that cluster.
We use more data for the less popular stations (31-
50), while reducing the number of stations in each
cluster by increasing the number of clusters. This
is necessary because the less popular stations have
data which is more sparse, and thus we need more
data to train on.

A 2nd-order polynomial was chosen experimen-
tally to optimize the performance of the model. The
performance of the model with varying degrees is
shown in Figure 3.

The K-Means within each set of stations uses the
GPS coordinates to ﬁnd stations that are located
near to each other. We train on features g(i) given
by

(cid:20) latitude

(cid:21)

g(i) =

longitude

∈ R2,

and then perform K-Means by optimizing the

NumberofFeatures020406080100Score0.70.750.80.850.9FeatureSelectiontoOptimizePerformanceTesterrorTrainerrorBike Share Usage Prediction in London

Ford Rylander
fordr@stanford.edu

Bo Peng

bpeng@stanford.edu

Jeff Wheeler

jeffsw@stanford.edu

Abstract—The rapid growth in recent years of bike shar-
ing around the world has led to real challenges balancing
supply and demand. To provide a possible solution for this
situation, bike sharing usage prediction is critical. In this
study, a model based on K-mean clustering and polynomial
regression is applied to predict the bike sharing usage in
a given time period and station. London bike sharing data
are used for training and testing our model.

I. INTRODUCTION

Bike sharing is a short-distance bike rental ser-
vice that allows customers to pick up a bike at
one station, use it for transportation as needed,
and return it to another station. This transportation
model delivers customers the conveniences of a bike
without the headaches of bike ownership including
theft, parking, storage and routine maintenance.

Demand for bike sharing services has exploded
times; as of June 2014, public bike-
in recent
sharing systems can be found in 712 cities on ﬁve
continents, with approximately 806,200 bicycles in
circulation at a total of 37,500 stations [1].

Demand prediction at bicycle sharing stations is
crucial to ensure that there are neither too many
bikes (and hence not enough bikes at some other
station that needs them) nor too few bikes (and
thus lost proﬁts). This paper presents the ﬁndings
of an algorithm that utilizes historical usage and
weather data to accurately predict the number of
bikes required at a given station in London.

An examination of prior arts reveals several pro-
posed methods to predict bike demand including an
autoregressive moving average (ARMA) algorithm
[2], linear regression, and support vector machine
(SVM) regression [3]. The ARMA model leverages
time-domain autocorrelation to highlight underlying
periodicity in the bike demand data, linear regres-
sion uses a computationally efﬁcient yet powerful
model to ﬁt the time-series data, and SVM regres-
sion is more computationally intensive. Taking the

pros and cons of these prior art approaches into
consideration, this paper presents the performance
of a 2nd-order polynomial model that is ﬁt after
clustering the training set using the geographical
coordinates of the start stations.

II. DATA

Historical bike share station demand and weather
data are publicly available for download as a list
of rides and weather data in a given day. The
bike rental raw data includes time and location
information for each rental. These raw data are then
processed with a binning algorithm to separate the
data in the required time-segment granularity and
then time-correlating this with the weather data. The
ride counts are then shifted by several time intervals
and joined back with the original data to generate
the historic features.

The list of all bike rides is available directly from

Transport for London [4].

III. FEATURES

We designed a pipeline structure that transforms
our basic feature set into a 2nd-order polynomial
feature set, and then reduces it to only the best
features.

X — Original Feature Set

We have aggregated these data into a single
design matrix, where each row represents an hour-
long segment of time for any given station. Each
row in X is given by

x(i) = [d1 d2 d3 d7 weather mean temp . . .

weekday is weekend . . .
h1 . . . h5]T ∈ R14.

The values in this vector are given by

1-15

16-30

31-50

Cluster11 → LR Cluster12 → LR Cluster13 → LR Cluster14 → LR
Cluster21 → LR Cluster22 → LR Cluster23 → LR Cluster24 → LR
Cluster31 → LR Cluster32 → LR Cluster33 → LR Cluster34 → LR Cluster35 → LR Cluster36 → LR

Figure 1. Geographic-Based K-Means on Stations Grouped by Overall Popularity

d1, d2, d3, d7

dn is the number of rides during the same
hour n days prior.

weather

is a binary representation of the weather
events from the day of x(i).

mean temp

is the mean temperature on the day of x(i).

weekday

is an integer representation of the day of
the week, with weekday ∈ [0, 6].

is a single bit that encodes whether x(i) is
on a weekend or not.

hn is the demand n hours prior to the hour
of x(i).

is weekend

h1, . . . ,h5

⇓

X(cid:48) — Polynomial Feature Set

We create X(cid:48) by applying a 2nd-order polynomial

transformation to X. Each row is given by

x(cid:48)(i) = [1 x(i)
x(i)
1 x(i)

1

1

x(i)
2
x(i)
1 x(i)

2

. . . x(i)

14 . . .
. . . x(i)

14 ]T ∈ R120.

14 x(i)

⇓

X(cid:48)(cid:48) — Reduced to Best Features

Finally, we perform feature selection to limit the
variance problems and reduce regression complexity
by selecting the best 75 features:

x(cid:48)(cid:48)(i) = f eature selection(x(cid:48)(i)) ∈ R75.

We chose 75 features experimentally by optimiz-
ing the test error, as shown in Figure 2. With large
numbers of features, the test error degrades because
we overﬁt the training samples.

Figure 2. Selecting number of features

IV. MODELS

We divide the dataset into stations by overall
popularity, and then cluster these divided data based
on GPS coordinates of each starting station. In
each cluster (denoted as Clusterrs in Figure 1),
we ﬁt a 2nd-degree polynomial model using linear
regression. When predicting, we ﬁnd the cluster
closest to the station whose demand we’re trying to
predict and use the ﬁtted polynomial for that cluster.
We use more data for the less popular stations (31-
50), while reducing the number of stations in each
cluster by increasing the number of clusters. This
is necessary because the less popular stations have
data which is more sparse, and thus we need more
data to train on.

A 2nd-order polynomial was chosen experimen-
tally to optimize the performance of the model. The
performance of the model with varying degrees is
shown in Figure 3.

The K-Means within each set of stations uses the
GPS coordinates to ﬁnd stations that are located
near to each other. We train on features g(i) given
by

(cid:20) latitude

(cid:21)

g(i) =

longitude

∈ R2,

and then perform K-Means by optimizing the

NumberofFeatures020406080100Score0.70.750.80.850.9FeatureSelectiontoOptimizePerformanceTesterrorTrainerrorThe performance metric that we used was ex-
plained variance, which computes the normalized
variance of the difference between the actual and
predicted bike demands in a given hour. In particu-
lar, explained variance is deﬁned as

Explained V ariance = 1 − V ar(y − ˆy)

.

V ar(y)

The performance results for each of our models

are summarized in Table I.
The learning curve shows, satisfyingly, that our
training and test scores converge to ≈ 0.83. This
suggests that we have sufﬁcient data; to improve our
learning algorithm, we required additional features
that convey unique information about the problem.
The polynomial regression (linear regression over
our polynomial feature set) performs very similarly
to ridge regression, which adds a penalty term for
outlying data. Because the penalty term was not
useful for our application, it was rarely signiﬁcant
and therefore ridge and simple polynomial regres-
sion performed almost identically. Because the extra
complexity of ridge came without beneﬁt, we chose
the simpler model.

VI. DISCUSSION

Our learning algorithm performs very well on
high trafﬁc stations. Performance degrades on less
popular stations as their demand is not as pre-
dictable because their usage is sporadic in nature.
That is, the number of bikes taken each hour from
less popular stations is much more difﬁcult to pre-
dict because only a few bikes are taken in an hour
and they do not follow regular patterns. Weekend
trafﬁc is also difﬁcult to predict at the high-trafﬁc
stations because most of the trafﬁc at those stations
occurs on weekdays (e.g. during commutes or lunch
breaks). Their weekday trafﬁc likely corresponds to

Figure 3. Performance vs. Model Complexity

cluster centers according to

m(cid:88)

i

arg min

µ,c

(cid:107)g(i) − µ(i)

c (cid:107)2 .

The use of K-Means was inspired by [3]. The
preliminary K-Means clustering algorithm used Eu-
clidean K-Means in the original feature space; how-
ever, we found that K-Means clustering on the geo-
graphic coordinates was more effective at predicting
the bike demand. The geographic-based K-Means
linked stations that were likely to have similar
trafﬁc patterns. Furthermore, segmenting stations
by total demand prior to K-Means, as shown in
Figure 1, helped separate the stations that performed
differently. Further work in [5] could also be useful
for clustering the stations according to their trafﬁc
proﬁles.

Each regressor cell performs regression on the

2nd-order polynomial feature set according to

k(cid:88)

hθ(x(cid:48)(cid:48)(i)) = θT x(cid:48)(cid:48)(i) =

θjx(cid:48)(cid:48)

j

(i) .

j=0

V. RESULTS

We tried several regression algorithms inside each
cluster, and have plotted the overall performance of
each of them in Figure 4. We found that a 2nd-order
polynomial regressor performed best among all the
algorithms we tried.

Table I

MODEL PERFORMANCE

1st-order polynomial regression
2nd-order polynomial regression
3rd-order polynomial regression
4th-order polynomial regression
SGD (L2 regularization)
Ridge, α = 0.5
Ridge, α = 2.0

0.801
0.858
0.851
0.823
0.637
0.856
0.865

0.786
0.832
0.791
0.775
0.620
0.831
0.829

PolynomialDegree1234Score0.750.80.850.9Performancevs.ModelComplexityTesterrorTrainerrorBike Share Usage Prediction in London

Ford Rylander
fordr@stanford.edu

Bo Peng

bpeng@stanford.edu

Jeff Wheeler

jeffsw@stanford.edu

Abstract—The rapid growth in recent years of bike shar-
ing around the world has led to real challenges balancing
supply and demand. To provide a possible solution for this
situation, bike sharing usage prediction is critical. In this
study, a model based on K-mean clustering and polynomial
regression is applied to predict the bike sharing usage in
a given time period and station. London bike sharing data
are used for training and testing our model.

I. INTRODUCTION

Bike sharing is a short-distance bike rental ser-
vice that allows customers to pick up a bike at
one station, use it for transportation as needed,
and return it to another station. This transportation
model delivers customers the conveniences of a bike
without the headaches of bike ownership including
theft, parking, storage and routine maintenance.

Demand for bike sharing services has exploded
times; as of June 2014, public bike-
in recent
sharing systems can be found in 712 cities on ﬁve
continents, with approximately 806,200 bicycles in
circulation at a total of 37,500 stations [1].

Demand prediction at bicycle sharing stations is
crucial to ensure that there are neither too many
bikes (and hence not enough bikes at some other
station that needs them) nor too few bikes (and
thus lost proﬁts). This paper presents the ﬁndings
of an algorithm that utilizes historical usage and
weather data to accurately predict the number of
bikes required at a given station in London.

An examination of prior arts reveals several pro-
posed methods to predict bike demand including an
autoregressive moving average (ARMA) algorithm
[2], linear regression, and support vector machine
(SVM) regression [3]. The ARMA model leverages
time-domain autocorrelation to highlight underlying
periodicity in the bike demand data, linear regres-
sion uses a computationally efﬁcient yet powerful
model to ﬁt the time-series data, and SVM regres-
sion is more computationally intensive. Taking the

pros and cons of these prior art approaches into
consideration, this paper presents the performance
of a 2nd-order polynomial model that is ﬁt after
clustering the training set using the geographical
coordinates of the start stations.

II. DATA

Historical bike share station demand and weather
data are publicly available for download as a list
of rides and weather data in a given day. The
bike rental raw data includes time and location
information for each rental. These raw data are then
processed with a binning algorithm to separate the
data in the required time-segment granularity and
then time-correlating this with the weather data. The
ride counts are then shifted by several time intervals
and joined back with the original data to generate
the historic features.

The list of all bike rides is available directly from

Transport for London [4].

III. FEATURES

We designed a pipeline structure that transforms
our basic feature set into a 2nd-order polynomial
feature set, and then reduces it to only the best
features.

X — Original Feature Set

We have aggregated these data into a single
design matrix, where each row represents an hour-
long segment of time for any given station. Each
row in X is given by

x(i) = [d1 d2 d3 d7 weather mean temp . . .

weekday is weekend . . .
h1 . . . h5]T ∈ R14.

The values in this vector are given by

1-15

16-30

31-50

Cluster11 → LR Cluster12 → LR Cluster13 → LR Cluster14 → LR
Cluster21 → LR Cluster22 → LR Cluster23 → LR Cluster24 → LR
Cluster31 → LR Cluster32 → LR Cluster33 → LR Cluster34 → LR Cluster35 → LR Cluster36 → LR

Figure 1. Geographic-Based K-Means on Stations Grouped by Overall Popularity

d1, d2, d3, d7

dn is the number of rides during the same
hour n days prior.

weather

is a binary representation of the weather
events from the day of x(i).

mean temp

is the mean temperature on the day of x(i).

weekday

is an integer representation of the day of
the week, with weekday ∈ [0, 6].

is a single bit that encodes whether x(i) is
on a weekend or not.

hn is the demand n hours prior to the hour
of x(i).

is weekend

h1, . . . ,h5

⇓

X(cid:48) — Polynomial Feature Set

We create X(cid:48) by applying a 2nd-order polynomial

transformation to X. Each row is given by

x(cid:48)(i) = [1 x(i)
x(i)
1 x(i)

1

1

x(i)
2
x(i)
1 x(i)

2

. . . x(i)

14 . . .
. . . x(i)

14 ]T ∈ R120.

14 x(i)

⇓

X(cid:48)(cid:48) — Reduced to Best Features

Finally, we perform feature selection to limit the
variance problems and reduce regression complexity
by selecting the best 75 features:

x(cid:48)(cid:48)(i) = f eature selection(x(cid:48)(i)) ∈ R75.

We chose 75 features experimentally by optimiz-
ing the test error, as shown in Figure 2. With large
numbers of features, the test error degrades because
we overﬁt the training samples.

Figure 2. Selecting number of features

IV. MODELS

We divide the dataset into stations by overall
popularity, and then cluster these divided data based
on GPS coordinates of each starting station. In
each cluster (denoted as Clusterrs in Figure 1),
we ﬁt a 2nd-degree polynomial model using linear
regression. When predicting, we ﬁnd the cluster
closest to the station whose demand we’re trying to
predict and use the ﬁtted polynomial for that cluster.
We use more data for the less popular stations (31-
50), while reducing the number of stations in each
cluster by increasing the number of clusters. This
is necessary because the less popular stations have
data which is more sparse, and thus we need more
data to train on.

A 2nd-order polynomial was chosen experimen-
tally to optimize the performance of the model. The
performance of the model with varying degrees is
shown in Figure 3.

The K-Means within each set of stations uses the
GPS coordinates to ﬁnd stations that are located
near to each other. We train on features g(i) given
by

(cid:20) latitude

(cid:21)

g(i) =

longitude

∈ R2,

and then perform K-Means by optimizing the

NumberofFeatures020406080100Score0.70.750.80.850.9FeatureSelectiontoOptimizePerformanceTesterrorTrainerrorThe performance metric that we used was ex-
plained variance, which computes the normalized
variance of the difference between the actual and
predicted bike demands in a given hour. In particu-
lar, explained variance is deﬁned as

Explained V ariance = 1 − V ar(y − ˆy)

.

V ar(y)

The performance results for each of our models

are summarized in Table I.
The learning curve shows, satisfyingly, that our
training and test scores converge to ≈ 0.83. This
suggests that we have sufﬁcient data; to improve our
learning algorithm, we required additional features
that convey unique information about the problem.
The polynomial regression (linear regression over
our polynomial feature set) performs very similarly
to ridge regression, which adds a penalty term for
outlying data. Because the penalty term was not
useful for our application, it was rarely signiﬁcant
and therefore ridge and simple polynomial regres-
sion performed almost identically. Because the extra
complexity of ridge came without beneﬁt, we chose
the simpler model.

VI. DISCUSSION

Our learning algorithm performs very well on
high trafﬁc stations. Performance degrades on less
popular stations as their demand is not as pre-
dictable because their usage is sporadic in nature.
That is, the number of bikes taken each hour from
less popular stations is much more difﬁcult to pre-
dict because only a few bikes are taken in an hour
and they do not follow regular patterns. Weekend
trafﬁc is also difﬁcult to predict at the high-trafﬁc
stations because most of the trafﬁc at those stations
occurs on weekdays (e.g. during commutes or lunch
breaks). Their weekday trafﬁc likely corresponds to

Figure 3. Performance vs. Model Complexity

cluster centers according to

m(cid:88)

i

arg min

µ,c

(cid:107)g(i) − µ(i)

c (cid:107)2 .

The use of K-Means was inspired by [3]. The
preliminary K-Means clustering algorithm used Eu-
clidean K-Means in the original feature space; how-
ever, we found that K-Means clustering on the geo-
graphic coordinates was more effective at predicting
the bike demand. The geographic-based K-Means
linked stations that were likely to have similar
trafﬁc patterns. Furthermore, segmenting stations
by total demand prior to K-Means, as shown in
Figure 1, helped separate the stations that performed
differently. Further work in [5] could also be useful
for clustering the stations according to their trafﬁc
proﬁles.

Each regressor cell performs regression on the

2nd-order polynomial feature set according to

k(cid:88)

hθ(x(cid:48)(cid:48)(i)) = θT x(cid:48)(cid:48)(i) =

θjx(cid:48)(cid:48)

j

(i) .

j=0

V. RESULTS

We tried several regression algorithms inside each
cluster, and have plotted the overall performance of
each of them in Figure 4. We found that a 2nd-order
polynomial regressor performed best among all the
algorithms we tried.

Table I

MODEL PERFORMANCE

1st-order polynomial regression
2nd-order polynomial regression
3rd-order polynomial regression
4th-order polynomial regression
SGD (L2 regularization)
Ridge, α = 0.5
Ridge, α = 2.0

0.801
0.858
0.851
0.823
0.637
0.856
0.865

0.786
0.832
0.791
0.775
0.620
0.831
0.829

PolynomialDegree1234Score0.750.80.850.9Performancevs.ModelComplexityTesterrorTrainerrorFigure 4. Learning Curve

local events happening nearby, which we did not
attempt to capture in our algorithm.

We expected to be able to predict the total usage
for all stations, but were surprised by our ability
to predict per-station trafﬁc quite well, to which
we credit our usage of K-Means clustering on the
geographic station coordinates.

Figure 5 shows our predicted bike demand vs.
actual bike demand for a particular week in January
2013. This particular station is relatively high-trafﬁc
and our algorithm does a very good job of predicting
demand. Many other high-trafﬁc stations share a
similar proﬁle; some others have two peaks each
weekday corresponding to morning and evening
commutes or morning and lunch commutes.

VII. FUTURE

The fundamental limit of our algorithm was de-
termined by the features that were available. Given
more time, we would like to incorporate other
features such as subway ridership which we believe
would be an accurate predictor of bike demand. We
would also like to implement geographic visualiza-
tions of the data to better understand how geography
affects our learning algorithm’s performance.

REFERENCES

[1] S. A. S. et al., “Public bikesharing in north america during
a period of rapid expansion: Understanding business models,

Figure 5. Bike Sharing Prediction vs. Actual Value

industry trends and user impacts,” in Mineta Transportation
Institure (MTI), 2015, p. pp.5.

[2] A. Kaltenbrunner, R. Meza, J. Grivolla, J. Codina, and R. Banchs,
“Urban cycles and mobility patterns: Exploring and predicting
trends in a bicycle-based public transport system,” Pervasive and
Mobile Computing, vol. 6, no. 4, pp. 455–466, 2010.

[3] H. Xu, J. Ying, H. Wu, and F. Lin, “Public bicycle trafﬁc ﬂow
prediction based on a hybrid model,” Appl. Math. Inf. Sci, vol. 7,
pp. 667–674, 2013.

[4] Transport for London. City of London. [Online]. Available:

https://www.tﬂ.gov.uk/

[5] H. Wu, X. Fang, and H. Xu, “Station segmentation of hangzhou
public free-bicycle system based on improved randomized algo-
rithm,” in Machine Learning and Cybernetics (ICMLC), 2011
International Conference on, vol. 4.
IEEE, 2011, pp. 1588–
1592.

Fractionofalldata0.10.20.30.40.50.60.70.80.91Score00.10.20.30.40.50.60.70.80.91TestingScore(ridge,,=0:5)TrainingScore(ridge,,=0:5)Testing(SGD)Training(SGD)Testing(linreg)Training(linreg)Time050100150BikesDemanded050100BikeSharingPredictionvs.ActualValuePredictedActual