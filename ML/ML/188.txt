Kernel Learning Framework for Cancer Subtype Analysis

with Multi-omics Data Integration

William Bradbury

Thomas Lau

Shivaal Roy

wbradbur@stanford

thomklau@stanford

shivaal@stanford

December 12, 2015

Abstract

Recent advances in Multiple Kernel Learning
(MKL) and unsupervised clustering methods have
each enabled large-scale analysis of integrated multi-
omics data for cancer subtyping. However, efforts
to combine the advantages of a kernel approach with
the ﬂexibility of unsupervised methods have not pro-
gressed due to the underconstrained nature of the
problem. In this paper, we present a novel approach to
solving this problem based on methods for Unsuper-
vised Multiple Kernel Learning (UMKL). We present
the possibility of constraining the problem with avail-
able data on clinically determined subtypes for cancer
patients. We successfully demonstrate that modifying
the constraints of the UMKL problem to include con-
ditions for sparse clinical labelling data is tractable
and thus presents a robust alternative for cancer sub-
type analysis by reducing it to an efﬁcient alternating
quadratic optimization problem.

1. Introduction

The expansion and increased availability of large-
scale biomedical data in the past decade has led to the
rising use of Machine Learning as an integral compo-
nent for providing biological insights - especially in
the domain of cancer research. Recently, with the in-
creasing number of large scale genomic projects, such
as The Cancer Genome Atlas (TCGA), the amount of
openly accessible patient data is greater than ever. In
order to make sense of this ever increasing quantity of
data, numerous statistical approaches have been devel-
oped to analyze TCGA and other similar datasets.

Cancer subtyping is of particular clinical relevance
because it has the potential to enable medical practi-
tioners to design more ﬁnely tuned cancer therapies.
Intuitively, the more precisely we are able to clas-
sify types of cancer, the more effective the treatments
we design can be. Futhermore, cancer subtyping has
the potential to more accurately predict treatment out-
comes and survival rates based on rates of cancer pro-
gression and other clinical factors. Currently, doctors
can identify cancer subtypes in patients by measuring
biological features during the progression of their can-
cer. However, these subtype classiﬁcations are slow
to perform, often very broad (Luminal A vs Luminal
B for BRCA), and have weak prognostic ability, since
subtyping by physicians can only be reﬁned as the can-
cer progresses. Recent advances in statistical analyses
suggest there is much potential to improve cancer sub-
typing.1,5,4

Previous approaches1,5,7 suggest the potential for a
UMKL solution which relies on sparse clinical data
to analyze multi-omic data in the search of more spe-
ciﬁc and useful subtype characterizations - thereby
empowering doctors to determine optimal methods of
treatment. We propose a method for such an ap-
proach, combining the techniques of previous UMKL
approaches with novel constraints based on clinical
data.

TCGA contains information about methylation,
copynumber variation (CNV), microRNA, mRNA,
RPPA, and other factors relevant to the onset and pro-
gression of cancer. The interaction between these lev-
els of genetic control is referred to as multi-omics.
Intuitively, by looking at the genomic causes of can-
cer, we can more accurately predict future outcomes -
as opposed to clinically looking at phenotypic cancer

Kernel Learning Framework for Cancer Subtype Analysis

with Multi-omics Data Integration

William Bradbury

Thomas Lau

Shivaal Roy

wbradbur@stanford

thomklau@stanford

shivaal@stanford

December 12, 2015

Abstract

Recent advances in Multiple Kernel Learning
(MKL) and unsupervised clustering methods have
each enabled large-scale analysis of integrated multi-
omics data for cancer subtyping. However, efforts
to combine the advantages of a kernel approach with
the ﬂexibility of unsupervised methods have not pro-
gressed due to the underconstrained nature of the
problem. In this paper, we present a novel approach to
solving this problem based on methods for Unsuper-
vised Multiple Kernel Learning (UMKL). We present
the possibility of constraining the problem with avail-
able data on clinically determined subtypes for cancer
patients. We successfully demonstrate that modifying
the constraints of the UMKL problem to include con-
ditions for sparse clinical labelling data is tractable
and thus presents a robust alternative for cancer sub-
type analysis by reducing it to an efﬁcient alternating
quadratic optimization problem.

1. Introduction

The expansion and increased availability of large-
scale biomedical data in the past decade has led to the
rising use of Machine Learning as an integral compo-
nent for providing biological insights - especially in
the domain of cancer research. Recently, with the in-
creasing number of large scale genomic projects, such
as The Cancer Genome Atlas (TCGA), the amount of
openly accessible patient data is greater than ever. In
order to make sense of this ever increasing quantity of
data, numerous statistical approaches have been devel-
oped to analyze TCGA and other similar datasets.

Cancer subtyping is of particular clinical relevance
because it has the potential to enable medical practi-
tioners to design more ﬁnely tuned cancer therapies.
Intuitively, the more precisely we are able to clas-
sify types of cancer, the more effective the treatments
we design can be. Futhermore, cancer subtyping has
the potential to more accurately predict treatment out-
comes and survival rates based on rates of cancer pro-
gression and other clinical factors. Currently, doctors
can identify cancer subtypes in patients by measuring
biological features during the progression of their can-
cer. However, these subtype classiﬁcations are slow
to perform, often very broad (Luminal A vs Luminal
B for BRCA), and have weak prognostic ability, since
subtyping by physicians can only be reﬁned as the can-
cer progresses. Recent advances in statistical analyses
suggest there is much potential to improve cancer sub-
typing.1,5,4

Previous approaches1,5,7 suggest the potential for a
UMKL solution which relies on sparse clinical data
to analyze multi-omic data in the search of more spe-
ciﬁc and useful subtype characterizations - thereby
empowering doctors to determine optimal methods of
treatment. We propose a method for such an ap-
proach, combining the techniques of previous UMKL
approaches with novel constraints based on clinical
data.

TCGA contains information about methylation,
copynumber variation (CNV), microRNA, mRNA,
RPPA, and other factors relevant to the onset and pro-
gression of cancer. The interaction between these lev-
els of genetic control is referred to as multi-omics.
Intuitively, by looking at the genomic causes of can-
cer, we can more accurately predict future outcomes -
as opposed to clinically looking at phenotypic cancer

progression.

Our goal in this paper is to extend the existing unsu-
pervised multiple kernel learning technique in Zhuang
et al. to leverage sparse labeling for speciﬁc use with
clinical and multi-omic data from TCGA. We wish to
demonstrate that such an extension is still a tractable
optimization problem.

2. Related Work

This paper builds off of Zhuang et al., which pro-
poses a method for UMKL that also integrates case-
speciﬁc constraint functions. This paper demonstrates
the feasibility of the original problem and provides
cases for which the algorithm performs especially
well. The algorithm presented in this paper, however,
does not perform well as the number of features in-
creases or as the number of sample points decreases.7
Supervised multiple kernel learning has been imple-
mented in Daemen et al. to classify rectal cancer mi-
croarray data. The resulting Support Vector Machine
accurately classiﬁed different outcomes, often reach-
ing above 0.90. The model also performed better when
using more than one genome-wide data set, suggesting
that integrating multiple genome-wide data sources al-
lows models to reach higher accuracy.1

The iCluster algorithm developed by Shen et al.
was used to accurately group cancer outcomes based
on multi-omic considerations found in TCGA data on
breast cancer. The algorithm clusters different can-
cer subtypes using multiple genomic features such
as DNA copy number changes and gene expression.
iCluster, however, does not take advantage of any ker-
nel methods.5

We build upon these papers to solve the problem
of harnessing the power of kernels for unsupervised
learning in a high dimensional feature space with a rel-
atively small number of samples.

3. Background

3.1. Multiple Kernel Learning

ily separated by an SVM. We write this as

m(cid:88)

Kconv =

µtkt

t=1

where µt is the weight assigned to each kernel.
In
MKL, we try to learn the kernel combination that lin-
early separates the data best.

3.2. Unsupervised Multiple Kernel Learning

Unsupervised Multiple Kernel Learning also imple-
ments a linear combination of kernels to create a dis-
tance metric in a higher-dimensional space. The dis-
tance metric is used to determine groupings using a
k-means or alternate clustering algorithm.7

3.3. Sparse Labels

We take advantage of various labels, although
sparse, to constrain our data and aid in the clustering
process. We incorporate the clinical data mentioned
earlier into our cost function to have the resultant com-
bination of kernels reﬂect this added restriction. We
also impose domain knowledge of the separation be-
tween cancer and non-cancer patients. This data can
be included in our model as further priors on the sub-
types.

4. Optimization Problem
4.1. Cluster-label alignment metric

We design an optimization problem to produce a
kernel and clustering which together yield the best as-
signment of patients to subtypes. In order to do this,
we employ the cost functions found in Zhuang et al.
as well as two new cost functions:

(cid:80)

• A good kernel should induce kernel values which
place samples of the same subtype together in
the feature mapping.
In other words for each
xi we expect that the optimal kernel minimizes
kii − 2kij + kjj, where Ci is the set
of all samples with the same known label as xi.
kij = k(xi, xj). Ci = ∅ for xi that do not have
known labels.

xj∈Li

Multiple Kernel Learning is the use of a linear
combination of kernels to map points to a higher-
dimensional feature space where they can be more eas-

• A good kernel should induce kernel values which
place samples of different subtypes apart in the
feature mapping. When this is the case, we expect

2

Kernel Learning Framework for Cancer Subtype Analysis

with Multi-omics Data Integration

William Bradbury

Thomas Lau

Shivaal Roy

wbradbur@stanford

thomklau@stanford

shivaal@stanford

December 12, 2015

Abstract

Recent advances in Multiple Kernel Learning
(MKL) and unsupervised clustering methods have
each enabled large-scale analysis of integrated multi-
omics data for cancer subtyping. However, efforts
to combine the advantages of a kernel approach with
the ﬂexibility of unsupervised methods have not pro-
gressed due to the underconstrained nature of the
problem. In this paper, we present a novel approach to
solving this problem based on methods for Unsuper-
vised Multiple Kernel Learning (UMKL). We present
the possibility of constraining the problem with avail-
able data on clinically determined subtypes for cancer
patients. We successfully demonstrate that modifying
the constraints of the UMKL problem to include con-
ditions for sparse clinical labelling data is tractable
and thus presents a robust alternative for cancer sub-
type analysis by reducing it to an efﬁcient alternating
quadratic optimization problem.

1. Introduction

The expansion and increased availability of large-
scale biomedical data in the past decade has led to the
rising use of Machine Learning as an integral compo-
nent for providing biological insights - especially in
the domain of cancer research. Recently, with the in-
creasing number of large scale genomic projects, such
as The Cancer Genome Atlas (TCGA), the amount of
openly accessible patient data is greater than ever. In
order to make sense of this ever increasing quantity of
data, numerous statistical approaches have been devel-
oped to analyze TCGA and other similar datasets.

Cancer subtyping is of particular clinical relevance
because it has the potential to enable medical practi-
tioners to design more ﬁnely tuned cancer therapies.
Intuitively, the more precisely we are able to clas-
sify types of cancer, the more effective the treatments
we design can be. Futhermore, cancer subtyping has
the potential to more accurately predict treatment out-
comes and survival rates based on rates of cancer pro-
gression and other clinical factors. Currently, doctors
can identify cancer subtypes in patients by measuring
biological features during the progression of their can-
cer. However, these subtype classiﬁcations are slow
to perform, often very broad (Luminal A vs Luminal
B for BRCA), and have weak prognostic ability, since
subtyping by physicians can only be reﬁned as the can-
cer progresses. Recent advances in statistical analyses
suggest there is much potential to improve cancer sub-
typing.1,5,4

Previous approaches1,5,7 suggest the potential for a
UMKL solution which relies on sparse clinical data
to analyze multi-omic data in the search of more spe-
ciﬁc and useful subtype characterizations - thereby
empowering doctors to determine optimal methods of
treatment. We propose a method for such an ap-
proach, combining the techniques of previous UMKL
approaches with novel constraints based on clinical
data.

TCGA contains information about methylation,
copynumber variation (CNV), microRNA, mRNA,
RPPA, and other factors relevant to the onset and pro-
gression of cancer. The interaction between these lev-
els of genetic control is referred to as multi-omics.
Intuitively, by looking at the genomic causes of can-
cer, we can more accurately predict future outcomes -
as opposed to clinically looking at phenotypic cancer

progression.

Our goal in this paper is to extend the existing unsu-
pervised multiple kernel learning technique in Zhuang
et al. to leverage sparse labeling for speciﬁc use with
clinical and multi-omic data from TCGA. We wish to
demonstrate that such an extension is still a tractable
optimization problem.

2. Related Work

This paper builds off of Zhuang et al., which pro-
poses a method for UMKL that also integrates case-
speciﬁc constraint functions. This paper demonstrates
the feasibility of the original problem and provides
cases for which the algorithm performs especially
well. The algorithm presented in this paper, however,
does not perform well as the number of features in-
creases or as the number of sample points decreases.7
Supervised multiple kernel learning has been imple-
mented in Daemen et al. to classify rectal cancer mi-
croarray data. The resulting Support Vector Machine
accurately classiﬁed different outcomes, often reach-
ing above 0.90. The model also performed better when
using more than one genome-wide data set, suggesting
that integrating multiple genome-wide data sources al-
lows models to reach higher accuracy.1

The iCluster algorithm developed by Shen et al.
was used to accurately group cancer outcomes based
on multi-omic considerations found in TCGA data on
breast cancer. The algorithm clusters different can-
cer subtypes using multiple genomic features such
as DNA copy number changes and gene expression.
iCluster, however, does not take advantage of any ker-
nel methods.5

We build upon these papers to solve the problem
of harnessing the power of kernels for unsupervised
learning in a high dimensional feature space with a rel-
atively small number of samples.

3. Background

3.1. Multiple Kernel Learning

ily separated by an SVM. We write this as

m(cid:88)

Kconv =

µtkt

t=1

where µt is the weight assigned to each kernel.
In
MKL, we try to learn the kernel combination that lin-
early separates the data best.

3.2. Unsupervised Multiple Kernel Learning

Unsupervised Multiple Kernel Learning also imple-
ments a linear combination of kernels to create a dis-
tance metric in a higher-dimensional space. The dis-
tance metric is used to determine groupings using a
k-means or alternate clustering algorithm.7

3.3. Sparse Labels

We take advantage of various labels, although
sparse, to constrain our data and aid in the clustering
process. We incorporate the clinical data mentioned
earlier into our cost function to have the resultant com-
bination of kernels reﬂect this added restriction. We
also impose domain knowledge of the separation be-
tween cancer and non-cancer patients. This data can
be included in our model as further priors on the sub-
types.

4. Optimization Problem
4.1. Cluster-label alignment metric

We design an optimization problem to produce a
kernel and clustering which together yield the best as-
signment of patients to subtypes. In order to do this,
we employ the cost functions found in Zhuang et al.
as well as two new cost functions:

(cid:80)

• A good kernel should induce kernel values which
place samples of the same subtype together in
the feature mapping.
In other words for each
xi we expect that the optimal kernel minimizes
kii − 2kij + kjj, where Ci is the set
of all samples with the same known label as xi.
kij = k(xi, xj). Ci = ∅ for xi that do not have
known labels.

xj∈Li

Multiple Kernel Learning is the use of a linear
combination of kernels to map points to a higher-
dimensional feature space where they can be more eas-

• A good kernel should induce kernel values which
place samples of different subtypes apart in the
feature mapping. When this is the case, we expect

2

the kernel to maximize(cid:80)

kii−2kij +kjj.
Where here Ci(cid:54)=j is the set of sample points la-
beled with a different subtype than xi.

xj∈Li(cid:54)=j

Each of these cost functions is designed to affect the
optimization problem in such a way as to yield a ker-
nel which places co-labeled points together and differ-
ently labeled points apart. These heuristics guide the
search for an optimal kernel based on the prior sparse
clinical information by encouraging the eventual adop-
tion of a kernel which conforms to the clinical data as
well as possible. Such a kernel will yield a clustering
which places co-labled points in the same or nearby
clusters, while placing differently labeled points in dif-
ferent clusters if possible.

4.3. A simpler formulation

Instead of optimizing the above function with re-
spect to κ ∈ K and Bi we instead formulate the prob-
lem as one of optimizing the cost function with respect
to µ and D. This allows us to eventually present the
problem as one of solving a quadratic program on the
vector µ and later D.
As in Zhuang et al. we deﬁne matrices D, S ∈
{0, 1}n×n where each element is given as

[D]ij = 1{xj ∈ Bi}
[S]ij = 1{xj ∈ Li}
[Q]ij = 1{xj ∈ Li(cid:54)=j}
[M]ij = xT

i xi + xT

j xj − 2xT

i xj

4.2. Cost function

so that we can write:

In order to obtain an overall cost function for the
optimization problem, we combine these cluster-label
alignment metrics with those found in Zhuang et al. to
form the overall optimization problem:

1
2

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)xi − (cid:88)
n(cid:88)
(cid:88)
n(cid:88)
(cid:88)
n(cid:88)
n(cid:88)

(cid:88)
(cid:88)

xj∈Bi
|Bi|

xj∈Li

i=1

i=1

i

i=1

xj∈Li(cid:54)=j

min

B,k∈Kconv

+ γ1

+ γ2

+ γ3

− γ4

where

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

kijxj

xj∈Bj

kij (cid:107)xi − xj(cid:107)2

(kii − 2kij + kjj)

(kii − 2kij + kjj)

min
µ,D

F

(cid:107)X(I − K ◦ D)(cid:107)2
1
2
+ γ1tr K ◦ D ◦ M(11T ) + γ2 (cid:107)D(cid:107)1,1

+ tr(cid:0)(cid:2)(K ◦ I)(11T ) − 2K + (11T )(K ◦ I)T(cid:3)
m(cid:88)

◦(γ3S + γ4Q)) (11T )

s.t [κ]ij =

µtkt(xi, xj),

t=1

1 ≤ i, j ≤ n,
µT 1 = 1,
µ ≥ 0,
D ∈ {0, 1}n×n

The function given is not quadratic and also not guar-
anteed to be convex. Because of this, there are no
off-the-shelf tools which are able to generally solve
this problem. This difﬁculty motivates our work to re-
formulate the problem as an alternating optimization
problem in the next section.

4.4. Alternating Optimization Algorithm

In order to formulate the problem in such a way
that it can be approximately solved, we break the
problem into two components.7 Consider the space
parametrized by µ and D. We can reduce the above
problem to one of coordinate descent by alternatively
optimizing µ and D while holding the other constant.
If we can show that each of these individual problems

B is a clustering assignment,
µtkt(·,·) :

Kconv = {k(·,·) =

m(cid:88)

m(cid:88)

µt = 1, µt ≥ 0},

t=1

t=1
γi control constraint trade-offs, and
L is given in §4.1.

This cost function is not–in its present form–feasible
to optimize given the size of the data.

3

Kernel Learning Framework for Cancer Subtype Analysis

with Multi-omics Data Integration

William Bradbury

Thomas Lau

Shivaal Roy

wbradbur@stanford

thomklau@stanford

shivaal@stanford

December 12, 2015

Abstract

Recent advances in Multiple Kernel Learning
(MKL) and unsupervised clustering methods have
each enabled large-scale analysis of integrated multi-
omics data for cancer subtyping. However, efforts
to combine the advantages of a kernel approach with
the ﬂexibility of unsupervised methods have not pro-
gressed due to the underconstrained nature of the
problem. In this paper, we present a novel approach to
solving this problem based on methods for Unsuper-
vised Multiple Kernel Learning (UMKL). We present
the possibility of constraining the problem with avail-
able data on clinically determined subtypes for cancer
patients. We successfully demonstrate that modifying
the constraints of the UMKL problem to include con-
ditions for sparse clinical labelling data is tractable
and thus presents a robust alternative for cancer sub-
type analysis by reducing it to an efﬁcient alternating
quadratic optimization problem.

1. Introduction

The expansion and increased availability of large-
scale biomedical data in the past decade has led to the
rising use of Machine Learning as an integral compo-
nent for providing biological insights - especially in
the domain of cancer research. Recently, with the in-
creasing number of large scale genomic projects, such
as The Cancer Genome Atlas (TCGA), the amount of
openly accessible patient data is greater than ever. In
order to make sense of this ever increasing quantity of
data, numerous statistical approaches have been devel-
oped to analyze TCGA and other similar datasets.

Cancer subtyping is of particular clinical relevance
because it has the potential to enable medical practi-
tioners to design more ﬁnely tuned cancer therapies.
Intuitively, the more precisely we are able to clas-
sify types of cancer, the more effective the treatments
we design can be. Futhermore, cancer subtyping has
the potential to more accurately predict treatment out-
comes and survival rates based on rates of cancer pro-
gression and other clinical factors. Currently, doctors
can identify cancer subtypes in patients by measuring
biological features during the progression of their can-
cer. However, these subtype classiﬁcations are slow
to perform, often very broad (Luminal A vs Luminal
B for BRCA), and have weak prognostic ability, since
subtyping by physicians can only be reﬁned as the can-
cer progresses. Recent advances in statistical analyses
suggest there is much potential to improve cancer sub-
typing.1,5,4

Previous approaches1,5,7 suggest the potential for a
UMKL solution which relies on sparse clinical data
to analyze multi-omic data in the search of more spe-
ciﬁc and useful subtype characterizations - thereby
empowering doctors to determine optimal methods of
treatment. We propose a method for such an ap-
proach, combining the techniques of previous UMKL
approaches with novel constraints based on clinical
data.

TCGA contains information about methylation,
copynumber variation (CNV), microRNA, mRNA,
RPPA, and other factors relevant to the onset and pro-
gression of cancer. The interaction between these lev-
els of genetic control is referred to as multi-omics.
Intuitively, by looking at the genomic causes of can-
cer, we can more accurately predict future outcomes -
as opposed to clinically looking at phenotypic cancer

progression.

Our goal in this paper is to extend the existing unsu-
pervised multiple kernel learning technique in Zhuang
et al. to leverage sparse labeling for speciﬁc use with
clinical and multi-omic data from TCGA. We wish to
demonstrate that such an extension is still a tractable
optimization problem.

2. Related Work

This paper builds off of Zhuang et al., which pro-
poses a method for UMKL that also integrates case-
speciﬁc constraint functions. This paper demonstrates
the feasibility of the original problem and provides
cases for which the algorithm performs especially
well. The algorithm presented in this paper, however,
does not perform well as the number of features in-
creases or as the number of sample points decreases.7
Supervised multiple kernel learning has been imple-
mented in Daemen et al. to classify rectal cancer mi-
croarray data. The resulting Support Vector Machine
accurately classiﬁed different outcomes, often reach-
ing above 0.90. The model also performed better when
using more than one genome-wide data set, suggesting
that integrating multiple genome-wide data sources al-
lows models to reach higher accuracy.1

The iCluster algorithm developed by Shen et al.
was used to accurately group cancer outcomes based
on multi-omic considerations found in TCGA data on
breast cancer. The algorithm clusters different can-
cer subtypes using multiple genomic features such
as DNA copy number changes and gene expression.
iCluster, however, does not take advantage of any ker-
nel methods.5

We build upon these papers to solve the problem
of harnessing the power of kernels for unsupervised
learning in a high dimensional feature space with a rel-
atively small number of samples.

3. Background

3.1. Multiple Kernel Learning

ily separated by an SVM. We write this as

m(cid:88)

Kconv =

µtkt

t=1

where µt is the weight assigned to each kernel.
In
MKL, we try to learn the kernel combination that lin-
early separates the data best.

3.2. Unsupervised Multiple Kernel Learning

Unsupervised Multiple Kernel Learning also imple-
ments a linear combination of kernels to create a dis-
tance metric in a higher-dimensional space. The dis-
tance metric is used to determine groupings using a
k-means or alternate clustering algorithm.7

3.3. Sparse Labels

We take advantage of various labels, although
sparse, to constrain our data and aid in the clustering
process. We incorporate the clinical data mentioned
earlier into our cost function to have the resultant com-
bination of kernels reﬂect this added restriction. We
also impose domain knowledge of the separation be-
tween cancer and non-cancer patients. This data can
be included in our model as further priors on the sub-
types.

4. Optimization Problem
4.1. Cluster-label alignment metric

We design an optimization problem to produce a
kernel and clustering which together yield the best as-
signment of patients to subtypes. In order to do this,
we employ the cost functions found in Zhuang et al.
as well as two new cost functions:

(cid:80)

• A good kernel should induce kernel values which
place samples of the same subtype together in
the feature mapping.
In other words for each
xi we expect that the optimal kernel minimizes
kii − 2kij + kjj, where Ci is the set
of all samples with the same known label as xi.
kij = k(xi, xj). Ci = ∅ for xi that do not have
known labels.

xj∈Li

Multiple Kernel Learning is the use of a linear
combination of kernels to map points to a higher-
dimensional feature space where they can be more eas-

• A good kernel should induce kernel values which
place samples of different subtypes apart in the
feature mapping. When this is the case, we expect

2

the kernel to maximize(cid:80)

kii−2kij +kjj.
Where here Ci(cid:54)=j is the set of sample points la-
beled with a different subtype than xi.

xj∈Li(cid:54)=j

Each of these cost functions is designed to affect the
optimization problem in such a way as to yield a ker-
nel which places co-labeled points together and differ-
ently labeled points apart. These heuristics guide the
search for an optimal kernel based on the prior sparse
clinical information by encouraging the eventual adop-
tion of a kernel which conforms to the clinical data as
well as possible. Such a kernel will yield a clustering
which places co-labled points in the same or nearby
clusters, while placing differently labeled points in dif-
ferent clusters if possible.

4.3. A simpler formulation

Instead of optimizing the above function with re-
spect to κ ∈ K and Bi we instead formulate the prob-
lem as one of optimizing the cost function with respect
to µ and D. This allows us to eventually present the
problem as one of solving a quadratic program on the
vector µ and later D.
As in Zhuang et al. we deﬁne matrices D, S ∈
{0, 1}n×n where each element is given as

[D]ij = 1{xj ∈ Bi}
[S]ij = 1{xj ∈ Li}
[Q]ij = 1{xj ∈ Li(cid:54)=j}
[M]ij = xT

i xi + xT

j xj − 2xT

i xj

4.2. Cost function

so that we can write:

In order to obtain an overall cost function for the
optimization problem, we combine these cluster-label
alignment metrics with those found in Zhuang et al. to
form the overall optimization problem:

1
2

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)xi − (cid:88)
n(cid:88)
(cid:88)
n(cid:88)
(cid:88)
n(cid:88)
n(cid:88)

(cid:88)
(cid:88)

xj∈Bi
|Bi|

xj∈Li

i=1

i=1

i

i=1

xj∈Li(cid:54)=j

min

B,k∈Kconv

+ γ1

+ γ2

+ γ3

− γ4

where

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

kijxj

xj∈Bj

kij (cid:107)xi − xj(cid:107)2

(kii − 2kij + kjj)

(kii − 2kij + kjj)

min
µ,D

F

(cid:107)X(I − K ◦ D)(cid:107)2
1
2
+ γ1tr K ◦ D ◦ M(11T ) + γ2 (cid:107)D(cid:107)1,1

+ tr(cid:0)(cid:2)(K ◦ I)(11T ) − 2K + (11T )(K ◦ I)T(cid:3)
m(cid:88)

◦(γ3S + γ4Q)) (11T )

s.t [κ]ij =

µtkt(xi, xj),

t=1

1 ≤ i, j ≤ n,
µT 1 = 1,
µ ≥ 0,
D ∈ {0, 1}n×n

The function given is not quadratic and also not guar-
anteed to be convex. Because of this, there are no
off-the-shelf tools which are able to generally solve
this problem. This difﬁculty motivates our work to re-
formulate the problem as an alternating optimization
problem in the next section.

4.4. Alternating Optimization Algorithm

In order to formulate the problem in such a way
that it can be approximately solved, we break the
problem into two components.7 Consider the space
parametrized by µ and D. We can reduce the above
problem to one of coordinate descent by alternatively
optimizing µ and D while holding the other constant.
If we can show that each of these individual problems

B is a clustering assignment,
µtkt(·,·) :

Kconv = {k(·,·) =

m(cid:88)

m(cid:88)

µt = 1, µt ≥ 0},

t=1

t=1
γi control constraint trade-offs, and
L is given in §4.1.

This cost function is not–in its present form–feasible
to optimize given the size of the data.

3

is itself quadratic and therefore tractable, we will have
broken the problem down into a tractable approxima-
tion.

4.4.1 Solving µ by ﬁxing D

We accomplish this separation by ﬁrst optimizing µ
while holding D constant and ignoring the otherwise
difﬁcult constraints on D. Fixing D we try to solve for
µ by minimizing the cost function:

(cid:33)T

κt,iκT

t,i ◦ didT

i ◦ P

µ + zT µ,

J(µ) = µT

where

[z]t =

(cid:32) m(cid:88)
n(cid:88)

t=1

i=1

+ γ3(ei

− γ4(ei

n(cid:88)

i=1

(cid:88)
(cid:88)

j

(2γ1vi ◦ di − 2pi ◦ di

(cid:88)
(cid:88)

j

Sij + ei

Qij + ei

Sji − 2si)

Qji − 2qi))T κt,i

j

j

P = XT X
κt,i = [kt(xi, x1), ..., kt(xi, xn)]T
pi is the i-th column of P
vi is the i-th column of M
si is the i-th column of S
qi is the i-th column of Q

Most importantly, this cost function is a Quadratic Pro-
gram and so is tractable, even for large X.

4.4.2 Solving D by ﬁxing µ

Because we introduced new constraints on the kernel,
but not the cluster matrix D, the optimization step for
D is unchanged from its presentation in Zhuang et al.
and so we simply give the result for each column of D:

J(d) = dT(cid:0)κκT ◦ P(cid:1) d + (2γ1κ ◦ v − 2κ ◦ p)T d,

which is of the form

J(d) = dT Wd + cT d

and so is also tractable with a QP convex optimization
by iterating over each sample point and using this min-
imization problem to ﬁnd the optimal set of neighbors.

4

5. Discussion

In deriving a quadratic problem from our initial
complex constrained optimization problem, we have
shown that our new approach for incorporating par-
tially labeled data is a solvable problem. Here, we ex-
plain the underpinnings of the initial constraints and
derivation of its quadratic form.

5.1. Underlying motivations

The original UMKL algorithm presents two con-
straints on the behavior of the kernels. The ﬁrst was
that of a standard clustering optimization problem:
minimize the sum of the distances between samples in
the same cluster. In this case, however, the distances
were computed by the kernel function being chosen.
Thus, a good choice of kernel function would place
points which end up in the same cluster together in the
feature mapping. This is useful for learning an optimal
kernel on a large quantity of ’training’ data, only to ap-
ply it using something other than a clustering method.
Zhuang et al. demonstrates this approach by learning
an optimal kernel for a data set, and then applying that
kernel to do classiﬁcation with an SVM.

The second constraint on the behavior of the kernels
was that of continuity with the given sample points.
This constraint enforces, as far as possible, that the
kernel should have a limited impact on the geometry of
the data. Points should not be mapped so that they are
distant from their neighbors in the original space. This
constraint ensures that the kernel does not map points
in an ad hoc manner such that essentially meaningless,
but perfect, clusters form in the feature mapping space.

5.2. Our contributions

Building on this last constraint, this paper intro-
duced two new constraints. The ﬁrst speciﬁed that
points which are given the same labels in the cluster
priors are placed as close to possible in the feature
mapping. Combined with the previous constraint, this
ensures the learned kernel will highlight the underly-
ing geometry in such a way as points with the same la-
bel are close together, but it will not mangle the space
unnecessarily. This forces the kernel to ’learn’ what-
ever is important to identifying each of the prior clus-
ters.

In a similar vein, the second new constraint requires

Kernel Learning Framework for Cancer Subtype Analysis

with Multi-omics Data Integration

William Bradbury

Thomas Lau

Shivaal Roy

wbradbur@stanford

thomklau@stanford

shivaal@stanford

December 12, 2015

Abstract

Recent advances in Multiple Kernel Learning
(MKL) and unsupervised clustering methods have
each enabled large-scale analysis of integrated multi-
omics data for cancer subtyping. However, efforts
to combine the advantages of a kernel approach with
the ﬂexibility of unsupervised methods have not pro-
gressed due to the underconstrained nature of the
problem. In this paper, we present a novel approach to
solving this problem based on methods for Unsuper-
vised Multiple Kernel Learning (UMKL). We present
the possibility of constraining the problem with avail-
able data on clinically determined subtypes for cancer
patients. We successfully demonstrate that modifying
the constraints of the UMKL problem to include con-
ditions for sparse clinical labelling data is tractable
and thus presents a robust alternative for cancer sub-
type analysis by reducing it to an efﬁcient alternating
quadratic optimization problem.

1. Introduction

The expansion and increased availability of large-
scale biomedical data in the past decade has led to the
rising use of Machine Learning as an integral compo-
nent for providing biological insights - especially in
the domain of cancer research. Recently, with the in-
creasing number of large scale genomic projects, such
as The Cancer Genome Atlas (TCGA), the amount of
openly accessible patient data is greater than ever. In
order to make sense of this ever increasing quantity of
data, numerous statistical approaches have been devel-
oped to analyze TCGA and other similar datasets.

Cancer subtyping is of particular clinical relevance
because it has the potential to enable medical practi-
tioners to design more ﬁnely tuned cancer therapies.
Intuitively, the more precisely we are able to clas-
sify types of cancer, the more effective the treatments
we design can be. Futhermore, cancer subtyping has
the potential to more accurately predict treatment out-
comes and survival rates based on rates of cancer pro-
gression and other clinical factors. Currently, doctors
can identify cancer subtypes in patients by measuring
biological features during the progression of their can-
cer. However, these subtype classiﬁcations are slow
to perform, often very broad (Luminal A vs Luminal
B for BRCA), and have weak prognostic ability, since
subtyping by physicians can only be reﬁned as the can-
cer progresses. Recent advances in statistical analyses
suggest there is much potential to improve cancer sub-
typing.1,5,4

Previous approaches1,5,7 suggest the potential for a
UMKL solution which relies on sparse clinical data
to analyze multi-omic data in the search of more spe-
ciﬁc and useful subtype characterizations - thereby
empowering doctors to determine optimal methods of
treatment. We propose a method for such an ap-
proach, combining the techniques of previous UMKL
approaches with novel constraints based on clinical
data.

TCGA contains information about methylation,
copynumber variation (CNV), microRNA, mRNA,
RPPA, and other factors relevant to the onset and pro-
gression of cancer. The interaction between these lev-
els of genetic control is referred to as multi-omics.
Intuitively, by looking at the genomic causes of can-
cer, we can more accurately predict future outcomes -
as opposed to clinically looking at phenotypic cancer

progression.

Our goal in this paper is to extend the existing unsu-
pervised multiple kernel learning technique in Zhuang
et al. to leverage sparse labeling for speciﬁc use with
clinical and multi-omic data from TCGA. We wish to
demonstrate that such an extension is still a tractable
optimization problem.

2. Related Work

This paper builds off of Zhuang et al., which pro-
poses a method for UMKL that also integrates case-
speciﬁc constraint functions. This paper demonstrates
the feasibility of the original problem and provides
cases for which the algorithm performs especially
well. The algorithm presented in this paper, however,
does not perform well as the number of features in-
creases or as the number of sample points decreases.7
Supervised multiple kernel learning has been imple-
mented in Daemen et al. to classify rectal cancer mi-
croarray data. The resulting Support Vector Machine
accurately classiﬁed different outcomes, often reach-
ing above 0.90. The model also performed better when
using more than one genome-wide data set, suggesting
that integrating multiple genome-wide data sources al-
lows models to reach higher accuracy.1

The iCluster algorithm developed by Shen et al.
was used to accurately group cancer outcomes based
on multi-omic considerations found in TCGA data on
breast cancer. The algorithm clusters different can-
cer subtypes using multiple genomic features such
as DNA copy number changes and gene expression.
iCluster, however, does not take advantage of any ker-
nel methods.5

We build upon these papers to solve the problem
of harnessing the power of kernels for unsupervised
learning in a high dimensional feature space with a rel-
atively small number of samples.

3. Background

3.1. Multiple Kernel Learning

ily separated by an SVM. We write this as

m(cid:88)

Kconv =

µtkt

t=1

where µt is the weight assigned to each kernel.
In
MKL, we try to learn the kernel combination that lin-
early separates the data best.

3.2. Unsupervised Multiple Kernel Learning

Unsupervised Multiple Kernel Learning also imple-
ments a linear combination of kernels to create a dis-
tance metric in a higher-dimensional space. The dis-
tance metric is used to determine groupings using a
k-means or alternate clustering algorithm.7

3.3. Sparse Labels

We take advantage of various labels, although
sparse, to constrain our data and aid in the clustering
process. We incorporate the clinical data mentioned
earlier into our cost function to have the resultant com-
bination of kernels reﬂect this added restriction. We
also impose domain knowledge of the separation be-
tween cancer and non-cancer patients. This data can
be included in our model as further priors on the sub-
types.

4. Optimization Problem
4.1. Cluster-label alignment metric

We design an optimization problem to produce a
kernel and clustering which together yield the best as-
signment of patients to subtypes. In order to do this,
we employ the cost functions found in Zhuang et al.
as well as two new cost functions:

(cid:80)

• A good kernel should induce kernel values which
place samples of the same subtype together in
the feature mapping.
In other words for each
xi we expect that the optimal kernel minimizes
kii − 2kij + kjj, where Ci is the set
of all samples with the same known label as xi.
kij = k(xi, xj). Ci = ∅ for xi that do not have
known labels.

xj∈Li

Multiple Kernel Learning is the use of a linear
combination of kernels to map points to a higher-
dimensional feature space where they can be more eas-

• A good kernel should induce kernel values which
place samples of different subtypes apart in the
feature mapping. When this is the case, we expect

2

the kernel to maximize(cid:80)

kii−2kij +kjj.
Where here Ci(cid:54)=j is the set of sample points la-
beled with a different subtype than xi.

xj∈Li(cid:54)=j

Each of these cost functions is designed to affect the
optimization problem in such a way as to yield a ker-
nel which places co-labeled points together and differ-
ently labeled points apart. These heuristics guide the
search for an optimal kernel based on the prior sparse
clinical information by encouraging the eventual adop-
tion of a kernel which conforms to the clinical data as
well as possible. Such a kernel will yield a clustering
which places co-labled points in the same or nearby
clusters, while placing differently labeled points in dif-
ferent clusters if possible.

4.3. A simpler formulation

Instead of optimizing the above function with re-
spect to κ ∈ K and Bi we instead formulate the prob-
lem as one of optimizing the cost function with respect
to µ and D. This allows us to eventually present the
problem as one of solving a quadratic program on the
vector µ and later D.
As in Zhuang et al. we deﬁne matrices D, S ∈
{0, 1}n×n where each element is given as

[D]ij = 1{xj ∈ Bi}
[S]ij = 1{xj ∈ Li}
[Q]ij = 1{xj ∈ Li(cid:54)=j}
[M]ij = xT

i xi + xT

j xj − 2xT

i xj

4.2. Cost function

so that we can write:

In order to obtain an overall cost function for the
optimization problem, we combine these cluster-label
alignment metrics with those found in Zhuang et al. to
form the overall optimization problem:

1
2

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)xi − (cid:88)
n(cid:88)
(cid:88)
n(cid:88)
(cid:88)
n(cid:88)
n(cid:88)

(cid:88)
(cid:88)

xj∈Bi
|Bi|

xj∈Li

i=1

i=1

i

i=1

xj∈Li(cid:54)=j

min

B,k∈Kconv

+ γ1

+ γ2

+ γ3

− γ4

where

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

kijxj

xj∈Bj

kij (cid:107)xi − xj(cid:107)2

(kii − 2kij + kjj)

(kii − 2kij + kjj)

min
µ,D

F

(cid:107)X(I − K ◦ D)(cid:107)2
1
2
+ γ1tr K ◦ D ◦ M(11T ) + γ2 (cid:107)D(cid:107)1,1

+ tr(cid:0)(cid:2)(K ◦ I)(11T ) − 2K + (11T )(K ◦ I)T(cid:3)
m(cid:88)

◦(γ3S + γ4Q)) (11T )

s.t [κ]ij =

µtkt(xi, xj),

t=1

1 ≤ i, j ≤ n,
µT 1 = 1,
µ ≥ 0,
D ∈ {0, 1}n×n

The function given is not quadratic and also not guar-
anteed to be convex. Because of this, there are no
off-the-shelf tools which are able to generally solve
this problem. This difﬁculty motivates our work to re-
formulate the problem as an alternating optimization
problem in the next section.

4.4. Alternating Optimization Algorithm

In order to formulate the problem in such a way
that it can be approximately solved, we break the
problem into two components.7 Consider the space
parametrized by µ and D. We can reduce the above
problem to one of coordinate descent by alternatively
optimizing µ and D while holding the other constant.
If we can show that each of these individual problems

B is a clustering assignment,
µtkt(·,·) :

Kconv = {k(·,·) =

m(cid:88)

m(cid:88)

µt = 1, µt ≥ 0},

t=1

t=1
γi control constraint trade-offs, and
L is given in §4.1.

This cost function is not–in its present form–feasible
to optimize given the size of the data.

3

is itself quadratic and therefore tractable, we will have
broken the problem down into a tractable approxima-
tion.

4.4.1 Solving µ by ﬁxing D

We accomplish this separation by ﬁrst optimizing µ
while holding D constant and ignoring the otherwise
difﬁcult constraints on D. Fixing D we try to solve for
µ by minimizing the cost function:

(cid:33)T

κt,iκT

t,i ◦ didT

i ◦ P

µ + zT µ,

J(µ) = µT

where

[z]t =

(cid:32) m(cid:88)
n(cid:88)

t=1

i=1

+ γ3(ei

− γ4(ei

n(cid:88)

i=1

(cid:88)
(cid:88)

j

(2γ1vi ◦ di − 2pi ◦ di

(cid:88)
(cid:88)

j

Sij + ei

Qij + ei

Sji − 2si)

Qji − 2qi))T κt,i

j

j

P = XT X
κt,i = [kt(xi, x1), ..., kt(xi, xn)]T
pi is the i-th column of P
vi is the i-th column of M
si is the i-th column of S
qi is the i-th column of Q

Most importantly, this cost function is a Quadratic Pro-
gram and so is tractable, even for large X.

4.4.2 Solving D by ﬁxing µ

Because we introduced new constraints on the kernel,
but not the cluster matrix D, the optimization step for
D is unchanged from its presentation in Zhuang et al.
and so we simply give the result for each column of D:

J(d) = dT(cid:0)κκT ◦ P(cid:1) d + (2γ1κ ◦ v − 2κ ◦ p)T d,

which is of the form

J(d) = dT Wd + cT d

and so is also tractable with a QP convex optimization
by iterating over each sample point and using this min-
imization problem to ﬁnd the optimal set of neighbors.

4

5. Discussion

In deriving a quadratic problem from our initial
complex constrained optimization problem, we have
shown that our new approach for incorporating par-
tially labeled data is a solvable problem. Here, we ex-
plain the underpinnings of the initial constraints and
derivation of its quadratic form.

5.1. Underlying motivations

The original UMKL algorithm presents two con-
straints on the behavior of the kernels. The ﬁrst was
that of a standard clustering optimization problem:
minimize the sum of the distances between samples in
the same cluster. In this case, however, the distances
were computed by the kernel function being chosen.
Thus, a good choice of kernel function would place
points which end up in the same cluster together in the
feature mapping. This is useful for learning an optimal
kernel on a large quantity of ’training’ data, only to ap-
ply it using something other than a clustering method.
Zhuang et al. demonstrates this approach by learning
an optimal kernel for a data set, and then applying that
kernel to do classiﬁcation with an SVM.

The second constraint on the behavior of the kernels
was that of continuity with the given sample points.
This constraint enforces, as far as possible, that the
kernel should have a limited impact on the geometry of
the data. Points should not be mapped so that they are
distant from their neighbors in the original space. This
constraint ensures that the kernel does not map points
in an ad hoc manner such that essentially meaningless,
but perfect, clusters form in the feature mapping space.

5.2. Our contributions

Building on this last constraint, this paper intro-
duced two new constraints. The ﬁrst speciﬁed that
points which are given the same labels in the cluster
priors are placed as close to possible in the feature
mapping. Combined with the previous constraint, this
ensures the learned kernel will highlight the underly-
ing geometry in such a way as points with the same la-
bel are close together, but it will not mangle the space
unnecessarily. This forces the kernel to ’learn’ what-
ever is important to identifying each of the prior clus-
ters.

In a similar vein, the second new constraint requires

ify whether two elements have the same or different
initial clusterings. Columns of these matrices can then
be used as masks for selecting only kernel elements
that refer to the distances between samples of the same
lable or of different labels. These correspond to each
of the two constraints.

5.4. Implications

We believe that the work presented in this paper
will be beneﬁcial to the bio-computation community
by providing an additional tool for making sense of
TCGA and other large-scale data sets.
In particu-
lar, we think that applications such as cancer subtype
analysis–which initially motivated this investigation–
are particularly suited due to the the small number of
patients, expansive feature space, complex geometry,
and existence of sparsely labeled subtype data.

6. Acknowledgements

We thank Prof. Olivier Gevaert for his help in for-
mulating and guiding this project and for his help in
understanding and navigating MKL and TCGA. We
also thank Prof. Seraﬁm Batzoglou for his initial sug-
gestions which led us to cancer subtype analysis.

that points with different prior clusters are placed as
far apart as possible. By the same mechanism as the
previous constraint, this forces the kernel optimization
algorithm to learn a kernel that highlights the features
which distinguish clusters.

Both of these new constraints operate on the princi-
ple that the goal of a kernel in a situation in which the
sample data is highly dimensional is to highlight the
features of that data which are most important. We de-
signed the optimization problem such that the optimal
kernel does just this.

5.3. Deriving quadratic form

Injecting these additional constraints into the prob-
lem would help to incorporate sparse training data,
but only if the new problem was solvable.
In or-
der to demonstrate that the new problem is solvable
we showed that it could be reduced to an alternating
optimization problem, each step of which was itself
quadratic. There are numerous know methods for efﬁ-
ciently solving Quadratic Programming problems. An
implementation of this algorithm would simply have to
make use of an available convex optimization toolkit.
This drastically reduces the complexity of the problem
from a general mixed integer problem to a quadratic
programming problem.

In order to demonstrate that the problem could be
reduced to two quadratic programming problems, we
primarily focused on the optimization of µ while hold-
ing the clusters (represented in D) constant. This al-
lowed us to forgo all the constraints on D, and thus
simpliﬁed the problem immensely.

We know that the the kernel matrix K is given by a
summation over the tensor κt,i,j in the dimension in-
dexed by t, weighted by µ. Thus,

m(cid:88)

K =

µtκt.

t=1

Thinking about K in this fashion allows for inter-
preting the additional constraints as selection and scal-
ing operations on columns of κ. We think of the prob-
lem as one of determining which kernel elements to
include and which to avoid for each xi and for each
κt. For each frame of the tensor κ, which elements
must be summed and with which coefﬁcients?

In order to accomplish this ﬁltering step, we trans-
late the initial cluster priors into matrices which spec-

5

Kernel Learning Framework for Cancer Subtype Analysis

with Multi-omics Data Integration

William Bradbury

Thomas Lau

Shivaal Roy

wbradbur@stanford

thomklau@stanford

shivaal@stanford

December 12, 2015

Abstract

Recent advances in Multiple Kernel Learning
(MKL) and unsupervised clustering methods have
each enabled large-scale analysis of integrated multi-
omics data for cancer subtyping. However, efforts
to combine the advantages of a kernel approach with
the ﬂexibility of unsupervised methods have not pro-
gressed due to the underconstrained nature of the
problem. In this paper, we present a novel approach to
solving this problem based on methods for Unsuper-
vised Multiple Kernel Learning (UMKL). We present
the possibility of constraining the problem with avail-
able data on clinically determined subtypes for cancer
patients. We successfully demonstrate that modifying
the constraints of the UMKL problem to include con-
ditions for sparse clinical labelling data is tractable
and thus presents a robust alternative for cancer sub-
type analysis by reducing it to an efﬁcient alternating
quadratic optimization problem.

1. Introduction

The expansion and increased availability of large-
scale biomedical data in the past decade has led to the
rising use of Machine Learning as an integral compo-
nent for providing biological insights - especially in
the domain of cancer research. Recently, with the in-
creasing number of large scale genomic projects, such
as The Cancer Genome Atlas (TCGA), the amount of
openly accessible patient data is greater than ever. In
order to make sense of this ever increasing quantity of
data, numerous statistical approaches have been devel-
oped to analyze TCGA and other similar datasets.

Cancer subtyping is of particular clinical relevance
because it has the potential to enable medical practi-
tioners to design more ﬁnely tuned cancer therapies.
Intuitively, the more precisely we are able to clas-
sify types of cancer, the more effective the treatments
we design can be. Futhermore, cancer subtyping has
the potential to more accurately predict treatment out-
comes and survival rates based on rates of cancer pro-
gression and other clinical factors. Currently, doctors
can identify cancer subtypes in patients by measuring
biological features during the progression of their can-
cer. However, these subtype classiﬁcations are slow
to perform, often very broad (Luminal A vs Luminal
B for BRCA), and have weak prognostic ability, since
subtyping by physicians can only be reﬁned as the can-
cer progresses. Recent advances in statistical analyses
suggest there is much potential to improve cancer sub-
typing.1,5,4

Previous approaches1,5,7 suggest the potential for a
UMKL solution which relies on sparse clinical data
to analyze multi-omic data in the search of more spe-
ciﬁc and useful subtype characterizations - thereby
empowering doctors to determine optimal methods of
treatment. We propose a method for such an ap-
proach, combining the techniques of previous UMKL
approaches with novel constraints based on clinical
data.

TCGA contains information about methylation,
copynumber variation (CNV), microRNA, mRNA,
RPPA, and other factors relevant to the onset and pro-
gression of cancer. The interaction between these lev-
els of genetic control is referred to as multi-omics.
Intuitively, by looking at the genomic causes of can-
cer, we can more accurately predict future outcomes -
as opposed to clinically looking at phenotypic cancer

progression.

Our goal in this paper is to extend the existing unsu-
pervised multiple kernel learning technique in Zhuang
et al. to leverage sparse labeling for speciﬁc use with
clinical and multi-omic data from TCGA. We wish to
demonstrate that such an extension is still a tractable
optimization problem.

2. Related Work

This paper builds off of Zhuang et al., which pro-
poses a method for UMKL that also integrates case-
speciﬁc constraint functions. This paper demonstrates
the feasibility of the original problem and provides
cases for which the algorithm performs especially
well. The algorithm presented in this paper, however,
does not perform well as the number of features in-
creases or as the number of sample points decreases.7
Supervised multiple kernel learning has been imple-
mented in Daemen et al. to classify rectal cancer mi-
croarray data. The resulting Support Vector Machine
accurately classiﬁed different outcomes, often reach-
ing above 0.90. The model also performed better when
using more than one genome-wide data set, suggesting
that integrating multiple genome-wide data sources al-
lows models to reach higher accuracy.1

The iCluster algorithm developed by Shen et al.
was used to accurately group cancer outcomes based
on multi-omic considerations found in TCGA data on
breast cancer. The algorithm clusters different can-
cer subtypes using multiple genomic features such
as DNA copy number changes and gene expression.
iCluster, however, does not take advantage of any ker-
nel methods.5

We build upon these papers to solve the problem
of harnessing the power of kernels for unsupervised
learning in a high dimensional feature space with a rel-
atively small number of samples.

3. Background

3.1. Multiple Kernel Learning

ily separated by an SVM. We write this as

m(cid:88)

Kconv =

µtkt

t=1

where µt is the weight assigned to each kernel.
In
MKL, we try to learn the kernel combination that lin-
early separates the data best.

3.2. Unsupervised Multiple Kernel Learning

Unsupervised Multiple Kernel Learning also imple-
ments a linear combination of kernels to create a dis-
tance metric in a higher-dimensional space. The dis-
tance metric is used to determine groupings using a
k-means or alternate clustering algorithm.7

3.3. Sparse Labels

We take advantage of various labels, although
sparse, to constrain our data and aid in the clustering
process. We incorporate the clinical data mentioned
earlier into our cost function to have the resultant com-
bination of kernels reﬂect this added restriction. We
also impose domain knowledge of the separation be-
tween cancer and non-cancer patients. This data can
be included in our model as further priors on the sub-
types.

4. Optimization Problem
4.1. Cluster-label alignment metric

We design an optimization problem to produce a
kernel and clustering which together yield the best as-
signment of patients to subtypes. In order to do this,
we employ the cost functions found in Zhuang et al.
as well as two new cost functions:

(cid:80)

• A good kernel should induce kernel values which
place samples of the same subtype together in
the feature mapping.
In other words for each
xi we expect that the optimal kernel minimizes
kii − 2kij + kjj, where Ci is the set
of all samples with the same known label as xi.
kij = k(xi, xj). Ci = ∅ for xi that do not have
known labels.

xj∈Li

Multiple Kernel Learning is the use of a linear
combination of kernels to map points to a higher-
dimensional feature space where they can be more eas-

• A good kernel should induce kernel values which
place samples of different subtypes apart in the
feature mapping. When this is the case, we expect

2

the kernel to maximize(cid:80)

kii−2kij +kjj.
Where here Ci(cid:54)=j is the set of sample points la-
beled with a different subtype than xi.

xj∈Li(cid:54)=j

Each of these cost functions is designed to affect the
optimization problem in such a way as to yield a ker-
nel which places co-labeled points together and differ-
ently labeled points apart. These heuristics guide the
search for an optimal kernel based on the prior sparse
clinical information by encouraging the eventual adop-
tion of a kernel which conforms to the clinical data as
well as possible. Such a kernel will yield a clustering
which places co-labled points in the same or nearby
clusters, while placing differently labeled points in dif-
ferent clusters if possible.

4.3. A simpler formulation

Instead of optimizing the above function with re-
spect to κ ∈ K and Bi we instead formulate the prob-
lem as one of optimizing the cost function with respect
to µ and D. This allows us to eventually present the
problem as one of solving a quadratic program on the
vector µ and later D.
As in Zhuang et al. we deﬁne matrices D, S ∈
{0, 1}n×n where each element is given as

[D]ij = 1{xj ∈ Bi}
[S]ij = 1{xj ∈ Li}
[Q]ij = 1{xj ∈ Li(cid:54)=j}
[M]ij = xT

i xi + xT

j xj − 2xT

i xj

4.2. Cost function

so that we can write:

In order to obtain an overall cost function for the
optimization problem, we combine these cluster-label
alignment metrics with those found in Zhuang et al. to
form the overall optimization problem:

1
2

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)xi − (cid:88)
n(cid:88)
(cid:88)
n(cid:88)
(cid:88)
n(cid:88)
n(cid:88)

(cid:88)
(cid:88)

xj∈Bi
|Bi|

xj∈Li

i=1

i=1

i

i=1

xj∈Li(cid:54)=j

min

B,k∈Kconv

+ γ1

+ γ2

+ γ3

− γ4

where

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

kijxj

xj∈Bj

kij (cid:107)xi − xj(cid:107)2

(kii − 2kij + kjj)

(kii − 2kij + kjj)

min
µ,D

F

(cid:107)X(I − K ◦ D)(cid:107)2
1
2
+ γ1tr K ◦ D ◦ M(11T ) + γ2 (cid:107)D(cid:107)1,1

+ tr(cid:0)(cid:2)(K ◦ I)(11T ) − 2K + (11T )(K ◦ I)T(cid:3)
m(cid:88)

◦(γ3S + γ4Q)) (11T )

s.t [κ]ij =

µtkt(xi, xj),

t=1

1 ≤ i, j ≤ n,
µT 1 = 1,
µ ≥ 0,
D ∈ {0, 1}n×n

The function given is not quadratic and also not guar-
anteed to be convex. Because of this, there are no
off-the-shelf tools which are able to generally solve
this problem. This difﬁculty motivates our work to re-
formulate the problem as an alternating optimization
problem in the next section.

4.4. Alternating Optimization Algorithm

In order to formulate the problem in such a way
that it can be approximately solved, we break the
problem into two components.7 Consider the space
parametrized by µ and D. We can reduce the above
problem to one of coordinate descent by alternatively
optimizing µ and D while holding the other constant.
If we can show that each of these individual problems

B is a clustering assignment,
µtkt(·,·) :

Kconv = {k(·,·) =

m(cid:88)

m(cid:88)

µt = 1, µt ≥ 0},

t=1

t=1
γi control constraint trade-offs, and
L is given in §4.1.

This cost function is not–in its present form–feasible
to optimize given the size of the data.

3

is itself quadratic and therefore tractable, we will have
broken the problem down into a tractable approxima-
tion.

4.4.1 Solving µ by ﬁxing D

We accomplish this separation by ﬁrst optimizing µ
while holding D constant and ignoring the otherwise
difﬁcult constraints on D. Fixing D we try to solve for
µ by minimizing the cost function:

(cid:33)T

κt,iκT

t,i ◦ didT

i ◦ P

µ + zT µ,

J(µ) = µT

where

[z]t =

(cid:32) m(cid:88)
n(cid:88)

t=1

i=1

+ γ3(ei

− γ4(ei

n(cid:88)

i=1

(cid:88)
(cid:88)

j

(2γ1vi ◦ di − 2pi ◦ di

(cid:88)
(cid:88)

j

Sij + ei

Qij + ei

Sji − 2si)

Qji − 2qi))T κt,i

j

j

P = XT X
κt,i = [kt(xi, x1), ..., kt(xi, xn)]T
pi is the i-th column of P
vi is the i-th column of M
si is the i-th column of S
qi is the i-th column of Q

Most importantly, this cost function is a Quadratic Pro-
gram and so is tractable, even for large X.

4.4.2 Solving D by ﬁxing µ

Because we introduced new constraints on the kernel,
but not the cluster matrix D, the optimization step for
D is unchanged from its presentation in Zhuang et al.
and so we simply give the result for each column of D:

J(d) = dT(cid:0)κκT ◦ P(cid:1) d + (2γ1κ ◦ v − 2κ ◦ p)T d,

which is of the form

J(d) = dT Wd + cT d

and so is also tractable with a QP convex optimization
by iterating over each sample point and using this min-
imization problem to ﬁnd the optimal set of neighbors.

4

5. Discussion

In deriving a quadratic problem from our initial
complex constrained optimization problem, we have
shown that our new approach for incorporating par-
tially labeled data is a solvable problem. Here, we ex-
plain the underpinnings of the initial constraints and
derivation of its quadratic form.

5.1. Underlying motivations

The original UMKL algorithm presents two con-
straints on the behavior of the kernels. The ﬁrst was
that of a standard clustering optimization problem:
minimize the sum of the distances between samples in
the same cluster. In this case, however, the distances
were computed by the kernel function being chosen.
Thus, a good choice of kernel function would place
points which end up in the same cluster together in the
feature mapping. This is useful for learning an optimal
kernel on a large quantity of ’training’ data, only to ap-
ply it using something other than a clustering method.
Zhuang et al. demonstrates this approach by learning
an optimal kernel for a data set, and then applying that
kernel to do classiﬁcation with an SVM.

The second constraint on the behavior of the kernels
was that of continuity with the given sample points.
This constraint enforces, as far as possible, that the
kernel should have a limited impact on the geometry of
the data. Points should not be mapped so that they are
distant from their neighbors in the original space. This
constraint ensures that the kernel does not map points
in an ad hoc manner such that essentially meaningless,
but perfect, clusters form in the feature mapping space.

5.2. Our contributions

Building on this last constraint, this paper intro-
duced two new constraints. The ﬁrst speciﬁed that
points which are given the same labels in the cluster
priors are placed as close to possible in the feature
mapping. Combined with the previous constraint, this
ensures the learned kernel will highlight the underly-
ing geometry in such a way as points with the same la-
bel are close together, but it will not mangle the space
unnecessarily. This forces the kernel to ’learn’ what-
ever is important to identifying each of the prior clus-
ters.

In a similar vein, the second new constraint requires

ify whether two elements have the same or different
initial clusterings. Columns of these matrices can then
be used as masks for selecting only kernel elements
that refer to the distances between samples of the same
lable or of different labels. These correspond to each
of the two constraints.

5.4. Implications

We believe that the work presented in this paper
will be beneﬁcial to the bio-computation community
by providing an additional tool for making sense of
TCGA and other large-scale data sets.
In particu-
lar, we think that applications such as cancer subtype
analysis–which initially motivated this investigation–
are particularly suited due to the the small number of
patients, expansive feature space, complex geometry,
and existence of sparsely labeled subtype data.

6. Acknowledgements

We thank Prof. Olivier Gevaert for his help in for-
mulating and guiding this project and for his help in
understanding and navigating MKL and TCGA. We
also thank Prof. Seraﬁm Batzoglou for his initial sug-
gestions which led us to cancer subtype analysis.

that points with different prior clusters are placed as
far apart as possible. By the same mechanism as the
previous constraint, this forces the kernel optimization
algorithm to learn a kernel that highlights the features
which distinguish clusters.

Both of these new constraints operate on the princi-
ple that the goal of a kernel in a situation in which the
sample data is highly dimensional is to highlight the
features of that data which are most important. We de-
signed the optimization problem such that the optimal
kernel does just this.

5.3. Deriving quadratic form

Injecting these additional constraints into the prob-
lem would help to incorporate sparse training data,
but only if the new problem was solvable.
In or-
der to demonstrate that the new problem is solvable
we showed that it could be reduced to an alternating
optimization problem, each step of which was itself
quadratic. There are numerous know methods for efﬁ-
ciently solving Quadratic Programming problems. An
implementation of this algorithm would simply have to
make use of an available convex optimization toolkit.
This drastically reduces the complexity of the problem
from a general mixed integer problem to a quadratic
programming problem.

In order to demonstrate that the problem could be
reduced to two quadratic programming problems, we
primarily focused on the optimization of µ while hold-
ing the clusters (represented in D) constant. This al-
lowed us to forgo all the constraints on D, and thus
simpliﬁed the problem immensely.

We know that the the kernel matrix K is given by a
summation over the tensor κt,i,j in the dimension in-
dexed by t, weighted by µ. Thus,

m(cid:88)

K =

µtκt.

t=1

Thinking about K in this fashion allows for inter-
preting the additional constraints as selection and scal-
ing operations on columns of κ. We think of the prob-
lem as one of determining which kernel elements to
include and which to avoid for each xi and for each
κt. For each frame of the tensor κ, which elements
must be summed and with which coefﬁcients?

In order to accomplish this ﬁltering step, we trans-
late the initial cluster priors into matrices which spec-

5

References
[1] Anneleen Daemen et al. “A kernel-based inte-
gration of genome-wide data for clinical decision
support.” In: Genome medicine 1.4 (2009), p. 39.
[2] Anneleen Daemen et al. “Improved microarray-
based decision support with graph encoded inter-
actome data.” In: PloS one 5.4 (2010), e10225.

[3] GRG Lanckriet and Nello Cristianini. “Learn-
ing the kernel matrix with semideﬁnite program-
ming”. In: Journal of Machine Learning Re-
search 5 (2004), pp. 27–72.

[4] R. Shen, A. B. Olshen, and M. Ladanyi. “Inte-
grative clustering of multiple genomic data types
using a joint latent variable model with applica-
tion to breast and lung cancer subtype analysis”.
In: Bioinformatics 25.22 (2009), pp. 2906–2912.
[5] Ronglai Shen et al. “Integrative Subtype Discov-
ery in Glioblastoma Using iCluster”. In: PLoS
ONE 7.4 (2012), e35236.

[6] Emily a. Vucic et al. “Translating cancer ’omics’
to improved outcomes”. In: Genome Research
22.2 (2012), pp. 188–195.
Jinfeng Zhuang et al. “Unsupervised multiple
kernel learning”. In: Proceedings of the Third
Asian Conference on Machine Learning 20
(2011), pp. 129–144.

[7]

6

