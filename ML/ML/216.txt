Assessing and Implementing Automated News 

Classification 

 

Department of Electrical Engineering 

Francisco Romero 
Stanford University 
Stanford, California 

faromero@stanford.edu 

Department of Electrical Engineering 

Zahra Koochak 
Stanford University 
Stanford, California 
zahraa@stanford.edu 

 
 

in 

(NLP) 

Abstractâ€”Newsfeed  websites  sort  articles  by 
subject to make it easier for readers to search for 
articles 
their  preferred  category.  When 
uploading a new article, authors are usually tasked 
with  selecting  the  most  pertinent  category  so  the 
new  addition  can  then  be  grouped  with  similar 
articles.  We  are  interested  in  further  developing 
the  framework  to  automatize  the  classification  of 
news articles using machine learning and Natural 
Language  Processing 
techniques.  We 
explore  three  classification  methods:  Support 
Vector Machine (SVM), NaÃ¯ve Bayes, and Softmax 
Regression, and evaluate each classifierâ€™s ability to 
select  the  appropriate  category  given  an  articleâ€™s 
title  and  a  brief  article  description.  Our  results 
show  Softmax  Regression  to  be  the  best  classifier 
among the three we evaluated. 
Index  Termsâ€”news,  articles,  NLP,  SVM,  NaÃ¯ve 
Bayes, Softmax, classification, tf-idf 
1.  Introduction 
When  visiting  a  newsfeed  website,  we  are  often 
interested  in  reading  articles  in  a  specific  category. 
Based on their content, articles are sorted by subject, 
which  allows  readers  to  effortlessly  find  articles  in 
their  preferred  category.  To  determine  the  articleâ€™s 
category,  most  newsfeed  websites  ask  the  author  to 
select the best-fit category for their article. Selecting 
an articleâ€™s category is not only based on the authorâ€™s 
opinion, but can also be tedious when several articles 
are  simultaneously  being  added 
to  a  newsfeed 
website.  Since  the  vocabulary  and  terminology  used 
by  an  articleâ€™s  author  is  indicative  of  the  target 
audience  and,  more  generally,  of 
the  articleâ€™s 
category,  we  believe  this  process  can  be  effectively 
automated. 
For  our  project,  we  are  interested  in  assessing  three 
classification  methods  to  determine  the  feasibility  of 
automatically  classifying  news  articles.  We  selected 
to  use  the  articleâ€™s  title  and  a  1-2  sentence  article 

description  as  the  input  to  our  classifier.  We  then 
evaluate  the  capability  of  our  classifier  using  a 
minimal  amount  of  information  about  the  articleâ€™s 
subject.  Finally,  we  used  three  supervised  learning 
classifier to output a predicted article category: NaÃ¯ve 
Bayes, Support Vector Machine (SVM), and Softmax 
Regression.  Our  data  spans  over  seven  categories: 
Sports,  US,  Science  and  Technology,  Business, 
World,  Entertainment,  and  Health.  Based  on  the 
lexical features of each article, it was the job of each 
classifier  to  select  the  most  appropriate  category  for 
the article. 
2.  Relevant Work 
Previous work has focused on developing algorithms 
and software to automate the process of accurate text 
classification.  Young  and  Jeong  implemented  a  new 
feature  scaling  method  that  uses  the  NaÃ¯ve  Bayes 
classifier. The feature scaling method was tested on a 
news  group  dataset  and  outperformed  other  popular 
ranking  schemes,  such  as  Information  Gain  while 
noting  NaÃ¯ve  Bayes  as  being  a  suitable  classifier  for 
news articles [10]. Wang et al. developed an optimal 
text  categorization  algorithm  that  is  based  on  the 
SVM algorithm used in this paper [11]. Using a news 
article  corpus  similar  to  ours,  they  found  their 
algorithm  to  outperform  other  classifiers  such  as  the 
decision-tree  algorithm  and  the  K-nearest  neighbor 
algorithm.  Hakim  et  al.  evaluated 
the  Term 
Frequency-Inverse  Document  Frequency  (TF-IDF) 
algorithmâ€™s ability to be used in text classification for 
news  articles  in  Bahasa  Indonesia  [13].  However, 
their approach did not focus on any machine learning 
techniques, only on the TF-IDF algorithm. Looking to 
our  future  work,  one  of  the  first  frameworks  for 
Neural  Networks  was  developed  by  Ruiz  and 
Srinivasan  [12].  Using  about  2,350  documents,  they 
showed  the  ability  of  Neural  Networks  to  accurately 
categorize text. Do and Ng explored text classification 
using a modified Softmax Regression algorithm [17]. 

 

1 

Assessing and Implementing Automated News 

Classification 

 

Department of Electrical Engineering 

Francisco Romero 
Stanford University 
Stanford, California 

faromero@stanford.edu 

Department of Electrical Engineering 

Zahra Koochak 
Stanford University 
Stanford, California 
zahraa@stanford.edu 

 
 

in 

(NLP) 

Abstractâ€”Newsfeed  websites  sort  articles  by 
subject to make it easier for readers to search for 
articles 
their  preferred  category.  When 
uploading a new article, authors are usually tasked 
with  selecting  the  most  pertinent  category  so  the 
new  addition  can  then  be  grouped  with  similar 
articles.  We  are  interested  in  further  developing 
the  framework  to  automatize  the  classification  of 
news articles using machine learning and Natural 
Language  Processing 
techniques.  We 
explore  three  classification  methods:  Support 
Vector Machine (SVM), NaÃ¯ve Bayes, and Softmax 
Regression, and evaluate each classifierâ€™s ability to 
select  the  appropriate  category  given  an  articleâ€™s 
title  and  a  brief  article  description.  Our  results 
show  Softmax  Regression  to  be  the  best  classifier 
among the three we evaluated. 
Index  Termsâ€”news,  articles,  NLP,  SVM,  NaÃ¯ve 
Bayes, Softmax, classification, tf-idf 
1.  Introduction 
When  visiting  a  newsfeed  website,  we  are  often 
interested  in  reading  articles  in  a  specific  category. 
Based on their content, articles are sorted by subject, 
which  allows  readers  to  effortlessly  find  articles  in 
their  preferred  category.  To  determine  the  articleâ€™s 
category,  most  newsfeed  websites  ask  the  author  to 
select the best-fit category for their article. Selecting 
an articleâ€™s category is not only based on the authorâ€™s 
opinion, but can also be tedious when several articles 
are  simultaneously  being  added 
to  a  newsfeed 
website.  Since  the  vocabulary  and  terminology  used 
by  an  articleâ€™s  author  is  indicative  of  the  target 
audience  and,  more  generally,  of 
the  articleâ€™s 
category,  we  believe  this  process  can  be  effectively 
automated. 
For  our  project,  we  are  interested  in  assessing  three 
classification  methods  to  determine  the  feasibility  of 
automatically  classifying  news  articles.  We  selected 
to  use  the  articleâ€™s  title  and  a  1-2  sentence  article 

description  as  the  input  to  our  classifier.  We  then 
evaluate  the  capability  of  our  classifier  using  a 
minimal  amount  of  information  about  the  articleâ€™s 
subject.  Finally,  we  used  three  supervised  learning 
classifier to output a predicted article category: NaÃ¯ve 
Bayes, Support Vector Machine (SVM), and Softmax 
Regression.  Our  data  spans  over  seven  categories: 
Sports,  US,  Science  and  Technology,  Business, 
World,  Entertainment,  and  Health.  Based  on  the 
lexical features of each article, it was the job of each 
classifier  to  select  the  most  appropriate  category  for 
the article. 
2.  Relevant Work 
Previous work has focused on developing algorithms 
and software to automate the process of accurate text 
classification.  Young  and  Jeong  implemented  a  new 
feature  scaling  method  that  uses  the  NaÃ¯ve  Bayes 
classifier. The feature scaling method was tested on a 
news  group  dataset  and  outperformed  other  popular 
ranking  schemes,  such  as  Information  Gain  while 
noting  NaÃ¯ve  Bayes  as  being  a  suitable  classifier  for 
news articles [10]. Wang et al. developed an optimal 
text  categorization  algorithm  that  is  based  on  the 
SVM algorithm used in this paper [11]. Using a news 
article  corpus  similar  to  ours,  they  found  their 
algorithm  to  outperform  other  classifiers  such  as  the 
decision-tree  algorithm  and  the  K-nearest  neighbor 
algorithm.  Hakim  et  al.  evaluated 
the  Term 
Frequency-Inverse  Document  Frequency  (TF-IDF) 
algorithmâ€™s ability to be used in text classification for 
news  articles  in  Bahasa  Indonesia  [13].  However, 
their approach did not focus on any machine learning 
techniques, only on the TF-IDF algorithm. Looking to 
our  future  work,  one  of  the  first  frameworks  for 
Neural  Networks  was  developed  by  Ruiz  and 
Srinivasan  [12].  Using  about  2,350  documents,  they 
showed  the  ability  of  Neural  Networks  to  accurately 
categorize text. Do and Ng explored text classification 
using a modified Softmax Regression algorithm [17]. 

 

1 

Table 1: Lexical Feature Extraction 

Top 10 Words for Each Category 
Science & 
World 
Technology 

Business 

PRESIDENT 

Entertainment 

Health 

US 
BILLION  US 
NEW 
PRICES 
OIL 
BANK 
SALES 
MAY 
STOCKS  BIN 
UP 

NEW 
THEATER 
SHOW 
STAR 
IDOL 

KILLED 
FORCES 
NUCLEAR 
GOVERNMENT  WEDDING 
AMERICAN 
POLICE 
UP 
PEOPLE 
FIRST 
FILM 

LIBYA 

STUDY 
NEW 
HEALTH 
CANCER 
US 
MAY 
DRUG 
RISK 
HEART 
PEOPLE 

necessarily reflect the classifierâ€™s ability to determine 
the category of an article from the testing set. 
Pre-processing the news article data involved three 
steps. First, we separated each articleâ€™s title, 
description, and pre-labeled category into a separate 
text file, since the corpus is formatted into a single 
file. Second, we removed all punctuation from the 
title and description. Third, we capitalized all letters 
in the title and description. The latter two steps are 
necessary for performing a lexical feature extraction 
using the vocabulary of the title and the article 
description. 
4.  Methodology 
To test the three classifiers, we divided our data into 
training  news  articles  and  testing  news  articles.  70% 
of  our  data  (22,819  articles)  were  designated  as  the 
training  articles  and 
the  remaining  30%  (9783 
articles) were designated as the testing articles. 
Since  an  author  writes  an  article  with  an  intended 
category  or  subject 
the 
vocabulary  can  be  used  as  our  features  for  our 
classifiers.  Thus,  our  objective  for  our  feature 
extraction  was  to  obtain  the f-most  salient  words  for 
each category, and count how many times each word 
appeared in a given article. We tested feature sizes of 
f = 40, 50, 70, 150 and 200 to obtain the best accuracy 
possible  for  each  classifier.  To  obtain  the  f-most 
salient words for each category, we used the TF-IDF 
algorithm, which we explain in the next section. The 
extracted  features  were  then  passed  to  each  of  the 
three classifiers. 
4.1  Extracting Salient Words with TF-IDF 
The TF-IDF weighting scheme will assign each term, 
t, a given weight in a document as follows: 

ğ‘¤!,!=ğ‘¡ğ‘“!,! ğ‘¥ logğ‘ ğ‘‘ğ‘“!  

(1) 
where  N  is  the  number  of  documents.  The  weight  is 

assigned by the product of ğ‘¡ğ‘“!,!, the term frequency, 
and logğ‘ ğ‘‘ğ‘“! , the inverse document frequency. For 

in  mind,  we  believe 

Sports 

US 

NEW 
US 
STATE 
TEXAS 
POLICE 
STATES 

NFL 
FIRST 
GAME 
WIN 
OVER 
NEW 
PLAYERS  TUESDAY 
SEASON 
COACH 
METS 
 

OVER 
WEDNESDAY 
COURT 

NEW 
APPLE 
GOOGLE 
SPACE 
FACEBOOK 
US 
ONLINE 
INTERNET 
SONY 
IPAD 

The classifier outperformed one-against-all SVM and 
multi-class SVM. 
Our work deviates from the aforementioned studies in 
that we used only the title and a short description of 
each article for our lexical feature extraction and we 
focused on evaluating all three classifiers rather than 
trying to optimize the performance of a particular one. 
In  addition,  we  are  using  a  variant  of  the  Softmax 
Regression  algorithm  presented  by  Do  and  Ng  to 
perform our text classification. 

 

Figure 1: Distribution of the TagMyNews Data 

 
3.  Dataset and Pre-Processing 
To perform our classification evaluation, we used the 
TagMyNews Dataset [1]. The corpus includes 32,602 
training examples of news articles. Each training 
example has a structure including a title, a description, 
a news article link, an ID, the date of publication, the 
news article source, and a subject category. Of interest 
to us were the articleâ€™s title, the brief article 
description, and the pre-labeled category. Figure 1 
shows the distribution of news articles for each 
category. The majority of the training examples were 
from the Sports category, while we had the fewest 
training examples from the Health category. 
However, as we show in Section 5, the number of 
training examples for each category did not 

 

2 

Assessing and Implementing Automated News 

Classification 

 

Department of Electrical Engineering 

Francisco Romero 
Stanford University 
Stanford, California 

faromero@stanford.edu 

Department of Electrical Engineering 

Zahra Koochak 
Stanford University 
Stanford, California 
zahraa@stanford.edu 

 
 

in 

(NLP) 

Abstractâ€”Newsfeed  websites  sort  articles  by 
subject to make it easier for readers to search for 
articles 
their  preferred  category.  When 
uploading a new article, authors are usually tasked 
with  selecting  the  most  pertinent  category  so  the 
new  addition  can  then  be  grouped  with  similar 
articles.  We  are  interested  in  further  developing 
the  framework  to  automatize  the  classification  of 
news articles using machine learning and Natural 
Language  Processing 
techniques.  We 
explore  three  classification  methods:  Support 
Vector Machine (SVM), NaÃ¯ve Bayes, and Softmax 
Regression, and evaluate each classifierâ€™s ability to 
select  the  appropriate  category  given  an  articleâ€™s 
title  and  a  brief  article  description.  Our  results 
show  Softmax  Regression  to  be  the  best  classifier 
among the three we evaluated. 
Index  Termsâ€”news,  articles,  NLP,  SVM,  NaÃ¯ve 
Bayes, Softmax, classification, tf-idf 
1.  Introduction 
When  visiting  a  newsfeed  website,  we  are  often 
interested  in  reading  articles  in  a  specific  category. 
Based on their content, articles are sorted by subject, 
which  allows  readers  to  effortlessly  find  articles  in 
their  preferred  category.  To  determine  the  articleâ€™s 
category,  most  newsfeed  websites  ask  the  author  to 
select the best-fit category for their article. Selecting 
an articleâ€™s category is not only based on the authorâ€™s 
opinion, but can also be tedious when several articles 
are  simultaneously  being  added 
to  a  newsfeed 
website.  Since  the  vocabulary  and  terminology  used 
by  an  articleâ€™s  author  is  indicative  of  the  target 
audience  and,  more  generally,  of 
the  articleâ€™s 
category,  we  believe  this  process  can  be  effectively 
automated. 
For  our  project,  we  are  interested  in  assessing  three 
classification  methods  to  determine  the  feasibility  of 
automatically  classifying  news  articles.  We  selected 
to  use  the  articleâ€™s  title  and  a  1-2  sentence  article 

description  as  the  input  to  our  classifier.  We  then 
evaluate  the  capability  of  our  classifier  using  a 
minimal  amount  of  information  about  the  articleâ€™s 
subject.  Finally,  we  used  three  supervised  learning 
classifier to output a predicted article category: NaÃ¯ve 
Bayes, Support Vector Machine (SVM), and Softmax 
Regression.  Our  data  spans  over  seven  categories: 
Sports,  US,  Science  and  Technology,  Business, 
World,  Entertainment,  and  Health.  Based  on  the 
lexical features of each article, it was the job of each 
classifier  to  select  the  most  appropriate  category  for 
the article. 
2.  Relevant Work 
Previous work has focused on developing algorithms 
and software to automate the process of accurate text 
classification.  Young  and  Jeong  implemented  a  new 
feature  scaling  method  that  uses  the  NaÃ¯ve  Bayes 
classifier. The feature scaling method was tested on a 
news  group  dataset  and  outperformed  other  popular 
ranking  schemes,  such  as  Information  Gain  while 
noting  NaÃ¯ve  Bayes  as  being  a  suitable  classifier  for 
news articles [10]. Wang et al. developed an optimal 
text  categorization  algorithm  that  is  based  on  the 
SVM algorithm used in this paper [11]. Using a news 
article  corpus  similar  to  ours,  they  found  their 
algorithm  to  outperform  other  classifiers  such  as  the 
decision-tree  algorithm  and  the  K-nearest  neighbor 
algorithm.  Hakim  et  al.  evaluated 
the  Term 
Frequency-Inverse  Document  Frequency  (TF-IDF) 
algorithmâ€™s ability to be used in text classification for 
news  articles  in  Bahasa  Indonesia  [13].  However, 
their approach did not focus on any machine learning 
techniques, only on the TF-IDF algorithm. Looking to 
our  future  work,  one  of  the  first  frameworks  for 
Neural  Networks  was  developed  by  Ruiz  and 
Srinivasan  [12].  Using  about  2,350  documents,  they 
showed  the  ability  of  Neural  Networks  to  accurately 
categorize text. Do and Ng explored text classification 
using a modified Softmax Regression algorithm [17]. 

 

1 

Table 1: Lexical Feature Extraction 

Top 10 Words for Each Category 
Science & 
World 
Technology 

Business 

PRESIDENT 

Entertainment 

Health 

US 
BILLION  US 
NEW 
PRICES 
OIL 
BANK 
SALES 
MAY 
STOCKS  BIN 
UP 

NEW 
THEATER 
SHOW 
STAR 
IDOL 

KILLED 
FORCES 
NUCLEAR 
GOVERNMENT  WEDDING 
AMERICAN 
POLICE 
UP 
PEOPLE 
FIRST 
FILM 

LIBYA 

STUDY 
NEW 
HEALTH 
CANCER 
US 
MAY 
DRUG 
RISK 
HEART 
PEOPLE 

necessarily reflect the classifierâ€™s ability to determine 
the category of an article from the testing set. 
Pre-processing the news article data involved three 
steps. First, we separated each articleâ€™s title, 
description, and pre-labeled category into a separate 
text file, since the corpus is formatted into a single 
file. Second, we removed all punctuation from the 
title and description. Third, we capitalized all letters 
in the title and description. The latter two steps are 
necessary for performing a lexical feature extraction 
using the vocabulary of the title and the article 
description. 
4.  Methodology 
To test the three classifiers, we divided our data into 
training  news  articles  and  testing  news  articles.  70% 
of  our  data  (22,819  articles)  were  designated  as  the 
training  articles  and 
the  remaining  30%  (9783 
articles) were designated as the testing articles. 
Since  an  author  writes  an  article  with  an  intended 
category  or  subject 
the 
vocabulary  can  be  used  as  our  features  for  our 
classifiers.  Thus,  our  objective  for  our  feature 
extraction  was  to  obtain  the f-most  salient  words  for 
each category, and count how many times each word 
appeared in a given article. We tested feature sizes of 
f = 40, 50, 70, 150 and 200 to obtain the best accuracy 
possible  for  each  classifier.  To  obtain  the  f-most 
salient words for each category, we used the TF-IDF 
algorithm, which we explain in the next section. The 
extracted  features  were  then  passed  to  each  of  the 
three classifiers. 
4.1  Extracting Salient Words with TF-IDF 
The TF-IDF weighting scheme will assign each term, 
t, a given weight in a document as follows: 

ğ‘¤!,!=ğ‘¡ğ‘“!,! ğ‘¥ logğ‘ ğ‘‘ğ‘“!  

(1) 
where  N  is  the  number  of  documents.  The  weight  is 

assigned by the product of ğ‘¡ğ‘“!,!, the term frequency, 
and logğ‘ ğ‘‘ğ‘“! , the inverse document frequency. For 

in  mind,  we  believe 

Sports 

US 

NEW 
US 
STATE 
TEXAS 
POLICE 
STATES 

NFL 
FIRST 
GAME 
WIN 
OVER 
NEW 
PLAYERS  TUESDAY 
SEASON 
COACH 
METS 
 

OVER 
WEDNESDAY 
COURT 

NEW 
APPLE 
GOOGLE 
SPACE 
FACEBOOK 
US 
ONLINE 
INTERNET 
SONY 
IPAD 

The classifier outperformed one-against-all SVM and 
multi-class SVM. 
Our work deviates from the aforementioned studies in 
that we used only the title and a short description of 
each article for our lexical feature extraction and we 
focused on evaluating all three classifiers rather than 
trying to optimize the performance of a particular one. 
In  addition,  we  are  using  a  variant  of  the  Softmax 
Regression  algorithm  presented  by  Do  and  Ng  to 
perform our text classification. 

 

Figure 1: Distribution of the TagMyNews Data 

 
3.  Dataset and Pre-Processing 
To perform our classification evaluation, we used the 
TagMyNews Dataset [1]. The corpus includes 32,602 
training examples of news articles. Each training 
example has a structure including a title, a description, 
a news article link, an ID, the date of publication, the 
news article source, and a subject category. Of interest 
to us were the articleâ€™s title, the brief article 
description, and the pre-labeled category. Figure 1 
shows the distribution of news articles for each 
category. The majority of the training examples were 
from the Sports category, while we had the fewest 
training examples from the Health category. 
However, as we show in Section 5, the number of 
training examples for each category did not 

 

2 

each category, we computed the TF-IDF of each term 
and obtained the f-most salient words from the sorted 
list of TF-IDF rankings. While the goal of using TF-
IDF was to extract the most meaningful and indicative 
words  for  each  category,  we  needed  to  further  filter 
the results of the algorithm to exclude words such as 
â€˜theâ€™,  â€˜orâ€™,  â€˜himâ€™,  which  carry  no  significance.  Thus, 
we  implemented  a  â€œstop-wordâ€  list  to  remove  these 
meaningless words from our feature set based on [18]. 
Table 1 shows a list of the top 10 words extracted for 
each category using TF-IDF. 
We  also  investigated  a  variation  of  the  TF-IDF 
algorithm called Sublinear TF Scaling [6]: 

ğ‘¤!,!=ğ‘¤ğ‘“!,! ğ‘¥ logğ‘ ğ‘‘ğ‘“!  
where ğ‘¤ğ‘“!,! is given by: 
ğ‘–ğ‘“ ğ‘¡ğ‘“!,! >0
ğ‘¤ğ‘“!,!= 1+logğ‘¡ğ‘“!,! ,
                     0,         ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’  

(2) 

(3) 

than 

for  all 

just  counting 

Each  termâ€™s  frequency  is  now  assigned  a  weight, 
which  may  represent  a  termâ€™s  significance  more 
accurately 
the  number  of 
occurrences.  However,  we  found  the  classic  TF-IDF 
to  have  better  performance  on  all  three  models  over 
this  modified  version  of  Equation  1.  Using  Equation 
2,  the  top  f-words  for  each  category  were  similar  to 
those of Table 1, but not exactly the same. Since the 
classic  TF-IDF  performed  better 
three 
classifiers,  we  did  not  use  Equation  2  in  our  final 
implementation.  
4.2  Implementing the Classifiers 
We  selected  to  evaluate  NaÃ¯ve  Bayes,  SVM,  and 
Softmax  Regression  due  to  their  ability  to  perform 
supervised  learning  on  multi-class  datasets.  In  the 
following  sections,  we  describe  each  algorithm  and 
how it pertains to our goal of news classification. 
4.2.1  Support Vector Machine 
The  SVM  algorithm  requires  the  solution  to  the 
following optimization problem [4]: 

min!,!,! 12 ğ‘¤ !+ğ¶
ğœ‰!!
!!!
ğ‘ .ğ‘¡.    ğ‘¦! ğ‘¤!ğ‘¥! +ğ‘ â‰¥1âˆ’ğœ‰! 
ğœ‰!â‰¥0 
where ğ¶ is a regularization weighting parameter. The 
dimensional  space  that ğ‘¥!  is  mapped  to  [4].  For  our 

goal  of  the  SVM  is  to  find  a  linear-separating 
hyperplane that has the maximal margin in the higher 

project,  we  used  the  LIBSVM  MATLAB  library  to 
implement  our  Support  Vector  Classifier  (SVC)  [2]. 

(4) 

 

 

 

 

3 

we  performed 
implementing 

In order to maximize the performance of the SVM, we 

The  LIBSVM  library  offers  different  options  that 
allow a user to set the SVM-type and the kernel-type, 
as well as values for the different parameters. For our 
SVM, we tested multiple kernel functions, including a 
linear,  polynomial,  and  radial  basis  function  (RBF). 
We found the RBF to have the best performance, and 
subsequently used it for our implementation of the C-

SVC. The RBF kernel is given by ğ‘’!!|!!!!|, where ğ›¾ 
is a weighting parameter and ğ‘¥ is a query point [3]. 
needed to optimize the parameters ğ›¾ and ğ¶. To do so, 
selected exponentially growing ranges for both ğ¶ and 
ğ›¾  (ğ¶=2!!,2!!,â€¦,2!"  and ğ›¾=2!!",2!!",â€¦,2!). 
training data, we iterated through the different ğ›¾ and 
ğ¶  options,  noting  the  values  that  gave  the  highest 
each label value, ğ‘¦={1,2,â€¦,ğ‘˜}, a different SVM 

accuracy during each fold. We did not include any of 
the  test  data  in  the  internal  cross  validation,  since 
optimizing for a given test set would be incorrect. 
LIBSVM implements a one-against-all training model 
for the SVM when using a multi-class dataset. For 

search  by 
internal  cross  validation  [3].  We 

Using  5-fold  internal  cross  validation  on  only  the 

a  parameter  grid 

model is trained. Thus, we make k-different binary 
models. We then test each model on the testing data 
and determine the model from which the highest 
prediction confidence is returned in order to classify 
the data. 
4.2.2  NaÃ¯ve Bayes 
NaÃ¯ve  Bayes  was  used  as  our  baseline  text  classifier 
because it could be quickly implemented for analysis. 
Since  our  news  classification  framework  has  been 
deï¬ned  for  multiple  classes,  we  have  developed  the 
appropriate algorithms for this case as: 

1{ğ‘¥!!
=1 âˆ© ğ‘¦!=1}
ğœ™!|!!!"#$$!=
!!!!
 1{ğ‘¦!=1}
!!!!
 1{ğ‘¦!=1}
ğœ™!!!=
!!!! ğ‘š
interpretation. For example, ğœ™!|!=ğ‘– is the fraction of 
ğ‘(ğ‘¦=ğ‘—|ğ‘¥)=ğ‘(ğ‘¥|ğ‘¦=ğ‘—)ğ‘(ğ‘¦=ğ‘–)
ğ‘(ğ‘¥)

the category i in which word j appears. Having fit all 
the parameters, we calculate: 

these  parameters  have  a  natural 

From 

[16], 

  

  

(5) 

(6) 

(7) 

 

Assessing and Implementing Automated News 

Classification 

 

Department of Electrical Engineering 

Francisco Romero 
Stanford University 
Stanford, California 

faromero@stanford.edu 

Department of Electrical Engineering 

Zahra Koochak 
Stanford University 
Stanford, California 
zahraa@stanford.edu 

 
 

in 

(NLP) 

Abstractâ€”Newsfeed  websites  sort  articles  by 
subject to make it easier for readers to search for 
articles 
their  preferred  category.  When 
uploading a new article, authors are usually tasked 
with  selecting  the  most  pertinent  category  so  the 
new  addition  can  then  be  grouped  with  similar 
articles.  We  are  interested  in  further  developing 
the  framework  to  automatize  the  classification  of 
news articles using machine learning and Natural 
Language  Processing 
techniques.  We 
explore  three  classification  methods:  Support 
Vector Machine (SVM), NaÃ¯ve Bayes, and Softmax 
Regression, and evaluate each classifierâ€™s ability to 
select  the  appropriate  category  given  an  articleâ€™s 
title  and  a  brief  article  description.  Our  results 
show  Softmax  Regression  to  be  the  best  classifier 
among the three we evaluated. 
Index  Termsâ€”news,  articles,  NLP,  SVM,  NaÃ¯ve 
Bayes, Softmax, classification, tf-idf 
1.  Introduction 
When  visiting  a  newsfeed  website,  we  are  often 
interested  in  reading  articles  in  a  specific  category. 
Based on their content, articles are sorted by subject, 
which  allows  readers  to  effortlessly  find  articles  in 
their  preferred  category.  To  determine  the  articleâ€™s 
category,  most  newsfeed  websites  ask  the  author  to 
select the best-fit category for their article. Selecting 
an articleâ€™s category is not only based on the authorâ€™s 
opinion, but can also be tedious when several articles 
are  simultaneously  being  added 
to  a  newsfeed 
website.  Since  the  vocabulary  and  terminology  used 
by  an  articleâ€™s  author  is  indicative  of  the  target 
audience  and,  more  generally,  of 
the  articleâ€™s 
category,  we  believe  this  process  can  be  effectively 
automated. 
For  our  project,  we  are  interested  in  assessing  three 
classification  methods  to  determine  the  feasibility  of 
automatically  classifying  news  articles.  We  selected 
to  use  the  articleâ€™s  title  and  a  1-2  sentence  article 

description  as  the  input  to  our  classifier.  We  then 
evaluate  the  capability  of  our  classifier  using  a 
minimal  amount  of  information  about  the  articleâ€™s 
subject.  Finally,  we  used  three  supervised  learning 
classifier to output a predicted article category: NaÃ¯ve 
Bayes, Support Vector Machine (SVM), and Softmax 
Regression.  Our  data  spans  over  seven  categories: 
Sports,  US,  Science  and  Technology,  Business, 
World,  Entertainment,  and  Health.  Based  on  the 
lexical features of each article, it was the job of each 
classifier  to  select  the  most  appropriate  category  for 
the article. 
2.  Relevant Work 
Previous work has focused on developing algorithms 
and software to automate the process of accurate text 
classification.  Young  and  Jeong  implemented  a  new 
feature  scaling  method  that  uses  the  NaÃ¯ve  Bayes 
classifier. The feature scaling method was tested on a 
news  group  dataset  and  outperformed  other  popular 
ranking  schemes,  such  as  Information  Gain  while 
noting  NaÃ¯ve  Bayes  as  being  a  suitable  classifier  for 
news articles [10]. Wang et al. developed an optimal 
text  categorization  algorithm  that  is  based  on  the 
SVM algorithm used in this paper [11]. Using a news 
article  corpus  similar  to  ours,  they  found  their 
algorithm  to  outperform  other  classifiers  such  as  the 
decision-tree  algorithm  and  the  K-nearest  neighbor 
algorithm.  Hakim  et  al.  evaluated 
the  Term 
Frequency-Inverse  Document  Frequency  (TF-IDF) 
algorithmâ€™s ability to be used in text classification for 
news  articles  in  Bahasa  Indonesia  [13].  However, 
their approach did not focus on any machine learning 
techniques, only on the TF-IDF algorithm. Looking to 
our  future  work,  one  of  the  first  frameworks  for 
Neural  Networks  was  developed  by  Ruiz  and 
Srinivasan  [12].  Using  about  2,350  documents,  they 
showed  the  ability  of  Neural  Networks  to  accurately 
categorize text. Do and Ng explored text classification 
using a modified Softmax Regression algorithm [17]. 

 

1 

Table 1: Lexical Feature Extraction 

Top 10 Words for Each Category 
Science & 
World 
Technology 

Business 

PRESIDENT 

Entertainment 

Health 

US 
BILLION  US 
NEW 
PRICES 
OIL 
BANK 
SALES 
MAY 
STOCKS  BIN 
UP 

NEW 
THEATER 
SHOW 
STAR 
IDOL 

KILLED 
FORCES 
NUCLEAR 
GOVERNMENT  WEDDING 
AMERICAN 
POLICE 
UP 
PEOPLE 
FIRST 
FILM 

LIBYA 

STUDY 
NEW 
HEALTH 
CANCER 
US 
MAY 
DRUG 
RISK 
HEART 
PEOPLE 

necessarily reflect the classifierâ€™s ability to determine 
the category of an article from the testing set. 
Pre-processing the news article data involved three 
steps. First, we separated each articleâ€™s title, 
description, and pre-labeled category into a separate 
text file, since the corpus is formatted into a single 
file. Second, we removed all punctuation from the 
title and description. Third, we capitalized all letters 
in the title and description. The latter two steps are 
necessary for performing a lexical feature extraction 
using the vocabulary of the title and the article 
description. 
4.  Methodology 
To test the three classifiers, we divided our data into 
training  news  articles  and  testing  news  articles.  70% 
of  our  data  (22,819  articles)  were  designated  as  the 
training  articles  and 
the  remaining  30%  (9783 
articles) were designated as the testing articles. 
Since  an  author  writes  an  article  with  an  intended 
category  or  subject 
the 
vocabulary  can  be  used  as  our  features  for  our 
classifiers.  Thus,  our  objective  for  our  feature 
extraction  was  to  obtain  the f-most  salient  words  for 
each category, and count how many times each word 
appeared in a given article. We tested feature sizes of 
f = 40, 50, 70, 150 and 200 to obtain the best accuracy 
possible  for  each  classifier.  To  obtain  the  f-most 
salient words for each category, we used the TF-IDF 
algorithm, which we explain in the next section. The 
extracted  features  were  then  passed  to  each  of  the 
three classifiers. 
4.1  Extracting Salient Words with TF-IDF 
The TF-IDF weighting scheme will assign each term, 
t, a given weight in a document as follows: 

ğ‘¤!,!=ğ‘¡ğ‘“!,! ğ‘¥ logğ‘ ğ‘‘ğ‘“!  

(1) 
where  N  is  the  number  of  documents.  The  weight  is 

assigned by the product of ğ‘¡ğ‘“!,!, the term frequency, 
and logğ‘ ğ‘‘ğ‘“! , the inverse document frequency. For 

in  mind,  we  believe 

Sports 

US 

NEW 
US 
STATE 
TEXAS 
POLICE 
STATES 

NFL 
FIRST 
GAME 
WIN 
OVER 
NEW 
PLAYERS  TUESDAY 
SEASON 
COACH 
METS 
 

OVER 
WEDNESDAY 
COURT 

NEW 
APPLE 
GOOGLE 
SPACE 
FACEBOOK 
US 
ONLINE 
INTERNET 
SONY 
IPAD 

The classifier outperformed one-against-all SVM and 
multi-class SVM. 
Our work deviates from the aforementioned studies in 
that we used only the title and a short description of 
each article for our lexical feature extraction and we 
focused on evaluating all three classifiers rather than 
trying to optimize the performance of a particular one. 
In  addition,  we  are  using  a  variant  of  the  Softmax 
Regression  algorithm  presented  by  Do  and  Ng  to 
perform our text classification. 

 

Figure 1: Distribution of the TagMyNews Data 

 
3.  Dataset and Pre-Processing 
To perform our classification evaluation, we used the 
TagMyNews Dataset [1]. The corpus includes 32,602 
training examples of news articles. Each training 
example has a structure including a title, a description, 
a news article link, an ID, the date of publication, the 
news article source, and a subject category. Of interest 
to us were the articleâ€™s title, the brief article 
description, and the pre-labeled category. Figure 1 
shows the distribution of news articles for each 
category. The majority of the training examples were 
from the Sports category, while we had the fewest 
training examples from the Health category. 
However, as we show in Section 5, the number of 
training examples for each category did not 

 

2 

each category, we computed the TF-IDF of each term 
and obtained the f-most salient words from the sorted 
list of TF-IDF rankings. While the goal of using TF-
IDF was to extract the most meaningful and indicative 
words  for  each  category,  we  needed  to  further  filter 
the results of the algorithm to exclude words such as 
â€˜theâ€™,  â€˜orâ€™,  â€˜himâ€™,  which  carry  no  significance.  Thus, 
we  implemented  a  â€œstop-wordâ€  list  to  remove  these 
meaningless words from our feature set based on [18]. 
Table 1 shows a list of the top 10 words extracted for 
each category using TF-IDF. 
We  also  investigated  a  variation  of  the  TF-IDF 
algorithm called Sublinear TF Scaling [6]: 

ğ‘¤!,!=ğ‘¤ğ‘“!,! ğ‘¥ logğ‘ ğ‘‘ğ‘“!  
where ğ‘¤ğ‘“!,! is given by: 
ğ‘–ğ‘“ ğ‘¡ğ‘“!,! >0
ğ‘¤ğ‘“!,!= 1+logğ‘¡ğ‘“!,! ,
                     0,         ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’  

(2) 

(3) 

than 

for  all 

just  counting 

Each  termâ€™s  frequency  is  now  assigned  a  weight, 
which  may  represent  a  termâ€™s  significance  more 
accurately 
the  number  of 
occurrences.  However,  we  found  the  classic  TF-IDF 
to  have  better  performance  on  all  three  models  over 
this  modified  version  of  Equation  1.  Using  Equation 
2,  the  top  f-words  for  each  category  were  similar  to 
those of Table 1, but not exactly the same. Since the 
classic  TF-IDF  performed  better 
three 
classifiers,  we  did  not  use  Equation  2  in  our  final 
implementation.  
4.2  Implementing the Classifiers 
We  selected  to  evaluate  NaÃ¯ve  Bayes,  SVM,  and 
Softmax  Regression  due  to  their  ability  to  perform 
supervised  learning  on  multi-class  datasets.  In  the 
following  sections,  we  describe  each  algorithm  and 
how it pertains to our goal of news classification. 
4.2.1  Support Vector Machine 
The  SVM  algorithm  requires  the  solution  to  the 
following optimization problem [4]: 

min!,!,! 12 ğ‘¤ !+ğ¶
ğœ‰!!
!!!
ğ‘ .ğ‘¡.    ğ‘¦! ğ‘¤!ğ‘¥! +ğ‘ â‰¥1âˆ’ğœ‰! 
ğœ‰!â‰¥0 
where ğ¶ is a regularization weighting parameter. The 
dimensional  space  that ğ‘¥!  is  mapped  to  [4].  For  our 

goal  of  the  SVM  is  to  find  a  linear-separating 
hyperplane that has the maximal margin in the higher 

project,  we  used  the  LIBSVM  MATLAB  library  to 
implement  our  Support  Vector  Classifier  (SVC)  [2]. 

(4) 

 

 

 

 

3 

we  performed 
implementing 

In order to maximize the performance of the SVM, we 

The  LIBSVM  library  offers  different  options  that 
allow a user to set the SVM-type and the kernel-type, 
as well as values for the different parameters. For our 
SVM, we tested multiple kernel functions, including a 
linear,  polynomial,  and  radial  basis  function  (RBF). 
We found the RBF to have the best performance, and 
subsequently used it for our implementation of the C-

SVC. The RBF kernel is given by ğ‘’!!|!!!!|, where ğ›¾ 
is a weighting parameter and ğ‘¥ is a query point [3]. 
needed to optimize the parameters ğ›¾ and ğ¶. To do so, 
selected exponentially growing ranges for both ğ¶ and 
ğ›¾  (ğ¶=2!!,2!!,â€¦,2!"  and ğ›¾=2!!",2!!",â€¦,2!). 
training data, we iterated through the different ğ›¾ and 
ğ¶  options,  noting  the  values  that  gave  the  highest 
each label value, ğ‘¦={1,2,â€¦,ğ‘˜}, a different SVM 

accuracy during each fold. We did not include any of 
the  test  data  in  the  internal  cross  validation,  since 
optimizing for a given test set would be incorrect. 
LIBSVM implements a one-against-all training model 
for the SVM when using a multi-class dataset. For 

search  by 
internal  cross  validation  [3].  We 

Using  5-fold  internal  cross  validation  on  only  the 

a  parameter  grid 

model is trained. Thus, we make k-different binary 
models. We then test each model on the testing data 
and determine the model from which the highest 
prediction confidence is returned in order to classify 
the data. 
4.2.2  NaÃ¯ve Bayes 
NaÃ¯ve  Bayes  was  used  as  our  baseline  text  classifier 
because it could be quickly implemented for analysis. 
Since  our  news  classification  framework  has  been 
deï¬ned  for  multiple  classes,  we  have  developed  the 
appropriate algorithms for this case as: 

1{ğ‘¥!!
=1 âˆ© ğ‘¦!=1}
ğœ™!|!!!"#$$!=
!!!!
 1{ğ‘¦!=1}
!!!!
 1{ğ‘¦!=1}
ğœ™!!!=
!!!! ğ‘š
interpretation. For example, ğœ™!|!=ğ‘– is the fraction of 
ğ‘(ğ‘¦=ğ‘—|ğ‘¥)=ğ‘(ğ‘¥|ğ‘¦=ğ‘—)ğ‘(ğ‘¦=ğ‘–)
ğ‘(ğ‘¥)

the category i in which word j appears. Having fit all 
the parameters, we calculate: 

these  parameters  have  a  natural 

From 

[16], 

  

  

(5) 

(6) 

(7) 

 

 

 

 

Figure 2: NaÃ¯ve Bayes Performance Measurements 
 

Figure 3: SVM Performance Measurements 
 
  
Since we are applying the classifier over a large 
vocabulary, we implemented Laplace Smoothing to 

avoid having ğœ™!â€²ğ‘  end up as zeros. 

 

 

Figure 4: Softmax Performance Measurements 

 

 

 
Figure 5: Average Performance Measurements for each 
Classifier 
 

4.2.3  Softmax Regression 
We  selected  to  use  Softmax  Regression,  also  known 
as the Multinomial Logistic Regression, as opposed to 
7-binary  classifiers  because  our  seven  classes  are 
mutually exclusive (i.e. a news article will be a part of 
at  most  one  category).  For  this  classifier,  the  class 

probabilities, ğ‘(ğ‘¦|ğ‘¥) are modeled as: 
= ğ‘’!!! !!ğ‘’!!! !!
ğ‘ğ‘¦ğ‘¥ = 
!!!!
where the ğœƒ parameters are learned from the training 

ğ‘’!!ğ‘’!!
!!!!

  

(8) 

set  by  maximizing  the  conditional  log  likelihood  of 
the data [16]. In this approach, a total of k-parameters 
are trained jointly using numerical optimization. 
5.  Results and Discussion 
The weighted accuracy of each classifier is presented 
in Figure 6. Softmax Regression achieved the best 

 

4 

Figure 6: Classifier Accuracy as Feature Size Varies  
performance  out  of  the  three  classifiers,  with  a 
maximum accuracy of 75.36% at 200 features. NaÃ¯ve 
Bayes  performed  significantly  worse  than  the  other 
two,  achieving  a  maximum  accuracy  of  56.12%  for 
150 features, which we attribute to its simplicity and 
weak  scalability.  SVMâ€™s  best  performance  was  not 
much  worse 
than  Softmax  Regression,  with  a 
maximum accuracy of 69.21% for 150 features. Both 

Assessing and Implementing Automated News 

Classification 

 

Department of Electrical Engineering 

Francisco Romero 
Stanford University 
Stanford, California 

faromero@stanford.edu 

Department of Electrical Engineering 

Zahra Koochak 
Stanford University 
Stanford, California 
zahraa@stanford.edu 

 
 

in 

(NLP) 

Abstractâ€”Newsfeed  websites  sort  articles  by 
subject to make it easier for readers to search for 
articles 
their  preferred  category.  When 
uploading a new article, authors are usually tasked 
with  selecting  the  most  pertinent  category  so  the 
new  addition  can  then  be  grouped  with  similar 
articles.  We  are  interested  in  further  developing 
the  framework  to  automatize  the  classification  of 
news articles using machine learning and Natural 
Language  Processing 
techniques.  We 
explore  three  classification  methods:  Support 
Vector Machine (SVM), NaÃ¯ve Bayes, and Softmax 
Regression, and evaluate each classifierâ€™s ability to 
select  the  appropriate  category  given  an  articleâ€™s 
title  and  a  brief  article  description.  Our  results 
show  Softmax  Regression  to  be  the  best  classifier 
among the three we evaluated. 
Index  Termsâ€”news,  articles,  NLP,  SVM,  NaÃ¯ve 
Bayes, Softmax, classification, tf-idf 
1.  Introduction 
When  visiting  a  newsfeed  website,  we  are  often 
interested  in  reading  articles  in  a  specific  category. 
Based on their content, articles are sorted by subject, 
which  allows  readers  to  effortlessly  find  articles  in 
their  preferred  category.  To  determine  the  articleâ€™s 
category,  most  newsfeed  websites  ask  the  author  to 
select the best-fit category for their article. Selecting 
an articleâ€™s category is not only based on the authorâ€™s 
opinion, but can also be tedious when several articles 
are  simultaneously  being  added 
to  a  newsfeed 
website.  Since  the  vocabulary  and  terminology  used 
by  an  articleâ€™s  author  is  indicative  of  the  target 
audience  and,  more  generally,  of 
the  articleâ€™s 
category,  we  believe  this  process  can  be  effectively 
automated. 
For  our  project,  we  are  interested  in  assessing  three 
classification  methods  to  determine  the  feasibility  of 
automatically  classifying  news  articles.  We  selected 
to  use  the  articleâ€™s  title  and  a  1-2  sentence  article 

description  as  the  input  to  our  classifier.  We  then 
evaluate  the  capability  of  our  classifier  using  a 
minimal  amount  of  information  about  the  articleâ€™s 
subject.  Finally,  we  used  three  supervised  learning 
classifier to output a predicted article category: NaÃ¯ve 
Bayes, Support Vector Machine (SVM), and Softmax 
Regression.  Our  data  spans  over  seven  categories: 
Sports,  US,  Science  and  Technology,  Business, 
World,  Entertainment,  and  Health.  Based  on  the 
lexical features of each article, it was the job of each 
classifier  to  select  the  most  appropriate  category  for 
the article. 
2.  Relevant Work 
Previous work has focused on developing algorithms 
and software to automate the process of accurate text 
classification.  Young  and  Jeong  implemented  a  new 
feature  scaling  method  that  uses  the  NaÃ¯ve  Bayes 
classifier. The feature scaling method was tested on a 
news  group  dataset  and  outperformed  other  popular 
ranking  schemes,  such  as  Information  Gain  while 
noting  NaÃ¯ve  Bayes  as  being  a  suitable  classifier  for 
news articles [10]. Wang et al. developed an optimal 
text  categorization  algorithm  that  is  based  on  the 
SVM algorithm used in this paper [11]. Using a news 
article  corpus  similar  to  ours,  they  found  their 
algorithm  to  outperform  other  classifiers  such  as  the 
decision-tree  algorithm  and  the  K-nearest  neighbor 
algorithm.  Hakim  et  al.  evaluated 
the  Term 
Frequency-Inverse  Document  Frequency  (TF-IDF) 
algorithmâ€™s ability to be used in text classification for 
news  articles  in  Bahasa  Indonesia  [13].  However, 
their approach did not focus on any machine learning 
techniques, only on the TF-IDF algorithm. Looking to 
our  future  work,  one  of  the  first  frameworks  for 
Neural  Networks  was  developed  by  Ruiz  and 
Srinivasan  [12].  Using  about  2,350  documents,  they 
showed  the  ability  of  Neural  Networks  to  accurately 
categorize text. Do and Ng explored text classification 
using a modified Softmax Regression algorithm [17]. 

 

1 

Table 1: Lexical Feature Extraction 

Top 10 Words for Each Category 
Science & 
World 
Technology 

Business 

PRESIDENT 

Entertainment 

Health 

US 
BILLION  US 
NEW 
PRICES 
OIL 
BANK 
SALES 
MAY 
STOCKS  BIN 
UP 

NEW 
THEATER 
SHOW 
STAR 
IDOL 

KILLED 
FORCES 
NUCLEAR 
GOVERNMENT  WEDDING 
AMERICAN 
POLICE 
UP 
PEOPLE 
FIRST 
FILM 

LIBYA 

STUDY 
NEW 
HEALTH 
CANCER 
US 
MAY 
DRUG 
RISK 
HEART 
PEOPLE 

necessarily reflect the classifierâ€™s ability to determine 
the category of an article from the testing set. 
Pre-processing the news article data involved three 
steps. First, we separated each articleâ€™s title, 
description, and pre-labeled category into a separate 
text file, since the corpus is formatted into a single 
file. Second, we removed all punctuation from the 
title and description. Third, we capitalized all letters 
in the title and description. The latter two steps are 
necessary for performing a lexical feature extraction 
using the vocabulary of the title and the article 
description. 
4.  Methodology 
To test the three classifiers, we divided our data into 
training  news  articles  and  testing  news  articles.  70% 
of  our  data  (22,819  articles)  were  designated  as  the 
training  articles  and 
the  remaining  30%  (9783 
articles) were designated as the testing articles. 
Since  an  author  writes  an  article  with  an  intended 
category  or  subject 
the 
vocabulary  can  be  used  as  our  features  for  our 
classifiers.  Thus,  our  objective  for  our  feature 
extraction  was  to  obtain  the f-most  salient  words  for 
each category, and count how many times each word 
appeared in a given article. We tested feature sizes of 
f = 40, 50, 70, 150 and 200 to obtain the best accuracy 
possible  for  each  classifier.  To  obtain  the  f-most 
salient words for each category, we used the TF-IDF 
algorithm, which we explain in the next section. The 
extracted  features  were  then  passed  to  each  of  the 
three classifiers. 
4.1  Extracting Salient Words with TF-IDF 
The TF-IDF weighting scheme will assign each term, 
t, a given weight in a document as follows: 

ğ‘¤!,!=ğ‘¡ğ‘“!,! ğ‘¥ logğ‘ ğ‘‘ğ‘“!  

(1) 
where  N  is  the  number  of  documents.  The  weight  is 

assigned by the product of ğ‘¡ğ‘“!,!, the term frequency, 
and logğ‘ ğ‘‘ğ‘“! , the inverse document frequency. For 

in  mind,  we  believe 

Sports 

US 

NEW 
US 
STATE 
TEXAS 
POLICE 
STATES 

NFL 
FIRST 
GAME 
WIN 
OVER 
NEW 
PLAYERS  TUESDAY 
SEASON 
COACH 
METS 
 

OVER 
WEDNESDAY 
COURT 

NEW 
APPLE 
GOOGLE 
SPACE 
FACEBOOK 
US 
ONLINE 
INTERNET 
SONY 
IPAD 

The classifier outperformed one-against-all SVM and 
multi-class SVM. 
Our work deviates from the aforementioned studies in 
that we used only the title and a short description of 
each article for our lexical feature extraction and we 
focused on evaluating all three classifiers rather than 
trying to optimize the performance of a particular one. 
In  addition,  we  are  using  a  variant  of  the  Softmax 
Regression  algorithm  presented  by  Do  and  Ng  to 
perform our text classification. 

 

Figure 1: Distribution of the TagMyNews Data 

 
3.  Dataset and Pre-Processing 
To perform our classification evaluation, we used the 
TagMyNews Dataset [1]. The corpus includes 32,602 
training examples of news articles. Each training 
example has a structure including a title, a description, 
a news article link, an ID, the date of publication, the 
news article source, and a subject category. Of interest 
to us were the articleâ€™s title, the brief article 
description, and the pre-labeled category. Figure 1 
shows the distribution of news articles for each 
category. The majority of the training examples were 
from the Sports category, while we had the fewest 
training examples from the Health category. 
However, as we show in Section 5, the number of 
training examples for each category did not 

 

2 

each category, we computed the TF-IDF of each term 
and obtained the f-most salient words from the sorted 
list of TF-IDF rankings. While the goal of using TF-
IDF was to extract the most meaningful and indicative 
words  for  each  category,  we  needed  to  further  filter 
the results of the algorithm to exclude words such as 
â€˜theâ€™,  â€˜orâ€™,  â€˜himâ€™,  which  carry  no  significance.  Thus, 
we  implemented  a  â€œstop-wordâ€  list  to  remove  these 
meaningless words from our feature set based on [18]. 
Table 1 shows a list of the top 10 words extracted for 
each category using TF-IDF. 
We  also  investigated  a  variation  of  the  TF-IDF 
algorithm called Sublinear TF Scaling [6]: 

ğ‘¤!,!=ğ‘¤ğ‘“!,! ğ‘¥ logğ‘ ğ‘‘ğ‘“!  
where ğ‘¤ğ‘“!,! is given by: 
ğ‘–ğ‘“ ğ‘¡ğ‘“!,! >0
ğ‘¤ğ‘“!,!= 1+logğ‘¡ğ‘“!,! ,
                     0,         ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’  

(2) 

(3) 

than 

for  all 

just  counting 

Each  termâ€™s  frequency  is  now  assigned  a  weight, 
which  may  represent  a  termâ€™s  significance  more 
accurately 
the  number  of 
occurrences.  However,  we  found  the  classic  TF-IDF 
to  have  better  performance  on  all  three  models  over 
this  modified  version  of  Equation  1.  Using  Equation 
2,  the  top  f-words  for  each  category  were  similar  to 
those of Table 1, but not exactly the same. Since the 
classic  TF-IDF  performed  better 
three 
classifiers,  we  did  not  use  Equation  2  in  our  final 
implementation.  
4.2  Implementing the Classifiers 
We  selected  to  evaluate  NaÃ¯ve  Bayes,  SVM,  and 
Softmax  Regression  due  to  their  ability  to  perform 
supervised  learning  on  multi-class  datasets.  In  the 
following  sections,  we  describe  each  algorithm  and 
how it pertains to our goal of news classification. 
4.2.1  Support Vector Machine 
The  SVM  algorithm  requires  the  solution  to  the 
following optimization problem [4]: 

min!,!,! 12 ğ‘¤ !+ğ¶
ğœ‰!!
!!!
ğ‘ .ğ‘¡.    ğ‘¦! ğ‘¤!ğ‘¥! +ğ‘ â‰¥1âˆ’ğœ‰! 
ğœ‰!â‰¥0 
where ğ¶ is a regularization weighting parameter. The 
dimensional  space  that ğ‘¥!  is  mapped  to  [4].  For  our 

goal  of  the  SVM  is  to  find  a  linear-separating 
hyperplane that has the maximal margin in the higher 

project,  we  used  the  LIBSVM  MATLAB  library  to 
implement  our  Support  Vector  Classifier  (SVC)  [2]. 

(4) 

 

 

 

 

3 

we  performed 
implementing 

In order to maximize the performance of the SVM, we 

The  LIBSVM  library  offers  different  options  that 
allow a user to set the SVM-type and the kernel-type, 
as well as values for the different parameters. For our 
SVM, we tested multiple kernel functions, including a 
linear,  polynomial,  and  radial  basis  function  (RBF). 
We found the RBF to have the best performance, and 
subsequently used it for our implementation of the C-

SVC. The RBF kernel is given by ğ‘’!!|!!!!|, where ğ›¾ 
is a weighting parameter and ğ‘¥ is a query point [3]. 
needed to optimize the parameters ğ›¾ and ğ¶. To do so, 
selected exponentially growing ranges for both ğ¶ and 
ğ›¾  (ğ¶=2!!,2!!,â€¦,2!"  and ğ›¾=2!!",2!!",â€¦,2!). 
training data, we iterated through the different ğ›¾ and 
ğ¶  options,  noting  the  values  that  gave  the  highest 
each label value, ğ‘¦={1,2,â€¦,ğ‘˜}, a different SVM 

accuracy during each fold. We did not include any of 
the  test  data  in  the  internal  cross  validation,  since 
optimizing for a given test set would be incorrect. 
LIBSVM implements a one-against-all training model 
for the SVM when using a multi-class dataset. For 

search  by 
internal  cross  validation  [3].  We 

Using  5-fold  internal  cross  validation  on  only  the 

a  parameter  grid 

model is trained. Thus, we make k-different binary 
models. We then test each model on the testing data 
and determine the model from which the highest 
prediction confidence is returned in order to classify 
the data. 
4.2.2  NaÃ¯ve Bayes 
NaÃ¯ve  Bayes  was  used  as  our  baseline  text  classifier 
because it could be quickly implemented for analysis. 
Since  our  news  classification  framework  has  been 
deï¬ned  for  multiple  classes,  we  have  developed  the 
appropriate algorithms for this case as: 

1{ğ‘¥!!
=1 âˆ© ğ‘¦!=1}
ğœ™!|!!!"#$$!=
!!!!
 1{ğ‘¦!=1}
!!!!
 1{ğ‘¦!=1}
ğœ™!!!=
!!!! ğ‘š
interpretation. For example, ğœ™!|!=ğ‘– is the fraction of 
ğ‘(ğ‘¦=ğ‘—|ğ‘¥)=ğ‘(ğ‘¥|ğ‘¦=ğ‘—)ğ‘(ğ‘¦=ğ‘–)
ğ‘(ğ‘¥)

the category i in which word j appears. Having fit all 
the parameters, we calculate: 

these  parameters  have  a  natural 

From 

[16], 

  

  

(5) 

(6) 

(7) 

 

 

 

 

Figure 2: NaÃ¯ve Bayes Performance Measurements 
 

Figure 3: SVM Performance Measurements 
 
  
Since we are applying the classifier over a large 
vocabulary, we implemented Laplace Smoothing to 

avoid having ğœ™!â€²ğ‘  end up as zeros. 

 

 

Figure 4: Softmax Performance Measurements 

 

 

 
Figure 5: Average Performance Measurements for each 
Classifier 
 

4.2.3  Softmax Regression 
We  selected  to  use  Softmax  Regression,  also  known 
as the Multinomial Logistic Regression, as opposed to 
7-binary  classifiers  because  our  seven  classes  are 
mutually exclusive (i.e. a news article will be a part of 
at  most  one  category).  For  this  classifier,  the  class 

probabilities, ğ‘(ğ‘¦|ğ‘¥) are modeled as: 
= ğ‘’!!! !!ğ‘’!!! !!
ğ‘ğ‘¦ğ‘¥ = 
!!!!
where the ğœƒ parameters are learned from the training 

ğ‘’!!ğ‘’!!
!!!!

  

(8) 

set  by  maximizing  the  conditional  log  likelihood  of 
the data [16]. In this approach, a total of k-parameters 
are trained jointly using numerical optimization. 
5.  Results and Discussion 
The weighted accuracy of each classifier is presented 
in Figure 6. Softmax Regression achieved the best 

 

4 

Figure 6: Classifier Accuracy as Feature Size Varies  
performance  out  of  the  three  classifiers,  with  a 
maximum accuracy of 75.36% at 200 features. NaÃ¯ve 
Bayes  performed  significantly  worse  than  the  other 
two,  achieving  a  maximum  accuracy  of  56.12%  for 
150 features, which we attribute to its simplicity and 
weak  scalability.  SVMâ€™s  best  performance  was  not 
much  worse 
than  Softmax  Regression,  with  a 
maximum accuracy of 69.21% for 150 features. Both 

NaÃ¯ve  Bayes  and  SVM  ran  into  overfitting  issues  at 
200 features (i.e. performance began to decrease). We 
attribute the performance drop to the minimal overlap 
in  words  between  the  different  categories,  especially 
as the feature size increased. However, the optimized 
SVM performed much better than the non-optimized 
SVM,  which  ran  into  overfitting  issues  after  70 
features  and  had  a  maximum  accuracy  of  61.37%. 
Since Softmax Regression did not run into overfitting 
issues  for  our  evaluationâ€™s  maximum  feature  size, 
future work will continue to increase the feature size 
and further evaluate the performance of the classifier. 
To  further  evaluate  each  classifierâ€™s  performance  on 
individual  classes,  we  used  the  three  measurements 
derived from the confusion matrix: Precision, Recall, 
and  F1-score.  The  latter  three  measurements  reflect 
the importance of retrieval of positive examples in our 
text  classification 
the  class 
agreement  of  the  data  labels  with  the  positive  labels 
given  by 
the 
effectiveness of a classifier to identify positive labels 
[14]. The F1-score is the harmonic mean of precision 
and recall and is given by: 

the  classifier,  while  Recall 

ğ¹!=2âˆ—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›âˆ—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™

 

(9) 

is 

[14].  Precision 

is 

the  highest  Precision 

and  Recall, 
the  highest  F1-score  for  all 

Figures  2-4  show  each  classifierâ€™s  Precision,  Recall, 
and  F1-Score  for  each  category.  The  Sports  category 
had 
and 
subsequently 
three 
classifiers.  This  is  due  to  the  esoteric  and  overly 
specific  terms  used  in  the  Sports  categories  lexical 
features, as seen in Table 1. For Softmax Regression 
and  SVM,  the  Science  and  Technology category  had 
the  lowest  F1-score,  while  for  NaÃ¯ve  Bayes,  Health 
had the lowest F1-score. While the plots of these three 
metrics  for  SVM  and  Softmax  Regression  strongly 
resembled  each  other,  NaÃ¯ve  Bayes  had  Precision 
higher than Recall for all categories except for Health. 
The  balance  between  Precision  and  Recall  for  SVM 
and  Softmax  Regression  is  visible  in  Figure  5.  The 
average  Precision  and  Recall  are  almost  identical, 
demonstrating  the  two  classifiers  ability  to  perform 
better on a per-category-basis, especially for Softmax 
Regression. 
The low F1-scores of both the Health and Science and 
Technology  categories  are  due  to  the  inability  of  the 
classifier to distinguish one category from another. In 
particular,  for  Science  and  Technology,  the  majority 
the  Business 
of 
category. This is most likely attributed to technology 

the  false-negatives  came  from 

 

5 

lead 

often being mentioned in a commercial setting. Even 
more interesting, the number of false-positives for the 
Health  category  according  to  NaÃ¯ve  Bayes  is  very 
high (evident by the very low Precision in Figure 2). 
Using  NaÃ¯ve  Bayes,  the  majority  of  the  false-
negatives for every category was Health, which is the 
source of the low weighted accuracy. 
6.  Future Work 
Although  the  Sublinear  TF  Scaling  modification  to 
the  TF-IDF  algorithm  did  not  outperform  the  classic 
TF-IDF,  we  would  like  to  look  into  more  TF-IDF 
variants  and  other  methods  to  improve  the  feature 
selection  process.  Other  methods  might 
include 
Information  Gain  or  Conditional  Mutual  Information 
[5].  To  explore  the  optimal  feature  size,  we  might 
want to try a Forward or Backward search procedure 
with  a  reduced  dataset.  We  are  also  interested  in 
testing other classification methods, such as Recurrent 
Neural  Networks,  to  compare  them  against  the  three 
implemented  in  this  work.  In  particular,  classifying 
articles 
into  more  specific  categories,  such  as 
Computing  instead  of  just  Science  and  Technology, 
may 
to  classifier  performance  differences 
compared to the results from this work. 
Beyond trying to classify news articles into a specific 
category, we would like to explore the application of 
our developed framework towards detecting emotions 
or  bias  in  a  news  article.  Work  has  been  done  to 
explore  using  SVMs  and  semi-supervised  learning 
models  for  political  bias  by  [7].  In  addition,  [8,9] 
investigated  emotional  classification  from  both  the 
writerâ€™s  and 
readerâ€™s  perspective  using  SVMs. 
Effectively, we want to see how well our framework 
generalizes  to  other  underlying  aspects  of  news 
articles. 
7.  Conclusion 
Based on the F1-Scores in Figure 5, our best classifier, 
Softmax  Regression,  performed  12.57%  worse  than 
the  proposed  methods  of  Wang  et  al.  for  a  similar 
dataset.  Nevertheless,    we  have  shown  the  ability  of 
three  different  classifiers  to  automatically  classify 
news articles into their subject category. NaÃ¯ve Bayesâ€™ 
performance  was  adversely  affected  by  the  high 
number  of  false-negatives  from  the  Health  category. 
The well-known SVM algorithm, although not the top 
performer,  was  also  found  to  be  highly  suitable  for 
classifying news article into their subject category. 
8.  REFERENCES 
[1]  A3 Lab. TagMyNews Dataset. 

http://acube.di.unipi.it/tmn-dataset/ 

Assessing and Implementing Automated News 

Classification 

 

Department of Electrical Engineering 

Francisco Romero 
Stanford University 
Stanford, California 

faromero@stanford.edu 

Department of Electrical Engineering 

Zahra Koochak 
Stanford University 
Stanford, California 
zahraa@stanford.edu 

 
 

in 

(NLP) 

Abstractâ€”Newsfeed  websites  sort  articles  by 
subject to make it easier for readers to search for 
articles 
their  preferred  category.  When 
uploading a new article, authors are usually tasked 
with  selecting  the  most  pertinent  category  so  the 
new  addition  can  then  be  grouped  with  similar 
articles.  We  are  interested  in  further  developing 
the  framework  to  automatize  the  classification  of 
news articles using machine learning and Natural 
Language  Processing 
techniques.  We 
explore  three  classification  methods:  Support 
Vector Machine (SVM), NaÃ¯ve Bayes, and Softmax 
Regression, and evaluate each classifierâ€™s ability to 
select  the  appropriate  category  given  an  articleâ€™s 
title  and  a  brief  article  description.  Our  results 
show  Softmax  Regression  to  be  the  best  classifier 
among the three we evaluated. 
Index  Termsâ€”news,  articles,  NLP,  SVM,  NaÃ¯ve 
Bayes, Softmax, classification, tf-idf 
1.  Introduction 
When  visiting  a  newsfeed  website,  we  are  often 
interested  in  reading  articles  in  a  specific  category. 
Based on their content, articles are sorted by subject, 
which  allows  readers  to  effortlessly  find  articles  in 
their  preferred  category.  To  determine  the  articleâ€™s 
category,  most  newsfeed  websites  ask  the  author  to 
select the best-fit category for their article. Selecting 
an articleâ€™s category is not only based on the authorâ€™s 
opinion, but can also be tedious when several articles 
are  simultaneously  being  added 
to  a  newsfeed 
website.  Since  the  vocabulary  and  terminology  used 
by  an  articleâ€™s  author  is  indicative  of  the  target 
audience  and,  more  generally,  of 
the  articleâ€™s 
category,  we  believe  this  process  can  be  effectively 
automated. 
For  our  project,  we  are  interested  in  assessing  three 
classification  methods  to  determine  the  feasibility  of 
automatically  classifying  news  articles.  We  selected 
to  use  the  articleâ€™s  title  and  a  1-2  sentence  article 

description  as  the  input  to  our  classifier.  We  then 
evaluate  the  capability  of  our  classifier  using  a 
minimal  amount  of  information  about  the  articleâ€™s 
subject.  Finally,  we  used  three  supervised  learning 
classifier to output a predicted article category: NaÃ¯ve 
Bayes, Support Vector Machine (SVM), and Softmax 
Regression.  Our  data  spans  over  seven  categories: 
Sports,  US,  Science  and  Technology,  Business, 
World,  Entertainment,  and  Health.  Based  on  the 
lexical features of each article, it was the job of each 
classifier  to  select  the  most  appropriate  category  for 
the article. 
2.  Relevant Work 
Previous work has focused on developing algorithms 
and software to automate the process of accurate text 
classification.  Young  and  Jeong  implemented  a  new 
feature  scaling  method  that  uses  the  NaÃ¯ve  Bayes 
classifier. The feature scaling method was tested on a 
news  group  dataset  and  outperformed  other  popular 
ranking  schemes,  such  as  Information  Gain  while 
noting  NaÃ¯ve  Bayes  as  being  a  suitable  classifier  for 
news articles [10]. Wang et al. developed an optimal 
text  categorization  algorithm  that  is  based  on  the 
SVM algorithm used in this paper [11]. Using a news 
article  corpus  similar  to  ours,  they  found  their 
algorithm  to  outperform  other  classifiers  such  as  the 
decision-tree  algorithm  and  the  K-nearest  neighbor 
algorithm.  Hakim  et  al.  evaluated 
the  Term 
Frequency-Inverse  Document  Frequency  (TF-IDF) 
algorithmâ€™s ability to be used in text classification for 
news  articles  in  Bahasa  Indonesia  [13].  However, 
their approach did not focus on any machine learning 
techniques, only on the TF-IDF algorithm. Looking to 
our  future  work,  one  of  the  first  frameworks  for 
Neural  Networks  was  developed  by  Ruiz  and 
Srinivasan  [12].  Using  about  2,350  documents,  they 
showed  the  ability  of  Neural  Networks  to  accurately 
categorize text. Do and Ng explored text classification 
using a modified Softmax Regression algorithm [17]. 

 

1 

Table 1: Lexical Feature Extraction 

Top 10 Words for Each Category 
Science & 
World 
Technology 

Business 

PRESIDENT 

Entertainment 

Health 

US 
BILLION  US 
NEW 
PRICES 
OIL 
BANK 
SALES 
MAY 
STOCKS  BIN 
UP 

NEW 
THEATER 
SHOW 
STAR 
IDOL 

KILLED 
FORCES 
NUCLEAR 
GOVERNMENT  WEDDING 
AMERICAN 
POLICE 
UP 
PEOPLE 
FIRST 
FILM 

LIBYA 

STUDY 
NEW 
HEALTH 
CANCER 
US 
MAY 
DRUG 
RISK 
HEART 
PEOPLE 

necessarily reflect the classifierâ€™s ability to determine 
the category of an article from the testing set. 
Pre-processing the news article data involved three 
steps. First, we separated each articleâ€™s title, 
description, and pre-labeled category into a separate 
text file, since the corpus is formatted into a single 
file. Second, we removed all punctuation from the 
title and description. Third, we capitalized all letters 
in the title and description. The latter two steps are 
necessary for performing a lexical feature extraction 
using the vocabulary of the title and the article 
description. 
4.  Methodology 
To test the three classifiers, we divided our data into 
training  news  articles  and  testing  news  articles.  70% 
of  our  data  (22,819  articles)  were  designated  as  the 
training  articles  and 
the  remaining  30%  (9783 
articles) were designated as the testing articles. 
Since  an  author  writes  an  article  with  an  intended 
category  or  subject 
the 
vocabulary  can  be  used  as  our  features  for  our 
classifiers.  Thus,  our  objective  for  our  feature 
extraction  was  to  obtain  the f-most  salient  words  for 
each category, and count how many times each word 
appeared in a given article. We tested feature sizes of 
f = 40, 50, 70, 150 and 200 to obtain the best accuracy 
possible  for  each  classifier.  To  obtain  the  f-most 
salient words for each category, we used the TF-IDF 
algorithm, which we explain in the next section. The 
extracted  features  were  then  passed  to  each  of  the 
three classifiers. 
4.1  Extracting Salient Words with TF-IDF 
The TF-IDF weighting scheme will assign each term, 
t, a given weight in a document as follows: 

ğ‘¤!,!=ğ‘¡ğ‘“!,! ğ‘¥ logğ‘ ğ‘‘ğ‘“!  

(1) 
where  N  is  the  number  of  documents.  The  weight  is 

assigned by the product of ğ‘¡ğ‘“!,!, the term frequency, 
and logğ‘ ğ‘‘ğ‘“! , the inverse document frequency. For 

in  mind,  we  believe 

Sports 

US 

NEW 
US 
STATE 
TEXAS 
POLICE 
STATES 

NFL 
FIRST 
GAME 
WIN 
OVER 
NEW 
PLAYERS  TUESDAY 
SEASON 
COACH 
METS 
 

OVER 
WEDNESDAY 
COURT 

NEW 
APPLE 
GOOGLE 
SPACE 
FACEBOOK 
US 
ONLINE 
INTERNET 
SONY 
IPAD 

The classifier outperformed one-against-all SVM and 
multi-class SVM. 
Our work deviates from the aforementioned studies in 
that we used only the title and a short description of 
each article for our lexical feature extraction and we 
focused on evaluating all three classifiers rather than 
trying to optimize the performance of a particular one. 
In  addition,  we  are  using  a  variant  of  the  Softmax 
Regression  algorithm  presented  by  Do  and  Ng  to 
perform our text classification. 

 

Figure 1: Distribution of the TagMyNews Data 

 
3.  Dataset and Pre-Processing 
To perform our classification evaluation, we used the 
TagMyNews Dataset [1]. The corpus includes 32,602 
training examples of news articles. Each training 
example has a structure including a title, a description, 
a news article link, an ID, the date of publication, the 
news article source, and a subject category. Of interest 
to us were the articleâ€™s title, the brief article 
description, and the pre-labeled category. Figure 1 
shows the distribution of news articles for each 
category. The majority of the training examples were 
from the Sports category, while we had the fewest 
training examples from the Health category. 
However, as we show in Section 5, the number of 
training examples for each category did not 

 

2 

each category, we computed the TF-IDF of each term 
and obtained the f-most salient words from the sorted 
list of TF-IDF rankings. While the goal of using TF-
IDF was to extract the most meaningful and indicative 
words  for  each  category,  we  needed  to  further  filter 
the results of the algorithm to exclude words such as 
â€˜theâ€™,  â€˜orâ€™,  â€˜himâ€™,  which  carry  no  significance.  Thus, 
we  implemented  a  â€œstop-wordâ€  list  to  remove  these 
meaningless words from our feature set based on [18]. 
Table 1 shows a list of the top 10 words extracted for 
each category using TF-IDF. 
We  also  investigated  a  variation  of  the  TF-IDF 
algorithm called Sublinear TF Scaling [6]: 

ğ‘¤!,!=ğ‘¤ğ‘“!,! ğ‘¥ logğ‘ ğ‘‘ğ‘“!  
where ğ‘¤ğ‘“!,! is given by: 
ğ‘–ğ‘“ ğ‘¡ğ‘“!,! >0
ğ‘¤ğ‘“!,!= 1+logğ‘¡ğ‘“!,! ,
                     0,         ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’  

(2) 

(3) 

than 

for  all 

just  counting 

Each  termâ€™s  frequency  is  now  assigned  a  weight, 
which  may  represent  a  termâ€™s  significance  more 
accurately 
the  number  of 
occurrences.  However,  we  found  the  classic  TF-IDF 
to  have  better  performance  on  all  three  models  over 
this  modified  version  of  Equation  1.  Using  Equation 
2,  the  top  f-words  for  each  category  were  similar  to 
those of Table 1, but not exactly the same. Since the 
classic  TF-IDF  performed  better 
three 
classifiers,  we  did  not  use  Equation  2  in  our  final 
implementation.  
4.2  Implementing the Classifiers 
We  selected  to  evaluate  NaÃ¯ve  Bayes,  SVM,  and 
Softmax  Regression  due  to  their  ability  to  perform 
supervised  learning  on  multi-class  datasets.  In  the 
following  sections,  we  describe  each  algorithm  and 
how it pertains to our goal of news classification. 
4.2.1  Support Vector Machine 
The  SVM  algorithm  requires  the  solution  to  the 
following optimization problem [4]: 

min!,!,! 12 ğ‘¤ !+ğ¶
ğœ‰!!
!!!
ğ‘ .ğ‘¡.    ğ‘¦! ğ‘¤!ğ‘¥! +ğ‘ â‰¥1âˆ’ğœ‰! 
ğœ‰!â‰¥0 
where ğ¶ is a regularization weighting parameter. The 
dimensional  space  that ğ‘¥!  is  mapped  to  [4].  For  our 

goal  of  the  SVM  is  to  find  a  linear-separating 
hyperplane that has the maximal margin in the higher 

project,  we  used  the  LIBSVM  MATLAB  library  to 
implement  our  Support  Vector  Classifier  (SVC)  [2]. 

(4) 

 

 

 

 

3 

we  performed 
implementing 

In order to maximize the performance of the SVM, we 

The  LIBSVM  library  offers  different  options  that 
allow a user to set the SVM-type and the kernel-type, 
as well as values for the different parameters. For our 
SVM, we tested multiple kernel functions, including a 
linear,  polynomial,  and  radial  basis  function  (RBF). 
We found the RBF to have the best performance, and 
subsequently used it for our implementation of the C-

SVC. The RBF kernel is given by ğ‘’!!|!!!!|, where ğ›¾ 
is a weighting parameter and ğ‘¥ is a query point [3]. 
needed to optimize the parameters ğ›¾ and ğ¶. To do so, 
selected exponentially growing ranges for both ğ¶ and 
ğ›¾  (ğ¶=2!!,2!!,â€¦,2!"  and ğ›¾=2!!",2!!",â€¦,2!). 
training data, we iterated through the different ğ›¾ and 
ğ¶  options,  noting  the  values  that  gave  the  highest 
each label value, ğ‘¦={1,2,â€¦,ğ‘˜}, a different SVM 

accuracy during each fold. We did not include any of 
the  test  data  in  the  internal  cross  validation,  since 
optimizing for a given test set would be incorrect. 
LIBSVM implements a one-against-all training model 
for the SVM when using a multi-class dataset. For 

search  by 
internal  cross  validation  [3].  We 

Using  5-fold  internal  cross  validation  on  only  the 

a  parameter  grid 

model is trained. Thus, we make k-different binary 
models. We then test each model on the testing data 
and determine the model from which the highest 
prediction confidence is returned in order to classify 
the data. 
4.2.2  NaÃ¯ve Bayes 
NaÃ¯ve  Bayes  was  used  as  our  baseline  text  classifier 
because it could be quickly implemented for analysis. 
Since  our  news  classification  framework  has  been 
deï¬ned  for  multiple  classes,  we  have  developed  the 
appropriate algorithms for this case as: 

1{ğ‘¥!!
=1 âˆ© ğ‘¦!=1}
ğœ™!|!!!"#$$!=
!!!!
 1{ğ‘¦!=1}
!!!!
 1{ğ‘¦!=1}
ğœ™!!!=
!!!! ğ‘š
interpretation. For example, ğœ™!|!=ğ‘– is the fraction of 
ğ‘(ğ‘¦=ğ‘—|ğ‘¥)=ğ‘(ğ‘¥|ğ‘¦=ğ‘—)ğ‘(ğ‘¦=ğ‘–)
ğ‘(ğ‘¥)

the category i in which word j appears. Having fit all 
the parameters, we calculate: 

these  parameters  have  a  natural 

From 

[16], 

  

  

(5) 

(6) 

(7) 

 

 

 

 

Figure 2: NaÃ¯ve Bayes Performance Measurements 
 

Figure 3: SVM Performance Measurements 
 
  
Since we are applying the classifier over a large 
vocabulary, we implemented Laplace Smoothing to 

avoid having ğœ™!â€²ğ‘  end up as zeros. 

 

 

Figure 4: Softmax Performance Measurements 

 

 

 
Figure 5: Average Performance Measurements for each 
Classifier 
 

4.2.3  Softmax Regression 
We  selected  to  use  Softmax  Regression,  also  known 
as the Multinomial Logistic Regression, as opposed to 
7-binary  classifiers  because  our  seven  classes  are 
mutually exclusive (i.e. a news article will be a part of 
at  most  one  category).  For  this  classifier,  the  class 

probabilities, ğ‘(ğ‘¦|ğ‘¥) are modeled as: 
= ğ‘’!!! !!ğ‘’!!! !!
ğ‘ğ‘¦ğ‘¥ = 
!!!!
where the ğœƒ parameters are learned from the training 

ğ‘’!!ğ‘’!!
!!!!

  

(8) 

set  by  maximizing  the  conditional  log  likelihood  of 
the data [16]. In this approach, a total of k-parameters 
are trained jointly using numerical optimization. 
5.  Results and Discussion 
The weighted accuracy of each classifier is presented 
in Figure 6. Softmax Regression achieved the best 

 

4 

Figure 6: Classifier Accuracy as Feature Size Varies  
performance  out  of  the  three  classifiers,  with  a 
maximum accuracy of 75.36% at 200 features. NaÃ¯ve 
Bayes  performed  significantly  worse  than  the  other 
two,  achieving  a  maximum  accuracy  of  56.12%  for 
150 features, which we attribute to its simplicity and 
weak  scalability.  SVMâ€™s  best  performance  was  not 
much  worse 
than  Softmax  Regression,  with  a 
maximum accuracy of 69.21% for 150 features. Both 

NaÃ¯ve  Bayes  and  SVM  ran  into  overfitting  issues  at 
200 features (i.e. performance began to decrease). We 
attribute the performance drop to the minimal overlap 
in  words  between  the  different  categories,  especially 
as the feature size increased. However, the optimized 
SVM performed much better than the non-optimized 
SVM,  which  ran  into  overfitting  issues  after  70 
features  and  had  a  maximum  accuracy  of  61.37%. 
Since Softmax Regression did not run into overfitting 
issues  for  our  evaluationâ€™s  maximum  feature  size, 
future work will continue to increase the feature size 
and further evaluate the performance of the classifier. 
To  further  evaluate  each  classifierâ€™s  performance  on 
individual  classes,  we  used  the  three  measurements 
derived from the confusion matrix: Precision, Recall, 
and  F1-score.  The  latter  three  measurements  reflect 
the importance of retrieval of positive examples in our 
text  classification 
the  class 
agreement  of  the  data  labels  with  the  positive  labels 
given  by 
the 
effectiveness of a classifier to identify positive labels 
[14]. The F1-score is the harmonic mean of precision 
and recall and is given by: 

the  classifier,  while  Recall 

ğ¹!=2âˆ—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›âˆ—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™

 

(9) 

is 

[14].  Precision 

is 

the  highest  Precision 

and  Recall, 
the  highest  F1-score  for  all 

Figures  2-4  show  each  classifierâ€™s  Precision,  Recall, 
and  F1-Score  for  each  category.  The  Sports  category 
had 
and 
subsequently 
three 
classifiers.  This  is  due  to  the  esoteric  and  overly 
specific  terms  used  in  the  Sports  categories  lexical 
features, as seen in Table 1. For Softmax Regression 
and  SVM,  the  Science  and  Technology category  had 
the  lowest  F1-score,  while  for  NaÃ¯ve  Bayes,  Health 
had the lowest F1-score. While the plots of these three 
metrics  for  SVM  and  Softmax  Regression  strongly 
resembled  each  other,  NaÃ¯ve  Bayes  had  Precision 
higher than Recall for all categories except for Health. 
The  balance  between  Precision  and  Recall  for  SVM 
and  Softmax  Regression  is  visible  in  Figure  5.  The 
average  Precision  and  Recall  are  almost  identical, 
demonstrating  the  two  classifiers  ability  to  perform 
better on a per-category-basis, especially for Softmax 
Regression. 
The low F1-scores of both the Health and Science and 
Technology  categories  are  due  to  the  inability  of  the 
classifier to distinguish one category from another. In 
particular,  for  Science  and  Technology,  the  majority 
the  Business 
of 
category. This is most likely attributed to technology 

the  false-negatives  came  from 

 

5 

lead 

often being mentioned in a commercial setting. Even 
more interesting, the number of false-positives for the 
Health  category  according  to  NaÃ¯ve  Bayes  is  very 
high (evident by the very low Precision in Figure 2). 
Using  NaÃ¯ve  Bayes,  the  majority  of  the  false-
negatives for every category was Health, which is the 
source of the low weighted accuracy. 
6.  Future Work 
Although  the  Sublinear  TF  Scaling  modification  to 
the  TF-IDF  algorithm  did  not  outperform  the  classic 
TF-IDF,  we  would  like  to  look  into  more  TF-IDF 
variants  and  other  methods  to  improve  the  feature 
selection  process.  Other  methods  might 
include 
Information  Gain  or  Conditional  Mutual  Information 
[5].  To  explore  the  optimal  feature  size,  we  might 
want to try a Forward or Backward search procedure 
with  a  reduced  dataset.  We  are  also  interested  in 
testing other classification methods, such as Recurrent 
Neural  Networks,  to  compare  them  against  the  three 
implemented  in  this  work.  In  particular,  classifying 
articles 
into  more  specific  categories,  such  as 
Computing  instead  of  just  Science  and  Technology, 
may 
to  classifier  performance  differences 
compared to the results from this work. 
Beyond trying to classify news articles into a specific 
category, we would like to explore the application of 
our developed framework towards detecting emotions 
or  bias  in  a  news  article.  Work  has  been  done  to 
explore  using  SVMs  and  semi-supervised  learning 
models  for  political  bias  by  [7].  In  addition,  [8,9] 
investigated  emotional  classification  from  both  the 
writerâ€™s  and 
readerâ€™s  perspective  using  SVMs. 
Effectively, we want to see how well our framework 
generalizes  to  other  underlying  aspects  of  news 
articles. 
7.  Conclusion 
Based on the F1-Scores in Figure 5, our best classifier, 
Softmax  Regression,  performed  12.57%  worse  than 
the  proposed  methods  of  Wang  et  al.  for  a  similar 
dataset.  Nevertheless,    we  have  shown  the  ability  of 
three  different  classifiers  to  automatically  classify 
news articles into their subject category. NaÃ¯ve Bayesâ€™ 
performance  was  adversely  affected  by  the  high 
number  of  false-negatives  from  the  Health  category. 
The well-known SVM algorithm, although not the top 
performer,  was  also  found  to  be  highly  suitable  for 
classifying news article into their subject category. 
8.  REFERENCES 
[1]  A3 Lab. TagMyNews Dataset. 

http://acube.di.unipi.it/tmn-dataset/ 

[2]  C. Chang and C. Lin, â€œLIBSVM: a library for 
support vector machines,â€ in ACM Trans. on 
Intelligent Syst. And Tech., vol. 2, no. 3, pp. 1-27, 
2001. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 

[3]  C. Hsu et al., â€œA Practical Guide to Support 

Vector Classification,â€ 2003 

[4]  A. Ng. CS 229. Class Lecture, Topic: â€œSupport 

Vector Machines.â€ Stanford University, Stanford, 
CA, October 14, 2015. 

[5]  A. Khan et al., â€œA Review of Machine Learning 

Algorithms for Text-Document Classification,â€ in 
Journal of Advances in Inform. Tech., vol. 1, no. 
1, pp. 4-20, 2010. 

[6]  C. Manning et al., â€œScoring, Term Weighting and 

Vector Space Model,â€ in Introduction to 
Information Retrieval, Cambridge University 
Press, 2008, ch. 6, sec. 6.4, pp. 126-127. 

[7]  D. Zhou et al., â€œClassifying the Political Leaning 
of News Articles and Users from User Votes,â€ in 
Proc. of the 5th International AAAI Conf. on 
Weblogs and Social Media, 2011. 

[8]  K. Lin et al., â€œEmotion Classification of Online 

News Articles from Readerâ€™s Perspective,â€ in 
Proc. of the 2008 IEEE/WIC/ACM Int. Conf. on 
Web Intelligence and Intelligent Agent Tech., 
2008. 

[9]  D. Bracewell et al., â€œDetermining the Emotion of 

 

 

News Articles,â€ in Computational Intelligence. 
Springer Berlin Heidelberg, pp. 918-923, 2006. 
E. Young and M. Jeong, â€œClass Dependent 

Feature Scaling Method using NaÃ¯ve Bayes 

[10] 

Classifier for Text Datamining,â€ in Pattern 
Recognition Letters, vol. 30, no. 5, pp. 477-485, 
2009. 

[11] 

Z. Wang et al., â€œAn Optimal SVM-based Text 

Classification Algorithm,â€ in 2005 International 
Conf. on Machine Learning and Cybernetics, pp. 
1378-1381, 2006. 

[12]  M. Ruiz and P. Srinivasan, â€œAutomatic Text 
Categorization using Neural Networks,â€ in Proc. 
of the 8th ASIS SIG/CR Workshop on 
Classification Research, pp. 59-72, 1998. 

[13]  A. Hakim et al., â€œAutomated Document 
Classification for News Articles in Bahasa 
Indonesia based on Term Frequency Inverse 
Document Frequency (TF-IDF) Approach,â€ in 6th 
International Conf. on Information Tech. and 
Elec. Eng, pp. 1-4, 2014. 

[14]  M. Sokolova and G Lapalme, â€œA systematic 

analysis of performance measures for 
classification tasks,â€ in Information Processing 
and Management, vol. 45, no. 4, pp. 427-437, 
2009. 

[15]  A. Ng. CS 229. Class Lecture, Topic: 

â€œSupervised Learning.â€ Stanford University, 
Stanford, CA, September 30, 2015. 

[16]  A. Ng. CS 229. Class Lecture, Topic: 

â€œGenerative Learning Algorithms.â€ Stanford 
University, Stanford, CA, October 5, 2015. 

[17] 

C. Do and A. Ng, â€œTransfer learning for text 

classification,â€ in NIPS, 2005. 

[18] 

Ranks. Stopwords list. 

http://www.ranks.nl/stopwords

6 

