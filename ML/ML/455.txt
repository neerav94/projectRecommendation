Artist Attribution via Song Lyrics

Michael Mara

December 12, 2014

1

Introduction

Song lyrics, separated from the audio signal of their
song, still contain a signiﬁcant amount of informa-
tion. Mood and meaning can still be conveyed ef-
fectively by a pure textual representation. There
has even been somewhat successful previous work on
genre classiﬁcation from song lyrics[7]. Building on
previous work, we seek to build an artist attribution
system for song lyrics.

This task is in the same vein as classic author attri-
bution tasks, which often are trained and evaluated
on extremely large datasets[8]; providing more data
per author than it is possible to get for most song-
writers. In order to focus our task, we focus only on
rap, as the songwriter and performer are usually the
same, and there is a heavy emphasis on distinctive
forms of lyricism. We actually enshrine that ﬁrst as-
sumption in our statement of the classiﬁcation task:
given a textual representation of the lyrics of a rap,
return the name of the artist who raps it. This is
a limitation we will have to live with for now, there
does not exist any large public database that provides
ghostwriting information for rappers.

Potential use cases for such a classiﬁer would be
for detecting misattributed songs in a music library
or as part of an auto-tagger in a music management
system, along with other uses of author-attribution
systems. A lyric-only classiﬁer could also be used in
an ensemble method that includes audio-only classi-
ﬁers.

Following previous work[2], we initially attempt to
distinguish between 4 proliﬁc rappers (Eminem, Nas,
Jay Z, and Nicki Minaj) before expanding the classi-
ﬁcation task to encompass more artists, testing thor-
oughly on a 12-artist dataset, and eventually testing
on over 300 rappers at once.

2 Dataset

structing our own dataset.

Song lyrics were obtained via the Genius API using
the Ruby gem rapgenius.rb1. They were then pro-
cessed using the Python Natural Language Toolkit
(NLTK)2. For each artist dataset, we downloaded the
lyrics to all available songs by each artist, and cre-
ated an ad-hoc blacklisting mechanism in python to
remove translated lyrics and non-songs (rap genius
sometimes has transcripts from movies or interviews
with the artist). We also initially excluded songs that
featured other artists even if our target artist was the
primary artist on the track, in order to mitigate cor-
rupted data from verses from the featured artist, ob-
taining the 4-artist (initial) dataset, consisting of 508
songs. Upon examination of the learning curve from
the dataset, which suggested more data would give a
signiﬁcant beneﬁt, we relaxed the requirement; then
obtaining the 4-artist (extended) dataset (887 Songs)
and the 12-artist dataset (2204 Songs).

As a ﬁnal test, we also tried testing on a dataset
made of all songs by all artists appearing on
Wikipedia’s List of Hip-Hop Musicians3 with over 40
songs available on Genius. This resulted in a dataset
with 348 artists (34,352 songs). Table 2 summarizes
our three datasets.

3 Features and Preprocessing

Given the raw lyrics to a song, we ﬁrst ﬁlter out song
descriptors (such as “[Chorus]”, “[Verse]”) via a sim-
ple handcrafted regex, then tokenize the remaining
lyrics. All features are extracted from this tokenized
representation. Except in our ﬁnal experiments, we
stick to a simple bag-of-words model, which has
proven to work very well on related tasks[8], often
beating painstakingly handcrafted features.

In order to obtain the bag-of-words representation,
we stem the tokens using the NLTK Snowball stem-
mer, construct a vocabulary consisting of every word

Previous work has run into the issue that there ap-
pears to be no reliable large dataset of lyrics with
author attribution[2], so we follow their lead in con-

1https://github.com/timrogers/rapgenius
2http://www.nltk.org/
3http://en.wikipedia.org/wiki/List_of_hip_hop_

musicians, accessed 12/10/2014

1

Artist Attribution via Song Lyrics

Michael Mara

December 12, 2014

1

Introduction

Song lyrics, separated from the audio signal of their
song, still contain a signiﬁcant amount of informa-
tion. Mood and meaning can still be conveyed ef-
fectively by a pure textual representation. There
has even been somewhat successful previous work on
genre classiﬁcation from song lyrics[7]. Building on
previous work, we seek to build an artist attribution
system for song lyrics.

This task is in the same vein as classic author attri-
bution tasks, which often are trained and evaluated
on extremely large datasets[8]; providing more data
per author than it is possible to get for most song-
writers. In order to focus our task, we focus only on
rap, as the songwriter and performer are usually the
same, and there is a heavy emphasis on distinctive
forms of lyricism. We actually enshrine that ﬁrst as-
sumption in our statement of the classiﬁcation task:
given a textual representation of the lyrics of a rap,
return the name of the artist who raps it. This is
a limitation we will have to live with for now, there
does not exist any large public database that provides
ghostwriting information for rappers.

Potential use cases for such a classiﬁer would be
for detecting misattributed songs in a music library
or as part of an auto-tagger in a music management
system, along with other uses of author-attribution
systems. A lyric-only classiﬁer could also be used in
an ensemble method that includes audio-only classi-
ﬁers.

Following previous work[2], we initially attempt to
distinguish between 4 proliﬁc rappers (Eminem, Nas,
Jay Z, and Nicki Minaj) before expanding the classi-
ﬁcation task to encompass more artists, testing thor-
oughly on a 12-artist dataset, and eventually testing
on over 300 rappers at once.

2 Dataset

structing our own dataset.

Song lyrics were obtained via the Genius API using
the Ruby gem rapgenius.rb1. They were then pro-
cessed using the Python Natural Language Toolkit
(NLTK)2. For each artist dataset, we downloaded the
lyrics to all available songs by each artist, and cre-
ated an ad-hoc blacklisting mechanism in python to
remove translated lyrics and non-songs (rap genius
sometimes has transcripts from movies or interviews
with the artist). We also initially excluded songs that
featured other artists even if our target artist was the
primary artist on the track, in order to mitigate cor-
rupted data from verses from the featured artist, ob-
taining the 4-artist (initial) dataset, consisting of 508
songs. Upon examination of the learning curve from
the dataset, which suggested more data would give a
signiﬁcant beneﬁt, we relaxed the requirement; then
obtaining the 4-artist (extended) dataset (887 Songs)
and the 12-artist dataset (2204 Songs).

As a ﬁnal test, we also tried testing on a dataset
made of all songs by all artists appearing on
Wikipedia’s List of Hip-Hop Musicians3 with over 40
songs available on Genius. This resulted in a dataset
with 348 artists (34,352 songs). Table 2 summarizes
our three datasets.

3 Features and Preprocessing

Given the raw lyrics to a song, we ﬁrst ﬁlter out song
descriptors (such as “[Chorus]”, “[Verse]”) via a sim-
ple handcrafted regex, then tokenize the remaining
lyrics. All features are extracted from this tokenized
representation. Except in our ﬁnal experiments, we
stick to a simple bag-of-words model, which has
proven to work very well on related tasks[8], often
beating painstakingly handcrafted features.

In order to obtain the bag-of-words representation,
we stem the tokens using the NLTK Snowball stem-
mer, construct a vocabulary consisting of every word

Previous work has run into the issue that there ap-
pears to be no reliable large dataset of lyrics with
author attribution[2], so we follow their lead in con-

1https://github.com/timrogers/rapgenius
2http://www.nltk.org/
3http://en.wikipedia.org/wiki/List_of_hip_hop_

musicians, accessed 12/10/2014

1

Artist
T.I.
2Pac
Snoop Dogg
Ice Cube
Nelly
Lil Jon
Sir Mix-a-Lot
Ying Yang Twins
Eminem
Nas
Kanye West
Nicki Minaj

Song Count
216
355
304
181
114
50
59
38
289
263
183
152

Table 1: The artists and song counts from the 12-
artist dataset. The 4-artist (extended) dataset con-
sists of just the songs from the ﬁnal four rows.

Dataset
4-artist (initial)
4-artist (extended)
12-artist
348-artist

Song # Vocab. Size

508
887
2,204
34,352

3,439
5,101
7,977
33,031

Table 2: The artists and song counts from the 12-
artist dataset. The 4-artist (extended) dataset con-
sists of just the songs from the ﬁnal four rows.

in the dataset, and construct a feature vector for each
song consisting of the count of each instance of each
word in the vocabulary appearing in the song. The
resulting in a bag-of-words representation is ideal for
our Naive Bayes classiﬁer[9].

On top of this, we implemented two feature selec-
tion methods in order to hopefully improve gener-
alization error[3]; ﬁrst a simple document frequency
thresholding, which removed words from the vocabu-
lary if they did not appear in at least 5 songs. Second,
we computed the χ2 statistic for feature selection[6].

(cid:88)

(cid:88)

ew∈{0,1}

ea∈{0,1}

χ2(w, a) =

(Newea − Eewea )

Eewea

is computed for each artist/word pair, where ew is
the occurence of the word w (1 when it occurs, 0
when it does not), ea is the occurrence of the artist
a, Newea is the observed frequency of co-occurence of
the events and Eewea is the expected frequency of the
co-occurence of the two events if the two events were
independent.

We then assigned a χ2 score to each word by taking
the max over all χ2 from artist/word pairs involving

2

the word:

χ2(w) = max

a

χ2(w, a)

We then chose n features by choosing the n words

with the highest value of χ2.

Feature weighting in Naive Bayes with the
Kullback-Leibler Measure[5] was brieﬂy considered,
but postponed due to the relative ineﬀectiveness of
our initial feature selection methods.

In our ﬁnal experiments, we add on part-of-speech
(POS) bigrams to test the value of adding a proxy
for syntactic structure. First each token in a song is
converted into a POS tag using the NTLK, and then
the count of each bigram of the resulting tags is used
as a feature.

4 Models

We use our own MATLAB implementation of a multi-
class Naive Bayes classiﬁer using the multinomial
event model and Laplace smoothing as our main
model. This was chosen based on its widespread suc-
cess in many text classiﬁcation tasks.

As a sanity check, we also implement a model based
on support vector machines. We use the built-in
MATLAB ﬁtcsvm() to train an ensemble of 1-vs-all
binary SVM classiﬁers, on the same features used for
Naive Bayes. We do multi-class classiﬁcation by se-
lecting the artist whose corresponding SVM returns
the highest score. The default C parameter (known
as the BoxConstraint parameter in some texts and
the MATLAB documentation) causes severe overﬁt-
ting (0 training error, 17%+ test error), so we also
train with hand-tuned smaller C values.

5 Results

Note that all results use 10-fold cross validation un-
less otherwise speciﬁed. Taking a cue from Com-
puter Vision (speciﬁcally the ImageNet classiﬁcation
tasks[4]), we report not only the standard error rate
for our larger datasets, but also some of the Top-N
error rates, where an example is counted as misclas-
siﬁed if its correct label was not among the N rated
as most probable by the model. Note that the Top-1
error rate is identical to the standard error rate.

5.1 Initial Results

Our initial experiments used the 4-artist (initial)
dataset, running the same classiﬁcation task as Guo
et al.[2]. With this dataset, and using all 3439 fea-
tures, our Naive Bayes classiﬁer achieves a test error

Artist Attribution via Song Lyrics

Michael Mara

December 12, 2014

1

Introduction

Song lyrics, separated from the audio signal of their
song, still contain a signiﬁcant amount of informa-
tion. Mood and meaning can still be conveyed ef-
fectively by a pure textual representation. There
has even been somewhat successful previous work on
genre classiﬁcation from song lyrics[7]. Building on
previous work, we seek to build an artist attribution
system for song lyrics.

This task is in the same vein as classic author attri-
bution tasks, which often are trained and evaluated
on extremely large datasets[8]; providing more data
per author than it is possible to get for most song-
writers. In order to focus our task, we focus only on
rap, as the songwriter and performer are usually the
same, and there is a heavy emphasis on distinctive
forms of lyricism. We actually enshrine that ﬁrst as-
sumption in our statement of the classiﬁcation task:
given a textual representation of the lyrics of a rap,
return the name of the artist who raps it. This is
a limitation we will have to live with for now, there
does not exist any large public database that provides
ghostwriting information for rappers.

Potential use cases for such a classiﬁer would be
for detecting misattributed songs in a music library
or as part of an auto-tagger in a music management
system, along with other uses of author-attribution
systems. A lyric-only classiﬁer could also be used in
an ensemble method that includes audio-only classi-
ﬁers.

Following previous work[2], we initially attempt to
distinguish between 4 proliﬁc rappers (Eminem, Nas,
Jay Z, and Nicki Minaj) before expanding the classi-
ﬁcation task to encompass more artists, testing thor-
oughly on a 12-artist dataset, and eventually testing
on over 300 rappers at once.

2 Dataset

structing our own dataset.

Song lyrics were obtained via the Genius API using
the Ruby gem rapgenius.rb1. They were then pro-
cessed using the Python Natural Language Toolkit
(NLTK)2. For each artist dataset, we downloaded the
lyrics to all available songs by each artist, and cre-
ated an ad-hoc blacklisting mechanism in python to
remove translated lyrics and non-songs (rap genius
sometimes has transcripts from movies or interviews
with the artist). We also initially excluded songs that
featured other artists even if our target artist was the
primary artist on the track, in order to mitigate cor-
rupted data from verses from the featured artist, ob-
taining the 4-artist (initial) dataset, consisting of 508
songs. Upon examination of the learning curve from
the dataset, which suggested more data would give a
signiﬁcant beneﬁt, we relaxed the requirement; then
obtaining the 4-artist (extended) dataset (887 Songs)
and the 12-artist dataset (2204 Songs).

As a ﬁnal test, we also tried testing on a dataset
made of all songs by all artists appearing on
Wikipedia’s List of Hip-Hop Musicians3 with over 40
songs available on Genius. This resulted in a dataset
with 348 artists (34,352 songs). Table 2 summarizes
our three datasets.

3 Features and Preprocessing

Given the raw lyrics to a song, we ﬁrst ﬁlter out song
descriptors (such as “[Chorus]”, “[Verse]”) via a sim-
ple handcrafted regex, then tokenize the remaining
lyrics. All features are extracted from this tokenized
representation. Except in our ﬁnal experiments, we
stick to a simple bag-of-words model, which has
proven to work very well on related tasks[8], often
beating painstakingly handcrafted features.

In order to obtain the bag-of-words representation,
we stem the tokens using the NLTK Snowball stem-
mer, construct a vocabulary consisting of every word

Previous work has run into the issue that there ap-
pears to be no reliable large dataset of lyrics with
author attribution[2], so we follow their lead in con-

1https://github.com/timrogers/rapgenius
2http://www.nltk.org/
3http://en.wikipedia.org/wiki/List_of_hip_hop_

musicians, accessed 12/10/2014

1

Artist
T.I.
2Pac
Snoop Dogg
Ice Cube
Nelly
Lil Jon
Sir Mix-a-Lot
Ying Yang Twins
Eminem
Nas
Kanye West
Nicki Minaj

Song Count
216
355
304
181
114
50
59
38
289
263
183
152

Table 1: The artists and song counts from the 12-
artist dataset. The 4-artist (extended) dataset con-
sists of just the songs from the ﬁnal four rows.

Dataset
4-artist (initial)
4-artist (extended)
12-artist
348-artist

Song # Vocab. Size

508
887
2,204
34,352

3,439
5,101
7,977
33,031

Table 2: The artists and song counts from the 12-
artist dataset. The 4-artist (extended) dataset con-
sists of just the songs from the ﬁnal four rows.

in the dataset, and construct a feature vector for each
song consisting of the count of each instance of each
word in the vocabulary appearing in the song. The
resulting in a bag-of-words representation is ideal for
our Naive Bayes classiﬁer[9].

On top of this, we implemented two feature selec-
tion methods in order to hopefully improve gener-
alization error[3]; ﬁrst a simple document frequency
thresholding, which removed words from the vocabu-
lary if they did not appear in at least 5 songs. Second,
we computed the χ2 statistic for feature selection[6].

(cid:88)

(cid:88)

ew∈{0,1}

ea∈{0,1}

χ2(w, a) =

(Newea − Eewea )

Eewea

is computed for each artist/word pair, where ew is
the occurence of the word w (1 when it occurs, 0
when it does not), ea is the occurrence of the artist
a, Newea is the observed frequency of co-occurence of
the events and Eewea is the expected frequency of the
co-occurence of the two events if the two events were
independent.

We then assigned a χ2 score to each word by taking
the max over all χ2 from artist/word pairs involving

2

the word:

χ2(w) = max

a

χ2(w, a)

We then chose n features by choosing the n words

with the highest value of χ2.

Feature weighting in Naive Bayes with the
Kullback-Leibler Measure[5] was brieﬂy considered,
but postponed due to the relative ineﬀectiveness of
our initial feature selection methods.

In our ﬁnal experiments, we add on part-of-speech
(POS) bigrams to test the value of adding a proxy
for syntactic structure. First each token in a song is
converted into a POS tag using the NTLK, and then
the count of each bigram of the resulting tags is used
as a feature.

4 Models

We use our own MATLAB implementation of a multi-
class Naive Bayes classiﬁer using the multinomial
event model and Laplace smoothing as our main
model. This was chosen based on its widespread suc-
cess in many text classiﬁcation tasks.

As a sanity check, we also implement a model based
on support vector machines. We use the built-in
MATLAB ﬁtcsvm() to train an ensemble of 1-vs-all
binary SVM classiﬁers, on the same features used for
Naive Bayes. We do multi-class classiﬁcation by se-
lecting the artist whose corresponding SVM returns
the highest score. The default C parameter (known
as the BoxConstraint parameter in some texts and
the MATLAB documentation) causes severe overﬁt-
ting (0 training error, 17%+ test error), so we also
train with hand-tuned smaller C values.

5 Results

Note that all results use 10-fold cross validation un-
less otherwise speciﬁed. Taking a cue from Com-
puter Vision (speciﬁcally the ImageNet classiﬁcation
tasks[4]), we report not only the standard error rate
for our larger datasets, but also some of the Top-N
error rates, where an example is counted as misclas-
siﬁed if its correct label was not among the N rated
as most probable by the model. Note that the Top-1
error rate is identical to the standard error rate.

5.1 Initial Results

Our initial experiments used the 4-artist (initial)
dataset, running the same classiﬁcation task as Guo
et al.[2]. With this dataset, and using all 3439 fea-
tures, our Naive Bayes classiﬁer achieves a test error

Figure 1: The learning curve from the initial data
suggested more training examples would continue to
decrease our test error rate.

Figure 2: The test error rates for our Naive Bayes
classiﬁer on the 12-artist dataset as we very the num-
ber of features, using our χ2 selection criteria.

of 14.27%; slightly lower than Guo et al (at 15%).
In order to improve results further, we plotted the
learning curve (Figure 5.1), and saw that more train-
ing examples would likely beneﬁt our model. This
is when we got rid of the “no featured artists” con-
straint, and obtained our other datasets.

Table 3: Our results on our 3 main datasets using all
of our models. Naive Bayes is the best model in all
of our tests.

Error Type
Training Error
Test Error
Top-3 Error
Top-5 Error
Top-10 Error
Top-50 Error
Top-100 Error

Error Rate
0.6395
0.7901
0.7044
0.6642
0.5974
0.3608
0.2167

Table 4: Error rates for our Naive Bayes Classiﬁer on
the 348-artist dataset.

5.3 Main Results

Table 5.3 is a table of our main results, and Fig-
ure 5.3 gives the learning curves for the 4-artist (ex-
tended) and 12-artist datasets on our highest per-
forming model. Figure 5.3 provides a visualization of
the confusion matrix (averaged over the 10-fold cross-
validation) of our highest performing model. The
only anomalous result is the unusually poor classiﬁca-
tion of the Yin-Yang Twins, which our model hardly
ever uses as the predicted label. This may be partially
attributable to the fact that the Yin-Yang Twins have
the lowest song count in all of our datasets at 38.

5.4 Scaling Up

5.2 Adjusting Feature Count

In Figure 5.2, we show that our feature selection
method does not seem to help results signiﬁcantly,
even on our 12-artist dataset, though it does allow
the removal of many features without negatively af-
fecting our error rates.

In order to get a taste of how our best model performs
on a dataset more than an order of magnitude larger
(both in song count and artist count), we ran our
Naive Bayes model on a dataset of 34,352 songs across
348 artists. Due to the fairly high error rate (though
fairly low compared to chance), and high computa-
tional cost, we report results only for Naive Bayes
using all available features (see Table 5.4).

3

# Training Examples50100150200250300350400Error rate00.050.10.150.20.250.30.35Learning Curve from Initial 4-artist DataTraining ErrorTest ErrorDataset4-artist (Initial) [508 Songs, 3439 Features]4-artist (Extended)[887 Songs, 5101 Features]12-artist[2204 Songs, 7977 Features]ModelTraining ErrorTest ErrorTraining ErrorTest ErrorTraining ErrorTest ErrorTop-2 Test ErrorTop-3 Test ErrorSVM (C=1, n=2000)00.188000.175200.28650.17020.1141SVM (C=0.005, n=2000)0.0180.18230.4120.19090.03840.23360.14280.0991SVM (C=0.002, n=2000)0.4210.20680.1530.17880.07400.25080.15330.1141Naïve Bayes (All features)0.01950.14270.0280.12440.05380.22270.12410.0834Naïve Bayes (n=2000)0.01990.14650.0340.12260.09080.2290.12620.0745Naïve Bayes (n=500)0.04890.15210.06960.15060.18110.26140.15280.1007Artist Attribution via Song Lyrics

Michael Mara

December 12, 2014

1

Introduction

Song lyrics, separated from the audio signal of their
song, still contain a signiﬁcant amount of informa-
tion. Mood and meaning can still be conveyed ef-
fectively by a pure textual representation. There
has even been somewhat successful previous work on
genre classiﬁcation from song lyrics[7]. Building on
previous work, we seek to build an artist attribution
system for song lyrics.

This task is in the same vein as classic author attri-
bution tasks, which often are trained and evaluated
on extremely large datasets[8]; providing more data
per author than it is possible to get for most song-
writers. In order to focus our task, we focus only on
rap, as the songwriter and performer are usually the
same, and there is a heavy emphasis on distinctive
forms of lyricism. We actually enshrine that ﬁrst as-
sumption in our statement of the classiﬁcation task:
given a textual representation of the lyrics of a rap,
return the name of the artist who raps it. This is
a limitation we will have to live with for now, there
does not exist any large public database that provides
ghostwriting information for rappers.

Potential use cases for such a classiﬁer would be
for detecting misattributed songs in a music library
or as part of an auto-tagger in a music management
system, along with other uses of author-attribution
systems. A lyric-only classiﬁer could also be used in
an ensemble method that includes audio-only classi-
ﬁers.

Following previous work[2], we initially attempt to
distinguish between 4 proliﬁc rappers (Eminem, Nas,
Jay Z, and Nicki Minaj) before expanding the classi-
ﬁcation task to encompass more artists, testing thor-
oughly on a 12-artist dataset, and eventually testing
on over 300 rappers at once.

2 Dataset

structing our own dataset.

Song lyrics were obtained via the Genius API using
the Ruby gem rapgenius.rb1. They were then pro-
cessed using the Python Natural Language Toolkit
(NLTK)2. For each artist dataset, we downloaded the
lyrics to all available songs by each artist, and cre-
ated an ad-hoc blacklisting mechanism in python to
remove translated lyrics and non-songs (rap genius
sometimes has transcripts from movies or interviews
with the artist). We also initially excluded songs that
featured other artists even if our target artist was the
primary artist on the track, in order to mitigate cor-
rupted data from verses from the featured artist, ob-
taining the 4-artist (initial) dataset, consisting of 508
songs. Upon examination of the learning curve from
the dataset, which suggested more data would give a
signiﬁcant beneﬁt, we relaxed the requirement; then
obtaining the 4-artist (extended) dataset (887 Songs)
and the 12-artist dataset (2204 Songs).

As a ﬁnal test, we also tried testing on a dataset
made of all songs by all artists appearing on
Wikipedia’s List of Hip-Hop Musicians3 with over 40
songs available on Genius. This resulted in a dataset
with 348 artists (34,352 songs). Table 2 summarizes
our three datasets.

3 Features and Preprocessing

Given the raw lyrics to a song, we ﬁrst ﬁlter out song
descriptors (such as “[Chorus]”, “[Verse]”) via a sim-
ple handcrafted regex, then tokenize the remaining
lyrics. All features are extracted from this tokenized
representation. Except in our ﬁnal experiments, we
stick to a simple bag-of-words model, which has
proven to work very well on related tasks[8], often
beating painstakingly handcrafted features.

In order to obtain the bag-of-words representation,
we stem the tokens using the NLTK Snowball stem-
mer, construct a vocabulary consisting of every word

Previous work has run into the issue that there ap-
pears to be no reliable large dataset of lyrics with
author attribution[2], so we follow their lead in con-

1https://github.com/timrogers/rapgenius
2http://www.nltk.org/
3http://en.wikipedia.org/wiki/List_of_hip_hop_

musicians, accessed 12/10/2014

1

Artist
T.I.
2Pac
Snoop Dogg
Ice Cube
Nelly
Lil Jon
Sir Mix-a-Lot
Ying Yang Twins
Eminem
Nas
Kanye West
Nicki Minaj

Song Count
216
355
304
181
114
50
59
38
289
263
183
152

Table 1: The artists and song counts from the 12-
artist dataset. The 4-artist (extended) dataset con-
sists of just the songs from the ﬁnal four rows.

Dataset
4-artist (initial)
4-artist (extended)
12-artist
348-artist

Song # Vocab. Size

508
887
2,204
34,352

3,439
5,101
7,977
33,031

Table 2: The artists and song counts from the 12-
artist dataset. The 4-artist (extended) dataset con-
sists of just the songs from the ﬁnal four rows.

in the dataset, and construct a feature vector for each
song consisting of the count of each instance of each
word in the vocabulary appearing in the song. The
resulting in a bag-of-words representation is ideal for
our Naive Bayes classiﬁer[9].

On top of this, we implemented two feature selec-
tion methods in order to hopefully improve gener-
alization error[3]; ﬁrst a simple document frequency
thresholding, which removed words from the vocabu-
lary if they did not appear in at least 5 songs. Second,
we computed the χ2 statistic for feature selection[6].

(cid:88)

(cid:88)

ew∈{0,1}

ea∈{0,1}

χ2(w, a) =

(Newea − Eewea )

Eewea

is computed for each artist/word pair, where ew is
the occurence of the word w (1 when it occurs, 0
when it does not), ea is the occurrence of the artist
a, Newea is the observed frequency of co-occurence of
the events and Eewea is the expected frequency of the
co-occurence of the two events if the two events were
independent.

We then assigned a χ2 score to each word by taking
the max over all χ2 from artist/word pairs involving

2

the word:

χ2(w) = max

a

χ2(w, a)

We then chose n features by choosing the n words

with the highest value of χ2.

Feature weighting in Naive Bayes with the
Kullback-Leibler Measure[5] was brieﬂy considered,
but postponed due to the relative ineﬀectiveness of
our initial feature selection methods.

In our ﬁnal experiments, we add on part-of-speech
(POS) bigrams to test the value of adding a proxy
for syntactic structure. First each token in a song is
converted into a POS tag using the NTLK, and then
the count of each bigram of the resulting tags is used
as a feature.

4 Models

We use our own MATLAB implementation of a multi-
class Naive Bayes classiﬁer using the multinomial
event model and Laplace smoothing as our main
model. This was chosen based on its widespread suc-
cess in many text classiﬁcation tasks.

As a sanity check, we also implement a model based
on support vector machines. We use the built-in
MATLAB ﬁtcsvm() to train an ensemble of 1-vs-all
binary SVM classiﬁers, on the same features used for
Naive Bayes. We do multi-class classiﬁcation by se-
lecting the artist whose corresponding SVM returns
the highest score. The default C parameter (known
as the BoxConstraint parameter in some texts and
the MATLAB documentation) causes severe overﬁt-
ting (0 training error, 17%+ test error), so we also
train with hand-tuned smaller C values.

5 Results

Note that all results use 10-fold cross validation un-
less otherwise speciﬁed. Taking a cue from Com-
puter Vision (speciﬁcally the ImageNet classiﬁcation
tasks[4]), we report not only the standard error rate
for our larger datasets, but also some of the Top-N
error rates, where an example is counted as misclas-
siﬁed if its correct label was not among the N rated
as most probable by the model. Note that the Top-1
error rate is identical to the standard error rate.

5.1 Initial Results

Our initial experiments used the 4-artist (initial)
dataset, running the same classiﬁcation task as Guo
et al.[2]. With this dataset, and using all 3439 fea-
tures, our Naive Bayes classiﬁer achieves a test error

Figure 1: The learning curve from the initial data
suggested more training examples would continue to
decrease our test error rate.

Figure 2: The test error rates for our Naive Bayes
classiﬁer on the 12-artist dataset as we very the num-
ber of features, using our χ2 selection criteria.

of 14.27%; slightly lower than Guo et al (at 15%).
In order to improve results further, we plotted the
learning curve (Figure 5.1), and saw that more train-
ing examples would likely beneﬁt our model. This
is when we got rid of the “no featured artists” con-
straint, and obtained our other datasets.

Table 3: Our results on our 3 main datasets using all
of our models. Naive Bayes is the best model in all
of our tests.

Error Type
Training Error
Test Error
Top-3 Error
Top-5 Error
Top-10 Error
Top-50 Error
Top-100 Error

Error Rate
0.6395
0.7901
0.7044
0.6642
0.5974
0.3608
0.2167

Table 4: Error rates for our Naive Bayes Classiﬁer on
the 348-artist dataset.

5.3 Main Results

Table 5.3 is a table of our main results, and Fig-
ure 5.3 gives the learning curves for the 4-artist (ex-
tended) and 12-artist datasets on our highest per-
forming model. Figure 5.3 provides a visualization of
the confusion matrix (averaged over the 10-fold cross-
validation) of our highest performing model. The
only anomalous result is the unusually poor classiﬁca-
tion of the Yin-Yang Twins, which our model hardly
ever uses as the predicted label. This may be partially
attributable to the fact that the Yin-Yang Twins have
the lowest song count in all of our datasets at 38.

5.4 Scaling Up

5.2 Adjusting Feature Count

In Figure 5.2, we show that our feature selection
method does not seem to help results signiﬁcantly,
even on our 12-artist dataset, though it does allow
the removal of many features without negatively af-
fecting our error rates.

In order to get a taste of how our best model performs
on a dataset more than an order of magnitude larger
(both in song count and artist count), we ran our
Naive Bayes model on a dataset of 34,352 songs across
348 artists. Due to the fairly high error rate (though
fairly low compared to chance), and high computa-
tional cost, we report results only for Naive Bayes
using all available features (see Table 5.4).

3

# Training Examples50100150200250300350400Error rate00.050.10.150.20.250.30.35Learning Curve from Initial 4-artist DataTraining ErrorTest ErrorDataset4-artist (Initial) [508 Songs, 3439 Features]4-artist (Extended)[887 Songs, 5101 Features]12-artist[2204 Songs, 7977 Features]ModelTraining ErrorTest ErrorTraining ErrorTest ErrorTraining ErrorTest ErrorTop-2 Test ErrorTop-3 Test ErrorSVM (C=1, n=2000)00.188000.175200.28650.17020.1141SVM (C=0.005, n=2000)0.0180.18230.4120.19090.03840.23360.14280.0991SVM (C=0.002, n=2000)0.4210.20680.1530.17880.07400.25080.15330.1141Naïve Bayes (All features)0.01950.14270.0280.12440.05380.22270.12410.0834Naïve Bayes (n=2000)0.01990.14650.0340.12260.09080.2290.12620.0745Naïve Bayes (n=500)0.04890.15210.06960.15060.18110.26140.15280.1007Figure 4: Confusion Matrix for our Naive Bayes Clas-
siﬁer on the 12-artist dataset (using all features). The
one anomalous result is the high misclassiﬁcation of
the Yin-Yang Twins, who have the least number of
songs in our dataset.

Model
Naive Bayes
SVM (C=1)
SVM (C=0.005)
SVM (C=0.002)

Bag-of-Words + POS
Bigrams
0.1382
0.1992
0.1799
0.1694

0.1244
0.1781
0.1632
0.1767

Table 5: Comparison between our base model and
our model augmented with POS bigrams. Our tests
showed no improvement (in fact a deterioration) from
adding POS bigrams to our model. Although it im-
proves the test error slightly on our SVM(C=0.002),
the best error for the SVM still comes without the
use of POS Bigrams.

5.5 Adding Features

As a quick ﬁnal test, to try and get more information
out of our limited number of training examples, we
tried augmenting our bag-of-words model with part-
of-speech (POS) bigrams, generated using the NLTK,
as a proxy for local syntactic structure.

6 Discussion

Judging by Figure 5.2, our feature selection mecha-
nism seem to be at best not-harmful; there’s no no-
ticeable improvement in the error rate by selecting
smaller feature sets, though it also doesn’t hurt until
<2000 features on the 12-artist dataset. Examining
our main results (Table 5.3) The Naive Bayes classi-
ﬁer does fairly well on the task, having a signiﬁcantly

4

Figure 3: The learning curves for our Naive Bayes
classiﬁer using all available features on both the 4-
artist (extended) and 12-artist datasets. Note the
similarity to the learning curved for the 4-artist (ini-
tial) dataset.

# Training Examples0100200300400500600700800Error rate00.050.10.150.20.250.30.350.4Learning Curve from Extended 4-artist DataTraining ErrorTest Error# Training Examples0200400600800100012001400160018002000Error Rate00.10.20.30.40.50.6Learning Curve for 12-artist DataIntended LabelNicki MinajKanye WestYin-Yang TwinsSir Mix-a-LotEminemNas12-artist Confusion Matrix VisualizationLil JonSnoop DoggIce CubeNelly2PacT.I.Nicki MinajKanye WestNasEminemYin-Yang TwinsSir Mix-a-LotLil JonNellyIce CubeChosen LabelSnoop Dogg2PacT.I.100500Mean Percentage ChoicesArtist Attribution via Song Lyrics

Michael Mara

December 12, 2014

1

Introduction

Song lyrics, separated from the audio signal of their
song, still contain a signiﬁcant amount of informa-
tion. Mood and meaning can still be conveyed ef-
fectively by a pure textual representation. There
has even been somewhat successful previous work on
genre classiﬁcation from song lyrics[7]. Building on
previous work, we seek to build an artist attribution
system for song lyrics.

This task is in the same vein as classic author attri-
bution tasks, which often are trained and evaluated
on extremely large datasets[8]; providing more data
per author than it is possible to get for most song-
writers. In order to focus our task, we focus only on
rap, as the songwriter and performer are usually the
same, and there is a heavy emphasis on distinctive
forms of lyricism. We actually enshrine that ﬁrst as-
sumption in our statement of the classiﬁcation task:
given a textual representation of the lyrics of a rap,
return the name of the artist who raps it. This is
a limitation we will have to live with for now, there
does not exist any large public database that provides
ghostwriting information for rappers.

Potential use cases for such a classiﬁer would be
for detecting misattributed songs in a music library
or as part of an auto-tagger in a music management
system, along with other uses of author-attribution
systems. A lyric-only classiﬁer could also be used in
an ensemble method that includes audio-only classi-
ﬁers.

Following previous work[2], we initially attempt to
distinguish between 4 proliﬁc rappers (Eminem, Nas,
Jay Z, and Nicki Minaj) before expanding the classi-
ﬁcation task to encompass more artists, testing thor-
oughly on a 12-artist dataset, and eventually testing
on over 300 rappers at once.

2 Dataset

structing our own dataset.

Song lyrics were obtained via the Genius API using
the Ruby gem rapgenius.rb1. They were then pro-
cessed using the Python Natural Language Toolkit
(NLTK)2. For each artist dataset, we downloaded the
lyrics to all available songs by each artist, and cre-
ated an ad-hoc blacklisting mechanism in python to
remove translated lyrics and non-songs (rap genius
sometimes has transcripts from movies or interviews
with the artist). We also initially excluded songs that
featured other artists even if our target artist was the
primary artist on the track, in order to mitigate cor-
rupted data from verses from the featured artist, ob-
taining the 4-artist (initial) dataset, consisting of 508
songs. Upon examination of the learning curve from
the dataset, which suggested more data would give a
signiﬁcant beneﬁt, we relaxed the requirement; then
obtaining the 4-artist (extended) dataset (887 Songs)
and the 12-artist dataset (2204 Songs).

As a ﬁnal test, we also tried testing on a dataset
made of all songs by all artists appearing on
Wikipedia’s List of Hip-Hop Musicians3 with over 40
songs available on Genius. This resulted in a dataset
with 348 artists (34,352 songs). Table 2 summarizes
our three datasets.

3 Features and Preprocessing

Given the raw lyrics to a song, we ﬁrst ﬁlter out song
descriptors (such as “[Chorus]”, “[Verse]”) via a sim-
ple handcrafted regex, then tokenize the remaining
lyrics. All features are extracted from this tokenized
representation. Except in our ﬁnal experiments, we
stick to a simple bag-of-words model, which has
proven to work very well on related tasks[8], often
beating painstakingly handcrafted features.

In order to obtain the bag-of-words representation,
we stem the tokens using the NLTK Snowball stem-
mer, construct a vocabulary consisting of every word

Previous work has run into the issue that there ap-
pears to be no reliable large dataset of lyrics with
author attribution[2], so we follow their lead in con-

1https://github.com/timrogers/rapgenius
2http://www.nltk.org/
3http://en.wikipedia.org/wiki/List_of_hip_hop_

musicians, accessed 12/10/2014

1

Artist
T.I.
2Pac
Snoop Dogg
Ice Cube
Nelly
Lil Jon
Sir Mix-a-Lot
Ying Yang Twins
Eminem
Nas
Kanye West
Nicki Minaj

Song Count
216
355
304
181
114
50
59
38
289
263
183
152

Table 1: The artists and song counts from the 12-
artist dataset. The 4-artist (extended) dataset con-
sists of just the songs from the ﬁnal four rows.

Dataset
4-artist (initial)
4-artist (extended)
12-artist
348-artist

Song # Vocab. Size

508
887
2,204
34,352

3,439
5,101
7,977
33,031

Table 2: The artists and song counts from the 12-
artist dataset. The 4-artist (extended) dataset con-
sists of just the songs from the ﬁnal four rows.

in the dataset, and construct a feature vector for each
song consisting of the count of each instance of each
word in the vocabulary appearing in the song. The
resulting in a bag-of-words representation is ideal for
our Naive Bayes classiﬁer[9].

On top of this, we implemented two feature selec-
tion methods in order to hopefully improve gener-
alization error[3]; ﬁrst a simple document frequency
thresholding, which removed words from the vocabu-
lary if they did not appear in at least 5 songs. Second,
we computed the χ2 statistic for feature selection[6].

(cid:88)

(cid:88)

ew∈{0,1}

ea∈{0,1}

χ2(w, a) =

(Newea − Eewea )

Eewea

is computed for each artist/word pair, where ew is
the occurence of the word w (1 when it occurs, 0
when it does not), ea is the occurrence of the artist
a, Newea is the observed frequency of co-occurence of
the events and Eewea is the expected frequency of the
co-occurence of the two events if the two events were
independent.

We then assigned a χ2 score to each word by taking
the max over all χ2 from artist/word pairs involving

2

the word:

χ2(w) = max

a

χ2(w, a)

We then chose n features by choosing the n words

with the highest value of χ2.

Feature weighting in Naive Bayes with the
Kullback-Leibler Measure[5] was brieﬂy considered,
but postponed due to the relative ineﬀectiveness of
our initial feature selection methods.

In our ﬁnal experiments, we add on part-of-speech
(POS) bigrams to test the value of adding a proxy
for syntactic structure. First each token in a song is
converted into a POS tag using the NTLK, and then
the count of each bigram of the resulting tags is used
as a feature.

4 Models

We use our own MATLAB implementation of a multi-
class Naive Bayes classiﬁer using the multinomial
event model and Laplace smoothing as our main
model. This was chosen based on its widespread suc-
cess in many text classiﬁcation tasks.

As a sanity check, we also implement a model based
on support vector machines. We use the built-in
MATLAB ﬁtcsvm() to train an ensemble of 1-vs-all
binary SVM classiﬁers, on the same features used for
Naive Bayes. We do multi-class classiﬁcation by se-
lecting the artist whose corresponding SVM returns
the highest score. The default C parameter (known
as the BoxConstraint parameter in some texts and
the MATLAB documentation) causes severe overﬁt-
ting (0 training error, 17%+ test error), so we also
train with hand-tuned smaller C values.

5 Results

Note that all results use 10-fold cross validation un-
less otherwise speciﬁed. Taking a cue from Com-
puter Vision (speciﬁcally the ImageNet classiﬁcation
tasks[4]), we report not only the standard error rate
for our larger datasets, but also some of the Top-N
error rates, where an example is counted as misclas-
siﬁed if its correct label was not among the N rated
as most probable by the model. Note that the Top-1
error rate is identical to the standard error rate.

5.1 Initial Results

Our initial experiments used the 4-artist (initial)
dataset, running the same classiﬁcation task as Guo
et al.[2]. With this dataset, and using all 3439 fea-
tures, our Naive Bayes classiﬁer achieves a test error

Figure 1: The learning curve from the initial data
suggested more training examples would continue to
decrease our test error rate.

Figure 2: The test error rates for our Naive Bayes
classiﬁer on the 12-artist dataset as we very the num-
ber of features, using our χ2 selection criteria.

of 14.27%; slightly lower than Guo et al (at 15%).
In order to improve results further, we plotted the
learning curve (Figure 5.1), and saw that more train-
ing examples would likely beneﬁt our model. This
is when we got rid of the “no featured artists” con-
straint, and obtained our other datasets.

Table 3: Our results on our 3 main datasets using all
of our models. Naive Bayes is the best model in all
of our tests.

Error Type
Training Error
Test Error
Top-3 Error
Top-5 Error
Top-10 Error
Top-50 Error
Top-100 Error

Error Rate
0.6395
0.7901
0.7044
0.6642
0.5974
0.3608
0.2167

Table 4: Error rates for our Naive Bayes Classiﬁer on
the 348-artist dataset.

5.3 Main Results

Table 5.3 is a table of our main results, and Fig-
ure 5.3 gives the learning curves for the 4-artist (ex-
tended) and 12-artist datasets on our highest per-
forming model. Figure 5.3 provides a visualization of
the confusion matrix (averaged over the 10-fold cross-
validation) of our highest performing model. The
only anomalous result is the unusually poor classiﬁca-
tion of the Yin-Yang Twins, which our model hardly
ever uses as the predicted label. This may be partially
attributable to the fact that the Yin-Yang Twins have
the lowest song count in all of our datasets at 38.

5.4 Scaling Up

5.2 Adjusting Feature Count

In Figure 5.2, we show that our feature selection
method does not seem to help results signiﬁcantly,
even on our 12-artist dataset, though it does allow
the removal of many features without negatively af-
fecting our error rates.

In order to get a taste of how our best model performs
on a dataset more than an order of magnitude larger
(both in song count and artist count), we ran our
Naive Bayes model on a dataset of 34,352 songs across
348 artists. Due to the fairly high error rate (though
fairly low compared to chance), and high computa-
tional cost, we report results only for Naive Bayes
using all available features (see Table 5.4).

3

# Training Examples50100150200250300350400Error rate00.050.10.150.20.250.30.35Learning Curve from Initial 4-artist DataTraining ErrorTest ErrorDataset4-artist (Initial) [508 Songs, 3439 Features]4-artist (Extended)[887 Songs, 5101 Features]12-artist[2204 Songs, 7977 Features]ModelTraining ErrorTest ErrorTraining ErrorTest ErrorTraining ErrorTest ErrorTop-2 Test ErrorTop-3 Test ErrorSVM (C=1, n=2000)00.188000.175200.28650.17020.1141SVM (C=0.005, n=2000)0.0180.18230.4120.19090.03840.23360.14280.0991SVM (C=0.002, n=2000)0.4210.20680.1530.17880.07400.25080.15330.1141Naïve Bayes (All features)0.01950.14270.0280.12440.05380.22270.12410.0834Naïve Bayes (n=2000)0.01990.14650.0340.12260.09080.2290.12620.0745Naïve Bayes (n=500)0.04890.15210.06960.15060.18110.26140.15280.1007Figure 4: Confusion Matrix for our Naive Bayes Clas-
siﬁer on the 12-artist dataset (using all features). The
one anomalous result is the high misclassiﬁcation of
the Yin-Yang Twins, who have the least number of
songs in our dataset.

Model
Naive Bayes
SVM (C=1)
SVM (C=0.005)
SVM (C=0.002)

Bag-of-Words + POS
Bigrams
0.1382
0.1992
0.1799
0.1694

0.1244
0.1781
0.1632
0.1767

Table 5: Comparison between our base model and
our model augmented with POS bigrams. Our tests
showed no improvement (in fact a deterioration) from
adding POS bigrams to our model. Although it im-
proves the test error slightly on our SVM(C=0.002),
the best error for the SVM still comes without the
use of POS Bigrams.

5.5 Adding Features

As a quick ﬁnal test, to try and get more information
out of our limited number of training examples, we
tried augmenting our bag-of-words model with part-
of-speech (POS) bigrams, generated using the NLTK,
as a proxy for local syntactic structure.

6 Discussion

Judging by Figure 5.2, our feature selection mecha-
nism seem to be at best not-harmful; there’s no no-
ticeable improvement in the error rate by selecting
smaller feature sets, though it also doesn’t hurt until
<2000 features on the 12-artist dataset. Examining
our main results (Table 5.3) The Naive Bayes classi-
ﬁer does fairly well on the task, having a signiﬁcantly

4

Figure 3: The learning curves for our Naive Bayes
classiﬁer using all available features on both the 4-
artist (extended) and 12-artist datasets. Note the
similarity to the learning curved for the 4-artist (ini-
tial) dataset.

# Training Examples0100200300400500600700800Error rate00.050.10.150.20.250.30.350.4Learning Curve from Extended 4-artist DataTraining ErrorTest Error# Training Examples0200400600800100012001400160018002000Error Rate00.10.20.30.40.50.6Learning Curve for 12-artist DataIntended LabelNicki MinajKanye WestYin-Yang TwinsSir Mix-a-LotEminemNas12-artist Confusion Matrix VisualizationLil JonSnoop DoggIce CubeNelly2PacT.I.Nicki MinajKanye WestNasEminemYin-Yang TwinsSir Mix-a-LotLil JonNellyIce CubeChosen LabelSnoop Dogg2PacT.I.100500Mean Percentage ChoicesReferences

[1] Boulis, C., and Ostendorf., M. Text classiﬁ-
cation by augmenting the bag-of-words represen-
tation with redundancy-compensated bigrams. In
In Proc. of the FSDM (2005).

[2] Guo, S., and Khamphoune, S. “i’m diﬀer-
ent, yeah i’m diﬀerent”: Classifying rap lyrics by
artist.
In Stanford CS229 Final Project Papers
(2013).

[3] John, G. H., Kohavi, R., and Pfleger, K.
Irrelevant features and the subset selection prob-
lem.
In MACHINE LEARNING: PROCEED-
INGS OF THE ELEVENTH INTERNATIONAL
(1994), Morgan Kaufmann, pp. 121–129.

[4] Krizhevsky, A., Sutskever, I., and Hinton,
G. E. Imagenet classiﬁcation with deep convolu-
tional neural networks.
In Advances in Neural
Information Processing Systems, p. 2012.

[5] Lee, C.-H., Gutierrez, F., and Dou, D.
Calculating feature weights in naive bayes with
kullback-leibler measure.
In Proceedings of the
2011 IEEE 11th International Conference on
Data Mining (Washington, DC, USA, 2011),
ICDM ’11, IEEE Computer Society, pp. 1146–
1151.

[6] Manning, C. D., Raghavan, P., and
Sch¨utze, H.
Introduction to Information Re-
trieval. Cambridge University Press, New York,
NY, USA, 2008.

[7] Mayer, R., Neumayer, R., and Rauber, A.
Rhyme and style features for musical genre clas-
siﬁcation by song lyrics. In In Proceedings of the
9th International Conference on Music Informa-
tion Retrieval (ISMIR’08) (2008).

[8] Stamatatos, E. A survey of modern authorship
attribution methods. J. Am. Soc. Inf. Sci. Tech-
nol. 60, 3 (Mar. 2009), 538–556.

[9] Telecommunications, V. M., and Metsis,
V. Spam ﬁltering with naive bayes – which naive
bayes? In Third Conference on Email and Anti-
Spam (CEAS) (2006).

lower error rate than the SVM, which takes far longer
to train. Given previous successes using Naive Bayes,
this is not entirely unexpected.

Our classiﬁcation methods hold up fairly well even
when tripling the number of categories, with our best
classiﬁer achieving <7.5% top-3 test error on the 12-
artist dataset. We outperform Guo et al[2] on the
4-artist task, likely due to our larger quantity of train-
ing examples, via our programmatic data extraction.
Inspection of the learning curves (Figure 5.3) sug-
gests more examples could lead to further reduction
of our error rate, as training error is much lower than
our test error. However, this is infeasible; artists only
put out a ﬁnite number of songs, and most have a
smaller discography than any of the twelve in our
toughest classiﬁcation task.

In order to get better classiﬁcation, to the point
where we could perhaps get reasonable classiﬁcation
on our 348-artist dataset, we must make better use of
the current data. Raw bag-of-words, though elegant,
throws away a lot of information that could be cap-
tured by higher-level features, such as rhyme scheme,
sentiment, and syntactic construction. Although we
found no beneﬁt from adding POS bigrams, there are
many other features that could be tried in a more ex-
tensive analysis, including rhyme and style features
used in previous work[7].

7 Future

Future work should seek to extend the bag-of-words
model, perhaps by implementing redundancy com-
pensated bigrams[1], sentiment analysis, and rhyme
and style features[7]. Class label reduction by using
some clustering of artists could also prove useful (or
perhaps necessary, as the sheer number of artists may
drive the error rate far too high for even the most so-
phisticated of models).

This paper purposefully avoids the question of at-
tributing ghost-written songs to songwriters, which
could be an interesting challenge. Our datasets are
also automatically generated from a crowd-sourced
lyric repository; the error rate of our ground truth is
unknown, and worthy of study in order to continue
this line of research.

8 Acknowledgements

The author would like to thank Professor Ng and the
CS229 course TAs for the fascinating introduction
to machine learning over the course of the last few
months.

5

