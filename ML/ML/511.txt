Evergreen Classiﬁcation: Exploring New Features

Shailesh Bavadekar
shail@stanford.edu

Introduction

Data

The advent of internet and social networking has
made it very easy to create and distribute content.
Discovering the most informative and engaging doc-
uments has become a needle in the haystack problem
for the consumers. We can divide user information
need in two broad classes:

1. Newsy, ephemeral: This class contains docu-
ments that are time sensitive and ephemeral.
Typical examples are news articles, classiﬁed
listings, blog posts about current or local events.
Typically these documents receive a big, short-
lived traﬃc spike. For example, a sports analyst
may write an article about possible outcome of a
game, which becomes almost instantly obsolete
as the game concludes.

2. Evergreen: These are documents that endure the

test of time. Typical examples are:

StumbleUpon and Kaggle provided a training set con-
taining 7395 web documents, and a test set with hid-
den labels containing 3171 documents. Following is
a summary of features available in this data set.

• Text content features: Full HTML source of the

pages.

• Document structural features: Ratios of tags vs
text, image vs text, spelling error counts, URL
word counts.

• Derived features: compressed document size (as
an approximate indicator of redundancy), docu-
ment category, number of outgoing links in the
page, linkword score (number of document words
that appear in links) etc.

I also further divided the training set to create cross
validation and test sets to measure the performance
of various classiﬁers.

• Well-written, informative articles that tell

the users how something works.

• High quality humor or opinion pieces. Of-
ten these articles may refer to speciﬁc news
event at the time of publication, but they
may provide a broader perspective that re-
mains of interest well after the event.

• Literature, arts and entertainment articles.

These can be truly timeless.

Naive Bayes Classiﬁers

The dataset contained both pre-ﬁltered text (without
HTML markup) as well as the raw HTML content of
each page. These were converted into bag-of-words
document models. This process involves many al-
gorithmic and parametric choices such as choice of
stemming algorithms, stop words, unigram and bi-
gram terms etc. I tried a few variants with succes-
sively better naive bayes classiﬁcation performance.

A robust evergreen content classiﬁer can alleviate this
content discovery problem. This report explores ev-
ergreen classiﬁcation using the data set provided by
Kaggle StumbleUpon evergreen competition.

NB-1 Classiﬁer

This classiﬁer was built following features:

1

Evergreen Classiﬁcation: Exploring New Features

Shailesh Bavadekar
shail@stanford.edu

Introduction

Data

The advent of internet and social networking has
made it very easy to create and distribute content.
Discovering the most informative and engaging doc-
uments has become a needle in the haystack problem
for the consumers. We can divide user information
need in two broad classes:

1. Newsy, ephemeral: This class contains docu-
ments that are time sensitive and ephemeral.
Typical examples are news articles, classiﬁed
listings, blog posts about current or local events.
Typically these documents receive a big, short-
lived traﬃc spike. For example, a sports analyst
may write an article about possible outcome of a
game, which becomes almost instantly obsolete
as the game concludes.

2. Evergreen: These are documents that endure the

test of time. Typical examples are:

StumbleUpon and Kaggle provided a training set con-
taining 7395 web documents, and a test set with hid-
den labels containing 3171 documents. Following is
a summary of features available in this data set.

• Text content features: Full HTML source of the

pages.

• Document structural features: Ratios of tags vs
text, image vs text, spelling error counts, URL
word counts.

• Derived features: compressed document size (as
an approximate indicator of redundancy), docu-
ment category, number of outgoing links in the
page, linkword score (number of document words
that appear in links) etc.

I also further divided the training set to create cross
validation and test sets to measure the performance
of various classiﬁers.

• Well-written, informative articles that tell

the users how something works.

• High quality humor or opinion pieces. Of-
ten these articles may refer to speciﬁc news
event at the time of publication, but they
may provide a broader perspective that re-
mains of interest well after the event.

• Literature, arts and entertainment articles.

These can be truly timeless.

Naive Bayes Classiﬁers

The dataset contained both pre-ﬁltered text (without
HTML markup) as well as the raw HTML content of
each page. These were converted into bag-of-words
document models. This process involves many al-
gorithmic and parametric choices such as choice of
stemming algorithms, stop words, unigram and bi-
gram terms etc. I tried a few variants with succes-
sively better naive bayes classiﬁcation performance.

A robust evergreen content classiﬁer can alleviate this
content discovery problem. This report explores ev-
ergreen classiﬁcation using the data set provided by
Kaggle StumbleUpon evergreen competition.

NB-1 Classiﬁer

This classiﬁer was built following features:

1

Evergreen
egg, cream, food, use, chees, into, sugar, thi,
my, add, butter, 4, minut, chocol, cook, make,
your, until, bake, or, cup, 2, recip, you

Ephemeral
he, hi, sport, fashion, news, video, said, imag,
s, game, ha, year, new, technolog, world,
who, she, their, team, her, app, show, design,
model, ﬂashvar

Table 1: Words most indicative of evergreen and ephemercal classes

• Title, URL and body text ﬁelds processed using
porter stemmping, converted to unigram and bi-
gram BoW models.

• Unigram and bigram IDF scores were computed
. Terms with low IDF scores (thresh-

N

as log10
old 0.2) were ﬁltered.

DFt

• Laplace smoothing was used to handle unseen

terms.

Table 1 shows top 25 stemmed words that are most
indicative of evergreen and ephemeral classes. Figure
1 shows the learning curves for this classiﬁer.

Figure 1: NB-1 Learning Curve

naive bayes, I bucketized these features and calcu-
lated the bucket likelihoods under naive bayes as-
sumption. However the resulting classiﬁer showed
negligible improvement in accuracy.

This is in agreement with the learning curve in Fig-
ure 1 which suggests a high bias setting. To further
conﬁrm this, I also developed an SVM classiﬁer with
just the quantitative features and it had a relatively
poor performance with approximately 62 percent ac-
curacy. This shows that the raw quantitative features
provided in the StumbleUpon dataset have insuﬃ-
cient variance to predict the classes.

Finally, the distribution of tokens in the Table 1 is
also very instructive. Many of the tokens (e.g. new,
news, world, year etc) that are strongly correlated
with the ephemeral class are commonly found in news
reports. On the other hand, the evergreen category
appears to be dominated by terms related to food
and recipes. Approximately 71% of the training doc-
uments labelled as evergreen contain one of the terms
’food’, ’recipe’ or ’cook’, while only 16% of ephemeral
documents contain these terms. This clearly seems
to be an artifact of the sampling process that created
the dataset and deserves further investigation.

Logistic Regression Classiﬁers

Logistic regression models typically work better than
Naive Bayes in high bias scenarios. In this section we
will explore the feature engineering and performance
of regularized logistic regression classiﬁers.

NB-2 Classiﬁer

The NB-1 classiﬁer used only textual features. The
StumbleUpon data set also contains quantitative fea-
tures such as compression ratio, spelling error ratio,
image ratio etc.
In order to use these features in

TF-IDF Features

• This classiﬁer employed the most commonly
used TF-IDF functions. Speciﬁcally, term fre-
quency was squashed using log(tf) so that longer

2

01000200030004000500060000.10.150.20.250.30.350.40.450.5Training set sizeError rate  TrainingTestEvergreen Classiﬁcation: Exploring New Features

Shailesh Bavadekar
shail@stanford.edu

Introduction

Data

The advent of internet and social networking has
made it very easy to create and distribute content.
Discovering the most informative and engaging doc-
uments has become a needle in the haystack problem
for the consumers. We can divide user information
need in two broad classes:

1. Newsy, ephemeral: This class contains docu-
ments that are time sensitive and ephemeral.
Typical examples are news articles, classiﬁed
listings, blog posts about current or local events.
Typically these documents receive a big, short-
lived traﬃc spike. For example, a sports analyst
may write an article about possible outcome of a
game, which becomes almost instantly obsolete
as the game concludes.

2. Evergreen: These are documents that endure the

test of time. Typical examples are:

StumbleUpon and Kaggle provided a training set con-
taining 7395 web documents, and a test set with hid-
den labels containing 3171 documents. Following is
a summary of features available in this data set.

• Text content features: Full HTML source of the

pages.

• Document structural features: Ratios of tags vs
text, image vs text, spelling error counts, URL
word counts.

• Derived features: compressed document size (as
an approximate indicator of redundancy), docu-
ment category, number of outgoing links in the
page, linkword score (number of document words
that appear in links) etc.

I also further divided the training set to create cross
validation and test sets to measure the performance
of various classiﬁers.

• Well-written, informative articles that tell

the users how something works.

• High quality humor or opinion pieces. Of-
ten these articles may refer to speciﬁc news
event at the time of publication, but they
may provide a broader perspective that re-
mains of interest well after the event.

• Literature, arts and entertainment articles.

These can be truly timeless.

Naive Bayes Classiﬁers

The dataset contained both pre-ﬁltered text (without
HTML markup) as well as the raw HTML content of
each page. These were converted into bag-of-words
document models. This process involves many al-
gorithmic and parametric choices such as choice of
stemming algorithms, stop words, unigram and bi-
gram terms etc. I tried a few variants with succes-
sively better naive bayes classiﬁcation performance.

A robust evergreen content classiﬁer can alleviate this
content discovery problem. This report explores ev-
ergreen classiﬁcation using the data set provided by
Kaggle StumbleUpon evergreen competition.

NB-1 Classiﬁer

This classiﬁer was built following features:

1

Evergreen
egg, cream, food, use, chees, into, sugar, thi,
my, add, butter, 4, minut, chocol, cook, make,
your, until, bake, or, cup, 2, recip, you

Ephemeral
he, hi, sport, fashion, news, video, said, imag,
s, game, ha, year, new, technolog, world,
who, she, their, team, her, app, show, design,
model, ﬂashvar

Table 1: Words most indicative of evergreen and ephemercal classes

• Title, URL and body text ﬁelds processed using
porter stemmping, converted to unigram and bi-
gram BoW models.

• Unigram and bigram IDF scores were computed
. Terms with low IDF scores (thresh-

N

as log10
old 0.2) were ﬁltered.

DFt

• Laplace smoothing was used to handle unseen

terms.

Table 1 shows top 25 stemmed words that are most
indicative of evergreen and ephemeral classes. Figure
1 shows the learning curves for this classiﬁer.

Figure 1: NB-1 Learning Curve

naive bayes, I bucketized these features and calcu-
lated the bucket likelihoods under naive bayes as-
sumption. However the resulting classiﬁer showed
negligible improvement in accuracy.

This is in agreement with the learning curve in Fig-
ure 1 which suggests a high bias setting. To further
conﬁrm this, I also developed an SVM classiﬁer with
just the quantitative features and it had a relatively
poor performance with approximately 62 percent ac-
curacy. This shows that the raw quantitative features
provided in the StumbleUpon dataset have insuﬃ-
cient variance to predict the classes.

Finally, the distribution of tokens in the Table 1 is
also very instructive. Many of the tokens (e.g. new,
news, world, year etc) that are strongly correlated
with the ephemeral class are commonly found in news
reports. On the other hand, the evergreen category
appears to be dominated by terms related to food
and recipes. Approximately 71% of the training doc-
uments labelled as evergreen contain one of the terms
’food’, ’recipe’ or ’cook’, while only 16% of ephemeral
documents contain these terms. This clearly seems
to be an artifact of the sampling process that created
the dataset and deserves further investigation.

Logistic Regression Classiﬁers

Logistic regression models typically work better than
Naive Bayes in high bias scenarios. In this section we
will explore the feature engineering and performance
of regularized logistic regression classiﬁers.

NB-2 Classiﬁer

The NB-1 classiﬁer used only textual features. The
StumbleUpon data set also contains quantitative fea-
tures such as compression ratio, spelling error ratio,
image ratio etc.
In order to use these features in

TF-IDF Features

• This classiﬁer employed the most commonly
used TF-IDF functions. Speciﬁcally, term fre-
quency was squashed using log(tf) so that longer

2

01000200030004000500060000.10.150.20.250.30.350.40.450.5Training set sizeError rate  TrainingTestdocuments do not get an unfair inﬂuence.

• Unigram and bigram IDF scores were computed

as log10

N

DFt

.

• The lexicon contains 1.5 million distinct uni-
grams and bigrams (after stemming) drawn from
a training corpus containing 6 million tokens.

LR-1 Classiﬁer

Due to large number of features and moderately large
number of training documents, I used the liblinear lo-
gistic regression with L2 regularization. As the ﬁgure
2 shows, this implementation largely ﬁxes the high
bias limitation of the NB-1 classiﬁer.

Figure 2: LR-1 Learning Curve

LR-2 Classiﬁer: Visual Page Appear-
ance Features

Since logistic regression has more capacity than naive
bayes, it should be possible to add more features and
improve the overall performance of the classiﬁer. One
way to do this is using features that represent the
visual appearance of the page. We hypothesize that it
is possible for human raters to predict the usefulness
of a web page based only on visual appearance of the
page without delving into the textual content.

3

For example, text legibility, placement of images
and videos, background color, choice of fonts and col-
ors, etc can have a big impact on the overall quality
and utility of a webpage. Furthermore, these features
should be independent of the textual features in the
bag-of-words models used in NB-1 and LR-1 classi-
ﬁers. To explore this hypothesis I developed LR-2, a
logistic regression classiﬁer that uses only the visual
features.

The feature extraction pipeline contains following

components.

• The Selenium WebDriver project provides a way
to automate browser interaction. This can be
used to automate crawling and browser screen-
shot generation.

• The X.org project provides a tool called Xvfb, a
virtual framebuﬀer server that implements the X
window protocol. Xvfb emulates a framebuﬀer
without any physical display hardware.
It can
also create framebuﬀers of arbitrary dimensions
and depth. Most computer displays do not have
enough vertical pixels to render a full webpage.
Using 1000x8000x24 framebuﬀer I was able to
create full page screenshots for most training and
test web pages.

• These screenshot images were then scaled to a
much more manageable size of 330x1959 to re-
duce the amount of computation.

• The ﬁnal stage converts the images to greyscale
and generates Histogram of Oriented Gradients
(HoG) features. HoG features have been suc-
cessfully used for object recognition tasks.

Figure 3 shows the information represented by the
HoG feature. The shape of images and text, the lay-
out of the page can be discerned from the image.
The LR-2 classiﬁer used just the HoG feature vector
to classify the documents. The Figure 4 shows the
learning curve for this classiﬁer. The accuracy is pre-
dictably low, however it provides an intuition about
the capacity of this approach to detect evergreen doc-
uments.

010002000300040005000600000.050.10.150.20.250.30.35Training set sizeError rate  TrainingTestEvergreen Classiﬁcation: Exploring New Features

Shailesh Bavadekar
shail@stanford.edu

Introduction

Data

The advent of internet and social networking has
made it very easy to create and distribute content.
Discovering the most informative and engaging doc-
uments has become a needle in the haystack problem
for the consumers. We can divide user information
need in two broad classes:

1. Newsy, ephemeral: This class contains docu-
ments that are time sensitive and ephemeral.
Typical examples are news articles, classiﬁed
listings, blog posts about current or local events.
Typically these documents receive a big, short-
lived traﬃc spike. For example, a sports analyst
may write an article about possible outcome of a
game, which becomes almost instantly obsolete
as the game concludes.

2. Evergreen: These are documents that endure the

test of time. Typical examples are:

StumbleUpon and Kaggle provided a training set con-
taining 7395 web documents, and a test set with hid-
den labels containing 3171 documents. Following is
a summary of features available in this data set.

• Text content features: Full HTML source of the

pages.

• Document structural features: Ratios of tags vs
text, image vs text, spelling error counts, URL
word counts.

• Derived features: compressed document size (as
an approximate indicator of redundancy), docu-
ment category, number of outgoing links in the
page, linkword score (number of document words
that appear in links) etc.

I also further divided the training set to create cross
validation and test sets to measure the performance
of various classiﬁers.

• Well-written, informative articles that tell

the users how something works.

• High quality humor or opinion pieces. Of-
ten these articles may refer to speciﬁc news
event at the time of publication, but they
may provide a broader perspective that re-
mains of interest well after the event.

• Literature, arts and entertainment articles.

These can be truly timeless.

Naive Bayes Classiﬁers

The dataset contained both pre-ﬁltered text (without
HTML markup) as well as the raw HTML content of
each page. These were converted into bag-of-words
document models. This process involves many al-
gorithmic and parametric choices such as choice of
stemming algorithms, stop words, unigram and bi-
gram terms etc. I tried a few variants with succes-
sively better naive bayes classiﬁcation performance.

A robust evergreen content classiﬁer can alleviate this
content discovery problem. This report explores ev-
ergreen classiﬁcation using the data set provided by
Kaggle StumbleUpon evergreen competition.

NB-1 Classiﬁer

This classiﬁer was built following features:

1

Evergreen
egg, cream, food, use, chees, into, sugar, thi,
my, add, butter, 4, minut, chocol, cook, make,
your, until, bake, or, cup, 2, recip, you

Ephemeral
he, hi, sport, fashion, news, video, said, imag,
s, game, ha, year, new, technolog, world,
who, she, their, team, her, app, show, design,
model, ﬂashvar

Table 1: Words most indicative of evergreen and ephemercal classes

• Title, URL and body text ﬁelds processed using
porter stemmping, converted to unigram and bi-
gram BoW models.

• Unigram and bigram IDF scores were computed
. Terms with low IDF scores (thresh-

N

as log10
old 0.2) were ﬁltered.

DFt

• Laplace smoothing was used to handle unseen

terms.

Table 1 shows top 25 stemmed words that are most
indicative of evergreen and ephemeral classes. Figure
1 shows the learning curves for this classiﬁer.

Figure 1: NB-1 Learning Curve

naive bayes, I bucketized these features and calcu-
lated the bucket likelihoods under naive bayes as-
sumption. However the resulting classiﬁer showed
negligible improvement in accuracy.

This is in agreement with the learning curve in Fig-
ure 1 which suggests a high bias setting. To further
conﬁrm this, I also developed an SVM classiﬁer with
just the quantitative features and it had a relatively
poor performance with approximately 62 percent ac-
curacy. This shows that the raw quantitative features
provided in the StumbleUpon dataset have insuﬃ-
cient variance to predict the classes.

Finally, the distribution of tokens in the Table 1 is
also very instructive. Many of the tokens (e.g. new,
news, world, year etc) that are strongly correlated
with the ephemeral class are commonly found in news
reports. On the other hand, the evergreen category
appears to be dominated by terms related to food
and recipes. Approximately 71% of the training doc-
uments labelled as evergreen contain one of the terms
’food’, ’recipe’ or ’cook’, while only 16% of ephemeral
documents contain these terms. This clearly seems
to be an artifact of the sampling process that created
the dataset and deserves further investigation.

Logistic Regression Classiﬁers

Logistic regression models typically work better than
Naive Bayes in high bias scenarios. In this section we
will explore the feature engineering and performance
of regularized logistic regression classiﬁers.

NB-2 Classiﬁer

The NB-1 classiﬁer used only textual features. The
StumbleUpon data set also contains quantitative fea-
tures such as compression ratio, spelling error ratio,
image ratio etc.
In order to use these features in

TF-IDF Features

• This classiﬁer employed the most commonly
used TF-IDF functions. Speciﬁcally, term fre-
quency was squashed using log(tf) so that longer

2

01000200030004000500060000.10.150.20.250.30.350.40.450.5Training set sizeError rate  TrainingTestdocuments do not get an unfair inﬂuence.

• Unigram and bigram IDF scores were computed

as log10

N

DFt

.

• The lexicon contains 1.5 million distinct uni-
grams and bigrams (after stemming) drawn from
a training corpus containing 6 million tokens.

LR-1 Classiﬁer

Due to large number of features and moderately large
number of training documents, I used the liblinear lo-
gistic regression with L2 regularization. As the ﬁgure
2 shows, this implementation largely ﬁxes the high
bias limitation of the NB-1 classiﬁer.

Figure 2: LR-1 Learning Curve

LR-2 Classiﬁer: Visual Page Appear-
ance Features

Since logistic regression has more capacity than naive
bayes, it should be possible to add more features and
improve the overall performance of the classiﬁer. One
way to do this is using features that represent the
visual appearance of the page. We hypothesize that it
is possible for human raters to predict the usefulness
of a web page based only on visual appearance of the
page without delving into the textual content.

3

For example, text legibility, placement of images
and videos, background color, choice of fonts and col-
ors, etc can have a big impact on the overall quality
and utility of a webpage. Furthermore, these features
should be independent of the textual features in the
bag-of-words models used in NB-1 and LR-1 classi-
ﬁers. To explore this hypothesis I developed LR-2, a
logistic regression classiﬁer that uses only the visual
features.

The feature extraction pipeline contains following

components.

• The Selenium WebDriver project provides a way
to automate browser interaction. This can be
used to automate crawling and browser screen-
shot generation.

• The X.org project provides a tool called Xvfb, a
virtual framebuﬀer server that implements the X
window protocol. Xvfb emulates a framebuﬀer
without any physical display hardware.
It can
also create framebuﬀers of arbitrary dimensions
and depth. Most computer displays do not have
enough vertical pixels to render a full webpage.
Using 1000x8000x24 framebuﬀer I was able to
create full page screenshots for most training and
test web pages.

• These screenshot images were then scaled to a
much more manageable size of 330x1959 to re-
duce the amount of computation.

• The ﬁnal stage converts the images to greyscale
and generates Histogram of Oriented Gradients
(HoG) features. HoG features have been suc-
cessfully used for object recognition tasks.

Figure 3 shows the information represented by the
HoG feature. The shape of images and text, the lay-
out of the page can be discerned from the image.
The LR-2 classiﬁer used just the HoG feature vector
to classify the documents. The Figure 4 shows the
learning curve for this classiﬁer. The accuracy is pre-
dictably low, however it provides an intuition about
the capacity of this approach to detect evergreen doc-
uments.

010002000300040005000600000.050.10.150.20.250.30.35Training set sizeError rate  TrainingTestFigure 3: Visualizing HoG features

Figure 5: LR-3 Learning Curve

• The HoG features performed better than the
quantitative features provided with the Kag-
gle/StumbleUpon dataset. Using combination
of TF-IDF and screenshot HoG features seems
promising but more training data will be re-
quired to translate that into improved perfor-
mance.

Future Work

There is clearly some headroom for improving the
performance on the Kaggle / StumbleUpon dataset.
Following are some ideas that are likely to improve
the results.

• I developed a new TF-IDF scoring library for
this exercise. Using the standard TfIdfVectorizer
class in python sklearn package yielded slightly
better results. This is clearly worth exploring
further.
In general, information retrieval liter-
ature suggests many ideas for improving term
scoring such as document length normalization,
using diﬀerent weights for title, URL and body
hits.

• Recent work on vector space representation of
words can be applied to this problem. The BoW
models can be mapped into a higher dimensional

Figure 4: LR-2 Learning Curve

LR-3 Classiﬁer: Text + HoG

The last model in this exploration is a logistic re-
gression model using a combination of textual and
HoG features. The ﬁgure 5 shows the learning curve
for LR-3. It is possible that with more training data
LR-3 could continue converging further than LR-2
but the results are inconclusive.

Conclusions

• The accuracy of all classiﬁers converged to
around 80%. Adding more features and better
sampled training data should improve the per-
formance.

4

Input imageHistogram of Oriented Gradients010002000300040005000600000.050.10.150.20.250.30.350.4Training set sizeError rate  TrainingTest010002000300040005000600000.050.10.150.20.250.30.35Training set sizeError rate  TrainingText−onlyText + HoGEvergreen Classiﬁcation: Exploring New Features

Shailesh Bavadekar
shail@stanford.edu

Introduction

Data

The advent of internet and social networking has
made it very easy to create and distribute content.
Discovering the most informative and engaging doc-
uments has become a needle in the haystack problem
for the consumers. We can divide user information
need in two broad classes:

1. Newsy, ephemeral: This class contains docu-
ments that are time sensitive and ephemeral.
Typical examples are news articles, classiﬁed
listings, blog posts about current or local events.
Typically these documents receive a big, short-
lived traﬃc spike. For example, a sports analyst
may write an article about possible outcome of a
game, which becomes almost instantly obsolete
as the game concludes.

2. Evergreen: These are documents that endure the

test of time. Typical examples are:

StumbleUpon and Kaggle provided a training set con-
taining 7395 web documents, and a test set with hid-
den labels containing 3171 documents. Following is
a summary of features available in this data set.

• Text content features: Full HTML source of the

pages.

• Document structural features: Ratios of tags vs
text, image vs text, spelling error counts, URL
word counts.

• Derived features: compressed document size (as
an approximate indicator of redundancy), docu-
ment category, number of outgoing links in the
page, linkword score (number of document words
that appear in links) etc.

I also further divided the training set to create cross
validation and test sets to measure the performance
of various classiﬁers.

• Well-written, informative articles that tell

the users how something works.

• High quality humor or opinion pieces. Of-
ten these articles may refer to speciﬁc news
event at the time of publication, but they
may provide a broader perspective that re-
mains of interest well after the event.

• Literature, arts and entertainment articles.

These can be truly timeless.

Naive Bayes Classiﬁers

The dataset contained both pre-ﬁltered text (without
HTML markup) as well as the raw HTML content of
each page. These were converted into bag-of-words
document models. This process involves many al-
gorithmic and parametric choices such as choice of
stemming algorithms, stop words, unigram and bi-
gram terms etc. I tried a few variants with succes-
sively better naive bayes classiﬁcation performance.

A robust evergreen content classiﬁer can alleviate this
content discovery problem. This report explores ev-
ergreen classiﬁcation using the data set provided by
Kaggle StumbleUpon evergreen competition.

NB-1 Classiﬁer

This classiﬁer was built following features:

1

Evergreen
egg, cream, food, use, chees, into, sugar, thi,
my, add, butter, 4, minut, chocol, cook, make,
your, until, bake, or, cup, 2, recip, you

Ephemeral
he, hi, sport, fashion, news, video, said, imag,
s, game, ha, year, new, technolog, world,
who, she, their, team, her, app, show, design,
model, ﬂashvar

Table 1: Words most indicative of evergreen and ephemercal classes

• Title, URL and body text ﬁelds processed using
porter stemmping, converted to unigram and bi-
gram BoW models.

• Unigram and bigram IDF scores were computed
. Terms with low IDF scores (thresh-

N

as log10
old 0.2) were ﬁltered.

DFt

• Laplace smoothing was used to handle unseen

terms.

Table 1 shows top 25 stemmed words that are most
indicative of evergreen and ephemeral classes. Figure
1 shows the learning curves for this classiﬁer.

Figure 1: NB-1 Learning Curve

naive bayes, I bucketized these features and calcu-
lated the bucket likelihoods under naive bayes as-
sumption. However the resulting classiﬁer showed
negligible improvement in accuracy.

This is in agreement with the learning curve in Fig-
ure 1 which suggests a high bias setting. To further
conﬁrm this, I also developed an SVM classiﬁer with
just the quantitative features and it had a relatively
poor performance with approximately 62 percent ac-
curacy. This shows that the raw quantitative features
provided in the StumbleUpon dataset have insuﬃ-
cient variance to predict the classes.

Finally, the distribution of tokens in the Table 1 is
also very instructive. Many of the tokens (e.g. new,
news, world, year etc) that are strongly correlated
with the ephemeral class are commonly found in news
reports. On the other hand, the evergreen category
appears to be dominated by terms related to food
and recipes. Approximately 71% of the training doc-
uments labelled as evergreen contain one of the terms
’food’, ’recipe’ or ’cook’, while only 16% of ephemeral
documents contain these terms. This clearly seems
to be an artifact of the sampling process that created
the dataset and deserves further investigation.

Logistic Regression Classiﬁers

Logistic regression models typically work better than
Naive Bayes in high bias scenarios. In this section we
will explore the feature engineering and performance
of regularized logistic regression classiﬁers.

NB-2 Classiﬁer

The NB-1 classiﬁer used only textual features. The
StumbleUpon data set also contains quantitative fea-
tures such as compression ratio, spelling error ratio,
image ratio etc.
In order to use these features in

TF-IDF Features

• This classiﬁer employed the most commonly
used TF-IDF functions. Speciﬁcally, term fre-
quency was squashed using log(tf) so that longer

2

01000200030004000500060000.10.150.20.250.30.350.40.450.5Training set sizeError rate  TrainingTestdocuments do not get an unfair inﬂuence.

• Unigram and bigram IDF scores were computed

as log10

N

DFt

.

• The lexicon contains 1.5 million distinct uni-
grams and bigrams (after stemming) drawn from
a training corpus containing 6 million tokens.

LR-1 Classiﬁer

Due to large number of features and moderately large
number of training documents, I used the liblinear lo-
gistic regression with L2 regularization. As the ﬁgure
2 shows, this implementation largely ﬁxes the high
bias limitation of the NB-1 classiﬁer.

Figure 2: LR-1 Learning Curve

LR-2 Classiﬁer: Visual Page Appear-
ance Features

Since logistic regression has more capacity than naive
bayes, it should be possible to add more features and
improve the overall performance of the classiﬁer. One
way to do this is using features that represent the
visual appearance of the page. We hypothesize that it
is possible for human raters to predict the usefulness
of a web page based only on visual appearance of the
page without delving into the textual content.

3

For example, text legibility, placement of images
and videos, background color, choice of fonts and col-
ors, etc can have a big impact on the overall quality
and utility of a webpage. Furthermore, these features
should be independent of the textual features in the
bag-of-words models used in NB-1 and LR-1 classi-
ﬁers. To explore this hypothesis I developed LR-2, a
logistic regression classiﬁer that uses only the visual
features.

The feature extraction pipeline contains following

components.

• The Selenium WebDriver project provides a way
to automate browser interaction. This can be
used to automate crawling and browser screen-
shot generation.

• The X.org project provides a tool called Xvfb, a
virtual framebuﬀer server that implements the X
window protocol. Xvfb emulates a framebuﬀer
without any physical display hardware.
It can
also create framebuﬀers of arbitrary dimensions
and depth. Most computer displays do not have
enough vertical pixels to render a full webpage.
Using 1000x8000x24 framebuﬀer I was able to
create full page screenshots for most training and
test web pages.

• These screenshot images were then scaled to a
much more manageable size of 330x1959 to re-
duce the amount of computation.

• The ﬁnal stage converts the images to greyscale
and generates Histogram of Oriented Gradients
(HoG) features. HoG features have been suc-
cessfully used for object recognition tasks.

Figure 3 shows the information represented by the
HoG feature. The shape of images and text, the lay-
out of the page can be discerned from the image.
The LR-2 classiﬁer used just the HoG feature vector
to classify the documents. The Figure 4 shows the
learning curve for this classiﬁer. The accuracy is pre-
dictably low, however it provides an intuition about
the capacity of this approach to detect evergreen doc-
uments.

010002000300040005000600000.050.10.150.20.250.30.35Training set sizeError rate  TrainingTestFigure 3: Visualizing HoG features

Figure 5: LR-3 Learning Curve

• The HoG features performed better than the
quantitative features provided with the Kag-
gle/StumbleUpon dataset. Using combination
of TF-IDF and screenshot HoG features seems
promising but more training data will be re-
quired to translate that into improved perfor-
mance.

Future Work

There is clearly some headroom for improving the
performance on the Kaggle / StumbleUpon dataset.
Following are some ideas that are likely to improve
the results.

• I developed a new TF-IDF scoring library for
this exercise. Using the standard TfIdfVectorizer
class in python sklearn package yielded slightly
better results. This is clearly worth exploring
further.
In general, information retrieval liter-
ature suggests many ideas for improving term
scoring such as document length normalization,
using diﬀerent weights for title, URL and body
hits.

• Recent work on vector space representation of
words can be applied to this problem. The BoW
models can be mapped into a higher dimensional

Figure 4: LR-2 Learning Curve

LR-3 Classiﬁer: Text + HoG

The last model in this exploration is a logistic re-
gression model using a combination of textual and
HoG features. The ﬁgure 5 shows the learning curve
for LR-3. It is possible that with more training data
LR-3 could continue converging further than LR-2
but the results are inconclusive.

Conclusions

• The accuracy of all classiﬁers converged to
around 80%. Adding more features and better
sampled training data should improve the per-
formance.

4

Input imageHistogram of Oriented Gradients010002000300040005000600000.050.10.150.20.250.30.350.4Training set sizeError rate  TrainingTest010002000300040005000600000.050.10.150.20.250.30.35Training set sizeError rate  TrainingText−onlyText + HoGspace which encapsulates meaning / synonyms of
the words.

• Both the feature generation and training algo-
rithms have many hyperparameters. For exam-
ple, the Histogram of oriented gradients feature
was generated using downscaled images. This
transformation loses information. Using system-
atic cross validation can yield a better ﬁt for
these hyperparameters.

• Using ensemble learning methods should also im-

prove the performance of this sytem.

Even though this dataset has limitations, the ideas
described in this paper translate well to a broader
class of document classiﬁcation problems. Classify-
ing documents based on visual appearance has many
interesting applications. For example, web design is
often conducted in an ad hoc manner in the industry
due to lack of resources. The methods described in
this report can be easily used to quantify utility and
quality of user interfaces.

References

[1] StumbleUpon Evergreen Classiﬁcation Challenge.

https://www.kaggle.com/c/stumbleupon/.

[2] Histograms of Oriented Gradients for Human De-

tection, 2005.

5

