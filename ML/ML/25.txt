1

Predicting Win Percentage and
Winning Features of NBA Teams

CS229 Final Project Report

Evan Giarta, Nattapoom Asavareongchai

I. INTRODUCTION

T He NBA is the premier basketball league of the world.

It is an enormous business, with an estimated revenue of
nearly $5 billion in 2014. Teams within the NBA with more
wins will gain popularity and increase income, beneﬁcial to
the league and its players. Thus, predicting the win percentage
of an NBA team based on the previous performance their
roster is quite valuable to general managers, coaches, players,
fans, gamblers, and statisticians alike. Moreover, knowing
which particular stat or feature is most inﬂuential to winning
games is also desirable. Our project aims to do exactly that.

We built a learning algorithm that takes in a data set of
team features, pre-processed from previous statistics of each
player in the current season’s team roster, and outputs an
estimate of the win percentage of that team during the regular
season. The input of our algorithm is a team feature vector
of the 30 different NBA for several seasons. Each team in
each season is considered one input data point. Each input
data point vector is computed from taking the stats of all the
players in the team combined by averaging or accumulating.
The stats of the players in the team are however taken from
one season prior. For example, if we want to compute the
feature vector for the Golden State Warriors during the
2014-2015 season, we will
the players in the
2014-2015 GSW team, then ﬁnd the statistics of each of these
players during the 2013-2014 season and pre-processed them
to create the required feature vector. The win percentage
output would be the win percentage of the current, or the
2014-2015 win percentage of the GSW in our example.

look at all

The learning algorithm model

that we use is a second
order polynomial ﬁt regression model. Unlike linear regression
where the feature vector is used as it is, all the elements of our
input feature vector will ﬁrst be square and both the original
value and the squared value are used as the ﬁnal input features
within the input vector. For example, we have 31 input features
in our vector (without squaring). Before inputting this into
our learning model, we will square the features to create a
62 feature vector. This vector includes the original 31 features
plus the squared of each of the 31 features.

Input F eature vector x(i) = [x1, x2, ..., x31, x2
Output W in P ercentage = y(i)

31]T

1, x2

2, ..., x2

For i = 1, ..., n with n = dataset size

II. RELATED WORKS

There have been many previous

research papers on
predicting NBA team wins. However, most of them tries
to predict wins of each individual NBA games, then sum
them up to then create the win percentage of a season. Two
papers, one by Bernard, L et al. (2009)[1] and the other by
Pedro et al. (2012)[2] used network based algorithm to predict
NBA wins. Because mere statistical features do not produce
as well of a result (as can be seen in our model results) a
network based feature model can deﬁnitely improve learning
algorithms. More speciﬁcally, Bernard, L et al. used neural
networks with feature selection to predict NBA games. In our
opinion, this is a very good model to use, since it takes in
many sophisticated nature of many features (ones we weren’t
able to incorporate into our model).

Another

interesting model done by Na Wei

(2011)[3]
utilizes Naive Bayes to predict playoff records from regular
season stats. Even though this is a different problem, the
nature of the problem is similar. The use of Naive Bayes
however is not a very good approach. Since the model relies
on a lot of assumptions, the model is not as accurate.

Online, statistical resources for sports fans span an enor-
mous spectrum. Basketball aﬁcionados of all kinds and skills
can enjoy interacting with the Buckets visualized shot chart
from BBall Breakdown[4] as well as studying the complex
decision making process of Markov state machines on Big
League Insights[5] as applied to the game. Using data from
thousands of basketball plays, the model analyzes and predicts
how multiple in-game states and transitions inﬂuence each
other and the overall outcome of a game.

III. DATASET AND FEATURES

A. Data Sources

We collected our data from two main sources. The
ﬁrst main source where we collected most player statis-
tics during each seasons, as well as team statistics (team
roster data of each seasons and team win percentage
of each seasons),
the main NBA statistics website
http://stats.nba.com/league/player/. The second source that we
obtained our feature stats from, especially the team salaries
stats is https://www.eskimo.com/ pbender/.

is

1

Predicting Win Percentage and
Winning Features of NBA Teams

CS229 Final Project Report

Evan Giarta, Nattapoom Asavareongchai

I. INTRODUCTION

T He NBA is the premier basketball league of the world.

It is an enormous business, with an estimated revenue of
nearly $5 billion in 2014. Teams within the NBA with more
wins will gain popularity and increase income, beneﬁcial to
the league and its players. Thus, predicting the win percentage
of an NBA team based on the previous performance their
roster is quite valuable to general managers, coaches, players,
fans, gamblers, and statisticians alike. Moreover, knowing
which particular stat or feature is most inﬂuential to winning
games is also desirable. Our project aims to do exactly that.

We built a learning algorithm that takes in a data set of
team features, pre-processed from previous statistics of each
player in the current season’s team roster, and outputs an
estimate of the win percentage of that team during the regular
season. The input of our algorithm is a team feature vector
of the 30 different NBA for several seasons. Each team in
each season is considered one input data point. Each input
data point vector is computed from taking the stats of all the
players in the team combined by averaging or accumulating.
The stats of the players in the team are however taken from
one season prior. For example, if we want to compute the
feature vector for the Golden State Warriors during the
2014-2015 season, we will
the players in the
2014-2015 GSW team, then ﬁnd the statistics of each of these
players during the 2013-2014 season and pre-processed them
to create the required feature vector. The win percentage
output would be the win percentage of the current, or the
2014-2015 win percentage of the GSW in our example.

look at all

The learning algorithm model

that we use is a second
order polynomial ﬁt regression model. Unlike linear regression
where the feature vector is used as it is, all the elements of our
input feature vector will ﬁrst be square and both the original
value and the squared value are used as the ﬁnal input features
within the input vector. For example, we have 31 input features
in our vector (without squaring). Before inputting this into
our learning model, we will square the features to create a
62 feature vector. This vector includes the original 31 features
plus the squared of each of the 31 features.

Input F eature vector x(i) = [x1, x2, ..., x31, x2
Output W in P ercentage = y(i)

31]T

1, x2

2, ..., x2

For i = 1, ..., n with n = dataset size

II. RELATED WORKS

There have been many previous

research papers on
predicting NBA team wins. However, most of them tries
to predict wins of each individual NBA games, then sum
them up to then create the win percentage of a season. Two
papers, one by Bernard, L et al. (2009)[1] and the other by
Pedro et al. (2012)[2] used network based algorithm to predict
NBA wins. Because mere statistical features do not produce
as well of a result (as can be seen in our model results) a
network based feature model can deﬁnitely improve learning
algorithms. More speciﬁcally, Bernard, L et al. used neural
networks with feature selection to predict NBA games. In our
opinion, this is a very good model to use, since it takes in
many sophisticated nature of many features (ones we weren’t
able to incorporate into our model).

Another

interesting model done by Na Wei

(2011)[3]
utilizes Naive Bayes to predict playoff records from regular
season stats. Even though this is a different problem, the
nature of the problem is similar. The use of Naive Bayes
however is not a very good approach. Since the model relies
on a lot of assumptions, the model is not as accurate.

Online, statistical resources for sports fans span an enor-
mous spectrum. Basketball aﬁcionados of all kinds and skills
can enjoy interacting with the Buckets visualized shot chart
from BBall Breakdown[4] as well as studying the complex
decision making process of Markov state machines on Big
League Insights[5] as applied to the game. Using data from
thousands of basketball plays, the model analyzes and predicts
how multiple in-game states and transitions inﬂuence each
other and the overall outcome of a game.

III. DATASET AND FEATURES

A. Data Sources

We collected our data from two main sources. The
ﬁrst main source where we collected most player statis-
tics during each seasons, as well as team statistics (team
roster data of each seasons and team win percentage
of each seasons),
the main NBA statistics website
http://stats.nba.com/league/player/. The second source that we
obtained our feature stats from, especially the team salaries
stats is https://www.eskimo.com/ pbender/.

is

B. Data set pre-processing

As described in the introduction, our input data is the set
of team statistics for each season taken from player roster
statistics from the season before. The raw data we obtained
are therefore every player in the league during the 2004-2005
season to the 2013-2014 season. We then pre-processed this
to obtain 30 team feature statistics for the year 2005-2006 to
2014-2015.

These features include:

16) Rebounds (REB)
1) Age
17) Assists (AST)
2) Games Played (GP)
18) Turnovers (TOV)
3) Wins (W)
19) Steals (STL)
4) Minutes (MIN)
20) Blocks (BLK)
5) Field goals Made (FGM)
21) Personal Fouls (PF)
6) Field goals Attempted (FGA)
22) Double-double (DD2)
7) Field goal % (FG%)
23) Triple-doubles (TP3)
8) 3-points Made (3PM)
24) Points (PTS)
9) 3-points attempted (3PA)
25) Efﬁciency (+/-)
10) 3-points % (3P%)
26) # of rookies
11) Free-throws Made (FTM)
27) # of allstars
12) Free-throws attempted (FTA)
28) # of Rebounders
13) Free-throws % (FT%)
29) # of Double-doubles
14) Offensive rebounds (OREB)
30) # of Triple-doubles
15) Defensive rebounds (DREB)
The Number of allstars is deﬁned as the number of players
with points per game of 1 standard deviation above the mean
of a season. Similarly, number of rebounders is the number
of players with rebounds higher than 1 standard deviation.
Likewise with number of double-doubles and triple-doubles.

The 1st feature up to the 25th feature are obtained from
averaging the stats of each player in the team per season
together. The last 5 features are obtained from counting the
players in the team per season with the feature requirements.

We then added the last or 31st feature, called team salaries,
using the team’s available salaries during the current season (so
from 2005-2006 to 2014-2015 seasons). All of the 31 features
are then normalized per season by whitening. With a total of
10 seasons and 30 teams, we have a total of 300 data points
in our data set.

C. Extra Data Manipulation

We tried to run PCA or Principle Component Analysis to
remove any dependent features. A new feature vector, with
reduced dimension/size, is created. The size of the new feature
was determined by how many dominant eigenvalues of the
i=1 x(i)x(i)T there is
empirical co-variance matrix Σ = 1
m
(where m = the number of train samples or data points and
x(i) is the input feature vector of data point i).

(cid:80)m

Suppose there are k dominant eigenvalues (k < 31), then

the new input feature will be:

2





uT
1 x(i)
2 x(i)
uT
...
uT
k x(i)

z(i) =

Where uj are the eigen-vector corresponding to the jth
eigenvalue of Σ

However, when the learning model is run using the reduced
feature vector from PCA versus the original feature vector,
there was no improvements (the generalized error even went
up) and so we decided not to use the reduced vector from PCA.
Instead, we decided to reduce the feature vector using feature
selection, more speciﬁcally backward search. This method
proves to give us a better result. We will describe how we
did feature selection in more detail in the methods section.

IV. METHOD

The learning algorithm that we chose to use for our
learning model is the second order polynomial ﬁt regression
model. However, before we decided on this model, we ran
our data through three different regression models,
linear
regression, locally weighted linear regression and polynomial
ﬁt (with different orders). We validated each method/model
using k-fold validation and chose the model that gave the
best generalization error (lowest RMS error of the test set).

For each regression model, we measured their performances
by measuring the cost function J(θ) of both the training set
and the test set. The cost function we used is deﬁned as
follows:

J(θ) =

(hθ(x(i)) − y(i))2

i=1

For locally weighted regression, we varied the parameter τ
between τ = 10 and τ = 40. For polynomial ﬁt, we ran our
data through second order and third order polynomial ﬁts.

Our data-set includes 300 data points, 30 teams during 10
seasons. We decided to validate our learning algorithm by
separating this data-set into the training set and the test set.
We separated 270 data samples as the training set and the
other 30 as the test set. For simplicity we used one season
as the 30 data test set and the other seasons as data to train.
Because of the nature of how we separated the data and
because there are 10 seasons, we decided to use 10-fold
validation. Therefore, each season becomes a test set once.
The pseudo-code for this 10-fold validation then becomes:

1) Split the data into 10 disjoint sets, S1, S2, ..., S10, where

2) For j = 1, ...,10:

each set contains data from each of the 10 seasons.
Train the model M on S1 ∪ ... ∪ Sj−1 ∪ Sj+1 ∪ ... ∪ S10
(train the model on every set except set Sj) to obtain
the hypothesis hj. Then test the hypothesis on Sj to get
the generalization error ˆ
j.

(cid:118)(cid:117)(cid:117)(cid:116) m(cid:88)

1

Predicting Win Percentage and
Winning Features of NBA Teams

CS229 Final Project Report

Evan Giarta, Nattapoom Asavareongchai

I. INTRODUCTION

T He NBA is the premier basketball league of the world.

It is an enormous business, with an estimated revenue of
nearly $5 billion in 2014. Teams within the NBA with more
wins will gain popularity and increase income, beneﬁcial to
the league and its players. Thus, predicting the win percentage
of an NBA team based on the previous performance their
roster is quite valuable to general managers, coaches, players,
fans, gamblers, and statisticians alike. Moreover, knowing
which particular stat or feature is most inﬂuential to winning
games is also desirable. Our project aims to do exactly that.

We built a learning algorithm that takes in a data set of
team features, pre-processed from previous statistics of each
player in the current season’s team roster, and outputs an
estimate of the win percentage of that team during the regular
season. The input of our algorithm is a team feature vector
of the 30 different NBA for several seasons. Each team in
each season is considered one input data point. Each input
data point vector is computed from taking the stats of all the
players in the team combined by averaging or accumulating.
The stats of the players in the team are however taken from
one season prior. For example, if we want to compute the
feature vector for the Golden State Warriors during the
2014-2015 season, we will
the players in the
2014-2015 GSW team, then ﬁnd the statistics of each of these
players during the 2013-2014 season and pre-processed them
to create the required feature vector. The win percentage
output would be the win percentage of the current, or the
2014-2015 win percentage of the GSW in our example.

look at all

The learning algorithm model

that we use is a second
order polynomial ﬁt regression model. Unlike linear regression
where the feature vector is used as it is, all the elements of our
input feature vector will ﬁrst be square and both the original
value and the squared value are used as the ﬁnal input features
within the input vector. For example, we have 31 input features
in our vector (without squaring). Before inputting this into
our learning model, we will square the features to create a
62 feature vector. This vector includes the original 31 features
plus the squared of each of the 31 features.

Input F eature vector x(i) = [x1, x2, ..., x31, x2
Output W in P ercentage = y(i)

31]T

1, x2

2, ..., x2

For i = 1, ..., n with n = dataset size

II. RELATED WORKS

There have been many previous

research papers on
predicting NBA team wins. However, most of them tries
to predict wins of each individual NBA games, then sum
them up to then create the win percentage of a season. Two
papers, one by Bernard, L et al. (2009)[1] and the other by
Pedro et al. (2012)[2] used network based algorithm to predict
NBA wins. Because mere statistical features do not produce
as well of a result (as can be seen in our model results) a
network based feature model can deﬁnitely improve learning
algorithms. More speciﬁcally, Bernard, L et al. used neural
networks with feature selection to predict NBA games. In our
opinion, this is a very good model to use, since it takes in
many sophisticated nature of many features (ones we weren’t
able to incorporate into our model).

Another

interesting model done by Na Wei

(2011)[3]
utilizes Naive Bayes to predict playoff records from regular
season stats. Even though this is a different problem, the
nature of the problem is similar. The use of Naive Bayes
however is not a very good approach. Since the model relies
on a lot of assumptions, the model is not as accurate.

Online, statistical resources for sports fans span an enor-
mous spectrum. Basketball aﬁcionados of all kinds and skills
can enjoy interacting with the Buckets visualized shot chart
from BBall Breakdown[4] as well as studying the complex
decision making process of Markov state machines on Big
League Insights[5] as applied to the game. Using data from
thousands of basketball plays, the model analyzes and predicts
how multiple in-game states and transitions inﬂuence each
other and the overall outcome of a game.

III. DATASET AND FEATURES

A. Data Sources

We collected our data from two main sources. The
ﬁrst main source where we collected most player statis-
tics during each seasons, as well as team statistics (team
roster data of each seasons and team win percentage
of each seasons),
the main NBA statistics website
http://stats.nba.com/league/player/. The second source that we
obtained our feature stats from, especially the team salaries
stats is https://www.eskimo.com/ pbender/.

is

B. Data set pre-processing

As described in the introduction, our input data is the set
of team statistics for each season taken from player roster
statistics from the season before. The raw data we obtained
are therefore every player in the league during the 2004-2005
season to the 2013-2014 season. We then pre-processed this
to obtain 30 team feature statistics for the year 2005-2006 to
2014-2015.

These features include:

16) Rebounds (REB)
1) Age
17) Assists (AST)
2) Games Played (GP)
18) Turnovers (TOV)
3) Wins (W)
19) Steals (STL)
4) Minutes (MIN)
20) Blocks (BLK)
5) Field goals Made (FGM)
21) Personal Fouls (PF)
6) Field goals Attempted (FGA)
22) Double-double (DD2)
7) Field goal % (FG%)
23) Triple-doubles (TP3)
8) 3-points Made (3PM)
24) Points (PTS)
9) 3-points attempted (3PA)
25) Efﬁciency (+/-)
10) 3-points % (3P%)
26) # of rookies
11) Free-throws Made (FTM)
27) # of allstars
12) Free-throws attempted (FTA)
28) # of Rebounders
13) Free-throws % (FT%)
29) # of Double-doubles
14) Offensive rebounds (OREB)
30) # of Triple-doubles
15) Defensive rebounds (DREB)
The Number of allstars is deﬁned as the number of players
with points per game of 1 standard deviation above the mean
of a season. Similarly, number of rebounders is the number
of players with rebounds higher than 1 standard deviation.
Likewise with number of double-doubles and triple-doubles.

The 1st feature up to the 25th feature are obtained from
averaging the stats of each player in the team per season
together. The last 5 features are obtained from counting the
players in the team per season with the feature requirements.

We then added the last or 31st feature, called team salaries,
using the team’s available salaries during the current season (so
from 2005-2006 to 2014-2015 seasons). All of the 31 features
are then normalized per season by whitening. With a total of
10 seasons and 30 teams, we have a total of 300 data points
in our data set.

C. Extra Data Manipulation

We tried to run PCA or Principle Component Analysis to
remove any dependent features. A new feature vector, with
reduced dimension/size, is created. The size of the new feature
was determined by how many dominant eigenvalues of the
i=1 x(i)x(i)T there is
empirical co-variance matrix Σ = 1
m
(where m = the number of train samples or data points and
x(i) is the input feature vector of data point i).

(cid:80)m

Suppose there are k dominant eigenvalues (k < 31), then

the new input feature will be:

2





uT
1 x(i)
2 x(i)
uT
...
uT
k x(i)

z(i) =

Where uj are the eigen-vector corresponding to the jth
eigenvalue of Σ

However, when the learning model is run using the reduced
feature vector from PCA versus the original feature vector,
there was no improvements (the generalized error even went
up) and so we decided not to use the reduced vector from PCA.
Instead, we decided to reduce the feature vector using feature
selection, more speciﬁcally backward search. This method
proves to give us a better result. We will describe how we
did feature selection in more detail in the methods section.

IV. METHOD

The learning algorithm that we chose to use for our
learning model is the second order polynomial ﬁt regression
model. However, before we decided on this model, we ran
our data through three different regression models,
linear
regression, locally weighted linear regression and polynomial
ﬁt (with different orders). We validated each method/model
using k-fold validation and chose the model that gave the
best generalization error (lowest RMS error of the test set).

For each regression model, we measured their performances
by measuring the cost function J(θ) of both the training set
and the test set. The cost function we used is deﬁned as
follows:

J(θ) =

(hθ(x(i)) − y(i))2

i=1

For locally weighted regression, we varied the parameter τ
between τ = 10 and τ = 40. For polynomial ﬁt, we ran our
data through second order and third order polynomial ﬁts.

Our data-set includes 300 data points, 30 teams during 10
seasons. We decided to validate our learning algorithm by
separating this data-set into the training set and the test set.
We separated 270 data samples as the training set and the
other 30 as the test set. For simplicity we used one season
as the 30 data test set and the other seasons as data to train.
Because of the nature of how we separated the data and
because there are 10 seasons, we decided to use 10-fold
validation. Therefore, each season becomes a test set once.
The pseudo-code for this 10-fold validation then becomes:

1) Split the data into 10 disjoint sets, S1, S2, ..., S10, where

2) For j = 1, ...,10:

each set contains data from each of the 10 seasons.
Train the model M on S1 ∪ ... ∪ Sj−1 ∪ Sj+1 ∪ ... ∪ S10
(train the model on every set except set Sj) to obtain
the hypothesis hj. Then test the hypothesis on Sj to get
the generalization error ˆ
j.

(cid:118)(cid:117)(cid:117)(cid:116) m(cid:88)

3) The generalization error ˆ for model M is then the

average of all the ˆ

j for j = 1, ..., 10.

10-fold validation is done with every regression model for

comparison.

A. Regression Model Descriptions

In this section, we will brieﬂy describe how each regression
model, linear regression, locally weighted linear regression,
and polynomial ﬁt regression, works.

We will start with linear regression. In linear regression,
we are using the data to create a linear model in the form
y = h(x) = θT x where θ = θ0 + θ1x1 + ...θ31x31 with
x ∈ R31. The model aims to minimize the objective function

(cid:80)m
i=1(h(x(i)) − y(i))2.
To minimize the object function, we used the normal

1
2

equation deﬁned by:

θ = (X T X)−1X T−→y





X =

(x(1))T
(x(2))T

...

(x(31))T



y(1)
y(2)
...
y(31)



−→y =

function as
the

Next for polynomial ﬁt with different orders, we ran
our data on second order and third order. For each
regression, we are trying to minimize
polynomial ﬁt
the
same objective
linear
regression. However,
the
hypothesis h(x). For second order polynomial ﬁt, we have
31.
h(x) = θT ˜x = θ0 + θ1x1 + ... + θ31x31 + θ32x2
1 + ... + θ62x2
For this case, θ ∈ R62 and ˜x ∈ R62. For the case of a third
polynomial ﬁt, we have h(x) = θT ˜x
= θ0 + θ1x1 + ... +
31. For this
31 + θ63x3
θ31x31 + θ32x2
case, θ ∈ R93 and ˜x

1 + ... + θ62x2
(cid:48) ∈ R93.

the
case
difference

1 + ... + θ93x3

for
is

only

in

(cid:48)

(cid:80)
Lastly, for locally weighted linear regression, we try to
to minimize the objective function,
i w(i)(y(i) − θT x(i))2. To do this we also used a modiﬁed

ﬁt θ to our model

normal equation in the form:

θ = (X T W X)−1X T W−→y

with Wii =

w(i), Wij = 0 for i (cid:54)= j

1
2

B. Learning Model Improvements

After we decided on second order polynomial ﬁt regression,
we worked on improving our learning model by trying to
reduce our variance using feature selection. The type of
feature selection we chose was backward search. To do this

we ran the following algorithm:

3

1) Initialize F = {1, ..., 31} with our 31 features.
2) Repeat {

(a) For i = 1, ..., 31, delete feature i from the set F and
call it the set Fi. Then use cross-validation to evaluate
the generalization error of the new Fi.
(b) Set F to be the set Fi
generalization error.
}

in (a) with the least

3) End the repeat loop when the generalization error does
not improve any longer or if it starts to increase again
with fewer features.

V. RESULTS & DISCUSSIONS

We will split our results discussion into three parts. The ﬁrst
part is the results obtained while choosing the best regression
learning algorithm between the ones we have mentioned. The
second part will be the results obtained from trying to improve
the model using different techniques like increasing the data
set size and feature selection (also PCA even though it did not
improve the model). The third part will be the end result or
the ﬁnal model we achieved and how we utilized our model
on the current 2015-2016 NBA season.

A. Different Regression Model Results

As mentioned, we ﬁrst learn our data on different regression
models. We measured the cost function of each model using
k-fold validation and decided on the model with the least RMS
i=1(hθ(x(i)) − y(i))2.
generalization error or J(θ) =
Initially, we were working with data from 6 seasons instead
of 10 seasons, from 2009-2010 to 2014-2015. With this
dataset we validated each model using 6-fold validation.

(cid:113)(cid:80)m

We ﬁrst ran our dataset with linear regression. We obtained
a training RMS error of 18.9 and a test RMS error (or
generalization error) of around 20. The result gives a good
variance but a very bad bias problem. We therefore decided
to ﬁt our data using different polynomial ﬁts. Second order
ﬁt gave us better RMS training data error of 11.4 and third
order gave us an even better RMS training data error of
8.2.However, both second order and third order ﬁts produced
worse RMS test error of 22.7 and 29.3 respectively. Therefore,
we improved our bias problem with the higher order ﬁt but
created a high variance problem (an over ﬁtting problem).

We tackled our high order ﬁt’s variance problem by in-
creasing the dataset to 10 seasons. This reduced the RMS test
error of all the polynomial ﬁt orders (including ﬁrst order or
linear regression) down. The resulting RMS test error of linear
regression, second order ﬁt, and third order ﬁt are 17.4, 16.9,
and 18.0 respectively. Although the RMS test error decreased,
the RMS training error increased for all three models with
errors of 15.9, 13.5, and 12.1 respectively. Even if the training
error increased, the test error decreased which means that the

1

Predicting Win Percentage and
Winning Features of NBA Teams

CS229 Final Project Report

Evan Giarta, Nattapoom Asavareongchai

I. INTRODUCTION

T He NBA is the premier basketball league of the world.

It is an enormous business, with an estimated revenue of
nearly $5 billion in 2014. Teams within the NBA with more
wins will gain popularity and increase income, beneﬁcial to
the league and its players. Thus, predicting the win percentage
of an NBA team based on the previous performance their
roster is quite valuable to general managers, coaches, players,
fans, gamblers, and statisticians alike. Moreover, knowing
which particular stat or feature is most inﬂuential to winning
games is also desirable. Our project aims to do exactly that.

We built a learning algorithm that takes in a data set of
team features, pre-processed from previous statistics of each
player in the current season’s team roster, and outputs an
estimate of the win percentage of that team during the regular
season. The input of our algorithm is a team feature vector
of the 30 different NBA for several seasons. Each team in
each season is considered one input data point. Each input
data point vector is computed from taking the stats of all the
players in the team combined by averaging or accumulating.
The stats of the players in the team are however taken from
one season prior. For example, if we want to compute the
feature vector for the Golden State Warriors during the
2014-2015 season, we will
the players in the
2014-2015 GSW team, then ﬁnd the statistics of each of these
players during the 2013-2014 season and pre-processed them
to create the required feature vector. The win percentage
output would be the win percentage of the current, or the
2014-2015 win percentage of the GSW in our example.

look at all

The learning algorithm model

that we use is a second
order polynomial ﬁt regression model. Unlike linear regression
where the feature vector is used as it is, all the elements of our
input feature vector will ﬁrst be square and both the original
value and the squared value are used as the ﬁnal input features
within the input vector. For example, we have 31 input features
in our vector (without squaring). Before inputting this into
our learning model, we will square the features to create a
62 feature vector. This vector includes the original 31 features
plus the squared of each of the 31 features.

Input F eature vector x(i) = [x1, x2, ..., x31, x2
Output W in P ercentage = y(i)

31]T

1, x2

2, ..., x2

For i = 1, ..., n with n = dataset size

II. RELATED WORKS

There have been many previous

research papers on
predicting NBA team wins. However, most of them tries
to predict wins of each individual NBA games, then sum
them up to then create the win percentage of a season. Two
papers, one by Bernard, L et al. (2009)[1] and the other by
Pedro et al. (2012)[2] used network based algorithm to predict
NBA wins. Because mere statistical features do not produce
as well of a result (as can be seen in our model results) a
network based feature model can deﬁnitely improve learning
algorithms. More speciﬁcally, Bernard, L et al. used neural
networks with feature selection to predict NBA games. In our
opinion, this is a very good model to use, since it takes in
many sophisticated nature of many features (ones we weren’t
able to incorporate into our model).

Another

interesting model done by Na Wei

(2011)[3]
utilizes Naive Bayes to predict playoff records from regular
season stats. Even though this is a different problem, the
nature of the problem is similar. The use of Naive Bayes
however is not a very good approach. Since the model relies
on a lot of assumptions, the model is not as accurate.

Online, statistical resources for sports fans span an enor-
mous spectrum. Basketball aﬁcionados of all kinds and skills
can enjoy interacting with the Buckets visualized shot chart
from BBall Breakdown[4] as well as studying the complex
decision making process of Markov state machines on Big
League Insights[5] as applied to the game. Using data from
thousands of basketball plays, the model analyzes and predicts
how multiple in-game states and transitions inﬂuence each
other and the overall outcome of a game.

III. DATASET AND FEATURES

A. Data Sources

We collected our data from two main sources. The
ﬁrst main source where we collected most player statis-
tics during each seasons, as well as team statistics (team
roster data of each seasons and team win percentage
of each seasons),
the main NBA statistics website
http://stats.nba.com/league/player/. The second source that we
obtained our feature stats from, especially the team salaries
stats is https://www.eskimo.com/ pbender/.

is

B. Data set pre-processing

As described in the introduction, our input data is the set
of team statistics for each season taken from player roster
statistics from the season before. The raw data we obtained
are therefore every player in the league during the 2004-2005
season to the 2013-2014 season. We then pre-processed this
to obtain 30 team feature statistics for the year 2005-2006 to
2014-2015.

These features include:

16) Rebounds (REB)
1) Age
17) Assists (AST)
2) Games Played (GP)
18) Turnovers (TOV)
3) Wins (W)
19) Steals (STL)
4) Minutes (MIN)
20) Blocks (BLK)
5) Field goals Made (FGM)
21) Personal Fouls (PF)
6) Field goals Attempted (FGA)
22) Double-double (DD2)
7) Field goal % (FG%)
23) Triple-doubles (TP3)
8) 3-points Made (3PM)
24) Points (PTS)
9) 3-points attempted (3PA)
25) Efﬁciency (+/-)
10) 3-points % (3P%)
26) # of rookies
11) Free-throws Made (FTM)
27) # of allstars
12) Free-throws attempted (FTA)
28) # of Rebounders
13) Free-throws % (FT%)
29) # of Double-doubles
14) Offensive rebounds (OREB)
30) # of Triple-doubles
15) Defensive rebounds (DREB)
The Number of allstars is deﬁned as the number of players
with points per game of 1 standard deviation above the mean
of a season. Similarly, number of rebounders is the number
of players with rebounds higher than 1 standard deviation.
Likewise with number of double-doubles and triple-doubles.

The 1st feature up to the 25th feature are obtained from
averaging the stats of each player in the team per season
together. The last 5 features are obtained from counting the
players in the team per season with the feature requirements.

We then added the last or 31st feature, called team salaries,
using the team’s available salaries during the current season (so
from 2005-2006 to 2014-2015 seasons). All of the 31 features
are then normalized per season by whitening. With a total of
10 seasons and 30 teams, we have a total of 300 data points
in our data set.

C. Extra Data Manipulation

We tried to run PCA or Principle Component Analysis to
remove any dependent features. A new feature vector, with
reduced dimension/size, is created. The size of the new feature
was determined by how many dominant eigenvalues of the
i=1 x(i)x(i)T there is
empirical co-variance matrix Σ = 1
m
(where m = the number of train samples or data points and
x(i) is the input feature vector of data point i).

(cid:80)m

Suppose there are k dominant eigenvalues (k < 31), then

the new input feature will be:

2





uT
1 x(i)
2 x(i)
uT
...
uT
k x(i)

z(i) =

Where uj are the eigen-vector corresponding to the jth
eigenvalue of Σ

However, when the learning model is run using the reduced
feature vector from PCA versus the original feature vector,
there was no improvements (the generalized error even went
up) and so we decided not to use the reduced vector from PCA.
Instead, we decided to reduce the feature vector using feature
selection, more speciﬁcally backward search. This method
proves to give us a better result. We will describe how we
did feature selection in more detail in the methods section.

IV. METHOD

The learning algorithm that we chose to use for our
learning model is the second order polynomial ﬁt regression
model. However, before we decided on this model, we ran
our data through three different regression models,
linear
regression, locally weighted linear regression and polynomial
ﬁt (with different orders). We validated each method/model
using k-fold validation and chose the model that gave the
best generalization error (lowest RMS error of the test set).

For each regression model, we measured their performances
by measuring the cost function J(θ) of both the training set
and the test set. The cost function we used is deﬁned as
follows:

J(θ) =

(hθ(x(i)) − y(i))2

i=1

For locally weighted regression, we varied the parameter τ
between τ = 10 and τ = 40. For polynomial ﬁt, we ran our
data through second order and third order polynomial ﬁts.

Our data-set includes 300 data points, 30 teams during 10
seasons. We decided to validate our learning algorithm by
separating this data-set into the training set and the test set.
We separated 270 data samples as the training set and the
other 30 as the test set. For simplicity we used one season
as the 30 data test set and the other seasons as data to train.
Because of the nature of how we separated the data and
because there are 10 seasons, we decided to use 10-fold
validation. Therefore, each season becomes a test set once.
The pseudo-code for this 10-fold validation then becomes:

1) Split the data into 10 disjoint sets, S1, S2, ..., S10, where

2) For j = 1, ...,10:

each set contains data from each of the 10 seasons.
Train the model M on S1 ∪ ... ∪ Sj−1 ∪ Sj+1 ∪ ... ∪ S10
(train the model on every set except set Sj) to obtain
the hypothesis hj. Then test the hypothesis on Sj to get
the generalization error ˆ
j.

(cid:118)(cid:117)(cid:117)(cid:116) m(cid:88)

3) The generalization error ˆ for model M is then the

average of all the ˆ

j for j = 1, ..., 10.

10-fold validation is done with every regression model for

comparison.

A. Regression Model Descriptions

In this section, we will brieﬂy describe how each regression
model, linear regression, locally weighted linear regression,
and polynomial ﬁt regression, works.

We will start with linear regression. In linear regression,
we are using the data to create a linear model in the form
y = h(x) = θT x where θ = θ0 + θ1x1 + ...θ31x31 with
x ∈ R31. The model aims to minimize the objective function

(cid:80)m
i=1(h(x(i)) − y(i))2.
To minimize the object function, we used the normal

1
2

equation deﬁned by:

θ = (X T X)−1X T−→y





X =

(x(1))T
(x(2))T

...

(x(31))T



y(1)
y(2)
...
y(31)



−→y =

function as
the

Next for polynomial ﬁt with different orders, we ran
our data on second order and third order. For each
regression, we are trying to minimize
polynomial ﬁt
the
same objective
linear
regression. However,
the
hypothesis h(x). For second order polynomial ﬁt, we have
31.
h(x) = θT ˜x = θ0 + θ1x1 + ... + θ31x31 + θ32x2
1 + ... + θ62x2
For this case, θ ∈ R62 and ˜x ∈ R62. For the case of a third
polynomial ﬁt, we have h(x) = θT ˜x
= θ0 + θ1x1 + ... +
31. For this
31 + θ63x3
θ31x31 + θ32x2
case, θ ∈ R93 and ˜x

1 + ... + θ62x2
(cid:48) ∈ R93.

the
case
difference

1 + ... + θ93x3

for
is

only

in

(cid:48)

(cid:80)
Lastly, for locally weighted linear regression, we try to
to minimize the objective function,
i w(i)(y(i) − θT x(i))2. To do this we also used a modiﬁed

ﬁt θ to our model

normal equation in the form:

θ = (X T W X)−1X T W−→y

with Wii =

w(i), Wij = 0 for i (cid:54)= j

1
2

B. Learning Model Improvements

After we decided on second order polynomial ﬁt regression,
we worked on improving our learning model by trying to
reduce our variance using feature selection. The type of
feature selection we chose was backward search. To do this

we ran the following algorithm:

3

1) Initialize F = {1, ..., 31} with our 31 features.
2) Repeat {

(a) For i = 1, ..., 31, delete feature i from the set F and
call it the set Fi. Then use cross-validation to evaluate
the generalization error of the new Fi.
(b) Set F to be the set Fi
generalization error.
}

in (a) with the least

3) End the repeat loop when the generalization error does
not improve any longer or if it starts to increase again
with fewer features.

V. RESULTS & DISCUSSIONS

We will split our results discussion into three parts. The ﬁrst
part is the results obtained while choosing the best regression
learning algorithm between the ones we have mentioned. The
second part will be the results obtained from trying to improve
the model using different techniques like increasing the data
set size and feature selection (also PCA even though it did not
improve the model). The third part will be the end result or
the ﬁnal model we achieved and how we utilized our model
on the current 2015-2016 NBA season.

A. Different Regression Model Results

As mentioned, we ﬁrst learn our data on different regression
models. We measured the cost function of each model using
k-fold validation and decided on the model with the least RMS
i=1(hθ(x(i)) − y(i))2.
generalization error or J(θ) =
Initially, we were working with data from 6 seasons instead
of 10 seasons, from 2009-2010 to 2014-2015. With this
dataset we validated each model using 6-fold validation.

(cid:113)(cid:80)m

We ﬁrst ran our dataset with linear regression. We obtained
a training RMS error of 18.9 and a test RMS error (or
generalization error) of around 20. The result gives a good
variance but a very bad bias problem. We therefore decided
to ﬁt our data using different polynomial ﬁts. Second order
ﬁt gave us better RMS training data error of 11.4 and third
order gave us an even better RMS training data error of
8.2.However, both second order and third order ﬁts produced
worse RMS test error of 22.7 and 29.3 respectively. Therefore,
we improved our bias problem with the higher order ﬁt but
created a high variance problem (an over ﬁtting problem).

We tackled our high order ﬁt’s variance problem by in-
creasing the dataset to 10 seasons. This reduced the RMS test
error of all the polynomial ﬁt orders (including ﬁrst order or
linear regression) down. The resulting RMS test error of linear
regression, second order ﬁt, and third order ﬁt are 17.4, 16.9,
and 18.0 respectively. Although the RMS test error decreased,
the RMS training error increased for all three models with
errors of 15.9, 13.5, and 12.1 respectively. Even if the training
error increased, the test error decreased which means that the

generalization error improved and so increasing the dataset
did make a good improvement. A plot showing this result is
shown below.

the different order polynomial ﬁts to decide which model is
best. We varied the parameter τ from 10 to 40 and measured
the RMS train and test error. The plot is shown below.

4

Fig. 1. Plot showing the RMS test and train error with using 10 season
datasets of different order polynomial ﬁts

Fig. 3. Plot showing the RMS test and train error of locally weighted linear
regression as τ is varied

We then varied the dataset size from 6 seasons to 10 seasons
for second order and ﬁrst order polynomial ﬁt and plotted a
graph to see the difference. The graph is shown below.

The RMS test error or generalization error did not fall below
17.0 with the τ varied and so the model with the best RMS test
error (the lowest error) is therefore second order polynomial
ﬁt.

B. Model Improvements - Feature Selection

With second order polynomial ﬁt model, we have a RMS
training error of 13.5 and RMS test error of 16.9. We have
already mentioned how we improved our test error and
variance problem by increasing the dataset size to 10 NBA
seasons from 6 NBA seasons (or from 180 data samples to
300 data samples).

We further improved our generalization error by doing
feature selection using backward search. We managed to
reduce our feature vector of 31 features down to 14 features.
This means that our input vector would now have a size of
28 instead of 62 for second order polynomial ﬁt. The size of
the feature vector reduced by more than half. The 17 features
that were reduced are shown below in order of removal:

1. num rookies
2. PF
3. team salaries
4. TOV
5. DREB
6. FGA
7. FG%
8. FT%
9. FTA

10. 3P%
11. PTS
12. MIN
13. FTM
14. AGE
15. REB
16. OREB
17. BLK

Fig. 2. Plot showing the RMS test and train error while changing the dataset
from 6 NBA seasons to 10 NBA seasons

With 10 NBA seasons of our dataset, we then ran locally
weighted linear regression to compare our RMS test data with

1

Predicting Win Percentage and
Winning Features of NBA Teams

CS229 Final Project Report

Evan Giarta, Nattapoom Asavareongchai

I. INTRODUCTION

T He NBA is the premier basketball league of the world.

It is an enormous business, with an estimated revenue of
nearly $5 billion in 2014. Teams within the NBA with more
wins will gain popularity and increase income, beneﬁcial to
the league and its players. Thus, predicting the win percentage
of an NBA team based on the previous performance their
roster is quite valuable to general managers, coaches, players,
fans, gamblers, and statisticians alike. Moreover, knowing
which particular stat or feature is most inﬂuential to winning
games is also desirable. Our project aims to do exactly that.

We built a learning algorithm that takes in a data set of
team features, pre-processed from previous statistics of each
player in the current season’s team roster, and outputs an
estimate of the win percentage of that team during the regular
season. The input of our algorithm is a team feature vector
of the 30 different NBA for several seasons. Each team in
each season is considered one input data point. Each input
data point vector is computed from taking the stats of all the
players in the team combined by averaging or accumulating.
The stats of the players in the team are however taken from
one season prior. For example, if we want to compute the
feature vector for the Golden State Warriors during the
2014-2015 season, we will
the players in the
2014-2015 GSW team, then ﬁnd the statistics of each of these
players during the 2013-2014 season and pre-processed them
to create the required feature vector. The win percentage
output would be the win percentage of the current, or the
2014-2015 win percentage of the GSW in our example.

look at all

The learning algorithm model

that we use is a second
order polynomial ﬁt regression model. Unlike linear regression
where the feature vector is used as it is, all the elements of our
input feature vector will ﬁrst be square and both the original
value and the squared value are used as the ﬁnal input features
within the input vector. For example, we have 31 input features
in our vector (without squaring). Before inputting this into
our learning model, we will square the features to create a
62 feature vector. This vector includes the original 31 features
plus the squared of each of the 31 features.

Input F eature vector x(i) = [x1, x2, ..., x31, x2
Output W in P ercentage = y(i)

31]T

1, x2

2, ..., x2

For i = 1, ..., n with n = dataset size

II. RELATED WORKS

There have been many previous

research papers on
predicting NBA team wins. However, most of them tries
to predict wins of each individual NBA games, then sum
them up to then create the win percentage of a season. Two
papers, one by Bernard, L et al. (2009)[1] and the other by
Pedro et al. (2012)[2] used network based algorithm to predict
NBA wins. Because mere statistical features do not produce
as well of a result (as can be seen in our model results) a
network based feature model can deﬁnitely improve learning
algorithms. More speciﬁcally, Bernard, L et al. used neural
networks with feature selection to predict NBA games. In our
opinion, this is a very good model to use, since it takes in
many sophisticated nature of many features (ones we weren’t
able to incorporate into our model).

Another

interesting model done by Na Wei

(2011)[3]
utilizes Naive Bayes to predict playoff records from regular
season stats. Even though this is a different problem, the
nature of the problem is similar. The use of Naive Bayes
however is not a very good approach. Since the model relies
on a lot of assumptions, the model is not as accurate.

Online, statistical resources for sports fans span an enor-
mous spectrum. Basketball aﬁcionados of all kinds and skills
can enjoy interacting with the Buckets visualized shot chart
from BBall Breakdown[4] as well as studying the complex
decision making process of Markov state machines on Big
League Insights[5] as applied to the game. Using data from
thousands of basketball plays, the model analyzes and predicts
how multiple in-game states and transitions inﬂuence each
other and the overall outcome of a game.

III. DATASET AND FEATURES

A. Data Sources

We collected our data from two main sources. The
ﬁrst main source where we collected most player statis-
tics during each seasons, as well as team statistics (team
roster data of each seasons and team win percentage
of each seasons),
the main NBA statistics website
http://stats.nba.com/league/player/. The second source that we
obtained our feature stats from, especially the team salaries
stats is https://www.eskimo.com/ pbender/.

is

B. Data set pre-processing

As described in the introduction, our input data is the set
of team statistics for each season taken from player roster
statistics from the season before. The raw data we obtained
are therefore every player in the league during the 2004-2005
season to the 2013-2014 season. We then pre-processed this
to obtain 30 team feature statistics for the year 2005-2006 to
2014-2015.

These features include:

16) Rebounds (REB)
1) Age
17) Assists (AST)
2) Games Played (GP)
18) Turnovers (TOV)
3) Wins (W)
19) Steals (STL)
4) Minutes (MIN)
20) Blocks (BLK)
5) Field goals Made (FGM)
21) Personal Fouls (PF)
6) Field goals Attempted (FGA)
22) Double-double (DD2)
7) Field goal % (FG%)
23) Triple-doubles (TP3)
8) 3-points Made (3PM)
24) Points (PTS)
9) 3-points attempted (3PA)
25) Efﬁciency (+/-)
10) 3-points % (3P%)
26) # of rookies
11) Free-throws Made (FTM)
27) # of allstars
12) Free-throws attempted (FTA)
28) # of Rebounders
13) Free-throws % (FT%)
29) # of Double-doubles
14) Offensive rebounds (OREB)
30) # of Triple-doubles
15) Defensive rebounds (DREB)
The Number of allstars is deﬁned as the number of players
with points per game of 1 standard deviation above the mean
of a season. Similarly, number of rebounders is the number
of players with rebounds higher than 1 standard deviation.
Likewise with number of double-doubles and triple-doubles.

The 1st feature up to the 25th feature are obtained from
averaging the stats of each player in the team per season
together. The last 5 features are obtained from counting the
players in the team per season with the feature requirements.

We then added the last or 31st feature, called team salaries,
using the team’s available salaries during the current season (so
from 2005-2006 to 2014-2015 seasons). All of the 31 features
are then normalized per season by whitening. With a total of
10 seasons and 30 teams, we have a total of 300 data points
in our data set.

C. Extra Data Manipulation

We tried to run PCA or Principle Component Analysis to
remove any dependent features. A new feature vector, with
reduced dimension/size, is created. The size of the new feature
was determined by how many dominant eigenvalues of the
i=1 x(i)x(i)T there is
empirical co-variance matrix Σ = 1
m
(where m = the number of train samples or data points and
x(i) is the input feature vector of data point i).

(cid:80)m

Suppose there are k dominant eigenvalues (k < 31), then

the new input feature will be:

2





uT
1 x(i)
2 x(i)
uT
...
uT
k x(i)

z(i) =

Where uj are the eigen-vector corresponding to the jth
eigenvalue of Σ

However, when the learning model is run using the reduced
feature vector from PCA versus the original feature vector,
there was no improvements (the generalized error even went
up) and so we decided not to use the reduced vector from PCA.
Instead, we decided to reduce the feature vector using feature
selection, more speciﬁcally backward search. This method
proves to give us a better result. We will describe how we
did feature selection in more detail in the methods section.

IV. METHOD

The learning algorithm that we chose to use for our
learning model is the second order polynomial ﬁt regression
model. However, before we decided on this model, we ran
our data through three different regression models,
linear
regression, locally weighted linear regression and polynomial
ﬁt (with different orders). We validated each method/model
using k-fold validation and chose the model that gave the
best generalization error (lowest RMS error of the test set).

For each regression model, we measured their performances
by measuring the cost function J(θ) of both the training set
and the test set. The cost function we used is deﬁned as
follows:

J(θ) =

(hθ(x(i)) − y(i))2

i=1

For locally weighted regression, we varied the parameter τ
between τ = 10 and τ = 40. For polynomial ﬁt, we ran our
data through second order and third order polynomial ﬁts.

Our data-set includes 300 data points, 30 teams during 10
seasons. We decided to validate our learning algorithm by
separating this data-set into the training set and the test set.
We separated 270 data samples as the training set and the
other 30 as the test set. For simplicity we used one season
as the 30 data test set and the other seasons as data to train.
Because of the nature of how we separated the data and
because there are 10 seasons, we decided to use 10-fold
validation. Therefore, each season becomes a test set once.
The pseudo-code for this 10-fold validation then becomes:

1) Split the data into 10 disjoint sets, S1, S2, ..., S10, where

2) For j = 1, ...,10:

each set contains data from each of the 10 seasons.
Train the model M on S1 ∪ ... ∪ Sj−1 ∪ Sj+1 ∪ ... ∪ S10
(train the model on every set except set Sj) to obtain
the hypothesis hj. Then test the hypothesis on Sj to get
the generalization error ˆ
j.

(cid:118)(cid:117)(cid:117)(cid:116) m(cid:88)

3) The generalization error ˆ for model M is then the

average of all the ˆ

j for j = 1, ..., 10.

10-fold validation is done with every regression model for

comparison.

A. Regression Model Descriptions

In this section, we will brieﬂy describe how each regression
model, linear regression, locally weighted linear regression,
and polynomial ﬁt regression, works.

We will start with linear regression. In linear regression,
we are using the data to create a linear model in the form
y = h(x) = θT x where θ = θ0 + θ1x1 + ...θ31x31 with
x ∈ R31. The model aims to minimize the objective function

(cid:80)m
i=1(h(x(i)) − y(i))2.
To minimize the object function, we used the normal

1
2

equation deﬁned by:

θ = (X T X)−1X T−→y





X =

(x(1))T
(x(2))T

...

(x(31))T



y(1)
y(2)
...
y(31)



−→y =

function as
the

Next for polynomial ﬁt with different orders, we ran
our data on second order and third order. For each
regression, we are trying to minimize
polynomial ﬁt
the
same objective
linear
regression. However,
the
hypothesis h(x). For second order polynomial ﬁt, we have
31.
h(x) = θT ˜x = θ0 + θ1x1 + ... + θ31x31 + θ32x2
1 + ... + θ62x2
For this case, θ ∈ R62 and ˜x ∈ R62. For the case of a third
polynomial ﬁt, we have h(x) = θT ˜x
= θ0 + θ1x1 + ... +
31. For this
31 + θ63x3
θ31x31 + θ32x2
case, θ ∈ R93 and ˜x

1 + ... + θ62x2
(cid:48) ∈ R93.

the
case
difference

1 + ... + θ93x3

for
is

only

in

(cid:48)

(cid:80)
Lastly, for locally weighted linear regression, we try to
to minimize the objective function,
i w(i)(y(i) − θT x(i))2. To do this we also used a modiﬁed

ﬁt θ to our model

normal equation in the form:

θ = (X T W X)−1X T W−→y

with Wii =

w(i), Wij = 0 for i (cid:54)= j

1
2

B. Learning Model Improvements

After we decided on second order polynomial ﬁt regression,
we worked on improving our learning model by trying to
reduce our variance using feature selection. The type of
feature selection we chose was backward search. To do this

we ran the following algorithm:

3

1) Initialize F = {1, ..., 31} with our 31 features.
2) Repeat {

(a) For i = 1, ..., 31, delete feature i from the set F and
call it the set Fi. Then use cross-validation to evaluate
the generalization error of the new Fi.
(b) Set F to be the set Fi
generalization error.
}

in (a) with the least

3) End the repeat loop when the generalization error does
not improve any longer or if it starts to increase again
with fewer features.

V. RESULTS & DISCUSSIONS

We will split our results discussion into three parts. The ﬁrst
part is the results obtained while choosing the best regression
learning algorithm between the ones we have mentioned. The
second part will be the results obtained from trying to improve
the model using different techniques like increasing the data
set size and feature selection (also PCA even though it did not
improve the model). The third part will be the end result or
the ﬁnal model we achieved and how we utilized our model
on the current 2015-2016 NBA season.

A. Different Regression Model Results

As mentioned, we ﬁrst learn our data on different regression
models. We measured the cost function of each model using
k-fold validation and decided on the model with the least RMS
i=1(hθ(x(i)) − y(i))2.
generalization error or J(θ) =
Initially, we were working with data from 6 seasons instead
of 10 seasons, from 2009-2010 to 2014-2015. With this
dataset we validated each model using 6-fold validation.

(cid:113)(cid:80)m

We ﬁrst ran our dataset with linear regression. We obtained
a training RMS error of 18.9 and a test RMS error (or
generalization error) of around 20. The result gives a good
variance but a very bad bias problem. We therefore decided
to ﬁt our data using different polynomial ﬁts. Second order
ﬁt gave us better RMS training data error of 11.4 and third
order gave us an even better RMS training data error of
8.2.However, both second order and third order ﬁts produced
worse RMS test error of 22.7 and 29.3 respectively. Therefore,
we improved our bias problem with the higher order ﬁt but
created a high variance problem (an over ﬁtting problem).

We tackled our high order ﬁt’s variance problem by in-
creasing the dataset to 10 seasons. This reduced the RMS test
error of all the polynomial ﬁt orders (including ﬁrst order or
linear regression) down. The resulting RMS test error of linear
regression, second order ﬁt, and third order ﬁt are 17.4, 16.9,
and 18.0 respectively. Although the RMS test error decreased,
the RMS training error increased for all three models with
errors of 15.9, 13.5, and 12.1 respectively. Even if the training
error increased, the test error decreased which means that the

generalization error improved and so increasing the dataset
did make a good improvement. A plot showing this result is
shown below.

the different order polynomial ﬁts to decide which model is
best. We varied the parameter τ from 10 to 40 and measured
the RMS train and test error. The plot is shown below.

4

Fig. 1. Plot showing the RMS test and train error with using 10 season
datasets of different order polynomial ﬁts

Fig. 3. Plot showing the RMS test and train error of locally weighted linear
regression as τ is varied

We then varied the dataset size from 6 seasons to 10 seasons
for second order and ﬁrst order polynomial ﬁt and plotted a
graph to see the difference. The graph is shown below.

The RMS test error or generalization error did not fall below
17.0 with the τ varied and so the model with the best RMS test
error (the lowest error) is therefore second order polynomial
ﬁt.

B. Model Improvements - Feature Selection

With second order polynomial ﬁt model, we have a RMS
training error of 13.5 and RMS test error of 16.9. We have
already mentioned how we improved our test error and
variance problem by increasing the dataset size to 10 NBA
seasons from 6 NBA seasons (or from 180 data samples to
300 data samples).

We further improved our generalization error by doing
feature selection using backward search. We managed to
reduce our feature vector of 31 features down to 14 features.
This means that our input vector would now have a size of
28 instead of 62 for second order polynomial ﬁt. The size of
the feature vector reduced by more than half. The 17 features
that were reduced are shown below in order of removal:

1. num rookies
2. PF
3. team salaries
4. TOV
5. DREB
6. FGA
7. FG%
8. FT%
9. FTA

10. 3P%
11. PTS
12. MIN
13. FTM
14. AGE
15. REB
16. OREB
17. BLK

Fig. 2. Plot showing the RMS test and train error while changing the dataset
from 6 NBA seasons to 10 NBA seasons

With 10 NBA seasons of our dataset, we then ran locally
weighted linear regression to compare our RMS test data with

5

has the largest corresponding θ weight of 15.754. The
next most inﬂuential feature is the number of players with
double-doubles of 1 standard deviation more than the mean in
a season. The corresponding θ weight of this feature is 11.528.

With our model, we predicted the win percentage of current

NBA regular season teams as follows:

Fig. 5. Plot showing our win percentage predictions of NBA teams during
the 2015-2016 season

The season is still ongoing and thus the accuracy and errors

cannot be determined yet.

VI. CONCLUSION & FUTURE WORKS

With our chosen learning algorithm/model and tools to
improve our model, such as feature selection, we got our
generalization RMS error down to 15.54. This is a big
improvement from an error of more than 20 in the beginning.
The second order polynomial ﬁt is the best model we found
because linear regression seems to under ﬁt the data, whereas
higher order polynomial ﬁts (i.e. third order or higher) over
ﬁts the data.

With more time and computational resources, we believe
that the error could be reduced further. To do this we would
need to gather more detailed data such as different plays run
or individual team head-to-head games and schedule to create
more relevant and detailed features. Some examples of these
features are coach stats, different plays run by team coaches
and their frequencies, etc. These data are hard to get and hard
to pre-process before being able to add to the model. Also
with better knowledge on other learning algorithms not taught
in class, we believe that using a neural network algorithm
approach would give a better result overall.

Fig. 4. Plot showing the RMS test and train error changes as features are
removed

With the features removed, we managed to reduce the RMS
test error or generalization error down to 15.54, with a training
error of 14.18.

C. End Results

Our resulting hypothesis h(x) = θT x with x, θ ∈ R28, with
all our improvements computed, has θ corresponding to the
ﬁrst order and second order weights of the feature as follows:

Order
(θ1

Features Used
(x1 to x14)

GP
W
FGM
3PM
3PA
AST
STL
DD2
TD3
+-
Allstars
Rebounders
num double
doubles
num triple
doubles

First
Weights
to θ14)
0.016
1.893
-2.873
3.758
-4.212
2.288
1.078
-2.830
-1.705
-1.141
15.754
6.977
11.528

6.725

Second order
Weights
(θ15
to θ28)
-0.114
-0.031
0.138
0.192
-0.047
-0.006
0.035
0.178
0.095
0.004
-2.412
-0.881
-1.731

-0.821

the feature that

As seen in the table of the values of θ obtained from
our model,
inﬂuential on win
percentage of NBA teams in a season is the number of
all-stars. This feature is the number of players with a point
per game of 1 standard deviation above the average. It

is most

1

Predicting Win Percentage and
Winning Features of NBA Teams

CS229 Final Project Report

Evan Giarta, Nattapoom Asavareongchai

I. INTRODUCTION

T He NBA is the premier basketball league of the world.

It is an enormous business, with an estimated revenue of
nearly $5 billion in 2014. Teams within the NBA with more
wins will gain popularity and increase income, beneﬁcial to
the league and its players. Thus, predicting the win percentage
of an NBA team based on the previous performance their
roster is quite valuable to general managers, coaches, players,
fans, gamblers, and statisticians alike. Moreover, knowing
which particular stat or feature is most inﬂuential to winning
games is also desirable. Our project aims to do exactly that.

We built a learning algorithm that takes in a data set of
team features, pre-processed from previous statistics of each
player in the current season’s team roster, and outputs an
estimate of the win percentage of that team during the regular
season. The input of our algorithm is a team feature vector
of the 30 different NBA for several seasons. Each team in
each season is considered one input data point. Each input
data point vector is computed from taking the stats of all the
players in the team combined by averaging or accumulating.
The stats of the players in the team are however taken from
one season prior. For example, if we want to compute the
feature vector for the Golden State Warriors during the
2014-2015 season, we will
the players in the
2014-2015 GSW team, then ﬁnd the statistics of each of these
players during the 2013-2014 season and pre-processed them
to create the required feature vector. The win percentage
output would be the win percentage of the current, or the
2014-2015 win percentage of the GSW in our example.

look at all

The learning algorithm model

that we use is a second
order polynomial ﬁt regression model. Unlike linear regression
where the feature vector is used as it is, all the elements of our
input feature vector will ﬁrst be square and both the original
value and the squared value are used as the ﬁnal input features
within the input vector. For example, we have 31 input features
in our vector (without squaring). Before inputting this into
our learning model, we will square the features to create a
62 feature vector. This vector includes the original 31 features
plus the squared of each of the 31 features.

Input F eature vector x(i) = [x1, x2, ..., x31, x2
Output W in P ercentage = y(i)

31]T

1, x2

2, ..., x2

For i = 1, ..., n with n = dataset size

II. RELATED WORKS

There have been many previous

research papers on
predicting NBA team wins. However, most of them tries
to predict wins of each individual NBA games, then sum
them up to then create the win percentage of a season. Two
papers, one by Bernard, L et al. (2009)[1] and the other by
Pedro et al. (2012)[2] used network based algorithm to predict
NBA wins. Because mere statistical features do not produce
as well of a result (as can be seen in our model results) a
network based feature model can deﬁnitely improve learning
algorithms. More speciﬁcally, Bernard, L et al. used neural
networks with feature selection to predict NBA games. In our
opinion, this is a very good model to use, since it takes in
many sophisticated nature of many features (ones we weren’t
able to incorporate into our model).

Another

interesting model done by Na Wei

(2011)[3]
utilizes Naive Bayes to predict playoff records from regular
season stats. Even though this is a different problem, the
nature of the problem is similar. The use of Naive Bayes
however is not a very good approach. Since the model relies
on a lot of assumptions, the model is not as accurate.

Online, statistical resources for sports fans span an enor-
mous spectrum. Basketball aﬁcionados of all kinds and skills
can enjoy interacting with the Buckets visualized shot chart
from BBall Breakdown[4] as well as studying the complex
decision making process of Markov state machines on Big
League Insights[5] as applied to the game. Using data from
thousands of basketball plays, the model analyzes and predicts
how multiple in-game states and transitions inﬂuence each
other and the overall outcome of a game.

III. DATASET AND FEATURES

A. Data Sources

We collected our data from two main sources. The
ﬁrst main source where we collected most player statis-
tics during each seasons, as well as team statistics (team
roster data of each seasons and team win percentage
of each seasons),
the main NBA statistics website
http://stats.nba.com/league/player/. The second source that we
obtained our feature stats from, especially the team salaries
stats is https://www.eskimo.com/ pbender/.

is

B. Data set pre-processing

As described in the introduction, our input data is the set
of team statistics for each season taken from player roster
statistics from the season before. The raw data we obtained
are therefore every player in the league during the 2004-2005
season to the 2013-2014 season. We then pre-processed this
to obtain 30 team feature statistics for the year 2005-2006 to
2014-2015.

These features include:

16) Rebounds (REB)
1) Age
17) Assists (AST)
2) Games Played (GP)
18) Turnovers (TOV)
3) Wins (W)
19) Steals (STL)
4) Minutes (MIN)
20) Blocks (BLK)
5) Field goals Made (FGM)
21) Personal Fouls (PF)
6) Field goals Attempted (FGA)
22) Double-double (DD2)
7) Field goal % (FG%)
23) Triple-doubles (TP3)
8) 3-points Made (3PM)
24) Points (PTS)
9) 3-points attempted (3PA)
25) Efﬁciency (+/-)
10) 3-points % (3P%)
26) # of rookies
11) Free-throws Made (FTM)
27) # of allstars
12) Free-throws attempted (FTA)
28) # of Rebounders
13) Free-throws % (FT%)
29) # of Double-doubles
14) Offensive rebounds (OREB)
30) # of Triple-doubles
15) Defensive rebounds (DREB)
The Number of allstars is deﬁned as the number of players
with points per game of 1 standard deviation above the mean
of a season. Similarly, number of rebounders is the number
of players with rebounds higher than 1 standard deviation.
Likewise with number of double-doubles and triple-doubles.

The 1st feature up to the 25th feature are obtained from
averaging the stats of each player in the team per season
together. The last 5 features are obtained from counting the
players in the team per season with the feature requirements.

We then added the last or 31st feature, called team salaries,
using the team’s available salaries during the current season (so
from 2005-2006 to 2014-2015 seasons). All of the 31 features
are then normalized per season by whitening. With a total of
10 seasons and 30 teams, we have a total of 300 data points
in our data set.

C. Extra Data Manipulation

We tried to run PCA or Principle Component Analysis to
remove any dependent features. A new feature vector, with
reduced dimension/size, is created. The size of the new feature
was determined by how many dominant eigenvalues of the
i=1 x(i)x(i)T there is
empirical co-variance matrix Σ = 1
m
(where m = the number of train samples or data points and
x(i) is the input feature vector of data point i).

(cid:80)m

Suppose there are k dominant eigenvalues (k < 31), then

the new input feature will be:

2





uT
1 x(i)
2 x(i)
uT
...
uT
k x(i)

z(i) =

Where uj are the eigen-vector corresponding to the jth
eigenvalue of Σ

However, when the learning model is run using the reduced
feature vector from PCA versus the original feature vector,
there was no improvements (the generalized error even went
up) and so we decided not to use the reduced vector from PCA.
Instead, we decided to reduce the feature vector using feature
selection, more speciﬁcally backward search. This method
proves to give us a better result. We will describe how we
did feature selection in more detail in the methods section.

IV. METHOD

The learning algorithm that we chose to use for our
learning model is the second order polynomial ﬁt regression
model. However, before we decided on this model, we ran
our data through three different regression models,
linear
regression, locally weighted linear regression and polynomial
ﬁt (with different orders). We validated each method/model
using k-fold validation and chose the model that gave the
best generalization error (lowest RMS error of the test set).

For each regression model, we measured their performances
by measuring the cost function J(θ) of both the training set
and the test set. The cost function we used is deﬁned as
follows:

J(θ) =

(hθ(x(i)) − y(i))2

i=1

For locally weighted regression, we varied the parameter τ
between τ = 10 and τ = 40. For polynomial ﬁt, we ran our
data through second order and third order polynomial ﬁts.

Our data-set includes 300 data points, 30 teams during 10
seasons. We decided to validate our learning algorithm by
separating this data-set into the training set and the test set.
We separated 270 data samples as the training set and the
other 30 as the test set. For simplicity we used one season
as the 30 data test set and the other seasons as data to train.
Because of the nature of how we separated the data and
because there are 10 seasons, we decided to use 10-fold
validation. Therefore, each season becomes a test set once.
The pseudo-code for this 10-fold validation then becomes:

1) Split the data into 10 disjoint sets, S1, S2, ..., S10, where

2) For j = 1, ...,10:

each set contains data from each of the 10 seasons.
Train the model M on S1 ∪ ... ∪ Sj−1 ∪ Sj+1 ∪ ... ∪ S10
(train the model on every set except set Sj) to obtain
the hypothesis hj. Then test the hypothesis on Sj to get
the generalization error ˆ
j.

(cid:118)(cid:117)(cid:117)(cid:116) m(cid:88)

3) The generalization error ˆ for model M is then the

average of all the ˆ

j for j = 1, ..., 10.

10-fold validation is done with every regression model for

comparison.

A. Regression Model Descriptions

In this section, we will brieﬂy describe how each regression
model, linear regression, locally weighted linear regression,
and polynomial ﬁt regression, works.

We will start with linear regression. In linear regression,
we are using the data to create a linear model in the form
y = h(x) = θT x where θ = θ0 + θ1x1 + ...θ31x31 with
x ∈ R31. The model aims to minimize the objective function

(cid:80)m
i=1(h(x(i)) − y(i))2.
To minimize the object function, we used the normal

1
2

equation deﬁned by:

θ = (X T X)−1X T−→y





X =

(x(1))T
(x(2))T

...

(x(31))T



y(1)
y(2)
...
y(31)



−→y =

function as
the

Next for polynomial ﬁt with different orders, we ran
our data on second order and third order. For each
regression, we are trying to minimize
polynomial ﬁt
the
same objective
linear
regression. However,
the
hypothesis h(x). For second order polynomial ﬁt, we have
31.
h(x) = θT ˜x = θ0 + θ1x1 + ... + θ31x31 + θ32x2
1 + ... + θ62x2
For this case, θ ∈ R62 and ˜x ∈ R62. For the case of a third
polynomial ﬁt, we have h(x) = θT ˜x
= θ0 + θ1x1 + ... +
31. For this
31 + θ63x3
θ31x31 + θ32x2
case, θ ∈ R93 and ˜x

1 + ... + θ62x2
(cid:48) ∈ R93.

the
case
difference

1 + ... + θ93x3

for
is

only

in

(cid:48)

(cid:80)
Lastly, for locally weighted linear regression, we try to
to minimize the objective function,
i w(i)(y(i) − θT x(i))2. To do this we also used a modiﬁed

ﬁt θ to our model

normal equation in the form:

θ = (X T W X)−1X T W−→y

with Wii =

w(i), Wij = 0 for i (cid:54)= j

1
2

B. Learning Model Improvements

After we decided on second order polynomial ﬁt regression,
we worked on improving our learning model by trying to
reduce our variance using feature selection. The type of
feature selection we chose was backward search. To do this

we ran the following algorithm:

3

1) Initialize F = {1, ..., 31} with our 31 features.
2) Repeat {

(a) For i = 1, ..., 31, delete feature i from the set F and
call it the set Fi. Then use cross-validation to evaluate
the generalization error of the new Fi.
(b) Set F to be the set Fi
generalization error.
}

in (a) with the least

3) End the repeat loop when the generalization error does
not improve any longer or if it starts to increase again
with fewer features.

V. RESULTS & DISCUSSIONS

We will split our results discussion into three parts. The ﬁrst
part is the results obtained while choosing the best regression
learning algorithm between the ones we have mentioned. The
second part will be the results obtained from trying to improve
the model using different techniques like increasing the data
set size and feature selection (also PCA even though it did not
improve the model). The third part will be the end result or
the ﬁnal model we achieved and how we utilized our model
on the current 2015-2016 NBA season.

A. Different Regression Model Results

As mentioned, we ﬁrst learn our data on different regression
models. We measured the cost function of each model using
k-fold validation and decided on the model with the least RMS
i=1(hθ(x(i)) − y(i))2.
generalization error or J(θ) =
Initially, we were working with data from 6 seasons instead
of 10 seasons, from 2009-2010 to 2014-2015. With this
dataset we validated each model using 6-fold validation.

(cid:113)(cid:80)m

We ﬁrst ran our dataset with linear regression. We obtained
a training RMS error of 18.9 and a test RMS error (or
generalization error) of around 20. The result gives a good
variance but a very bad bias problem. We therefore decided
to ﬁt our data using different polynomial ﬁts. Second order
ﬁt gave us better RMS training data error of 11.4 and third
order gave us an even better RMS training data error of
8.2.However, both second order and third order ﬁts produced
worse RMS test error of 22.7 and 29.3 respectively. Therefore,
we improved our bias problem with the higher order ﬁt but
created a high variance problem (an over ﬁtting problem).

We tackled our high order ﬁt’s variance problem by in-
creasing the dataset to 10 seasons. This reduced the RMS test
error of all the polynomial ﬁt orders (including ﬁrst order or
linear regression) down. The resulting RMS test error of linear
regression, second order ﬁt, and third order ﬁt are 17.4, 16.9,
and 18.0 respectively. Although the RMS test error decreased,
the RMS training error increased for all three models with
errors of 15.9, 13.5, and 12.1 respectively. Even if the training
error increased, the test error decreased which means that the

generalization error improved and so increasing the dataset
did make a good improvement. A plot showing this result is
shown below.

the different order polynomial ﬁts to decide which model is
best. We varied the parameter τ from 10 to 40 and measured
the RMS train and test error. The plot is shown below.

4

Fig. 1. Plot showing the RMS test and train error with using 10 season
datasets of different order polynomial ﬁts

Fig. 3. Plot showing the RMS test and train error of locally weighted linear
regression as τ is varied

We then varied the dataset size from 6 seasons to 10 seasons
for second order and ﬁrst order polynomial ﬁt and plotted a
graph to see the difference. The graph is shown below.

The RMS test error or generalization error did not fall below
17.0 with the τ varied and so the model with the best RMS test
error (the lowest error) is therefore second order polynomial
ﬁt.

B. Model Improvements - Feature Selection

With second order polynomial ﬁt model, we have a RMS
training error of 13.5 and RMS test error of 16.9. We have
already mentioned how we improved our test error and
variance problem by increasing the dataset size to 10 NBA
seasons from 6 NBA seasons (or from 180 data samples to
300 data samples).

We further improved our generalization error by doing
feature selection using backward search. We managed to
reduce our feature vector of 31 features down to 14 features.
This means that our input vector would now have a size of
28 instead of 62 for second order polynomial ﬁt. The size of
the feature vector reduced by more than half. The 17 features
that were reduced are shown below in order of removal:

1. num rookies
2. PF
3. team salaries
4. TOV
5. DREB
6. FGA
7. FG%
8. FT%
9. FTA

10. 3P%
11. PTS
12. MIN
13. FTM
14. AGE
15. REB
16. OREB
17. BLK

Fig. 2. Plot showing the RMS test and train error while changing the dataset
from 6 NBA seasons to 10 NBA seasons

With 10 NBA seasons of our dataset, we then ran locally
weighted linear regression to compare our RMS test data with

5

has the largest corresponding θ weight of 15.754. The
next most inﬂuential feature is the number of players with
double-doubles of 1 standard deviation more than the mean in
a season. The corresponding θ weight of this feature is 11.528.

With our model, we predicted the win percentage of current

NBA regular season teams as follows:

Fig. 5. Plot showing our win percentage predictions of NBA teams during
the 2015-2016 season

The season is still ongoing and thus the accuracy and errors

cannot be determined yet.

VI. CONCLUSION & FUTURE WORKS

With our chosen learning algorithm/model and tools to
improve our model, such as feature selection, we got our
generalization RMS error down to 15.54. This is a big
improvement from an error of more than 20 in the beginning.
The second order polynomial ﬁt is the best model we found
because linear regression seems to under ﬁt the data, whereas
higher order polynomial ﬁts (i.e. third order or higher) over
ﬁts the data.

With more time and computational resources, we believe
that the error could be reduced further. To do this we would
need to gather more detailed data such as different plays run
or individual team head-to-head games and schedule to create
more relevant and detailed features. Some examples of these
features are coach stats, different plays run by team coaches
and their frequencies, etc. These data are hard to get and hard
to pre-process before being able to add to the model. Also
with better knowledge on other learning algorithms not taught
in class, we believe that using a neural network algorithm
approach would give a better result overall.

Fig. 4. Plot showing the RMS test and train error changes as features are
removed

With the features removed, we managed to reduce the RMS
test error or generalization error down to 15.54, with a training
error of 14.18.

C. End Results

Our resulting hypothesis h(x) = θT x with x, θ ∈ R28, with
all our improvements computed, has θ corresponding to the
ﬁrst order and second order weights of the feature as follows:

Order
(θ1

Features Used
(x1 to x14)

GP
W
FGM
3PM
3PA
AST
STL
DD2
TD3
+-
Allstars
Rebounders
num double
doubles
num triple
doubles

First
Weights
to θ14)
0.016
1.893
-2.873
3.758
-4.212
2.288
1.078
-2.830
-1.705
-1.141
15.754
6.977
11.528

6.725

Second order
Weights
(θ15
to θ28)
-0.114
-0.031
0.138
0.192
-0.047
-0.006
0.035
0.178
0.095
0.004
-2.412
-0.881
-1.731

-0.821

the feature that

As seen in the table of the values of θ obtained from
our model,
inﬂuential on win
percentage of NBA teams in a season is the number of
all-stars. This feature is the number of players with a point
per game of 1 standard deviation above the average. It

is most

6

VII. REFERENCES

[1] Loeffelholz, Bernard, Earl Bednar, and Kenneth W.
Bauer. Predicting NBA games using neural networks. Journal
of Quantitative Analysis in Sports 5.1 (2009).

[2] Vaz de Melo, Pedro OS, et al. Forecasting in the
NBA and other team sports: Network effects in action. ACM
Transactions on Knowledge Discovery from Data (TKDD)
6.3 (2012): 13.

[3] Wei, Na. Predicting the outcome of NBA playoffs using
the nave bayes algorithms. University of South Florida,
College of Engineering (2011).

[4] Beshai, Peter. Buckets. BBall Breakdown, 3 Dec. 2015.
Web. 5 Dec 2015. <http://bballbreakdown.com/buckets/app/>

[5] Simulating an NBA Game: Building a Markov
Model. Big League Insights, 19 Jan. 2015. Web. 17
Nov. 2015. <http://bigleagueinsights.com/simulate-nba-game-
markov-model/>

