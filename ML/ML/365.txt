Information-based Feature Selection

Farzan Farnia, Abbas Kazerouni, Afshin Babveyh

Email: {farnia,abbask,afshinb}@stanford.edu

1

Introduction

Feature selection is a topic of great interest in applications dealing with high-dimensional datasets.
These applications include gene expression array analysis, combinatorial chemistry and text process-
ing of online documents. Using feature selection brings about several advantages. First, it leads
to lower computational cost and time. Less memory is needed to store the data and less process-
ing power is needed. Feature selection helps improve the performance of the predictors by avoiding
overﬁtting. It can also capture the underlying connection between the data. And perhaps the most
important aspect, it can break through the barrier of high-dimensionality.

To select the most relevant subset of features, we need a mathematical tool to measure dependence
among random variables. In this work, we use the concept of mutual information. Mutual information
is a well-known dependence measure in information theory. For any arbitrary pair of discrete random
variables, X ∈ X and Y ∈ Y, Mutual Information is deﬁned as

(cid:88)

x∈X , y∈Y

I(X; Y ) =

pX,Y (x, y) log

pX,Y (x, y)
pX(x) pY (y)

.

(1)

The paper is organized as follows. In section 2 the method of Maximum-Relevance Minimum-
Redundancy (MRMR) is presented along with Maximum Joint Relevant (MJR) method. In section
3, we present our method to solve the feature selection problem. Section 4 presents the result of our
algorithm tested on Madelon dataset. Finally, section 5 discusses the conclusion.

2 Mutual Information as a tool for Feature Selection

As discussed earlier, mutual information is a powerful tool in measuring relevance among random
variables. Hence, it can be a useful mathematical tool to ﬁnd and select relevant features. In other
words, if our goal is to select no more than k features an optimal task is to solve

arg max
|S|=k

I(XS; Y ),

(2)

where XS = {Xi : i ∈ S}. However, as k gets larger our estimation of mutual information becomes
less accurate. It is because for large k’s we do not have enough samples to estimate mutual information
accurately. Hence, the objective function in (2) should be modiﬁed so that it becomes estimable by
available samples. In the next sections, we ﬁrst discuss a past approach to solve this issue and then
propose a new solution to improve such approaches.

1

Information-based Feature Selection

Farzan Farnia, Abbas Kazerouni, Afshin Babveyh

Email: {farnia,abbask,afshinb}@stanford.edu

1

Introduction

Feature selection is a topic of great interest in applications dealing with high-dimensional datasets.
These applications include gene expression array analysis, combinatorial chemistry and text process-
ing of online documents. Using feature selection brings about several advantages. First, it leads
to lower computational cost and time. Less memory is needed to store the data and less process-
ing power is needed. Feature selection helps improve the performance of the predictors by avoiding
overﬁtting. It can also capture the underlying connection between the data. And perhaps the most
important aspect, it can break through the barrier of high-dimensionality.

To select the most relevant subset of features, we need a mathematical tool to measure dependence
among random variables. In this work, we use the concept of mutual information. Mutual information
is a well-known dependence measure in information theory. For any arbitrary pair of discrete random
variables, X ∈ X and Y ∈ Y, Mutual Information is deﬁned as

(cid:88)

x∈X , y∈Y

I(X; Y ) =

pX,Y (x, y) log

pX,Y (x, y)
pX(x) pY (y)

.

(1)

The paper is organized as follows. In section 2 the method of Maximum-Relevance Minimum-
Redundancy (MRMR) is presented along with Maximum Joint Relevant (MJR) method. In section
3, we present our method to solve the feature selection problem. Section 4 presents the result of our
algorithm tested on Madelon dataset. Finally, section 5 discusses the conclusion.

2 Mutual Information as a tool for Feature Selection

As discussed earlier, mutual information is a powerful tool in measuring relevance among random
variables. Hence, it can be a useful mathematical tool to ﬁnd and select relevant features. In other
words, if our goal is to select no more than k features an optimal task is to solve

arg max
|S|=k

I(XS; Y ),

(2)

where XS = {Xi : i ∈ S}. However, as k gets larger our estimation of mutual information becomes
less accurate. It is because for large k’s we do not have enough samples to estimate mutual information
accurately. Hence, the objective function in (2) should be modiﬁed so that it becomes estimable by
available samples. In the next sections, we ﬁrst discuss a past approach to solve this issue and then
propose a new solution to improve such approaches.

1

2.1 Max-Relevance Min-Redundancy (MRMR) approach

As mentioned earlier, we aim to identify the most relevant subset of features whose size is limited
to a given factor. Note that this is not the same as characterizing the k best features with the
most individual mutual information to the target Y . In fact, diﬀerent features may share redundant
information on the target. Thus, redundancy is another important factor to be considered in fea-
ture selection. To balance the trade-oﬀ between relevance and redundancy, the following modiﬁed
objective function (MRMR) has been suggested in [2]:

Φ(XS, Y ) =

1
|S|

I(Xi; Y ) − 1
|S|2

I(Xi; Xj).

(3)

(cid:88)

i∈S

(cid:88)

i,j∈S

(cid:88)

Xi∈Sm

Here, the ﬁrst term measures the average relevance of features to the target, while the second term
measures average pairwise redundancy among selected features. Therefore, maximizing Φ(XS, Y )
leads to identifying a well-characterizing feature subset whose total information on the target is close
to the optimal feature subset’s. To maximize this objective, they used an inductive approach where
ﬁrst the most informative feature is chosen, and then next features are inductively added by solving
the following at every step:

arg max

Xj∈X\Sm

I(Xj; Y ) − 1

m − 1

2.2 Maximum Joint Relevance

I(Xj; Xi).

(4)

Although MRMR is a well-known feature selection method, there are several applications where the
test error rate never goes below some large thresholds like 34% which seems quite unsatisfactory.
Note that (3) includes only up to pairwise interactions. By considering higher order interactions we
can become able to select a more informative feature subset which in turn results in smaller error
rates. To this end, Maximum Joint Relevant (MJR) algorithm changes the inductive rule of (4) to a
more sensitive one [3]:

arg max

Xj∈X\Sm

I(Xj, Xi ; Y ).

(5)

(cid:88)

Xi∈Sm

Nevertheless, we may again encounter the issue of lack of enough samples to estimate the second
order mutual information appeared in the above formulation. As a matter of fact, a considerable
number of third order empirical marginals may become too small, and thus it requires a more accurate
estimation of mutual information than the empirical one. Therefore, in next section we are going
to propose a new algorithm to estimate mutual information with higher accuracy. As an important
advantage, this estimation technique reduces the required sample size to estimate mutual information
within the same accuracy.

3 Adaptive Maximum Joint Relevant

In this section, we propose the Adaptive Maximum Joint Relevant (AMJR) feature selection al-
gorithm to tackle the instability problem in MJR. Similar to MJR, we use the criterion in (5) to
iteratively select the most relevant features. However, we propose a new scheme to estimate the
mutual informations which stabilize the algorithm in small training set regimes. We build our es-
timation technique based on functional estimation method proposed in [4]. Speciﬁcally, in order to

2

Information-based Feature Selection

Farzan Farnia, Abbas Kazerouni, Afshin Babveyh

Email: {farnia,abbask,afshinb}@stanford.edu

1

Introduction

Feature selection is a topic of great interest in applications dealing with high-dimensional datasets.
These applications include gene expression array analysis, combinatorial chemistry and text process-
ing of online documents. Using feature selection brings about several advantages. First, it leads
to lower computational cost and time. Less memory is needed to store the data and less process-
ing power is needed. Feature selection helps improve the performance of the predictors by avoiding
overﬁtting. It can also capture the underlying connection between the data. And perhaps the most
important aspect, it can break through the barrier of high-dimensionality.

To select the most relevant subset of features, we need a mathematical tool to measure dependence
among random variables. In this work, we use the concept of mutual information. Mutual information
is a well-known dependence measure in information theory. For any arbitrary pair of discrete random
variables, X ∈ X and Y ∈ Y, Mutual Information is deﬁned as

(cid:88)

x∈X , y∈Y

I(X; Y ) =

pX,Y (x, y) log

pX,Y (x, y)
pX(x) pY (y)

.

(1)

The paper is organized as follows. In section 2 the method of Maximum-Relevance Minimum-
Redundancy (MRMR) is presented along with Maximum Joint Relevant (MJR) method. In section
3, we present our method to solve the feature selection problem. Section 4 presents the result of our
algorithm tested on Madelon dataset. Finally, section 5 discusses the conclusion.

2 Mutual Information as a tool for Feature Selection

As discussed earlier, mutual information is a powerful tool in measuring relevance among random
variables. Hence, it can be a useful mathematical tool to ﬁnd and select relevant features. In other
words, if our goal is to select no more than k features an optimal task is to solve

arg max
|S|=k

I(XS; Y ),

(2)

where XS = {Xi : i ∈ S}. However, as k gets larger our estimation of mutual information becomes
less accurate. It is because for large k’s we do not have enough samples to estimate mutual information
accurately. Hence, the objective function in (2) should be modiﬁed so that it becomes estimable by
available samples. In the next sections, we ﬁrst discuss a past approach to solve this issue and then
propose a new solution to improve such approaches.

1

2.1 Max-Relevance Min-Redundancy (MRMR) approach

As mentioned earlier, we aim to identify the most relevant subset of features whose size is limited
to a given factor. Note that this is not the same as characterizing the k best features with the
most individual mutual information to the target Y . In fact, diﬀerent features may share redundant
information on the target. Thus, redundancy is another important factor to be considered in fea-
ture selection. To balance the trade-oﬀ between relevance and redundancy, the following modiﬁed
objective function (MRMR) has been suggested in [2]:

Φ(XS, Y ) =

1
|S|

I(Xi; Y ) − 1
|S|2

I(Xi; Xj).

(3)

(cid:88)

i∈S

(cid:88)

i,j∈S

(cid:88)

Xi∈Sm

Here, the ﬁrst term measures the average relevance of features to the target, while the second term
measures average pairwise redundancy among selected features. Therefore, maximizing Φ(XS, Y )
leads to identifying a well-characterizing feature subset whose total information on the target is close
to the optimal feature subset’s. To maximize this objective, they used an inductive approach where
ﬁrst the most informative feature is chosen, and then next features are inductively added by solving
the following at every step:

arg max

Xj∈X\Sm

I(Xj; Y ) − 1

m − 1

2.2 Maximum Joint Relevance

I(Xj; Xi).

(4)

Although MRMR is a well-known feature selection method, there are several applications where the
test error rate never goes below some large thresholds like 34% which seems quite unsatisfactory.
Note that (3) includes only up to pairwise interactions. By considering higher order interactions we
can become able to select a more informative feature subset which in turn results in smaller error
rates. To this end, Maximum Joint Relevant (MJR) algorithm changes the inductive rule of (4) to a
more sensitive one [3]:

arg max

Xj∈X\Sm

I(Xj, Xi ; Y ).

(5)

(cid:88)

Xi∈Sm

Nevertheless, we may again encounter the issue of lack of enough samples to estimate the second
order mutual information appeared in the above formulation. As a matter of fact, a considerable
number of third order empirical marginals may become too small, and thus it requires a more accurate
estimation of mutual information than the empirical one. Therefore, in next section we are going
to propose a new algorithm to estimate mutual information with higher accuracy. As an important
advantage, this estimation technique reduces the required sample size to estimate mutual information
within the same accuracy.

3 Adaptive Maximum Joint Relevant

In this section, we propose the Adaptive Maximum Joint Relevant (AMJR) feature selection al-
gorithm to tackle the instability problem in MJR. Similar to MJR, we use the criterion in (5) to
iteratively select the most relevant features. However, we propose a new scheme to estimate the
mutual informations which stabilize the algorithm in small training set regimes. We build our es-
timation technique based on functional estimation method proposed in [4]. Speciﬁcally, in order to

2

estimate I(Xj, Xi; Y ) at each step, we have to estimate the joint entropies according to the following
identity:

I(Xj, Xi; Y ) = H(Xj, Xi) + H(Y ) − H(Xj, Xi, Y ).

(6)

In order to describe the estimation method in AMJR, consider for example, estimating H(Xj, Xi).

Following from [4], ﬁrst the empirical joint distribution of (Xj, Xi) is computed according to

1{(Xj, Xi)(t) = (a, b)},

(7)

n(cid:88)

t=1

ˆPa,b =

1
n

H(Xj, Xi) = − (cid:88)

a∈A, b∈B

where n is the size of training set and (Xj, Xi)(t) is the joint value of tth training example. Note that
a and b are assumed to take value in some ﬁnte sets A and B, respectively. Now, assuming that Pa,b
is the true joint probability of (Xj, Xi) at point (a, b), the true joint entropy would be

Pa,b log Pa,b.

(8)

In order to provide the estimator ˆH(Xj, Xi) of H(Xj, Xi), one naive way is substitute each Pa,b in
(8) with its estimate ˆPa,b. This method which is used in MJR, is in fact the source of instability on
the performance since most of the estimated probabilities are very small. In AMJR, we consider two
cases for the estimated joint probabilities:

• If ˆPa,b ≥ log n
• If ˆPa,b < log n

n , we use it as an estimation of Pa,b in (8).
n , ﬁrst we ﬁt a polynomial f of order (cid:98)log n(cid:99) to the function x log x in the interval

(0, log n

n ). Then, we use f ( ˆPa,b) as an estimation for Pa,b log Pa,b in (8).

As we see in Section 4, the approximation polynomial f introduces stability to the algorithm and
improves its performance. Consequently, the estimation of H(Xj, Xi) in AMJR would be

(cid:32) (cid:88)

(cid:88)

(cid:33)

ˆH(Xj, Xi) = −

ˆPa,b log ˆPa,b +

f ( ˆPa,b)

.

(9)

ˆPa,b≥ log n

n

ˆPa,b< log n
n

Similarly, the estimations ˆH(Xj, Xi, Y ) and ˆH(Y ) are provided for H(Xj, Xi, Y ) and H(Y ),

respectively. Finally, the mutual information is estimated as

ˆI(Xj, Xi; Y ) = ˆH(Xj, Xi) + ˆH(Y ) − ˆH(Xj, Xi, Y ).

(10)

4 Numerical Results

In this section we provide numerical results to conﬁrm our theoretical analysis. We perform diﬀerent
feature selection and classiﬁcation methods on the dataset ”Madelon” released in NIPS 2003 feature
selection challenge [5]. This data set consists of 2000 samples each containing 500 continuous input
features and one binary output response. Here we have used 1400 samples (70%) as the training set
and used the other 600 samples (30%) as the test set.

In order to explore the eﬀect of sample size on diﬀerent feature selection methods, we quantize the
input space into 3 and 5 levels, uniformly. Thus, we have two scenarios. In the ﬁrst one, the input
features are quantized separately into three levels which corresponds to the large training set regime

3

Information-based Feature Selection

Farzan Farnia, Abbas Kazerouni, Afshin Babveyh

Email: {farnia,abbask,afshinb}@stanford.edu

1

Introduction

Feature selection is a topic of great interest in applications dealing with high-dimensional datasets.
These applications include gene expression array analysis, combinatorial chemistry and text process-
ing of online documents. Using feature selection brings about several advantages. First, it leads
to lower computational cost and time. Less memory is needed to store the data and less process-
ing power is needed. Feature selection helps improve the performance of the predictors by avoiding
overﬁtting. It can also capture the underlying connection between the data. And perhaps the most
important aspect, it can break through the barrier of high-dimensionality.

To select the most relevant subset of features, we need a mathematical tool to measure dependence
among random variables. In this work, we use the concept of mutual information. Mutual information
is a well-known dependence measure in information theory. For any arbitrary pair of discrete random
variables, X ∈ X and Y ∈ Y, Mutual Information is deﬁned as

(cid:88)

x∈X , y∈Y

I(X; Y ) =

pX,Y (x, y) log

pX,Y (x, y)
pX(x) pY (y)

.

(1)

The paper is organized as follows. In section 2 the method of Maximum-Relevance Minimum-
Redundancy (MRMR) is presented along with Maximum Joint Relevant (MJR) method. In section
3, we present our method to solve the feature selection problem. Section 4 presents the result of our
algorithm tested on Madelon dataset. Finally, section 5 discusses the conclusion.

2 Mutual Information as a tool for Feature Selection

As discussed earlier, mutual information is a powerful tool in measuring relevance among random
variables. Hence, it can be a useful mathematical tool to ﬁnd and select relevant features. In other
words, if our goal is to select no more than k features an optimal task is to solve

arg max
|S|=k

I(XS; Y ),

(2)

where XS = {Xi : i ∈ S}. However, as k gets larger our estimation of mutual information becomes
less accurate. It is because for large k’s we do not have enough samples to estimate mutual information
accurately. Hence, the objective function in (2) should be modiﬁed so that it becomes estimable by
available samples. In the next sections, we ﬁrst discuss a past approach to solve this issue and then
propose a new solution to improve such approaches.

1

2.1 Max-Relevance Min-Redundancy (MRMR) approach

As mentioned earlier, we aim to identify the most relevant subset of features whose size is limited
to a given factor. Note that this is not the same as characterizing the k best features with the
most individual mutual information to the target Y . In fact, diﬀerent features may share redundant
information on the target. Thus, redundancy is another important factor to be considered in fea-
ture selection. To balance the trade-oﬀ between relevance and redundancy, the following modiﬁed
objective function (MRMR) has been suggested in [2]:

Φ(XS, Y ) =

1
|S|

I(Xi; Y ) − 1
|S|2

I(Xi; Xj).

(3)

(cid:88)

i∈S

(cid:88)

i,j∈S

(cid:88)

Xi∈Sm

Here, the ﬁrst term measures the average relevance of features to the target, while the second term
measures average pairwise redundancy among selected features. Therefore, maximizing Φ(XS, Y )
leads to identifying a well-characterizing feature subset whose total information on the target is close
to the optimal feature subset’s. To maximize this objective, they used an inductive approach where
ﬁrst the most informative feature is chosen, and then next features are inductively added by solving
the following at every step:

arg max

Xj∈X\Sm

I(Xj; Y ) − 1

m − 1

2.2 Maximum Joint Relevance

I(Xj; Xi).

(4)

Although MRMR is a well-known feature selection method, there are several applications where the
test error rate never goes below some large thresholds like 34% which seems quite unsatisfactory.
Note that (3) includes only up to pairwise interactions. By considering higher order interactions we
can become able to select a more informative feature subset which in turn results in smaller error
rates. To this end, Maximum Joint Relevant (MJR) algorithm changes the inductive rule of (4) to a
more sensitive one [3]:

arg max

Xj∈X\Sm

I(Xj, Xi ; Y ).

(5)

(cid:88)

Xi∈Sm

Nevertheless, we may again encounter the issue of lack of enough samples to estimate the second
order mutual information appeared in the above formulation. As a matter of fact, a considerable
number of third order empirical marginals may become too small, and thus it requires a more accurate
estimation of mutual information than the empirical one. Therefore, in next section we are going
to propose a new algorithm to estimate mutual information with higher accuracy. As an important
advantage, this estimation technique reduces the required sample size to estimate mutual information
within the same accuracy.

3 Adaptive Maximum Joint Relevant

In this section, we propose the Adaptive Maximum Joint Relevant (AMJR) feature selection al-
gorithm to tackle the instability problem in MJR. Similar to MJR, we use the criterion in (5) to
iteratively select the most relevant features. However, we propose a new scheme to estimate the
mutual informations which stabilize the algorithm in small training set regimes. We build our es-
timation technique based on functional estimation method proposed in [4]. Speciﬁcally, in order to

2

estimate I(Xj, Xi; Y ) at each step, we have to estimate the joint entropies according to the following
identity:

I(Xj, Xi; Y ) = H(Xj, Xi) + H(Y ) − H(Xj, Xi, Y ).

(6)

In order to describe the estimation method in AMJR, consider for example, estimating H(Xj, Xi).

Following from [4], ﬁrst the empirical joint distribution of (Xj, Xi) is computed according to

1{(Xj, Xi)(t) = (a, b)},

(7)

n(cid:88)

t=1

ˆPa,b =

1
n

H(Xj, Xi) = − (cid:88)

a∈A, b∈B

where n is the size of training set and (Xj, Xi)(t) is the joint value of tth training example. Note that
a and b are assumed to take value in some ﬁnte sets A and B, respectively. Now, assuming that Pa,b
is the true joint probability of (Xj, Xi) at point (a, b), the true joint entropy would be

Pa,b log Pa,b.

(8)

In order to provide the estimator ˆH(Xj, Xi) of H(Xj, Xi), one naive way is substitute each Pa,b in
(8) with its estimate ˆPa,b. This method which is used in MJR, is in fact the source of instability on
the performance since most of the estimated probabilities are very small. In AMJR, we consider two
cases for the estimated joint probabilities:

• If ˆPa,b ≥ log n
• If ˆPa,b < log n

n , we use it as an estimation of Pa,b in (8).
n , ﬁrst we ﬁt a polynomial f of order (cid:98)log n(cid:99) to the function x log x in the interval

(0, log n

n ). Then, we use f ( ˆPa,b) as an estimation for Pa,b log Pa,b in (8).

As we see in Section 4, the approximation polynomial f introduces stability to the algorithm and
improves its performance. Consequently, the estimation of H(Xj, Xi) in AMJR would be

(cid:32) (cid:88)

(cid:88)

(cid:33)

ˆH(Xj, Xi) = −

ˆPa,b log ˆPa,b +

f ( ˆPa,b)

.

(9)

ˆPa,b≥ log n

n

ˆPa,b< log n
n

Similarly, the estimations ˆH(Xj, Xi, Y ) and ˆH(Y ) are provided for H(Xj, Xi, Y ) and H(Y ),

respectively. Finally, the mutual information is estimated as

ˆI(Xj, Xi; Y ) = ˆH(Xj, Xi) + ˆH(Y ) − ˆH(Xj, Xi, Y ).

(10)

4 Numerical Results

In this section we provide numerical results to conﬁrm our theoretical analysis. We perform diﬀerent
feature selection and classiﬁcation methods on the dataset ”Madelon” released in NIPS 2003 feature
selection challenge [5]. This data set consists of 2000 samples each containing 500 continuous input
features and one binary output response. Here we have used 1400 samples (70%) as the training set
and used the other 600 samples (30%) as the test set.

In order to explore the eﬀect of sample size on diﬀerent feature selection methods, we quantize the
input space into 3 and 5 levels, uniformly. Thus, we have two scenarios. In the ﬁrst one, the input
features are quantized separately into three levels which corresponds to the large training set regime

3

Figure 1: SVM classiﬁcation error for 3-level quantization of input space.

(since each level happens too many times and we have small number of probabilities to estimate).
In the second scenario, the input features are quantized separately into 5 levels. The later scenario
corresponds to a small training set regime where there are a large number of probabilities to estimate.
Figure 1 compares the misclassiﬁcation error of MRMR and MJR feature selection algorithms for
diﬀerent number of features. Here, SVM is used as the classiﬁcation method and the input space
is quantized into 3 levels. Since this scenario corresponds to large training set regime, the MJR
outperforms MRMR as depicted in the ﬁgure.

In Fig. 2, the SVM misclassiﬁcation error of MJR and AMJR has been compared for diﬀerent
number of selected features. Here, the input space is quantized into 5 level which corresponds to the
small training set scenario. As depicted in this ﬁgure, MJR has unstable performance in this scenario
while AMJR shows stable and better performance. This ﬁgure conﬁrms our theoretical analysis of
instability of MJR and shows that our proposed method (AMJR) removes the instability problem
almost completely.

The advantage of the proposed method AMJR method is further described in Fig. 3. In this
ﬁgure, the SVM misclassiﬁcation error of AMJR and MRMR methods are compared for diﬀernt
number of selected features. Here, the input space is quantized into 5 levels (small training set
regime). As depicted in this ﬁgure, AMJR substantially outperforms MRMR for any number of

Figure 2: SVM classiﬁcation error for 5-level quantization of input space.

4

51015200.20.250.30.350.40.450.5# selected featuresclassification error rate  MRMRMJR468101214161820220.10.150.20.250.30.350.40.450.5# selected featuresclassification error rate  AMJRMJRInformation-based Feature Selection

Farzan Farnia, Abbas Kazerouni, Afshin Babveyh

Email: {farnia,abbask,afshinb}@stanford.edu

1

Introduction

Feature selection is a topic of great interest in applications dealing with high-dimensional datasets.
These applications include gene expression array analysis, combinatorial chemistry and text process-
ing of online documents. Using feature selection brings about several advantages. First, it leads
to lower computational cost and time. Less memory is needed to store the data and less process-
ing power is needed. Feature selection helps improve the performance of the predictors by avoiding
overﬁtting. It can also capture the underlying connection between the data. And perhaps the most
important aspect, it can break through the barrier of high-dimensionality.

To select the most relevant subset of features, we need a mathematical tool to measure dependence
among random variables. In this work, we use the concept of mutual information. Mutual information
is a well-known dependence measure in information theory. For any arbitrary pair of discrete random
variables, X ∈ X and Y ∈ Y, Mutual Information is deﬁned as

(cid:88)

x∈X , y∈Y

I(X; Y ) =

pX,Y (x, y) log

pX,Y (x, y)
pX(x) pY (y)

.

(1)

The paper is organized as follows. In section 2 the method of Maximum-Relevance Minimum-
Redundancy (MRMR) is presented along with Maximum Joint Relevant (MJR) method. In section
3, we present our method to solve the feature selection problem. Section 4 presents the result of our
algorithm tested on Madelon dataset. Finally, section 5 discusses the conclusion.

2 Mutual Information as a tool for Feature Selection

As discussed earlier, mutual information is a powerful tool in measuring relevance among random
variables. Hence, it can be a useful mathematical tool to ﬁnd and select relevant features. In other
words, if our goal is to select no more than k features an optimal task is to solve

arg max
|S|=k

I(XS; Y ),

(2)

where XS = {Xi : i ∈ S}. However, as k gets larger our estimation of mutual information becomes
less accurate. It is because for large k’s we do not have enough samples to estimate mutual information
accurately. Hence, the objective function in (2) should be modiﬁed so that it becomes estimable by
available samples. In the next sections, we ﬁrst discuss a past approach to solve this issue and then
propose a new solution to improve such approaches.

1

2.1 Max-Relevance Min-Redundancy (MRMR) approach

As mentioned earlier, we aim to identify the most relevant subset of features whose size is limited
to a given factor. Note that this is not the same as characterizing the k best features with the
most individual mutual information to the target Y . In fact, diﬀerent features may share redundant
information on the target. Thus, redundancy is another important factor to be considered in fea-
ture selection. To balance the trade-oﬀ between relevance and redundancy, the following modiﬁed
objective function (MRMR) has been suggested in [2]:

Φ(XS, Y ) =

1
|S|

I(Xi; Y ) − 1
|S|2

I(Xi; Xj).

(3)

(cid:88)

i∈S

(cid:88)

i,j∈S

(cid:88)

Xi∈Sm

Here, the ﬁrst term measures the average relevance of features to the target, while the second term
measures average pairwise redundancy among selected features. Therefore, maximizing Φ(XS, Y )
leads to identifying a well-characterizing feature subset whose total information on the target is close
to the optimal feature subset’s. To maximize this objective, they used an inductive approach where
ﬁrst the most informative feature is chosen, and then next features are inductively added by solving
the following at every step:

arg max

Xj∈X\Sm

I(Xj; Y ) − 1

m − 1

2.2 Maximum Joint Relevance

I(Xj; Xi).

(4)

Although MRMR is a well-known feature selection method, there are several applications where the
test error rate never goes below some large thresholds like 34% which seems quite unsatisfactory.
Note that (3) includes only up to pairwise interactions. By considering higher order interactions we
can become able to select a more informative feature subset which in turn results in smaller error
rates. To this end, Maximum Joint Relevant (MJR) algorithm changes the inductive rule of (4) to a
more sensitive one [3]:

arg max

Xj∈X\Sm

I(Xj, Xi ; Y ).

(5)

(cid:88)

Xi∈Sm

Nevertheless, we may again encounter the issue of lack of enough samples to estimate the second
order mutual information appeared in the above formulation. As a matter of fact, a considerable
number of third order empirical marginals may become too small, and thus it requires a more accurate
estimation of mutual information than the empirical one. Therefore, in next section we are going
to propose a new algorithm to estimate mutual information with higher accuracy. As an important
advantage, this estimation technique reduces the required sample size to estimate mutual information
within the same accuracy.

3 Adaptive Maximum Joint Relevant

In this section, we propose the Adaptive Maximum Joint Relevant (AMJR) feature selection al-
gorithm to tackle the instability problem in MJR. Similar to MJR, we use the criterion in (5) to
iteratively select the most relevant features. However, we propose a new scheme to estimate the
mutual informations which stabilize the algorithm in small training set regimes. We build our es-
timation technique based on functional estimation method proposed in [4]. Speciﬁcally, in order to

2

estimate I(Xj, Xi; Y ) at each step, we have to estimate the joint entropies according to the following
identity:

I(Xj, Xi; Y ) = H(Xj, Xi) + H(Y ) − H(Xj, Xi, Y ).

(6)

In order to describe the estimation method in AMJR, consider for example, estimating H(Xj, Xi).

Following from [4], ﬁrst the empirical joint distribution of (Xj, Xi) is computed according to

1{(Xj, Xi)(t) = (a, b)},

(7)

n(cid:88)

t=1

ˆPa,b =

1
n

H(Xj, Xi) = − (cid:88)

a∈A, b∈B

where n is the size of training set and (Xj, Xi)(t) is the joint value of tth training example. Note that
a and b are assumed to take value in some ﬁnte sets A and B, respectively. Now, assuming that Pa,b
is the true joint probability of (Xj, Xi) at point (a, b), the true joint entropy would be

Pa,b log Pa,b.

(8)

In order to provide the estimator ˆH(Xj, Xi) of H(Xj, Xi), one naive way is substitute each Pa,b in
(8) with its estimate ˆPa,b. This method which is used in MJR, is in fact the source of instability on
the performance since most of the estimated probabilities are very small. In AMJR, we consider two
cases for the estimated joint probabilities:

• If ˆPa,b ≥ log n
• If ˆPa,b < log n

n , we use it as an estimation of Pa,b in (8).
n , ﬁrst we ﬁt a polynomial f of order (cid:98)log n(cid:99) to the function x log x in the interval

(0, log n

n ). Then, we use f ( ˆPa,b) as an estimation for Pa,b log Pa,b in (8).

As we see in Section 4, the approximation polynomial f introduces stability to the algorithm and
improves its performance. Consequently, the estimation of H(Xj, Xi) in AMJR would be

(cid:32) (cid:88)

(cid:88)

(cid:33)

ˆH(Xj, Xi) = −

ˆPa,b log ˆPa,b +

f ( ˆPa,b)

.

(9)

ˆPa,b≥ log n

n

ˆPa,b< log n
n

Similarly, the estimations ˆH(Xj, Xi, Y ) and ˆH(Y ) are provided for H(Xj, Xi, Y ) and H(Y ),

respectively. Finally, the mutual information is estimated as

ˆI(Xj, Xi; Y ) = ˆH(Xj, Xi) + ˆH(Y ) − ˆH(Xj, Xi, Y ).

(10)

4 Numerical Results

In this section we provide numerical results to conﬁrm our theoretical analysis. We perform diﬀerent
feature selection and classiﬁcation methods on the dataset ”Madelon” released in NIPS 2003 feature
selection challenge [5]. This data set consists of 2000 samples each containing 500 continuous input
features and one binary output response. Here we have used 1400 samples (70%) as the training set
and used the other 600 samples (30%) as the test set.

In order to explore the eﬀect of sample size on diﬀerent feature selection methods, we quantize the
input space into 3 and 5 levels, uniformly. Thus, we have two scenarios. In the ﬁrst one, the input
features are quantized separately into three levels which corresponds to the large training set regime

3

Figure 1: SVM classiﬁcation error for 3-level quantization of input space.

(since each level happens too many times and we have small number of probabilities to estimate).
In the second scenario, the input features are quantized separately into 5 levels. The later scenario
corresponds to a small training set regime where there are a large number of probabilities to estimate.
Figure 1 compares the misclassiﬁcation error of MRMR and MJR feature selection algorithms for
diﬀerent number of features. Here, SVM is used as the classiﬁcation method and the input space
is quantized into 3 levels. Since this scenario corresponds to large training set regime, the MJR
outperforms MRMR as depicted in the ﬁgure.

In Fig. 2, the SVM misclassiﬁcation error of MJR and AMJR has been compared for diﬀerent
number of selected features. Here, the input space is quantized into 5 level which corresponds to the
small training set scenario. As depicted in this ﬁgure, MJR has unstable performance in this scenario
while AMJR shows stable and better performance. This ﬁgure conﬁrms our theoretical analysis of
instability of MJR and shows that our proposed method (AMJR) removes the instability problem
almost completely.

The advantage of the proposed method AMJR method is further described in Fig. 3. In this
ﬁgure, the SVM misclassiﬁcation error of AMJR and MRMR methods are compared for diﬀernt
number of selected features. Here, the input space is quantized into 5 levels (small training set
regime). As depicted in this ﬁgure, AMJR substantially outperforms MRMR for any number of

Figure 2: SVM classiﬁcation error for 5-level quantization of input space.

4

51015200.20.250.30.350.40.450.5# selected featuresclassification error rate  MRMRMJR468101214161820220.10.150.20.250.30.350.40.450.5# selected featuresclassification error rate  AMJRMJRFigure 3: SVM classiﬁcation error for 5-level quantization of input space.

selected features.

It worth mentioning that other than SVM, we have also repeated the above experiments for
logistic regression and classiﬁcation trees and the same relative results were obtained. Since our
focus is on comparing the feature selection algorithms (and not the classiﬁcation methods), and also
due to the lack of space, the results for these methods are not provided here.

5 Conclusion

Feature selection is an indispensable part of solution when dealing with high-dimensional datasets.
One powerful tool to address this problem is mutual information. A common approach is to use Max-
imum Relevance Minimum Redundancy (MRMR) approach to solve the feature selection problem. In
this paper, based on insight from information theory, a new objective function is used. Also, a novel
mutual information estimator is used enabling us to discretize the data into ﬁner levels. Combining
the novel mutual information estimator with the new objective function, an error rate 3 times lower
than that of MRMR is demonstrated.

References

[1] T. Cover, and J. Thomas. ”Elements of information theory”, John Wiley & Sons, 2012.

[2] H. Peng, H. Long, and C. Ding, ”Feature selection based on mutual information criteria of max-
dependency, max-relevance, and min-redundancy.” Pattern Analysis and Machine Intelligence,
IEEE Transactions on 27.8, 2005, 1226-1238.

[3] H. Yang, and J. Moody. ”Data Visualization and Feature Selection: New Algorithms for Non-

gaussian Data.” NIPS. 1999.

[4] J. Jiao, K. Venkat, Y. Han, T. Weissman, ”Minimax Estimation of Functionals of Discrete Dis-

tributions”, available on arXiv. 2014.

[5] Available online: http://www.nipsfsc.ecs.soton.ac.uk/datasets

5

51015200.10.150.20.250.30.350.40.450.5# Selected Featuresclassification error  AMJRMRMR