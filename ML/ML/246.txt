The Lowest Form of Wit: Identifying Sarcasm in Social Media

Saachi Jain, Vivian Hsu

1

Abstract—Sarcasm detection is an important problem in text
classiﬁcation and has many applications in areas such as security,
health, and sales. Previous works have tried various features
such as sentiment, capitalization, and n-grams to classify sarcastic
texts. Downloading a corpus of tweets through Twitter Archiver,
we used multinomial naive Bayes, logistic regression, and support
vector machine to classify tweets as sarcastic or not sarcastic.
We found that unigrams and bigrams were the most indicative
features of sarcastic tweets, and we achieved an accuracy of
80.27% using logistic regression on a data set with oversampling
on sarcastic tweets.

I.

INTRODUCTION

Deﬁned by Merriam Webster as the use of words that
mean the opposite of what you really want to say, sarcasm
is a counterintuitive social construct that befuddles programs
and humans alike. Often cited as an exception in sentiment
classiﬁcation strategies, it reverses the intention of a concept
or idea by relying on grammatical structure, hyperbolical
vocabulary, and context.

example,

security agencies have

Sarcasm can be difﬁcult to spot, especially when expressed
through written text. Humans often mistake the true sentiment
that others convey in emails, messages, or posts on social
media. This mistake proves to be a problem in many different
contexts. For
trouble
differentiating false comments about planning terrorist attacks
from real ones. People also need to distinguish sarcastic
ratings and reviews from non-sarcastic ones on websites
such as Amazon and Yelp. Furthermore, some ﬁnd it hard to
distinguish tweets and posts that joke about depression from
those that are cries for help. A sarcasm detector can not only
help people interpret others’ writings, but can also assist the
writers themselves in avoiding being misunderstood.

An efﬁcient sarcasm detector has proved to be difﬁcult
to implement, as many companies and research groups have
tried to develop algorithms to detect sarcasm with varying
success. In this paper, we use a corpus of sarcastic and
non-sarcastic tweets,
isolating features such as n-grams,
capitalization, sentiment split, and subjectivity, to classify a
tweet as sarcastic or not sarcastic using multinomial naive
Bayes, logistic regression, and linear support vector machine
(SVM) classiﬁers.

II. RELATED WORK

Due to the signiﬁcance of sarcasm in text classiﬁcation,
several groups have already explored using machine learning
techniques to detect sarcasm in text. Throughout many of
these studies, social media, especially Twitter, is often the

primary data source for sarcastic and non-sarcastic texts.

A signiﬁcant challenge in using supervised learning on
sarcastic texts is annotating the corpus as sarcastic or not
sarcastic beforehand. Dr. Mathieu Cliche from Cornell
University separated tweets as sarcastic or not sarcastic
according to the presence of the hashtag #sarcasm, arguing
that
tweets with #sarcasm are likely to be truly sarcastic
tweets, and tweets without the tag, although they may contain
sarcastic tweets, have a large enough corpus of regular tweets
that the existent sarcastic samples in the set can be considered
noise. Liebrecht et. al from Radboud University Nijmegen and
Dr. David Bamman and Dr. Noah A. Smith from Carnegie
Mellon University employed similar techniques to create their
datasets. Gonzalez-Ibanez et. al from Rutgers University also
used #sarcasm to identify sarcastic tweets, but rather than
choosing non-sarcastic tweets as tweets lacking sarcastic
hashtags, he used tweets presenting positive or negative
tags (#happy, #sadness, #angry, etc) under the hypothesis
that
tweets with tags representing pure emotions are less
likely to be sarcastic. Although this approach caused the
non-sarcastic dataset
to be less representative of general
tweets, we thought that it is a better choice because it reduces
the noise associated with the non-sarcastic set
if it were
obtained by simply choosing tweets without “#sarcasm.”

Dr. Cliche, Liebrecht et. al, Bammam et. al, and Gonzalez-
Ibanez et. al all had n-grams as a critical feature in their
classiﬁers. In addition, Bammam et. al counted the number of
words in all caps in a tweet as a feature. Riloff et. al from the
University of Utah explored another feature involving a split
in sentiment using a bootstrapping algorithm. Speciﬁcally,
they found that sarcastic tweets were likely to have a positive
verb phrase juxtaposed to a negative activity or state (i.e.
“I love taking exams”). Their algorithm learned positive
sentiment phrases and negative activity or situation phrases to
recognize sarcastic tweets.

Several of the groups focused on logistic regression as a
promising classiﬁer for sarcastic tweets (Cliche, Gonzalez-
Ibanez et. al, Bammam et. al). Dr. Cliche also employed
multinomial naive Bayes and a linear SVM, and achieved an
F-score of 0.60. Liebrecht et. al from Radboud University
Nijmegen used a Balanced Winnow Classiﬁer, based off of
the perceptron algorithm, and obtained an accuracy of 75%.
Gonzalez-Ibanez et. al had a 66% accuracy with logistic regres-
sion and a 71% accuracy with sequential minimal optimization
algorithm (SMO). Bammam et. al used logistic regression to
get an accuracy rate of 85.1%. Overall, it seems that logistic
regression was the most popular and consistent classiﬁer, while
unigrams and bigrams were the more effective features.

The Lowest Form of Wit: Identifying Sarcasm in Social Media

Saachi Jain, Vivian Hsu

1

Abstract—Sarcasm detection is an important problem in text
classiﬁcation and has many applications in areas such as security,
health, and sales. Previous works have tried various features
such as sentiment, capitalization, and n-grams to classify sarcastic
texts. Downloading a corpus of tweets through Twitter Archiver,
we used multinomial naive Bayes, logistic regression, and support
vector machine to classify tweets as sarcastic or not sarcastic.
We found that unigrams and bigrams were the most indicative
features of sarcastic tweets, and we achieved an accuracy of
80.27% using logistic regression on a data set with oversampling
on sarcastic tweets.

I.

INTRODUCTION

Deﬁned by Merriam Webster as the use of words that
mean the opposite of what you really want to say, sarcasm
is a counterintuitive social construct that befuddles programs
and humans alike. Often cited as an exception in sentiment
classiﬁcation strategies, it reverses the intention of a concept
or idea by relying on grammatical structure, hyperbolical
vocabulary, and context.

example,

security agencies have

Sarcasm can be difﬁcult to spot, especially when expressed
through written text. Humans often mistake the true sentiment
that others convey in emails, messages, or posts on social
media. This mistake proves to be a problem in many different
contexts. For
trouble
differentiating false comments about planning terrorist attacks
from real ones. People also need to distinguish sarcastic
ratings and reviews from non-sarcastic ones on websites
such as Amazon and Yelp. Furthermore, some ﬁnd it hard to
distinguish tweets and posts that joke about depression from
those that are cries for help. A sarcasm detector can not only
help people interpret others’ writings, but can also assist the
writers themselves in avoiding being misunderstood.

An efﬁcient sarcasm detector has proved to be difﬁcult
to implement, as many companies and research groups have
tried to develop algorithms to detect sarcasm with varying
success. In this paper, we use a corpus of sarcastic and
non-sarcastic tweets,
isolating features such as n-grams,
capitalization, sentiment split, and subjectivity, to classify a
tweet as sarcastic or not sarcastic using multinomial naive
Bayes, logistic regression, and linear support vector machine
(SVM) classiﬁers.

II. RELATED WORK

Due to the signiﬁcance of sarcasm in text classiﬁcation,
several groups have already explored using machine learning
techniques to detect sarcasm in text. Throughout many of
these studies, social media, especially Twitter, is often the

primary data source for sarcastic and non-sarcastic texts.

A signiﬁcant challenge in using supervised learning on
sarcastic texts is annotating the corpus as sarcastic or not
sarcastic beforehand. Dr. Mathieu Cliche from Cornell
University separated tweets as sarcastic or not sarcastic
according to the presence of the hashtag #sarcasm, arguing
that
tweets with #sarcasm are likely to be truly sarcastic
tweets, and tweets without the tag, although they may contain
sarcastic tweets, have a large enough corpus of regular tweets
that the existent sarcastic samples in the set can be considered
noise. Liebrecht et. al from Radboud University Nijmegen and
Dr. David Bamman and Dr. Noah A. Smith from Carnegie
Mellon University employed similar techniques to create their
datasets. Gonzalez-Ibanez et. al from Rutgers University also
used #sarcasm to identify sarcastic tweets, but rather than
choosing non-sarcastic tweets as tweets lacking sarcastic
hashtags, he used tweets presenting positive or negative
tags (#happy, #sadness, #angry, etc) under the hypothesis
that
tweets with tags representing pure emotions are less
likely to be sarcastic. Although this approach caused the
non-sarcastic dataset
to be less representative of general
tweets, we thought that it is a better choice because it reduces
the noise associated with the non-sarcastic set
if it were
obtained by simply choosing tweets without “#sarcasm.”

Dr. Cliche, Liebrecht et. al, Bammam et. al, and Gonzalez-
Ibanez et. al all had n-grams as a critical feature in their
classiﬁers. In addition, Bammam et. al counted the number of
words in all caps in a tweet as a feature. Riloff et. al from the
University of Utah explored another feature involving a split
in sentiment using a bootstrapping algorithm. Speciﬁcally,
they found that sarcastic tweets were likely to have a positive
verb phrase juxtaposed to a negative activity or state (i.e.
“I love taking exams”). Their algorithm learned positive
sentiment phrases and negative activity or situation phrases to
recognize sarcastic tweets.

Several of the groups focused on logistic regression as a
promising classiﬁer for sarcastic tweets (Cliche, Gonzalez-
Ibanez et. al, Bammam et. al). Dr. Cliche also employed
multinomial naive Bayes and a linear SVM, and achieved an
F-score of 0.60. Liebrecht et. al from Radboud University
Nijmegen used a Balanced Winnow Classiﬁer, based off of
the perceptron algorithm, and obtained an accuracy of 75%.
Gonzalez-Ibanez et. al had a 66% accuracy with logistic regres-
sion and a 71% accuracy with sequential minimal optimization
algorithm (SMO). Bammam et. al used logistic regression to
get an accuracy rate of 85.1%. Overall, it seems that logistic
regression was the most popular and consistent classiﬁer, while
unigrams and bigrams were the more effective features.

III. DATA AND FEATURE EXTRACTION

IV. METHODS

2

Our dataset consisted of English tweets obtained from
Twitter
through Twitter Archiver, a Google add-on that
downloads tweets into a Google Spreadsheet based on ﬁlters
such as hashtags and language. We downloaded tweets
from November 10th to December 3rd, 2015. Based on
the assumption that the writers of the tweets are the best
people to judge whether their tweets are sarcastic or not,
we used hashtags to annotate sarcastic tweets. We obtained
sarcastic tweets by getting tweets with the hashtag #sarcastic.
We further assumed that any tweets with “emotional”
hashtags such as “happy,” “joy,” “lucky,” “sad,” “angry,” and
“disappointed” were non-sarcastic tweets expressing positive
or negative sentiment.

To clean the data, we ﬁltered out symbols and strings that
did not contribute to the overall meaning of the tweets. We
took out all words followed by a hashtag (“#”), all links to
other websites (tokens beginning with “http”), and all tags to
other accounts (tokens beginning with “@”). If a tweet has
fewer than three tokens left after cleaning, we took it out of
our data set. We were left with 26,206 sarcastic tweets and
101,361 non-sarcastic tweets in our dataset.

of

set

Our

feature

consisted

unigrams,

bigrams,
capitalization, sentiment-split, and subjectivity. Each unigram
and bigram was its own feature. We created the unigrams
by parsing the tweets into lemmatized individual words and
punctuation marks (“!”, “?”, and “,”), and counted the overall
occurrences of each in sarcastic texts and non-sarcastic texts.
We created the bigrams by parsing the tweets into pairs
of consecutive words, and also counted their frequencies
in sarcastic and non-sarcastic texts. However, we took out
punctuation marks in the bigrams because we wanted to
analyze the impact that two adjacent words have on whether
or not a tweet
is sarcastic, rather than whether or not a
punctuation mark precedes or follows a word. We then further
reduced our data set by eliminating unigrams and bigrams that
appear fewer than 10 times. In total, we had 8,223 unigrams
and 20,790 bigrams.

Aside from unigrams and bigrams, we extrapolated three
more features based on the overall context of the tweets. One
of the features was the number of words in all caps (greater
than one letter), as suggested by Bammam et. al. Another was
sentiment-split, which captures the difference in the sentiment
between the part of the tweet before the verb phrase and the
part of the tweet after the verb phrase. For example, if the tweet
was “I hate Christmas presents,” we used the library pattern.en
to split the tweet into two chunks: “I hate” and “Christmas
presents”. Using the NLP library TextBlob, we calculated the
sentiment score for both chunks (a score of from -1 to 1,
where -1 is very negative and 1 is very positive), and found
the difference between the two scores as the tweet’s sentiment-
split score. Finally, our last feature was the subjectivity score
(fact or opinion) of the entire tweet, which we also calculated
using TextBlob.

After collecting the data and extracting the features, we used
three classiﬁers (all from the library scikit-learn): multinomial
naive Bayes, logistic regression, and SVM, to predict whether
a tweet was sarcastic or not sarcastic.

A. Multinomial Naive Bayes
Theorem states

Bayes’

(x1, x2, ..., xm)
relationship holds:

and

resulting

that,

class

for
y,

feature
the

vector
following

P (y)P (x1, x2, ...xm|y)

P (y|x1, x2, ..., xm) =

(1)
Under a naive Bayes classiﬁer, we model P (x1, x2, ..., xm|y)
with the assumption that each xi is conditionally independent
on y. Thus, we can simplify (1) to be:

P (x1, x2, ..., xm)

m(cid:81)

P (y|x1, x2, ..., xm) =

P (y)

p(xi|y)

i=1

P (x1, x2, ..., xm)

(2)

After ﬁnding the prior distributions using maximum likelihood
estimates, we simply choose the class ˆy that gives the higher
posterior probability in (2).

Multinomial naive Bayes, a variation on the naive Bayes
algorithm above, is commonly used for text classiﬁcation. It
is parameterized by (θy1, θy2, θy3, ..., θyn) where θyi is the
probability of feature i appearing in the class y. Speciﬁcally,
scikit-learn’s version of multinomial naive Bayes that we used
further employed laplace smoothing, such that the parameters
were calculated as:

ˆθyi =

Nyi + 1
Ny + n

(3)

where Nyi is the number of times feature i appeared in class
y and Ny is the total count of features in y.

B. Logistic Regression

Under logistic regression, given a feature vector x, we use

the following hypothesis function:

hθ(x) = g(θT x) =

1

1 + e−θT x

(4)

where g is the sigmoid function.

Following most

linear classiﬁers, we set

the hypothesis

function as:

p(y|x; θ) = (hθ(x))y(1 − hθ(x))1−y

We then ﬁnd the likelihood of the parameters θ as:

m(cid:89)
m(cid:89)

i=1

i=1

L(θ) =

=

p(y(i)|x(i); θ)

(hθ(x(i)))y(i)

(1 − hθ(x(i)))1−y(i)

(5)

(6)

The Lowest Form of Wit: Identifying Sarcasm in Social Media

Saachi Jain, Vivian Hsu

1

Abstract—Sarcasm detection is an important problem in text
classiﬁcation and has many applications in areas such as security,
health, and sales. Previous works have tried various features
such as sentiment, capitalization, and n-grams to classify sarcastic
texts. Downloading a corpus of tweets through Twitter Archiver,
we used multinomial naive Bayes, logistic regression, and support
vector machine to classify tweets as sarcastic or not sarcastic.
We found that unigrams and bigrams were the most indicative
features of sarcastic tweets, and we achieved an accuracy of
80.27% using logistic regression on a data set with oversampling
on sarcastic tweets.

I.

INTRODUCTION

Deﬁned by Merriam Webster as the use of words that
mean the opposite of what you really want to say, sarcasm
is a counterintuitive social construct that befuddles programs
and humans alike. Often cited as an exception in sentiment
classiﬁcation strategies, it reverses the intention of a concept
or idea by relying on grammatical structure, hyperbolical
vocabulary, and context.

example,

security agencies have

Sarcasm can be difﬁcult to spot, especially when expressed
through written text. Humans often mistake the true sentiment
that others convey in emails, messages, or posts on social
media. This mistake proves to be a problem in many different
contexts. For
trouble
differentiating false comments about planning terrorist attacks
from real ones. People also need to distinguish sarcastic
ratings and reviews from non-sarcastic ones on websites
such as Amazon and Yelp. Furthermore, some ﬁnd it hard to
distinguish tweets and posts that joke about depression from
those that are cries for help. A sarcasm detector can not only
help people interpret others’ writings, but can also assist the
writers themselves in avoiding being misunderstood.

An efﬁcient sarcasm detector has proved to be difﬁcult
to implement, as many companies and research groups have
tried to develop algorithms to detect sarcasm with varying
success. In this paper, we use a corpus of sarcastic and
non-sarcastic tweets,
isolating features such as n-grams,
capitalization, sentiment split, and subjectivity, to classify a
tweet as sarcastic or not sarcastic using multinomial naive
Bayes, logistic regression, and linear support vector machine
(SVM) classiﬁers.

II. RELATED WORK

Due to the signiﬁcance of sarcasm in text classiﬁcation,
several groups have already explored using machine learning
techniques to detect sarcasm in text. Throughout many of
these studies, social media, especially Twitter, is often the

primary data source for sarcastic and non-sarcastic texts.

A signiﬁcant challenge in using supervised learning on
sarcastic texts is annotating the corpus as sarcastic or not
sarcastic beforehand. Dr. Mathieu Cliche from Cornell
University separated tweets as sarcastic or not sarcastic
according to the presence of the hashtag #sarcasm, arguing
that
tweets with #sarcasm are likely to be truly sarcastic
tweets, and tweets without the tag, although they may contain
sarcastic tweets, have a large enough corpus of regular tweets
that the existent sarcastic samples in the set can be considered
noise. Liebrecht et. al from Radboud University Nijmegen and
Dr. David Bamman and Dr. Noah A. Smith from Carnegie
Mellon University employed similar techniques to create their
datasets. Gonzalez-Ibanez et. al from Rutgers University also
used #sarcasm to identify sarcastic tweets, but rather than
choosing non-sarcastic tweets as tweets lacking sarcastic
hashtags, he used tweets presenting positive or negative
tags (#happy, #sadness, #angry, etc) under the hypothesis
that
tweets with tags representing pure emotions are less
likely to be sarcastic. Although this approach caused the
non-sarcastic dataset
to be less representative of general
tweets, we thought that it is a better choice because it reduces
the noise associated with the non-sarcastic set
if it were
obtained by simply choosing tweets without “#sarcasm.”

Dr. Cliche, Liebrecht et. al, Bammam et. al, and Gonzalez-
Ibanez et. al all had n-grams as a critical feature in their
classiﬁers. In addition, Bammam et. al counted the number of
words in all caps in a tweet as a feature. Riloff et. al from the
University of Utah explored another feature involving a split
in sentiment using a bootstrapping algorithm. Speciﬁcally,
they found that sarcastic tweets were likely to have a positive
verb phrase juxtaposed to a negative activity or state (i.e.
“I love taking exams”). Their algorithm learned positive
sentiment phrases and negative activity or situation phrases to
recognize sarcastic tweets.

Several of the groups focused on logistic regression as a
promising classiﬁer for sarcastic tweets (Cliche, Gonzalez-
Ibanez et. al, Bammam et. al). Dr. Cliche also employed
multinomial naive Bayes and a linear SVM, and achieved an
F-score of 0.60. Liebrecht et. al from Radboud University
Nijmegen used a Balanced Winnow Classiﬁer, based off of
the perceptron algorithm, and obtained an accuracy of 75%.
Gonzalez-Ibanez et. al had a 66% accuracy with logistic regres-
sion and a 71% accuracy with sequential minimal optimization
algorithm (SMO). Bammam et. al used logistic regression to
get an accuracy rate of 85.1%. Overall, it seems that logistic
regression was the most popular and consistent classiﬁer, while
unigrams and bigrams were the more effective features.

III. DATA AND FEATURE EXTRACTION

IV. METHODS

2

Our dataset consisted of English tweets obtained from
Twitter
through Twitter Archiver, a Google add-on that
downloads tweets into a Google Spreadsheet based on ﬁlters
such as hashtags and language. We downloaded tweets
from November 10th to December 3rd, 2015. Based on
the assumption that the writers of the tweets are the best
people to judge whether their tweets are sarcastic or not,
we used hashtags to annotate sarcastic tweets. We obtained
sarcastic tweets by getting tweets with the hashtag #sarcastic.
We further assumed that any tweets with “emotional”
hashtags such as “happy,” “joy,” “lucky,” “sad,” “angry,” and
“disappointed” were non-sarcastic tweets expressing positive
or negative sentiment.

To clean the data, we ﬁltered out symbols and strings that
did not contribute to the overall meaning of the tweets. We
took out all words followed by a hashtag (“#”), all links to
other websites (tokens beginning with “http”), and all tags to
other accounts (tokens beginning with “@”). If a tweet has
fewer than three tokens left after cleaning, we took it out of
our data set. We were left with 26,206 sarcastic tweets and
101,361 non-sarcastic tweets in our dataset.

of

set

Our

feature

consisted

unigrams,

bigrams,
capitalization, sentiment-split, and subjectivity. Each unigram
and bigram was its own feature. We created the unigrams
by parsing the tweets into lemmatized individual words and
punctuation marks (“!”, “?”, and “,”), and counted the overall
occurrences of each in sarcastic texts and non-sarcastic texts.
We created the bigrams by parsing the tweets into pairs
of consecutive words, and also counted their frequencies
in sarcastic and non-sarcastic texts. However, we took out
punctuation marks in the bigrams because we wanted to
analyze the impact that two adjacent words have on whether
or not a tweet
is sarcastic, rather than whether or not a
punctuation mark precedes or follows a word. We then further
reduced our data set by eliminating unigrams and bigrams that
appear fewer than 10 times. In total, we had 8,223 unigrams
and 20,790 bigrams.

Aside from unigrams and bigrams, we extrapolated three
more features based on the overall context of the tweets. One
of the features was the number of words in all caps (greater
than one letter), as suggested by Bammam et. al. Another was
sentiment-split, which captures the difference in the sentiment
between the part of the tweet before the verb phrase and the
part of the tweet after the verb phrase. For example, if the tweet
was “I hate Christmas presents,” we used the library pattern.en
to split the tweet into two chunks: “I hate” and “Christmas
presents”. Using the NLP library TextBlob, we calculated the
sentiment score for both chunks (a score of from -1 to 1,
where -1 is very negative and 1 is very positive), and found
the difference between the two scores as the tweet’s sentiment-
split score. Finally, our last feature was the subjectivity score
(fact or opinion) of the entire tweet, which we also calculated
using TextBlob.

After collecting the data and extracting the features, we used
three classiﬁers (all from the library scikit-learn): multinomial
naive Bayes, logistic regression, and SVM, to predict whether
a tweet was sarcastic or not sarcastic.

A. Multinomial Naive Bayes
Theorem states

Bayes’

(x1, x2, ..., xm)
relationship holds:

and

resulting

that,

class

for
y,

feature
the

vector
following

P (y)P (x1, x2, ...xm|y)

P (y|x1, x2, ..., xm) =

(1)
Under a naive Bayes classiﬁer, we model P (x1, x2, ..., xm|y)
with the assumption that each xi is conditionally independent
on y. Thus, we can simplify (1) to be:

P (x1, x2, ..., xm)

m(cid:81)

P (y|x1, x2, ..., xm) =

P (y)

p(xi|y)

i=1

P (x1, x2, ..., xm)

(2)

After ﬁnding the prior distributions using maximum likelihood
estimates, we simply choose the class ˆy that gives the higher
posterior probability in (2).

Multinomial naive Bayes, a variation on the naive Bayes
algorithm above, is commonly used for text classiﬁcation. It
is parameterized by (θy1, θy2, θy3, ..., θyn) where θyi is the
probability of feature i appearing in the class y. Speciﬁcally,
scikit-learn’s version of multinomial naive Bayes that we used
further employed laplace smoothing, such that the parameters
were calculated as:

ˆθyi =

Nyi + 1
Ny + n

(3)

where Nyi is the number of times feature i appeared in class
y and Ny is the total count of features in y.

B. Logistic Regression

Under logistic regression, given a feature vector x, we use

the following hypothesis function:

hθ(x) = g(θT x) =

1

1 + e−θT x

(4)

where g is the sigmoid function.

Following most

linear classiﬁers, we set

the hypothesis

function as:

p(y|x; θ) = (hθ(x))y(1 − hθ(x))1−y

We then ﬁnd the likelihood of the parameters θ as:

m(cid:89)
m(cid:89)

i=1

i=1

L(θ) =

=

p(y(i)|x(i); θ)

(hθ(x(i)))y(i)

(1 − hθ(x(i)))1−y(i)

(5)

(6)

We can then maximize the likelihood by maximizing the log

likelihood:

(cid:96)(θ) = log L(θ)

m(cid:88)

i=1

=

y(i) log h(x(i)) + (1 − y(i)) log(1 − h(x(i)))

(7)

In order to maximize the log likelihood, we use gradient ascent
over a period of updates. Our gradient ascent update rule will
then be given by:

θ := θ + α∇θ(cid:96)(θ)

∇θ(cid:96)(θ) simipliﬁes to:
∇θ(cid:96)(θ) =

m(cid:88)

(y(i) − hθ(x(i)))x(i)

(8)

(9)

i=1

Then gradient ascent is performed until convergence to create
the parameter vector θ. After training, tests can be performed
by calculating the value of the hypothesis hθ(x) = g(θT x)

C. Support Vector Machine (SVM)

We used scikit-learn’s linear support vector classiﬁcation
algorithm, which is an implementation of
linear support
vector machine that scales better to large numbers of samples.

In support vector machine, we denote the class that each
point xi belongs to by yi, which is either 1 or −1. SVM works
by ﬁnding the maximum-margin hyperplane that divides the
xi’s for which yi = 1 from the xi’s for which yi = −1. A
hyperplane is a set of points x such that w · x − b = 0, in
which w = [θ1...θn]T and b = θ0. We need to solve the dual
optimization problem:

m(cid:88)

i=1

αi − 1
2

m(cid:88)

i,j=1

max

α

W (α) =

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105) (10)

s.t. 0 ≤ αi ≤ C, i = 1, ..., m and
αiy(i) = 0, where
the ai’s are Lagrange multipliers and C controls the relative
weighing between the goals of making the ||w||2 small and of
ensuring that most examples have functional margins at least
1 in (cid:96)1 regularization. In our case, C = 1, the default set by
scikit-learn.

i=1

We then use the sequential minimal optimization (SMO)

m(cid:80)

algorithm to solve the dual problem:
Repeat until convergence {

1) Select some pair αi and αj to update.
2) Reoptimize W (α) with respect to αi and αj, while

holding the other αk’s ﬁxed.

}

The decision function used by scikit-learn is:

sgn(

yiαi(cid:104)xi, x(cid:105) + ρ)

(11)

m(cid:88)

where ρ is the intercept term.

i=1

V. RESULTS AND DISCUSSION

3

A. Original Runs: Unbalanced Data Set

We partitioned 70% of our tweets into a training set, and
30% into a testing set. We then ran each of the three models
on the two sets. To see that our models have learned from
the features, we also ran them under the condition that each
tweet had only one feature, randomly assigned to be 0 or 1.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Accuracy Random
0.7843
0.8250
0.7843
0.8247
0.7986
0.7843

Table 1: Model ccuracy when splitting 70% of the samples

into the training set and 30% into the test set

Here multinomial naive Bayes and logistic regression were
more effective than SVM. However, because the dataset was
heavily unbalanced with far more non-sarcastic samples than
sarcastic samples, the random classiﬁcation rate was close to
the model classiﬁcation rates. To more closely evaluate the
differences in the three models, we found the confusion matrix
to report the false positive and false negative rates.

Figure 1: False positive and negative rates for multinomial

naive Bayes, logistic regression, and SVM

For all three models, the false positive rate (non-sarcastic
samples classiﬁed as sarcastic) was lower than the false
negative rate (sarcastic samples labelled as non-sarcastic). We
believe this may be due to the fact
the dataset was
unbalanced, and took measures to address this issue later in
the report (see Oversampling, Undersampling).

that

Furthermore, we found the precision recall-curves for each
of the three models. Multinomial naive Bayes performed the
best in terms of the precision/recall tradeoff, while SVM per-
formed the worst. This result matches the accuracies presented
in Table 1.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Average Precision

0.6236
0.5862
0.5230

Table 2: Average precision accuracy (AUC for PR curve) for

each model

The Lowest Form of Wit: Identifying Sarcasm in Social Media

Saachi Jain, Vivian Hsu

1

Abstract—Sarcasm detection is an important problem in text
classiﬁcation and has many applications in areas such as security,
health, and sales. Previous works have tried various features
such as sentiment, capitalization, and n-grams to classify sarcastic
texts. Downloading a corpus of tweets through Twitter Archiver,
we used multinomial naive Bayes, logistic regression, and support
vector machine to classify tweets as sarcastic or not sarcastic.
We found that unigrams and bigrams were the most indicative
features of sarcastic tweets, and we achieved an accuracy of
80.27% using logistic regression on a data set with oversampling
on sarcastic tweets.

I.

INTRODUCTION

Deﬁned by Merriam Webster as the use of words that
mean the opposite of what you really want to say, sarcasm
is a counterintuitive social construct that befuddles programs
and humans alike. Often cited as an exception in sentiment
classiﬁcation strategies, it reverses the intention of a concept
or idea by relying on grammatical structure, hyperbolical
vocabulary, and context.

example,

security agencies have

Sarcasm can be difﬁcult to spot, especially when expressed
through written text. Humans often mistake the true sentiment
that others convey in emails, messages, or posts on social
media. This mistake proves to be a problem in many different
contexts. For
trouble
differentiating false comments about planning terrorist attacks
from real ones. People also need to distinguish sarcastic
ratings and reviews from non-sarcastic ones on websites
such as Amazon and Yelp. Furthermore, some ﬁnd it hard to
distinguish tweets and posts that joke about depression from
those that are cries for help. A sarcasm detector can not only
help people interpret others’ writings, but can also assist the
writers themselves in avoiding being misunderstood.

An efﬁcient sarcasm detector has proved to be difﬁcult
to implement, as many companies and research groups have
tried to develop algorithms to detect sarcasm with varying
success. In this paper, we use a corpus of sarcastic and
non-sarcastic tweets,
isolating features such as n-grams,
capitalization, sentiment split, and subjectivity, to classify a
tweet as sarcastic or not sarcastic using multinomial naive
Bayes, logistic regression, and linear support vector machine
(SVM) classiﬁers.

II. RELATED WORK

Due to the signiﬁcance of sarcasm in text classiﬁcation,
several groups have already explored using machine learning
techniques to detect sarcasm in text. Throughout many of
these studies, social media, especially Twitter, is often the

primary data source for sarcastic and non-sarcastic texts.

A signiﬁcant challenge in using supervised learning on
sarcastic texts is annotating the corpus as sarcastic or not
sarcastic beforehand. Dr. Mathieu Cliche from Cornell
University separated tweets as sarcastic or not sarcastic
according to the presence of the hashtag #sarcasm, arguing
that
tweets with #sarcasm are likely to be truly sarcastic
tweets, and tweets without the tag, although they may contain
sarcastic tweets, have a large enough corpus of regular tweets
that the existent sarcastic samples in the set can be considered
noise. Liebrecht et. al from Radboud University Nijmegen and
Dr. David Bamman and Dr. Noah A. Smith from Carnegie
Mellon University employed similar techniques to create their
datasets. Gonzalez-Ibanez et. al from Rutgers University also
used #sarcasm to identify sarcastic tweets, but rather than
choosing non-sarcastic tweets as tweets lacking sarcastic
hashtags, he used tweets presenting positive or negative
tags (#happy, #sadness, #angry, etc) under the hypothesis
that
tweets with tags representing pure emotions are less
likely to be sarcastic. Although this approach caused the
non-sarcastic dataset
to be less representative of general
tweets, we thought that it is a better choice because it reduces
the noise associated with the non-sarcastic set
if it were
obtained by simply choosing tweets without “#sarcasm.”

Dr. Cliche, Liebrecht et. al, Bammam et. al, and Gonzalez-
Ibanez et. al all had n-grams as a critical feature in their
classiﬁers. In addition, Bammam et. al counted the number of
words in all caps in a tweet as a feature. Riloff et. al from the
University of Utah explored another feature involving a split
in sentiment using a bootstrapping algorithm. Speciﬁcally,
they found that sarcastic tweets were likely to have a positive
verb phrase juxtaposed to a negative activity or state (i.e.
“I love taking exams”). Their algorithm learned positive
sentiment phrases and negative activity or situation phrases to
recognize sarcastic tweets.

Several of the groups focused on logistic regression as a
promising classiﬁer for sarcastic tweets (Cliche, Gonzalez-
Ibanez et. al, Bammam et. al). Dr. Cliche also employed
multinomial naive Bayes and a linear SVM, and achieved an
F-score of 0.60. Liebrecht et. al from Radboud University
Nijmegen used a Balanced Winnow Classiﬁer, based off of
the perceptron algorithm, and obtained an accuracy of 75%.
Gonzalez-Ibanez et. al had a 66% accuracy with logistic regres-
sion and a 71% accuracy with sequential minimal optimization
algorithm (SMO). Bammam et. al used logistic regression to
get an accuracy rate of 85.1%. Overall, it seems that logistic
regression was the most popular and consistent classiﬁer, while
unigrams and bigrams were the more effective features.

III. DATA AND FEATURE EXTRACTION

IV. METHODS

2

Our dataset consisted of English tweets obtained from
Twitter
through Twitter Archiver, a Google add-on that
downloads tweets into a Google Spreadsheet based on ﬁlters
such as hashtags and language. We downloaded tweets
from November 10th to December 3rd, 2015. Based on
the assumption that the writers of the tweets are the best
people to judge whether their tweets are sarcastic or not,
we used hashtags to annotate sarcastic tweets. We obtained
sarcastic tweets by getting tweets with the hashtag #sarcastic.
We further assumed that any tweets with “emotional”
hashtags such as “happy,” “joy,” “lucky,” “sad,” “angry,” and
“disappointed” were non-sarcastic tweets expressing positive
or negative sentiment.

To clean the data, we ﬁltered out symbols and strings that
did not contribute to the overall meaning of the tweets. We
took out all words followed by a hashtag (“#”), all links to
other websites (tokens beginning with “http”), and all tags to
other accounts (tokens beginning with “@”). If a tweet has
fewer than three tokens left after cleaning, we took it out of
our data set. We were left with 26,206 sarcastic tweets and
101,361 non-sarcastic tweets in our dataset.

of

set

Our

feature

consisted

unigrams,

bigrams,
capitalization, sentiment-split, and subjectivity. Each unigram
and bigram was its own feature. We created the unigrams
by parsing the tweets into lemmatized individual words and
punctuation marks (“!”, “?”, and “,”), and counted the overall
occurrences of each in sarcastic texts and non-sarcastic texts.
We created the bigrams by parsing the tweets into pairs
of consecutive words, and also counted their frequencies
in sarcastic and non-sarcastic texts. However, we took out
punctuation marks in the bigrams because we wanted to
analyze the impact that two adjacent words have on whether
or not a tweet
is sarcastic, rather than whether or not a
punctuation mark precedes or follows a word. We then further
reduced our data set by eliminating unigrams and bigrams that
appear fewer than 10 times. In total, we had 8,223 unigrams
and 20,790 bigrams.

Aside from unigrams and bigrams, we extrapolated three
more features based on the overall context of the tweets. One
of the features was the number of words in all caps (greater
than one letter), as suggested by Bammam et. al. Another was
sentiment-split, which captures the difference in the sentiment
between the part of the tweet before the verb phrase and the
part of the tweet after the verb phrase. For example, if the tweet
was “I hate Christmas presents,” we used the library pattern.en
to split the tweet into two chunks: “I hate” and “Christmas
presents”. Using the NLP library TextBlob, we calculated the
sentiment score for both chunks (a score of from -1 to 1,
where -1 is very negative and 1 is very positive), and found
the difference between the two scores as the tweet’s sentiment-
split score. Finally, our last feature was the subjectivity score
(fact or opinion) of the entire tweet, which we also calculated
using TextBlob.

After collecting the data and extracting the features, we used
three classiﬁers (all from the library scikit-learn): multinomial
naive Bayes, logistic regression, and SVM, to predict whether
a tweet was sarcastic or not sarcastic.

A. Multinomial Naive Bayes
Theorem states

Bayes’

(x1, x2, ..., xm)
relationship holds:

and

resulting

that,

class

for
y,

feature
the

vector
following

P (y)P (x1, x2, ...xm|y)

P (y|x1, x2, ..., xm) =

(1)
Under a naive Bayes classiﬁer, we model P (x1, x2, ..., xm|y)
with the assumption that each xi is conditionally independent
on y. Thus, we can simplify (1) to be:

P (x1, x2, ..., xm)

m(cid:81)

P (y|x1, x2, ..., xm) =

P (y)

p(xi|y)

i=1

P (x1, x2, ..., xm)

(2)

After ﬁnding the prior distributions using maximum likelihood
estimates, we simply choose the class ˆy that gives the higher
posterior probability in (2).

Multinomial naive Bayes, a variation on the naive Bayes
algorithm above, is commonly used for text classiﬁcation. It
is parameterized by (θy1, θy2, θy3, ..., θyn) where θyi is the
probability of feature i appearing in the class y. Speciﬁcally,
scikit-learn’s version of multinomial naive Bayes that we used
further employed laplace smoothing, such that the parameters
were calculated as:

ˆθyi =

Nyi + 1
Ny + n

(3)

where Nyi is the number of times feature i appeared in class
y and Ny is the total count of features in y.

B. Logistic Regression

Under logistic regression, given a feature vector x, we use

the following hypothesis function:

hθ(x) = g(θT x) =

1

1 + e−θT x

(4)

where g is the sigmoid function.

Following most

linear classiﬁers, we set

the hypothesis

function as:

p(y|x; θ) = (hθ(x))y(1 − hθ(x))1−y

We then ﬁnd the likelihood of the parameters θ as:

m(cid:89)
m(cid:89)

i=1

i=1

L(θ) =

=

p(y(i)|x(i); θ)

(hθ(x(i)))y(i)

(1 − hθ(x(i)))1−y(i)

(5)

(6)

We can then maximize the likelihood by maximizing the log

likelihood:

(cid:96)(θ) = log L(θ)

m(cid:88)

i=1

=

y(i) log h(x(i)) + (1 − y(i)) log(1 − h(x(i)))

(7)

In order to maximize the log likelihood, we use gradient ascent
over a period of updates. Our gradient ascent update rule will
then be given by:

θ := θ + α∇θ(cid:96)(θ)

∇θ(cid:96)(θ) simipliﬁes to:
∇θ(cid:96)(θ) =

m(cid:88)

(y(i) − hθ(x(i)))x(i)

(8)

(9)

i=1

Then gradient ascent is performed until convergence to create
the parameter vector θ. After training, tests can be performed
by calculating the value of the hypothesis hθ(x) = g(θT x)

C. Support Vector Machine (SVM)

We used scikit-learn’s linear support vector classiﬁcation
algorithm, which is an implementation of
linear support
vector machine that scales better to large numbers of samples.

In support vector machine, we denote the class that each
point xi belongs to by yi, which is either 1 or −1. SVM works
by ﬁnding the maximum-margin hyperplane that divides the
xi’s for which yi = 1 from the xi’s for which yi = −1. A
hyperplane is a set of points x such that w · x − b = 0, in
which w = [θ1...θn]T and b = θ0. We need to solve the dual
optimization problem:

m(cid:88)

i=1

αi − 1
2

m(cid:88)

i,j=1

max

α

W (α) =

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105) (10)

s.t. 0 ≤ αi ≤ C, i = 1, ..., m and
αiy(i) = 0, where
the ai’s are Lagrange multipliers and C controls the relative
weighing between the goals of making the ||w||2 small and of
ensuring that most examples have functional margins at least
1 in (cid:96)1 regularization. In our case, C = 1, the default set by
scikit-learn.

i=1

We then use the sequential minimal optimization (SMO)

m(cid:80)

algorithm to solve the dual problem:
Repeat until convergence {

1) Select some pair αi and αj to update.
2) Reoptimize W (α) with respect to αi and αj, while

holding the other αk’s ﬁxed.

}

The decision function used by scikit-learn is:

sgn(

yiαi(cid:104)xi, x(cid:105) + ρ)

(11)

m(cid:88)

where ρ is the intercept term.

i=1

V. RESULTS AND DISCUSSION

3

A. Original Runs: Unbalanced Data Set

We partitioned 70% of our tweets into a training set, and
30% into a testing set. We then ran each of the three models
on the two sets. To see that our models have learned from
the features, we also ran them under the condition that each
tweet had only one feature, randomly assigned to be 0 or 1.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Accuracy Random
0.7843
0.8250
0.7843
0.8247
0.7986
0.7843

Table 1: Model ccuracy when splitting 70% of the samples

into the training set and 30% into the test set

Here multinomial naive Bayes and logistic regression were
more effective than SVM. However, because the dataset was
heavily unbalanced with far more non-sarcastic samples than
sarcastic samples, the random classiﬁcation rate was close to
the model classiﬁcation rates. To more closely evaluate the
differences in the three models, we found the confusion matrix
to report the false positive and false negative rates.

Figure 1: False positive and negative rates for multinomial

naive Bayes, logistic regression, and SVM

For all three models, the false positive rate (non-sarcastic
samples classiﬁed as sarcastic) was lower than the false
negative rate (sarcastic samples labelled as non-sarcastic). We
believe this may be due to the fact
the dataset was
unbalanced, and took measures to address this issue later in
the report (see Oversampling, Undersampling).

that

Furthermore, we found the precision recall-curves for each
of the three models. Multinomial naive Bayes performed the
best in terms of the precision/recall tradeoff, while SVM per-
formed the worst. This result matches the accuracies presented
in Table 1.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Average Precision

0.6236
0.5862
0.5230

Table 2: Average precision accuracy (AUC for PR curve) for

each model

4

The false negative rate was signiﬁcantly lower, which
indicates that the training phase may be more effective with
a balanced data set because the classiﬁer was no longer
automatically classifying the samples as not sarcastic. The
false positive rate was slightly higher, because more samples
were now being classiﬁed as sarcastic.

Finally, we found the precision-recall curves under the new
data set. Both the precision and the recall improved when
training on the oversampled set. Among the three curves,
multinomial naive Bayes continued to have the highest average
precision while SVM performed relatively poorly.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Average Precision

0.8855
0.8797
0.8549

Table 5: Average precision accuracy (AUC for PR curve) for

each model with an oversampled data set

Figure 4: Precision-recall curves for oversampled data set

C. Undersampling

Undersampling involved taking only 1

4th of the non-sarcastic
tweets for the testing and training sets each time so that both
sarcastic and non-sarcastic sets had around 25,000 tweets. We
performed undersampling 4 times (each with a different quarter
of the non-sarcastic set) and averaged the accuracies in Table
6.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Accuracy Random
0.5225
0.7308
0.7235
0.5225
0.5225
0.6955

Figure 2: Precision and recall curves

We further performed 4-fold cross validation using scikit-
learn’s cross validation module on each of the three models.
Here logistic regression performed better than multinomial
naive Bayes, but the accuracies were relatively close.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Accuracy
0.8341
0.8475
0.8284

Table 3: Accuracy with 4-fold cross validation

B. Oversampling

In order to achieve a more balanced data set, we imple-
mented oversampling and undersampling. In oversampling, the
samples belonging to the more scarce class are overrepresented
in the ﬁnal data set. Thus, we had each sarcastic tweet occur
4 times in the data set, so that both sarcastic and non-
sarcastic sets had about 100,000 samples. Thus, the random
classiﬁcation rate was closer to 50%. We again split the ﬁnal
data set into 70% train and 30% test. Under the oversampled
set, logistic regression had the highest accuracy.

Model Name

Multinomial Naive Bayes

Accuracy Random
0.5246
0.7807
0.5246
0.8027
0.7997
0.5246
Table 4: Accuracy with an oversampled data set

Logistic Regression

Linear SVM

We again found the confusion matrix to analyze false

positive and false negative rates.

Figure 3: False positive/negative rates for oversampled data

Table 6: Accuracy with an undersampled data set

Similar

to the unbalanced data set, multinomial naive
Bayes and logistic regression were most effective while SVM
performed more poorly. Overall, comparing the unbalanced,
oversampled, and undersampled sets,
the oversampled set
had the greatest improvement in classiﬁcation accuracy over
the random classiﬁer (Figure 5). The oversampled set may
have performed better than the undersampled set because it

The Lowest Form of Wit: Identifying Sarcasm in Social Media

Saachi Jain, Vivian Hsu

1

Abstract—Sarcasm detection is an important problem in text
classiﬁcation and has many applications in areas such as security,
health, and sales. Previous works have tried various features
such as sentiment, capitalization, and n-grams to classify sarcastic
texts. Downloading a corpus of tweets through Twitter Archiver,
we used multinomial naive Bayes, logistic regression, and support
vector machine to classify tweets as sarcastic or not sarcastic.
We found that unigrams and bigrams were the most indicative
features of sarcastic tweets, and we achieved an accuracy of
80.27% using logistic regression on a data set with oversampling
on sarcastic tweets.

I.

INTRODUCTION

Deﬁned by Merriam Webster as the use of words that
mean the opposite of what you really want to say, sarcasm
is a counterintuitive social construct that befuddles programs
and humans alike. Often cited as an exception in sentiment
classiﬁcation strategies, it reverses the intention of a concept
or idea by relying on grammatical structure, hyperbolical
vocabulary, and context.

example,

security agencies have

Sarcasm can be difﬁcult to spot, especially when expressed
through written text. Humans often mistake the true sentiment
that others convey in emails, messages, or posts on social
media. This mistake proves to be a problem in many different
contexts. For
trouble
differentiating false comments about planning terrorist attacks
from real ones. People also need to distinguish sarcastic
ratings and reviews from non-sarcastic ones on websites
such as Amazon and Yelp. Furthermore, some ﬁnd it hard to
distinguish tweets and posts that joke about depression from
those that are cries for help. A sarcasm detector can not only
help people interpret others’ writings, but can also assist the
writers themselves in avoiding being misunderstood.

An efﬁcient sarcasm detector has proved to be difﬁcult
to implement, as many companies and research groups have
tried to develop algorithms to detect sarcasm with varying
success. In this paper, we use a corpus of sarcastic and
non-sarcastic tweets,
isolating features such as n-grams,
capitalization, sentiment split, and subjectivity, to classify a
tweet as sarcastic or not sarcastic using multinomial naive
Bayes, logistic regression, and linear support vector machine
(SVM) classiﬁers.

II. RELATED WORK

Due to the signiﬁcance of sarcasm in text classiﬁcation,
several groups have already explored using machine learning
techniques to detect sarcasm in text. Throughout many of
these studies, social media, especially Twitter, is often the

primary data source for sarcastic and non-sarcastic texts.

A signiﬁcant challenge in using supervised learning on
sarcastic texts is annotating the corpus as sarcastic or not
sarcastic beforehand. Dr. Mathieu Cliche from Cornell
University separated tweets as sarcastic or not sarcastic
according to the presence of the hashtag #sarcasm, arguing
that
tweets with #sarcasm are likely to be truly sarcastic
tweets, and tweets without the tag, although they may contain
sarcastic tweets, have a large enough corpus of regular tweets
that the existent sarcastic samples in the set can be considered
noise. Liebrecht et. al from Radboud University Nijmegen and
Dr. David Bamman and Dr. Noah A. Smith from Carnegie
Mellon University employed similar techniques to create their
datasets. Gonzalez-Ibanez et. al from Rutgers University also
used #sarcasm to identify sarcastic tweets, but rather than
choosing non-sarcastic tweets as tweets lacking sarcastic
hashtags, he used tweets presenting positive or negative
tags (#happy, #sadness, #angry, etc) under the hypothesis
that
tweets with tags representing pure emotions are less
likely to be sarcastic. Although this approach caused the
non-sarcastic dataset
to be less representative of general
tweets, we thought that it is a better choice because it reduces
the noise associated with the non-sarcastic set
if it were
obtained by simply choosing tweets without “#sarcasm.”

Dr. Cliche, Liebrecht et. al, Bammam et. al, and Gonzalez-
Ibanez et. al all had n-grams as a critical feature in their
classiﬁers. In addition, Bammam et. al counted the number of
words in all caps in a tweet as a feature. Riloff et. al from the
University of Utah explored another feature involving a split
in sentiment using a bootstrapping algorithm. Speciﬁcally,
they found that sarcastic tweets were likely to have a positive
verb phrase juxtaposed to a negative activity or state (i.e.
“I love taking exams”). Their algorithm learned positive
sentiment phrases and negative activity or situation phrases to
recognize sarcastic tweets.

Several of the groups focused on logistic regression as a
promising classiﬁer for sarcastic tweets (Cliche, Gonzalez-
Ibanez et. al, Bammam et. al). Dr. Cliche also employed
multinomial naive Bayes and a linear SVM, and achieved an
F-score of 0.60. Liebrecht et. al from Radboud University
Nijmegen used a Balanced Winnow Classiﬁer, based off of
the perceptron algorithm, and obtained an accuracy of 75%.
Gonzalez-Ibanez et. al had a 66% accuracy with logistic regres-
sion and a 71% accuracy with sequential minimal optimization
algorithm (SMO). Bammam et. al used logistic regression to
get an accuracy rate of 85.1%. Overall, it seems that logistic
regression was the most popular and consistent classiﬁer, while
unigrams and bigrams were the more effective features.

III. DATA AND FEATURE EXTRACTION

IV. METHODS

2

Our dataset consisted of English tweets obtained from
Twitter
through Twitter Archiver, a Google add-on that
downloads tweets into a Google Spreadsheet based on ﬁlters
such as hashtags and language. We downloaded tweets
from November 10th to December 3rd, 2015. Based on
the assumption that the writers of the tweets are the best
people to judge whether their tweets are sarcastic or not,
we used hashtags to annotate sarcastic tweets. We obtained
sarcastic tweets by getting tweets with the hashtag #sarcastic.
We further assumed that any tweets with “emotional”
hashtags such as “happy,” “joy,” “lucky,” “sad,” “angry,” and
“disappointed” were non-sarcastic tweets expressing positive
or negative sentiment.

To clean the data, we ﬁltered out symbols and strings that
did not contribute to the overall meaning of the tweets. We
took out all words followed by a hashtag (“#”), all links to
other websites (tokens beginning with “http”), and all tags to
other accounts (tokens beginning with “@”). If a tweet has
fewer than three tokens left after cleaning, we took it out of
our data set. We were left with 26,206 sarcastic tweets and
101,361 non-sarcastic tweets in our dataset.

of

set

Our

feature

consisted

unigrams,

bigrams,
capitalization, sentiment-split, and subjectivity. Each unigram
and bigram was its own feature. We created the unigrams
by parsing the tweets into lemmatized individual words and
punctuation marks (“!”, “?”, and “,”), and counted the overall
occurrences of each in sarcastic texts and non-sarcastic texts.
We created the bigrams by parsing the tweets into pairs
of consecutive words, and also counted their frequencies
in sarcastic and non-sarcastic texts. However, we took out
punctuation marks in the bigrams because we wanted to
analyze the impact that two adjacent words have on whether
or not a tweet
is sarcastic, rather than whether or not a
punctuation mark precedes or follows a word. We then further
reduced our data set by eliminating unigrams and bigrams that
appear fewer than 10 times. In total, we had 8,223 unigrams
and 20,790 bigrams.

Aside from unigrams and bigrams, we extrapolated three
more features based on the overall context of the tweets. One
of the features was the number of words in all caps (greater
than one letter), as suggested by Bammam et. al. Another was
sentiment-split, which captures the difference in the sentiment
between the part of the tweet before the verb phrase and the
part of the tweet after the verb phrase. For example, if the tweet
was “I hate Christmas presents,” we used the library pattern.en
to split the tweet into two chunks: “I hate” and “Christmas
presents”. Using the NLP library TextBlob, we calculated the
sentiment score for both chunks (a score of from -1 to 1,
where -1 is very negative and 1 is very positive), and found
the difference between the two scores as the tweet’s sentiment-
split score. Finally, our last feature was the subjectivity score
(fact or opinion) of the entire tweet, which we also calculated
using TextBlob.

After collecting the data and extracting the features, we used
three classiﬁers (all from the library scikit-learn): multinomial
naive Bayes, logistic regression, and SVM, to predict whether
a tweet was sarcastic or not sarcastic.

A. Multinomial Naive Bayes
Theorem states

Bayes’

(x1, x2, ..., xm)
relationship holds:

and

resulting

that,

class

for
y,

feature
the

vector
following

P (y)P (x1, x2, ...xm|y)

P (y|x1, x2, ..., xm) =

(1)
Under a naive Bayes classiﬁer, we model P (x1, x2, ..., xm|y)
with the assumption that each xi is conditionally independent
on y. Thus, we can simplify (1) to be:

P (x1, x2, ..., xm)

m(cid:81)

P (y|x1, x2, ..., xm) =

P (y)

p(xi|y)

i=1

P (x1, x2, ..., xm)

(2)

After ﬁnding the prior distributions using maximum likelihood
estimates, we simply choose the class ˆy that gives the higher
posterior probability in (2).

Multinomial naive Bayes, a variation on the naive Bayes
algorithm above, is commonly used for text classiﬁcation. It
is parameterized by (θy1, θy2, θy3, ..., θyn) where θyi is the
probability of feature i appearing in the class y. Speciﬁcally,
scikit-learn’s version of multinomial naive Bayes that we used
further employed laplace smoothing, such that the parameters
were calculated as:

ˆθyi =

Nyi + 1
Ny + n

(3)

where Nyi is the number of times feature i appeared in class
y and Ny is the total count of features in y.

B. Logistic Regression

Under logistic regression, given a feature vector x, we use

the following hypothesis function:

hθ(x) = g(θT x) =

1

1 + e−θT x

(4)

where g is the sigmoid function.

Following most

linear classiﬁers, we set

the hypothesis

function as:

p(y|x; θ) = (hθ(x))y(1 − hθ(x))1−y

We then ﬁnd the likelihood of the parameters θ as:

m(cid:89)
m(cid:89)

i=1

i=1

L(θ) =

=

p(y(i)|x(i); θ)

(hθ(x(i)))y(i)

(1 − hθ(x(i)))1−y(i)

(5)

(6)

We can then maximize the likelihood by maximizing the log

likelihood:

(cid:96)(θ) = log L(θ)

m(cid:88)

i=1

=

y(i) log h(x(i)) + (1 − y(i)) log(1 − h(x(i)))

(7)

In order to maximize the log likelihood, we use gradient ascent
over a period of updates. Our gradient ascent update rule will
then be given by:

θ := θ + α∇θ(cid:96)(θ)

∇θ(cid:96)(θ) simipliﬁes to:
∇θ(cid:96)(θ) =

m(cid:88)

(y(i) − hθ(x(i)))x(i)

(8)

(9)

i=1

Then gradient ascent is performed until convergence to create
the parameter vector θ. After training, tests can be performed
by calculating the value of the hypothesis hθ(x) = g(θT x)

C. Support Vector Machine (SVM)

We used scikit-learn’s linear support vector classiﬁcation
algorithm, which is an implementation of
linear support
vector machine that scales better to large numbers of samples.

In support vector machine, we denote the class that each
point xi belongs to by yi, which is either 1 or −1. SVM works
by ﬁnding the maximum-margin hyperplane that divides the
xi’s for which yi = 1 from the xi’s for which yi = −1. A
hyperplane is a set of points x such that w · x − b = 0, in
which w = [θ1...θn]T and b = θ0. We need to solve the dual
optimization problem:

m(cid:88)

i=1

αi − 1
2

m(cid:88)

i,j=1

max

α

W (α) =

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105) (10)

s.t. 0 ≤ αi ≤ C, i = 1, ..., m and
αiy(i) = 0, where
the ai’s are Lagrange multipliers and C controls the relative
weighing between the goals of making the ||w||2 small and of
ensuring that most examples have functional margins at least
1 in (cid:96)1 regularization. In our case, C = 1, the default set by
scikit-learn.

i=1

We then use the sequential minimal optimization (SMO)

m(cid:80)

algorithm to solve the dual problem:
Repeat until convergence {

1) Select some pair αi and αj to update.
2) Reoptimize W (α) with respect to αi and αj, while

holding the other αk’s ﬁxed.

}

The decision function used by scikit-learn is:

sgn(

yiαi(cid:104)xi, x(cid:105) + ρ)

(11)

m(cid:88)

where ρ is the intercept term.

i=1

V. RESULTS AND DISCUSSION

3

A. Original Runs: Unbalanced Data Set

We partitioned 70% of our tweets into a training set, and
30% into a testing set. We then ran each of the three models
on the two sets. To see that our models have learned from
the features, we also ran them under the condition that each
tweet had only one feature, randomly assigned to be 0 or 1.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Accuracy Random
0.7843
0.8250
0.7843
0.8247
0.7986
0.7843

Table 1: Model ccuracy when splitting 70% of the samples

into the training set and 30% into the test set

Here multinomial naive Bayes and logistic regression were
more effective than SVM. However, because the dataset was
heavily unbalanced with far more non-sarcastic samples than
sarcastic samples, the random classiﬁcation rate was close to
the model classiﬁcation rates. To more closely evaluate the
differences in the three models, we found the confusion matrix
to report the false positive and false negative rates.

Figure 1: False positive and negative rates for multinomial

naive Bayes, logistic regression, and SVM

For all three models, the false positive rate (non-sarcastic
samples classiﬁed as sarcastic) was lower than the false
negative rate (sarcastic samples labelled as non-sarcastic). We
believe this may be due to the fact
the dataset was
unbalanced, and took measures to address this issue later in
the report (see Oversampling, Undersampling).

that

Furthermore, we found the precision recall-curves for each
of the three models. Multinomial naive Bayes performed the
best in terms of the precision/recall tradeoff, while SVM per-
formed the worst. This result matches the accuracies presented
in Table 1.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Average Precision

0.6236
0.5862
0.5230

Table 2: Average precision accuracy (AUC for PR curve) for

each model

4

The false negative rate was signiﬁcantly lower, which
indicates that the training phase may be more effective with
a balanced data set because the classiﬁer was no longer
automatically classifying the samples as not sarcastic. The
false positive rate was slightly higher, because more samples
were now being classiﬁed as sarcastic.

Finally, we found the precision-recall curves under the new
data set. Both the precision and the recall improved when
training on the oversampled set. Among the three curves,
multinomial naive Bayes continued to have the highest average
precision while SVM performed relatively poorly.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Average Precision

0.8855
0.8797
0.8549

Table 5: Average precision accuracy (AUC for PR curve) for

each model with an oversampled data set

Figure 4: Precision-recall curves for oversampled data set

C. Undersampling

Undersampling involved taking only 1

4th of the non-sarcastic
tweets for the testing and training sets each time so that both
sarcastic and non-sarcastic sets had around 25,000 tweets. We
performed undersampling 4 times (each with a different quarter
of the non-sarcastic set) and averaged the accuracies in Table
6.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Accuracy Random
0.5225
0.7308
0.7235
0.5225
0.5225
0.6955

Figure 2: Precision and recall curves

We further performed 4-fold cross validation using scikit-
learn’s cross validation module on each of the three models.
Here logistic regression performed better than multinomial
naive Bayes, but the accuracies were relatively close.

Model Name

Multinomial Naive Bayes

Logistic Regression

Linear SVM

Accuracy
0.8341
0.8475
0.8284

Table 3: Accuracy with 4-fold cross validation

B. Oversampling

In order to achieve a more balanced data set, we imple-
mented oversampling and undersampling. In oversampling, the
samples belonging to the more scarce class are overrepresented
in the ﬁnal data set. Thus, we had each sarcastic tweet occur
4 times in the data set, so that both sarcastic and non-
sarcastic sets had about 100,000 samples. Thus, the random
classiﬁcation rate was closer to 50%. We again split the ﬁnal
data set into 70% train and 30% test. Under the oversampled
set, logistic regression had the highest accuracy.

Model Name

Multinomial Naive Bayes

Accuracy Random
0.5246
0.7807
0.5246
0.8027
0.7997
0.5246
Table 4: Accuracy with an oversampled data set

Logistic Regression

Linear SVM

We again found the confusion matrix to analyze false

positive and false negative rates.

Figure 3: False positive/negative rates for oversampled data

Table 6: Accuracy with an undersampled data set

Similar

to the unbalanced data set, multinomial naive
Bayes and logistic regression were most effective while SVM
performed more poorly. Overall, comparing the unbalanced,
oversampled, and undersampled sets,
the oversampled set
had the greatest improvement in classiﬁcation accuracy over
the random classiﬁer (Figure 5). The oversampled set may
have performed better than the undersampled set because it

5

based on emotion-related hashtags, we have no non-sarcastic
tweets that express neutral emotion. In the future, we can
try the Liebrecht et. al’s proposed method of obtaining non-
sarcastic tweets, which involves simply taking tweets that do
not have the tag “#sarcastic” and accepting the resulting noise.

In order to provide a reasonable classiﬁer, we created a data
set with comparable numbers of sarcastic and non-sarcastic
tweets. However, sarcasm is relatively rare, so these sets do
not represent the proportion of sarcastic tweets in real life.
In the future, we can explore other classiﬁers that handle
unbalanced sets more appropriately.

Finally, sarcasm is often based on current events. We polled
sarcastic and non-sarcastic tweets over a period of four weeks.
Ideally, however, tweets should be pulled over a longer period
of time to provide a larger and more unbiased corpus of tweets.

REFERENCES

[1] Bamman, David and Noah A. Smith. “Contextualized Sarcasm Detection
on Twitter.” Association for the Advancement of Artiﬁcial Intelligence
(2015): 574-578.

[2] Cliche, Mathieu, Ph.D. The Sarcasm Detector. N.p., n.d. Web.

[3] De Smedt, Tom. and Walter Daelemans. “Pattern for Python.” Journal

of Machine Learning Research (2012): 20312035.

[4] Gonzalez-Ibanez, Roberto, Smaranda Muresan and Nina Wacholder.
“Identifying Sarcasm in Twitter: A Closer Look.” Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics
(2011): 581-586.

[5] Liebrecht, Christine, Florian Kunneman, and Antal Van Den Bosch. “The
Perfect Solution for Detecting Sarcasm in Tweets #not.” Proceedings
of
the 4th Workshop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis (2013): 29-37.

[6] Loria, Steven, Pete Keen, Matthew Honnibal, Roman Yankovsky, David
Karesh, Evan Dempsey, Wesley Childs, Jeff Schnurr, Adel Qalieh, Lage
Ragnarsson, and Jonathon Coe. TextBlob, version v0.11.0. Available at
https://textblob.readthedocs.org

[7] Pedregosa, Fabian, Gal Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,
Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Perrot, and douard
Duchesnay. “Scikit-learn: Machine Learning in Python.” Journal of
Machine Learning Research (2011): 2825-2830.

[8] Riloff, Ellen, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan
Gilbert, and Ruihong Huang. “Sarcasm as Contrast between Positive
Sentiment and Negative Situation.” Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Processing (2013): 704-714.

encompassed more of the non-sarcastic examples during the
training phase.

Figure 5: Difference between random and model accuracies
for unbalanced, oversampled, and undersampled data sets

D. Feature Analysis

To analyze the impact of each feature on the classiﬁer, we
found the accuracy of the classiﬁers depending on each feature
alone when trained and tested on the oversampled data set.
Table 7 contains the average single feature accuracies over the
three models

Feature Name

Unigram
Bigram

Capitalization
Sentiment Split

Subjectivity

Random

Accuracy
0.7033
0.7759
0.5247
0.5249
0.5242
0.5246

Table 7: Accuracy of classiﬁer with single feature averaged
over multinomial naive Bayes, logistic regression, and SVM
As shown, unigrams and bigrams were the most effective
in classifying a tweet as sarcastic or not sarcastic, while
capitalization and sentiment split achieved results that were
only slightly better than those obtained by random feature.

VI. CONCLUSION AND FUTURE WORK

Of the three models,

logistic regression was the most
effective at classifying sarcastic tweets, with an accuracy of
0.8027 on an oversampled set. SVM generally performed
more poorly;
this may be because there is not a large
enough margin between the two classes for a sufﬁcient
linear hyperplane. Multinomial naive Bayes performed well
overall, and even performed better than logistic regression in
the unbalanced and undersampled sets. However, it makes
the underlying assumption that each of
the features are
conditionally independent, but unigrams and bigrams are
dependent by deﬁnition. Thus, multinomial naive Bayes may
be overweighting the importance of certain unigrams and
bigrams. Of our features, bigrams were the most signiﬁcant in
performing a correct estimate, with a single feature accuracy
of 0.7759 on an oversampled set.

Currently,

there are some limitations to our data set.
Due to our methodology of selecting non-sarcastic tweets

