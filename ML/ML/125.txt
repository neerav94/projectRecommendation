CS229 Final Paper

Predicting Sales for Rossmann Drug

Stores

Brian Knott, Hanbin Liu, Andrew Simpson

Abstract

In this paper we examined four different methods for time series forecasting: Random Forests, Gradient
Boosting, Hidden Markov Models, and Recurrent Neural Networks. We found that using Gradient
Boosting yielded the best results with root-mean-square percent error (RMPSE) of 10.439% (785 of 3429).
III. Dataset and Features

Introduction

I.

The goal of Rossmann Kaggle competi-

tion is to forecast the daily sales of Ross-
mann stores located across Germany us-
ing store, promotion, and competitor data. It
is known that store sales are inﬂuenced by
many factors, including promotions, compe-
tition, school and state holidays, seasonality,
and locality. A reliable and robust prediction
model will enable store managers to create
effective staff schedules that increase produc-
tivity. The data are provided by Rossmann
through Kaggle.

II. Related Work

The models we use in this paper have been
widely studied. The framework for Gradient
Boosting is laid out in [3] and [4], Random
Forests are studied in [2], Recurrent Neural
Networks in [5], and Hidden Markov Models
in [6]. These papers describe how parameters
effect model prediction.

Additionally, 3429 groups have submitted
results to this Kaggle competition. The code
for many of these submission is available on
the competition’s Scripts page. The majority of
the submissions used Random Forest or Gradi-
ent Boosting approaches, likely because there
are reliable packages for their implementation.
Results of these approaches vary widely, how-
ever, due to different choices in parameters,
preprocessing, and data splitting.

Rossmann provided data with 15 features
for 1115 stores. Not all stores have data for
each feature on each day, so we ﬁlled in miss-
ing data with median data for non-ensemble
methods (ensemble methods did not require
data for every ﬁeld). We then extended the
data to 18 features since some features, like the
date, captured multiple variables (like month
of the year and day of the month). The result-
ing data had the following feature set:

Table 1: Feature Set

Feature ID Description
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Store ID
Day Of Week
Open / Closed
Promotion Active
School Holiday
Store Type
Assortment
Distance to Competing Store
Month Competitor Opened
Year Competitor Opened
Promo2 Active
Week Promo2 Started
Year Promo2 Started
Interval of Promo2
Month
Year
Day
Competition Age (GB only)

We then normalized the data to have zero mean

1

CS229 Final Paper

Predicting Sales for Rossmann Drug

Stores

Brian Knott, Hanbin Liu, Andrew Simpson

Abstract

In this paper we examined four different methods for time series forecasting: Random Forests, Gradient
Boosting, Hidden Markov Models, and Recurrent Neural Networks. We found that using Gradient
Boosting yielded the best results with root-mean-square percent error (RMPSE) of 10.439% (785 of 3429).
III. Dataset and Features

Introduction

I.

The goal of Rossmann Kaggle competi-

tion is to forecast the daily sales of Ross-
mann stores located across Germany us-
ing store, promotion, and competitor data. It
is known that store sales are inﬂuenced by
many factors, including promotions, compe-
tition, school and state holidays, seasonality,
and locality. A reliable and robust prediction
model will enable store managers to create
effective staff schedules that increase produc-
tivity. The data are provided by Rossmann
through Kaggle.

II. Related Work

The models we use in this paper have been
widely studied. The framework for Gradient
Boosting is laid out in [3] and [4], Random
Forests are studied in [2], Recurrent Neural
Networks in [5], and Hidden Markov Models
in [6]. These papers describe how parameters
effect model prediction.

Additionally, 3429 groups have submitted
results to this Kaggle competition. The code
for many of these submission is available on
the competition’s Scripts page. The majority of
the submissions used Random Forest or Gradi-
ent Boosting approaches, likely because there
are reliable packages for their implementation.
Results of these approaches vary widely, how-
ever, due to different choices in parameters,
preprocessing, and data splitting.

Rossmann provided data with 15 features
for 1115 stores. Not all stores have data for
each feature on each day, so we ﬁlled in miss-
ing data with median data for non-ensemble
methods (ensemble methods did not require
data for every ﬁeld). We then extended the
data to 18 features since some features, like the
date, captured multiple variables (like month
of the year and day of the month). The result-
ing data had the following feature set:

Table 1: Feature Set

Feature ID Description
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Store ID
Day Of Week
Open / Closed
Promotion Active
School Holiday
Store Type
Assortment
Distance to Competing Store
Month Competitor Opened
Year Competitor Opened
Promo2 Active
Week Promo2 Started
Year Promo2 Started
Interval of Promo2
Month
Year
Day
Competition Age (GB only)

We then normalized the data to have zero mean

1

CS229 Final Paper

before applying our learning algorithms.

IV. Methods

I. Random Forests
Random Forest Regression is an ensemble
learning algorithm that operates by aggregat-
ing many random decision trees to make pre-
dictions while avoiding overﬁtting. We started
by using the Random Forest algorithm for
black box prediction because its bagging tech-
niques are robust to data anomalies (like miss-
ing data) and because random forest packages
are widely available.

In particular, we used R’s RandomForest
package to carry out the training and predic-
tion. We then used parameter optimization
to improve on our prediction model. To see
our parameter optimization method (and the
parameters used) see the Results section.

II. Hidden Markov Models
Hidden Markov Modeling is a sequence / state
estimation algorithm that assumes that the
dataset derives from a Markov Process with
hidden state information. Hidden Markov
Models are appropriate for this dataset since
its data evolves over time, a phenomenon un-
accounted for in ensemble learning.

P(zt|zt−1, zt−2, ...z1) = P(zt|zt−1)

(1)

Hidden Markov models rely on two sets of
Markov assumptions. The ﬁrst is the limited
horizon assumption. This assumes that the
probability of being in a state depends only on
the most recent states. For a ﬁrst order Markov
model, the current state zt depends only on the
state zt−1 (Eq. 1).

The second assumption is that the state tran-
sition process is stationary. This means that
the conditional distribution of state transitions
does not change over time (Eq. 2).

P(zt|zt−1) = P(z2|z1); t ∈ 2...T

(2)

The transition between states can be esti-
mated by averaging the number of times a

2

state transitioned from another stated. We used
Laplace smoothing to replace zero probabili-
ties with a small probabilities (Eq. 3 where k =
number of states).

ˆp(si|sj) =

t=1 1{zt−1 = si ∧ zt = sj} + 1
∑T

t=1{zt−1 = si} + k
∑T

(3)

A hidden Markov model is made of two
sets of states one hidden and the other ob-
served. We want to ﬁnd the most likely se-
ries of hidden states given the observed states
and known transitions between the observed
states and the hidden states. This can be done
naively by keeping track of all the provabilities;
however, this is computationally expensive. A
dynamic programing approach, known as the
Viterbi algorithm, is used instead. This algo-
rithm instead keeps track only of the maximum
probabilities through a recursive deﬁnition.

III. Recurrent Neural Networks

Recurrent Neural Networks work similarly
to standard artiﬁcial neural networks with the
addition of feedback loops. The internal recur-
sive states allow this kind of neural network to
exhibit dynamic temporal behavior, making it
appropriate for processing time series data.

A neuron is made up of two parts a
weighted sum (weights, inputs, and a bias)
and an activation function. In order to learn
on non-linear data, the hidden layer neurons
usually use an activation function that is in the
sigmoid family (logistic, Gaussian, etc.). The
output of each neuron is the weighted sum
passed into the activation function. The out-
put of each neuron is passed to each neuron in
the next layer. When predicting a continuous
value, a single linear output neuron is usually
used. Learning is usually done on neural net-
works through the back propagation algorithm.
This algorithm splits the error (squared error)
between each weight it the network. In order to
do this, we use the chain rule. On the hidden
layers the sum of the derivative is part of the
chain that is multiplied to ﬁnd the error. An
example of how error ﬂows back to the hidden

CS229 Final Paper

Predicting Sales for Rossmann Drug

Stores

Brian Knott, Hanbin Liu, Andrew Simpson

Abstract

In this paper we examined four different methods for time series forecasting: Random Forests, Gradient
Boosting, Hidden Markov Models, and Recurrent Neural Networks. We found that using Gradient
Boosting yielded the best results with root-mean-square percent error (RMPSE) of 10.439% (785 of 3429).
III. Dataset and Features

Introduction

I.

The goal of Rossmann Kaggle competi-

tion is to forecast the daily sales of Ross-
mann stores located across Germany us-
ing store, promotion, and competitor data. It
is known that store sales are inﬂuenced by
many factors, including promotions, compe-
tition, school and state holidays, seasonality,
and locality. A reliable and robust prediction
model will enable store managers to create
effective staff schedules that increase produc-
tivity. The data are provided by Rossmann
through Kaggle.

II. Related Work

The models we use in this paper have been
widely studied. The framework for Gradient
Boosting is laid out in [3] and [4], Random
Forests are studied in [2], Recurrent Neural
Networks in [5], and Hidden Markov Models
in [6]. These papers describe how parameters
effect model prediction.

Additionally, 3429 groups have submitted
results to this Kaggle competition. The code
for many of these submission is available on
the competition’s Scripts page. The majority of
the submissions used Random Forest or Gradi-
ent Boosting approaches, likely because there
are reliable packages for their implementation.
Results of these approaches vary widely, how-
ever, due to different choices in parameters,
preprocessing, and data splitting.

Rossmann provided data with 15 features
for 1115 stores. Not all stores have data for
each feature on each day, so we ﬁlled in miss-
ing data with median data for non-ensemble
methods (ensemble methods did not require
data for every ﬁeld). We then extended the
data to 18 features since some features, like the
date, captured multiple variables (like month
of the year and day of the month). The result-
ing data had the following feature set:

Table 1: Feature Set

Feature ID Description
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Store ID
Day Of Week
Open / Closed
Promotion Active
School Holiday
Store Type
Assortment
Distance to Competing Store
Month Competitor Opened
Year Competitor Opened
Promo2 Active
Week Promo2 Started
Year Promo2 Started
Interval of Promo2
Month
Year
Day
Competition Age (GB only)

We then normalized the data to have zero mean

1

CS229 Final Paper

before applying our learning algorithms.

IV. Methods

I. Random Forests
Random Forest Regression is an ensemble
learning algorithm that operates by aggregat-
ing many random decision trees to make pre-
dictions while avoiding overﬁtting. We started
by using the Random Forest algorithm for
black box prediction because its bagging tech-
niques are robust to data anomalies (like miss-
ing data) and because random forest packages
are widely available.

In particular, we used R’s RandomForest
package to carry out the training and predic-
tion. We then used parameter optimization
to improve on our prediction model. To see
our parameter optimization method (and the
parameters used) see the Results section.

II. Hidden Markov Models
Hidden Markov Modeling is a sequence / state
estimation algorithm that assumes that the
dataset derives from a Markov Process with
hidden state information. Hidden Markov
Models are appropriate for this dataset since
its data evolves over time, a phenomenon un-
accounted for in ensemble learning.

P(zt|zt−1, zt−2, ...z1) = P(zt|zt−1)

(1)

Hidden Markov models rely on two sets of
Markov assumptions. The ﬁrst is the limited
horizon assumption. This assumes that the
probability of being in a state depends only on
the most recent states. For a ﬁrst order Markov
model, the current state zt depends only on the
state zt−1 (Eq. 1).

The second assumption is that the state tran-
sition process is stationary. This means that
the conditional distribution of state transitions
does not change over time (Eq. 2).

P(zt|zt−1) = P(z2|z1); t ∈ 2...T

(2)

The transition between states can be esti-
mated by averaging the number of times a

2

state transitioned from another stated. We used
Laplace smoothing to replace zero probabili-
ties with a small probabilities (Eq. 3 where k =
number of states).

ˆp(si|sj) =

t=1 1{zt−1 = si ∧ zt = sj} + 1
∑T

t=1{zt−1 = si} + k
∑T

(3)

A hidden Markov model is made of two
sets of states one hidden and the other ob-
served. We want to ﬁnd the most likely se-
ries of hidden states given the observed states
and known transitions between the observed
states and the hidden states. This can be done
naively by keeping track of all the provabilities;
however, this is computationally expensive. A
dynamic programing approach, known as the
Viterbi algorithm, is used instead. This algo-
rithm instead keeps track only of the maximum
probabilities through a recursive deﬁnition.

III. Recurrent Neural Networks

Recurrent Neural Networks work similarly
to standard artiﬁcial neural networks with the
addition of feedback loops. The internal recur-
sive states allow this kind of neural network to
exhibit dynamic temporal behavior, making it
appropriate for processing time series data.

A neuron is made up of two parts a
weighted sum (weights, inputs, and a bias)
and an activation function. In order to learn
on non-linear data, the hidden layer neurons
usually use an activation function that is in the
sigmoid family (logistic, Gaussian, etc.). The
output of each neuron is the weighted sum
passed into the activation function. The out-
put of each neuron is passed to each neuron in
the next layer. When predicting a continuous
value, a single linear output neuron is usually
used. Learning is usually done on neural net-
works through the back propagation algorithm.
This algorithm splits the error (squared error)
between each weight it the network. In order to
do this, we use the chain rule. On the hidden
layers the sum of the derivative is part of the
chain that is multiplied to ﬁnd the error. An
example of how error ﬂows back to the hidden

CS229 Final Paper

(cid:32)

∂Errortotal
∂whidden

=

∑

HigherLayer

∂Errortotal
∂outputhigher

∗ ∂outputhigher
∂nethigher

∗

∂nethigher

∂outputhidden

(cid:33)

∗ ∂outputhidden
∂nethidden

∗ ∂nethidden
∂whidden
(4)

layer is given in Eq. 4 (net is net sum of weights
times inputs and bias).

In recurrent neural networks the output of
the hidden layer neurons feeds back as another
input. A basic layout of a recurrent neural net-
work is given in ﬁgure 1. Since the input from
the previous time is passed as input to the cur-
rent time, recurrent neural networks have a
form of memory. With a recursive deﬁnition,
this memory covers every input set.

Figure 1: An example recurrent neural network. Out-
puts from the hidden layer neurons from the
previous time step are fed back in as input to
the next time step.

IV. Gradient Boosting Regression

Gradient Boosting is an ensemble learning
algorithm that uses a weighted average of sim-
ple models to learn a more complex model.
The algorithm ﬁrst uses a simple model to ﬁt
the data, then a simple model to ﬁt the residu-
als between the data and the ﬁrst model. This
process continues on each model’s residuals
until an appropriate ﬁt is found.
As the model increases in complexity, overﬁt-
ting tends to start occurring. To avoid over-
ﬁtting, the Gradient Boosting algorithm keeps
a portion of the data (different in each itera-
tion) out of the training set and uses it for cross

validation. The algorithm keeps track of the
cross validation error and determines that an
appropriate ﬁt has been found once the cross
validation error increases for a given number
of iterations.

We used the R package XGBoost to train
our Gradient Boosting models, then used pa-
rameter optimization to ﬁnd the best solution.

V. Results and Discussion

I. Random Forest

For our Random Forest approach, we used
the R package RandomForest for training mod-
els and making predictions. This package al-
lows for the variation of three major parame-
ters. Each parameter is mainly used to balance
the tradeoff between runtime and ﬁt quality.
Because random forest regression is a very re-
source hungry algorithm, parameters must be
set with feasibility in mind. The parameters
given to the model are as follows:

• mtry - the number of features allowed
in each decision tree. For a given value
k, trees may not use more than k regres-
sor variables from the 18 features given
to the model. Higher values for mtry
allows each tree to model more informa-
tion from the features, but requires more
trees to generate an informative average.
• ntrees - the number of trees averaged in
the model. A higher value means run-
time will be longer, but with a better ﬁt.
• sample size - because the training algo-
rithm is so resource hungry and Ross-
mann provided so much data, it is not
feasible to run Random Forest Regression

3

CS229 Final Paper

Predicting Sales for Rossmann Drug

Stores

Brian Knott, Hanbin Liu, Andrew Simpson

Abstract

In this paper we examined four different methods for time series forecasting: Random Forests, Gradient
Boosting, Hidden Markov Models, and Recurrent Neural Networks. We found that using Gradient
Boosting yielded the best results with root-mean-square percent error (RMPSE) of 10.439% (785 of 3429).
III. Dataset and Features

Introduction

I.

The goal of Rossmann Kaggle competi-

tion is to forecast the daily sales of Ross-
mann stores located across Germany us-
ing store, promotion, and competitor data. It
is known that store sales are inﬂuenced by
many factors, including promotions, compe-
tition, school and state holidays, seasonality,
and locality. A reliable and robust prediction
model will enable store managers to create
effective staff schedules that increase produc-
tivity. The data are provided by Rossmann
through Kaggle.

II. Related Work

The models we use in this paper have been
widely studied. The framework for Gradient
Boosting is laid out in [3] and [4], Random
Forests are studied in [2], Recurrent Neural
Networks in [5], and Hidden Markov Models
in [6]. These papers describe how parameters
effect model prediction.

Additionally, 3429 groups have submitted
results to this Kaggle competition. The code
for many of these submission is available on
the competition’s Scripts page. The majority of
the submissions used Random Forest or Gradi-
ent Boosting approaches, likely because there
are reliable packages for their implementation.
Results of these approaches vary widely, how-
ever, due to different choices in parameters,
preprocessing, and data splitting.

Rossmann provided data with 15 features
for 1115 stores. Not all stores have data for
each feature on each day, so we ﬁlled in miss-
ing data with median data for non-ensemble
methods (ensemble methods did not require
data for every ﬁeld). We then extended the
data to 18 features since some features, like the
date, captured multiple variables (like month
of the year and day of the month). The result-
ing data had the following feature set:

Table 1: Feature Set

Feature ID Description
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Store ID
Day Of Week
Open / Closed
Promotion Active
School Holiday
Store Type
Assortment
Distance to Competing Store
Month Competitor Opened
Year Competitor Opened
Promo2 Active
Week Promo2 Started
Year Promo2 Started
Interval of Promo2
Month
Year
Day
Competition Age (GB only)

We then normalized the data to have zero mean

1

CS229 Final Paper

before applying our learning algorithms.

IV. Methods

I. Random Forests
Random Forest Regression is an ensemble
learning algorithm that operates by aggregat-
ing many random decision trees to make pre-
dictions while avoiding overﬁtting. We started
by using the Random Forest algorithm for
black box prediction because its bagging tech-
niques are robust to data anomalies (like miss-
ing data) and because random forest packages
are widely available.

In particular, we used R’s RandomForest
package to carry out the training and predic-
tion. We then used parameter optimization
to improve on our prediction model. To see
our parameter optimization method (and the
parameters used) see the Results section.

II. Hidden Markov Models
Hidden Markov Modeling is a sequence / state
estimation algorithm that assumes that the
dataset derives from a Markov Process with
hidden state information. Hidden Markov
Models are appropriate for this dataset since
its data evolves over time, a phenomenon un-
accounted for in ensemble learning.

P(zt|zt−1, zt−2, ...z1) = P(zt|zt−1)

(1)

Hidden Markov models rely on two sets of
Markov assumptions. The ﬁrst is the limited
horizon assumption. This assumes that the
probability of being in a state depends only on
the most recent states. For a ﬁrst order Markov
model, the current state zt depends only on the
state zt−1 (Eq. 1).

The second assumption is that the state tran-
sition process is stationary. This means that
the conditional distribution of state transitions
does not change over time (Eq. 2).

P(zt|zt−1) = P(z2|z1); t ∈ 2...T

(2)

The transition between states can be esti-
mated by averaging the number of times a

2

state transitioned from another stated. We used
Laplace smoothing to replace zero probabili-
ties with a small probabilities (Eq. 3 where k =
number of states).

ˆp(si|sj) =

t=1 1{zt−1 = si ∧ zt = sj} + 1
∑T

t=1{zt−1 = si} + k
∑T

(3)

A hidden Markov model is made of two
sets of states one hidden and the other ob-
served. We want to ﬁnd the most likely se-
ries of hidden states given the observed states
and known transitions between the observed
states and the hidden states. This can be done
naively by keeping track of all the provabilities;
however, this is computationally expensive. A
dynamic programing approach, known as the
Viterbi algorithm, is used instead. This algo-
rithm instead keeps track only of the maximum
probabilities through a recursive deﬁnition.

III. Recurrent Neural Networks

Recurrent Neural Networks work similarly
to standard artiﬁcial neural networks with the
addition of feedback loops. The internal recur-
sive states allow this kind of neural network to
exhibit dynamic temporal behavior, making it
appropriate for processing time series data.

A neuron is made up of two parts a
weighted sum (weights, inputs, and a bias)
and an activation function. In order to learn
on non-linear data, the hidden layer neurons
usually use an activation function that is in the
sigmoid family (logistic, Gaussian, etc.). The
output of each neuron is the weighted sum
passed into the activation function. The out-
put of each neuron is passed to each neuron in
the next layer. When predicting a continuous
value, a single linear output neuron is usually
used. Learning is usually done on neural net-
works through the back propagation algorithm.
This algorithm splits the error (squared error)
between each weight it the network. In order to
do this, we use the chain rule. On the hidden
layers the sum of the derivative is part of the
chain that is multiplied to ﬁnd the error. An
example of how error ﬂows back to the hidden

CS229 Final Paper

(cid:32)

∂Errortotal
∂whidden

=

∑

HigherLayer

∂Errortotal
∂outputhigher

∗ ∂outputhigher
∂nethigher

∗

∂nethigher

∂outputhidden

(cid:33)

∗ ∂outputhidden
∂nethidden

∗ ∂nethidden
∂whidden
(4)

layer is given in Eq. 4 (net is net sum of weights
times inputs and bias).

In recurrent neural networks the output of
the hidden layer neurons feeds back as another
input. A basic layout of a recurrent neural net-
work is given in ﬁgure 1. Since the input from
the previous time is passed as input to the cur-
rent time, recurrent neural networks have a
form of memory. With a recursive deﬁnition,
this memory covers every input set.

Figure 1: An example recurrent neural network. Out-
puts from the hidden layer neurons from the
previous time step are fed back in as input to
the next time step.

IV. Gradient Boosting Regression

Gradient Boosting is an ensemble learning
algorithm that uses a weighted average of sim-
ple models to learn a more complex model.
The algorithm ﬁrst uses a simple model to ﬁt
the data, then a simple model to ﬁt the residu-
als between the data and the ﬁrst model. This
process continues on each model’s residuals
until an appropriate ﬁt is found.
As the model increases in complexity, overﬁt-
ting tends to start occurring. To avoid over-
ﬁtting, the Gradient Boosting algorithm keeps
a portion of the data (different in each itera-
tion) out of the training set and uses it for cross

validation. The algorithm keeps track of the
cross validation error and determines that an
appropriate ﬁt has been found once the cross
validation error increases for a given number
of iterations.

We used the R package XGBoost to train
our Gradient Boosting models, then used pa-
rameter optimization to ﬁnd the best solution.

V. Results and Discussion

I. Random Forest

For our Random Forest approach, we used
the R package RandomForest for training mod-
els and making predictions. This package al-
lows for the variation of three major parame-
ters. Each parameter is mainly used to balance
the tradeoff between runtime and ﬁt quality.
Because random forest regression is a very re-
source hungry algorithm, parameters must be
set with feasibility in mind. The parameters
given to the model are as follows:

• mtry - the number of features allowed
in each decision tree. For a given value
k, trees may not use more than k regres-
sor variables from the 18 features given
to the model. Higher values for mtry
allows each tree to model more informa-
tion from the features, but requires more
trees to generate an informative average.
• ntrees - the number of trees averaged in
the model. A higher value means run-
time will be longer, but with a better ﬁt.
• sample size - because the training algo-
rithm is so resource hungry and Ross-
mann provided so much data, it is not
feasible to run Random Forest Regression

3

CS229 Final Paper

discretized to work with the hidden Markov
model. A smaller discretization leads to the
potential ability to be close to the true value;
however, it also decreases the probability esti-
mation between states as there are more states.
Figure 2 show the results of different levels of
sales discretization. This experiment showed
that dividing sales by 1500 and rounding pro-
duced the lowest training error.

on the whole dataset. Therefore this pa-
rameter sets the number of random data
samples that the model is trained on.

Table 2 shows the parameters for our best
two Random Forest models as well as the pre-
diction RMSPE as scored by Kaggle (Kaggle
scores on data for which we only have access
to the feature data).

Table 2: Parameter Selection for Random Forest Regres-

sion

mtry
7
9

ntrees
100
300

samplesize RMSPE
12.09%
11.98%

100000
100000

II. Hidden Markov Model

Figure 3: HMM error using Laplace Smoothing and

month data

In order to account for seasonality effects,
the month was also included as an experiment.
However, when tested the training error was
higher than not including this data (see ﬁgure
3). Increasing the number of observed states
by a factor of 12 (2688 states) likely decreased
the estimation of state transitioning.

Figure 2: HMM error using Laplace Smoothing

III. Recurrent Neural Network

For testing with HMMs, the stores were
treated separately. The decision to use the
stores separately came from An experiment
normalizing the data to zero mean and com-
bining the stores to estimate transitions. Com-
bining all this produced a much higher error
(over 70% RMSPE) and was computationally
expensive.

The observed states, what we know in ad-
vance, is everything except sales and customer
data. There are 224 state for different obser-
vations (7 days in a week, Store Open/Closed,
Promotion Active, 4 state holidays, and a
school holiday). The sales data needed to be

Due to the temporal nature of the data
hold out cross validation was used. 20% of
the data was left out for validation. In order
to work with the Matlab’s functionality, the
data was normalized. Each store was treated
separately for reasons similar to the Hidden
Markov method. During testing it was found
that the predicted values for closed days did
not always go to zero, so we set values for
closed days to zero. An experiment was to test
the effect the different numbers of neurons had
in the hidden layer. The experiment show that
7 neurons produced the lowest training error
(see ﬁgure 4).

4

CS229 Final Paper

Predicting Sales for Rossmann Drug

Stores

Brian Knott, Hanbin Liu, Andrew Simpson

Abstract

In this paper we examined four different methods for time series forecasting: Random Forests, Gradient
Boosting, Hidden Markov Models, and Recurrent Neural Networks. We found that using Gradient
Boosting yielded the best results with root-mean-square percent error (RMPSE) of 10.439% (785 of 3429).
III. Dataset and Features

Introduction

I.

The goal of Rossmann Kaggle competi-

tion is to forecast the daily sales of Ross-
mann stores located across Germany us-
ing store, promotion, and competitor data. It
is known that store sales are inﬂuenced by
many factors, including promotions, compe-
tition, school and state holidays, seasonality,
and locality. A reliable and robust prediction
model will enable store managers to create
effective staff schedules that increase produc-
tivity. The data are provided by Rossmann
through Kaggle.

II. Related Work

The models we use in this paper have been
widely studied. The framework for Gradient
Boosting is laid out in [3] and [4], Random
Forests are studied in [2], Recurrent Neural
Networks in [5], and Hidden Markov Models
in [6]. These papers describe how parameters
effect model prediction.

Additionally, 3429 groups have submitted
results to this Kaggle competition. The code
for many of these submission is available on
the competition’s Scripts page. The majority of
the submissions used Random Forest or Gradi-
ent Boosting approaches, likely because there
are reliable packages for their implementation.
Results of these approaches vary widely, how-
ever, due to different choices in parameters,
preprocessing, and data splitting.

Rossmann provided data with 15 features
for 1115 stores. Not all stores have data for
each feature on each day, so we ﬁlled in miss-
ing data with median data for non-ensemble
methods (ensemble methods did not require
data for every ﬁeld). We then extended the
data to 18 features since some features, like the
date, captured multiple variables (like month
of the year and day of the month). The result-
ing data had the following feature set:

Table 1: Feature Set

Feature ID Description
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Store ID
Day Of Week
Open / Closed
Promotion Active
School Holiday
Store Type
Assortment
Distance to Competing Store
Month Competitor Opened
Year Competitor Opened
Promo2 Active
Week Promo2 Started
Year Promo2 Started
Interval of Promo2
Month
Year
Day
Competition Age (GB only)

We then normalized the data to have zero mean

1

CS229 Final Paper

before applying our learning algorithms.

IV. Methods

I. Random Forests
Random Forest Regression is an ensemble
learning algorithm that operates by aggregat-
ing many random decision trees to make pre-
dictions while avoiding overﬁtting. We started
by using the Random Forest algorithm for
black box prediction because its bagging tech-
niques are robust to data anomalies (like miss-
ing data) and because random forest packages
are widely available.

In particular, we used R’s RandomForest
package to carry out the training and predic-
tion. We then used parameter optimization
to improve on our prediction model. To see
our parameter optimization method (and the
parameters used) see the Results section.

II. Hidden Markov Models
Hidden Markov Modeling is a sequence / state
estimation algorithm that assumes that the
dataset derives from a Markov Process with
hidden state information. Hidden Markov
Models are appropriate for this dataset since
its data evolves over time, a phenomenon un-
accounted for in ensemble learning.

P(zt|zt−1, zt−2, ...z1) = P(zt|zt−1)

(1)

Hidden Markov models rely on two sets of
Markov assumptions. The ﬁrst is the limited
horizon assumption. This assumes that the
probability of being in a state depends only on
the most recent states. For a ﬁrst order Markov
model, the current state zt depends only on the
state zt−1 (Eq. 1).

The second assumption is that the state tran-
sition process is stationary. This means that
the conditional distribution of state transitions
does not change over time (Eq. 2).

P(zt|zt−1) = P(z2|z1); t ∈ 2...T

(2)

The transition between states can be esti-
mated by averaging the number of times a

2

state transitioned from another stated. We used
Laplace smoothing to replace zero probabili-
ties with a small probabilities (Eq. 3 where k =
number of states).

ˆp(si|sj) =

t=1 1{zt−1 = si ∧ zt = sj} + 1
∑T

t=1{zt−1 = si} + k
∑T

(3)

A hidden Markov model is made of two
sets of states one hidden and the other ob-
served. We want to ﬁnd the most likely se-
ries of hidden states given the observed states
and known transitions between the observed
states and the hidden states. This can be done
naively by keeping track of all the provabilities;
however, this is computationally expensive. A
dynamic programing approach, known as the
Viterbi algorithm, is used instead. This algo-
rithm instead keeps track only of the maximum
probabilities through a recursive deﬁnition.

III. Recurrent Neural Networks

Recurrent Neural Networks work similarly
to standard artiﬁcial neural networks with the
addition of feedback loops. The internal recur-
sive states allow this kind of neural network to
exhibit dynamic temporal behavior, making it
appropriate for processing time series data.

A neuron is made up of two parts a
weighted sum (weights, inputs, and a bias)
and an activation function. In order to learn
on non-linear data, the hidden layer neurons
usually use an activation function that is in the
sigmoid family (logistic, Gaussian, etc.). The
output of each neuron is the weighted sum
passed into the activation function. The out-
put of each neuron is passed to each neuron in
the next layer. When predicting a continuous
value, a single linear output neuron is usually
used. Learning is usually done on neural net-
works through the back propagation algorithm.
This algorithm splits the error (squared error)
between each weight it the network. In order to
do this, we use the chain rule. On the hidden
layers the sum of the derivative is part of the
chain that is multiplied to ﬁnd the error. An
example of how error ﬂows back to the hidden

CS229 Final Paper

(cid:32)

∂Errortotal
∂whidden

=

∑

HigherLayer

∂Errortotal
∂outputhigher

∗ ∂outputhigher
∂nethigher

∗

∂nethigher

∂outputhidden

(cid:33)

∗ ∂outputhidden
∂nethidden

∗ ∂nethidden
∂whidden
(4)

layer is given in Eq. 4 (net is net sum of weights
times inputs and bias).

In recurrent neural networks the output of
the hidden layer neurons feeds back as another
input. A basic layout of a recurrent neural net-
work is given in ﬁgure 1. Since the input from
the previous time is passed as input to the cur-
rent time, recurrent neural networks have a
form of memory. With a recursive deﬁnition,
this memory covers every input set.

Figure 1: An example recurrent neural network. Out-
puts from the hidden layer neurons from the
previous time step are fed back in as input to
the next time step.

IV. Gradient Boosting Regression

Gradient Boosting is an ensemble learning
algorithm that uses a weighted average of sim-
ple models to learn a more complex model.
The algorithm ﬁrst uses a simple model to ﬁt
the data, then a simple model to ﬁt the residu-
als between the data and the ﬁrst model. This
process continues on each model’s residuals
until an appropriate ﬁt is found.
As the model increases in complexity, overﬁt-
ting tends to start occurring. To avoid over-
ﬁtting, the Gradient Boosting algorithm keeps
a portion of the data (different in each itera-
tion) out of the training set and uses it for cross

validation. The algorithm keeps track of the
cross validation error and determines that an
appropriate ﬁt has been found once the cross
validation error increases for a given number
of iterations.

We used the R package XGBoost to train
our Gradient Boosting models, then used pa-
rameter optimization to ﬁnd the best solution.

V. Results and Discussion

I. Random Forest

For our Random Forest approach, we used
the R package RandomForest for training mod-
els and making predictions. This package al-
lows for the variation of three major parame-
ters. Each parameter is mainly used to balance
the tradeoff between runtime and ﬁt quality.
Because random forest regression is a very re-
source hungry algorithm, parameters must be
set with feasibility in mind. The parameters
given to the model are as follows:

• mtry - the number of features allowed
in each decision tree. For a given value
k, trees may not use more than k regres-
sor variables from the 18 features given
to the model. Higher values for mtry
allows each tree to model more informa-
tion from the features, but requires more
trees to generate an informative average.
• ntrees - the number of trees averaged in
the model. A higher value means run-
time will be longer, but with a better ﬁt.
• sample size - because the training algo-
rithm is so resource hungry and Ross-
mann provided so much data, it is not
feasible to run Random Forest Regression

3

CS229 Final Paper

discretized to work with the hidden Markov
model. A smaller discretization leads to the
potential ability to be close to the true value;
however, it also decreases the probability esti-
mation between states as there are more states.
Figure 2 show the results of different levels of
sales discretization. This experiment showed
that dividing sales by 1500 and rounding pro-
duced the lowest training error.

on the whole dataset. Therefore this pa-
rameter sets the number of random data
samples that the model is trained on.

Table 2 shows the parameters for our best
two Random Forest models as well as the pre-
diction RMSPE as scored by Kaggle (Kaggle
scores on data for which we only have access
to the feature data).

Table 2: Parameter Selection for Random Forest Regres-

sion

mtry
7
9

ntrees
100
300

samplesize RMSPE
12.09%
11.98%

100000
100000

II. Hidden Markov Model

Figure 3: HMM error using Laplace Smoothing and

month data

In order to account for seasonality effects,
the month was also included as an experiment.
However, when tested the training error was
higher than not including this data (see ﬁgure
3). Increasing the number of observed states
by a factor of 12 (2688 states) likely decreased
the estimation of state transitioning.

Figure 2: HMM error using Laplace Smoothing

III. Recurrent Neural Network

For testing with HMMs, the stores were
treated separately. The decision to use the
stores separately came from An experiment
normalizing the data to zero mean and com-
bining the stores to estimate transitions. Com-
bining all this produced a much higher error
(over 70% RMSPE) and was computationally
expensive.

The observed states, what we know in ad-
vance, is everything except sales and customer
data. There are 224 state for different obser-
vations (7 days in a week, Store Open/Closed,
Promotion Active, 4 state holidays, and a
school holiday). The sales data needed to be

Due to the temporal nature of the data
hold out cross validation was used. 20% of
the data was left out for validation. In order
to work with the Matlab’s functionality, the
data was normalized. Each store was treated
separately for reasons similar to the Hidden
Markov method. During testing it was found
that the predicted values for closed days did
not always go to zero, so we set values for
closed days to zero. An experiment was to test
the effect the different numbers of neurons had
in the hidden layer. The experiment show that
7 neurons produced the lowest training error
(see ﬁgure 4).

4

CS229 Final Paper

to 5000, however the model wight 5000 trees
performed worse than the previous model. We
believe this to be due to overﬁtting since the
cross-validation error does not improve from
3000 to 5000 trees while the training error goes
down (as seen in Figure 5). Therefore, we chose
to keep the number of trees set at 3000. We
then added feature 18 to our feature set (Com-
petition Age).

Figure 4: RNN Error averages of different number of

hidden neurons (closed days forces to 0).

The actual error from Kaggle had much
higher error of around 24%. This may be due
to over ﬁtting. The neural network may have
learned the pattern leading up to the test date,
but the Kaggle test data started at a different
time.

IV. Gradient Boosting

Similar to Random Forests, Gradient Boost-
ing is an ensemble based regression, efﬁcient
for black-box prediction. However, Gradient
Boosting is much faster to train than Random
Forests.
It has been used in several Kaggle
competition-winning solutions and has been
developed into a the R package XGBoost.

Table 3: Model Performance

ntrees Parallel Trees
default(1)
3000
5000
default(1)
3000
3000
3000
3000

default(1)

10
10

10

# Features RMSPE
10.68%
10.70%
10.52%
10.50%
10.58%
10.56%

17
17
17
17
18
18

As shown in Table 3, our ﬁrst run of Gradi-
ent Boosting generated a RMSPE of 10.681%, a
signiﬁcant improvement on our best Random
Forest model. To further improve our model,
we increased the number of trees from 3000

Figure 5: Learning Curve for 5000 tree model

Adding Competiton Age to our feature set
did not improve our model on its own, but
combining models (runs 3, 5, and 6) lead to a
model with an improved RMSPE of 10.439%,
our best result.

Table 4: Model Performance

Model
Gradient Boosting
Random Forest
Hidden Markov Model
Predict Median Sales
Recurrent Neural Nets

RMSPE
10.44%
11.98%
16.52%
19.25%
24.88%

VI. Conclusion

We found that though Hidden Markov
Models and Recurrent Neural Networks are
better in theory for time-series data, ensemble
methods performed much better in practice.
We believe this to be due to their robustness
in black box optimization considering the data
provided is unstructured and some features
may not correlate well with sales.

5

CS229 Final Paper

Predicting Sales for Rossmann Drug

Stores

Brian Knott, Hanbin Liu, Andrew Simpson

Abstract

In this paper we examined four different methods for time series forecasting: Random Forests, Gradient
Boosting, Hidden Markov Models, and Recurrent Neural Networks. We found that using Gradient
Boosting yielded the best results with root-mean-square percent error (RMPSE) of 10.439% (785 of 3429).
III. Dataset and Features

Introduction

I.

The goal of Rossmann Kaggle competi-

tion is to forecast the daily sales of Ross-
mann stores located across Germany us-
ing store, promotion, and competitor data. It
is known that store sales are inﬂuenced by
many factors, including promotions, compe-
tition, school and state holidays, seasonality,
and locality. A reliable and robust prediction
model will enable store managers to create
effective staff schedules that increase produc-
tivity. The data are provided by Rossmann
through Kaggle.

II. Related Work

The models we use in this paper have been
widely studied. The framework for Gradient
Boosting is laid out in [3] and [4], Random
Forests are studied in [2], Recurrent Neural
Networks in [5], and Hidden Markov Models
in [6]. These papers describe how parameters
effect model prediction.

Additionally, 3429 groups have submitted
results to this Kaggle competition. The code
for many of these submission is available on
the competition’s Scripts page. The majority of
the submissions used Random Forest or Gradi-
ent Boosting approaches, likely because there
are reliable packages for their implementation.
Results of these approaches vary widely, how-
ever, due to different choices in parameters,
preprocessing, and data splitting.

Rossmann provided data with 15 features
for 1115 stores. Not all stores have data for
each feature on each day, so we ﬁlled in miss-
ing data with median data for non-ensemble
methods (ensemble methods did not require
data for every ﬁeld). We then extended the
data to 18 features since some features, like the
date, captured multiple variables (like month
of the year and day of the month). The result-
ing data had the following feature set:

Table 1: Feature Set

Feature ID Description
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

Store ID
Day Of Week
Open / Closed
Promotion Active
School Holiday
Store Type
Assortment
Distance to Competing Store
Month Competitor Opened
Year Competitor Opened
Promo2 Active
Week Promo2 Started
Year Promo2 Started
Interval of Promo2
Month
Year
Day
Competition Age (GB only)

We then normalized the data to have zero mean

1

CS229 Final Paper

before applying our learning algorithms.

IV. Methods

I. Random Forests
Random Forest Regression is an ensemble
learning algorithm that operates by aggregat-
ing many random decision trees to make pre-
dictions while avoiding overﬁtting. We started
by using the Random Forest algorithm for
black box prediction because its bagging tech-
niques are robust to data anomalies (like miss-
ing data) and because random forest packages
are widely available.

In particular, we used R’s RandomForest
package to carry out the training and predic-
tion. We then used parameter optimization
to improve on our prediction model. To see
our parameter optimization method (and the
parameters used) see the Results section.

II. Hidden Markov Models
Hidden Markov Modeling is a sequence / state
estimation algorithm that assumes that the
dataset derives from a Markov Process with
hidden state information. Hidden Markov
Models are appropriate for this dataset since
its data evolves over time, a phenomenon un-
accounted for in ensemble learning.

P(zt|zt−1, zt−2, ...z1) = P(zt|zt−1)

(1)

Hidden Markov models rely on two sets of
Markov assumptions. The ﬁrst is the limited
horizon assumption. This assumes that the
probability of being in a state depends only on
the most recent states. For a ﬁrst order Markov
model, the current state zt depends only on the
state zt−1 (Eq. 1).

The second assumption is that the state tran-
sition process is stationary. This means that
the conditional distribution of state transitions
does not change over time (Eq. 2).

P(zt|zt−1) = P(z2|z1); t ∈ 2...T

(2)

The transition between states can be esti-
mated by averaging the number of times a

2

state transitioned from another stated. We used
Laplace smoothing to replace zero probabili-
ties with a small probabilities (Eq. 3 where k =
number of states).

ˆp(si|sj) =

t=1 1{zt−1 = si ∧ zt = sj} + 1
∑T

t=1{zt−1 = si} + k
∑T

(3)

A hidden Markov model is made of two
sets of states one hidden and the other ob-
served. We want to ﬁnd the most likely se-
ries of hidden states given the observed states
and known transitions between the observed
states and the hidden states. This can be done
naively by keeping track of all the provabilities;
however, this is computationally expensive. A
dynamic programing approach, known as the
Viterbi algorithm, is used instead. This algo-
rithm instead keeps track only of the maximum
probabilities through a recursive deﬁnition.

III. Recurrent Neural Networks

Recurrent Neural Networks work similarly
to standard artiﬁcial neural networks with the
addition of feedback loops. The internal recur-
sive states allow this kind of neural network to
exhibit dynamic temporal behavior, making it
appropriate for processing time series data.

A neuron is made up of two parts a
weighted sum (weights, inputs, and a bias)
and an activation function. In order to learn
on non-linear data, the hidden layer neurons
usually use an activation function that is in the
sigmoid family (logistic, Gaussian, etc.). The
output of each neuron is the weighted sum
passed into the activation function. The out-
put of each neuron is passed to each neuron in
the next layer. When predicting a continuous
value, a single linear output neuron is usually
used. Learning is usually done on neural net-
works through the back propagation algorithm.
This algorithm splits the error (squared error)
between each weight it the network. In order to
do this, we use the chain rule. On the hidden
layers the sum of the derivative is part of the
chain that is multiplied to ﬁnd the error. An
example of how error ﬂows back to the hidden

CS229 Final Paper

(cid:32)

∂Errortotal
∂whidden

=

∑

HigherLayer

∂Errortotal
∂outputhigher

∗ ∂outputhigher
∂nethigher

∗

∂nethigher

∂outputhidden

(cid:33)

∗ ∂outputhidden
∂nethidden

∗ ∂nethidden
∂whidden
(4)

layer is given in Eq. 4 (net is net sum of weights
times inputs and bias).

In recurrent neural networks the output of
the hidden layer neurons feeds back as another
input. A basic layout of a recurrent neural net-
work is given in ﬁgure 1. Since the input from
the previous time is passed as input to the cur-
rent time, recurrent neural networks have a
form of memory. With a recursive deﬁnition,
this memory covers every input set.

Figure 1: An example recurrent neural network. Out-
puts from the hidden layer neurons from the
previous time step are fed back in as input to
the next time step.

IV. Gradient Boosting Regression

Gradient Boosting is an ensemble learning
algorithm that uses a weighted average of sim-
ple models to learn a more complex model.
The algorithm ﬁrst uses a simple model to ﬁt
the data, then a simple model to ﬁt the residu-
als between the data and the ﬁrst model. This
process continues on each model’s residuals
until an appropriate ﬁt is found.
As the model increases in complexity, overﬁt-
ting tends to start occurring. To avoid over-
ﬁtting, the Gradient Boosting algorithm keeps
a portion of the data (different in each itera-
tion) out of the training set and uses it for cross

validation. The algorithm keeps track of the
cross validation error and determines that an
appropriate ﬁt has been found once the cross
validation error increases for a given number
of iterations.

We used the R package XGBoost to train
our Gradient Boosting models, then used pa-
rameter optimization to ﬁnd the best solution.

V. Results and Discussion

I. Random Forest

For our Random Forest approach, we used
the R package RandomForest for training mod-
els and making predictions. This package al-
lows for the variation of three major parame-
ters. Each parameter is mainly used to balance
the tradeoff between runtime and ﬁt quality.
Because random forest regression is a very re-
source hungry algorithm, parameters must be
set with feasibility in mind. The parameters
given to the model are as follows:

• mtry - the number of features allowed
in each decision tree. For a given value
k, trees may not use more than k regres-
sor variables from the 18 features given
to the model. Higher values for mtry
allows each tree to model more informa-
tion from the features, but requires more
trees to generate an informative average.
• ntrees - the number of trees averaged in
the model. A higher value means run-
time will be longer, but with a better ﬁt.
• sample size - because the training algo-
rithm is so resource hungry and Ross-
mann provided so much data, it is not
feasible to run Random Forest Regression

3

CS229 Final Paper

discretized to work with the hidden Markov
model. A smaller discretization leads to the
potential ability to be close to the true value;
however, it also decreases the probability esti-
mation between states as there are more states.
Figure 2 show the results of different levels of
sales discretization. This experiment showed
that dividing sales by 1500 and rounding pro-
duced the lowest training error.

on the whole dataset. Therefore this pa-
rameter sets the number of random data
samples that the model is trained on.

Table 2 shows the parameters for our best
two Random Forest models as well as the pre-
diction RMSPE as scored by Kaggle (Kaggle
scores on data for which we only have access
to the feature data).

Table 2: Parameter Selection for Random Forest Regres-

sion

mtry
7
9

ntrees
100
300

samplesize RMSPE
12.09%
11.98%

100000
100000

II. Hidden Markov Model

Figure 3: HMM error using Laplace Smoothing and

month data

In order to account for seasonality effects,
the month was also included as an experiment.
However, when tested the training error was
higher than not including this data (see ﬁgure
3). Increasing the number of observed states
by a factor of 12 (2688 states) likely decreased
the estimation of state transitioning.

Figure 2: HMM error using Laplace Smoothing

III. Recurrent Neural Network

For testing with HMMs, the stores were
treated separately. The decision to use the
stores separately came from An experiment
normalizing the data to zero mean and com-
bining the stores to estimate transitions. Com-
bining all this produced a much higher error
(over 70% RMSPE) and was computationally
expensive.

The observed states, what we know in ad-
vance, is everything except sales and customer
data. There are 224 state for different obser-
vations (7 days in a week, Store Open/Closed,
Promotion Active, 4 state holidays, and a
school holiday). The sales data needed to be

Due to the temporal nature of the data
hold out cross validation was used. 20% of
the data was left out for validation. In order
to work with the Matlab’s functionality, the
data was normalized. Each store was treated
separately for reasons similar to the Hidden
Markov method. During testing it was found
that the predicted values for closed days did
not always go to zero, so we set values for
closed days to zero. An experiment was to test
the effect the different numbers of neurons had
in the hidden layer. The experiment show that
7 neurons produced the lowest training error
(see ﬁgure 4).

4

CS229 Final Paper

to 5000, however the model wight 5000 trees
performed worse than the previous model. We
believe this to be due to overﬁtting since the
cross-validation error does not improve from
3000 to 5000 trees while the training error goes
down (as seen in Figure 5). Therefore, we chose
to keep the number of trees set at 3000. We
then added feature 18 to our feature set (Com-
petition Age).

Figure 4: RNN Error averages of different number of

hidden neurons (closed days forces to 0).

The actual error from Kaggle had much
higher error of around 24%. This may be due
to over ﬁtting. The neural network may have
learned the pattern leading up to the test date,
but the Kaggle test data started at a different
time.

IV. Gradient Boosting

Similar to Random Forests, Gradient Boost-
ing is an ensemble based regression, efﬁcient
for black-box prediction. However, Gradient
Boosting is much faster to train than Random
Forests.
It has been used in several Kaggle
competition-winning solutions and has been
developed into a the R package XGBoost.

Table 3: Model Performance

ntrees Parallel Trees
default(1)
3000
5000
default(1)
3000
3000
3000
3000

default(1)

10
10

10

# Features RMSPE
10.68%
10.70%
10.52%
10.50%
10.58%
10.56%

17
17
17
17
18
18

As shown in Table 3, our ﬁrst run of Gradi-
ent Boosting generated a RMSPE of 10.681%, a
signiﬁcant improvement on our best Random
Forest model. To further improve our model,
we increased the number of trees from 3000

Figure 5: Learning Curve for 5000 tree model

Adding Competiton Age to our feature set
did not improve our model on its own, but
combining models (runs 3, 5, and 6) lead to a
model with an improved RMSPE of 10.439%,
our best result.

Table 4: Model Performance

Model
Gradient Boosting
Random Forest
Hidden Markov Model
Predict Median Sales
Recurrent Neural Nets

RMSPE
10.44%
11.98%
16.52%
19.25%
24.88%

VI. Conclusion

We found that though Hidden Markov
Models and Recurrent Neural Networks are
better in theory for time-series data, ensemble
methods performed much better in practice.
We believe this to be due to their robustness
in black box optimization considering the data
provided is unstructured and some features
may not correlate well with sales.

5

CS229 Final Paper

References

[1] Figueredo, A. J. and Wolf, P. S. A. (2009).
Assortative pairing and life history strat-
egy - a cross-cultural study. Human Nature,
20:317–330.

[2] A. Liaw and M. Wiener (2002). Classiﬁca-
tion and Regression by randomForest. R
News, 2(3):18–22.

[3] Friedman, Jerome H (2001). Greedy Func-
tion Approximation: A Gradient Boost-
The Annals of Statistics,
ing Machine.
29(5):1189-1232.

[4] Friedman, Jerome H (2002). Stochastic
Gradient Boosting. Computational Statis-
tics & Data Analysis, 38(4):367-378.

[5] Samarasinghe, Sandhya (2006). Neural
Networks for Applied Sciences and Engi-
neering: From Fundamentals to Complex
Pattern Recognition. Auerbach Publications

[6] Zucchini, Walter and MacDonald, Iain L.
(2009) Hidden Markov Models for Time
Series: An Introduction Using R (Chap-
man & Hall/CRC Monographs on Statis-
tics & Applied Probability) Chapman and
Hall

6

