Forecasting Rossmann Store Leading 6-month Sales

CS 229 Fall 2015

-

Sen Lin, Eric Yu, Xiuzhen Guo

Abstract

Related Work

We investigated the comparative performance of Fre-
quency Domain Regression (FDR) and Support Vector
Regression (SVR) for time-series prediction of Rossman
Store Sales. Due to the extent of the data variables pro-
vided, SVR clearly outperformed FDR. Within SVR, our
results reviewed that a polynomial kernel with regulariza-
tion is most effective.

Introduction

Sales forecasting is critical for inventory management in the
retail industries. Ideally, store managers can use accurate pre-
dictions to meet demand while minimizing inventory footprint
and therefore operational costs. Further, discrete factors such
as holidays, opening of competitors and promotions all have a
signiﬁcant level of demand at any given day. We seek to an-
alyze the impact of these factors with the aid of time series
analysis and machine learning techniques.

One standard approach in dealing with time-series data was
the use of frequency domain regression, with band-spectrum
selection (Harvey, 1978). This model assumes that the distur-
bances from the mean are periodic and can effectively cap-
ture features like seasonal sales ﬂuctuations in weather-related
equipment sales (Wilson, Reale and Laywood, 2015).

Other papers have also investigated the use of support vec-
tor machines for time series forecasting (Muller and Vapnik,
1999) . Speciﬁc examples include electricity load prediction,
as conducted by researchers from National Taiwan University
(Hu, Bao and Xiong, 2013)

Lastly, groups have explored the use of neural networks for
the same purpose (Connor, Martin and Atlas, 1994). We did
not investigate this owing to the lack of resources, but this is a
promising area for further research.

Data Set

We used data from the Kaggle competition Rossmann Store
Sales - Forecast sales using store, promotion, and competitor
data. Rossmann GmbH is a major pharmaceutical chain with
over 3,000 stores across Europe, including stores in Poland,
Hungary, Czech Republic, Albania, and Turkey. Rossmann is
very similar to the pharmacy company Walgreens in the U.S.
The data contains a rich set of features, including both boolean
and continuous variables.

We were provided with data on 1115 stores located across
Germany. The data included sales records for each store over
the course of 942 days, giving us a total of about 1 million
data points. A second set of data included additional informa-
tion on the model of the store, assortment of goods sold and
presence of competitors in the area. We believe that this is
sufﬁcient data for our purposes. A summary of the raw data is
shown in Table 1 below:

We investigated both the Frequency Domain Regression and
the SVR method. We found that time-series methods under-
performed more powerful machine learning techniques. Upon
further scrutiny, we realized that this is because sales varia-
tions were mostly driven by these discrete events, while the
time-series trends of seasonal or inter-year trends were mini-
mal. We believe that this ﬁnding is generalized to many fore-
casting problems, where more granular day-to-day predictions
are required on a short span of 1-3 years.

Copyright c(cid:13) 2015, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Cross Validation: We divided 70% of the training examples
into the training set, and used the remaining 30% as the test
set. We chose the ﬁrst 70% of training examples in chronolog-
ical order, since we wanted to test our models on their ability
to extrapolate on dates outside of their given range.

Data Preprocessing: There we some general steps to take, in-
cluding numerizing all data, calculating the day of the week,
month of the year and so on. We also had to clean up the data
for routine store closures. Further data processing was done
differently for Frequency Domain Regression and SVR.

1. We observed that some of the shops were closed for ex-

Forecasting Rossmann Store Leading 6-month Sales

CS 229 Fall 2015

-

Sen Lin, Eric Yu, Xiuzhen Guo

Abstract

Related Work

We investigated the comparative performance of Fre-
quency Domain Regression (FDR) and Support Vector
Regression (SVR) for time-series prediction of Rossman
Store Sales. Due to the extent of the data variables pro-
vided, SVR clearly outperformed FDR. Within SVR, our
results reviewed that a polynomial kernel with regulariza-
tion is most effective.

Introduction

Sales forecasting is critical for inventory management in the
retail industries. Ideally, store managers can use accurate pre-
dictions to meet demand while minimizing inventory footprint
and therefore operational costs. Further, discrete factors such
as holidays, opening of competitors and promotions all have a
signiﬁcant level of demand at any given day. We seek to an-
alyze the impact of these factors with the aid of time series
analysis and machine learning techniques.

One standard approach in dealing with time-series data was
the use of frequency domain regression, with band-spectrum
selection (Harvey, 1978). This model assumes that the distur-
bances from the mean are periodic and can effectively cap-
ture features like seasonal sales ﬂuctuations in weather-related
equipment sales (Wilson, Reale and Laywood, 2015).

Other papers have also investigated the use of support vec-
tor machines for time series forecasting (Muller and Vapnik,
1999) . Speciﬁc examples include electricity load prediction,
as conducted by researchers from National Taiwan University
(Hu, Bao and Xiong, 2013)

Lastly, groups have explored the use of neural networks for
the same purpose (Connor, Martin and Atlas, 1994). We did
not investigate this owing to the lack of resources, but this is a
promising area for further research.

Data Set

We used data from the Kaggle competition Rossmann Store
Sales - Forecast sales using store, promotion, and competitor
data. Rossmann GmbH is a major pharmaceutical chain with
over 3,000 stores across Europe, including stores in Poland,
Hungary, Czech Republic, Albania, and Turkey. Rossmann is
very similar to the pharmacy company Walgreens in the U.S.
The data contains a rich set of features, including both boolean
and continuous variables.

We were provided with data on 1115 stores located across
Germany. The data included sales records for each store over
the course of 942 days, giving us a total of about 1 million
data points. A second set of data included additional informa-
tion on the model of the store, assortment of goods sold and
presence of competitors in the area. We believe that this is
sufﬁcient data for our purposes. A summary of the raw data is
shown in Table 1 below:

We investigated both the Frequency Domain Regression and
the SVR method. We found that time-series methods under-
performed more powerful machine learning techniques. Upon
further scrutiny, we realized that this is because sales varia-
tions were mostly driven by these discrete events, while the
time-series trends of seasonal or inter-year trends were mini-
mal. We believe that this ﬁnding is generalized to many fore-
casting problems, where more granular day-to-day predictions
are required on a short span of 1-3 years.

Copyright c(cid:13) 2015, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Cross Validation: We divided 70% of the training examples
into the training set, and used the remaining 30% as the test
set. We chose the ﬁrst 70% of training examples in chronolog-
ical order, since we wanted to test our models on their ability
to extrapolate on dates outside of their given range.

Data Preprocessing: There we some general steps to take, in-
cluding numerizing all data, calculating the day of the week,
month of the year and so on. We also had to clean up the data
for routine store closures. Further data processing was done
differently for Frequency Domain Regression and SVR.

1. We observed that some of the shops were closed for ex-

Table 1: Raw Data Fields

Field

Value Range

y(x) = µ +

(cid:88)

(Ak cos

k

2πkx

N

+ Bk cos

2πkx

N

) + (x)

Store ID
Date
Sales
Customers
Open
School Holidays
State Holidays
Store Type
Product Assortment
Competitor Distance
Date Since Competitor Open
Promo1
Existence of Promo2
Date Since Promo2 Began
Months with Promo2

1 – 1115
1 Jan 2013 – 31 July 2015
$0 – $41,551
0 – 7388
0,1
0,1
0,1,2,3
a,b,c,d
a,b,c
100 – 76,000 meters
1 Jan 2013 – 31 July 2015
0,1
0,1
1 Jan 2013 – 31 July 2015
Jan – Dec

tended periods of time and that these data-points with 0
sales affected our predictive algorithms. Since we only want
to assess the algorithm’s predictive power on days when the
stores were open, we opted to remove the days when a store
was closed from the data.

2. For SVR: We ﬁrst normalized the sales by subtracting the
mean sales for each store from the store’s sales numbers.
The mean sales number was retained as a parameter. This
allows us to focus the SVR on impact of events on distur-
bances from the mean. Then, we made numeral/boolean the
variables of the store type and product assortment, whether
there was a promotion, a speciﬁc holiday, a competitor
opening, and how long it has been open for.

3. For FDR: We treated the event variables as a blackbox and

simply worked with sales versus time for each store.

Methodology

We used three different approaches in order to predict store
sales with respect to time. Each method was trained on data
from a single store, and then used to predict the sales for that
particular store. This process was subsequently repeated for
every store.

First we used linear regression to obtain a baseline for the
prediction and to capture any inter-year trends which we may
want to use to further normalize the data for SVR and FDR.
The parameters for linear regression were found by using
MATLAB to solve the normal equations
θ = (X T X)−1X T y

We then ran a discrete Fourier Domain Regression to construct
a regression model for the periodic time series behavior. In
short, this method attempts to model the sales y on a particular
day x over the time period N by choosing the top k frequen-
cies as shown in the following equation:

We also used Support Vector Regression, which we felt was
better suited to predict the effects of events, such as pro-
motions and holidays, on store sales. For the training set
{(x(1), y(1)), ..., (x(m), y(m))}, where y(i) is the sales for a
particular point in time x(i), we seek to ﬁnd a hypothesis of
the form hw,b(x) = wT x + b with a small value of w. Our
optimization problem is

l(cid:88)

min
w,b

s.t.

i=1

||w||2 + C
(ξi + ξ∗
1
i )
2
y(i) − wT x(i) − b ≤  + ξi
wT x(i) + b − y(i) ≤  + ξ∗
ξi, ξ∗

i ≥ 0

i

i = 1, ..., m

i = 1, ..., m

Where  > 0 is a given ﬁxed value. We solved this prob-
lem with the aid of of the support vector regression function
in the scikit-learn package. We can then avoid underﬁtting or
overﬁtting of the training data via conventional-validation on
the variable C to control the slack allowance through the term
i ), as well as a choice between linear, polynomial

(ξi + ξ∗

l(cid:80)

C
or gaussian kernels.

i=1

Error Metric

The metrics that we used for our analysis was the root-mean-
squared-percentage-error (RMSPE), which is calculated as

(cid:118)(cid:117)(cid:117)(cid:116) 1

n

n(cid:88)

(cid:18) yi − ˆyi

(cid:19)2

i=1

yi

RMSPE =

where n is the number of days,yi is the sales of a store on
a single day and ˆyi is the corresponding prediction. This is a
number used operationally in inventory planning and is hence
pertinent to the problem at hand.

Experiment Results and Discussion

We only managed to train/test on a limited number of stores
owing to computation resource constraints. Our observations
for linear regression, FDR and SVR are summarized in
Table2.

Table 2: Test errors for training methods

Method

Test Error

Linear Regression
Frequency Domain Linear Regression
Support Vector Regression

30.2%
29.1%
17.4%

Forecasting Rossmann Store Leading 6-month Sales

CS 229 Fall 2015

-

Sen Lin, Eric Yu, Xiuzhen Guo

Abstract

Related Work

We investigated the comparative performance of Fre-
quency Domain Regression (FDR) and Support Vector
Regression (SVR) for time-series prediction of Rossman
Store Sales. Due to the extent of the data variables pro-
vided, SVR clearly outperformed FDR. Within SVR, our
results reviewed that a polynomial kernel with regulariza-
tion is most effective.

Introduction

Sales forecasting is critical for inventory management in the
retail industries. Ideally, store managers can use accurate pre-
dictions to meet demand while minimizing inventory footprint
and therefore operational costs. Further, discrete factors such
as holidays, opening of competitors and promotions all have a
signiﬁcant level of demand at any given day. We seek to an-
alyze the impact of these factors with the aid of time series
analysis and machine learning techniques.

One standard approach in dealing with time-series data was
the use of frequency domain regression, with band-spectrum
selection (Harvey, 1978). This model assumes that the distur-
bances from the mean are periodic and can effectively cap-
ture features like seasonal sales ﬂuctuations in weather-related
equipment sales (Wilson, Reale and Laywood, 2015).

Other papers have also investigated the use of support vec-
tor machines for time series forecasting (Muller and Vapnik,
1999) . Speciﬁc examples include electricity load prediction,
as conducted by researchers from National Taiwan University
(Hu, Bao and Xiong, 2013)

Lastly, groups have explored the use of neural networks for
the same purpose (Connor, Martin and Atlas, 1994). We did
not investigate this owing to the lack of resources, but this is a
promising area for further research.

Data Set

We used data from the Kaggle competition Rossmann Store
Sales - Forecast sales using store, promotion, and competitor
data. Rossmann GmbH is a major pharmaceutical chain with
over 3,000 stores across Europe, including stores in Poland,
Hungary, Czech Republic, Albania, and Turkey. Rossmann is
very similar to the pharmacy company Walgreens in the U.S.
The data contains a rich set of features, including both boolean
and continuous variables.

We were provided with data on 1115 stores located across
Germany. The data included sales records for each store over
the course of 942 days, giving us a total of about 1 million
data points. A second set of data included additional informa-
tion on the model of the store, assortment of goods sold and
presence of competitors in the area. We believe that this is
sufﬁcient data for our purposes. A summary of the raw data is
shown in Table 1 below:

We investigated both the Frequency Domain Regression and
the SVR method. We found that time-series methods under-
performed more powerful machine learning techniques. Upon
further scrutiny, we realized that this is because sales varia-
tions were mostly driven by these discrete events, while the
time-series trends of seasonal or inter-year trends were mini-
mal. We believe that this ﬁnding is generalized to many fore-
casting problems, where more granular day-to-day predictions
are required on a short span of 1-3 years.

Copyright c(cid:13) 2015, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Cross Validation: We divided 70% of the training examples
into the training set, and used the remaining 30% as the test
set. We chose the ﬁrst 70% of training examples in chronolog-
ical order, since we wanted to test our models on their ability
to extrapolate on dates outside of their given range.

Data Preprocessing: There we some general steps to take, in-
cluding numerizing all data, calculating the day of the week,
month of the year and so on. We also had to clean up the data
for routine store closures. Further data processing was done
differently for Frequency Domain Regression and SVR.

1. We observed that some of the shops were closed for ex-

Table 1: Raw Data Fields

Field

Value Range

y(x) = µ +

(cid:88)

(Ak cos

k

2πkx

N

+ Bk cos

2πkx

N

) + (x)

Store ID
Date
Sales
Customers
Open
School Holidays
State Holidays
Store Type
Product Assortment
Competitor Distance
Date Since Competitor Open
Promo1
Existence of Promo2
Date Since Promo2 Began
Months with Promo2

1 – 1115
1 Jan 2013 – 31 July 2015
$0 – $41,551
0 – 7388
0,1
0,1
0,1,2,3
a,b,c,d
a,b,c
100 – 76,000 meters
1 Jan 2013 – 31 July 2015
0,1
0,1
1 Jan 2013 – 31 July 2015
Jan – Dec

tended periods of time and that these data-points with 0
sales affected our predictive algorithms. Since we only want
to assess the algorithm’s predictive power on days when the
stores were open, we opted to remove the days when a store
was closed from the data.

2. For SVR: We ﬁrst normalized the sales by subtracting the
mean sales for each store from the store’s sales numbers.
The mean sales number was retained as a parameter. This
allows us to focus the SVR on impact of events on distur-
bances from the mean. Then, we made numeral/boolean the
variables of the store type and product assortment, whether
there was a promotion, a speciﬁc holiday, a competitor
opening, and how long it has been open for.

3. For FDR: We treated the event variables as a blackbox and

simply worked with sales versus time for each store.

Methodology

We used three different approaches in order to predict store
sales with respect to time. Each method was trained on data
from a single store, and then used to predict the sales for that
particular store. This process was subsequently repeated for
every store.

First we used linear regression to obtain a baseline for the
prediction and to capture any inter-year trends which we may
want to use to further normalize the data for SVR and FDR.
The parameters for linear regression were found by using
MATLAB to solve the normal equations
θ = (X T X)−1X T y

We then ran a discrete Fourier Domain Regression to construct
a regression model for the periodic time series behavior. In
short, this method attempts to model the sales y on a particular
day x over the time period N by choosing the top k frequen-
cies as shown in the following equation:

We also used Support Vector Regression, which we felt was
better suited to predict the effects of events, such as pro-
motions and holidays, on store sales. For the training set
{(x(1), y(1)), ..., (x(m), y(m))}, where y(i) is the sales for a
particular point in time x(i), we seek to ﬁnd a hypothesis of
the form hw,b(x) = wT x + b with a small value of w. Our
optimization problem is

l(cid:88)

min
w,b

s.t.

i=1

||w||2 + C
(ξi + ξ∗
1
i )
2
y(i) − wT x(i) − b ≤  + ξi
wT x(i) + b − y(i) ≤  + ξ∗
ξi, ξ∗

i ≥ 0

i

i = 1, ..., m

i = 1, ..., m

Where  > 0 is a given ﬁxed value. We solved this prob-
lem with the aid of of the support vector regression function
in the scikit-learn package. We can then avoid underﬁtting or
overﬁtting of the training data via conventional-validation on
the variable C to control the slack allowance through the term
i ), as well as a choice between linear, polynomial

(ξi + ξ∗

l(cid:80)

C
or gaussian kernels.

i=1

Error Metric

The metrics that we used for our analysis was the root-mean-
squared-percentage-error (RMSPE), which is calculated as

(cid:118)(cid:117)(cid:117)(cid:116) 1

n

n(cid:88)

(cid:18) yi − ˆyi

(cid:19)2

i=1

yi

RMSPE =

where n is the number of days,yi is the sales of a store on
a single day and ˆyi is the corresponding prediction. This is a
number used operationally in inventory planning and is hence
pertinent to the problem at hand.

Experiment Results and Discussion

We only managed to train/test on a limited number of stores
owing to computation resource constraints. Our observations
for linear regression, FDR and SVR are summarized in
Table2.

Table 2: Test errors for training methods

Method

Test Error

Linear Regression
Frequency Domain Linear Regression
Support Vector Regression

30.2%
29.1%
17.4%

Perhaps unsurprisingly,
the more powerful SVR, with a
degree-2 polynomial kernel and C = 1000 signiﬁcantly out-
performed LR. However, it was a surprising that FDR did so
poorly - in many cases even worse than linear regression on
the test data. We analyze this result below.

Linear Regression

This model is a rudimentary ﬁrst look into the large scale
trends. We did not expect it to capture the granular movements
of the sales numbers and indeed it didn’t, reporting an average
RMSPE of 30.2%. Shown in Figure 1 below is the trend ob-
servations for a single store over the relevant time period.

The use of FDR was motivated by the periodic movement of
sales data over 2-3 weeks, which becomes evident when we
plot over a period of 2-3 months. Shown in Figure 3a at the
top is periodicity observed and the corresponding ﬁtted trend
over training data. The plot in 3b at the bottom shows the
performance of the model with a chosen set of extrapolated
test data.

Figure 1: Linear Regression

We observe that there are minimal inter-year trends and there-
fore can safely disregard them in future considerations.

Frequency Domain Regression

The results of FDR are as follows:

Figure 2: Frequency Domain Regression Results

The green line and points correspond to the training data and
FDR trendline. The red line and points are the extrapolated
prediction plotted against the test set. In the Frequency Do-
main graph, the black line shows the amplitude of each fre-
quency, plotted as Cycles per Year. The red crosses denote the
frequencies chosen for the regression model.

Figure 3: 2 Month Plot with FDR Line

Clearly, the model seemed to be almost shifted by half a phase
from the actual periodicity of the data. Although it ﬁtted the
training data extremely well, its extrapolated performance was
unacceptable. We believe that it is due to the following factors:

1. The periodicity was not due to unknown weekly or fort-
nightly factors but due to company driven actions - Promo1.
Later, when plotting Promo1 against the sales, we realized
that the sales increases when the boolean Promo1 is true. In
this sense, there is no true periodic factors with consistent
phase and frequency. The company has introduced period-
icity on its own schedule.

2. The model is numerically unstable. Even if there is a natu-
ral periodicity, a small error in frequency is equivalent to a
beats phenomenon in harmonic analysis, causing the model
to be eventually π out of phase with the original trendline
after some time. Given the low resolution (14 data points
per cycle) of each period, the inherent imprecision is too
big given our extrapolation time span.

With the knowledge that the periodicity is event-driven, we
then approached the problem with SVR.

Forecasting Rossmann Store Leading 6-month Sales

CS 229 Fall 2015

-

Sen Lin, Eric Yu, Xiuzhen Guo

Abstract

Related Work

We investigated the comparative performance of Fre-
quency Domain Regression (FDR) and Support Vector
Regression (SVR) for time-series prediction of Rossman
Store Sales. Due to the extent of the data variables pro-
vided, SVR clearly outperformed FDR. Within SVR, our
results reviewed that a polynomial kernel with regulariza-
tion is most effective.

Introduction

Sales forecasting is critical for inventory management in the
retail industries. Ideally, store managers can use accurate pre-
dictions to meet demand while minimizing inventory footprint
and therefore operational costs. Further, discrete factors such
as holidays, opening of competitors and promotions all have a
signiﬁcant level of demand at any given day. We seek to an-
alyze the impact of these factors with the aid of time series
analysis and machine learning techniques.

One standard approach in dealing with time-series data was
the use of frequency domain regression, with band-spectrum
selection (Harvey, 1978). This model assumes that the distur-
bances from the mean are periodic and can effectively cap-
ture features like seasonal sales ﬂuctuations in weather-related
equipment sales (Wilson, Reale and Laywood, 2015).

Other papers have also investigated the use of support vec-
tor machines for time series forecasting (Muller and Vapnik,
1999) . Speciﬁc examples include electricity load prediction,
as conducted by researchers from National Taiwan University
(Hu, Bao and Xiong, 2013)

Lastly, groups have explored the use of neural networks for
the same purpose (Connor, Martin and Atlas, 1994). We did
not investigate this owing to the lack of resources, but this is a
promising area for further research.

Data Set

We used data from the Kaggle competition Rossmann Store
Sales - Forecast sales using store, promotion, and competitor
data. Rossmann GmbH is a major pharmaceutical chain with
over 3,000 stores across Europe, including stores in Poland,
Hungary, Czech Republic, Albania, and Turkey. Rossmann is
very similar to the pharmacy company Walgreens in the U.S.
The data contains a rich set of features, including both boolean
and continuous variables.

We were provided with data on 1115 stores located across
Germany. The data included sales records for each store over
the course of 942 days, giving us a total of about 1 million
data points. A second set of data included additional informa-
tion on the model of the store, assortment of goods sold and
presence of competitors in the area. We believe that this is
sufﬁcient data for our purposes. A summary of the raw data is
shown in Table 1 below:

We investigated both the Frequency Domain Regression and
the SVR method. We found that time-series methods under-
performed more powerful machine learning techniques. Upon
further scrutiny, we realized that this is because sales varia-
tions were mostly driven by these discrete events, while the
time-series trends of seasonal or inter-year trends were mini-
mal. We believe that this ﬁnding is generalized to many fore-
casting problems, where more granular day-to-day predictions
are required on a short span of 1-3 years.

Copyright c(cid:13) 2015, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Cross Validation: We divided 70% of the training examples
into the training set, and used the remaining 30% as the test
set. We chose the ﬁrst 70% of training examples in chronolog-
ical order, since we wanted to test our models on their ability
to extrapolate on dates outside of their given range.

Data Preprocessing: There we some general steps to take, in-
cluding numerizing all data, calculating the day of the week,
month of the year and so on. We also had to clean up the data
for routine store closures. Further data processing was done
differently for Frequency Domain Regression and SVR.

1. We observed that some of the shops were closed for ex-

Table 1: Raw Data Fields

Field

Value Range

y(x) = µ +

(cid:88)

(Ak cos

k

2πkx

N

+ Bk cos

2πkx

N

) + (x)

Store ID
Date
Sales
Customers
Open
School Holidays
State Holidays
Store Type
Product Assortment
Competitor Distance
Date Since Competitor Open
Promo1
Existence of Promo2
Date Since Promo2 Began
Months with Promo2

1 – 1115
1 Jan 2013 – 31 July 2015
$0 – $41,551
0 – 7388
0,1
0,1
0,1,2,3
a,b,c,d
a,b,c
100 – 76,000 meters
1 Jan 2013 – 31 July 2015
0,1
0,1
1 Jan 2013 – 31 July 2015
Jan – Dec

tended periods of time and that these data-points with 0
sales affected our predictive algorithms. Since we only want
to assess the algorithm’s predictive power on days when the
stores were open, we opted to remove the days when a store
was closed from the data.

2. For SVR: We ﬁrst normalized the sales by subtracting the
mean sales for each store from the store’s sales numbers.
The mean sales number was retained as a parameter. This
allows us to focus the SVR on impact of events on distur-
bances from the mean. Then, we made numeral/boolean the
variables of the store type and product assortment, whether
there was a promotion, a speciﬁc holiday, a competitor
opening, and how long it has been open for.

3. For FDR: We treated the event variables as a blackbox and

simply worked with sales versus time for each store.

Methodology

We used three different approaches in order to predict store
sales with respect to time. Each method was trained on data
from a single store, and then used to predict the sales for that
particular store. This process was subsequently repeated for
every store.

First we used linear regression to obtain a baseline for the
prediction and to capture any inter-year trends which we may
want to use to further normalize the data for SVR and FDR.
The parameters for linear regression were found by using
MATLAB to solve the normal equations
θ = (X T X)−1X T y

We then ran a discrete Fourier Domain Regression to construct
a regression model for the periodic time series behavior. In
short, this method attempts to model the sales y on a particular
day x over the time period N by choosing the top k frequen-
cies as shown in the following equation:

We also used Support Vector Regression, which we felt was
better suited to predict the effects of events, such as pro-
motions and holidays, on store sales. For the training set
{(x(1), y(1)), ..., (x(m), y(m))}, where y(i) is the sales for a
particular point in time x(i), we seek to ﬁnd a hypothesis of
the form hw,b(x) = wT x + b with a small value of w. Our
optimization problem is

l(cid:88)

min
w,b

s.t.

i=1

||w||2 + C
(ξi + ξ∗
1
i )
2
y(i) − wT x(i) − b ≤  + ξi
wT x(i) + b − y(i) ≤  + ξ∗
ξi, ξ∗

i ≥ 0

i

i = 1, ..., m

i = 1, ..., m

Where  > 0 is a given ﬁxed value. We solved this prob-
lem with the aid of of the support vector regression function
in the scikit-learn package. We can then avoid underﬁtting or
overﬁtting of the training data via conventional-validation on
the variable C to control the slack allowance through the term
i ), as well as a choice between linear, polynomial

(ξi + ξ∗

l(cid:80)

C
or gaussian kernels.

i=1

Error Metric

The metrics that we used for our analysis was the root-mean-
squared-percentage-error (RMSPE), which is calculated as

(cid:118)(cid:117)(cid:117)(cid:116) 1

n

n(cid:88)

(cid:18) yi − ˆyi

(cid:19)2

i=1

yi

RMSPE =

where n is the number of days,yi is the sales of a store on
a single day and ˆyi is the corresponding prediction. This is a
number used operationally in inventory planning and is hence
pertinent to the problem at hand.

Experiment Results and Discussion

We only managed to train/test on a limited number of stores
owing to computation resource constraints. Our observations
for linear regression, FDR and SVR are summarized in
Table2.

Table 2: Test errors for training methods

Method

Test Error

Linear Regression
Frequency Domain Linear Regression
Support Vector Regression

30.2%
29.1%
17.4%

Perhaps unsurprisingly,
the more powerful SVR, with a
degree-2 polynomial kernel and C = 1000 signiﬁcantly out-
performed LR. However, it was a surprising that FDR did so
poorly - in many cases even worse than linear regression on
the test data. We analyze this result below.

Linear Regression

This model is a rudimentary ﬁrst look into the large scale
trends. We did not expect it to capture the granular movements
of the sales numbers and indeed it didn’t, reporting an average
RMSPE of 30.2%. Shown in Figure 1 below is the trend ob-
servations for a single store over the relevant time period.

The use of FDR was motivated by the periodic movement of
sales data over 2-3 weeks, which becomes evident when we
plot over a period of 2-3 months. Shown in Figure 3a at the
top is periodicity observed and the corresponding ﬁtted trend
over training data. The plot in 3b at the bottom shows the
performance of the model with a chosen set of extrapolated
test data.

Figure 1: Linear Regression

We observe that there are minimal inter-year trends and there-
fore can safely disregard them in future considerations.

Frequency Domain Regression

The results of FDR are as follows:

Figure 2: Frequency Domain Regression Results

The green line and points correspond to the training data and
FDR trendline. The red line and points are the extrapolated
prediction plotted against the test set. In the Frequency Do-
main graph, the black line shows the amplitude of each fre-
quency, plotted as Cycles per Year. The red crosses denote the
frequencies chosen for the regression model.

Figure 3: 2 Month Plot with FDR Line

Clearly, the model seemed to be almost shifted by half a phase
from the actual periodicity of the data. Although it ﬁtted the
training data extremely well, its extrapolated performance was
unacceptable. We believe that it is due to the following factors:

1. The periodicity was not due to unknown weekly or fort-
nightly factors but due to company driven actions - Promo1.
Later, when plotting Promo1 against the sales, we realized
that the sales increases when the boolean Promo1 is true. In
this sense, there is no true periodic factors with consistent
phase and frequency. The company has introduced period-
icity on its own schedule.

2. The model is numerically unstable. Even if there is a natu-
ral periodicity, a small error in frequency is equivalent to a
beats phenomenon in harmonic analysis, causing the model
to be eventually π out of phase with the original trendline
after some time. Given the low resolution (14 data points
per cycle) of each period, the inherent imprecision is too
big given our extrapolation time span.

With the knowledge that the periodicity is event-driven, we
then approached the problem with SVR.

Support Vector Regression

From LR and FDR, we now know that the sales is unexpect-
edly lacking in event-agnostic time-series behavior. Virtually
all movements in the sales volume are event-driven. Thus, we
stripped the data of all time-based information - save for hol-
idays, which we store as boolean variables - and plugged the
entire dataset into an SVR.

Here, a stumbling block was the computational complexity of
SVR. As a rule of thumb, the big-O for SVR algorithms are
given (Chapelle, 2007)

Gaussian Kernel: T (n, k) = O(n2k)

Linear Kernel: T (n, k) = O(nk2)

Where n is the size of the training set and k is the number of
features. By this estimate, with 700,000 training data points,
this was clearly unfeasible. An approximate SVM algorithm
T (n, k) = O(k2) exists (Claesen et al, 2014), but we did not
have time to try this.

We did however analyze a single store of type b and prod-
uct assortment b, which is the most representative among the
population of stores. The results are summarized in Figure 4,
5 and 6.

As we increase C, we are forcing the SVR algorithm to work
with smaller slack variables. Thus, as C increases, we expect
the training errors to fall.

Figure 5: SVM with Gaussian Kernel Error Plots

The Gaussian Kernel on the other hand exhibited clear high-
variance behavior. It was able to incrementally improve its
performance on the training data as we increase C. Indeed,
it has almost perfected ﬁtted the training set at C = 105. Un-
fortunately, the test error did not follow and only reached an
optimal at around C = 500.

Figure 6: SVM with Linear Kernel Error Plots

Figure 4: SVM with Linear Kernel Error Plots

The Linear Kernel is seen to be unable to increase its accu-
racy, despite our increasing C. Our results suggest that after
C ≥ 10, all the SVR is able to achieve is a great cost function
without changing the underlying regression line. The predic-
tion error thus plateaued in a high-bias situation.

Clearly, Polynomial Kernels outperformed both Gaussian and
Linear Kernels. It seems to be the optimal model, with C =
1000, in the bias-variance trade-off. Here we note an anomaly
in the Polynomial Kernel, where the training error brieﬂy rose
at C = 105. This is cause for further investigation beyond this
report.

We are also concerned that the Gaussian kernel underper-
formed Polynomial kernels and would like to look deeper into
this phenomenon.

Forecasting Rossmann Store Leading 6-month Sales

CS 229 Fall 2015

-

Sen Lin, Eric Yu, Xiuzhen Guo

Abstract

Related Work

We investigated the comparative performance of Fre-
quency Domain Regression (FDR) and Support Vector
Regression (SVR) for time-series prediction of Rossman
Store Sales. Due to the extent of the data variables pro-
vided, SVR clearly outperformed FDR. Within SVR, our
results reviewed that a polynomial kernel with regulariza-
tion is most effective.

Introduction

Sales forecasting is critical for inventory management in the
retail industries. Ideally, store managers can use accurate pre-
dictions to meet demand while minimizing inventory footprint
and therefore operational costs. Further, discrete factors such
as holidays, opening of competitors and promotions all have a
signiﬁcant level of demand at any given day. We seek to an-
alyze the impact of these factors with the aid of time series
analysis and machine learning techniques.

One standard approach in dealing with time-series data was
the use of frequency domain regression, with band-spectrum
selection (Harvey, 1978). This model assumes that the distur-
bances from the mean are periodic and can effectively cap-
ture features like seasonal sales ﬂuctuations in weather-related
equipment sales (Wilson, Reale and Laywood, 2015).

Other papers have also investigated the use of support vec-
tor machines for time series forecasting (Muller and Vapnik,
1999) . Speciﬁc examples include electricity load prediction,
as conducted by researchers from National Taiwan University
(Hu, Bao and Xiong, 2013)

Lastly, groups have explored the use of neural networks for
the same purpose (Connor, Martin and Atlas, 1994). We did
not investigate this owing to the lack of resources, but this is a
promising area for further research.

Data Set

We used data from the Kaggle competition Rossmann Store
Sales - Forecast sales using store, promotion, and competitor
data. Rossmann GmbH is a major pharmaceutical chain with
over 3,000 stores across Europe, including stores in Poland,
Hungary, Czech Republic, Albania, and Turkey. Rossmann is
very similar to the pharmacy company Walgreens in the U.S.
The data contains a rich set of features, including both boolean
and continuous variables.

We were provided with data on 1115 stores located across
Germany. The data included sales records for each store over
the course of 942 days, giving us a total of about 1 million
data points. A second set of data included additional informa-
tion on the model of the store, assortment of goods sold and
presence of competitors in the area. We believe that this is
sufﬁcient data for our purposes. A summary of the raw data is
shown in Table 1 below:

We investigated both the Frequency Domain Regression and
the SVR method. We found that time-series methods under-
performed more powerful machine learning techniques. Upon
further scrutiny, we realized that this is because sales varia-
tions were mostly driven by these discrete events, while the
time-series trends of seasonal or inter-year trends were mini-
mal. We believe that this ﬁnding is generalized to many fore-
casting problems, where more granular day-to-day predictions
are required on a short span of 1-3 years.

Copyright c(cid:13) 2015, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Cross Validation: We divided 70% of the training examples
into the training set, and used the remaining 30% as the test
set. We chose the ﬁrst 70% of training examples in chronolog-
ical order, since we wanted to test our models on their ability
to extrapolate on dates outside of their given range.

Data Preprocessing: There we some general steps to take, in-
cluding numerizing all data, calculating the day of the week,
month of the year and so on. We also had to clean up the data
for routine store closures. Further data processing was done
differently for Frequency Domain Regression and SVR.

1. We observed that some of the shops were closed for ex-

Table 1: Raw Data Fields

Field

Value Range

y(x) = µ +

(cid:88)

(Ak cos

k

2πkx

N

+ Bk cos

2πkx

N

) + (x)

Store ID
Date
Sales
Customers
Open
School Holidays
State Holidays
Store Type
Product Assortment
Competitor Distance
Date Since Competitor Open
Promo1
Existence of Promo2
Date Since Promo2 Began
Months with Promo2

1 – 1115
1 Jan 2013 – 31 July 2015
$0 – $41,551
0 – 7388
0,1
0,1
0,1,2,3
a,b,c,d
a,b,c
100 – 76,000 meters
1 Jan 2013 – 31 July 2015
0,1
0,1
1 Jan 2013 – 31 July 2015
Jan – Dec

tended periods of time and that these data-points with 0
sales affected our predictive algorithms. Since we only want
to assess the algorithm’s predictive power on days when the
stores were open, we opted to remove the days when a store
was closed from the data.

2. For SVR: We ﬁrst normalized the sales by subtracting the
mean sales for each store from the store’s sales numbers.
The mean sales number was retained as a parameter. This
allows us to focus the SVR on impact of events on distur-
bances from the mean. Then, we made numeral/boolean the
variables of the store type and product assortment, whether
there was a promotion, a speciﬁc holiday, a competitor
opening, and how long it has been open for.

3. For FDR: We treated the event variables as a blackbox and

simply worked with sales versus time for each store.

Methodology

We used three different approaches in order to predict store
sales with respect to time. Each method was trained on data
from a single store, and then used to predict the sales for that
particular store. This process was subsequently repeated for
every store.

First we used linear regression to obtain a baseline for the
prediction and to capture any inter-year trends which we may
want to use to further normalize the data for SVR and FDR.
The parameters for linear regression were found by using
MATLAB to solve the normal equations
θ = (X T X)−1X T y

We then ran a discrete Fourier Domain Regression to construct
a regression model for the periodic time series behavior. In
short, this method attempts to model the sales y on a particular
day x over the time period N by choosing the top k frequen-
cies as shown in the following equation:

We also used Support Vector Regression, which we felt was
better suited to predict the effects of events, such as pro-
motions and holidays, on store sales. For the training set
{(x(1), y(1)), ..., (x(m), y(m))}, where y(i) is the sales for a
particular point in time x(i), we seek to ﬁnd a hypothesis of
the form hw,b(x) = wT x + b with a small value of w. Our
optimization problem is

l(cid:88)

min
w,b

s.t.

i=1

||w||2 + C
(ξi + ξ∗
1
i )
2
y(i) − wT x(i) − b ≤  + ξi
wT x(i) + b − y(i) ≤  + ξ∗
ξi, ξ∗

i ≥ 0

i

i = 1, ..., m

i = 1, ..., m

Where  > 0 is a given ﬁxed value. We solved this prob-
lem with the aid of of the support vector regression function
in the scikit-learn package. We can then avoid underﬁtting or
overﬁtting of the training data via conventional-validation on
the variable C to control the slack allowance through the term
i ), as well as a choice between linear, polynomial

(ξi + ξ∗

l(cid:80)

C
or gaussian kernels.

i=1

Error Metric

The metrics that we used for our analysis was the root-mean-
squared-percentage-error (RMSPE), which is calculated as

(cid:118)(cid:117)(cid:117)(cid:116) 1

n

n(cid:88)

(cid:18) yi − ˆyi

(cid:19)2

i=1

yi

RMSPE =

where n is the number of days,yi is the sales of a store on
a single day and ˆyi is the corresponding prediction. This is a
number used operationally in inventory planning and is hence
pertinent to the problem at hand.

Experiment Results and Discussion

We only managed to train/test on a limited number of stores
owing to computation resource constraints. Our observations
for linear regression, FDR and SVR are summarized in
Table2.

Table 2: Test errors for training methods

Method

Test Error

Linear Regression
Frequency Domain Linear Regression
Support Vector Regression

30.2%
29.1%
17.4%

Perhaps unsurprisingly,
the more powerful SVR, with a
degree-2 polynomial kernel and C = 1000 signiﬁcantly out-
performed LR. However, it was a surprising that FDR did so
poorly - in many cases even worse than linear regression on
the test data. We analyze this result below.

Linear Regression

This model is a rudimentary ﬁrst look into the large scale
trends. We did not expect it to capture the granular movements
of the sales numbers and indeed it didn’t, reporting an average
RMSPE of 30.2%. Shown in Figure 1 below is the trend ob-
servations for a single store over the relevant time period.

The use of FDR was motivated by the periodic movement of
sales data over 2-3 weeks, which becomes evident when we
plot over a period of 2-3 months. Shown in Figure 3a at the
top is periodicity observed and the corresponding ﬁtted trend
over training data. The plot in 3b at the bottom shows the
performance of the model with a chosen set of extrapolated
test data.

Figure 1: Linear Regression

We observe that there are minimal inter-year trends and there-
fore can safely disregard them in future considerations.

Frequency Domain Regression

The results of FDR are as follows:

Figure 2: Frequency Domain Regression Results

The green line and points correspond to the training data and
FDR trendline. The red line and points are the extrapolated
prediction plotted against the test set. In the Frequency Do-
main graph, the black line shows the amplitude of each fre-
quency, plotted as Cycles per Year. The red crosses denote the
frequencies chosen for the regression model.

Figure 3: 2 Month Plot with FDR Line

Clearly, the model seemed to be almost shifted by half a phase
from the actual periodicity of the data. Although it ﬁtted the
training data extremely well, its extrapolated performance was
unacceptable. We believe that it is due to the following factors:

1. The periodicity was not due to unknown weekly or fort-
nightly factors but due to company driven actions - Promo1.
Later, when plotting Promo1 against the sales, we realized
that the sales increases when the boolean Promo1 is true. In
this sense, there is no true periodic factors with consistent
phase and frequency. The company has introduced period-
icity on its own schedule.

2. The model is numerically unstable. Even if there is a natu-
ral periodicity, a small error in frequency is equivalent to a
beats phenomenon in harmonic analysis, causing the model
to be eventually π out of phase with the original trendline
after some time. Given the low resolution (14 data points
per cycle) of each period, the inherent imprecision is too
big given our extrapolation time span.

With the knowledge that the periodicity is event-driven, we
then approached the problem with SVR.

Support Vector Regression

From LR and FDR, we now know that the sales is unexpect-
edly lacking in event-agnostic time-series behavior. Virtually
all movements in the sales volume are event-driven. Thus, we
stripped the data of all time-based information - save for hol-
idays, which we store as boolean variables - and plugged the
entire dataset into an SVR.

Here, a stumbling block was the computational complexity of
SVR. As a rule of thumb, the big-O for SVR algorithms are
given (Chapelle, 2007)

Gaussian Kernel: T (n, k) = O(n2k)

Linear Kernel: T (n, k) = O(nk2)

Where n is the size of the training set and k is the number of
features. By this estimate, with 700,000 training data points,
this was clearly unfeasible. An approximate SVM algorithm
T (n, k) = O(k2) exists (Claesen et al, 2014), but we did not
have time to try this.

We did however analyze a single store of type b and prod-
uct assortment b, which is the most representative among the
population of stores. The results are summarized in Figure 4,
5 and 6.

As we increase C, we are forcing the SVR algorithm to work
with smaller slack variables. Thus, as C increases, we expect
the training errors to fall.

Figure 5: SVM with Gaussian Kernel Error Plots

The Gaussian Kernel on the other hand exhibited clear high-
variance behavior. It was able to incrementally improve its
performance on the training data as we increase C. Indeed,
it has almost perfected ﬁtted the training set at C = 105. Un-
fortunately, the test error did not follow and only reached an
optimal at around C = 500.

Figure 6: SVM with Linear Kernel Error Plots

Figure 4: SVM with Linear Kernel Error Plots

The Linear Kernel is seen to be unable to increase its accu-
racy, despite our increasing C. Our results suggest that after
C ≥ 10, all the SVR is able to achieve is a great cost function
without changing the underlying regression line. The predic-
tion error thus plateaued in a high-bias situation.

Clearly, Polynomial Kernels outperformed both Gaussian and
Linear Kernels. It seems to be the optimal model, with C =
1000, in the bias-variance trade-off. Here we note an anomaly
in the Polynomial Kernel, where the training error brieﬂy rose
at C = 105. This is cause for further investigation beyond this
report.

We are also concerned that the Gaussian kernel underper-
formed Polynomial kernels and would like to look deeper into
this phenomenon.

Connor, Jerome T., R. Douglas Martin, and Les E. Atlas. ”Re-
current neural networks and robust time series prediction.”
Neural Networks, IEEE Transactions on 5.2 (1994): 240-254.

Harvey, Andrew C. ”Linear regression in the frequency do-
main.” International Economic Review (1978): 507-512.

Kaggle, 2015. https://www.kaggle.com/

Hu, Zhongyi, Yukun Bao, and Tao Xiong. ”Electricity load
forecasting using support vector regression with memetic al-
gorithms.” The Scientiﬁc World Journal 2013 (2013).

Muller, Klaus Robert, et al. ”Using support vector machines
for time series prediction.” Advances in kernel methodssup-
port vector learning, MIT Press, Cambridge, MA (1999): 243-
254.

Smola, Alex J., and Bernhard Schlkopf. ”A tutorial on support
vector regression.” Statistics and computing 14.3 (2004): 199-
222.

Wilson, Granville Tunnicliffe, Marco Reale, and John Hay-
wood. Models for dependent time series. Vol. 139. CRC Press,
2015.

Conclusion

Our highest performing method was the support vector ma-
chine, which displayed the lowest amount of testing error, and
the worst performing method was linear regression. For the
support vector machine we achieved the best results with the
polynomial kernel, which achieved the best balance between
overﬁtting and underﬁtting. As expected, linear regression did
not perform very well due to the non-linearity of the data.
Unfortunately, the frequency domain linear regression model
failed to work as well as we had hoped, due to the fact that the
factors driving the sales were not really periodic in nature, but
rather due to company-driven promotions.

There are numerous areas for future work. We may include
additional features in relation to some of the boolean vari-
ables in order to improve our models. For example, a more
careful treatment of the promotional data would have possibly
improved the prediction power of our algorithms, since the
presence of Promo1 seems to be more closely related to the
ﬁrst derivative of sales.

We could have also implemented the O(k2) approximation
scheme for the Gaussian kernel, so that our model can scale
to the full 700,000 data set.

Additionally, it would be good to investigate the anomaly ob-
served. A more thorough investigation of the tradeoffs be-
tween using a polynomial kernel and a Gaussian kernel would
possibly have allowed us to optimize the accuracy of our
model.

In the future, we also hope to explore the usage of neural net-
works in time series prediction, since they are also a widely
used method for time-series prediction. Given the amount of
data we have, this can be very promising. It will also be inter-
esting to compare their performance with the other methods at
hand.

Acknowledgments

We would like to thank our mentor Bryan McCann for his
helpful input on this project.

References

Chapelle, Olivier. ”Training a support vector machine in the
primal.” Neural Computation 19.5 (2007): 1155-1178.

Chen, Bo-Juen, Ming-Wei Chang, and Chih-Jen Lin. ”Load
forecasting using support vector machines: A study on EU-
NITE competition 2001.” Power Systems, IEEE Transactions
on 19.4 (2004): 1821-1830.

Claesen, Marc, et al. ”Fast prediction with SVM models con-
taining RBF kernels.” arXiv preprint arXiv:1403.0736 (2014).

