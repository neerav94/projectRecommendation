Predicting Kidney Cancer Survival from Genomic Data

Christopher Sauer, Rishi Bedi, Duc Nguyen, Benedikt B¨unz

Abstract

Cancers are on par with heart disease as the lead-
ing cause for mortality in the United States. In par-
ticular, Kidney Renal Clear Cell Carcinoma (KIRC)
has an approximate ﬁve-year mortality rate of 53%.
Cancer mortality, however, is highly variable, and the
side eﬀects of current standard of care treatment reg-
imens are severe and costly. Survival indications and
the course of action pursued are largely determined
by the stage and the grade of the cancer, metrics
with varying predictive values. We apply supervised
machine learning techniques to predict mortality us-
ing genetic mutations and gene expression, achiev-
ing maximum predictive accuracy of 97.2%. Addi-
tionally, we explore various feature selection meth-
ods, noting the cost constraints that become relevant
when conducting expression microarrays and SNP
genotyping on increasingly large numbers of genes.
We identify 27 particularly notable genes mutually
identiﬁed by multiple feature selection metrics. Fi-
nally, we consider unsupervised learning techniques
to search for distinguishable genetic subtypes with
signiﬁcantly diﬀering survival outcomes.

1

Introduction

Current survival indications of cancer are largely tied
to discrete measures of disease progression (stage and
grade), metrics with varying predictive value for ac-
tual prognosis. There is value in increasing progno-
sis accuracy, in terms of both patient lifestyle deci-
sions and selection of treatment. Rather than relying
on clinical prognostic indicators, there is signiﬁcant
recent evidence in the literature that superior sur-
vival predictions can be made from applying a sta-
tistical approach to genetic indicators, in numerous
malignancies. We seek to apply similar approaches to
KIRC, relying on supervised machine learning tech-
niques that the univariate analyses that dominate
existing literature [8, 9] . Recent advances, partic-
ularly with regard to breast cancer, have addition-
ally shown promise in the quest to ﬁnd structure in

mutation clusters [7]. We thus apply unsupervised
machine learning techniques here to attempt to dis-
tinguish between survival outcome groups based on
clusters of shared mutations.

2 Data Set

The Cancer Genome Atlas (TCGA) is a National
Cancer Institute-supervised project to make avail-
able genomic and exomic data for various cancer
types. All our samples come from their publicly avail-
able data for the most prevalent type of kidney can-
cer, renal clear cell carcinoma. Many diﬀerent fea-
ture types are available on a subset of the data. We
selected single nucleotide polymorphism (SNP) mu-
tation data (available for 417 samples) and gene ex-
pression data (72 samples) because they likely re-
ﬂect both the source of the cancer and its current
state. All samples had clinical data associated with
them, i.e., the patients were tracked over a period of
time to determine how long they survived. As with
all such data, some patients were alive at the time the
data was submitted or were not able to be contacted,
but this information was recorded in addition to the
number of days we know they survived for. We also
used American Joint Committee on Cancer (AJCC)
stage and grade criteria as baseline clinical predictors
against which to measure the success of our genetic
classiﬁcation model. We therefore attempted to pre-
dict whether patients had survived to the point of
last contact. In the unsupervised part of the study,
we used the standard Kaplan-Meier plot to also take
the duration of survival into account, based on the
survival data as described above.

3 Learning Setup

Creating our preliminary training set required us to
conduct an initial phase of preprocessing. Due to the
sporadic nature of the TCGA dataset, one of the
challenges that we were met with was normalizing
the data in such a way that allowed us to compile
data about the same patient across several diﬀerent

1

Predicting Kidney Cancer Survival from Genomic Data

Christopher Sauer, Rishi Bedi, Duc Nguyen, Benedikt B¨unz

Abstract

Cancers are on par with heart disease as the lead-
ing cause for mortality in the United States. In par-
ticular, Kidney Renal Clear Cell Carcinoma (KIRC)
has an approximate ﬁve-year mortality rate of 53%.
Cancer mortality, however, is highly variable, and the
side eﬀects of current standard of care treatment reg-
imens are severe and costly. Survival indications and
the course of action pursued are largely determined
by the stage and the grade of the cancer, metrics
with varying predictive values. We apply supervised
machine learning techniques to predict mortality us-
ing genetic mutations and gene expression, achiev-
ing maximum predictive accuracy of 97.2%. Addi-
tionally, we explore various feature selection meth-
ods, noting the cost constraints that become relevant
when conducting expression microarrays and SNP
genotyping on increasingly large numbers of genes.
We identify 27 particularly notable genes mutually
identiﬁed by multiple feature selection metrics. Fi-
nally, we consider unsupervised learning techniques
to search for distinguishable genetic subtypes with
signiﬁcantly diﬀering survival outcomes.

1

Introduction

Current survival indications of cancer are largely tied
to discrete measures of disease progression (stage and
grade), metrics with varying predictive value for ac-
tual prognosis. There is value in increasing progno-
sis accuracy, in terms of both patient lifestyle deci-
sions and selection of treatment. Rather than relying
on clinical prognostic indicators, there is signiﬁcant
recent evidence in the literature that superior sur-
vival predictions can be made from applying a sta-
tistical approach to genetic indicators, in numerous
malignancies. We seek to apply similar approaches to
KIRC, relying on supervised machine learning tech-
niques that the univariate analyses that dominate
existing literature [8, 9] . Recent advances, partic-
ularly with regard to breast cancer, have addition-
ally shown promise in the quest to ﬁnd structure in

mutation clusters [7]. We thus apply unsupervised
machine learning techniques here to attempt to dis-
tinguish between survival outcome groups based on
clusters of shared mutations.

2 Data Set

The Cancer Genome Atlas (TCGA) is a National
Cancer Institute-supervised project to make avail-
able genomic and exomic data for various cancer
types. All our samples come from their publicly avail-
able data for the most prevalent type of kidney can-
cer, renal clear cell carcinoma. Many diﬀerent fea-
ture types are available on a subset of the data. We
selected single nucleotide polymorphism (SNP) mu-
tation data (available for 417 samples) and gene ex-
pression data (72 samples) because they likely re-
ﬂect both the source of the cancer and its current
state. All samples had clinical data associated with
them, i.e., the patients were tracked over a period of
time to determine how long they survived. As with
all such data, some patients were alive at the time the
data was submitted or were not able to be contacted,
but this information was recorded in addition to the
number of days we know they survived for. We also
used American Joint Committee on Cancer (AJCC)
stage and grade criteria as baseline clinical predictors
against which to measure the success of our genetic
classiﬁcation model. We therefore attempted to pre-
dict whether patients had survived to the point of
last contact. In the unsupervised part of the study,
we used the standard Kaplan-Meier plot to also take
the duration of survival into account, based on the
survival data as described above.

3 Learning Setup

Creating our preliminary training set required us to
conduct an initial phase of preprocessing. Due to the
sporadic nature of the TCGA dataset, one of the
challenges that we were met with was normalizing
the data in such a way that allowed us to compile
data about the same patient across several diﬀerent

1

sources. Furthermore, not all the patients had the
same depth of information linked to them. For ex-
ample, the NIH provides Somatic Mutation Data for
417 patients, but Gene Expression data for only 72
patients, overlapping but non-identical subsets.

4 Experimental Results

In this paper, we use the following deﬁnition for our
accuracy metric: TRUE POSITIVE + TRUE NEG-
ATIVE / TOTAL. Conversely, we deﬁne error as
FALSE POSITIVE + FALSE NEGATIVE / TO-
TAL.

4.1 Establishing Baseline Models

In the absence of any feature selection metrics, we
attempted to predict boolean survival using the so-
matic mutation and gene expression datasets sepa-
rately. As shown below, our best result was 81.8%
accuracy, using a Neural Network on gene expression
data. This is substantially below comparable pub-
lished results in the literature, indicating severe over-
ﬁtting.

Algorithm

Somatic Mutations Gene Expression

Na¨ıve Bayes
Decision Trees
SVM
Neural Net

49%
75.0%
74.6%
65.0%

77.78%
69.3%
80.56%
81.8%

Table 1: Accuracy of four supervised learning algo-
rithms on the entire datasets.

4.2 Preliminary Feature Selection

Although the eﬃciency of Na¨ıve Bayes classiﬁcation
and Support Vector Machines allows us to learn on
all of the features in each dataset, machine learn-
ing using other algorithms (e.g. logistic regression)
proved infeasible on our extremely large feature set.
Though one potential avenue of addressing this prob-
lem was to use established methods of feature selec-
tion, we decided to test our own algorithms in order
to achieve quick improvements from our baseline re-
sults, as well as get a feel for the nature of the data.
Our naive feature selection algorithm was choosing
the top X single features that alone contributed max-
imum value.

4.2.1 Gene Expression Feature Selection

Our preliminary attempt at feature selection on the
Gene Expression Dataset involved running logistic
regression on each feature (gene) individually and
ranking the features by lowest LOOCV error. The
speed of this algorithm comes from the fact that each
regression is only run on a one-dimensional feature
space, and the fact that the algorithm only makes
one pass through the original feature set. A feature
subset would be constructed from the top N num-
ber of features on our ranking. This smaller subset
allowed us to test not only Na¨ıve Bayes classiﬁca-
tion and Support Vector Machines, but also logistic
regression and regularized logistic regression.

Algorithm

Gene Expression

SVM – top 50
SVM – top 100
Na¨ıve Bayes – top 50
Na¨ıve Bayes – top 100
Log. Regression – top 14
Reg. Log. Reg. – top 14

76.39%
81.94%
84.72%
87.50%
79.17%
84.72%

Table 2: Rudimentary feature selection accuracy on
the gene expression dataset.

Though testing error improved as compared to our
baseline results, we noticed that this approach does
not completely address the problem of overﬁtting.
This is apparent in the relationship between training
and testing error as the number of features increases.
Figure 1 below shows both training and testing error
of Na¨ıve Bayes Classiﬁcation as the number of genes
examined is increased.

As training error decreases, testing error increases–
a telling result of overﬁtting. This trend was consis-
tent across Na¨ıve Bayes, SVM, and logistic regres-
sion. Though overﬁtting was mitigated using regular-
ized logistic regression, it is evident that this prelimi-
nary feature selection strategy is choosing extraneous
features leading to overﬁtting. Another problem for
the logistic regression was that our feature matrix
is very sparse. This leads to singularity issues when
running gradient descent. One approach to resolve
this issue is to reduce the dimension by ﬁnding la-
tent variables using factor analysis, or projecting our
entire training dataset onto a lower-dimensional sub-
space using PCA. However, one of our main goals was

2

Predicting Kidney Cancer Survival from Genomic Data

Christopher Sauer, Rishi Bedi, Duc Nguyen, Benedikt B¨unz

Abstract

Cancers are on par with heart disease as the lead-
ing cause for mortality in the United States. In par-
ticular, Kidney Renal Clear Cell Carcinoma (KIRC)
has an approximate ﬁve-year mortality rate of 53%.
Cancer mortality, however, is highly variable, and the
side eﬀects of current standard of care treatment reg-
imens are severe and costly. Survival indications and
the course of action pursued are largely determined
by the stage and the grade of the cancer, metrics
with varying predictive values. We apply supervised
machine learning techniques to predict mortality us-
ing genetic mutations and gene expression, achiev-
ing maximum predictive accuracy of 97.2%. Addi-
tionally, we explore various feature selection meth-
ods, noting the cost constraints that become relevant
when conducting expression microarrays and SNP
genotyping on increasingly large numbers of genes.
We identify 27 particularly notable genes mutually
identiﬁed by multiple feature selection metrics. Fi-
nally, we consider unsupervised learning techniques
to search for distinguishable genetic subtypes with
signiﬁcantly diﬀering survival outcomes.

1

Introduction

Current survival indications of cancer are largely tied
to discrete measures of disease progression (stage and
grade), metrics with varying predictive value for ac-
tual prognosis. There is value in increasing progno-
sis accuracy, in terms of both patient lifestyle deci-
sions and selection of treatment. Rather than relying
on clinical prognostic indicators, there is signiﬁcant
recent evidence in the literature that superior sur-
vival predictions can be made from applying a sta-
tistical approach to genetic indicators, in numerous
malignancies. We seek to apply similar approaches to
KIRC, relying on supervised machine learning tech-
niques that the univariate analyses that dominate
existing literature [8, 9] . Recent advances, partic-
ularly with regard to breast cancer, have addition-
ally shown promise in the quest to ﬁnd structure in

mutation clusters [7]. We thus apply unsupervised
machine learning techniques here to attempt to dis-
tinguish between survival outcome groups based on
clusters of shared mutations.

2 Data Set

The Cancer Genome Atlas (TCGA) is a National
Cancer Institute-supervised project to make avail-
able genomic and exomic data for various cancer
types. All our samples come from their publicly avail-
able data for the most prevalent type of kidney can-
cer, renal clear cell carcinoma. Many diﬀerent fea-
ture types are available on a subset of the data. We
selected single nucleotide polymorphism (SNP) mu-
tation data (available for 417 samples) and gene ex-
pression data (72 samples) because they likely re-
ﬂect both the source of the cancer and its current
state. All samples had clinical data associated with
them, i.e., the patients were tracked over a period of
time to determine how long they survived. As with
all such data, some patients were alive at the time the
data was submitted or were not able to be contacted,
but this information was recorded in addition to the
number of days we know they survived for. We also
used American Joint Committee on Cancer (AJCC)
stage and grade criteria as baseline clinical predictors
against which to measure the success of our genetic
classiﬁcation model. We therefore attempted to pre-
dict whether patients had survived to the point of
last contact. In the unsupervised part of the study,
we used the standard Kaplan-Meier plot to also take
the duration of survival into account, based on the
survival data as described above.

3 Learning Setup

Creating our preliminary training set required us to
conduct an initial phase of preprocessing. Due to the
sporadic nature of the TCGA dataset, one of the
challenges that we were met with was normalizing
the data in such a way that allowed us to compile
data about the same patient across several diﬀerent

1

sources. Furthermore, not all the patients had the
same depth of information linked to them. For ex-
ample, the NIH provides Somatic Mutation Data for
417 patients, but Gene Expression data for only 72
patients, overlapping but non-identical subsets.

4 Experimental Results

In this paper, we use the following deﬁnition for our
accuracy metric: TRUE POSITIVE + TRUE NEG-
ATIVE / TOTAL. Conversely, we deﬁne error as
FALSE POSITIVE + FALSE NEGATIVE / TO-
TAL.

4.1 Establishing Baseline Models

In the absence of any feature selection metrics, we
attempted to predict boolean survival using the so-
matic mutation and gene expression datasets sepa-
rately. As shown below, our best result was 81.8%
accuracy, using a Neural Network on gene expression
data. This is substantially below comparable pub-
lished results in the literature, indicating severe over-
ﬁtting.

Algorithm

Somatic Mutations Gene Expression

Na¨ıve Bayes
Decision Trees
SVM
Neural Net

49%
75.0%
74.6%
65.0%

77.78%
69.3%
80.56%
81.8%

Table 1: Accuracy of four supervised learning algo-
rithms on the entire datasets.

4.2 Preliminary Feature Selection

Although the eﬃciency of Na¨ıve Bayes classiﬁcation
and Support Vector Machines allows us to learn on
all of the features in each dataset, machine learn-
ing using other algorithms (e.g. logistic regression)
proved infeasible on our extremely large feature set.
Though one potential avenue of addressing this prob-
lem was to use established methods of feature selec-
tion, we decided to test our own algorithms in order
to achieve quick improvements from our baseline re-
sults, as well as get a feel for the nature of the data.
Our naive feature selection algorithm was choosing
the top X single features that alone contributed max-
imum value.

4.2.1 Gene Expression Feature Selection

Our preliminary attempt at feature selection on the
Gene Expression Dataset involved running logistic
regression on each feature (gene) individually and
ranking the features by lowest LOOCV error. The
speed of this algorithm comes from the fact that each
regression is only run on a one-dimensional feature
space, and the fact that the algorithm only makes
one pass through the original feature set. A feature
subset would be constructed from the top N num-
ber of features on our ranking. This smaller subset
allowed us to test not only Na¨ıve Bayes classiﬁca-
tion and Support Vector Machines, but also logistic
regression and regularized logistic regression.

Algorithm

Gene Expression

SVM – top 50
SVM – top 100
Na¨ıve Bayes – top 50
Na¨ıve Bayes – top 100
Log. Regression – top 14
Reg. Log. Reg. – top 14

76.39%
81.94%
84.72%
87.50%
79.17%
84.72%

Table 2: Rudimentary feature selection accuracy on
the gene expression dataset.

Though testing error improved as compared to our
baseline results, we noticed that this approach does
not completely address the problem of overﬁtting.
This is apparent in the relationship between training
and testing error as the number of features increases.
Figure 1 below shows both training and testing error
of Na¨ıve Bayes Classiﬁcation as the number of genes
examined is increased.

As training error decreases, testing error increases–
a telling result of overﬁtting. This trend was consis-
tent across Na¨ıve Bayes, SVM, and logistic regres-
sion. Though overﬁtting was mitigated using regular-
ized logistic regression, it is evident that this prelimi-
nary feature selection strategy is choosing extraneous
features leading to overﬁtting. Another problem for
the logistic regression was that our feature matrix
is very sparse. This leads to singularity issues when
running gradient descent. One approach to resolve
this issue is to reduce the dimension by ﬁnding la-
tent variables using factor analysis, or projecting our
entire training dataset onto a lower-dimensional sub-
space using PCA. However, one of our main goals was

2

results achieved through our preliminary feature se-
lection, both training error and testing error de-
creased as the number of elements in our feature
set increased. On the gene expression dataset, Na¨ıve
Bayes forward search selected 5 out of the 16383
genes in the set, and achieved a 97.22% leave-one-out
cross validated hit rate. SVM forward search selected
a grand total of 3 genes, achieving a 93.1% hit rate.

4.3.2 Forward Search on Somatic Mutations

Seeing the success of forward search on the gene ex-
pression we decided to run the same analysis for the
somatic mutations data set. Again we could see that
using the feature selection we were able to drastically
decrease both the training and the testing error. For
Na¨ıve Bayes the feature section converged after 39
iterations and had a LOOCV accuracy of 95.7%. Fig-
ure 2 plots the training and testing error during the
forward selection. Additionally we ran the same for-
ward feature selection using SVMs. Observing only
the top 41 genes, the SVM’s accuracy was 92.3%.
One interesting result was that the genes selected by
both the Na¨ıve Bayes and the SVM feature selection
were very similar. Concretely over 65% of the genes
selected by the Na¨ıve Bayes feaature selection were
also seleced by the SVM’s feature selection. Further-
more the top 13 genes were not only the same but
even in the same order. We further validate the se-
lected features using mutual information analysis in
section 4.3.3

These top 13 genes ordered by their selection were:
ITGB1, MAP3K2, SPTBN2, RABEP1, C12orf64,
SLC12A1, PIPSL, COL17A1, CMA1, OR5P2,
CLCN3, FBXL19.

These results are valuable from a clinical stand-
point, considering the signiﬁcant real cost diﬀerential
in collecting gene expression data for 5 genes or so-
matic mutation data for 40 genes than it is for over
10,000 genes.

Having a better understanding of which genes to
examine could improve practicality of making predic-
tions on prognosis based on gene expression data.

Furthermore, there has been a growing ﬁeld of re-
search on how to personalize medicine involving the
genomic signature of a malignancy[1] .

Figure 1: Na¨ıve Bayes Training vs Testing error

to reduce the number of features that need to be mea-
sured as there is a physical cost associated with them.
Furthermore, although PCA reduces the amount of
features examined and increases eﬃciency, the fea-
tures that it produces are not selected by predicting
power– they are simply lower-dimensional represen-
tations of the original data. We thus proceeded using
a more robust feature selection on the original feature
set.

4.3 Robust Feature Selection

Despite the immediate increase in hit rate achieved
through our rapid feature selection approaches, the
apparent overﬁtting led us to pursue more intensive
feature selection. We decided on the forward search
algorithm in order to choose features that strictly
improved testing error. Our forward search termi-
nates once it fails to strictly improve the testing error
for 5 consecutive iterations. Each round of the for-
ward search requires evaluating O(n) diﬀerent mod-
els. Given our large feature space had to make some
design decisions to get a tractable feature selection.
Firstly we only the Na¨ıve Bayes and SVM algorithm
were fast enough to be considered. Furthermore, run-
ning an O(m) evaluation algorithm, such as LOOCV
on each of these models wasn’t tractable so we de-
cided to evaluate the models using 10-fold cross val-
idation.

4.3.1 Forward Search on Gene Expression

Dataset

4.3.3 Mutual Information Heuristics

Running forward search on the gene expression
dataset proved extremely fruitful. In contrast to the

Given the large time complexity of forward feature
selection, we examine the eﬀectiveness of heuristics

3

Predicting Kidney Cancer Survival from Genomic Data

Christopher Sauer, Rishi Bedi, Duc Nguyen, Benedikt B¨unz

Abstract

Cancers are on par with heart disease as the lead-
ing cause for mortality in the United States. In par-
ticular, Kidney Renal Clear Cell Carcinoma (KIRC)
has an approximate ﬁve-year mortality rate of 53%.
Cancer mortality, however, is highly variable, and the
side eﬀects of current standard of care treatment reg-
imens are severe and costly. Survival indications and
the course of action pursued are largely determined
by the stage and the grade of the cancer, metrics
with varying predictive values. We apply supervised
machine learning techniques to predict mortality us-
ing genetic mutations and gene expression, achiev-
ing maximum predictive accuracy of 97.2%. Addi-
tionally, we explore various feature selection meth-
ods, noting the cost constraints that become relevant
when conducting expression microarrays and SNP
genotyping on increasingly large numbers of genes.
We identify 27 particularly notable genes mutually
identiﬁed by multiple feature selection metrics. Fi-
nally, we consider unsupervised learning techniques
to search for distinguishable genetic subtypes with
signiﬁcantly diﬀering survival outcomes.

1

Introduction

Current survival indications of cancer are largely tied
to discrete measures of disease progression (stage and
grade), metrics with varying predictive value for ac-
tual prognosis. There is value in increasing progno-
sis accuracy, in terms of both patient lifestyle deci-
sions and selection of treatment. Rather than relying
on clinical prognostic indicators, there is signiﬁcant
recent evidence in the literature that superior sur-
vival predictions can be made from applying a sta-
tistical approach to genetic indicators, in numerous
malignancies. We seek to apply similar approaches to
KIRC, relying on supervised machine learning tech-
niques that the univariate analyses that dominate
existing literature [8, 9] . Recent advances, partic-
ularly with regard to breast cancer, have addition-
ally shown promise in the quest to ﬁnd structure in

mutation clusters [7]. We thus apply unsupervised
machine learning techniques here to attempt to dis-
tinguish between survival outcome groups based on
clusters of shared mutations.

2 Data Set

The Cancer Genome Atlas (TCGA) is a National
Cancer Institute-supervised project to make avail-
able genomic and exomic data for various cancer
types. All our samples come from their publicly avail-
able data for the most prevalent type of kidney can-
cer, renal clear cell carcinoma. Many diﬀerent fea-
ture types are available on a subset of the data. We
selected single nucleotide polymorphism (SNP) mu-
tation data (available for 417 samples) and gene ex-
pression data (72 samples) because they likely re-
ﬂect both the source of the cancer and its current
state. All samples had clinical data associated with
them, i.e., the patients were tracked over a period of
time to determine how long they survived. As with
all such data, some patients were alive at the time the
data was submitted or were not able to be contacted,
but this information was recorded in addition to the
number of days we know they survived for. We also
used American Joint Committee on Cancer (AJCC)
stage and grade criteria as baseline clinical predictors
against which to measure the success of our genetic
classiﬁcation model. We therefore attempted to pre-
dict whether patients had survived to the point of
last contact. In the unsupervised part of the study,
we used the standard Kaplan-Meier plot to also take
the duration of survival into account, based on the
survival data as described above.

3 Learning Setup

Creating our preliminary training set required us to
conduct an initial phase of preprocessing. Due to the
sporadic nature of the TCGA dataset, one of the
challenges that we were met with was normalizing
the data in such a way that allowed us to compile
data about the same patient across several diﬀerent

1

sources. Furthermore, not all the patients had the
same depth of information linked to them. For ex-
ample, the NIH provides Somatic Mutation Data for
417 patients, but Gene Expression data for only 72
patients, overlapping but non-identical subsets.

4 Experimental Results

In this paper, we use the following deﬁnition for our
accuracy metric: TRUE POSITIVE + TRUE NEG-
ATIVE / TOTAL. Conversely, we deﬁne error as
FALSE POSITIVE + FALSE NEGATIVE / TO-
TAL.

4.1 Establishing Baseline Models

In the absence of any feature selection metrics, we
attempted to predict boolean survival using the so-
matic mutation and gene expression datasets sepa-
rately. As shown below, our best result was 81.8%
accuracy, using a Neural Network on gene expression
data. This is substantially below comparable pub-
lished results in the literature, indicating severe over-
ﬁtting.

Algorithm

Somatic Mutations Gene Expression

Na¨ıve Bayes
Decision Trees
SVM
Neural Net

49%
75.0%
74.6%
65.0%

77.78%
69.3%
80.56%
81.8%

Table 1: Accuracy of four supervised learning algo-
rithms on the entire datasets.

4.2 Preliminary Feature Selection

Although the eﬃciency of Na¨ıve Bayes classiﬁcation
and Support Vector Machines allows us to learn on
all of the features in each dataset, machine learn-
ing using other algorithms (e.g. logistic regression)
proved infeasible on our extremely large feature set.
Though one potential avenue of addressing this prob-
lem was to use established methods of feature selec-
tion, we decided to test our own algorithms in order
to achieve quick improvements from our baseline re-
sults, as well as get a feel for the nature of the data.
Our naive feature selection algorithm was choosing
the top X single features that alone contributed max-
imum value.

4.2.1 Gene Expression Feature Selection

Our preliminary attempt at feature selection on the
Gene Expression Dataset involved running logistic
regression on each feature (gene) individually and
ranking the features by lowest LOOCV error. The
speed of this algorithm comes from the fact that each
regression is only run on a one-dimensional feature
space, and the fact that the algorithm only makes
one pass through the original feature set. A feature
subset would be constructed from the top N num-
ber of features on our ranking. This smaller subset
allowed us to test not only Na¨ıve Bayes classiﬁca-
tion and Support Vector Machines, but also logistic
regression and regularized logistic regression.

Algorithm

Gene Expression

SVM – top 50
SVM – top 100
Na¨ıve Bayes – top 50
Na¨ıve Bayes – top 100
Log. Regression – top 14
Reg. Log. Reg. – top 14

76.39%
81.94%
84.72%
87.50%
79.17%
84.72%

Table 2: Rudimentary feature selection accuracy on
the gene expression dataset.

Though testing error improved as compared to our
baseline results, we noticed that this approach does
not completely address the problem of overﬁtting.
This is apparent in the relationship between training
and testing error as the number of features increases.
Figure 1 below shows both training and testing error
of Na¨ıve Bayes Classiﬁcation as the number of genes
examined is increased.

As training error decreases, testing error increases–
a telling result of overﬁtting. This trend was consis-
tent across Na¨ıve Bayes, SVM, and logistic regres-
sion. Though overﬁtting was mitigated using regular-
ized logistic regression, it is evident that this prelimi-
nary feature selection strategy is choosing extraneous
features leading to overﬁtting. Another problem for
the logistic regression was that our feature matrix
is very sparse. This leads to singularity issues when
running gradient descent. One approach to resolve
this issue is to reduce the dimension by ﬁnding la-
tent variables using factor analysis, or projecting our
entire training dataset onto a lower-dimensional sub-
space using PCA. However, one of our main goals was

2

results achieved through our preliminary feature se-
lection, both training error and testing error de-
creased as the number of elements in our feature
set increased. On the gene expression dataset, Na¨ıve
Bayes forward search selected 5 out of the 16383
genes in the set, and achieved a 97.22% leave-one-out
cross validated hit rate. SVM forward search selected
a grand total of 3 genes, achieving a 93.1% hit rate.

4.3.2 Forward Search on Somatic Mutations

Seeing the success of forward search on the gene ex-
pression we decided to run the same analysis for the
somatic mutations data set. Again we could see that
using the feature selection we were able to drastically
decrease both the training and the testing error. For
Na¨ıve Bayes the feature section converged after 39
iterations and had a LOOCV accuracy of 95.7%. Fig-
ure 2 plots the training and testing error during the
forward selection. Additionally we ran the same for-
ward feature selection using SVMs. Observing only
the top 41 genes, the SVM’s accuracy was 92.3%.
One interesting result was that the genes selected by
both the Na¨ıve Bayes and the SVM feature selection
were very similar. Concretely over 65% of the genes
selected by the Na¨ıve Bayes feaature selection were
also seleced by the SVM’s feature selection. Further-
more the top 13 genes were not only the same but
even in the same order. We further validate the se-
lected features using mutual information analysis in
section 4.3.3

These top 13 genes ordered by their selection were:
ITGB1, MAP3K2, SPTBN2, RABEP1, C12orf64,
SLC12A1, PIPSL, COL17A1, CMA1, OR5P2,
CLCN3, FBXL19.

These results are valuable from a clinical stand-
point, considering the signiﬁcant real cost diﬀerential
in collecting gene expression data for 5 genes or so-
matic mutation data for 40 genes than it is for over
10,000 genes.

Having a better understanding of which genes to
examine could improve practicality of making predic-
tions on prognosis based on gene expression data.

Furthermore, there has been a growing ﬁeld of re-
search on how to personalize medicine involving the
genomic signature of a malignancy[1] .

Figure 1: Na¨ıve Bayes Training vs Testing error

to reduce the number of features that need to be mea-
sured as there is a physical cost associated with them.
Furthermore, although PCA reduces the amount of
features examined and increases eﬃciency, the fea-
tures that it produces are not selected by predicting
power– they are simply lower-dimensional represen-
tations of the original data. We thus proceeded using
a more robust feature selection on the original feature
set.

4.3 Robust Feature Selection

Despite the immediate increase in hit rate achieved
through our rapid feature selection approaches, the
apparent overﬁtting led us to pursue more intensive
feature selection. We decided on the forward search
algorithm in order to choose features that strictly
improved testing error. Our forward search termi-
nates once it fails to strictly improve the testing error
for 5 consecutive iterations. Each round of the for-
ward search requires evaluating O(n) diﬀerent mod-
els. Given our large feature space had to make some
design decisions to get a tractable feature selection.
Firstly we only the Na¨ıve Bayes and SVM algorithm
were fast enough to be considered. Furthermore, run-
ning an O(m) evaluation algorithm, such as LOOCV
on each of these models wasn’t tractable so we de-
cided to evaluate the models using 10-fold cross val-
idation.

4.3.1 Forward Search on Gene Expression

Dataset

4.3.3 Mutual Information Heuristics

Running forward search on the gene expression
dataset proved extremely fruitful. In contrast to the

Given the large time complexity of forward feature
selection, we examine the eﬀectiveness of heuristics

3

Algorithm

Somatic Mutations Gene Expression

Na¨ıve Bayes

clinical only

SVM

clinical only

95.7%
77.6%
92.3%
76.7%

+18.1%

+15.6%

97.2%
77.6%
93.1%
76.7%

+19.6%

+16.4%

Table 3: Forward search feature selection accuracy re-
sults, with the red number denoting the added accu-
racy from including genetic data versus clinical alone.

Figure 4: k-means clustering with k={2,3}

to replicate similar results, to within a certain gene
depth, past which its selections begin to diﬀer with
forward search’s.

4.4 Unsupervised Clustering

Having achieved some success in directly predicting
clinical outcomes, we decided to also explore whether
kidney cancer might have several distinguishable sub-
types with diﬀering survival outcomes. This would
support the longstanding theory that even within
each anatomical region, there are several discrete
paths of mutations that can lead to cancer. Each path
ought to lead to a diﬀerent subtype of the cancer that
would be distinguishable from other subtypes from
diﬀerences in both gene expression and mutations.

We ﬁrst ran k-means clustering on purely the gene
expression data with 2, 3, and 4 centroids. Since there
are only 72 samples with gene expression informa-
tion, we are restricted to relatively few centroids to
maintain a meaningful number of samples in each
group.

After clustering the samples into several groups
based solely on gene expression, we compared the
survival outcomes of the groups characterized by dif-
ferences in gene expression. The Kaplan-Meier plot
showing the survival diﬀerences for each of the three
sets of groupings is shown below:

The signiﬁcance of the diﬀerence in survival was
determined using the multivariate logrank test. Split-
ting the data into 3 groups yielded a highly signif-

Figure 2: Na¨ıve Bayes Mutations

Figure 3: Confusion matrices for Na¨ıve Bayes and
SVM with forward search feature selection

that use the mutual information metric to deter-
mine important features much more quickly, using
the well-studied mRMR metric.

S denotes the feature-set, i, j are any two features
in S, and h denotes the output variable. We seek
to minimize expression (2), the ”redundancy” be-
tween two features (i.e, their mutual information),
while maximizing included features’ ”relevance,” ex-
pression (3), given by a feature’s mutual informa-
tion with the output variable. Expression (1) gives
the deﬁnition of mutual information, speciﬁcally for
binary variables (i.e., taking on values of 0 and 1,
exclusively).

I(i, j) =

p(i, j) log

p(i, j)
p(i)p(j)

(1)

(2)

(3)

(cid:88)

(cid:88)

i∈{0,1}

j∈{0,1}
1
|S|2
1
|S|

(cid:88)
(cid:88)

i,j∈S

i∈S

min WI , WI =

I(i, j)

max VI , VI =

I(h, i)

Interestingly, 11 of the top 13 genes selected by
SVM and Na¨ıve Bayes feature selection were also cho-
sen by the mRMR heuristic. This indicates that for-
ward selection is making intelligent choices by max-
imizing relevance while minimizing redundancy, but
also that mRMR is a computationally eﬃcient way

4

Predicting Kidney Cancer Survival from Genomic Data

Christopher Sauer, Rishi Bedi, Duc Nguyen, Benedikt B¨unz

Abstract

Cancers are on par with heart disease as the lead-
ing cause for mortality in the United States. In par-
ticular, Kidney Renal Clear Cell Carcinoma (KIRC)
has an approximate ﬁve-year mortality rate of 53%.
Cancer mortality, however, is highly variable, and the
side eﬀects of current standard of care treatment reg-
imens are severe and costly. Survival indications and
the course of action pursued are largely determined
by the stage and the grade of the cancer, metrics
with varying predictive values. We apply supervised
machine learning techniques to predict mortality us-
ing genetic mutations and gene expression, achiev-
ing maximum predictive accuracy of 97.2%. Addi-
tionally, we explore various feature selection meth-
ods, noting the cost constraints that become relevant
when conducting expression microarrays and SNP
genotyping on increasingly large numbers of genes.
We identify 27 particularly notable genes mutually
identiﬁed by multiple feature selection metrics. Fi-
nally, we consider unsupervised learning techniques
to search for distinguishable genetic subtypes with
signiﬁcantly diﬀering survival outcomes.

1

Introduction

Current survival indications of cancer are largely tied
to discrete measures of disease progression (stage and
grade), metrics with varying predictive value for ac-
tual prognosis. There is value in increasing progno-
sis accuracy, in terms of both patient lifestyle deci-
sions and selection of treatment. Rather than relying
on clinical prognostic indicators, there is signiﬁcant
recent evidence in the literature that superior sur-
vival predictions can be made from applying a sta-
tistical approach to genetic indicators, in numerous
malignancies. We seek to apply similar approaches to
KIRC, relying on supervised machine learning tech-
niques that the univariate analyses that dominate
existing literature [8, 9] . Recent advances, partic-
ularly with regard to breast cancer, have addition-
ally shown promise in the quest to ﬁnd structure in

mutation clusters [7]. We thus apply unsupervised
machine learning techniques here to attempt to dis-
tinguish between survival outcome groups based on
clusters of shared mutations.

2 Data Set

The Cancer Genome Atlas (TCGA) is a National
Cancer Institute-supervised project to make avail-
able genomic and exomic data for various cancer
types. All our samples come from their publicly avail-
able data for the most prevalent type of kidney can-
cer, renal clear cell carcinoma. Many diﬀerent fea-
ture types are available on a subset of the data. We
selected single nucleotide polymorphism (SNP) mu-
tation data (available for 417 samples) and gene ex-
pression data (72 samples) because they likely re-
ﬂect both the source of the cancer and its current
state. All samples had clinical data associated with
them, i.e., the patients were tracked over a period of
time to determine how long they survived. As with
all such data, some patients were alive at the time the
data was submitted or were not able to be contacted,
but this information was recorded in addition to the
number of days we know they survived for. We also
used American Joint Committee on Cancer (AJCC)
stage and grade criteria as baseline clinical predictors
against which to measure the success of our genetic
classiﬁcation model. We therefore attempted to pre-
dict whether patients had survived to the point of
last contact. In the unsupervised part of the study,
we used the standard Kaplan-Meier plot to also take
the duration of survival into account, based on the
survival data as described above.

3 Learning Setup

Creating our preliminary training set required us to
conduct an initial phase of preprocessing. Due to the
sporadic nature of the TCGA dataset, one of the
challenges that we were met with was normalizing
the data in such a way that allowed us to compile
data about the same patient across several diﬀerent

1

sources. Furthermore, not all the patients had the
same depth of information linked to them. For ex-
ample, the NIH provides Somatic Mutation Data for
417 patients, but Gene Expression data for only 72
patients, overlapping but non-identical subsets.

4 Experimental Results

In this paper, we use the following deﬁnition for our
accuracy metric: TRUE POSITIVE + TRUE NEG-
ATIVE / TOTAL. Conversely, we deﬁne error as
FALSE POSITIVE + FALSE NEGATIVE / TO-
TAL.

4.1 Establishing Baseline Models

In the absence of any feature selection metrics, we
attempted to predict boolean survival using the so-
matic mutation and gene expression datasets sepa-
rately. As shown below, our best result was 81.8%
accuracy, using a Neural Network on gene expression
data. This is substantially below comparable pub-
lished results in the literature, indicating severe over-
ﬁtting.

Algorithm

Somatic Mutations Gene Expression

Na¨ıve Bayes
Decision Trees
SVM
Neural Net

49%
75.0%
74.6%
65.0%

77.78%
69.3%
80.56%
81.8%

Table 1: Accuracy of four supervised learning algo-
rithms on the entire datasets.

4.2 Preliminary Feature Selection

Although the eﬃciency of Na¨ıve Bayes classiﬁcation
and Support Vector Machines allows us to learn on
all of the features in each dataset, machine learn-
ing using other algorithms (e.g. logistic regression)
proved infeasible on our extremely large feature set.
Though one potential avenue of addressing this prob-
lem was to use established methods of feature selec-
tion, we decided to test our own algorithms in order
to achieve quick improvements from our baseline re-
sults, as well as get a feel for the nature of the data.
Our naive feature selection algorithm was choosing
the top X single features that alone contributed max-
imum value.

4.2.1 Gene Expression Feature Selection

Our preliminary attempt at feature selection on the
Gene Expression Dataset involved running logistic
regression on each feature (gene) individually and
ranking the features by lowest LOOCV error. The
speed of this algorithm comes from the fact that each
regression is only run on a one-dimensional feature
space, and the fact that the algorithm only makes
one pass through the original feature set. A feature
subset would be constructed from the top N num-
ber of features on our ranking. This smaller subset
allowed us to test not only Na¨ıve Bayes classiﬁca-
tion and Support Vector Machines, but also logistic
regression and regularized logistic regression.

Algorithm

Gene Expression

SVM – top 50
SVM – top 100
Na¨ıve Bayes – top 50
Na¨ıve Bayes – top 100
Log. Regression – top 14
Reg. Log. Reg. – top 14

76.39%
81.94%
84.72%
87.50%
79.17%
84.72%

Table 2: Rudimentary feature selection accuracy on
the gene expression dataset.

Though testing error improved as compared to our
baseline results, we noticed that this approach does
not completely address the problem of overﬁtting.
This is apparent in the relationship between training
and testing error as the number of features increases.
Figure 1 below shows both training and testing error
of Na¨ıve Bayes Classiﬁcation as the number of genes
examined is increased.

As training error decreases, testing error increases–
a telling result of overﬁtting. This trend was consis-
tent across Na¨ıve Bayes, SVM, and logistic regres-
sion. Though overﬁtting was mitigated using regular-
ized logistic regression, it is evident that this prelimi-
nary feature selection strategy is choosing extraneous
features leading to overﬁtting. Another problem for
the logistic regression was that our feature matrix
is very sparse. This leads to singularity issues when
running gradient descent. One approach to resolve
this issue is to reduce the dimension by ﬁnding la-
tent variables using factor analysis, or projecting our
entire training dataset onto a lower-dimensional sub-
space using PCA. However, one of our main goals was

2

results achieved through our preliminary feature se-
lection, both training error and testing error de-
creased as the number of elements in our feature
set increased. On the gene expression dataset, Na¨ıve
Bayes forward search selected 5 out of the 16383
genes in the set, and achieved a 97.22% leave-one-out
cross validated hit rate. SVM forward search selected
a grand total of 3 genes, achieving a 93.1% hit rate.

4.3.2 Forward Search on Somatic Mutations

Seeing the success of forward search on the gene ex-
pression we decided to run the same analysis for the
somatic mutations data set. Again we could see that
using the feature selection we were able to drastically
decrease both the training and the testing error. For
Na¨ıve Bayes the feature section converged after 39
iterations and had a LOOCV accuracy of 95.7%. Fig-
ure 2 plots the training and testing error during the
forward selection. Additionally we ran the same for-
ward feature selection using SVMs. Observing only
the top 41 genes, the SVM’s accuracy was 92.3%.
One interesting result was that the genes selected by
both the Na¨ıve Bayes and the SVM feature selection
were very similar. Concretely over 65% of the genes
selected by the Na¨ıve Bayes feaature selection were
also seleced by the SVM’s feature selection. Further-
more the top 13 genes were not only the same but
even in the same order. We further validate the se-
lected features using mutual information analysis in
section 4.3.3

These top 13 genes ordered by their selection were:
ITGB1, MAP3K2, SPTBN2, RABEP1, C12orf64,
SLC12A1, PIPSL, COL17A1, CMA1, OR5P2,
CLCN3, FBXL19.

These results are valuable from a clinical stand-
point, considering the signiﬁcant real cost diﬀerential
in collecting gene expression data for 5 genes or so-
matic mutation data for 40 genes than it is for over
10,000 genes.

Having a better understanding of which genes to
examine could improve practicality of making predic-
tions on prognosis based on gene expression data.

Furthermore, there has been a growing ﬁeld of re-
search on how to personalize medicine involving the
genomic signature of a malignancy[1] .

Figure 1: Na¨ıve Bayes Training vs Testing error

to reduce the number of features that need to be mea-
sured as there is a physical cost associated with them.
Furthermore, although PCA reduces the amount of
features examined and increases eﬃciency, the fea-
tures that it produces are not selected by predicting
power– they are simply lower-dimensional represen-
tations of the original data. We thus proceeded using
a more robust feature selection on the original feature
set.

4.3 Robust Feature Selection

Despite the immediate increase in hit rate achieved
through our rapid feature selection approaches, the
apparent overﬁtting led us to pursue more intensive
feature selection. We decided on the forward search
algorithm in order to choose features that strictly
improved testing error. Our forward search termi-
nates once it fails to strictly improve the testing error
for 5 consecutive iterations. Each round of the for-
ward search requires evaluating O(n) diﬀerent mod-
els. Given our large feature space had to make some
design decisions to get a tractable feature selection.
Firstly we only the Na¨ıve Bayes and SVM algorithm
were fast enough to be considered. Furthermore, run-
ning an O(m) evaluation algorithm, such as LOOCV
on each of these models wasn’t tractable so we de-
cided to evaluate the models using 10-fold cross val-
idation.

4.3.1 Forward Search on Gene Expression

Dataset

4.3.3 Mutual Information Heuristics

Running forward search on the gene expression
dataset proved extremely fruitful. In contrast to the

Given the large time complexity of forward feature
selection, we examine the eﬀectiveness of heuristics

3

Algorithm

Somatic Mutations Gene Expression

Na¨ıve Bayes

clinical only

SVM

clinical only

95.7%
77.6%
92.3%
76.7%

+18.1%

+15.6%

97.2%
77.6%
93.1%
76.7%

+19.6%

+16.4%

Table 3: Forward search feature selection accuracy re-
sults, with the red number denoting the added accu-
racy from including genetic data versus clinical alone.

Figure 4: k-means clustering with k={2,3}

to replicate similar results, to within a certain gene
depth, past which its selections begin to diﬀer with
forward search’s.

4.4 Unsupervised Clustering

Having achieved some success in directly predicting
clinical outcomes, we decided to also explore whether
kidney cancer might have several distinguishable sub-
types with diﬀering survival outcomes. This would
support the longstanding theory that even within
each anatomical region, there are several discrete
paths of mutations that can lead to cancer. Each path
ought to lead to a diﬀerent subtype of the cancer that
would be distinguishable from other subtypes from
diﬀerences in both gene expression and mutations.

We ﬁrst ran k-means clustering on purely the gene
expression data with 2, 3, and 4 centroids. Since there
are only 72 samples with gene expression informa-
tion, we are restricted to relatively few centroids to
maintain a meaningful number of samples in each
group.

After clustering the samples into several groups
based solely on gene expression, we compared the
survival outcomes of the groups characterized by dif-
ferences in gene expression. The Kaplan-Meier plot
showing the survival diﬀerences for each of the three
sets of groupings is shown below:

The signiﬁcance of the diﬀerence in survival was
determined using the multivariate logrank test. Split-
ting the data into 3 groups yielded a highly signif-

Figure 2: Na¨ıve Bayes Mutations

Figure 3: Confusion matrices for Na¨ıve Bayes and
SVM with forward search feature selection

that use the mutual information metric to deter-
mine important features much more quickly, using
the well-studied mRMR metric.

S denotes the feature-set, i, j are any two features
in S, and h denotes the output variable. We seek
to minimize expression (2), the ”redundancy” be-
tween two features (i.e, their mutual information),
while maximizing included features’ ”relevance,” ex-
pression (3), given by a feature’s mutual informa-
tion with the output variable. Expression (1) gives
the deﬁnition of mutual information, speciﬁcally for
binary variables (i.e., taking on values of 0 and 1,
exclusively).

I(i, j) =

p(i, j) log

p(i, j)
p(i)p(j)

(1)

(2)

(3)

(cid:88)

(cid:88)

i∈{0,1}

j∈{0,1}
1
|S|2
1
|S|

(cid:88)
(cid:88)

i,j∈S

i∈S

min WI , WI =

I(i, j)

max VI , VI =

I(h, i)

Interestingly, 11 of the top 13 genes selected by
SVM and Na¨ıve Bayes feature selection were also cho-
sen by the mRMR heuristic. This indicates that for-
ward selection is making intelligent choices by max-
imizing relevance while minimizing redundancy, but
also that mRMR is a computationally eﬃcient way

4

However, if we apply the same algorithms on a spe-
ciﬁc subset of the genomic data we can successfully
provide high accuracy predictions. Our experiments
show that such a subset can be several magnitudes
smaller than the original feature set. Concretely, by
using forward feature selection, we have been able to
correctly predict the mortality for over 95% of the
patients using only 40 somatic m ation features in-
stead of the original 12,000.

Moreover this extreme reduction of the feature
space does not only reduce the risk of overﬁtting.
Measuring a feature has a signiﬁcant real world cost
both in terms of time and money.

Finally we additionally demonstrated that there
seem to be inherently diﬀerent subtypes of kidney
cancers. Using unsupervised learning techniques we
were able to separate the patients into three groups.
The diﬀerence in survival probability for each of these
groups was highly signiﬁcant.

In conclusion we have shown that machine learn-
ing techniques can be very successfully applied to
genomic data for cancer patients.

References

[1] Villarroel, Maria C., et al. ”Personalizing cancer treatment
in the age of global genomic analyses: PALB2 gene muta-
tions and the response to DNA damaging agents in pan-
creatic cancer.” Molecular cancer therapeutics 10.1 (2011):
3-8.

[2] Chow W-H, Dong LM, Devesa SS. Epidemiology and
risk factors for kidney cancer. Nature reviews. Urology
2010;7(5):245-257. doi: 10.1038/nrurol.2010.46.

[3] Pirooznia M, Yang JY, Yang MQ, Deng Y. A comparative
study of diﬀerent machine learning methods on microarray
gene expression data. BMC Genomics. 2008;9 Suppl 1:S13.
[4] Loeb LA, Loeb KR, Anderson JP. Multiple mutations and

cancer. Proc Natl Acad Sci USA. 2003;100(3):776-81.

[5] Nowell PC. The clonal evolution of tumor cell populations.

Science. 1976;194(4260):23-28.

[6] The results shown here are in whole or part based
upon data generated by the TCGA Research Network:
http://cancergenome.nih.gov/.

[7] Cancer Genome Atlas Network. ”Comprehensive molecu-
lar portraits of human breast tumours.” Nature 490.7418
(2012): 61-70.

[8] Gross, Andrew M., et al. ”Multi-tiered genomic analysis
of head and neck cancer ties TP53 mutation to 3p loss.”
Nature genetics (2014).

[9] Rossi, Davide, et al. ”Mutations of NOTCH1 are an in-
dependent predictor of survival in chronic lymphocytic
leukemia.” Blood 119.2 (2012): 521-529.

Figure 5: K-means clustering on PCA features and
associated Kaplan-Meier survival plot.

icant diﬀerence in survival with one group surviv-
ing much better then the other two. Some of this is
preserved with 4 centroids. Since, again, the group-
ings were determined only on gene expression, this
suggests that there are at least three subtypes of
KIRC, with one having signiﬁcantly better survival
outcomes than the others.

To better visualize the results, we next tried ﬁrst
performing PCA to reduce the dimensionality of the
data before running k-means and plotting survival
diﬀerences. The plot of the expression data reduced
to two dimensions and the corresponding Kaplan-
Meier plot are below. Group numbers are preserved
between the two. The diﬀerences between groups
only became more signiﬁcant when PCA was per-
formed ﬁrst.

We attempted to perform similar a similar set of
analysis on mutations, but k-means and other clus-
tering algorithms performed poorly on Boolean data.
This is because the distances between samples are
discrete and the same, so the algorithms tend to pro-
duce a single monolithic group with all other groups
having size one. Normalizing and performing PCA to
reduce the dimensionality of the data failed to cor-
rect for this problem, despite the fact that this made
the features closer to being real-valued.

5 Conclusion

In this paper we have applied several diﬀerent ma-
chine learning techniques to genomic data from kid-
ney cancer patients. Concretely we attempted to pre-
dict the mortality of cancer patients based on the ex-
pressions and somatic mutations of their genes. The
genomic data is characterized by an extremely high
dimensional feature space and a relatively small num-
ber of samples. We have shown that in such a setting
a plain application of these state of the art algorithms
does not result in good and generalizable predictions
of the mortality of such patients.

5

