Predicting Hit Songs with MIDI Musical Features

Keven (Kedao) Wang

Stanford University

kvw@stanford.edu

ABSTRACT
This paper predicts hit songs based on musical features from
MIDI ﬁles. The task is modeled as a binary classiﬁcation
problem optimizing for precision, with Billboard ranking as
labels. Million Song Dataset (MSD) is inspected audibly,
visually, and with a logistic regression model. MSD features
is determined too noisy for the task. MIDI ﬁles encodes
pitch duration as separate instrument tracks, and is chosen
over MSD. Fine-grained instrument, melody, and beats fea-
tures are extracted. Language models of n-grams are used
to transform raw musical features into word-document fre-
quency matrices. Logistic Regression is chosen as the clas-
siﬁer, with increased probability cutoﬀ to optimize for pre-
cision. An ensemble method that uses both instruments/
melody as well as beats features produces the peak precision
0.882 at probability cutoﬀ 0.998 (recall is 0.279). Alterna-
tive models and applications are discussed.

Keywords
Music, Hit Song, Classiﬁcation, MIDI

1.

INTRODUCTION

The goal of this project is to predict hit songs based on
musical features. A song is deﬁned as a hit if it has ever
reached top 10 position on a Billboard weekly ranking. Pre-
dicting hit songs is meaningful in numerous ways:

1. Help music stream services to surface upcoming hits

for better user engagement

2. Improve the iteration process for artists before releas-

ing music to the public

Various factors determine whether a song is popular/a
hit, including the intrinsic quality of the music piece, and
psychological/social factors [Pachet and Sony 2012]. The
latter encompass peer pressure, public opinion, frequency
of listening, and artists’ reputation. These psychological/
behavioral factors are harder to quantify, and is out of scope
for this project. The popularity of a song does correlate to
its intrinsic quality [Pachet and Sony 2012]. Therefore this
paper focuses on the analysis of musical features, which are
quantiﬁable and encapsulated in the audio ﬁle itself. The
following musical features are analyzed.

1. Timbre/instrument
2. Melody
3. Beats
Relatively few projects have explored the hit song predic-
tion space [Pachet and Sony 2012, Ni et al. 2011, Dhanaraj
and Logan 2005, HERREMANS et al. 2014, Fan and Casey

2013, Monterola et al. 2009]. A majority of the research
have taken low level features from audio ﬁle formats such
as .mp3, and .wav by using signal processing techniques to
extract MFCC values spanning short time window [Serr`a
et al. 2012, Pachet and Roy 2009]. This project extract in-
strument, melody and beats features from MIDI ﬁles. Sur-
prisingly good results are obtained in this project.

Since it is more valuable to correctly predict popular songs
than correctly predicting unpopular songs, the precision met-
ric is optimized.

2. DATA

Both MIDI and Million Song Dataset are explored for
feature extraction. MIDI is shown to produce much higher
quality features that results in higher performance, and is
therefore chosen for this project.
2.1 MIDI

Musical Instrument Digital Interface (MIDI) is a technical
standard that allows musical devices to communicate with
each other. A MIDI ﬁle contains up to 16 tracks, each repre-
senting an instrument. Each track contains messages, which
encodes the pitch and duration of an instrument key press.
MIDI ﬁles are close approximations of original music piece.
Although lacking the ﬁdelity and human-ness of raw audio
ﬁles such as .mp3 and .wav, it is nevertheless a faithful rep-
resentation of high level musical features of timbre, melody,
and beats. MIDI suﬃces the purpose of feature extraction
in this paper.

1752 MIDI songs are used as training samples, with ex-
actly 50-50 split between positive and negative training ex-
amples. The deﬁnition of positive and negative labels is
outlined below in ”Labels” section.
2.2 Million Song Dataset

The Million Song Dataset (MSD) is a musical feature
dataset of one million contemporary songs.
It is publicly
available from the joint collaboration between LabROSA
and the Echo Nest [LabROSA 2014]. The dataset is readily
parseable via a Python API with getters and setters to in-
dividual features. Further information could be queried via
the Echo Nest API, which is freely available [the EchoNest
2014].

In pursuing this project, visual and audio examination,
as well as a Logistic Regression model was used to test the
eﬀectiveness of the 10K subset. However, the result was un-
satisfactory. The following introduces inspections and ﬁnd-
ings on why MSD is not ideal for this project.

Predicting Hit Songs with MIDI Musical Features

Keven (Kedao) Wang

Stanford University

kvw@stanford.edu

ABSTRACT
This paper predicts hit songs based on musical features from
MIDI ﬁles. The task is modeled as a binary classiﬁcation
problem optimizing for precision, with Billboard ranking as
labels. Million Song Dataset (MSD) is inspected audibly,
visually, and with a logistic regression model. MSD features
is determined too noisy for the task. MIDI ﬁles encodes
pitch duration as separate instrument tracks, and is chosen
over MSD. Fine-grained instrument, melody, and beats fea-
tures are extracted. Language models of n-grams are used
to transform raw musical features into word-document fre-
quency matrices. Logistic Regression is chosen as the clas-
siﬁer, with increased probability cutoﬀ to optimize for pre-
cision. An ensemble method that uses both instruments/
melody as well as beats features produces the peak precision
0.882 at probability cutoﬀ 0.998 (recall is 0.279). Alterna-
tive models and applications are discussed.

Keywords
Music, Hit Song, Classiﬁcation, MIDI

1.

INTRODUCTION

The goal of this project is to predict hit songs based on
musical features. A song is deﬁned as a hit if it has ever
reached top 10 position on a Billboard weekly ranking. Pre-
dicting hit songs is meaningful in numerous ways:

1. Help music stream services to surface upcoming hits

for better user engagement

2. Improve the iteration process for artists before releas-

ing music to the public

Various factors determine whether a song is popular/a
hit, including the intrinsic quality of the music piece, and
psychological/social factors [Pachet and Sony 2012]. The
latter encompass peer pressure, public opinion, frequency
of listening, and artists’ reputation. These psychological/
behavioral factors are harder to quantify, and is out of scope
for this project. The popularity of a song does correlate to
its intrinsic quality [Pachet and Sony 2012]. Therefore this
paper focuses on the analysis of musical features, which are
quantiﬁable and encapsulated in the audio ﬁle itself. The
following musical features are analyzed.

1. Timbre/instrument
2. Melody
3. Beats
Relatively few projects have explored the hit song predic-
tion space [Pachet and Sony 2012, Ni et al. 2011, Dhanaraj
and Logan 2005, HERREMANS et al. 2014, Fan and Casey

2013, Monterola et al. 2009]. A majority of the research
have taken low level features from audio ﬁle formats such
as .mp3, and .wav by using signal processing techniques to
extract MFCC values spanning short time window [Serr`a
et al. 2012, Pachet and Roy 2009]. This project extract in-
strument, melody and beats features from MIDI ﬁles. Sur-
prisingly good results are obtained in this project.

Since it is more valuable to correctly predict popular songs
than correctly predicting unpopular songs, the precision met-
ric is optimized.

2. DATA

Both MIDI and Million Song Dataset are explored for
feature extraction. MIDI is shown to produce much higher
quality features that results in higher performance, and is
therefore chosen for this project.
2.1 MIDI

Musical Instrument Digital Interface (MIDI) is a technical
standard that allows musical devices to communicate with
each other. A MIDI ﬁle contains up to 16 tracks, each repre-
senting an instrument. Each track contains messages, which
encodes the pitch and duration of an instrument key press.
MIDI ﬁles are close approximations of original music piece.
Although lacking the ﬁdelity and human-ness of raw audio
ﬁles such as .mp3 and .wav, it is nevertheless a faithful rep-
resentation of high level musical features of timbre, melody,
and beats. MIDI suﬃces the purpose of feature extraction
in this paper.

1752 MIDI songs are used as training samples, with ex-
actly 50-50 split between positive and negative training ex-
amples. The deﬁnition of positive and negative labels is
outlined below in ”Labels” section.
2.2 Million Song Dataset

The Million Song Dataset (MSD) is a musical feature
dataset of one million contemporary songs.
It is publicly
available from the joint collaboration between LabROSA
and the Echo Nest [LabROSA 2014]. The dataset is readily
parseable via a Python API with getters and setters to in-
dividual features. Further information could be queried via
the Echo Nest API, which is freely available [the EchoNest
2014].

In pursuing this project, visual and audio examination,
as well as a Logistic Regression model was used to test the
eﬀectiveness of the 10K subset. However, the result was un-
satisfactory. The following introduces inspections and ﬁnd-
ings on why MSD is not ideal for this project.

3. PREPROCESSING
3.1 Type 0 to Type 1

Two types of MIDI music ﬁles are of interest. Type 1 is
easier to process for feature extraction, as each track repre-
sents distinct instruments. Therefore all type 0 MIDI ﬁles
are converted into type 1.

1. Type 1: each individual track represents one musical

instrument. (80% of training samples)

2. Type 0: one single track contains messages across all

channels. (20% of training samples)

The Open-Source Python module MIDO provides a friendly

API that parses a MIDI ﬁle into native python data struc-
tures [Bjørndalen and Binkys 2014].
It is used to extract
instruments/melody and beats features from MIDI ﬁles.

4. FEATURE EXTRACTION

Fine-grained features are engineered to capture the subtle
characteristics of a song. Language models of n-grams are
used to capture the building blocks of melody and beats. In
order to extract the features, the following MIDI messages
are of interest:
• Set tempo: speciﬁes tempo of music piece in microsec-
• Note on: speciﬁes the start of a music note event (e.g.

onds per quarter note (beat)

piano keyboard press)

– Note: speciﬁes the pitch, with 60 representing the

middle C

– Velocity: speciﬁes how much force a note is played
with (a note on message with velocity 0 is the
same as a note oﬀ message)

• Note off: speciﬁes the end of a music note event
• Program change: speciﬁes the instrument for track
• Delta time: speciﬁes the number of ticks since last MIDI
event, for each MIDI event. This can be converted to
delta time in seconds.

Besides, the metadata of Pulse per Quarter-Note is needed.
It speciﬁes number of ticks per beat. This is needed to
compute time delta between MIDI messages.
4.1

Instruments

Each MIDI track contains a program change message,
with instrument type encoded with 0 - 127. A manual group-
ing on instrument types is done based on suggestions from
[McKay 2004] and [Association 2014]. The grouping shrinks
the feature space, while capturing the distinguishing timbre
of instrument classes.

Table 1: MIDI instrument grouping by program
change number (0 - 127)

MIDI Program Change Number
0 - 4
5 - 6, 17, 19
7, 8,
9 - 16
18 - 24
25, 26
27 - 32
33 - 40
...
113 - 120

Instrument
Keyboard
Electric
Other
Chromatic percussion
Organ
Acoustic guitar
Electric guitar
Bass

Percussive

Figure 1: Left: Million Song Dataset: Does not dis-
tinguish between tracks. Right: MIDI : Represents
each instrument as separated tracks.

to 1.0 from the Echo Nest representing popularity.

In MSD, the features of interest are:
• Popularity labels: ”Hotttness” score, a score from 0
• instruments/melody features: ”Pitch” matrix of
size 12 by length-of-song. Size 12 represents the 12
semitones in an octave. The value of each cell corre-
lates to the relative strength of particular semitone at
a given time. Sampling is done every 250 milliseconds,
with some variation based on the beats.

The pitch matrices are not eﬀective representation of in-
struments/melody features, as it does not distinguish be-
tween instruments. Percussion can greatly distort the dom-
inant melody of the song [Jiang et al. 2011]. Figure 1 is a
visualization comparison between MSD and MIDI melody
features. In the left ﬁgure, the red color represents a ”loud”
signal and the blue color represents a ”quiet” signal. The
visualization shows a noisy representation of melody. A lis-
tening test was performed on the transformed sine wave,
with frequency determined by the ”loudest” semitone in a
pitch matrix column. The result audio is completely un-
recognizable for multi-instruments/track songs. A baseline
model is implemented, with 10-fold cross validation show-
ing a ﬂuctuation around 50% precision, recall, and f1 score.
This is not an improvement over random baseline.

MIDI ﬁles, on the other hand, encodes each instrument
into separate tracks as in the right ﬁgure. Therefore it is
decided that the MSD dataset is not eﬀective as melodic
features, and is not used for this project.

2.3 Labels

The problem of hit song prediction is modeled as a binary
classiﬁcation problem, with positive labels representing the
popular songs and negative labels representing unpopular
ones. The Billboard ranking is used to determine whether
a song is popular. Billboard is a prominent music popular-
ity ranking based on radio plays, music streaming and sales
published weekly. Billboard contains ranking dating back to
the 1950s. In this paper, the labels are assigned as follows:
• Positive:
if a song ever reached the top 10 position
• Negative: if a song’s artist never had any song reach-

on any Billboard ranking (since 1950)

ing top 100 position on any Billboard ranking

This requirement is rather strong, leaving out a large mid-
class songs in between. This is done to emphasize the dif-
ferences between the two classes.

Predicting Hit Songs with MIDI Musical Features

Keven (Kedao) Wang

Stanford University

kvw@stanford.edu

ABSTRACT
This paper predicts hit songs based on musical features from
MIDI ﬁles. The task is modeled as a binary classiﬁcation
problem optimizing for precision, with Billboard ranking as
labels. Million Song Dataset (MSD) is inspected audibly,
visually, and with a logistic regression model. MSD features
is determined too noisy for the task. MIDI ﬁles encodes
pitch duration as separate instrument tracks, and is chosen
over MSD. Fine-grained instrument, melody, and beats fea-
tures are extracted. Language models of n-grams are used
to transform raw musical features into word-document fre-
quency matrices. Logistic Regression is chosen as the clas-
siﬁer, with increased probability cutoﬀ to optimize for pre-
cision. An ensemble method that uses both instruments/
melody as well as beats features produces the peak precision
0.882 at probability cutoﬀ 0.998 (recall is 0.279). Alterna-
tive models and applications are discussed.

Keywords
Music, Hit Song, Classiﬁcation, MIDI

1.

INTRODUCTION

The goal of this project is to predict hit songs based on
musical features. A song is deﬁned as a hit if it has ever
reached top 10 position on a Billboard weekly ranking. Pre-
dicting hit songs is meaningful in numerous ways:

1. Help music stream services to surface upcoming hits

for better user engagement

2. Improve the iteration process for artists before releas-

ing music to the public

Various factors determine whether a song is popular/a
hit, including the intrinsic quality of the music piece, and
psychological/social factors [Pachet and Sony 2012]. The
latter encompass peer pressure, public opinion, frequency
of listening, and artists’ reputation. These psychological/
behavioral factors are harder to quantify, and is out of scope
for this project. The popularity of a song does correlate to
its intrinsic quality [Pachet and Sony 2012]. Therefore this
paper focuses on the analysis of musical features, which are
quantiﬁable and encapsulated in the audio ﬁle itself. The
following musical features are analyzed.

1. Timbre/instrument
2. Melody
3. Beats
Relatively few projects have explored the hit song predic-
tion space [Pachet and Sony 2012, Ni et al. 2011, Dhanaraj
and Logan 2005, HERREMANS et al. 2014, Fan and Casey

2013, Monterola et al. 2009]. A majority of the research
have taken low level features from audio ﬁle formats such
as .mp3, and .wav by using signal processing techniques to
extract MFCC values spanning short time window [Serr`a
et al. 2012, Pachet and Roy 2009]. This project extract in-
strument, melody and beats features from MIDI ﬁles. Sur-
prisingly good results are obtained in this project.

Since it is more valuable to correctly predict popular songs
than correctly predicting unpopular songs, the precision met-
ric is optimized.

2. DATA

Both MIDI and Million Song Dataset are explored for
feature extraction. MIDI is shown to produce much higher
quality features that results in higher performance, and is
therefore chosen for this project.
2.1 MIDI

Musical Instrument Digital Interface (MIDI) is a technical
standard that allows musical devices to communicate with
each other. A MIDI ﬁle contains up to 16 tracks, each repre-
senting an instrument. Each track contains messages, which
encodes the pitch and duration of an instrument key press.
MIDI ﬁles are close approximations of original music piece.
Although lacking the ﬁdelity and human-ness of raw audio
ﬁles such as .mp3 and .wav, it is nevertheless a faithful rep-
resentation of high level musical features of timbre, melody,
and beats. MIDI suﬃces the purpose of feature extraction
in this paper.

1752 MIDI songs are used as training samples, with ex-
actly 50-50 split between positive and negative training ex-
amples. The deﬁnition of positive and negative labels is
outlined below in ”Labels” section.
2.2 Million Song Dataset

The Million Song Dataset (MSD) is a musical feature
dataset of one million contemporary songs.
It is publicly
available from the joint collaboration between LabROSA
and the Echo Nest [LabROSA 2014]. The dataset is readily
parseable via a Python API with getters and setters to in-
dividual features. Further information could be queried via
the Echo Nest API, which is freely available [the EchoNest
2014].

In pursuing this project, visual and audio examination,
as well as a Logistic Regression model was used to test the
eﬀectiveness of the 10K subset. However, the result was un-
satisfactory. The following introduces inspections and ﬁnd-
ings on why MSD is not ideal for this project.

3. PREPROCESSING
3.1 Type 0 to Type 1

Two types of MIDI music ﬁles are of interest. Type 1 is
easier to process for feature extraction, as each track repre-
sents distinct instruments. Therefore all type 0 MIDI ﬁles
are converted into type 1.

1. Type 1: each individual track represents one musical

instrument. (80% of training samples)

2. Type 0: one single track contains messages across all

channels. (20% of training samples)

The Open-Source Python module MIDO provides a friendly

API that parses a MIDI ﬁle into native python data struc-
tures [Bjørndalen and Binkys 2014].
It is used to extract
instruments/melody and beats features from MIDI ﬁles.

4. FEATURE EXTRACTION

Fine-grained features are engineered to capture the subtle
characteristics of a song. Language models of n-grams are
used to capture the building blocks of melody and beats. In
order to extract the features, the following MIDI messages
are of interest:
• Set tempo: speciﬁes tempo of music piece in microsec-
• Note on: speciﬁes the start of a music note event (e.g.

onds per quarter note (beat)

piano keyboard press)

– Note: speciﬁes the pitch, with 60 representing the

middle C

– Velocity: speciﬁes how much force a note is played
with (a note on message with velocity 0 is the
same as a note oﬀ message)

• Note off: speciﬁes the end of a music note event
• Program change: speciﬁes the instrument for track
• Delta time: speciﬁes the number of ticks since last MIDI
event, for each MIDI event. This can be converted to
delta time in seconds.

Besides, the metadata of Pulse per Quarter-Note is needed.
It speciﬁes number of ticks per beat. This is needed to
compute time delta between MIDI messages.
4.1

Instruments

Each MIDI track contains a program change message,
with instrument type encoded with 0 - 127. A manual group-
ing on instrument types is done based on suggestions from
[McKay 2004] and [Association 2014]. The grouping shrinks
the feature space, while capturing the distinguishing timbre
of instrument classes.

Table 1: MIDI instrument grouping by program
change number (0 - 127)

MIDI Program Change Number
0 - 4
5 - 6, 17, 19
7, 8,
9 - 16
18 - 24
25, 26
27 - 32
33 - 40
...
113 - 120

Instrument
Keyboard
Electric
Other
Chromatic percussion
Organ
Acoustic guitar
Electric guitar
Bass

Percussive

Figure 1: Left: Million Song Dataset: Does not dis-
tinguish between tracks. Right: MIDI : Represents
each instrument as separated tracks.

to 1.0 from the Echo Nest representing popularity.

In MSD, the features of interest are:
• Popularity labels: ”Hotttness” score, a score from 0
• instruments/melody features: ”Pitch” matrix of
size 12 by length-of-song. Size 12 represents the 12
semitones in an octave. The value of each cell corre-
lates to the relative strength of particular semitone at
a given time. Sampling is done every 250 milliseconds,
with some variation based on the beats.

The pitch matrices are not eﬀective representation of in-
struments/melody features, as it does not distinguish be-
tween instruments. Percussion can greatly distort the dom-
inant melody of the song [Jiang et al. 2011]. Figure 1 is a
visualization comparison between MSD and MIDI melody
features. In the left ﬁgure, the red color represents a ”loud”
signal and the blue color represents a ”quiet” signal. The
visualization shows a noisy representation of melody. A lis-
tening test was performed on the transformed sine wave,
with frequency determined by the ”loudest” semitone in a
pitch matrix column. The result audio is completely un-
recognizable for multi-instruments/track songs. A baseline
model is implemented, with 10-fold cross validation show-
ing a ﬂuctuation around 50% precision, recall, and f1 score.
This is not an improvement over random baseline.

MIDI ﬁles, on the other hand, encodes each instrument
into separate tracks as in the right ﬁgure. Therefore it is
decided that the MSD dataset is not eﬀective as melodic
features, and is not used for this project.

2.3 Labels

The problem of hit song prediction is modeled as a binary
classiﬁcation problem, with positive labels representing the
popular songs and negative labels representing unpopular
ones. The Billboard ranking is used to determine whether
a song is popular. Billboard is a prominent music popular-
ity ranking based on radio plays, music streaming and sales
published weekly. Billboard contains ranking dating back to
the 1950s. In this paper, the labels are assigned as follows:
• Positive:
if a song ever reached the top 10 position
• Negative: if a song’s artist never had any song reach-

on any Billboard ranking (since 1950)

ing top 100 position on any Billboard ranking

This requirement is rather strong, leaving out a large mid-
class songs in between. This is done to emphasize the dif-
ferences between the two classes.

4.2 Melody

Melody features are represented by chord progression char-
acteristics. These deﬁning characteristics in music theory is
captured in features below.
• Consecutive two notes (2-grams): captures musical
interval. Musical interval deﬁnes transition between
two consecutive music notes. [Wikipedia 2014].
• Consecutive three notes (3-grams): [Caswell and Ji
2013] suggests that Markov models looking at previous
3 or 4 note pitches produced the best results.

The instruments and melody features are combined to rep-
resent the distinct instruments/melody combination. As an
example, the following MIDI note on messages are trans-
formed into the following features.

MIDI messages:
<track 1>
program_change, value = 0 # instrument: piano
note_on, note = 60, velocity = 64
note_on, note = 62, velocity = 64
note_on, note = 67, velocity = 64
<track 2>
program_change, value = 27 # instrument:
note_on, note = 72, velocity = 64
note_on, note = 76, velocity = 64

electric guitar

Extracted features:
2-grams: keyboard:60:62, keyboard:62:67, electric_guitar:72:76
3-grams: keyboard:60:62:67

4.3 Beats

The beats features are represented by extracting time
delta between consecutive musical notes. The idea of 2-
grams and 3-grams is used here again. In MIDI, time delta
between MIDI messages is speciﬁed in number of ticks. The
following calculation is needed to calculate the delta time (in
seconds) between consecutive MIDI messages:

delta time = delta ticks ·

tempo

1000000.0 · P P QN

(1)

tempo is in microseconds per quarter note.
PPQN (Pulse Per Quarter Note) is in ticks per quarter note.

4.3.1 Note duration
MIDI contains various messages other than note on. Time
delta occurs between each consecutive MIDI messages. Work
is done to accumulate the time delta between consecutive
note on messages. Afterwards, time delta is converted to
beat-per-minute, a standard way of capturing tempo infor-
mation and accounting for small time delta.

4.3.2 Percussion
Track 9 is always the percussion track. Percussion track
is more important as a beats feature than other instrument
tracks. Therefore a preﬁx is added for features extracted
from percussion track to distinguish from other tracks.
4.4 Dimensionality Reduction

Two dimensionality reduction techniques are explored to
avoid overﬁtting. However, due to their limitations and the
eﬀectiveness of regularization term in Logistic Regression,
no dimensionality reduction algorithm is used in this project.

PCA (Principal Component Analysis) transforms feature
space into a lower-dimensional subspace composed of pair-
wise linearly independent vectors.
In practice PCA tends
to favor features with high variance. Feature Selection pri-
oritizes features with highest marginal increase/decrease in
performance. Feature Selection itself is time consuming.

5. MODELS

The problem is modeled as a binary classiﬁcation problem.
This allows for plug-and-play of many oﬀ-the-shelf classiﬁca-
tion algorithms. A more practical model is outlier detection,
since it is much more valuable to predict a popular song than
an unpopular song. However, I was not able to ﬁnd such a
model with satisfying results. The following models are used
in training and testing on the dataset.
5.1 Logistic Regression

Logistic Regression is chosen as the model for task. Lo-
gistic Regression outputs the conﬁdence probability for each
prediction. This is ideal for optimizing for precision, since
the probability cutoﬀ for positive labels can be increased to
form a ”stricter” criteria on popular songs. A regularization
coeﬃcient λ is added and iterated on to decrease overﬁtting.

m(cid:88)

J(θ) =

1
2m

[

n(cid:88)

((hθ(x(i)) − y(i))2 + λ

θ2
j ]

(2)

i=1

j=1

hθ(x) = P (y = 1|x; θ) =

1

1 + e−θT x

(3)

5.2 SVM

SVM is regarded as one of the best ”oﬀ-the-shelf” models.
It has the advantages of being time-eﬃcient and avoiding
overﬁtting. In this project, time-eﬃciency is not a concern
given the relatively small sample size (1700+) to work with.

yi(w · xi − b) ≥ 1 − ζi

(cid:40)

argmin

w,ζ,b

(cid:107)w(cid:107)2 + C

1
2

(cid:41)

ζi

n(cid:88)

i=1

yi(w · x − b) ≥ 1 − ξi, ξi ≥ 0

5.3 Naive Bayes

(4)

(5)

(6)

Naive Bayes assumes each feature is independently dis-
tributed. This assumption is too strong for melody segment
features (n-grams), as they are dependent on neighbors and
the overall chord progression distribution of the song.

(cid:40)

(cid:41)

n(cid:89)

i=1

P (xi|y)

(7)

argmin

P (y = yi)

y

5.4 One-Class SVM

The supervised outlier detection problem is attempted.
One-Class SVM is an one class classiﬁcation algorithm that
takes only positive training examples and adds a negative
example at the origin. Compared to other clustering (K-
means) and outlier detection algorithms (Mixtures of Gaus-
sian), One-Class SVM allows for supervised learning.

Predicting Hit Songs with MIDI Musical Features

Keven (Kedao) Wang

Stanford University

kvw@stanford.edu

ABSTRACT
This paper predicts hit songs based on musical features from
MIDI ﬁles. The task is modeled as a binary classiﬁcation
problem optimizing for precision, with Billboard ranking as
labels. Million Song Dataset (MSD) is inspected audibly,
visually, and with a logistic regression model. MSD features
is determined too noisy for the task. MIDI ﬁles encodes
pitch duration as separate instrument tracks, and is chosen
over MSD. Fine-grained instrument, melody, and beats fea-
tures are extracted. Language models of n-grams are used
to transform raw musical features into word-document fre-
quency matrices. Logistic Regression is chosen as the clas-
siﬁer, with increased probability cutoﬀ to optimize for pre-
cision. An ensemble method that uses both instruments/
melody as well as beats features produces the peak precision
0.882 at probability cutoﬀ 0.998 (recall is 0.279). Alterna-
tive models and applications are discussed.

Keywords
Music, Hit Song, Classiﬁcation, MIDI

1.

INTRODUCTION

The goal of this project is to predict hit songs based on
musical features. A song is deﬁned as a hit if it has ever
reached top 10 position on a Billboard weekly ranking. Pre-
dicting hit songs is meaningful in numerous ways:

1. Help music stream services to surface upcoming hits

for better user engagement

2. Improve the iteration process for artists before releas-

ing music to the public

Various factors determine whether a song is popular/a
hit, including the intrinsic quality of the music piece, and
psychological/social factors [Pachet and Sony 2012]. The
latter encompass peer pressure, public opinion, frequency
of listening, and artists’ reputation. These psychological/
behavioral factors are harder to quantify, and is out of scope
for this project. The popularity of a song does correlate to
its intrinsic quality [Pachet and Sony 2012]. Therefore this
paper focuses on the analysis of musical features, which are
quantiﬁable and encapsulated in the audio ﬁle itself. The
following musical features are analyzed.

1. Timbre/instrument
2. Melody
3. Beats
Relatively few projects have explored the hit song predic-
tion space [Pachet and Sony 2012, Ni et al. 2011, Dhanaraj
and Logan 2005, HERREMANS et al. 2014, Fan and Casey

2013, Monterola et al. 2009]. A majority of the research
have taken low level features from audio ﬁle formats such
as .mp3, and .wav by using signal processing techniques to
extract MFCC values spanning short time window [Serr`a
et al. 2012, Pachet and Roy 2009]. This project extract in-
strument, melody and beats features from MIDI ﬁles. Sur-
prisingly good results are obtained in this project.

Since it is more valuable to correctly predict popular songs
than correctly predicting unpopular songs, the precision met-
ric is optimized.

2. DATA

Both MIDI and Million Song Dataset are explored for
feature extraction. MIDI is shown to produce much higher
quality features that results in higher performance, and is
therefore chosen for this project.
2.1 MIDI

Musical Instrument Digital Interface (MIDI) is a technical
standard that allows musical devices to communicate with
each other. A MIDI ﬁle contains up to 16 tracks, each repre-
senting an instrument. Each track contains messages, which
encodes the pitch and duration of an instrument key press.
MIDI ﬁles are close approximations of original music piece.
Although lacking the ﬁdelity and human-ness of raw audio
ﬁles such as .mp3 and .wav, it is nevertheless a faithful rep-
resentation of high level musical features of timbre, melody,
and beats. MIDI suﬃces the purpose of feature extraction
in this paper.

1752 MIDI songs are used as training samples, with ex-
actly 50-50 split between positive and negative training ex-
amples. The deﬁnition of positive and negative labels is
outlined below in ”Labels” section.
2.2 Million Song Dataset

The Million Song Dataset (MSD) is a musical feature
dataset of one million contemporary songs.
It is publicly
available from the joint collaboration between LabROSA
and the Echo Nest [LabROSA 2014]. The dataset is readily
parseable via a Python API with getters and setters to in-
dividual features. Further information could be queried via
the Echo Nest API, which is freely available [the EchoNest
2014].

In pursuing this project, visual and audio examination,
as well as a Logistic Regression model was used to test the
eﬀectiveness of the 10K subset. However, the result was un-
satisfactory. The following introduces inspections and ﬁnd-
ings on why MSD is not ideal for this project.

3. PREPROCESSING
3.1 Type 0 to Type 1

Two types of MIDI music ﬁles are of interest. Type 1 is
easier to process for feature extraction, as each track repre-
sents distinct instruments. Therefore all type 0 MIDI ﬁles
are converted into type 1.

1. Type 1: each individual track represents one musical

instrument. (80% of training samples)

2. Type 0: one single track contains messages across all

channels. (20% of training samples)

The Open-Source Python module MIDO provides a friendly

API that parses a MIDI ﬁle into native python data struc-
tures [Bjørndalen and Binkys 2014].
It is used to extract
instruments/melody and beats features from MIDI ﬁles.

4. FEATURE EXTRACTION

Fine-grained features are engineered to capture the subtle
characteristics of a song. Language models of n-grams are
used to capture the building blocks of melody and beats. In
order to extract the features, the following MIDI messages
are of interest:
• Set tempo: speciﬁes tempo of music piece in microsec-
• Note on: speciﬁes the start of a music note event (e.g.

onds per quarter note (beat)

piano keyboard press)

– Note: speciﬁes the pitch, with 60 representing the

middle C

– Velocity: speciﬁes how much force a note is played
with (a note on message with velocity 0 is the
same as a note oﬀ message)

• Note off: speciﬁes the end of a music note event
• Program change: speciﬁes the instrument for track
• Delta time: speciﬁes the number of ticks since last MIDI
event, for each MIDI event. This can be converted to
delta time in seconds.

Besides, the metadata of Pulse per Quarter-Note is needed.
It speciﬁes number of ticks per beat. This is needed to
compute time delta between MIDI messages.
4.1

Instruments

Each MIDI track contains a program change message,
with instrument type encoded with 0 - 127. A manual group-
ing on instrument types is done based on suggestions from
[McKay 2004] and [Association 2014]. The grouping shrinks
the feature space, while capturing the distinguishing timbre
of instrument classes.

Table 1: MIDI instrument grouping by program
change number (0 - 127)

MIDI Program Change Number
0 - 4
5 - 6, 17, 19
7, 8,
9 - 16
18 - 24
25, 26
27 - 32
33 - 40
...
113 - 120

Instrument
Keyboard
Electric
Other
Chromatic percussion
Organ
Acoustic guitar
Electric guitar
Bass

Percussive

Figure 1: Left: Million Song Dataset: Does not dis-
tinguish between tracks. Right: MIDI : Represents
each instrument as separated tracks.

to 1.0 from the Echo Nest representing popularity.

In MSD, the features of interest are:
• Popularity labels: ”Hotttness” score, a score from 0
• instruments/melody features: ”Pitch” matrix of
size 12 by length-of-song. Size 12 represents the 12
semitones in an octave. The value of each cell corre-
lates to the relative strength of particular semitone at
a given time. Sampling is done every 250 milliseconds,
with some variation based on the beats.

The pitch matrices are not eﬀective representation of in-
struments/melody features, as it does not distinguish be-
tween instruments. Percussion can greatly distort the dom-
inant melody of the song [Jiang et al. 2011]. Figure 1 is a
visualization comparison between MSD and MIDI melody
features. In the left ﬁgure, the red color represents a ”loud”
signal and the blue color represents a ”quiet” signal. The
visualization shows a noisy representation of melody. A lis-
tening test was performed on the transformed sine wave,
with frequency determined by the ”loudest” semitone in a
pitch matrix column. The result audio is completely un-
recognizable for multi-instruments/track songs. A baseline
model is implemented, with 10-fold cross validation show-
ing a ﬂuctuation around 50% precision, recall, and f1 score.
This is not an improvement over random baseline.

MIDI ﬁles, on the other hand, encodes each instrument
into separate tracks as in the right ﬁgure. Therefore it is
decided that the MSD dataset is not eﬀective as melodic
features, and is not used for this project.

2.3 Labels

The problem of hit song prediction is modeled as a binary
classiﬁcation problem, with positive labels representing the
popular songs and negative labels representing unpopular
ones. The Billboard ranking is used to determine whether
a song is popular. Billboard is a prominent music popular-
ity ranking based on radio plays, music streaming and sales
published weekly. Billboard contains ranking dating back to
the 1950s. In this paper, the labels are assigned as follows:
• Positive:
if a song ever reached the top 10 position
• Negative: if a song’s artist never had any song reach-

on any Billboard ranking (since 1950)

ing top 100 position on any Billboard ranking

This requirement is rather strong, leaving out a large mid-
class songs in between. This is done to emphasize the dif-
ferences between the two classes.

4.2 Melody

Melody features are represented by chord progression char-
acteristics. These deﬁning characteristics in music theory is
captured in features below.
• Consecutive two notes (2-grams): captures musical
interval. Musical interval deﬁnes transition between
two consecutive music notes. [Wikipedia 2014].
• Consecutive three notes (3-grams): [Caswell and Ji
2013] suggests that Markov models looking at previous
3 or 4 note pitches produced the best results.

The instruments and melody features are combined to rep-
resent the distinct instruments/melody combination. As an
example, the following MIDI note on messages are trans-
formed into the following features.

MIDI messages:
<track 1>
program_change, value = 0 # instrument: piano
note_on, note = 60, velocity = 64
note_on, note = 62, velocity = 64
note_on, note = 67, velocity = 64
<track 2>
program_change, value = 27 # instrument:
note_on, note = 72, velocity = 64
note_on, note = 76, velocity = 64

electric guitar

Extracted features:
2-grams: keyboard:60:62, keyboard:62:67, electric_guitar:72:76
3-grams: keyboard:60:62:67

4.3 Beats

The beats features are represented by extracting time
delta between consecutive musical notes. The idea of 2-
grams and 3-grams is used here again. In MIDI, time delta
between MIDI messages is speciﬁed in number of ticks. The
following calculation is needed to calculate the delta time (in
seconds) between consecutive MIDI messages:

delta time = delta ticks ·

tempo

1000000.0 · P P QN

(1)

tempo is in microseconds per quarter note.
PPQN (Pulse Per Quarter Note) is in ticks per quarter note.

4.3.1 Note duration
MIDI contains various messages other than note on. Time
delta occurs between each consecutive MIDI messages. Work
is done to accumulate the time delta between consecutive
note on messages. Afterwards, time delta is converted to
beat-per-minute, a standard way of capturing tempo infor-
mation and accounting for small time delta.

4.3.2 Percussion
Track 9 is always the percussion track. Percussion track
is more important as a beats feature than other instrument
tracks. Therefore a preﬁx is added for features extracted
from percussion track to distinguish from other tracks.
4.4 Dimensionality Reduction

Two dimensionality reduction techniques are explored to
avoid overﬁtting. However, due to their limitations and the
eﬀectiveness of regularization term in Logistic Regression,
no dimensionality reduction algorithm is used in this project.

PCA (Principal Component Analysis) transforms feature
space into a lower-dimensional subspace composed of pair-
wise linearly independent vectors.
In practice PCA tends
to favor features with high variance. Feature Selection pri-
oritizes features with highest marginal increase/decrease in
performance. Feature Selection itself is time consuming.

5. MODELS

The problem is modeled as a binary classiﬁcation problem.
This allows for plug-and-play of many oﬀ-the-shelf classiﬁca-
tion algorithms. A more practical model is outlier detection,
since it is much more valuable to predict a popular song than
an unpopular song. However, I was not able to ﬁnd such a
model with satisfying results. The following models are used
in training and testing on the dataset.
5.1 Logistic Regression

Logistic Regression is chosen as the model for task. Lo-
gistic Regression outputs the conﬁdence probability for each
prediction. This is ideal for optimizing for precision, since
the probability cutoﬀ for positive labels can be increased to
form a ”stricter” criteria on popular songs. A regularization
coeﬃcient λ is added and iterated on to decrease overﬁtting.

m(cid:88)

J(θ) =

1
2m

[

n(cid:88)

((hθ(x(i)) − y(i))2 + λ

θ2
j ]

(2)

i=1

j=1

hθ(x) = P (y = 1|x; θ) =

1

1 + e−θT x

(3)

5.2 SVM

SVM is regarded as one of the best ”oﬀ-the-shelf” models.
It has the advantages of being time-eﬃcient and avoiding
overﬁtting. In this project, time-eﬃciency is not a concern
given the relatively small sample size (1700+) to work with.

yi(w · xi − b) ≥ 1 − ζi

(cid:40)

argmin

w,ζ,b

(cid:107)w(cid:107)2 + C

1
2

(cid:41)

ζi

n(cid:88)

i=1

yi(w · x − b) ≥ 1 − ξi, ξi ≥ 0

5.3 Naive Bayes

(4)

(5)

(6)

Naive Bayes assumes each feature is independently dis-
tributed. This assumption is too strong for melody segment
features (n-grams), as they are dependent on neighbors and
the overall chord progression distribution of the song.

(cid:40)

(cid:41)

n(cid:89)

i=1

P (xi|y)

(7)

argmin

P (y = yi)

y

5.4 One-Class SVM

The supervised outlier detection problem is attempted.
One-Class SVM is an one class classiﬁcation algorithm that
takes only positive training examples and adds a negative
example at the origin. Compared to other clustering (K-
means) and outlier detection algorithms (Mixtures of Gaus-
sian), One-Class SVM allows for supervised learning.

6. RESULTS
6.1 Models Comparison

Logistic Regression (cutoﬀ probability = 0.5, regulariza-
tion λ = 1.0), SVM, Naive Bayes, and One-Class SVM are
used to compare performance. 10-fold cross-validation is
performed on the 1752 samples (50% positive, 50% nega-
tive). Mean precision is used as evaluation criteria. Logistic
Regression and SVM resulted in the highest mean precision.

Table 2: Logistic Regression and SVM produces the
highest mean precision
Model

features

Beats

Melody +
Instrument

1-Class SVM
Naive Bayes
Logistic Reg.
SVM C=1.0
1-Class SVM
Naive Bayes
Logistic Reg.
SVM C=1.0

6.2 Ensemble Method

Precision Recall
0.406
0.647
0.618
0.575
0.491
0.628
0.633
0.608

0.488
0.577
0.600
0.625
0.514
0.559
0.596
0.595

F1
0.439
0.608
0.607
0.597
0.501
0.590
0.612
0.601

The ensemble method with Logistic Regression (regular-
ization λ = 1.0) gave the highest overall precision. This
method uses both instruments/melody features and beats
features. Two separate Logistic Regression classiﬁers are
run on each feature sets separately. The combined output
is positive if both predicted probabilities are greater than a
conﬁdence cutoﬀ.

Precision is optimized by increasing the probability cutoﬀ.
The best precision 0.882 is achieved at probability cutoﬀ
of 0.998 (recall is 0.279). The increase in probability cutoﬀ
can be intuitively understood as the increased conﬁdence
required to qualify for a positive label (a popular song).

Figure 3: Using the ensemble method, precision
peaks at 0.882. Recall is 0.279.

Figure 4: Using the ensemble method, probability
cutoﬀ can be increased to increase precision.

Figure 2: Ensemble method produces the best per-
formance by combining both instruments/melody
features and Beats.

6.3 Features Comparison

Using only instruments/melody features or only beats fea-
tures resulted in identical performance (roughly 60% preci-
sion). Combining the two features as in ensemble method
improves the precision by roughly 15% or more.

Figure 5: Combining instruments/melody and beats
features results in 15%+ increase for mean precision.

Predicting Hit Songs with MIDI Musical Features

Keven (Kedao) Wang

Stanford University

kvw@stanford.edu

ABSTRACT
This paper predicts hit songs based on musical features from
MIDI ﬁles. The task is modeled as a binary classiﬁcation
problem optimizing for precision, with Billboard ranking as
labels. Million Song Dataset (MSD) is inspected audibly,
visually, and with a logistic regression model. MSD features
is determined too noisy for the task. MIDI ﬁles encodes
pitch duration as separate instrument tracks, and is chosen
over MSD. Fine-grained instrument, melody, and beats fea-
tures are extracted. Language models of n-grams are used
to transform raw musical features into word-document fre-
quency matrices. Logistic Regression is chosen as the clas-
siﬁer, with increased probability cutoﬀ to optimize for pre-
cision. An ensemble method that uses both instruments/
melody as well as beats features produces the peak precision
0.882 at probability cutoﬀ 0.998 (recall is 0.279). Alterna-
tive models and applications are discussed.

Keywords
Music, Hit Song, Classiﬁcation, MIDI

1.

INTRODUCTION

The goal of this project is to predict hit songs based on
musical features. A song is deﬁned as a hit if it has ever
reached top 10 position on a Billboard weekly ranking. Pre-
dicting hit songs is meaningful in numerous ways:

1. Help music stream services to surface upcoming hits

for better user engagement

2. Improve the iteration process for artists before releas-

ing music to the public

Various factors determine whether a song is popular/a
hit, including the intrinsic quality of the music piece, and
psychological/social factors [Pachet and Sony 2012]. The
latter encompass peer pressure, public opinion, frequency
of listening, and artists’ reputation. These psychological/
behavioral factors are harder to quantify, and is out of scope
for this project. The popularity of a song does correlate to
its intrinsic quality [Pachet and Sony 2012]. Therefore this
paper focuses on the analysis of musical features, which are
quantiﬁable and encapsulated in the audio ﬁle itself. The
following musical features are analyzed.

1. Timbre/instrument
2. Melody
3. Beats
Relatively few projects have explored the hit song predic-
tion space [Pachet and Sony 2012, Ni et al. 2011, Dhanaraj
and Logan 2005, HERREMANS et al. 2014, Fan and Casey

2013, Monterola et al. 2009]. A majority of the research
have taken low level features from audio ﬁle formats such
as .mp3, and .wav by using signal processing techniques to
extract MFCC values spanning short time window [Serr`a
et al. 2012, Pachet and Roy 2009]. This project extract in-
strument, melody and beats features from MIDI ﬁles. Sur-
prisingly good results are obtained in this project.

Since it is more valuable to correctly predict popular songs
than correctly predicting unpopular songs, the precision met-
ric is optimized.

2. DATA

Both MIDI and Million Song Dataset are explored for
feature extraction. MIDI is shown to produce much higher
quality features that results in higher performance, and is
therefore chosen for this project.
2.1 MIDI

Musical Instrument Digital Interface (MIDI) is a technical
standard that allows musical devices to communicate with
each other. A MIDI ﬁle contains up to 16 tracks, each repre-
senting an instrument. Each track contains messages, which
encodes the pitch and duration of an instrument key press.
MIDI ﬁles are close approximations of original music piece.
Although lacking the ﬁdelity and human-ness of raw audio
ﬁles such as .mp3 and .wav, it is nevertheless a faithful rep-
resentation of high level musical features of timbre, melody,
and beats. MIDI suﬃces the purpose of feature extraction
in this paper.

1752 MIDI songs are used as training samples, with ex-
actly 50-50 split between positive and negative training ex-
amples. The deﬁnition of positive and negative labels is
outlined below in ”Labels” section.
2.2 Million Song Dataset

The Million Song Dataset (MSD) is a musical feature
dataset of one million contemporary songs.
It is publicly
available from the joint collaboration between LabROSA
and the Echo Nest [LabROSA 2014]. The dataset is readily
parseable via a Python API with getters and setters to in-
dividual features. Further information could be queried via
the Echo Nest API, which is freely available [the EchoNest
2014].

In pursuing this project, visual and audio examination,
as well as a Logistic Regression model was used to test the
eﬀectiveness of the 10K subset. However, the result was un-
satisfactory. The following introduces inspections and ﬁnd-
ings on why MSD is not ideal for this project.

3. PREPROCESSING
3.1 Type 0 to Type 1

Two types of MIDI music ﬁles are of interest. Type 1 is
easier to process for feature extraction, as each track repre-
sents distinct instruments. Therefore all type 0 MIDI ﬁles
are converted into type 1.

1. Type 1: each individual track represents one musical

instrument. (80% of training samples)

2. Type 0: one single track contains messages across all

channels. (20% of training samples)

The Open-Source Python module MIDO provides a friendly

API that parses a MIDI ﬁle into native python data struc-
tures [Bjørndalen and Binkys 2014].
It is used to extract
instruments/melody and beats features from MIDI ﬁles.

4. FEATURE EXTRACTION

Fine-grained features are engineered to capture the subtle
characteristics of a song. Language models of n-grams are
used to capture the building blocks of melody and beats. In
order to extract the features, the following MIDI messages
are of interest:
• Set tempo: speciﬁes tempo of music piece in microsec-
• Note on: speciﬁes the start of a music note event (e.g.

onds per quarter note (beat)

piano keyboard press)

– Note: speciﬁes the pitch, with 60 representing the

middle C

– Velocity: speciﬁes how much force a note is played
with (a note on message with velocity 0 is the
same as a note oﬀ message)

• Note off: speciﬁes the end of a music note event
• Program change: speciﬁes the instrument for track
• Delta time: speciﬁes the number of ticks since last MIDI
event, for each MIDI event. This can be converted to
delta time in seconds.

Besides, the metadata of Pulse per Quarter-Note is needed.
It speciﬁes number of ticks per beat. This is needed to
compute time delta between MIDI messages.
4.1

Instruments

Each MIDI track contains a program change message,
with instrument type encoded with 0 - 127. A manual group-
ing on instrument types is done based on suggestions from
[McKay 2004] and [Association 2014]. The grouping shrinks
the feature space, while capturing the distinguishing timbre
of instrument classes.

Table 1: MIDI instrument grouping by program
change number (0 - 127)

MIDI Program Change Number
0 - 4
5 - 6, 17, 19
7, 8,
9 - 16
18 - 24
25, 26
27 - 32
33 - 40
...
113 - 120

Instrument
Keyboard
Electric
Other
Chromatic percussion
Organ
Acoustic guitar
Electric guitar
Bass

Percussive

Figure 1: Left: Million Song Dataset: Does not dis-
tinguish between tracks. Right: MIDI : Represents
each instrument as separated tracks.

to 1.0 from the Echo Nest representing popularity.

In MSD, the features of interest are:
• Popularity labels: ”Hotttness” score, a score from 0
• instruments/melody features: ”Pitch” matrix of
size 12 by length-of-song. Size 12 represents the 12
semitones in an octave. The value of each cell corre-
lates to the relative strength of particular semitone at
a given time. Sampling is done every 250 milliseconds,
with some variation based on the beats.

The pitch matrices are not eﬀective representation of in-
struments/melody features, as it does not distinguish be-
tween instruments. Percussion can greatly distort the dom-
inant melody of the song [Jiang et al. 2011]. Figure 1 is a
visualization comparison between MSD and MIDI melody
features. In the left ﬁgure, the red color represents a ”loud”
signal and the blue color represents a ”quiet” signal. The
visualization shows a noisy representation of melody. A lis-
tening test was performed on the transformed sine wave,
with frequency determined by the ”loudest” semitone in a
pitch matrix column. The result audio is completely un-
recognizable for multi-instruments/track songs. A baseline
model is implemented, with 10-fold cross validation show-
ing a ﬂuctuation around 50% precision, recall, and f1 score.
This is not an improvement over random baseline.

MIDI ﬁles, on the other hand, encodes each instrument
into separate tracks as in the right ﬁgure. Therefore it is
decided that the MSD dataset is not eﬀective as melodic
features, and is not used for this project.

2.3 Labels

The problem of hit song prediction is modeled as a binary
classiﬁcation problem, with positive labels representing the
popular songs and negative labels representing unpopular
ones. The Billboard ranking is used to determine whether
a song is popular. Billboard is a prominent music popular-
ity ranking based on radio plays, music streaming and sales
published weekly. Billboard contains ranking dating back to
the 1950s. In this paper, the labels are assigned as follows:
• Positive:
if a song ever reached the top 10 position
• Negative: if a song’s artist never had any song reach-

on any Billboard ranking (since 1950)

ing top 100 position on any Billboard ranking

This requirement is rather strong, leaving out a large mid-
class songs in between. This is done to emphasize the dif-
ferences between the two classes.

4.2 Melody

Melody features are represented by chord progression char-
acteristics. These deﬁning characteristics in music theory is
captured in features below.
• Consecutive two notes (2-grams): captures musical
interval. Musical interval deﬁnes transition between
two consecutive music notes. [Wikipedia 2014].
• Consecutive three notes (3-grams): [Caswell and Ji
2013] suggests that Markov models looking at previous
3 or 4 note pitches produced the best results.

The instruments and melody features are combined to rep-
resent the distinct instruments/melody combination. As an
example, the following MIDI note on messages are trans-
formed into the following features.

MIDI messages:
<track 1>
program_change, value = 0 # instrument: piano
note_on, note = 60, velocity = 64
note_on, note = 62, velocity = 64
note_on, note = 67, velocity = 64
<track 2>
program_change, value = 27 # instrument:
note_on, note = 72, velocity = 64
note_on, note = 76, velocity = 64

electric guitar

Extracted features:
2-grams: keyboard:60:62, keyboard:62:67, electric_guitar:72:76
3-grams: keyboard:60:62:67

4.3 Beats

The beats features are represented by extracting time
delta between consecutive musical notes. The idea of 2-
grams and 3-grams is used here again. In MIDI, time delta
between MIDI messages is speciﬁed in number of ticks. The
following calculation is needed to calculate the delta time (in
seconds) between consecutive MIDI messages:

delta time = delta ticks ·

tempo

1000000.0 · P P QN

(1)

tempo is in microseconds per quarter note.
PPQN (Pulse Per Quarter Note) is in ticks per quarter note.

4.3.1 Note duration
MIDI contains various messages other than note on. Time
delta occurs between each consecutive MIDI messages. Work
is done to accumulate the time delta between consecutive
note on messages. Afterwards, time delta is converted to
beat-per-minute, a standard way of capturing tempo infor-
mation and accounting for small time delta.

4.3.2 Percussion
Track 9 is always the percussion track. Percussion track
is more important as a beats feature than other instrument
tracks. Therefore a preﬁx is added for features extracted
from percussion track to distinguish from other tracks.
4.4 Dimensionality Reduction

Two dimensionality reduction techniques are explored to
avoid overﬁtting. However, due to their limitations and the
eﬀectiveness of regularization term in Logistic Regression,
no dimensionality reduction algorithm is used in this project.

PCA (Principal Component Analysis) transforms feature
space into a lower-dimensional subspace composed of pair-
wise linearly independent vectors.
In practice PCA tends
to favor features with high variance. Feature Selection pri-
oritizes features with highest marginal increase/decrease in
performance. Feature Selection itself is time consuming.

5. MODELS

The problem is modeled as a binary classiﬁcation problem.
This allows for plug-and-play of many oﬀ-the-shelf classiﬁca-
tion algorithms. A more practical model is outlier detection,
since it is much more valuable to predict a popular song than
an unpopular song. However, I was not able to ﬁnd such a
model with satisfying results. The following models are used
in training and testing on the dataset.
5.1 Logistic Regression

Logistic Regression is chosen as the model for task. Lo-
gistic Regression outputs the conﬁdence probability for each
prediction. This is ideal for optimizing for precision, since
the probability cutoﬀ for positive labels can be increased to
form a ”stricter” criteria on popular songs. A regularization
coeﬃcient λ is added and iterated on to decrease overﬁtting.

m(cid:88)

J(θ) =

1
2m

[

n(cid:88)

((hθ(x(i)) − y(i))2 + λ

θ2
j ]

(2)

i=1

j=1

hθ(x) = P (y = 1|x; θ) =

1

1 + e−θT x

(3)

5.2 SVM

SVM is regarded as one of the best ”oﬀ-the-shelf” models.
It has the advantages of being time-eﬃcient and avoiding
overﬁtting. In this project, time-eﬃciency is not a concern
given the relatively small sample size (1700+) to work with.

yi(w · xi − b) ≥ 1 − ζi

(cid:40)

argmin

w,ζ,b

(cid:107)w(cid:107)2 + C

1
2

(cid:41)

ζi

n(cid:88)

i=1

yi(w · x − b) ≥ 1 − ξi, ξi ≥ 0

5.3 Naive Bayes

(4)

(5)

(6)

Naive Bayes assumes each feature is independently dis-
tributed. This assumption is too strong for melody segment
features (n-grams), as they are dependent on neighbors and
the overall chord progression distribution of the song.

(cid:40)

(cid:41)

n(cid:89)

i=1

P (xi|y)

(7)

argmin

P (y = yi)

y

5.4 One-Class SVM

The supervised outlier detection problem is attempted.
One-Class SVM is an one class classiﬁcation algorithm that
takes only positive training examples and adds a negative
example at the origin. Compared to other clustering (K-
means) and outlier detection algorithms (Mixtures of Gaus-
sian), One-Class SVM allows for supervised learning.

6. RESULTS
6.1 Models Comparison

Logistic Regression (cutoﬀ probability = 0.5, regulariza-
tion λ = 1.0), SVM, Naive Bayes, and One-Class SVM are
used to compare performance. 10-fold cross-validation is
performed on the 1752 samples (50% positive, 50% nega-
tive). Mean precision is used as evaluation criteria. Logistic
Regression and SVM resulted in the highest mean precision.

Table 2: Logistic Regression and SVM produces the
highest mean precision
Model

features

Beats

Melody +
Instrument

1-Class SVM
Naive Bayes
Logistic Reg.
SVM C=1.0
1-Class SVM
Naive Bayes
Logistic Reg.
SVM C=1.0

6.2 Ensemble Method

Precision Recall
0.406
0.647
0.618
0.575
0.491
0.628
0.633
0.608

0.488
0.577
0.600
0.625
0.514
0.559
0.596
0.595

F1
0.439
0.608
0.607
0.597
0.501
0.590
0.612
0.601

The ensemble method with Logistic Regression (regular-
ization λ = 1.0) gave the highest overall precision. This
method uses both instruments/melody features and beats
features. Two separate Logistic Regression classiﬁers are
run on each feature sets separately. The combined output
is positive if both predicted probabilities are greater than a
conﬁdence cutoﬀ.

Precision is optimized by increasing the probability cutoﬀ.
The best precision 0.882 is achieved at probability cutoﬀ
of 0.998 (recall is 0.279). The increase in probability cutoﬀ
can be intuitively understood as the increased conﬁdence
required to qualify for a positive label (a popular song).

Figure 3: Using the ensemble method, precision
peaks at 0.882. Recall is 0.279.

Figure 4: Using the ensemble method, probability
cutoﬀ can be increased to increase precision.

Figure 2: Ensemble method produces the best per-
formance by combining both instruments/melody
features and Beats.

6.3 Features Comparison

Using only instruments/melody features or only beats fea-
tures resulted in identical performance (roughly 60% preci-
sion). Combining the two features as in ensemble method
improves the precision by roughly 15% or more.

Figure 5: Combining instruments/melody and beats
features results in 15%+ increase for mean precision.

6.4 Regularization

Experiment is done in tweaking the regularization param-
eter λ in Logistic Regression (p = 0.5). The result shows an
insigniﬁcant change in precision as a result of regularization
parameter change. Therefore λ is kept at the default 1.0.

7. DISCUSSION

The peak precision 0.882 at probability cutoﬀ 0.998 is sur-
prisingly good. The corresponding recall is 0.279. Precision
is optimized since it is more valuable to have true positives
than true negatives. The high precision demonstrates that:
1. The distinguishing characteristics between popular and

unpopular songs can be learned.

2. MIDI ﬁles are able to produce high quality features

for hit song prediction purposes.

3. It is promising to borrow language models for melody

and beats feature extraction.

4. The feature extraction of instruments, melody, and
beats features is able to capture the distinguishing
characteristics of a song. The ensemble of instruments/
melody and beats features is promising.

The top 100 features with highest Logistic Regression co-
eﬃcients are analyzed. There is no clear pattern on these
features. Given the large feature space and the large number
of features a single song has, a positive prediction is likely
attributed by the aggregation of a large number of features.
One-Class SVM hardly gives any improvement over ran-
dom baseline. This can be explained by the strict informa-
tion loss by replacing all negative training examples with a
single negative example at origin.

8. FUTURE WORK

Supervised outlier detection models could be explored. In
this project, the hit song prediction problem is modeled as a
binary classiﬁcation problem. Modeling as outlier detection
would be more suitable because:

1. It is more valuable to correctly predict popular songs
than predicting negative ones. Modeling as outlier
detection removes the need to collect negative label
songs, which are vast and harder to deﬁne.

2. There could be a vast number of reasons as to why a
It is better to focus on ﬁnding

song is not popular.
deﬁning features of popular songs.

Neural Network has produced excellent results for speech
recognition tasks, and could be used for feature selection
purposes. Generative models such as Mixture of Gaussians
and K-means can be used for outlier detection. Neural Net-
works could be supervised to learn features for these unsu-
pervised models.

A natural next step would be auto hit song composition.
The result could augment human in composing hit songs by
oﬀering inspirations. A combiner is needed to reconstruct a
song coherently from building blocks of melody and beats.
The bottom-up construction mechanism would ﬁrst combine
n-grams into musical bars, then into verses/choruses, and
eventually into the entire song.

9. REFERENCES
[Association 2014] MIDI Manufacturers Association. 2014.

General MIDI Level 1 Sound Set. (2014).
http://www.midi.org/techspecs/gm1sound.php

[Bjørndalen and Binkys 2014] Ole Martin Bjørndalen and

Rapolas Binkys. 2014. Mido - MIDI Objects for
Python. (2014). http://mido.readthedocs.org/en/latest/

[Camenzind and Goel 2013] Tom Camenzind and

Shubham Goel. 2013. #jazz : Automatic Music Genre
Detection. (2013).

[Caswell and Ji 2013] Isaac Caswell and Erika Ji. 2013.

Analysis and Clustering of Musical Compositions
using Melody-based Features. (2013).

[Dhanaraj and Logan 2005] Ruth Dhanaraj and Beth

Logan. 2005. Automatic Prediction of Hit Songs.. In
ISMIR. 488–491.

[Fan and Casey 2013] Jianyu Fan and Michael A Casey.

2013. Study of Chinese and UK Hit Songs Prediction.
(2013).

[HERREMANS et al. 2014] Dorien HERREMANS, David
MARTENS, and Kenneth S ¨ORENSEN. 2014. Dance
hit song prediction. Technical Report.

[Jiang et al. 2011] Nanzhu Jiang, Peter Grosche, Verena
Konz, and Meinard M¨uller. 2011. Analyzing chroma
feature types for automated chord recognition. In
Audio Engineering Society Conference: 42nd
International Conference: Semantic Audio. Audio
Engineering Society.

[LabROSA 2014] Columbia LabROSA. 2014. Million Song

Dataset. (2014).
http://labrosa.ee.columbia.edu/millionsong/

[McKay 2004] Cory McKay. 2004. Automatic genre

classiﬁcation of MIDI recordings. Ph.D. Dissertation.
McGill University.

[Monterola et al. 2009] Christopher Monterola, Cheryl

Abundo, Jeric Tugaﬀ, and Lorcel Ericka Venturina.
2009. Prediction of potential hit song and musical
genre using artiﬁcial neural networks. International
Journal of Modern Physics C 20, 11 (2009),
1697–1718.

[Ni et al. 2011] Yizhao Ni, Ra´ul Santos-Rodr´ıguez, Matt

Mcvicar, and Tijl De Bie. 2011. Hit song science once
again a science? (2011).

[Pachet and Roy 2009] Fran¸cois Pachet and Pierre Roy.

2009. Analytical features: a knowledge-based approach
to audio feature generation. EURASIP Journal on
Audio, Speech, and Music Processing 2009 (2009), 1.

[Pachet and Sony 2012] Fran¸cois Pachet and CSL Sony.
2012. Hit song science. Music Data Mining (2012),
305–26.

[Serr`a et al. 2012] Joan Serr`a, ´Alvaro Corral, Mari´an
Bogu˜n´a, Mart´ın Haro, and Josep Ll Arcos. 2012.
Measuring the evolution of contemporary western
popular music. Scientiﬁc reports 2 (2012).

[the EchoNest 2014] the EchoNest. 2014. Echo Nest API
Overview. (2014). http://developer.echonest.com/docs/v4
[Tzanetakis and Cook 2002] George Tzanetakis and Perry

Cook. 2002. Musical genre classiﬁcation of audio
signals. Speech and Audio Processing, IEEE
transactions on 10, 5 (2002), 293–302.

[Wikipedia 2014] Wikipedia. 2014. Interval (music) —

Wikipedia, The Free Encyclopedia. (2014).
http://en.wikipedia.org/w/index.php?title=Interval\
_(music)&oldid=627299987

