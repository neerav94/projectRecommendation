Cross-Domain Product Classiﬁcation with Deep Learning

Luke de Oliveira,1 Alfredo L´ainez Rodrigo,1 and Akua Abu1

1Stanford University ICME, CS229 Final Project

In our ﬁnal project, we took on the challenge of cross-domain classiﬁcation–the adaptation of a
model suitable for one domain to one or more diﬀerent domains with identical features. Speciﬁcally,
we developed a cross-domain classiﬁcation schema to predict product categories in tweets and eBay
reviews based on user reviews from Amazon. We ﬁrst developed a set of classical machine learning
classiﬁers that performed reasonably well on our principal data set of Amazon products and then
applied the same classiﬁers to predict product categories from Twitter tweets and eBay reviews. We
then developed Deep Learning models to approach to our cross-domain classiﬁcation problem and
compared performance. In what follows, we walk step by step through the procedure, illustrate our
results, then discuss future work. We show that while Deep Learning deﬁnitively helps models work
out-of-domain, considerable work needs to be done to create a context-free model of language that
can work across domains.

I.

INTRODUCTION

There is no longer a boundary between social media,
advertising, and consumer culture. The intersection of
these diﬀerent spaces has resulted in an immesurable in-
crease in the amount and prevalence of user data. Users
develop online identities that span across many plat-
forms, from Facebook to Twitter to YouTube and to
consumer communities such as eBay and Amazon. An
open problem of great relevance to these overlapping so-
cial spheres is that of cross-domain learning – that is,
given an estimated model suitable for one domain, can
the model perform in an entirely diﬀerent context given
identical features? Using a dataset of Amazon prod-
ucts labeled within the Amazon hierarchical categories,
we have developed a set of classiﬁers that predict prod-
uct categories based on users’ reviews. Then, we apply
the same predictors of user content to diﬀerent domains
to see how these models behaves in a diﬀerent context
over diﬀerent underlying distributions of features. The
key challenge is that we impose no prior over the tar-
get distribution – we simply aim to develop a model
that is not entirely dependent on a training distribu-
tion. Speciﬁcally, we have predicted product categoriza-
tion using tweets from Twitter and reviews from eBay,
and we determine the validity of an out-of-context clas-
siﬁer using carefully chosen metrics. Finally, we have
engineered two Deep Learning models, and use them to
illustrate a potential new application for Deep Architec-
tures – we outperform classical approaches in this task,
showing adaptability to new domains.

II. SETUP

A. Data

This project makes use of a principal data set of Ama-
zon product reviews and an Amazon product hierar-
chy. Secondary data sets include product-related Twit-
ter data (tweets), and ebay product reviews – each of
which are hand labeled. The main Amazon data sets
consists of descriptions and 10 reviews for each of the

top 10 products in every Amazon product category, com-
prising a hierarchy of roughly 400,000 products ranging
across 40,000 categories. The secondary data set is of
eBay and Twitter data: the set contains tweets and re-
views along with an annotated Amazon category. For
our classiﬁcation purposes, we used the 24 main cate-
gories in the Amazon hierarchy, merging the main cat-
egories KindleStore and Books. Then, our classiﬁcation
operates over text (reviews or tweets) labeled by a main
Amazon category.

The data used was saved in JSON format, including
node categorization in the Amazon product hierarchy
for each product. We built a parser and graph creation
codebase that allowed us to extract this hierarchy in the
form of a tree and traverse it for each product, allowing
us to extract labels at any depth. Speciﬁcally, we utilized
the nodes with d = 1 away from the root node – i.e., the
main categories. For the purposes of our investigation
into cross domain text classiﬁcation, we care less about
a ﬁne study of large-scale hierarchical clasiﬁcation and
more about whether or not we can ﬁnd methods that
are valid across the domain – in some sense, a problem
completely independent from the former.

B. Evaluation

We are working in a classiﬁcation space with 24 dif-
ferent groups with highly imbalanced classes. In order
to compensate for this, we utilize F1-scores averaged out
across classes, where

1 = 2 · precision · recall

precision + recall

F (j)

is the F1 score for class j. We compute precision and re-
call for each class in a one-vs-rest approach. Our overall
F1 score (for J classes) is then

J(cid:88)

j=1

F1 =

1
J

F (j)
1 .

More generally, one can imagine a scenario in which we
care about some convex combination denoted by a vector

Cross-Domain Product Classiﬁcation with Deep Learning

Luke de Oliveira,1 Alfredo L´ainez Rodrigo,1 and Akua Abu1

1Stanford University ICME, CS229 Final Project

In our ﬁnal project, we took on the challenge of cross-domain classiﬁcation–the adaptation of a
model suitable for one domain to one or more diﬀerent domains with identical features. Speciﬁcally,
we developed a cross-domain classiﬁcation schema to predict product categories in tweets and eBay
reviews based on user reviews from Amazon. We ﬁrst developed a set of classical machine learning
classiﬁers that performed reasonably well on our principal data set of Amazon products and then
applied the same classiﬁers to predict product categories from Twitter tweets and eBay reviews. We
then developed Deep Learning models to approach to our cross-domain classiﬁcation problem and
compared performance. In what follows, we walk step by step through the procedure, illustrate our
results, then discuss future work. We show that while Deep Learning deﬁnitively helps models work
out-of-domain, considerable work needs to be done to create a context-free model of language that
can work across domains.

I.

INTRODUCTION

There is no longer a boundary between social media,
advertising, and consumer culture. The intersection of
these diﬀerent spaces has resulted in an immesurable in-
crease in the amount and prevalence of user data. Users
develop online identities that span across many plat-
forms, from Facebook to Twitter to YouTube and to
consumer communities such as eBay and Amazon. An
open problem of great relevance to these overlapping so-
cial spheres is that of cross-domain learning – that is,
given an estimated model suitable for one domain, can
the model perform in an entirely diﬀerent context given
identical features? Using a dataset of Amazon prod-
ucts labeled within the Amazon hierarchical categories,
we have developed a set of classiﬁers that predict prod-
uct categories based on users’ reviews. Then, we apply
the same predictors of user content to diﬀerent domains
to see how these models behaves in a diﬀerent context
over diﬀerent underlying distributions of features. The
key challenge is that we impose no prior over the tar-
get distribution – we simply aim to develop a model
that is not entirely dependent on a training distribu-
tion. Speciﬁcally, we have predicted product categoriza-
tion using tweets from Twitter and reviews from eBay,
and we determine the validity of an out-of-context clas-
siﬁer using carefully chosen metrics. Finally, we have
engineered two Deep Learning models, and use them to
illustrate a potential new application for Deep Architec-
tures – we outperform classical approaches in this task,
showing adaptability to new domains.

II. SETUP

A. Data

This project makes use of a principal data set of Ama-
zon product reviews and an Amazon product hierar-
chy. Secondary data sets include product-related Twit-
ter data (tweets), and ebay product reviews – each of
which are hand labeled. The main Amazon data sets
consists of descriptions and 10 reviews for each of the

top 10 products in every Amazon product category, com-
prising a hierarchy of roughly 400,000 products ranging
across 40,000 categories. The secondary data set is of
eBay and Twitter data: the set contains tweets and re-
views along with an annotated Amazon category. For
our classiﬁcation purposes, we used the 24 main cate-
gories in the Amazon hierarchy, merging the main cat-
egories KindleStore and Books. Then, our classiﬁcation
operates over text (reviews or tweets) labeled by a main
Amazon category.

The data used was saved in JSON format, including
node categorization in the Amazon product hierarchy
for each product. We built a parser and graph creation
codebase that allowed us to extract this hierarchy in the
form of a tree and traverse it for each product, allowing
us to extract labels at any depth. Speciﬁcally, we utilized
the nodes with d = 1 away from the root node – i.e., the
main categories. For the purposes of our investigation
into cross domain text classiﬁcation, we care less about
a ﬁne study of large-scale hierarchical clasiﬁcation and
more about whether or not we can ﬁnd methods that
are valid across the domain – in some sense, a problem
completely independent from the former.

B. Evaluation

We are working in a classiﬁcation space with 24 dif-
ferent groups with highly imbalanced classes. In order
to compensate for this, we utilize F1-scores averaged out
across classes, where

1 = 2 · precision · recall

precision + recall

F (j)

is the F1 score for class j. We compute precision and re-
call for each class in a one-vs-rest approach. Our overall
F1 score (for J classes) is then

J(cid:88)

j=1

F1 =

1
J

F (j)
1 .

More generally, one can imagine a scenario in which we
care about some convex combination denoted by a vector

c, where

J(cid:88)

j=1

F c

1 =

cjF (j)
1 ,

and each cj denotes our “relative cost” of an F1 error
on class j. However, we opt for a standard mean to give
each class equal inﬂuence in terms of how well we predict
its membership. Traditional classiﬁcation error was not
used as this provides us with a better metric given our
imbalances.

It is important to note that a few classes are heav-
ily underrepresented in the data, even considering the
whole dataset. Hence, the average computed as a qual-
ity measure will be aﬀected by these classes, and will
be signiﬁcantly skewed downward, moving the bias away
from well-represented classes.

C. Features

As pure text was the raw input to our model, we uti-
lized a bag-of-words representation of our review texts,
incorporating tf-ifd and removing common stop words to
improve performance and direct our attention to seman-
tically relevant aspects of cross domain language gener-
alization. The features thus corresponded to a sparse
matrix of word frequencies from our selected corpus.
The inverse document frequency (ifd) improved the re-
sults ruling out words commonly present in the corpus.
Moreover, we incorporated an n-gram approach into our
model to better take account of diﬀerences in mean-
ing resulting from contiguous sequences of words. For
n > 1, run-times were longer, but we obtained a slight
increase in test set performance with worse results for
cross-domain classiﬁcation. This is consistent with a pri-
ori intuition regarding these features, as n-grams with
n > 1 provide a very reﬁned look at language structure,
meaning any model learned will not generalize well.

III. CLASSICAL METHODS

As a ﬁrst classiﬁcation step, we selected three well-
known algorithms for the task of text classiﬁcation:
Naive Bayes, Stochastic Gradient Descent and Support
Vector Machines, followed by a grid search / CV for pa-
rameter optimization.
In particular, we utilized Naive
Bayes with Laplacian smoothing and SGD with a modi-
ﬁed huber loss function and α = 0.001. Also, the SVM is
a linear SVM which follows a one-vs-the-rest multiclass
strategy, since other SVMs with one-vs-one strategies
are too time consuming for our space of 24 categories. In
particular, given the constraints of scikit-learn, uti-
lization of any Kernel Machine requires the estimation of

(cid:1) = 276 (in our case) Kernel Machines – a prohibitive

cost.

(cid:0)J

2

During the implementation and evaluation, we no-
ticed that these classically derived “shallow” methods

2

FIG. 1: Confusion matrix for predicting categories in the
Amazon dataset using a SVM with 2-grams bag of words

presented certain bias towards the majority class. Since
Amazon started as a bookstore, the number of products
categorized as books is about 30% of the entire dataset,
with other categories also comprising a large proportion
of the training examples. At the opposite end of the
spectrum, we have a pair of product classes comprising
less than 100 products in total. The problem of training
with such overrepresented classes made a Naive Bayes
classiﬁer simply predict the class “Books” for every given
example.

In order to avoid this training bias, and considering
the large amount of data available, we loaded the whole
dataset for training and testing purposes and selectively
pruned it in order to balance the classes for training.
While this does not solve the underrepresentation of
some classes, it provides more balance to the majority of
them and utilizes all the examples available of the less
common ones. In FIG 1 we can see a confusion matrix
illustrating the distribution of the classes after pruning
and classiﬁcation for the Amazon dataset.

IV. DEEP LEARNING

As our foray into classical methods proved, a simple
application of a classiﬁer is not appropriate for estimat-
ing a model with the expectation of out-of-domain per-
formance. To examine how we can construct a model
that can learn features that can prove useful both in-
side and outside the training domain, we look to deep
learning.
In particular, we look to ways in which we
can provide regularization, as this will help our models
adapt to diﬀerent domains. The challenge in our cho-
sen problem is that we have no prior on the language
distribution in the target domain. A series of struc-
tures / paradigms were decided upon that would be able
to test the relevance of Deep Learning for this partic-
ular cross-domain classiﬁcation problem. We consider
stacked autoencoders of regular and denoising variety,
and a “stacked thesaurus,” as we call it – a novel modiﬁ-
cation to Dense Cohorts of Terms. These are all trained
in an unsupervised fashion, with supervised ﬁne-tuning

Cross-Domain Product Classiﬁcation with Deep Learning

Luke de Oliveira,1 Alfredo L´ainez Rodrigo,1 and Akua Abu1

1Stanford University ICME, CS229 Final Project

In our ﬁnal project, we took on the challenge of cross-domain classiﬁcation–the adaptation of a
model suitable for one domain to one or more diﬀerent domains with identical features. Speciﬁcally,
we developed a cross-domain classiﬁcation schema to predict product categories in tweets and eBay
reviews based on user reviews from Amazon. We ﬁrst developed a set of classical machine learning
classiﬁers that performed reasonably well on our principal data set of Amazon products and then
applied the same classiﬁers to predict product categories from Twitter tweets and eBay reviews. We
then developed Deep Learning models to approach to our cross-domain classiﬁcation problem and
compared performance. In what follows, we walk step by step through the procedure, illustrate our
results, then discuss future work. We show that while Deep Learning deﬁnitively helps models work
out-of-domain, considerable work needs to be done to create a context-free model of language that
can work across domains.

I.

INTRODUCTION

There is no longer a boundary between social media,
advertising, and consumer culture. The intersection of
these diﬀerent spaces has resulted in an immesurable in-
crease in the amount and prevalence of user data. Users
develop online identities that span across many plat-
forms, from Facebook to Twitter to YouTube and to
consumer communities such as eBay and Amazon. An
open problem of great relevance to these overlapping so-
cial spheres is that of cross-domain learning – that is,
given an estimated model suitable for one domain, can
the model perform in an entirely diﬀerent context given
identical features? Using a dataset of Amazon prod-
ucts labeled within the Amazon hierarchical categories,
we have developed a set of classiﬁers that predict prod-
uct categories based on users’ reviews. Then, we apply
the same predictors of user content to diﬀerent domains
to see how these models behaves in a diﬀerent context
over diﬀerent underlying distributions of features. The
key challenge is that we impose no prior over the tar-
get distribution – we simply aim to develop a model
that is not entirely dependent on a training distribu-
tion. Speciﬁcally, we have predicted product categoriza-
tion using tweets from Twitter and reviews from eBay,
and we determine the validity of an out-of-context clas-
siﬁer using carefully chosen metrics. Finally, we have
engineered two Deep Learning models, and use them to
illustrate a potential new application for Deep Architec-
tures – we outperform classical approaches in this task,
showing adaptability to new domains.

II. SETUP

A. Data

This project makes use of a principal data set of Ama-
zon product reviews and an Amazon product hierar-
chy. Secondary data sets include product-related Twit-
ter data (tweets), and ebay product reviews – each of
which are hand labeled. The main Amazon data sets
consists of descriptions and 10 reviews for each of the

top 10 products in every Amazon product category, com-
prising a hierarchy of roughly 400,000 products ranging
across 40,000 categories. The secondary data set is of
eBay and Twitter data: the set contains tweets and re-
views along with an annotated Amazon category. For
our classiﬁcation purposes, we used the 24 main cate-
gories in the Amazon hierarchy, merging the main cat-
egories KindleStore and Books. Then, our classiﬁcation
operates over text (reviews or tweets) labeled by a main
Amazon category.

The data used was saved in JSON format, including
node categorization in the Amazon product hierarchy
for each product. We built a parser and graph creation
codebase that allowed us to extract this hierarchy in the
form of a tree and traverse it for each product, allowing
us to extract labels at any depth. Speciﬁcally, we utilized
the nodes with d = 1 away from the root node – i.e., the
main categories. For the purposes of our investigation
into cross domain text classiﬁcation, we care less about
a ﬁne study of large-scale hierarchical clasiﬁcation and
more about whether or not we can ﬁnd methods that
are valid across the domain – in some sense, a problem
completely independent from the former.

B. Evaluation

We are working in a classiﬁcation space with 24 dif-
ferent groups with highly imbalanced classes. In order
to compensate for this, we utilize F1-scores averaged out
across classes, where

1 = 2 · precision · recall

precision + recall

F (j)

is the F1 score for class j. We compute precision and re-
call for each class in a one-vs-rest approach. Our overall
F1 score (for J classes) is then

J(cid:88)

j=1

F1 =

1
J

F (j)
1 .

More generally, one can imagine a scenario in which we
care about some convex combination denoted by a vector

c, where

J(cid:88)

j=1

F c

1 =

cjF (j)
1 ,

and each cj denotes our “relative cost” of an F1 error
on class j. However, we opt for a standard mean to give
each class equal inﬂuence in terms of how well we predict
its membership. Traditional classiﬁcation error was not
used as this provides us with a better metric given our
imbalances.

It is important to note that a few classes are heav-
ily underrepresented in the data, even considering the
whole dataset. Hence, the average computed as a qual-
ity measure will be aﬀected by these classes, and will
be signiﬁcantly skewed downward, moving the bias away
from well-represented classes.

C. Features

As pure text was the raw input to our model, we uti-
lized a bag-of-words representation of our review texts,
incorporating tf-ifd and removing common stop words to
improve performance and direct our attention to seman-
tically relevant aspects of cross domain language gener-
alization. The features thus corresponded to a sparse
matrix of word frequencies from our selected corpus.
The inverse document frequency (ifd) improved the re-
sults ruling out words commonly present in the corpus.
Moreover, we incorporated an n-gram approach into our
model to better take account of diﬀerences in mean-
ing resulting from contiguous sequences of words. For
n > 1, run-times were longer, but we obtained a slight
increase in test set performance with worse results for
cross-domain classiﬁcation. This is consistent with a pri-
ori intuition regarding these features, as n-grams with
n > 1 provide a very reﬁned look at language structure,
meaning any model learned will not generalize well.

III. CLASSICAL METHODS

As a ﬁrst classiﬁcation step, we selected three well-
known algorithms for the task of text classiﬁcation:
Naive Bayes, Stochastic Gradient Descent and Support
Vector Machines, followed by a grid search / CV for pa-
rameter optimization.
In particular, we utilized Naive
Bayes with Laplacian smoothing and SGD with a modi-
ﬁed huber loss function and α = 0.001. Also, the SVM is
a linear SVM which follows a one-vs-the-rest multiclass
strategy, since other SVMs with one-vs-one strategies
are too time consuming for our space of 24 categories. In
particular, given the constraints of scikit-learn, uti-
lization of any Kernel Machine requires the estimation of

(cid:1) = 276 (in our case) Kernel Machines – a prohibitive

cost.

(cid:0)J

2

During the implementation and evaluation, we no-
ticed that these classically derived “shallow” methods

2

FIG. 1: Confusion matrix for predicting categories in the
Amazon dataset using a SVM with 2-grams bag of words

presented certain bias towards the majority class. Since
Amazon started as a bookstore, the number of products
categorized as books is about 30% of the entire dataset,
with other categories also comprising a large proportion
of the training examples. At the opposite end of the
spectrum, we have a pair of product classes comprising
less than 100 products in total. The problem of training
with such overrepresented classes made a Naive Bayes
classiﬁer simply predict the class “Books” for every given
example.

In order to avoid this training bias, and considering
the large amount of data available, we loaded the whole
dataset for training and testing purposes and selectively
pruned it in order to balance the classes for training.
While this does not solve the underrepresentation of
some classes, it provides more balance to the majority of
them and utilizes all the examples available of the less
common ones. In FIG 1 we can see a confusion matrix
illustrating the distribution of the classes after pruning
and classiﬁcation for the Amazon dataset.

IV. DEEP LEARNING

As our foray into classical methods proved, a simple
application of a classiﬁer is not appropriate for estimat-
ing a model with the expectation of out-of-domain per-
formance. To examine how we can construct a model
that can learn features that can prove useful both in-
side and outside the training domain, we look to deep
learning.
In particular, we look to ways in which we
can provide regularization, as this will help our models
adapt to diﬀerent domains. The challenge in our cho-
sen problem is that we have no prior on the language
distribution in the target domain. A series of struc-
tures / paradigms were decided upon that would be able
to test the relevance of Deep Learning for this partic-
ular cross-domain classiﬁcation problem. We consider
stacked autoencoders of regular and denoising variety,
and a “stacked thesaurus,” as we call it – a novel modiﬁ-
cation to Dense Cohorts of Terms. These are all trained
in an unsupervised fashion, with supervised ﬁne-tuning

3

FIG. 2: Network structure used for Autoencoders

applied later. Since Deep Models are expensive to esti-
mate, we utilize a subset of possible features from tf-idf
matrices, restricting ourselves to the top 500 features for
Autoencoder-based methods.

For all deep learning methods, we use Stochastic Gra-
dient Descent as our training algorithm.
In addition,
we use momentum to speed up training. All learning
rates, momentum parameters, and noising / regulariza-
tion parameters are found using grid search. Using our
own implementation, we rely on scipy/numpy to handle
matrix math.

A. Stacked Autoencoders

An autoencoder is a tranformed linear mapping that
creates a “bottleneck” of dimensionality. Formally, con-
sider a data vector x ∈ RD. Suppose we want to ﬁnd
a representation r ∈ Rd, d < D. An autoencoder
consists of an encoder, which is a pair (W1, b1), with
W1 ∈ Rd×D, b1 ∈ Rd, and a decoder, which is a pair
(W2, b2), with W2 ∈ RD×d, b2 ∈ RD. We also need two
mappings f, g : R −→ R, that can be applied element-
wise to vectors and matricies. The encoder then gener-
ates a new space by the map ϕ(x) = f (W1x + b1), and
the decoder reproduces the original vector by the map
ρ(x) = g(W2ϕ(x) + b2). To ensure this new space repre-
sents the original data vector well, we would like to ﬁnd
Wi, bi such that x ≈ ρθ(x).

B. Stacked Denoising Autoencoders

The basic autoencoder aims to create a lower dimen-
sional coding that contains all information about the
original data vector. Speciﬁcally, over a data batch
X ∈ RD×n, a standard autoencoder deﬁned by the loss
function

L(X; θ) =

1
2n

(cid:107)X − g(W2f (W1X + b11T ) + b21T )(cid:107)2
F ,

has no regularization terms, and aims to build the best
reconstruction possible. However, we have no guarantees
as to whether or not the locally optimal mapping we ﬁnd
to generate this coding is robust – that is, how sharply
does the reconstruction error varies around a given data

FIG. 3: Error Curves for training – ﬁrst layer of deep model.

vector x.
In order to force the autoencoders we stack
to construct robust representations, we require that the
autoencoders be able to reconstruct a clean version of x
from a corrupted version x + δx. Speciﬁcally, this is a
denoising requirement, and acts as a regularizer to local
changes in distributions.

The denoising autoencoder is deﬁned by the loss func-

tion

L(X; θ) =

1
2n

(cid:107)X − g(W2f (W1 ˜X + b11T ) + b21T )(cid:107)2
F ,

where ˜X ∼ P ( ˜X|X) is a stochastic, corrupting map-

ping. Common examples of corruption mappings are

• P ( ˜X|X) = X (cid:12) B, Bij ∼ Bernoulli(p) (i.e., a bi-
nary mask)
• P ( ˜X|X) = X+N(0,ε2), where [N(0,ε2)]ij ∼ N (0, ε2)

We greedily train each denoising autoencoder, stacking
as we go along. At the top level of such a stack, we hope
to have a set of features that is both robust in construc-
tion and captures correlations between words that are
useful across domains.

Now, we examine the training procedure of the ﬁrst
layer, using diﬀerent variants of Autoencoders with
diﬀerent regularization procedures to understand what
each layer will accomplish. Consider FIG. 3. We see
that we have better test sample performance with denois-
ing autoencoders with gaussian noise when compared
with the normal-type autoencoders. This makes sense
– our generalization to examples should be better with
imposed denoising criteria. We proceed with a similar
training procedure layer-by-layer – helping the network
learn a stacked, hierarchical representation of our train-
ing sample.

C. Stacked Thesauri

We present a modiﬁcation of Dense Cohorts of Terms,
as introduced in [5]. In particular, we aim to reconstruct
smaller and smaller subsets of our vocabulary in each

Input Layer (500 units)(400 units)(250 units)(50 units)(24 units)02004006008001000Epochs0.10.20.30.40.50.60.70.80.91.0Reconstruction ErrorTraining Procedure, first layer (500×400)Bernoulli Noise DAE, Train ErrorBernoulli Noise DAE, Test ErrorNormal Noise DAE, Train ErrorNormal Noise DAE, Test ErrorAE Train ErrorAE Test ErrorCross-Domain Product Classiﬁcation with Deep Learning

Luke de Oliveira,1 Alfredo L´ainez Rodrigo,1 and Akua Abu1

1Stanford University ICME, CS229 Final Project

In our ﬁnal project, we took on the challenge of cross-domain classiﬁcation–the adaptation of a
model suitable for one domain to one or more diﬀerent domains with identical features. Speciﬁcally,
we developed a cross-domain classiﬁcation schema to predict product categories in tweets and eBay
reviews based on user reviews from Amazon. We ﬁrst developed a set of classical machine learning
classiﬁers that performed reasonably well on our principal data set of Amazon products and then
applied the same classiﬁers to predict product categories from Twitter tweets and eBay reviews. We
then developed Deep Learning models to approach to our cross-domain classiﬁcation problem and
compared performance. In what follows, we walk step by step through the procedure, illustrate our
results, then discuss future work. We show that while Deep Learning deﬁnitively helps models work
out-of-domain, considerable work needs to be done to create a context-free model of language that
can work across domains.

I.

INTRODUCTION

There is no longer a boundary between social media,
advertising, and consumer culture. The intersection of
these diﬀerent spaces has resulted in an immesurable in-
crease in the amount and prevalence of user data. Users
develop online identities that span across many plat-
forms, from Facebook to Twitter to YouTube and to
consumer communities such as eBay and Amazon. An
open problem of great relevance to these overlapping so-
cial spheres is that of cross-domain learning – that is,
given an estimated model suitable for one domain, can
the model perform in an entirely diﬀerent context given
identical features? Using a dataset of Amazon prod-
ucts labeled within the Amazon hierarchical categories,
we have developed a set of classiﬁers that predict prod-
uct categories based on users’ reviews. Then, we apply
the same predictors of user content to diﬀerent domains
to see how these models behaves in a diﬀerent context
over diﬀerent underlying distributions of features. The
key challenge is that we impose no prior over the tar-
get distribution – we simply aim to develop a model
that is not entirely dependent on a training distribu-
tion. Speciﬁcally, we have predicted product categoriza-
tion using tweets from Twitter and reviews from eBay,
and we determine the validity of an out-of-context clas-
siﬁer using carefully chosen metrics. Finally, we have
engineered two Deep Learning models, and use them to
illustrate a potential new application for Deep Architec-
tures – we outperform classical approaches in this task,
showing adaptability to new domains.

II. SETUP

A. Data

This project makes use of a principal data set of Ama-
zon product reviews and an Amazon product hierar-
chy. Secondary data sets include product-related Twit-
ter data (tweets), and ebay product reviews – each of
which are hand labeled. The main Amazon data sets
consists of descriptions and 10 reviews for each of the

top 10 products in every Amazon product category, com-
prising a hierarchy of roughly 400,000 products ranging
across 40,000 categories. The secondary data set is of
eBay and Twitter data: the set contains tweets and re-
views along with an annotated Amazon category. For
our classiﬁcation purposes, we used the 24 main cate-
gories in the Amazon hierarchy, merging the main cat-
egories KindleStore and Books. Then, our classiﬁcation
operates over text (reviews or tweets) labeled by a main
Amazon category.

The data used was saved in JSON format, including
node categorization in the Amazon product hierarchy
for each product. We built a parser and graph creation
codebase that allowed us to extract this hierarchy in the
form of a tree and traverse it for each product, allowing
us to extract labels at any depth. Speciﬁcally, we utilized
the nodes with d = 1 away from the root node – i.e., the
main categories. For the purposes of our investigation
into cross domain text classiﬁcation, we care less about
a ﬁne study of large-scale hierarchical clasiﬁcation and
more about whether or not we can ﬁnd methods that
are valid across the domain – in some sense, a problem
completely independent from the former.

B. Evaluation

We are working in a classiﬁcation space with 24 dif-
ferent groups with highly imbalanced classes. In order
to compensate for this, we utilize F1-scores averaged out
across classes, where

1 = 2 · precision · recall

precision + recall

F (j)

is the F1 score for class j. We compute precision and re-
call for each class in a one-vs-rest approach. Our overall
F1 score (for J classes) is then

J(cid:88)

j=1

F1 =

1
J

F (j)
1 .

More generally, one can imagine a scenario in which we
care about some convex combination denoted by a vector

c, where

J(cid:88)

j=1

F c

1 =

cjF (j)
1 ,

and each cj denotes our “relative cost” of an F1 error
on class j. However, we opt for a standard mean to give
each class equal inﬂuence in terms of how well we predict
its membership. Traditional classiﬁcation error was not
used as this provides us with a better metric given our
imbalances.

It is important to note that a few classes are heav-
ily underrepresented in the data, even considering the
whole dataset. Hence, the average computed as a qual-
ity measure will be aﬀected by these classes, and will
be signiﬁcantly skewed downward, moving the bias away
from well-represented classes.

C. Features

As pure text was the raw input to our model, we uti-
lized a bag-of-words representation of our review texts,
incorporating tf-ifd and removing common stop words to
improve performance and direct our attention to seman-
tically relevant aspects of cross domain language gener-
alization. The features thus corresponded to a sparse
matrix of word frequencies from our selected corpus.
The inverse document frequency (ifd) improved the re-
sults ruling out words commonly present in the corpus.
Moreover, we incorporated an n-gram approach into our
model to better take account of diﬀerences in mean-
ing resulting from contiguous sequences of words. For
n > 1, run-times were longer, but we obtained a slight
increase in test set performance with worse results for
cross-domain classiﬁcation. This is consistent with a pri-
ori intuition regarding these features, as n-grams with
n > 1 provide a very reﬁned look at language structure,
meaning any model learned will not generalize well.

III. CLASSICAL METHODS

As a ﬁrst classiﬁcation step, we selected three well-
known algorithms for the task of text classiﬁcation:
Naive Bayes, Stochastic Gradient Descent and Support
Vector Machines, followed by a grid search / CV for pa-
rameter optimization.
In particular, we utilized Naive
Bayes with Laplacian smoothing and SGD with a modi-
ﬁed huber loss function and α = 0.001. Also, the SVM is
a linear SVM which follows a one-vs-the-rest multiclass
strategy, since other SVMs with one-vs-one strategies
are too time consuming for our space of 24 categories. In
particular, given the constraints of scikit-learn, uti-
lization of any Kernel Machine requires the estimation of

(cid:1) = 276 (in our case) Kernel Machines – a prohibitive

cost.

(cid:0)J

2

During the implementation and evaluation, we no-
ticed that these classically derived “shallow” methods

2

FIG. 1: Confusion matrix for predicting categories in the
Amazon dataset using a SVM with 2-grams bag of words

presented certain bias towards the majority class. Since
Amazon started as a bookstore, the number of products
categorized as books is about 30% of the entire dataset,
with other categories also comprising a large proportion
of the training examples. At the opposite end of the
spectrum, we have a pair of product classes comprising
less than 100 products in total. The problem of training
with such overrepresented classes made a Naive Bayes
classiﬁer simply predict the class “Books” for every given
example.

In order to avoid this training bias, and considering
the large amount of data available, we loaded the whole
dataset for training and testing purposes and selectively
pruned it in order to balance the classes for training.
While this does not solve the underrepresentation of
some classes, it provides more balance to the majority of
them and utilizes all the examples available of the less
common ones. In FIG 1 we can see a confusion matrix
illustrating the distribution of the classes after pruning
and classiﬁcation for the Amazon dataset.

IV. DEEP LEARNING

As our foray into classical methods proved, a simple
application of a classiﬁer is not appropriate for estimat-
ing a model with the expectation of out-of-domain per-
formance. To examine how we can construct a model
that can learn features that can prove useful both in-
side and outside the training domain, we look to deep
learning.
In particular, we look to ways in which we
can provide regularization, as this will help our models
adapt to diﬀerent domains. The challenge in our cho-
sen problem is that we have no prior on the language
distribution in the target domain. A series of struc-
tures / paradigms were decided upon that would be able
to test the relevance of Deep Learning for this partic-
ular cross-domain classiﬁcation problem. We consider
stacked autoencoders of regular and denoising variety,
and a “stacked thesaurus,” as we call it – a novel modiﬁ-
cation to Dense Cohorts of Terms. These are all trained
in an unsupervised fashion, with supervised ﬁne-tuning

3

FIG. 2: Network structure used for Autoencoders

applied later. Since Deep Models are expensive to esti-
mate, we utilize a subset of possible features from tf-idf
matrices, restricting ourselves to the top 500 features for
Autoencoder-based methods.

For all deep learning methods, we use Stochastic Gra-
dient Descent as our training algorithm.
In addition,
we use momentum to speed up training. All learning
rates, momentum parameters, and noising / regulariza-
tion parameters are found using grid search. Using our
own implementation, we rely on scipy/numpy to handle
matrix math.

A. Stacked Autoencoders

An autoencoder is a tranformed linear mapping that
creates a “bottleneck” of dimensionality. Formally, con-
sider a data vector x ∈ RD. Suppose we want to ﬁnd
a representation r ∈ Rd, d < D. An autoencoder
consists of an encoder, which is a pair (W1, b1), with
W1 ∈ Rd×D, b1 ∈ Rd, and a decoder, which is a pair
(W2, b2), with W2 ∈ RD×d, b2 ∈ RD. We also need two
mappings f, g : R −→ R, that can be applied element-
wise to vectors and matricies. The encoder then gener-
ates a new space by the map ϕ(x) = f (W1x + b1), and
the decoder reproduces the original vector by the map
ρ(x) = g(W2ϕ(x) + b2). To ensure this new space repre-
sents the original data vector well, we would like to ﬁnd
Wi, bi such that x ≈ ρθ(x).

B. Stacked Denoising Autoencoders

The basic autoencoder aims to create a lower dimen-
sional coding that contains all information about the
original data vector. Speciﬁcally, over a data batch
X ∈ RD×n, a standard autoencoder deﬁned by the loss
function

L(X; θ) =

1
2n

(cid:107)X − g(W2f (W1X + b11T ) + b21T )(cid:107)2
F ,

has no regularization terms, and aims to build the best
reconstruction possible. However, we have no guarantees
as to whether or not the locally optimal mapping we ﬁnd
to generate this coding is robust – that is, how sharply
does the reconstruction error varies around a given data

FIG. 3: Error Curves for training – ﬁrst layer of deep model.

vector x.
In order to force the autoencoders we stack
to construct robust representations, we require that the
autoencoders be able to reconstruct a clean version of x
from a corrupted version x + δx. Speciﬁcally, this is a
denoising requirement, and acts as a regularizer to local
changes in distributions.

The denoising autoencoder is deﬁned by the loss func-

tion

L(X; θ) =

1
2n

(cid:107)X − g(W2f (W1 ˜X + b11T ) + b21T )(cid:107)2
F ,

where ˜X ∼ P ( ˜X|X) is a stochastic, corrupting map-

ping. Common examples of corruption mappings are

• P ( ˜X|X) = X (cid:12) B, Bij ∼ Bernoulli(p) (i.e., a bi-
nary mask)
• P ( ˜X|X) = X+N(0,ε2), where [N(0,ε2)]ij ∼ N (0, ε2)

We greedily train each denoising autoencoder, stacking
as we go along. At the top level of such a stack, we hope
to have a set of features that is both robust in construc-
tion and captures correlations between words that are
useful across domains.

Now, we examine the training procedure of the ﬁrst
layer, using diﬀerent variants of Autoencoders with
diﬀerent regularization procedures to understand what
each layer will accomplish. Consider FIG. 3. We see
that we have better test sample performance with denois-
ing autoencoders with gaussian noise when compared
with the normal-type autoencoders. This makes sense
– our generalization to examples should be better with
imposed denoising criteria. We proceed with a similar
training procedure layer-by-layer – helping the network
learn a stacked, hierarchical representation of our train-
ing sample.

C. Stacked Thesauri

We present a modiﬁcation of Dense Cohorts of Terms,
as introduced in [5]. In particular, we aim to reconstruct
smaller and smaller subsets of our vocabulary in each

Input Layer (500 units)(400 units)(250 units)(50 units)(24 units)02004006008001000Epochs0.10.20.30.40.50.60.70.80.91.0Reconstruction ErrorTraining Procedure, first layer (500×400)Bernoulli Noise DAE, Train ErrorBernoulli Noise DAE, Test ErrorNormal Noise DAE, Train ErrorNormal Noise DAE, Test ErrorAE Train ErrorAE Test Errorlayer, corresponding to higher and higher levels of se-
mantic meaning. Instead of creating a coding that con-
tains all information contained in data vectors, we in-
stead want to create a mapping that ﬁnds ways to map
corrupted, rare words into a set of more common words,
in a sense acting as a thesaurus. Let our vocabulary
be of size D, and assume we obtained a Bag-of-Words
representation X ∈ Rn×D, there all nonzero entries have
value 1 for ease. Now, suppose we want a Deep Model to
learn what it means for words to be synonyms – that is,
can we link many uncommon words such as “exquisite,”
“marvelous,” and “impeccable” to a more common word
like “good”?

We take the top R < D words in terms of frequency,
and construct a (binary) Bag-of-Words representation
P(R) ∈ Rn×R. Now, we ask – can we ﬁnd a mapping
such that σ(XW1) ≈ P(R), where σ(·) is the standard
sigmoid? In order to be resistant to any diﬀerences in
distribution, we can corrupt X with Bernoulli noise –
that is, we set each non-bias feature to zero with some
probability p.
To stack, we use the output of the previous layer, call
it Z ∈ (0, 1)m×R, as input, and we simply now choose
a smaller target “Thesaurus” size – let this be R(cid:48). We
now wish to ﬁnd a mapping σ(ZW2) ≈ P(R(cid:48)). We can
repeat this in a greedy manner until we have the desired
structure We utilize the same structure as outlined in
FIG. 2.

V. RESULTS

For the following results, the classiﬁers were trained
on a total set of 400,000 training examples and 100,000
test examples. The data was pruned in a preprocessing
step from an even bigger dataset in order to reduce class
imbalance, as explained before. The cross-domain sets
contain 62,000 tweets and 33,000 eBay reviews.

Method Train Test eBay Twitter

Naive Bayes 0.54
0.54
0.77

SGD
SVM

0.51 0.31
0.53 0.30
0.67
0.29

0.18
0.15
0.16

TABLE I: F1-scores for classical methods

In TABLE I, we can see how the best Amazon train-
ing and Amazon test results are yielded by the most
sophisticated algorithm, the Linear support vector ma-
chine. However, we take note how this improvement do
not translate cross-domain, where the performance even
decreases. This is a clear signal of the fact that there are
distinct characteristics in diﬀerent text domains. In FIG.
4 we can appreciate how the F1 classiﬁcation scores for
the domains in eBay and twitter stop increasing rather
soon, even worsening when the models are ﬁtting better
the training domain.

Next, consider results (TABLE II) from Deep Learning
using the aforementioned pretraining methods – Stacked

4

FIG. 4: Learning curve for the linear SVM

Autoencoders (SAE), Stacked Denoising Autoencoders
(SdAE), and Stacked Thesauri (SThi).

Simple Softmax

Fine-tuning

Method Train Test eBay Twitter

0.85 0.801 0.31
SAE
0.83 0.371
0.85
SdAE
0.81 0.341
0.84
SThi
SAE
0.871 0.82
0.29
SdAE 0.862 0.831 0.36
0.849 0.827 0.381
SThi

0.198
0.221
0.202
0.15
0.21
0.229

TABLE II: F1-scores for Deep Learning

VI. DISCUSSION

It is clear from our results that it is not possible to fully
extrapolate a model to a diﬀerent domain and expect
similar behavior. While the combination of our tech-
niques did allow for accurate cross-domain classiﬁcation
for a few subsets of product reviews, we were not able to
develop a completely accurate model that could be ex-
trapolated across all domains. Among classical classiﬁers
used, support vector machine produced the strongest re-
sults on the original data set; however, as with the other
classiﬁers, cross-domain results were relatively weak.

Particularly we experienced particularly low perfor-
mance for the Twitter data set, which can be partially
attributed to the inability of our model to fully take into
account the slang-based vocabulary. While Amazon re-
views tend to be reasoned opinions about a product,
tweets are extremely short texts using a very particu-
lar vocabulary, many times utilizing hashtags and var-
ied abbreviations – semantic units that were in no way
present over the data the model was trained over. More
surprising is the diﬀerence in the performance of the clas-
siﬁers in the realm of eBay product reviews, where one
would suppose a strong resemblance to those of Amazon.
However, from the results, and assuming that the data

Cross-Domain Product Classiﬁcation with Deep Learning

Luke de Oliveira,1 Alfredo L´ainez Rodrigo,1 and Akua Abu1

1Stanford University ICME, CS229 Final Project

In our ﬁnal project, we took on the challenge of cross-domain classiﬁcation–the adaptation of a
model suitable for one domain to one or more diﬀerent domains with identical features. Speciﬁcally,
we developed a cross-domain classiﬁcation schema to predict product categories in tweets and eBay
reviews based on user reviews from Amazon. We ﬁrst developed a set of classical machine learning
classiﬁers that performed reasonably well on our principal data set of Amazon products and then
applied the same classiﬁers to predict product categories from Twitter tweets and eBay reviews. We
then developed Deep Learning models to approach to our cross-domain classiﬁcation problem and
compared performance. In what follows, we walk step by step through the procedure, illustrate our
results, then discuss future work. We show that while Deep Learning deﬁnitively helps models work
out-of-domain, considerable work needs to be done to create a context-free model of language that
can work across domains.

I.

INTRODUCTION

There is no longer a boundary between social media,
advertising, and consumer culture. The intersection of
these diﬀerent spaces has resulted in an immesurable in-
crease in the amount and prevalence of user data. Users
develop online identities that span across many plat-
forms, from Facebook to Twitter to YouTube and to
consumer communities such as eBay and Amazon. An
open problem of great relevance to these overlapping so-
cial spheres is that of cross-domain learning – that is,
given an estimated model suitable for one domain, can
the model perform in an entirely diﬀerent context given
identical features? Using a dataset of Amazon prod-
ucts labeled within the Amazon hierarchical categories,
we have developed a set of classiﬁers that predict prod-
uct categories based on users’ reviews. Then, we apply
the same predictors of user content to diﬀerent domains
to see how these models behaves in a diﬀerent context
over diﬀerent underlying distributions of features. The
key challenge is that we impose no prior over the tar-
get distribution – we simply aim to develop a model
that is not entirely dependent on a training distribu-
tion. Speciﬁcally, we have predicted product categoriza-
tion using tweets from Twitter and reviews from eBay,
and we determine the validity of an out-of-context clas-
siﬁer using carefully chosen metrics. Finally, we have
engineered two Deep Learning models, and use them to
illustrate a potential new application for Deep Architec-
tures – we outperform classical approaches in this task,
showing adaptability to new domains.

II. SETUP

A. Data

This project makes use of a principal data set of Ama-
zon product reviews and an Amazon product hierar-
chy. Secondary data sets include product-related Twit-
ter data (tweets), and ebay product reviews – each of
which are hand labeled. The main Amazon data sets
consists of descriptions and 10 reviews for each of the

top 10 products in every Amazon product category, com-
prising a hierarchy of roughly 400,000 products ranging
across 40,000 categories. The secondary data set is of
eBay and Twitter data: the set contains tweets and re-
views along with an annotated Amazon category. For
our classiﬁcation purposes, we used the 24 main cate-
gories in the Amazon hierarchy, merging the main cat-
egories KindleStore and Books. Then, our classiﬁcation
operates over text (reviews or tweets) labeled by a main
Amazon category.

The data used was saved in JSON format, including
node categorization in the Amazon product hierarchy
for each product. We built a parser and graph creation
codebase that allowed us to extract this hierarchy in the
form of a tree and traverse it for each product, allowing
us to extract labels at any depth. Speciﬁcally, we utilized
the nodes with d = 1 away from the root node – i.e., the
main categories. For the purposes of our investigation
into cross domain text classiﬁcation, we care less about
a ﬁne study of large-scale hierarchical clasiﬁcation and
more about whether or not we can ﬁnd methods that
are valid across the domain – in some sense, a problem
completely independent from the former.

B. Evaluation

We are working in a classiﬁcation space with 24 dif-
ferent groups with highly imbalanced classes. In order
to compensate for this, we utilize F1-scores averaged out
across classes, where

1 = 2 · precision · recall

precision + recall

F (j)

is the F1 score for class j. We compute precision and re-
call for each class in a one-vs-rest approach. Our overall
F1 score (for J classes) is then

J(cid:88)

j=1

F1 =

1
J

F (j)
1 .

More generally, one can imagine a scenario in which we
care about some convex combination denoted by a vector

c, where

J(cid:88)

j=1

F c

1 =

cjF (j)
1 ,

and each cj denotes our “relative cost” of an F1 error
on class j. However, we opt for a standard mean to give
each class equal inﬂuence in terms of how well we predict
its membership. Traditional classiﬁcation error was not
used as this provides us with a better metric given our
imbalances.

It is important to note that a few classes are heav-
ily underrepresented in the data, even considering the
whole dataset. Hence, the average computed as a qual-
ity measure will be aﬀected by these classes, and will
be signiﬁcantly skewed downward, moving the bias away
from well-represented classes.

C. Features

As pure text was the raw input to our model, we uti-
lized a bag-of-words representation of our review texts,
incorporating tf-ifd and removing common stop words to
improve performance and direct our attention to seman-
tically relevant aspects of cross domain language gener-
alization. The features thus corresponded to a sparse
matrix of word frequencies from our selected corpus.
The inverse document frequency (ifd) improved the re-
sults ruling out words commonly present in the corpus.
Moreover, we incorporated an n-gram approach into our
model to better take account of diﬀerences in mean-
ing resulting from contiguous sequences of words. For
n > 1, run-times were longer, but we obtained a slight
increase in test set performance with worse results for
cross-domain classiﬁcation. This is consistent with a pri-
ori intuition regarding these features, as n-grams with
n > 1 provide a very reﬁned look at language structure,
meaning any model learned will not generalize well.

III. CLASSICAL METHODS

As a ﬁrst classiﬁcation step, we selected three well-
known algorithms for the task of text classiﬁcation:
Naive Bayes, Stochastic Gradient Descent and Support
Vector Machines, followed by a grid search / CV for pa-
rameter optimization.
In particular, we utilized Naive
Bayes with Laplacian smoothing and SGD with a modi-
ﬁed huber loss function and α = 0.001. Also, the SVM is
a linear SVM which follows a one-vs-the-rest multiclass
strategy, since other SVMs with one-vs-one strategies
are too time consuming for our space of 24 categories. In
particular, given the constraints of scikit-learn, uti-
lization of any Kernel Machine requires the estimation of

(cid:1) = 276 (in our case) Kernel Machines – a prohibitive

cost.

(cid:0)J

2

During the implementation and evaluation, we no-
ticed that these classically derived “shallow” methods

2

FIG. 1: Confusion matrix for predicting categories in the
Amazon dataset using a SVM with 2-grams bag of words

presented certain bias towards the majority class. Since
Amazon started as a bookstore, the number of products
categorized as books is about 30% of the entire dataset,
with other categories also comprising a large proportion
of the training examples. At the opposite end of the
spectrum, we have a pair of product classes comprising
less than 100 products in total. The problem of training
with such overrepresented classes made a Naive Bayes
classiﬁer simply predict the class “Books” for every given
example.

In order to avoid this training bias, and considering
the large amount of data available, we loaded the whole
dataset for training and testing purposes and selectively
pruned it in order to balance the classes for training.
While this does not solve the underrepresentation of
some classes, it provides more balance to the majority of
them and utilizes all the examples available of the less
common ones. In FIG 1 we can see a confusion matrix
illustrating the distribution of the classes after pruning
and classiﬁcation for the Amazon dataset.

IV. DEEP LEARNING

As our foray into classical methods proved, a simple
application of a classiﬁer is not appropriate for estimat-
ing a model with the expectation of out-of-domain per-
formance. To examine how we can construct a model
that can learn features that can prove useful both in-
side and outside the training domain, we look to deep
learning.
In particular, we look to ways in which we
can provide regularization, as this will help our models
adapt to diﬀerent domains. The challenge in our cho-
sen problem is that we have no prior on the language
distribution in the target domain. A series of struc-
tures / paradigms were decided upon that would be able
to test the relevance of Deep Learning for this partic-
ular cross-domain classiﬁcation problem. We consider
stacked autoencoders of regular and denoising variety,
and a “stacked thesaurus,” as we call it – a novel modiﬁ-
cation to Dense Cohorts of Terms. These are all trained
in an unsupervised fashion, with supervised ﬁne-tuning

3

FIG. 2: Network structure used for Autoencoders

applied later. Since Deep Models are expensive to esti-
mate, we utilize a subset of possible features from tf-idf
matrices, restricting ourselves to the top 500 features for
Autoencoder-based methods.

For all deep learning methods, we use Stochastic Gra-
dient Descent as our training algorithm.
In addition,
we use momentum to speed up training. All learning
rates, momentum parameters, and noising / regulariza-
tion parameters are found using grid search. Using our
own implementation, we rely on scipy/numpy to handle
matrix math.

A. Stacked Autoencoders

An autoencoder is a tranformed linear mapping that
creates a “bottleneck” of dimensionality. Formally, con-
sider a data vector x ∈ RD. Suppose we want to ﬁnd
a representation r ∈ Rd, d < D. An autoencoder
consists of an encoder, which is a pair (W1, b1), with
W1 ∈ Rd×D, b1 ∈ Rd, and a decoder, which is a pair
(W2, b2), with W2 ∈ RD×d, b2 ∈ RD. We also need two
mappings f, g : R −→ R, that can be applied element-
wise to vectors and matricies. The encoder then gener-
ates a new space by the map ϕ(x) = f (W1x + b1), and
the decoder reproduces the original vector by the map
ρ(x) = g(W2ϕ(x) + b2). To ensure this new space repre-
sents the original data vector well, we would like to ﬁnd
Wi, bi such that x ≈ ρθ(x).

B. Stacked Denoising Autoencoders

The basic autoencoder aims to create a lower dimen-
sional coding that contains all information about the
original data vector. Speciﬁcally, over a data batch
X ∈ RD×n, a standard autoencoder deﬁned by the loss
function

L(X; θ) =

1
2n

(cid:107)X − g(W2f (W1X + b11T ) + b21T )(cid:107)2
F ,

has no regularization terms, and aims to build the best
reconstruction possible. However, we have no guarantees
as to whether or not the locally optimal mapping we ﬁnd
to generate this coding is robust – that is, how sharply
does the reconstruction error varies around a given data

FIG. 3: Error Curves for training – ﬁrst layer of deep model.

vector x.
In order to force the autoencoders we stack
to construct robust representations, we require that the
autoencoders be able to reconstruct a clean version of x
from a corrupted version x + δx. Speciﬁcally, this is a
denoising requirement, and acts as a regularizer to local
changes in distributions.

The denoising autoencoder is deﬁned by the loss func-

tion

L(X; θ) =

1
2n

(cid:107)X − g(W2f (W1 ˜X + b11T ) + b21T )(cid:107)2
F ,

where ˜X ∼ P ( ˜X|X) is a stochastic, corrupting map-

ping. Common examples of corruption mappings are

• P ( ˜X|X) = X (cid:12) B, Bij ∼ Bernoulli(p) (i.e., a bi-
nary mask)
• P ( ˜X|X) = X+N(0,ε2), where [N(0,ε2)]ij ∼ N (0, ε2)

We greedily train each denoising autoencoder, stacking
as we go along. At the top level of such a stack, we hope
to have a set of features that is both robust in construc-
tion and captures correlations between words that are
useful across domains.

Now, we examine the training procedure of the ﬁrst
layer, using diﬀerent variants of Autoencoders with
diﬀerent regularization procedures to understand what
each layer will accomplish. Consider FIG. 3. We see
that we have better test sample performance with denois-
ing autoencoders with gaussian noise when compared
with the normal-type autoencoders. This makes sense
– our generalization to examples should be better with
imposed denoising criteria. We proceed with a similar
training procedure layer-by-layer – helping the network
learn a stacked, hierarchical representation of our train-
ing sample.

C. Stacked Thesauri

We present a modiﬁcation of Dense Cohorts of Terms,
as introduced in [5]. In particular, we aim to reconstruct
smaller and smaller subsets of our vocabulary in each

Input Layer (500 units)(400 units)(250 units)(50 units)(24 units)02004006008001000Epochs0.10.20.30.40.50.60.70.80.91.0Reconstruction ErrorTraining Procedure, first layer (500×400)Bernoulli Noise DAE, Train ErrorBernoulli Noise DAE, Test ErrorNormal Noise DAE, Train ErrorNormal Noise DAE, Test ErrorAE Train ErrorAE Test Errorlayer, corresponding to higher and higher levels of se-
mantic meaning. Instead of creating a coding that con-
tains all information contained in data vectors, we in-
stead want to create a mapping that ﬁnds ways to map
corrupted, rare words into a set of more common words,
in a sense acting as a thesaurus. Let our vocabulary
be of size D, and assume we obtained a Bag-of-Words
representation X ∈ Rn×D, there all nonzero entries have
value 1 for ease. Now, suppose we want a Deep Model to
learn what it means for words to be synonyms – that is,
can we link many uncommon words such as “exquisite,”
“marvelous,” and “impeccable” to a more common word
like “good”?

We take the top R < D words in terms of frequency,
and construct a (binary) Bag-of-Words representation
P(R) ∈ Rn×R. Now, we ask – can we ﬁnd a mapping
such that σ(XW1) ≈ P(R), where σ(·) is the standard
sigmoid? In order to be resistant to any diﬀerences in
distribution, we can corrupt X with Bernoulli noise –
that is, we set each non-bias feature to zero with some
probability p.
To stack, we use the output of the previous layer, call
it Z ∈ (0, 1)m×R, as input, and we simply now choose
a smaller target “Thesaurus” size – let this be R(cid:48). We
now wish to ﬁnd a mapping σ(ZW2) ≈ P(R(cid:48)). We can
repeat this in a greedy manner until we have the desired
structure We utilize the same structure as outlined in
FIG. 2.

V. RESULTS

For the following results, the classiﬁers were trained
on a total set of 400,000 training examples and 100,000
test examples. The data was pruned in a preprocessing
step from an even bigger dataset in order to reduce class
imbalance, as explained before. The cross-domain sets
contain 62,000 tweets and 33,000 eBay reviews.

Method Train Test eBay Twitter

Naive Bayes 0.54
0.54
0.77

SGD
SVM

0.51 0.31
0.53 0.30
0.67
0.29

0.18
0.15
0.16

TABLE I: F1-scores for classical methods

In TABLE I, we can see how the best Amazon train-
ing and Amazon test results are yielded by the most
sophisticated algorithm, the Linear support vector ma-
chine. However, we take note how this improvement do
not translate cross-domain, where the performance even
decreases. This is a clear signal of the fact that there are
distinct characteristics in diﬀerent text domains. In FIG.
4 we can appreciate how the F1 classiﬁcation scores for
the domains in eBay and twitter stop increasing rather
soon, even worsening when the models are ﬁtting better
the training domain.

Next, consider results (TABLE II) from Deep Learning
using the aforementioned pretraining methods – Stacked

4

FIG. 4: Learning curve for the linear SVM

Autoencoders (SAE), Stacked Denoising Autoencoders
(SdAE), and Stacked Thesauri (SThi).

Simple Softmax

Fine-tuning

Method Train Test eBay Twitter

0.85 0.801 0.31
SAE
0.83 0.371
0.85
SdAE
0.81 0.341
0.84
SThi
SAE
0.871 0.82
0.29
SdAE 0.862 0.831 0.36
0.849 0.827 0.381
SThi

0.198
0.221
0.202
0.15
0.21
0.229

TABLE II: F1-scores for Deep Learning

VI. DISCUSSION

It is clear from our results that it is not possible to fully
extrapolate a model to a diﬀerent domain and expect
similar behavior. While the combination of our tech-
niques did allow for accurate cross-domain classiﬁcation
for a few subsets of product reviews, we were not able to
develop a completely accurate model that could be ex-
trapolated across all domains. Among classical classiﬁers
used, support vector machine produced the strongest re-
sults on the original data set; however, as with the other
classiﬁers, cross-domain results were relatively weak.

Particularly we experienced particularly low perfor-
mance for the Twitter data set, which can be partially
attributed to the inability of our model to fully take into
account the slang-based vocabulary. While Amazon re-
views tend to be reasoned opinions about a product,
tweets are extremely short texts using a very particu-
lar vocabulary, many times utilizing hashtags and var-
ied abbreviations – semantic units that were in no way
present over the data the model was trained over. More
surprising is the diﬀerence in the performance of the clas-
siﬁers in the realm of eBay product reviews, where one
would suppose a strong resemblance to those of Amazon.
However, from the results, and assuming that the data

is correctly labeled, we can conclude that the users vo-
cabulary when reviewing products in these domains are
diﬀerent.

It is important to note that Deep Learning did non-
negligibly improve the performance of out classiﬁers on
the original data set while also increasing results on the
cross-domain data sets – most notably, the eBay data
set – when compared to “shallow” counterparts.

Our results from deep learning ﬁt nicely into our be-
liefs with respect to how we think regularization will
help out model extrapolate to a priorless, unseen do-
main. In particular, note that pure SAE with ﬁne tuning
performed best on the training set of Amazon – an un-
surprising fact, seeing as though SAE try to ﬁt an exact
reconstruction during training time. Note that with this
excellent training set performance comes serious diﬃcul-
ties generalizing to our cross-domain targets – in partic-
ular, we notice in TABLE II that we obtain F1 scores
that are just as bad as the classical Linear SVM.

However, we make note of what happens when we
impose the restriction that our model must denoise as
well as reconstruct. In particular, we notice that though
our Amazon training set accuracy decreases, our per-
formance on the Amazon testing set increases. In this
regime, it is impossible to overtrain the pretrained model
– every training example is noised in a diﬀerent man-
ner each time. Also noteworthy is the increase in cross-
domain performance, though we obtain better general-
izability by simply appending and training a softmax
layer.

Most interesting, we see that our Stacked Thesauri
yield the best cross-domain performance. We postu-
late that this is due to the fact that the representa-
tions learned by SThi are dense in commonly used terms,
which (we believe) are more constant across the domains
we considered than the language distribution at large.
The ability to encode synonyms allows greater ﬂexibility
when dealing with limited vocabularies, and reduces re-
liance on product-speciﬁc jargon or vernacular to make
predictions.

5

zon product hierarchy. Moreover, our project could be
extended in a follow-up study to detect diﬀerences in
the vocabularies used in the diﬀerent domains, and ﬁne-
tune our approaches to overcome these diﬀerences and
increase cross-domain classiﬁcation performance. In ad-
dition, it would be interesting to see if we could design
some sort of generative model related to Conditional Re-
stricted Boltzmann Machines which would help us con-
dition on small subsets of the distribution as we learn
more about the diﬀerent domains we apply to.

We could also modify our principal data set, perhaps
training on the domains with more speciﬁc vernacular
such as Twitter with its slang-based vocabulary. In ad-
dition, we would be interested in using models such as
the over-replicated softmax model as an alternative ap-
proach to feature engineering.

VIII. CONCLUSION

We examined the cross-domain classiﬁcation of on-
line user behavior in terms of text.
In particular, we
asked: can we use labeled Amazon data to extrapolate
out-of-domain and determine what products individu-
als in Twitter and eBay are discussing? While classical
methods performed relatively well on the original data
set, they were inherently unable to translate this perfor-
mance to the cross-domain problem. Our deep models
with ﬁne-tuning provided a considerably higher degree
of accuracy on the original data set and on the cross-
domain problem.
In particular, we showed promising
results using Stacked Thesauri – a method that relies
on stacking layers combining semantically similar words.
However, we have found that without prior knowledge
of the language distribution, it is a diﬃcult task to ex-
trapolate machine learning models to diﬀerent domains,
even with the powerful and generalizable toolbox of deep
learning.

VII. FUTURE WORK

IX. ACKNOWLEDGEMENTS

Firstly, with greater computing resources, it would be
interesting to improve the eﬃciency of our schema and
apply it to the complete set of labels from the Ama-

We wish to acknowledge Professor Ashutosh Saxena
and Aditya Jami for the very interesting project idea
and for providing the data used.

[1] Sha Fei Gong Boqing and Kristen Grauman. Overcom-
ing Dataset Bias: An unsupervised domain adaptation
approach. NIPS, 2012.

[2] Yun. Jiang and Ashutosh Saxena. Discovering Diﬀerent
Types of Topics: Factored topic models. Cornell, August
2012.

[3] Nitish Srivastava, Ruslan Salakhutdinov, and Geoﬀrey E.
Hinton. Modeling documents with deep boltzmann ma-
chines. CoRR, abs/1309.6865, 2013. URL http://arxiv.
org/abs/1309.6865.

[4] Beal M. Teh Y., Jordan M. and D. Blei. Hierarchical

Dirichlet Processes. Berkeley, March 2005.

[5] Zhixiang (Eddie) Xu, Minmin Chen, Kilian Q. Wein-
berger, and Fei Sha. From sbow to dcot marginalized
encoders for text representation.
In Proceedings of the
21st ACM International Conference on Information and
Knowledge Management, CIKM ’12, pages 1879–1884,
New York, NY, USA, 2012. ACM. ISBN 978-1-4503-1156-
4. doi: 10.1145/2396761.2398536. URL http://doi.acm.
org/10.1145/2396761.2398536.

