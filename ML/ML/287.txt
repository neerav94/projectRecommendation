What Project Should I Choose?

Andrew Poon

poon-andrew@stanfordalumni.org

Abstract

This work analyzes the distribution of past CS229 projects by applying hierarchi-
cal agglomerative clustering. The clusters reveal which topics are very popular
and which topics are more unique. Tracking the clusters over time also provides
insight into how student projects have shifted over time. This knowledge will help
future students select interesting and unique projects.

1

Introduction

Choosing a project for CS229 is difﬁcult. Students want to choose projects that are not only in-
teresting, but are also unique from other projects chosen by current and past students. This work
attempts to address this problem by using text analysis and clustering to organize past projects. The
distribution of projects changes over time and provides insight into where student interests lie and
how the ﬁeld is evolving. As a result, this work is able to identify popular topics worked on by many
students, and highlights the most unique projects that have been submitted over the years.

2 Data Set

The data set for this project comes from the archive of past CS229 projects. These can be accessed at
the course website1. The papers for each year have been collected, converted to text2, and converted
into word frequency vectors (i.e. a histogram of words).

2.1 Document Processing

Converting project papers into useful word frequency vectors requires processing. Standard pro-
cessing techniques such as stop word removal [4, p. 27], lower casing all words, and word stem-
ming [4, p. 32] were applied. By far, the most tedious part of this process was building a ﬁlter
list to remove “contentless” words. This problem was exacerbated by the fact that the texts were
CS229 project papers with different vocabularies than standard English texts. For example, words
such as “algorithm” and “learning” would be relevant for ordinary topic modeling. But in this case,
every project applies some algorithm that attempts to learn something so these words do not indicate
anything particular about the project. Words like this are considered “contentless” in this context
and were subsequently added to the stop word list.
After painstakingly removing as many irrelevant words as possible, each paper is converted into its
own word frequency vector. The resulting vector indicates the key topics of the paper. An example
of the top entries in a word frequency vector is shown in Table 1. In this example, the paper [2]
investigates using neural networks to identify handwritten digits and training a robot to write those
digits. Thus, is it not surprising that terms such as “nn” and “digit” appear frequently.
Unfortunately, not every project paper was available for analysis. Some papers (and all papers in
2004) were not available for download. Additionally, some papers failed to convert from PDF to

1http://cs229.stanford.edu
2The UNIX utility pdftotext was used.

1

What Project Should I Choose?

Andrew Poon

poon-andrew@stanfordalumni.org

Abstract

This work analyzes the distribution of past CS229 projects by applying hierarchi-
cal agglomerative clustering. The clusters reveal which topics are very popular
and which topics are more unique. Tracking the clusters over time also provides
insight into how student projects have shifted over time. This knowledge will help
future students select interesting and unique projects.

1

Introduction

Choosing a project for CS229 is difﬁcult. Students want to choose projects that are not only in-
teresting, but are also unique from other projects chosen by current and past students. This work
attempts to address this problem by using text analysis and clustering to organize past projects. The
distribution of projects changes over time and provides insight into where student interests lie and
how the ﬁeld is evolving. As a result, this work is able to identify popular topics worked on by many
students, and highlights the most unique projects that have been submitted over the years.

2 Data Set

The data set for this project comes from the archive of past CS229 projects. These can be accessed at
the course website1. The papers for each year have been collected, converted to text2, and converted
into word frequency vectors (i.e. a histogram of words).

2.1 Document Processing

Converting project papers into useful word frequency vectors requires processing. Standard pro-
cessing techniques such as stop word removal [4, p. 27], lower casing all words, and word stem-
ming [4, p. 32] were applied. By far, the most tedious part of this process was building a ﬁlter
list to remove “contentless” words. This problem was exacerbated by the fact that the texts were
CS229 project papers with different vocabularies than standard English texts. For example, words
such as “algorithm” and “learning” would be relevant for ordinary topic modeling. But in this case,
every project applies some algorithm that attempts to learn something so these words do not indicate
anything particular about the project. Words like this are considered “contentless” in this context
and were subsequently added to the stop word list.
After painstakingly removing as many irrelevant words as possible, each paper is converted into its
own word frequency vector. The resulting vector indicates the key topics of the paper. An example
of the top entries in a word frequency vector is shown in Table 1. In this example, the paper [2]
investigates using neural networks to identify handwritten digits and training a robot to write those
digits. Thus, is it not surprising that terms such as “nn” and “digit” appear frequently.
Unfortunately, not every project paper was available for analysis. Some papers (and all papers in
2004) were not available for download. Additionally, some papers failed to convert from PDF to

1http://cs229.stanford.edu
2The UNIX utility pdftotext was used.

1

Table 1: Word frequency vector of a paper investigating digit recognition using neural networks.

nn
reconstruction

32
22

image
recognition

29
17

program 28 motor
mnist

digit

15

28
15

plain text. In the end, 1179 papers were converted to word frequency vectors to form a sufﬁcient
data set.

3 Clustering

Hierarchical Agglomerative Clustering (HAC) [3, p. 520] was used to cluster the projects. HAC
starts by initializing each sample in its own cluster. On each iteration, the clusters that are most
similar are merged together. As the clustering iterates, the threshold for merging clusters becomes
more relaxed. The result is that the most similar clusters are merged ﬁrst, less similar clusters are
merged later, and dissimilar clusters are left unmerged. This continues until the similarity threshold
drops below a certain value, or until sufﬁciently few clusters remain.
Similarity between clusters was measured using Cosine Similarity [5] given by the expression

A · B
||A||||B||

where A and B are the two word frequency vectors. This essentially performs a vector dot product.
This approach is straightforward and provides a simple way to measure the similarity of two papers.
After two clusters are merged, the word frequency vectors are averaged and only the twenty most
frequent words are kept. This was done so that the new vector would only contain the most relevant
words. Otherwise, the vocabulary of the word vector would grow after each merge, causing many
clusters to merge together quickly.
The standard k-means clustering algorithm was also attempted, but did not perform well. The ran-
dom initial clusters resulted in inconsistent ﬁnal clusters. There were also cases with empty clusters
and cases with all samples in one huge cluster. After several attempts, k-means clustering was
abandoned in favor of HAC.

4 Results

The clustering sequences for 2005 and 2013 are shown in Figure 1. Initially, each project is placed in
its own cluster. As the algorithm runs, the most similar clusters are merged ﬁrst. These early clusters
are marked and keywords from those clusters are shown in Table 2. As the algorithm continues to
run, the threshold for merging clusters relaxes and clusters with lower similarity are merged. This
results in a very large cluster which can be seen at the bottom of the graphs. This large cluster does
not have very distinctive keywords and is not meaningful. However, there are a handful of clusters
that remain separate from the large cluster. These remaining individual clusters can be interpreted
as the most unique papers.
The clustering sequences for each year follow the same pattern: similar clusters merge early on, but
eventually most clusters merge into a huge cluster while only a few unique clusters remain separate.
By analyzing the early clusters and the remaining unique clusters for each year, we can see how
popular topics and unique topics change over time.
Clustering reveals several popular project areas such as: ﬁnance, robotics, music, image classiﬁ-
cation, text analysis, biology, sports, and movies. The cluster sizes over the years are shown in
Figure 2. Tracking these clusters over time shows some trends in student interest. For example, all
clusters, except for robotics, are generally growing over time. This is expected as the class size has
grown signiﬁcantly over time.
In the case of robotics, student interest has shifted as more projects are attempting to control other
machines such as cars and rockets rather than the typical robotic arm, causing the keyword “robot”

2

What Project Should I Choose?

Andrew Poon

poon-andrew@stanfordalumni.org

Abstract

This work analyzes the distribution of past CS229 projects by applying hierarchi-
cal agglomerative clustering. The clusters reveal which topics are very popular
and which topics are more unique. Tracking the clusters over time also provides
insight into how student projects have shifted over time. This knowledge will help
future students select interesting and unique projects.

1

Introduction

Choosing a project for CS229 is difﬁcult. Students want to choose projects that are not only in-
teresting, but are also unique from other projects chosen by current and past students. This work
attempts to address this problem by using text analysis and clustering to organize past projects. The
distribution of projects changes over time and provides insight into where student interests lie and
how the ﬁeld is evolving. As a result, this work is able to identify popular topics worked on by many
students, and highlights the most unique projects that have been submitted over the years.

2 Data Set

The data set for this project comes from the archive of past CS229 projects. These can be accessed at
the course website1. The papers for each year have been collected, converted to text2, and converted
into word frequency vectors (i.e. a histogram of words).

2.1 Document Processing

Converting project papers into useful word frequency vectors requires processing. Standard pro-
cessing techniques such as stop word removal [4, p. 27], lower casing all words, and word stem-
ming [4, p. 32] were applied. By far, the most tedious part of this process was building a ﬁlter
list to remove “contentless” words. This problem was exacerbated by the fact that the texts were
CS229 project papers with different vocabularies than standard English texts. For example, words
such as “algorithm” and “learning” would be relevant for ordinary topic modeling. But in this case,
every project applies some algorithm that attempts to learn something so these words do not indicate
anything particular about the project. Words like this are considered “contentless” in this context
and were subsequently added to the stop word list.
After painstakingly removing as many irrelevant words as possible, each paper is converted into its
own word frequency vector. The resulting vector indicates the key topics of the paper. An example
of the top entries in a word frequency vector is shown in Table 1. In this example, the paper [2]
investigates using neural networks to identify handwritten digits and training a robot to write those
digits. Thus, is it not surprising that terms such as “nn” and “digit” appear frequently.
Unfortunately, not every project paper was available for analysis. Some papers (and all papers in
2004) were not available for download. Additionally, some papers failed to convert from PDF to

1http://cs229.stanford.edu
2The UNIX utility pdftotext was used.

1

Table 1: Word frequency vector of a paper investigating digit recognition using neural networks.

nn
reconstruction

32
22

image
recognition

29
17

program 28 motor
mnist

digit

15

28
15

plain text. In the end, 1179 papers were converted to word frequency vectors to form a sufﬁcient
data set.

3 Clustering

Hierarchical Agglomerative Clustering (HAC) [3, p. 520] was used to cluster the projects. HAC
starts by initializing each sample in its own cluster. On each iteration, the clusters that are most
similar are merged together. As the clustering iterates, the threshold for merging clusters becomes
more relaxed. The result is that the most similar clusters are merged ﬁrst, less similar clusters are
merged later, and dissimilar clusters are left unmerged. This continues until the similarity threshold
drops below a certain value, or until sufﬁciently few clusters remain.
Similarity between clusters was measured using Cosine Similarity [5] given by the expression

A · B
||A||||B||

where A and B are the two word frequency vectors. This essentially performs a vector dot product.
This approach is straightforward and provides a simple way to measure the similarity of two papers.
After two clusters are merged, the word frequency vectors are averaged and only the twenty most
frequent words are kept. This was done so that the new vector would only contain the most relevant
words. Otherwise, the vocabulary of the word vector would grow after each merge, causing many
clusters to merge together quickly.
The standard k-means clustering algorithm was also attempted, but did not perform well. The ran-
dom initial clusters resulted in inconsistent ﬁnal clusters. There were also cases with empty clusters
and cases with all samples in one huge cluster. After several attempts, k-means clustering was
abandoned in favor of HAC.

4 Results

The clustering sequences for 2005 and 2013 are shown in Figure 1. Initially, each project is placed in
its own cluster. As the algorithm runs, the most similar clusters are merged ﬁrst. These early clusters
are marked and keywords from those clusters are shown in Table 2. As the algorithm continues to
run, the threshold for merging clusters relaxes and clusters with lower similarity are merged. This
results in a very large cluster which can be seen at the bottom of the graphs. This large cluster does
not have very distinctive keywords and is not meaningful. However, there are a handful of clusters
that remain separate from the large cluster. These remaining individual clusters can be interpreted
as the most unique papers.
The clustering sequences for each year follow the same pattern: similar clusters merge early on, but
eventually most clusters merge into a huge cluster while only a few unique clusters remain separate.
By analyzing the early clusters and the remaining unique clusters for each year, we can see how
popular topics and unique topics change over time.
Clustering reveals several popular project areas such as: ﬁnance, robotics, music, image classiﬁ-
cation, text analysis, biology, sports, and movies. The cluster sizes over the years are shown in
Figure 2. Tracking these clusters over time shows some trends in student interest. For example, all
clusters, except for robotics, are generally growing over time. This is expected as the class size has
grown signiﬁcantly over time.
In the case of robotics, student interest has shifted as more projects are attempting to control other
machines such as cars and rockets rather than the typical robotic arm, causing the keyword “robot”

2

Figure 1: Evolution of clusters.

Table 2: Keywords from early clusters.

Grouping

(1)

(2)

(3)

(4)

(5)

2005

Keywords

clustering, mean, word, parti-
tion, node
image, feature, classify, skin,
object
note, music, component, in-
strument, microtiming
query,
precision, default
reinforcement learning, light,
player, trafﬁc, game

document,

classify,

Grouping

(1)

(2)

(3)

(4)

(5)

2013

Keywords

feature, josquin, note, music,
classify
tag, word, question, feature,
classify
stock, return, feature, price,
trading
game, team, season, predic-
tion, play
click, query, search, rank, url

to decline in frequency. The cluster for movies ﬂuctuates wildly during the early years. Apparently,
the surge in popularity was due to the introduction of the Netﬂix Prize [6]. In the years following a
glut of Netﬂix projects, students were less interested in applying machine learning to movies. Also
of interest is the ﬁnance cluster. Surprisingly, there were no stock trading projects in 2005. However,
there were two large peaks in 2009 and 2011. The ﬁrst peak came after the housing bubble bust,
while the second peak is due to a sponsored project3 investigating stock trading based on Twitter
messages. Finally, we can also see that image classiﬁcation (plotted separately) is the most common
topic because of wide applicability in areas such as computer vision and medical imaging. Oddly,
there was a drop in 2012 in image classiﬁcation projects. Closer inspection shows that many projects
that year investigated astronomical applications such as detecting dark matter, causing the keyword
“image” to not appear as often.
At the other end of the spectrum, we also ﬁnd many examples of unique projects by examining the
clusters that remain separate. Some examples of unique projects include: ionospheric corruption
of radio waves, tracking vehicles using an autonomous helicopter, optimizing wind farms, and de-
tecting arguments in online forums. Ironically, this algorithm found a unique project [1] from 2012
that also analyzed past CS229 projects in a similar fashion. This algorithm, while attempting to ﬁnd
unique projects, discovered that it itself was not unique. This was truly a surprising result!

5 Discussion

The clustering algorithm presented here is able to discover some general patterns in past projects.
However, the clusters are not very precise because they are based only on word frequency. The
biggest factor that could improve this system is to use better natural language processing (NLP).

3Supervised by Mihai Surdeanu and John Bauer.

3

What Project Should I Choose?

Andrew Poon

poon-andrew@stanfordalumni.org

Abstract

This work analyzes the distribution of past CS229 projects by applying hierarchi-
cal agglomerative clustering. The clusters reveal which topics are very popular
and which topics are more unique. Tracking the clusters over time also provides
insight into how student projects have shifted over time. This knowledge will help
future students select interesting and unique projects.

1

Introduction

Choosing a project for CS229 is difﬁcult. Students want to choose projects that are not only in-
teresting, but are also unique from other projects chosen by current and past students. This work
attempts to address this problem by using text analysis and clustering to organize past projects. The
distribution of projects changes over time and provides insight into where student interests lie and
how the ﬁeld is evolving. As a result, this work is able to identify popular topics worked on by many
students, and highlights the most unique projects that have been submitted over the years.

2 Data Set

The data set for this project comes from the archive of past CS229 projects. These can be accessed at
the course website1. The papers for each year have been collected, converted to text2, and converted
into word frequency vectors (i.e. a histogram of words).

2.1 Document Processing

Converting project papers into useful word frequency vectors requires processing. Standard pro-
cessing techniques such as stop word removal [4, p. 27], lower casing all words, and word stem-
ming [4, p. 32] were applied. By far, the most tedious part of this process was building a ﬁlter
list to remove “contentless” words. This problem was exacerbated by the fact that the texts were
CS229 project papers with different vocabularies than standard English texts. For example, words
such as “algorithm” and “learning” would be relevant for ordinary topic modeling. But in this case,
every project applies some algorithm that attempts to learn something so these words do not indicate
anything particular about the project. Words like this are considered “contentless” in this context
and were subsequently added to the stop word list.
After painstakingly removing as many irrelevant words as possible, each paper is converted into its
own word frequency vector. The resulting vector indicates the key topics of the paper. An example
of the top entries in a word frequency vector is shown in Table 1. In this example, the paper [2]
investigates using neural networks to identify handwritten digits and training a robot to write those
digits. Thus, is it not surprising that terms such as “nn” and “digit” appear frequently.
Unfortunately, not every project paper was available for analysis. Some papers (and all papers in
2004) were not available for download. Additionally, some papers failed to convert from PDF to

1http://cs229.stanford.edu
2The UNIX utility pdftotext was used.

1

Table 1: Word frequency vector of a paper investigating digit recognition using neural networks.

nn
reconstruction

32
22

image
recognition

29
17

program 28 motor
mnist

digit

15

28
15

plain text. In the end, 1179 papers were converted to word frequency vectors to form a sufﬁcient
data set.

3 Clustering

Hierarchical Agglomerative Clustering (HAC) [3, p. 520] was used to cluster the projects. HAC
starts by initializing each sample in its own cluster. On each iteration, the clusters that are most
similar are merged together. As the clustering iterates, the threshold for merging clusters becomes
more relaxed. The result is that the most similar clusters are merged ﬁrst, less similar clusters are
merged later, and dissimilar clusters are left unmerged. This continues until the similarity threshold
drops below a certain value, or until sufﬁciently few clusters remain.
Similarity between clusters was measured using Cosine Similarity [5] given by the expression

A · B
||A||||B||

where A and B are the two word frequency vectors. This essentially performs a vector dot product.
This approach is straightforward and provides a simple way to measure the similarity of two papers.
After two clusters are merged, the word frequency vectors are averaged and only the twenty most
frequent words are kept. This was done so that the new vector would only contain the most relevant
words. Otherwise, the vocabulary of the word vector would grow after each merge, causing many
clusters to merge together quickly.
The standard k-means clustering algorithm was also attempted, but did not perform well. The ran-
dom initial clusters resulted in inconsistent ﬁnal clusters. There were also cases with empty clusters
and cases with all samples in one huge cluster. After several attempts, k-means clustering was
abandoned in favor of HAC.

4 Results

The clustering sequences for 2005 and 2013 are shown in Figure 1. Initially, each project is placed in
its own cluster. As the algorithm runs, the most similar clusters are merged ﬁrst. These early clusters
are marked and keywords from those clusters are shown in Table 2. As the algorithm continues to
run, the threshold for merging clusters relaxes and clusters with lower similarity are merged. This
results in a very large cluster which can be seen at the bottom of the graphs. This large cluster does
not have very distinctive keywords and is not meaningful. However, there are a handful of clusters
that remain separate from the large cluster. These remaining individual clusters can be interpreted
as the most unique papers.
The clustering sequences for each year follow the same pattern: similar clusters merge early on, but
eventually most clusters merge into a huge cluster while only a few unique clusters remain separate.
By analyzing the early clusters and the remaining unique clusters for each year, we can see how
popular topics and unique topics change over time.
Clustering reveals several popular project areas such as: ﬁnance, robotics, music, image classiﬁ-
cation, text analysis, biology, sports, and movies. The cluster sizes over the years are shown in
Figure 2. Tracking these clusters over time shows some trends in student interest. For example, all
clusters, except for robotics, are generally growing over time. This is expected as the class size has
grown signiﬁcantly over time.
In the case of robotics, student interest has shifted as more projects are attempting to control other
machines such as cars and rockets rather than the typical robotic arm, causing the keyword “robot”

2

Figure 1: Evolution of clusters.

Table 2: Keywords from early clusters.

Grouping

(1)

(2)

(3)

(4)

(5)

2005

Keywords

clustering, mean, word, parti-
tion, node
image, feature, classify, skin,
object
note, music, component, in-
strument, microtiming
query,
precision, default
reinforcement learning, light,
player, trafﬁc, game

document,

classify,

Grouping

(1)

(2)

(3)

(4)

(5)

2013

Keywords

feature, josquin, note, music,
classify
tag, word, question, feature,
classify
stock, return, feature, price,
trading
game, team, season, predic-
tion, play
click, query, search, rank, url

to decline in frequency. The cluster for movies ﬂuctuates wildly during the early years. Apparently,
the surge in popularity was due to the introduction of the Netﬂix Prize [6]. In the years following a
glut of Netﬂix projects, students were less interested in applying machine learning to movies. Also
of interest is the ﬁnance cluster. Surprisingly, there were no stock trading projects in 2005. However,
there were two large peaks in 2009 and 2011. The ﬁrst peak came after the housing bubble bust,
while the second peak is due to a sponsored project3 investigating stock trading based on Twitter
messages. Finally, we can also see that image classiﬁcation (plotted separately) is the most common
topic because of wide applicability in areas such as computer vision and medical imaging. Oddly,
there was a drop in 2012 in image classiﬁcation projects. Closer inspection shows that many projects
that year investigated astronomical applications such as detecting dark matter, causing the keyword
“image” to not appear as often.
At the other end of the spectrum, we also ﬁnd many examples of unique projects by examining the
clusters that remain separate. Some examples of unique projects include: ionospheric corruption
of radio waves, tracking vehicles using an autonomous helicopter, optimizing wind farms, and de-
tecting arguments in online forums. Ironically, this algorithm found a unique project [1] from 2012
that also analyzed past CS229 projects in a similar fashion. This algorithm, while attempting to ﬁnd
unique projects, discovered that it itself was not unique. This was truly a surprising result!

5 Discussion

The clustering algorithm presented here is able to discover some general patterns in past projects.
However, the clusters are not very precise because they are based only on word frequency. The
biggest factor that could improve this system is to use better natural language processing (NLP).

3Supervised by Mihai Surdeanu and John Bauer.

3

Figure 2: Topic trends over time.

The simple word frequency approach used here loses information that could be found in sequences
of words (N-grams). Named entity recognition (NER) could be used to identify topic keywords and
eliminate “contentless” words.
Another problem encountered in this work is that project topics are becoming more broad as well.
For example, robot vision combines robotics and image classiﬁcation. Trading stock based on Twit-
ter messages combines ﬁnance and text analysis. These kinds of projects do not ﬁt neatly into the
common topics and introduce a lot of ambiguity in the results. Thus, a crude attempt at topic model-
ing was made by manually reading through past projects and labeling the paper with topic keywords.
This provided more precise labels for each project, but was too tedious to be scalable. With better
topic modeling, each topic would have a ﬁngerprint of distinctive keywords which could be used for
more accurate clustering.

6 Conclusion

This project performed unsupervised clustering on past CS229 projects. This revealed several com-
mon topic areas and well as some unique projects. Interest in the popular topic areas has also varied
with time and has been inﬂuenced by external factors such as sponsored projects. These results
provide guidance to future students by showing which topics have been very popular, and providing
examples of unique projects. This information should help students choose more varied and unique
projects in the future.

7 Future Work

This project has been described as “very meta” and ironically discovered itself to not be unique
when searching for unique projects. But why stop there? We must go deeper. In the future, more
projects can apply machine learning to past CS229 projects. Then, there could be a new project that
analyzes the other project analyzers.
Regarding this system, the performance could be improved by incorporating better NLP and topic
modeling. This would lead to better clustering and could reveal ﬁner details than the general trends
found in this work. The visualization of the clustering process also has much room for improvement.

References
[1] Michael Chang

and

Ethan

Saeta.

Analyzing CS

229

Projects,

http://cs229.stanford.edu/proj2012/ChangSaeta-AnalyzingCS229Projects.pdf.

[2] John A. Conley and My Phuong Le.

vestigation and Improvement of
http://cs229.stanford.edu/proj2005/ConleyLe-HandwrittenDigitRecognition.pdf.

the

Handwritten Digit Recognition:
Inferred Motor Program Algorithm,

4

2014.

In-
2005.

What Project Should I Choose?

Andrew Poon

poon-andrew@stanfordalumni.org

Abstract

This work analyzes the distribution of past CS229 projects by applying hierarchi-
cal agglomerative clustering. The clusters reveal which topics are very popular
and which topics are more unique. Tracking the clusters over time also provides
insight into how student projects have shifted over time. This knowledge will help
future students select interesting and unique projects.

1

Introduction

Choosing a project for CS229 is difﬁcult. Students want to choose projects that are not only in-
teresting, but are also unique from other projects chosen by current and past students. This work
attempts to address this problem by using text analysis and clustering to organize past projects. The
distribution of projects changes over time and provides insight into where student interests lie and
how the ﬁeld is evolving. As a result, this work is able to identify popular topics worked on by many
students, and highlights the most unique projects that have been submitted over the years.

2 Data Set

The data set for this project comes from the archive of past CS229 projects. These can be accessed at
the course website1. The papers for each year have been collected, converted to text2, and converted
into word frequency vectors (i.e. a histogram of words).

2.1 Document Processing

Converting project papers into useful word frequency vectors requires processing. Standard pro-
cessing techniques such as stop word removal [4, p. 27], lower casing all words, and word stem-
ming [4, p. 32] were applied. By far, the most tedious part of this process was building a ﬁlter
list to remove “contentless” words. This problem was exacerbated by the fact that the texts were
CS229 project papers with different vocabularies than standard English texts. For example, words
such as “algorithm” and “learning” would be relevant for ordinary topic modeling. But in this case,
every project applies some algorithm that attempts to learn something so these words do not indicate
anything particular about the project. Words like this are considered “contentless” in this context
and were subsequently added to the stop word list.
After painstakingly removing as many irrelevant words as possible, each paper is converted into its
own word frequency vector. The resulting vector indicates the key topics of the paper. An example
of the top entries in a word frequency vector is shown in Table 1. In this example, the paper [2]
investigates using neural networks to identify handwritten digits and training a robot to write those
digits. Thus, is it not surprising that terms such as “nn” and “digit” appear frequently.
Unfortunately, not every project paper was available for analysis. Some papers (and all papers in
2004) were not available for download. Additionally, some papers failed to convert from PDF to

1http://cs229.stanford.edu
2The UNIX utility pdftotext was used.

1

Table 1: Word frequency vector of a paper investigating digit recognition using neural networks.

nn
reconstruction

32
22

image
recognition

29
17

program 28 motor
mnist

digit

15

28
15

plain text. In the end, 1179 papers were converted to word frequency vectors to form a sufﬁcient
data set.

3 Clustering

Hierarchical Agglomerative Clustering (HAC) [3, p. 520] was used to cluster the projects. HAC
starts by initializing each sample in its own cluster. On each iteration, the clusters that are most
similar are merged together. As the clustering iterates, the threshold for merging clusters becomes
more relaxed. The result is that the most similar clusters are merged ﬁrst, less similar clusters are
merged later, and dissimilar clusters are left unmerged. This continues until the similarity threshold
drops below a certain value, or until sufﬁciently few clusters remain.
Similarity between clusters was measured using Cosine Similarity [5] given by the expression

A · B
||A||||B||

where A and B are the two word frequency vectors. This essentially performs a vector dot product.
This approach is straightforward and provides a simple way to measure the similarity of two papers.
After two clusters are merged, the word frequency vectors are averaged and only the twenty most
frequent words are kept. This was done so that the new vector would only contain the most relevant
words. Otherwise, the vocabulary of the word vector would grow after each merge, causing many
clusters to merge together quickly.
The standard k-means clustering algorithm was also attempted, but did not perform well. The ran-
dom initial clusters resulted in inconsistent ﬁnal clusters. There were also cases with empty clusters
and cases with all samples in one huge cluster. After several attempts, k-means clustering was
abandoned in favor of HAC.

4 Results

The clustering sequences for 2005 and 2013 are shown in Figure 1. Initially, each project is placed in
its own cluster. As the algorithm runs, the most similar clusters are merged ﬁrst. These early clusters
are marked and keywords from those clusters are shown in Table 2. As the algorithm continues to
run, the threshold for merging clusters relaxes and clusters with lower similarity are merged. This
results in a very large cluster which can be seen at the bottom of the graphs. This large cluster does
not have very distinctive keywords and is not meaningful. However, there are a handful of clusters
that remain separate from the large cluster. These remaining individual clusters can be interpreted
as the most unique papers.
The clustering sequences for each year follow the same pattern: similar clusters merge early on, but
eventually most clusters merge into a huge cluster while only a few unique clusters remain separate.
By analyzing the early clusters and the remaining unique clusters for each year, we can see how
popular topics and unique topics change over time.
Clustering reveals several popular project areas such as: ﬁnance, robotics, music, image classiﬁ-
cation, text analysis, biology, sports, and movies. The cluster sizes over the years are shown in
Figure 2. Tracking these clusters over time shows some trends in student interest. For example, all
clusters, except for robotics, are generally growing over time. This is expected as the class size has
grown signiﬁcantly over time.
In the case of robotics, student interest has shifted as more projects are attempting to control other
machines such as cars and rockets rather than the typical robotic arm, causing the keyword “robot”

2

Figure 1: Evolution of clusters.

Table 2: Keywords from early clusters.

Grouping

(1)

(2)

(3)

(4)

(5)

2005

Keywords

clustering, mean, word, parti-
tion, node
image, feature, classify, skin,
object
note, music, component, in-
strument, microtiming
query,
precision, default
reinforcement learning, light,
player, trafﬁc, game

document,

classify,

Grouping

(1)

(2)

(3)

(4)

(5)

2013

Keywords

feature, josquin, note, music,
classify
tag, word, question, feature,
classify
stock, return, feature, price,
trading
game, team, season, predic-
tion, play
click, query, search, rank, url

to decline in frequency. The cluster for movies ﬂuctuates wildly during the early years. Apparently,
the surge in popularity was due to the introduction of the Netﬂix Prize [6]. In the years following a
glut of Netﬂix projects, students were less interested in applying machine learning to movies. Also
of interest is the ﬁnance cluster. Surprisingly, there were no stock trading projects in 2005. However,
there were two large peaks in 2009 and 2011. The ﬁrst peak came after the housing bubble bust,
while the second peak is due to a sponsored project3 investigating stock trading based on Twitter
messages. Finally, we can also see that image classiﬁcation (plotted separately) is the most common
topic because of wide applicability in areas such as computer vision and medical imaging. Oddly,
there was a drop in 2012 in image classiﬁcation projects. Closer inspection shows that many projects
that year investigated astronomical applications such as detecting dark matter, causing the keyword
“image” to not appear as often.
At the other end of the spectrum, we also ﬁnd many examples of unique projects by examining the
clusters that remain separate. Some examples of unique projects include: ionospheric corruption
of radio waves, tracking vehicles using an autonomous helicopter, optimizing wind farms, and de-
tecting arguments in online forums. Ironically, this algorithm found a unique project [1] from 2012
that also analyzed past CS229 projects in a similar fashion. This algorithm, while attempting to ﬁnd
unique projects, discovered that it itself was not unique. This was truly a surprising result!

5 Discussion

The clustering algorithm presented here is able to discover some general patterns in past projects.
However, the clusters are not very precise because they are based only on word frequency. The
biggest factor that could improve this system is to use better natural language processing (NLP).

3Supervised by Mihai Surdeanu and John Bauer.

3

Figure 2: Topic trends over time.

The simple word frequency approach used here loses information that could be found in sequences
of words (N-grams). Named entity recognition (NER) could be used to identify topic keywords and
eliminate “contentless” words.
Another problem encountered in this work is that project topics are becoming more broad as well.
For example, robot vision combines robotics and image classiﬁcation. Trading stock based on Twit-
ter messages combines ﬁnance and text analysis. These kinds of projects do not ﬁt neatly into the
common topics and introduce a lot of ambiguity in the results. Thus, a crude attempt at topic model-
ing was made by manually reading through past projects and labeling the paper with topic keywords.
This provided more precise labels for each project, but was too tedious to be scalable. With better
topic modeling, each topic would have a ﬁngerprint of distinctive keywords which could be used for
more accurate clustering.

6 Conclusion

This project performed unsupervised clustering on past CS229 projects. This revealed several com-
mon topic areas and well as some unique projects. Interest in the popular topic areas has also varied
with time and has been inﬂuenced by external factors such as sponsored projects. These results
provide guidance to future students by showing which topics have been very popular, and providing
examples of unique projects. This information should help students choose more varied and unique
projects in the future.

7 Future Work

This project has been described as “very meta” and ironically discovered itself to not be unique
when searching for unique projects. But why stop there? We must go deeper. In the future, more
projects can apply machine learning to past CS229 projects. Then, there could be a new project that
analyzes the other project analyzers.
Regarding this system, the performance could be improved by incorporating better NLP and topic
modeling. This would lead to better clustering and could reveal ﬁner details than the general trends
found in this work. The visualization of the clustering process also has much room for improvement.

References
[1] Michael Chang

and

Ethan

Saeta.

Analyzing CS

229

Projects,

http://cs229.stanford.edu/proj2012/ChangSaeta-AnalyzingCS229Projects.pdf.

[2] John A. Conley and My Phuong Le.

vestigation and Improvement of
http://cs229.stanford.edu/proj2005/ConleyLe-HandwrittenDigitRecognition.pdf.

the

Handwritten Digit Recognition:
Inferred Motor Program Algorithm,

4

2014.

In-
2005.

[3] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining,

Inference, and Prediction, Second Edition. Springer Series in Statistics. Springer, 2009.

[4] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. Introduction to Informa-

tion Retrieval. Cambridge University Press, New York, NY, USA, 2008.

[5] Wikipedia. Cosine Similarity, 2014. http://en.wikipedia.org/wiki/Cosine similarity.
[6] Wikipedia. Netﬂix Prize, 2014. http://en.wikipedia.org/wiki/Netﬂix Prize.

5

