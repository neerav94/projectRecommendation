Predicting Heart Attacks

Luyang Chen, Qi Cao, Sihua Li, Xiao Ju

Stanford University

lych, qcao, sihua, xju@stanford.edu

Abstract

This paper aims at a better understanding and application of machine learning in medical
domain. In this paper, we modify three classical models for multiclass problems: Logistic Re-
gression, Naive Bayes and SVM, and then implement them to predict cardiac arrhythmia based
on patients’ medical records. First, we use all features provided to build the models. By com-
paring the accuracy of different models, we ﬁnd that Multiclass Naive Bayes and SVM models
have better performance. Afterwards, we implement feature selection to improve the accuracy
of prediction. Forward search procedure is used to choose different amount of features for each
model. By comparing the accuracy between using all features and using features selected, we
ﬁnd that feature selection can signiﬁcantly enhance the performance of our models.

I.

Introduction

In the medical industry, machine learning
algorithms can be used to diagnose some se-
rious diseases. Among all diseases, cardiac
arrhythmia is the top one cause of death in
the world, claiming more lives than cancer and
HIV combined. Thus, how to predict cardiac
arrhythmia in real life is of great signiﬁcance,
both to research and application.

In this paper, we apply machine learning
algorithms to predict cardiac arrhythmia based
on a patient’s medical record. We use the UCI
Arrhythmia Data Set for both training and test-
ing. We are provided with 452 clinical records
of patients. Each record contains 279 attributes,
such as age, sex, weight and information col-
lected from ECG signals. The diagnosis of
cardiac arrhythmia is divided into 16 classes.
Class 1 refers to normal case. Class 2 to 15
represent different kinds of cardiac arrhyth-
mia, such as Ischemic Changes, Old Anterior
Myocardial Infarction, Supraventricular Prema-
ture Contraction, Right Bundle Branch Block
and etc. Class 16 refers to the rest.

Our objective is to classify a patient into
one class according to his or her clinical mea-
surements. This paper applies Logistic Regres-
sion, Naive Bayes, and SVM algorithms to this
realistic problem, compares their accuracy, and

modiﬁes the models to get the best possible
results.

II. Features and Preprocessing

I. Extract valid columns

The dataset contains 279 columns rep-
resenting 279 features, but some data in 5
columns are invalid or missing. So these 5
columns are completely ignored. We extract
the rest 274 columns to form a new examples
matrix for modeling.

II. Data discretization

After observing the raw data provided, we
notice that some features have discrete values,
such as age, sex, etc. while the other features
have continuous, real values, such as the fea-
tures extracted from ECG signals (e.g. ampli-
tude of waves of each channel). Since we plan
to use Naive Bayes algorithm, in which the in-
put examples are discrete-valued, we have to
turn all the raw data into discrete values before
computing the probabilities. For the conve-
nience of computation, we divide the values of
each feature into 10 intervals. For the features
that can only take two possible values 0 or 1,
we consider 0 lies in the ﬁrst interval and 1

1

Predicting Heart Attacks

Luyang Chen, Qi Cao, Sihua Li, Xiao Ju

Stanford University

lych, qcao, sihua, xju@stanford.edu

Abstract

This paper aims at a better understanding and application of machine learning in medical
domain. In this paper, we modify three classical models for multiclass problems: Logistic Re-
gression, Naive Bayes and SVM, and then implement them to predict cardiac arrhythmia based
on patients’ medical records. First, we use all features provided to build the models. By com-
paring the accuracy of different models, we ﬁnd that Multiclass Naive Bayes and SVM models
have better performance. Afterwards, we implement feature selection to improve the accuracy
of prediction. Forward search procedure is used to choose different amount of features for each
model. By comparing the accuracy between using all features and using features selected, we
ﬁnd that feature selection can signiﬁcantly enhance the performance of our models.

I.

Introduction

In the medical industry, machine learning
algorithms can be used to diagnose some se-
rious diseases. Among all diseases, cardiac
arrhythmia is the top one cause of death in
the world, claiming more lives than cancer and
HIV combined. Thus, how to predict cardiac
arrhythmia in real life is of great signiﬁcance,
both to research and application.

In this paper, we apply machine learning
algorithms to predict cardiac arrhythmia based
on a patient’s medical record. We use the UCI
Arrhythmia Data Set for both training and test-
ing. We are provided with 452 clinical records
of patients. Each record contains 279 attributes,
such as age, sex, weight and information col-
lected from ECG signals. The diagnosis of
cardiac arrhythmia is divided into 16 classes.
Class 1 refers to normal case. Class 2 to 15
represent different kinds of cardiac arrhyth-
mia, such as Ischemic Changes, Old Anterior
Myocardial Infarction, Supraventricular Prema-
ture Contraction, Right Bundle Branch Block
and etc. Class 16 refers to the rest.

Our objective is to classify a patient into
one class according to his or her clinical mea-
surements. This paper applies Logistic Regres-
sion, Naive Bayes, and SVM algorithms to this
realistic problem, compares their accuracy, and

modiﬁes the models to get the best possible
results.

II. Features and Preprocessing

I. Extract valid columns

The dataset contains 279 columns rep-
resenting 279 features, but some data in 5
columns are invalid or missing. So these 5
columns are completely ignored. We extract
the rest 274 columns to form a new examples
matrix for modeling.

II. Data discretization

After observing the raw data provided, we
notice that some features have discrete values,
such as age, sex, etc. while the other features
have continuous, real values, such as the fea-
tures extracted from ECG signals (e.g. ampli-
tude of waves of each channel). Since we plan
to use Naive Bayes algorithm, in which the in-
put examples are discrete-valued, we have to
turn all the raw data into discrete values before
computing the probabilities. For the conve-
nience of computation, we divide the values of
each feature into 10 intervals. For the features
that can only take two possible values 0 or 1,
we consider 0 lies in the ﬁrst interval and 1

1

lies in the 10th interval. Then we convert each
value into the order number of the interval it
lies in. For example, if the height of one patient
is 172cm and it lies in the 8th interval, which is
[164.5, 173.0), we assign 8 to the height of the
patient. Thus, all features take values from 1
to 10.

III. Models

I. Logistic Regression and SVM

Since the supervised learning algorithms
we have learnt can only classify examples into
2 classes, our ﬁrst idea is to check whether an
example lies in class 1 to 16 one by one until
we ﬁnd the class it lies in and then stop. We
try both Logistic Regression and SVM.
First, we relabel class 1 to be 1 and class 2
to 16 to be −1. Then we can use Logistic Re-
gression or SVM to get a decision boundary. If
this decision boundary tells that a test example
belongs to 1, we predict it belongs to class 1
and then stop. Otherwise, we relabel class 2 to
be 1 and the other classes to be −1. We can get
a new decision boundary, which tells whether
the test example belongs to class 2 or not. We
continue doing this until we ﬁnd a class for
this test example. If we cannot ﬁnd a class for
it after the ﬁrst 15 trials, we predict it to be in
class 16.

These algorithms can work, but there are
two ﬂaws. First, why should we start from
class 1? If we start from a different class, we
might get a different prediction. Second, why
should we stop immediately after we get the
ﬁrst prediction. We can continue, pretending
we haven’t got a prediction. It is possible that
two different decision boundaries tell an exam-
ple belongs to class i and class j respectively.
Which one should we believe? Due to these
two ﬂaws, we want to modify SVM later so
that it becomes more reasonable and reliable
when used for multiclass classiﬁcations.

II. Multiclass Naive Bayes

After data preprocessing, all the features
can only take values from 1 to 10. Also,

2

we would like to classify training set into 16
classes. To parameterize a multinomial over k
outcomes, we can use k parameters φ1,· · · , φk
specifying the probability of each of the out-
comes. We deﬁne φk and φj,s|y=k as follows:

φk = P(y = k)

φj,s|y=k = P(xj = s|y = k)

16∑

k=1

φk = 1

(1)
(2)

(3)

10∑

s=1

φj,s|y=k = 1

(4)
We assume that all xj|y = k are indepen-
dent. Then using formulas of conditional prob-
ability, we have the probability of the whole
training set:

(cid:16) n∏

j=1

P(x, y) =

m∏

i=1

(cid:17)

φj,x(i)
j

|y=y(i)

φy(i)

(5)

Maximizing P(x, y) under the constraints

(3) and (4), we get the following formulas:

φk =

∑m

i=1 1{y(i)=k}

m

φj,s|y=k =

∑m
i=1 1{y(i)=k}1{x(i)
i=1 1{y(i)=k}

∑m

j =s}

(6)

(7)

And we can also use Laplace Smoothing to
modify the formulas above.

To make predictions, we use Bayes for-

mula:

P(x|y = k)P(y = k)

P(y = k|x) =
(8)
We predict y to be arg maxk P(x|y =

P(x)

k)P(y = k).

III. Multiclass SVM

In the lecture, we talked about using SVM
to classify our training examples into two
classes. We want to modify the algorithm so
that it can be used to make multiclass classiﬁ-
cations.

First, we draw k hyperplanes in the feature
j x + bj = 0

space, so that the jth hyperplane ωT

Predicting Heart Attacks

Luyang Chen, Qi Cao, Sihua Li, Xiao Ju

Stanford University

lych, qcao, sihua, xju@stanford.edu

Abstract

This paper aims at a better understanding and application of machine learning in medical
domain. In this paper, we modify three classical models for multiclass problems: Logistic Re-
gression, Naive Bayes and SVM, and then implement them to predict cardiac arrhythmia based
on patients’ medical records. First, we use all features provided to build the models. By com-
paring the accuracy of different models, we ﬁnd that Multiclass Naive Bayes and SVM models
have better performance. Afterwards, we implement feature selection to improve the accuracy
of prediction. Forward search procedure is used to choose different amount of features for each
model. By comparing the accuracy between using all features and using features selected, we
ﬁnd that feature selection can signiﬁcantly enhance the performance of our models.

I.

Introduction

In the medical industry, machine learning
algorithms can be used to diagnose some se-
rious diseases. Among all diseases, cardiac
arrhythmia is the top one cause of death in
the world, claiming more lives than cancer and
HIV combined. Thus, how to predict cardiac
arrhythmia in real life is of great signiﬁcance,
both to research and application.

In this paper, we apply machine learning
algorithms to predict cardiac arrhythmia based
on a patient’s medical record. We use the UCI
Arrhythmia Data Set for both training and test-
ing. We are provided with 452 clinical records
of patients. Each record contains 279 attributes,
such as age, sex, weight and information col-
lected from ECG signals. The diagnosis of
cardiac arrhythmia is divided into 16 classes.
Class 1 refers to normal case. Class 2 to 15
represent different kinds of cardiac arrhyth-
mia, such as Ischemic Changes, Old Anterior
Myocardial Infarction, Supraventricular Prema-
ture Contraction, Right Bundle Branch Block
and etc. Class 16 refers to the rest.

Our objective is to classify a patient into
one class according to his or her clinical mea-
surements. This paper applies Logistic Regres-
sion, Naive Bayes, and SVM algorithms to this
realistic problem, compares their accuracy, and

modiﬁes the models to get the best possible
results.

II. Features and Preprocessing

I. Extract valid columns

The dataset contains 279 columns rep-
resenting 279 features, but some data in 5
columns are invalid or missing. So these 5
columns are completely ignored. We extract
the rest 274 columns to form a new examples
matrix for modeling.

II. Data discretization

After observing the raw data provided, we
notice that some features have discrete values,
such as age, sex, etc. while the other features
have continuous, real values, such as the fea-
tures extracted from ECG signals (e.g. ampli-
tude of waves of each channel). Since we plan
to use Naive Bayes algorithm, in which the in-
put examples are discrete-valued, we have to
turn all the raw data into discrete values before
computing the probabilities. For the conve-
nience of computation, we divide the values of
each feature into 10 intervals. For the features
that can only take two possible values 0 or 1,
we consider 0 lies in the ﬁrst interval and 1

1

lies in the 10th interval. Then we convert each
value into the order number of the interval it
lies in. For example, if the height of one patient
is 172cm and it lies in the 8th interval, which is
[164.5, 173.0), we assign 8 to the height of the
patient. Thus, all features take values from 1
to 10.

III. Models

I. Logistic Regression and SVM

Since the supervised learning algorithms
we have learnt can only classify examples into
2 classes, our ﬁrst idea is to check whether an
example lies in class 1 to 16 one by one until
we ﬁnd the class it lies in and then stop. We
try both Logistic Regression and SVM.
First, we relabel class 1 to be 1 and class 2
to 16 to be −1. Then we can use Logistic Re-
gression or SVM to get a decision boundary. If
this decision boundary tells that a test example
belongs to 1, we predict it belongs to class 1
and then stop. Otherwise, we relabel class 2 to
be 1 and the other classes to be −1. We can get
a new decision boundary, which tells whether
the test example belongs to class 2 or not. We
continue doing this until we ﬁnd a class for
this test example. If we cannot ﬁnd a class for
it after the ﬁrst 15 trials, we predict it to be in
class 16.

These algorithms can work, but there are
two ﬂaws. First, why should we start from
class 1? If we start from a different class, we
might get a different prediction. Second, why
should we stop immediately after we get the
ﬁrst prediction. We can continue, pretending
we haven’t got a prediction. It is possible that
two different decision boundaries tell an exam-
ple belongs to class i and class j respectively.
Which one should we believe? Due to these
two ﬂaws, we want to modify SVM later so
that it becomes more reasonable and reliable
when used for multiclass classiﬁcations.

II. Multiclass Naive Bayes

After data preprocessing, all the features
can only take values from 1 to 10. Also,

2

we would like to classify training set into 16
classes. To parameterize a multinomial over k
outcomes, we can use k parameters φ1,· · · , φk
specifying the probability of each of the out-
comes. We deﬁne φk and φj,s|y=k as follows:

φk = P(y = k)

φj,s|y=k = P(xj = s|y = k)

16∑

k=1

φk = 1

(1)
(2)

(3)

10∑

s=1

φj,s|y=k = 1

(4)
We assume that all xj|y = k are indepen-
dent. Then using formulas of conditional prob-
ability, we have the probability of the whole
training set:

(cid:16) n∏

j=1

P(x, y) =

m∏

i=1

(cid:17)

φj,x(i)
j

|y=y(i)

φy(i)

(5)

Maximizing P(x, y) under the constraints

(3) and (4), we get the following formulas:

φk =

∑m

i=1 1{y(i)=k}

m

φj,s|y=k =

∑m
i=1 1{y(i)=k}1{x(i)
i=1 1{y(i)=k}

∑m

j =s}

(6)

(7)

And we can also use Laplace Smoothing to
modify the formulas above.

To make predictions, we use Bayes for-

mula:

P(x|y = k)P(y = k)

P(y = k|x) =
(8)
We predict y to be arg maxk P(x|y =

P(x)

k)P(y = k).

III. Multiclass SVM

In the lecture, we talked about using SVM
to classify our training examples into two
classes. We want to modify the algorithm so
that it can be used to make multiclass classiﬁ-
cations.

First, we draw k hyperplanes in the feature
j x + bj = 0

space, so that the jth hyperplane ωT

can separate training examples labelled j from
those not labelled j. This can be easily achieved
by using SVM algorithms. Each hyperplane
tells whether a training example belongs to
class j or not. However, chances are that the ith
hyperplane tells a training example belongs to
class i while the jth hyperplane tells it belongs
to class j. We want to know which one is more
reliable.

not belong to class ’GREEN’. Therefore, we pre-
dict it to be in class ’BLUE’. We can easily check
other cases and obtain the same results as the
algorithm achieves, which is quite reasonable.

We deﬁne the geometric margin of a train-
ing example with respect to the jth hyperplane
as γj:

|ωT

j x + bj|
(cid:107)ωj(cid:107)

γj =

(9)

If γj is small, then ωT
j x + bj is likely to change
its sign even if the jth hyperplane changes a lit-
tle bit. Therefore, if γj is larger, the conclusion
the jth hyperplane makes whether it belongs to
class j is more reliable.

After we calculate all the geometric mar-
gins, we sort them from the largest to the small-
est. We ﬁrst look at the largest one j1. If it tells
this example belongs to class j1, we trust it and
assign y = j1. If it tells this example doesn’t
belong to the class j1, we also trust it and then
we look at the next largest one j2. If it tells this
example belongs to the class j2, we trust it and
assign y = j2. Otherwise, we look at the next
largest one j3, until we ﬁnd some j and it tells
this example belongs to class j. If none of all
the hyperplanes tell this example belongs to
some class, we predict it should be in the class
with the smallest geometric margin.

Please look at the following simple exam-
ple. If we consider 3 classes with 2D feature
space, it is what the algorithm above attains.
First, we use SVM to separate blue dots from
the others with a blue dashed line, separate
red dots from the others with a red dashed line
and separate green dots from the others with
a green dashed line. Then we draw three lines
which are the angle bisectors and divide the
plane into three areas. Now we want to classify
the black dot. We ﬁnd its geometric margin
to the blue dashed line is the smallest. The
red dashed line tells it does not belong to class
’RED’ and the green dashed line tells it does

Figure 1: Simple Example

So we conclude our algorithm as follows:

Table 1: Algorithm

Pseudo Code
for j=1:16

if y(i) = j

else

set Y(i) = 1;
set Y(i) = −1;

end
run SVM using (X, Y) to get ωj and bj;

end
for j=1:16

calculate γj =

j x+bj|
|ωT
(cid:107)ωj(cid:107)

;

end
sort γj from the largest to the smallest;
ﬁnd the ﬁrst j such that ωT
j x + bj > 0;
if such j can be found

assign y = j;

else

end

assign y = j16;

In addition, if the training examples are
not well separable, we can also use SVM with

3

Predicting Heart Attacks

Luyang Chen, Qi Cao, Sihua Li, Xiao Ju

Stanford University

lych, qcao, sihua, xju@stanford.edu

Abstract

This paper aims at a better understanding and application of machine learning in medical
domain. In this paper, we modify three classical models for multiclass problems: Logistic Re-
gression, Naive Bayes and SVM, and then implement them to predict cardiac arrhythmia based
on patients’ medical records. First, we use all features provided to build the models. By com-
paring the accuracy of different models, we ﬁnd that Multiclass Naive Bayes and SVM models
have better performance. Afterwards, we implement feature selection to improve the accuracy
of prediction. Forward search procedure is used to choose different amount of features for each
model. By comparing the accuracy between using all features and using features selected, we
ﬁnd that feature selection can signiﬁcantly enhance the performance of our models.

I.

Introduction

In the medical industry, machine learning
algorithms can be used to diagnose some se-
rious diseases. Among all diseases, cardiac
arrhythmia is the top one cause of death in
the world, claiming more lives than cancer and
HIV combined. Thus, how to predict cardiac
arrhythmia in real life is of great signiﬁcance,
both to research and application.

In this paper, we apply machine learning
algorithms to predict cardiac arrhythmia based
on a patient’s medical record. We use the UCI
Arrhythmia Data Set for both training and test-
ing. We are provided with 452 clinical records
of patients. Each record contains 279 attributes,
such as age, sex, weight and information col-
lected from ECG signals. The diagnosis of
cardiac arrhythmia is divided into 16 classes.
Class 1 refers to normal case. Class 2 to 15
represent different kinds of cardiac arrhyth-
mia, such as Ischemic Changes, Old Anterior
Myocardial Infarction, Supraventricular Prema-
ture Contraction, Right Bundle Branch Block
and etc. Class 16 refers to the rest.

Our objective is to classify a patient into
one class according to his or her clinical mea-
surements. This paper applies Logistic Regres-
sion, Naive Bayes, and SVM algorithms to this
realistic problem, compares their accuracy, and

modiﬁes the models to get the best possible
results.

II. Features and Preprocessing

I. Extract valid columns

The dataset contains 279 columns rep-
resenting 279 features, but some data in 5
columns are invalid or missing. So these 5
columns are completely ignored. We extract
the rest 274 columns to form a new examples
matrix for modeling.

II. Data discretization

After observing the raw data provided, we
notice that some features have discrete values,
such as age, sex, etc. while the other features
have continuous, real values, such as the fea-
tures extracted from ECG signals (e.g. ampli-
tude of waves of each channel). Since we plan
to use Naive Bayes algorithm, in which the in-
put examples are discrete-valued, we have to
turn all the raw data into discrete values before
computing the probabilities. For the conve-
nience of computation, we divide the values of
each feature into 10 intervals. For the features
that can only take two possible values 0 or 1,
we consider 0 lies in the ﬁrst interval and 1

1

lies in the 10th interval. Then we convert each
value into the order number of the interval it
lies in. For example, if the height of one patient
is 172cm and it lies in the 8th interval, which is
[164.5, 173.0), we assign 8 to the height of the
patient. Thus, all features take values from 1
to 10.

III. Models

I. Logistic Regression and SVM

Since the supervised learning algorithms
we have learnt can only classify examples into
2 classes, our ﬁrst idea is to check whether an
example lies in class 1 to 16 one by one until
we ﬁnd the class it lies in and then stop. We
try both Logistic Regression and SVM.
First, we relabel class 1 to be 1 and class 2
to 16 to be −1. Then we can use Logistic Re-
gression or SVM to get a decision boundary. If
this decision boundary tells that a test example
belongs to 1, we predict it belongs to class 1
and then stop. Otherwise, we relabel class 2 to
be 1 and the other classes to be −1. We can get
a new decision boundary, which tells whether
the test example belongs to class 2 or not. We
continue doing this until we ﬁnd a class for
this test example. If we cannot ﬁnd a class for
it after the ﬁrst 15 trials, we predict it to be in
class 16.

These algorithms can work, but there are
two ﬂaws. First, why should we start from
class 1? If we start from a different class, we
might get a different prediction. Second, why
should we stop immediately after we get the
ﬁrst prediction. We can continue, pretending
we haven’t got a prediction. It is possible that
two different decision boundaries tell an exam-
ple belongs to class i and class j respectively.
Which one should we believe? Due to these
two ﬂaws, we want to modify SVM later so
that it becomes more reasonable and reliable
when used for multiclass classiﬁcations.

II. Multiclass Naive Bayes

After data preprocessing, all the features
can only take values from 1 to 10. Also,

2

we would like to classify training set into 16
classes. To parameterize a multinomial over k
outcomes, we can use k parameters φ1,· · · , φk
specifying the probability of each of the out-
comes. We deﬁne φk and φj,s|y=k as follows:

φk = P(y = k)

φj,s|y=k = P(xj = s|y = k)

16∑

k=1

φk = 1

(1)
(2)

(3)

10∑

s=1

φj,s|y=k = 1

(4)
We assume that all xj|y = k are indepen-
dent. Then using formulas of conditional prob-
ability, we have the probability of the whole
training set:

(cid:16) n∏

j=1

P(x, y) =

m∏

i=1

(cid:17)

φj,x(i)
j

|y=y(i)

φy(i)

(5)

Maximizing P(x, y) under the constraints

(3) and (4), we get the following formulas:

φk =

∑m

i=1 1{y(i)=k}

m

φj,s|y=k =

∑m
i=1 1{y(i)=k}1{x(i)
i=1 1{y(i)=k}

∑m

j =s}

(6)

(7)

And we can also use Laplace Smoothing to
modify the formulas above.

To make predictions, we use Bayes for-

mula:

P(x|y = k)P(y = k)

P(y = k|x) =
(8)
We predict y to be arg maxk P(x|y =

P(x)

k)P(y = k).

III. Multiclass SVM

In the lecture, we talked about using SVM
to classify our training examples into two
classes. We want to modify the algorithm so
that it can be used to make multiclass classiﬁ-
cations.

First, we draw k hyperplanes in the feature
j x + bj = 0

space, so that the jth hyperplane ωT

can separate training examples labelled j from
those not labelled j. This can be easily achieved
by using SVM algorithms. Each hyperplane
tells whether a training example belongs to
class j or not. However, chances are that the ith
hyperplane tells a training example belongs to
class i while the jth hyperplane tells it belongs
to class j. We want to know which one is more
reliable.

not belong to class ’GREEN’. Therefore, we pre-
dict it to be in class ’BLUE’. We can easily check
other cases and obtain the same results as the
algorithm achieves, which is quite reasonable.

We deﬁne the geometric margin of a train-
ing example with respect to the jth hyperplane
as γj:

|ωT

j x + bj|
(cid:107)ωj(cid:107)

γj =

(9)

If γj is small, then ωT
j x + bj is likely to change
its sign even if the jth hyperplane changes a lit-
tle bit. Therefore, if γj is larger, the conclusion
the jth hyperplane makes whether it belongs to
class j is more reliable.

After we calculate all the geometric mar-
gins, we sort them from the largest to the small-
est. We ﬁrst look at the largest one j1. If it tells
this example belongs to class j1, we trust it and
assign y = j1. If it tells this example doesn’t
belong to the class j1, we also trust it and then
we look at the next largest one j2. If it tells this
example belongs to the class j2, we trust it and
assign y = j2. Otherwise, we look at the next
largest one j3, until we ﬁnd some j and it tells
this example belongs to class j. If none of all
the hyperplanes tell this example belongs to
some class, we predict it should be in the class
with the smallest geometric margin.

Please look at the following simple exam-
ple. If we consider 3 classes with 2D feature
space, it is what the algorithm above attains.
First, we use SVM to separate blue dots from
the others with a blue dashed line, separate
red dots from the others with a red dashed line
and separate green dots from the others with
a green dashed line. Then we draw three lines
which are the angle bisectors and divide the
plane into three areas. Now we want to classify
the black dot. We ﬁnd its geometric margin
to the blue dashed line is the smallest. The
red dashed line tells it does not belong to class
’RED’ and the green dashed line tells it does

Figure 1: Simple Example

So we conclude our algorithm as follows:

Table 1: Algorithm

Pseudo Code
for j=1:16

if y(i) = j

else

set Y(i) = 1;
set Y(i) = −1;

end
run SVM using (X, Y) to get ωj and bj;

end
for j=1:16

calculate γj =

j x+bj|
|ωT
(cid:107)ωj(cid:107)

;

end
sort γj from the largest to the smallest;
ﬁnd the ﬁrst j such that ωT
j x + bj > 0;
if such j can be found

assign y = j;

else

end

assign y = j16;

In addition, if the training examples are
not well separable, we can also use SVM with

3

l1 regularization. Just need to modify this algo-
rithm and run SVM with l1 regularization for
16 times to get 16 hyperplanes.

IV. Feature Selection and Results

For classiﬁcation problems, the accuracy
is a vital performance measurement of the
classiﬁer. To test the accuracy, 10-fold cross-
validation technique is used in our experi-
ments.

Since the results we obtain by using all

features are unsatisfactory, we decide to imple-
ment feature selection to our above-mentioned
models to improve the accuracy.

In class, three kinds of heuristic search
procedures used for feature selection were in-
troduced: forward search, backward search,
and ﬁlter feature selection. We choose forward
search because of its easy implementation and
good performance.

For our four models, accuracy before fea-
ture selection, after feature selection and num-
bers of selected features are shown in the fol-
lowing table.

Table 2: Results

Model

Accuracy (before) Number of Selected Features Accuracy (after)

Logistic Regression

SVM

Multiclass Naive Bayes

Multiclass SVM

50.67%
62.44%
64.28%
61.11%

42
50
52
—

68.22%
75.56%
72.67%

—

Figure 2: Feature Selection for Logistic Regression

Figure 3: Feature Selection for Multiclass Naive Bayes

V. Discussion

In our project, before feature selection,
Naive Bayes achieves lower cross validation
error than SVM. While after feature selection,
SVM achieves lower cross validation error than
Naive Bayes. We think the problem may lie
in the lack of enough training examples(475)

and excessive amount of features(274).
Just
as we can see in Problem Set 2 when we use
both Naive Bayes and SVM to classify spams
and non-spams, SVM algorithm works better if
there are more training examples. However, in
this problem, only 475 training examples are
available.

4

Predicting Heart Attacks

Luyang Chen, Qi Cao, Sihua Li, Xiao Ju

Stanford University

lych, qcao, sihua, xju@stanford.edu

Abstract

This paper aims at a better understanding and application of machine learning in medical
domain. In this paper, we modify three classical models for multiclass problems: Logistic Re-
gression, Naive Bayes and SVM, and then implement them to predict cardiac arrhythmia based
on patients’ medical records. First, we use all features provided to build the models. By com-
paring the accuracy of different models, we ﬁnd that Multiclass Naive Bayes and SVM models
have better performance. Afterwards, we implement feature selection to improve the accuracy
of prediction. Forward search procedure is used to choose different amount of features for each
model. By comparing the accuracy between using all features and using features selected, we
ﬁnd that feature selection can signiﬁcantly enhance the performance of our models.

I.

Introduction

In the medical industry, machine learning
algorithms can be used to diagnose some se-
rious diseases. Among all diseases, cardiac
arrhythmia is the top one cause of death in
the world, claiming more lives than cancer and
HIV combined. Thus, how to predict cardiac
arrhythmia in real life is of great signiﬁcance,
both to research and application.

In this paper, we apply machine learning
algorithms to predict cardiac arrhythmia based
on a patient’s medical record. We use the UCI
Arrhythmia Data Set for both training and test-
ing. We are provided with 452 clinical records
of patients. Each record contains 279 attributes,
such as age, sex, weight and information col-
lected from ECG signals. The diagnosis of
cardiac arrhythmia is divided into 16 classes.
Class 1 refers to normal case. Class 2 to 15
represent different kinds of cardiac arrhyth-
mia, such as Ischemic Changes, Old Anterior
Myocardial Infarction, Supraventricular Prema-
ture Contraction, Right Bundle Branch Block
and etc. Class 16 refers to the rest.

Our objective is to classify a patient into
one class according to his or her clinical mea-
surements. This paper applies Logistic Regres-
sion, Naive Bayes, and SVM algorithms to this
realistic problem, compares their accuracy, and

modiﬁes the models to get the best possible
results.

II. Features and Preprocessing

I. Extract valid columns

The dataset contains 279 columns rep-
resenting 279 features, but some data in 5
columns are invalid or missing. So these 5
columns are completely ignored. We extract
the rest 274 columns to form a new examples
matrix for modeling.

II. Data discretization

After observing the raw data provided, we
notice that some features have discrete values,
such as age, sex, etc. while the other features
have continuous, real values, such as the fea-
tures extracted from ECG signals (e.g. ampli-
tude of waves of each channel). Since we plan
to use Naive Bayes algorithm, in which the in-
put examples are discrete-valued, we have to
turn all the raw data into discrete values before
computing the probabilities. For the conve-
nience of computation, we divide the values of
each feature into 10 intervals. For the features
that can only take two possible values 0 or 1,
we consider 0 lies in the ﬁrst interval and 1

1

lies in the 10th interval. Then we convert each
value into the order number of the interval it
lies in. For example, if the height of one patient
is 172cm and it lies in the 8th interval, which is
[164.5, 173.0), we assign 8 to the height of the
patient. Thus, all features take values from 1
to 10.

III. Models

I. Logistic Regression and SVM

Since the supervised learning algorithms
we have learnt can only classify examples into
2 classes, our ﬁrst idea is to check whether an
example lies in class 1 to 16 one by one until
we ﬁnd the class it lies in and then stop. We
try both Logistic Regression and SVM.
First, we relabel class 1 to be 1 and class 2
to 16 to be −1. Then we can use Logistic Re-
gression or SVM to get a decision boundary. If
this decision boundary tells that a test example
belongs to 1, we predict it belongs to class 1
and then stop. Otherwise, we relabel class 2 to
be 1 and the other classes to be −1. We can get
a new decision boundary, which tells whether
the test example belongs to class 2 or not. We
continue doing this until we ﬁnd a class for
this test example. If we cannot ﬁnd a class for
it after the ﬁrst 15 trials, we predict it to be in
class 16.

These algorithms can work, but there are
two ﬂaws. First, why should we start from
class 1? If we start from a different class, we
might get a different prediction. Second, why
should we stop immediately after we get the
ﬁrst prediction. We can continue, pretending
we haven’t got a prediction. It is possible that
two different decision boundaries tell an exam-
ple belongs to class i and class j respectively.
Which one should we believe? Due to these
two ﬂaws, we want to modify SVM later so
that it becomes more reasonable and reliable
when used for multiclass classiﬁcations.

II. Multiclass Naive Bayes

After data preprocessing, all the features
can only take values from 1 to 10. Also,

2

we would like to classify training set into 16
classes. To parameterize a multinomial over k
outcomes, we can use k parameters φ1,· · · , φk
specifying the probability of each of the out-
comes. We deﬁne φk and φj,s|y=k as follows:

φk = P(y = k)

φj,s|y=k = P(xj = s|y = k)

16∑

k=1

φk = 1

(1)
(2)

(3)

10∑

s=1

φj,s|y=k = 1

(4)
We assume that all xj|y = k are indepen-
dent. Then using formulas of conditional prob-
ability, we have the probability of the whole
training set:

(cid:16) n∏

j=1

P(x, y) =

m∏

i=1

(cid:17)

φj,x(i)
j

|y=y(i)

φy(i)

(5)

Maximizing P(x, y) under the constraints

(3) and (4), we get the following formulas:

φk =

∑m

i=1 1{y(i)=k}

m

φj,s|y=k =

∑m
i=1 1{y(i)=k}1{x(i)
i=1 1{y(i)=k}

∑m

j =s}

(6)

(7)

And we can also use Laplace Smoothing to
modify the formulas above.

To make predictions, we use Bayes for-

mula:

P(x|y = k)P(y = k)

P(y = k|x) =
(8)
We predict y to be arg maxk P(x|y =

P(x)

k)P(y = k).

III. Multiclass SVM

In the lecture, we talked about using SVM
to classify our training examples into two
classes. We want to modify the algorithm so
that it can be used to make multiclass classiﬁ-
cations.

First, we draw k hyperplanes in the feature
j x + bj = 0

space, so that the jth hyperplane ωT

can separate training examples labelled j from
those not labelled j. This can be easily achieved
by using SVM algorithms. Each hyperplane
tells whether a training example belongs to
class j or not. However, chances are that the ith
hyperplane tells a training example belongs to
class i while the jth hyperplane tells it belongs
to class j. We want to know which one is more
reliable.

not belong to class ’GREEN’. Therefore, we pre-
dict it to be in class ’BLUE’. We can easily check
other cases and obtain the same results as the
algorithm achieves, which is quite reasonable.

We deﬁne the geometric margin of a train-
ing example with respect to the jth hyperplane
as γj:

|ωT

j x + bj|
(cid:107)ωj(cid:107)

γj =

(9)

If γj is small, then ωT
j x + bj is likely to change
its sign even if the jth hyperplane changes a lit-
tle bit. Therefore, if γj is larger, the conclusion
the jth hyperplane makes whether it belongs to
class j is more reliable.

After we calculate all the geometric mar-
gins, we sort them from the largest to the small-
est. We ﬁrst look at the largest one j1. If it tells
this example belongs to class j1, we trust it and
assign y = j1. If it tells this example doesn’t
belong to the class j1, we also trust it and then
we look at the next largest one j2. If it tells this
example belongs to the class j2, we trust it and
assign y = j2. Otherwise, we look at the next
largest one j3, until we ﬁnd some j and it tells
this example belongs to class j. If none of all
the hyperplanes tell this example belongs to
some class, we predict it should be in the class
with the smallest geometric margin.

Please look at the following simple exam-
ple. If we consider 3 classes with 2D feature
space, it is what the algorithm above attains.
First, we use SVM to separate blue dots from
the others with a blue dashed line, separate
red dots from the others with a red dashed line
and separate green dots from the others with
a green dashed line. Then we draw three lines
which are the angle bisectors and divide the
plane into three areas. Now we want to classify
the black dot. We ﬁnd its geometric margin
to the blue dashed line is the smallest. The
red dashed line tells it does not belong to class
’RED’ and the green dashed line tells it does

Figure 1: Simple Example

So we conclude our algorithm as follows:

Table 1: Algorithm

Pseudo Code
for j=1:16

if y(i) = j

else

set Y(i) = 1;
set Y(i) = −1;

end
run SVM using (X, Y) to get ωj and bj;

end
for j=1:16

calculate γj =

j x+bj|
|ωT
(cid:107)ωj(cid:107)

;

end
sort γj from the largest to the smallest;
ﬁnd the ﬁrst j such that ωT
j x + bj > 0;
if such j can be found

assign y = j;

else

end

assign y = j16;

In addition, if the training examples are
not well separable, we can also use SVM with

3

l1 regularization. Just need to modify this algo-
rithm and run SVM with l1 regularization for
16 times to get 16 hyperplanes.

IV. Feature Selection and Results

For classiﬁcation problems, the accuracy
is a vital performance measurement of the
classiﬁer. To test the accuracy, 10-fold cross-
validation technique is used in our experi-
ments.

Since the results we obtain by using all

features are unsatisfactory, we decide to imple-
ment feature selection to our above-mentioned
models to improve the accuracy.

In class, three kinds of heuristic search
procedures used for feature selection were in-
troduced: forward search, backward search,
and ﬁlter feature selection. We choose forward
search because of its easy implementation and
good performance.

For our four models, accuracy before fea-
ture selection, after feature selection and num-
bers of selected features are shown in the fol-
lowing table.

Table 2: Results

Model

Accuracy (before) Number of Selected Features Accuracy (after)

Logistic Regression

SVM

Multiclass Naive Bayes

Multiclass SVM

50.67%
62.44%
64.28%
61.11%

42
50
52
—

68.22%
75.56%
72.67%

—

Figure 2: Feature Selection for Logistic Regression

Figure 3: Feature Selection for Multiclass Naive Bayes

V. Discussion

In our project, before feature selection,
Naive Bayes achieves lower cross validation
error than SVM. While after feature selection,
SVM achieves lower cross validation error than
Naive Bayes. We think the problem may lie
in the lack of enough training examples(475)

and excessive amount of features(274).
Just
as we can see in Problem Set 2 when we use
both Naive Bayes and SVM to classify spams
and non-spams, SVM algorithm works better if
there are more training examples. However, in
this problem, only 475 training examples are
available.

4

As shown in the Table 2, we didn’t imple-
ment feature selection for Multiclass SVM. This
is because all the codes are written by ourselves
and they are not efﬁcient enough to complete
feature selection within acceptable time.

VI. Future

There are certainly rooms for improve-
ment. First, we can extract new features. Since
we are provided with ECG raw data, methods
like FFT and wavelet decomposition can be
used to gain new features that cannot be easily
recognized in time domain. Then we need to
give these features some physiological explana-
tions. Moreover, we can also use deep learning
methods to generate new features. Also, we

can use PCA to reduce the dimension of fea-
ture space and ﬁgure out what features are
informative.

References

[1] Guvenir, H. Altay, et al. "A supervised ma-
chine learning algorithm for arrhythmia
analysis." Computers in Cardiology 1997.
IEEE, 1997.

[2] Mishra, Binod Kumar, Prashant Lakkad-
wala, and Naveen Kumar Shrivastava.
"Novel Approach to Predict Cardiovas-
cular Disease Using Incremental SVM."
Communication Systems and Network
Technologies (CSNT), 2013 International
Conference on. IEEE, 2013.

5

