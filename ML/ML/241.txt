1

Predicting and Identifying Hypertext in

Wikipedia Articles

Neel Guha, Annie Hu, and Cindy Wang
{nguha, anniehu, ciwang}@stanford.edu

I. Introduction

II. Related Work

Wikipedia now has millions of articles online and
tens of millions of views each hour. In order to
maintain a high standard of organization across this
sheer volume of content, it is essential to ensure accu-
rate and helpful intra-article linking. The Wikipedia
Manual of Style states that links represent "major
connections with the subject of another article that
will help readers to understand the current article
more fully."1

In this project, we optimize article linking towards
this goal by applying machine learning to predict
hypertext in Wikipedia articles (see Figure 1 for
an example of hypertext). As inputs, our algorithm
processes tokens (words and n-grams) from articles.
We then use Naive Bayes, logistic regression, and a
SVM to output a predicted classiﬁcation: plain text
(no link) or hypertext (link). In this paper we describe
the process for our data collection, feature selection,
and the relative performance of various algorithms.
Note that we have limited the scope of this project
to predicting solely hypertext and not hyperlinks.
While there has been considerable work on predicting
hyperlinks (especially using context resolution based
techniques), we felt there had been signiﬁcantly less
work on predicting hypertext.

Our model enables large scale automated editing of
Wikipedia linking in a low cost but eﬀective way and
can also be used to make link suggestions to individual
contributors. Outside of Wikipedia, our system can
be generalized to any broad database of documents
or texts. For example, it could be applied to predict
hypertext in news corpora or academic literature.

There is considerable prior work involving using
existing Wikipedia links for entity disambiguation
[1]. Rather than general applications, the goal of our
paper is to identify semantically important hypertext
within the articles themselves.

Prior approaches to the problem of hypertext detec-
tion rely heavily on keyword identiﬁcation algorithms.
Generally, these approaches score potential keywords
by evaluating 1), word occurrence statistics or 2),
linking patterns on similar pages. The traditional
statistical approach, which depends solely on word
occurence counts, involves methods such as tf-idf, χ2
independence test, and "keyphraseness" (proportion
of occurrences that are hypertext) across the training
set [2]. These methods have moderate success, with
upper precision/recall/F1 scores in the 0.5-0.6 range.
The second approach involves semantic analysis to
calculate metrics such as contextual commonness and
relatedness. [3] and [4] outline speciﬁc formulas to
evaluate these measures. [5] introduces a novel al-
gorithm (LTRank) to identify similar pages, then
identify candidate links from those similar pages that
might be missing on the given page. The state-of-the-
art algorithm for link detection has a precision and
recall of approximately 0.75 [4].

We aim to develop a better model by incorporating
not only statistical and contextual features, but con-
sidering the absolute importance of each word itself;
i.e. proper nouns are more likely to be relevant links.
[6] suggests a computational linguistics approach to
keyword extraction involving part of speech (POS)
tagging in additional to statistical features. We ex-
plain our feature selection in more detail below.

Fig. 1.
cian’ are hypertext.

In this sentence, the terms ’American’ and ’academi-

1http://en.wikipedia.org/wiki/Wikipedia:Manual of Style

III. Data Set and Features

For clarity, we deﬁne "hypertext" as a word in
an article that appears, in the article content, as a
clickable link to another article. Our algorithm works
as follows:

1) Model Training: For each article ai in a training
set of articles A, we we extract each unique token
(word) ti from ai and represent ai as the set of unique

1

Predicting and Identifying Hypertext in

Wikipedia Articles

Neel Guha, Annie Hu, and Cindy Wang
{nguha, anniehu, ciwang}@stanford.edu

I. Introduction

II. Related Work

Wikipedia now has millions of articles online and
tens of millions of views each hour. In order to
maintain a high standard of organization across this
sheer volume of content, it is essential to ensure accu-
rate and helpful intra-article linking. The Wikipedia
Manual of Style states that links represent "major
connections with the subject of another article that
will help readers to understand the current article
more fully."1

In this project, we optimize article linking towards
this goal by applying machine learning to predict
hypertext in Wikipedia articles (see Figure 1 for
an example of hypertext). As inputs, our algorithm
processes tokens (words and n-grams) from articles.
We then use Naive Bayes, logistic regression, and a
SVM to output a predicted classiﬁcation: plain text
(no link) or hypertext (link). In this paper we describe
the process for our data collection, feature selection,
and the relative performance of various algorithms.
Note that we have limited the scope of this project
to predicting solely hypertext and not hyperlinks.
While there has been considerable work on predicting
hyperlinks (especially using context resolution based
techniques), we felt there had been signiﬁcantly less
work on predicting hypertext.

Our model enables large scale automated editing of
Wikipedia linking in a low cost but eﬀective way and
can also be used to make link suggestions to individual
contributors. Outside of Wikipedia, our system can
be generalized to any broad database of documents
or texts. For example, it could be applied to predict
hypertext in news corpora or academic literature.

There is considerable prior work involving using
existing Wikipedia links for entity disambiguation
[1]. Rather than general applications, the goal of our
paper is to identify semantically important hypertext
within the articles themselves.

Prior approaches to the problem of hypertext detec-
tion rely heavily on keyword identiﬁcation algorithms.
Generally, these approaches score potential keywords
by evaluating 1), word occurrence statistics or 2),
linking patterns on similar pages. The traditional
statistical approach, which depends solely on word
occurence counts, involves methods such as tf-idf, χ2
independence test, and "keyphraseness" (proportion
of occurrences that are hypertext) across the training
set [2]. These methods have moderate success, with
upper precision/recall/F1 scores in the 0.5-0.6 range.
The second approach involves semantic analysis to
calculate metrics such as contextual commonness and
relatedness. [3] and [4] outline speciﬁc formulas to
evaluate these measures. [5] introduces a novel al-
gorithm (LTRank) to identify similar pages, then
identify candidate links from those similar pages that
might be missing on the given page. The state-of-the-
art algorithm for link detection has a precision and
recall of approximately 0.75 [4].

We aim to develop a better model by incorporating
not only statistical and contextual features, but con-
sidering the absolute importance of each word itself;
i.e. proper nouns are more likely to be relevant links.
[6] suggests a computational linguistics approach to
keyword extraction involving part of speech (POS)
tagging in additional to statistical features. We ex-
plain our feature selection in more detail below.

Fig. 1.
cian’ are hypertext.

In this sentence, the terms ’American’ and ’academi-

1http://en.wikipedia.org/wiki/Wikipedia:Manual of Style

III. Data Set and Features

For clarity, we deﬁne "hypertext" as a word in
an article that appears, in the article content, as a
clickable link to another article. Our algorithm works
as follows:

1) Model Training: For each article ai in a training
set of articles A, we we extract each unique token
(word) ti from ai and represent ai as the set of unique

tokens it contains. For each token we then construct a
feature vector xi based on the attributes of ti relative
to ai (its parent article) and A (all the of articles). If
in ai, ti is a word with a link to another Wikipedia
article, we label ti’s output vector yi as hypertext
(y=1). Otherwise we label it as plaintext (y = 0). The
set of all xi, yi’s across all ai in A forms our training
set.

2) Model Testing: When testing this model on an
article aT , we follow a similar initial procedure. We
extract each unique token from aT and construct its
corresponding feature vector. We then use the model
we trained above to classify the set of feature vectors
corresponding to the tokens in aT . If a feature vector
xi is classiﬁed as hypertext, then its corresponding
token is predicted to be linked on in the article aT .
Because Wikipedia only links on the ﬁrst occur-
rence of a word (as opposed to every occurrence of
the word), note that we represent each article by the
list of unique tokens it contains.

A. Features

To determine whether a word should be linked, we
thus considered features pertaining to the word itself,
as well as the word in the context of the article and
data set as a whole. The four features included in our
model are:

1: Whether or not a word is proper noun, 0 or 1.
Proper nouns typically denote signiﬁcant peo-
ple, places, or things, which usually have their
own Wikipedia pages. If these words appear in
an article, they should be linked.

2: Length of the word, an integer value. Longer
words are more likely to be less common, names,
or technical terms. In any of these cases, the
word should be linked.

3: The tf-idf score of the word (a ﬂoat value
between 0 and 1) relative to the article in which
the word appears in. Tf-idf, or term frequency-
inverse document frequency,
is a numerical
statistic that denotes how important a word is
to a document. Speciﬁcally, for a term t in a
document d ∈ D, tﬁdf(t, d) = tf(t, d)× idf(t, D)
where tf(t, d) is the normalized frequency of
term t in document d and idf(t, D) is the log
of the proportion of the number of documents
in D containing the term t.

4: The proportion of [the number of articles in
which the word is linked] to [the number of
articles in which the word is mentioned but not
linked], a ﬂoat value. This gave us a measure of
how often a word was linked if it was mentioned.

B. Data set and data collection

2

The words that constitute links to other pages
diﬀer signiﬁcantly from page to page and are highly
dependent on the context of the page. Entities in an
article that are more closely related to the topic of
the article are more likely to constitute links then
entities that are mentioned in passing. For example,
in the Wikipedia article on Paul Ryan, the entity
"marathon" is mentioned but does not link to its
corresponding wikipedia page. However in the page
for "Running", "marathon" is mentioned and links to
its corresponding Wikipedia page. Because of this,
attempting to predict hypertext across a set of ar-
ticles covering diverse topics can be tricky. With such
diverse data, it can be hard to determine meaningful
conclusions on which terms should and shouldn’t
constitute links.

We hypothesize that accurately predicting hyper-
text must leverage an implicit understanding of the
article’s context and topical focus. We tested this by
running our model on two diﬀerent data sets.

1: Intuitively, articles from the same Wikipedia
category should have similar distributions of
hypertext. Limiting the articles in the training
and testing set to one category thus allows
the model to implicitly account for the broader
topic/context of the articles. In this project, we
selected random samples of 100 training articles
and 30 test articles from the Wikipedia "Forms
of Government" category. We called this our
government dataset.

2: In order to test our model

in a case where
the articles
in the dataset do not origi-
nate from the same context, we ran our
model on a 600 training articles and 200
test articles uniformly drawn from a mixture
of 4 Wikipedia categories ("Judaism","Ancient
Greece","Environmental science","Technology").
We called this our multicategory dataset.

We scraped and parsed each of these articles using
BeautifulSoup. For each article we extracted a list of
unigrams/bigrams as well as a list of terms that are
linked on.

We evaluated a variety of possible features to
model. We treated each unique word in a given article
as a separate training example.

Creating the feature vectors for the training set
required signiﬁcant pre-processing of our data. We
utilized the NLTK and scikit libraries to perform part
of speech classiﬁcation and tf-idf scoring, respectively.
We applied NLTK part of speech tagging to the
tokenized plaintext of each article, and ﬁltered the

1

Predicting and Identifying Hypertext in

Wikipedia Articles

Neel Guha, Annie Hu, and Cindy Wang
{nguha, anniehu, ciwang}@stanford.edu

I. Introduction

II. Related Work

Wikipedia now has millions of articles online and
tens of millions of views each hour. In order to
maintain a high standard of organization across this
sheer volume of content, it is essential to ensure accu-
rate and helpful intra-article linking. The Wikipedia
Manual of Style states that links represent "major
connections with the subject of another article that
will help readers to understand the current article
more fully."1

In this project, we optimize article linking towards
this goal by applying machine learning to predict
hypertext in Wikipedia articles (see Figure 1 for
an example of hypertext). As inputs, our algorithm
processes tokens (words and n-grams) from articles.
We then use Naive Bayes, logistic regression, and a
SVM to output a predicted classiﬁcation: plain text
(no link) or hypertext (link). In this paper we describe
the process for our data collection, feature selection,
and the relative performance of various algorithms.
Note that we have limited the scope of this project
to predicting solely hypertext and not hyperlinks.
While there has been considerable work on predicting
hyperlinks (especially using context resolution based
techniques), we felt there had been signiﬁcantly less
work on predicting hypertext.

Our model enables large scale automated editing of
Wikipedia linking in a low cost but eﬀective way and
can also be used to make link suggestions to individual
contributors. Outside of Wikipedia, our system can
be generalized to any broad database of documents
or texts. For example, it could be applied to predict
hypertext in news corpora or academic literature.

There is considerable prior work involving using
existing Wikipedia links for entity disambiguation
[1]. Rather than general applications, the goal of our
paper is to identify semantically important hypertext
within the articles themselves.

Prior approaches to the problem of hypertext detec-
tion rely heavily on keyword identiﬁcation algorithms.
Generally, these approaches score potential keywords
by evaluating 1), word occurrence statistics or 2),
linking patterns on similar pages. The traditional
statistical approach, which depends solely on word
occurence counts, involves methods such as tf-idf, χ2
independence test, and "keyphraseness" (proportion
of occurrences that are hypertext) across the training
set [2]. These methods have moderate success, with
upper precision/recall/F1 scores in the 0.5-0.6 range.
The second approach involves semantic analysis to
calculate metrics such as contextual commonness and
relatedness. [3] and [4] outline speciﬁc formulas to
evaluate these measures. [5] introduces a novel al-
gorithm (LTRank) to identify similar pages, then
identify candidate links from those similar pages that
might be missing on the given page. The state-of-the-
art algorithm for link detection has a precision and
recall of approximately 0.75 [4].

We aim to develop a better model by incorporating
not only statistical and contextual features, but con-
sidering the absolute importance of each word itself;
i.e. proper nouns are more likely to be relevant links.
[6] suggests a computational linguistics approach to
keyword extraction involving part of speech (POS)
tagging in additional to statistical features. We ex-
plain our feature selection in more detail below.

Fig. 1.
cian’ are hypertext.

In this sentence, the terms ’American’ and ’academi-

1http://en.wikipedia.org/wiki/Wikipedia:Manual of Style

III. Data Set and Features

For clarity, we deﬁne "hypertext" as a word in
an article that appears, in the article content, as a
clickable link to another article. Our algorithm works
as follows:

1) Model Training: For each article ai in a training
set of articles A, we we extract each unique token
(word) ti from ai and represent ai as the set of unique

tokens it contains. For each token we then construct a
feature vector xi based on the attributes of ti relative
to ai (its parent article) and A (all the of articles). If
in ai, ti is a word with a link to another Wikipedia
article, we label ti’s output vector yi as hypertext
(y=1). Otherwise we label it as plaintext (y = 0). The
set of all xi, yi’s across all ai in A forms our training
set.

2) Model Testing: When testing this model on an
article aT , we follow a similar initial procedure. We
extract each unique token from aT and construct its
corresponding feature vector. We then use the model
we trained above to classify the set of feature vectors
corresponding to the tokens in aT . If a feature vector
xi is classiﬁed as hypertext, then its corresponding
token is predicted to be linked on in the article aT .
Because Wikipedia only links on the ﬁrst occur-
rence of a word (as opposed to every occurrence of
the word), note that we represent each article by the
list of unique tokens it contains.

A. Features

To determine whether a word should be linked, we
thus considered features pertaining to the word itself,
as well as the word in the context of the article and
data set as a whole. The four features included in our
model are:

1: Whether or not a word is proper noun, 0 or 1.
Proper nouns typically denote signiﬁcant peo-
ple, places, or things, which usually have their
own Wikipedia pages. If these words appear in
an article, they should be linked.

2: Length of the word, an integer value. Longer
words are more likely to be less common, names,
or technical terms. In any of these cases, the
word should be linked.

3: The tf-idf score of the word (a ﬂoat value
between 0 and 1) relative to the article in which
the word appears in. Tf-idf, or term frequency-
inverse document frequency,
is a numerical
statistic that denotes how important a word is
to a document. Speciﬁcally, for a term t in a
document d ∈ D, tﬁdf(t, d) = tf(t, d)× idf(t, D)
where tf(t, d) is the normalized frequency of
term t in document d and idf(t, D) is the log
of the proportion of the number of documents
in D containing the term t.

4: The proportion of [the number of articles in
which the word is linked] to [the number of
articles in which the word is mentioned but not
linked], a ﬂoat value. This gave us a measure of
how often a word was linked if it was mentioned.

B. Data set and data collection

2

The words that constitute links to other pages
diﬀer signiﬁcantly from page to page and are highly
dependent on the context of the page. Entities in an
article that are more closely related to the topic of
the article are more likely to constitute links then
entities that are mentioned in passing. For example,
in the Wikipedia article on Paul Ryan, the entity
"marathon" is mentioned but does not link to its
corresponding wikipedia page. However in the page
for "Running", "marathon" is mentioned and links to
its corresponding Wikipedia page. Because of this,
attempting to predict hypertext across a set of ar-
ticles covering diverse topics can be tricky. With such
diverse data, it can be hard to determine meaningful
conclusions on which terms should and shouldn’t
constitute links.

We hypothesize that accurately predicting hyper-
text must leverage an implicit understanding of the
article’s context and topical focus. We tested this by
running our model on two diﬀerent data sets.

1: Intuitively, articles from the same Wikipedia
category should have similar distributions of
hypertext. Limiting the articles in the training
and testing set to one category thus allows
the model to implicitly account for the broader
topic/context of the articles. In this project, we
selected random samples of 100 training articles
and 30 test articles from the Wikipedia "Forms
of Government" category. We called this our
government dataset.

2: In order to test our model

in a case where
the articles
in the dataset do not origi-
nate from the same context, we ran our
model on a 600 training articles and 200
test articles uniformly drawn from a mixture
of 4 Wikipedia categories ("Judaism","Ancient
Greece","Environmental science","Technology").
We called this our multicategory dataset.

We scraped and parsed each of these articles using
BeautifulSoup. For each article we extracted a list of
unigrams/bigrams as well as a list of terms that are
linked on.

We evaluated a variety of possible features to
model. We treated each unique word in a given article
as a separate training example.

Creating the feature vectors for the training set
required signiﬁcant pre-processing of our data. We
utilized the NLTK and scikit libraries to perform part
of speech classiﬁcation and tf-idf scoring, respectively.
We applied NLTK part of speech tagging to the
tokenized plaintext of each article, and ﬁltered the

words tagged as proper nouns. We then used scikit
tf-idf term weighting to vectorize and score each word
in the context of the article in which it appeared. We
wrote custom scripts to determine the other features.

IV. Methods

In our algorithm, we experimented with the follow-

ing models:

A. Dummy Model

In order to establish a baseline for our algorithm’s
performance we used a dummy stratiﬁed classiﬁer
from the sklearn library [6]. This classiﬁer establishes
a baseline performance by randomly predicting a
class for each training example. Because our data
set contains a fairly signiﬁcant class imbalance (the
number of terms that aren’t links far exceeds the
number that are), the probability of picking each class
was weighted by the class distribution in the training
set.

B. Gaussian Naive Bayes

Naive Bayes models are useful for quick and initial
evaluations of a dataset to explore the feasibility
of building some classiﬁer. Though they make the
"naive" assumption of conditional independence be-
tween features, they’ve proved to be surprisingly reli-
able. For a yi with with an associated set of features
x0, ...xn:

nY

p(y)

P(xi|y)
P(x1, ..., xn)

i=1

P(yi|x1, ..., xn) =

(1)

We ﬁrst implemented a Gaussian Naive Bayes model
where the feature likelihood is assumed to follow a
Gaussian distribution.

C. Logistic Regression

We used a logistic regression model with l2 regu-
larization [7]. In this type of classiﬁer the hypothesis
function takes the form of a logistic curve:

hθ(x) =

1

1+exp(−θx)

(2)

where hθ(x) can be viewed as the probability y = 1
given an x. We used an l2 penalty with logistic
regression.

D. Support Vector Machine

3

Support Vector Machines attempt to classify data
by constructing a multidimensional hyperplane to
separate data points of diﬀering classes (in eﬀect
identifying a decision boundary). The ideal decision
boundary is given by a hyperplane which is at the
maximum distance from the closest points of each
class (referred to as support vectors). In the case
where the data is not linearly separable, SVMs utilize
Kernel functions to project the data into a higher
dimension where a linear separator can be found.
Speciﬁcally, SVMs operate by solving the optimiza-
tion problem:

mX

i,j=1

mX

αi − 1
maxαW(α) =
2
s.t. αi ≥ 0, i = 1, . . . , m

i=1

mX

αiy(i) = 0

y(i)y(j)αiαjhx(i), x(j)i(3)

(4)

(5)

i=1

V. Results and Discussion

A. Data

The government category dataset yielded 148,303 train-
ing feature vectors and 43,310 test feature vectors. For
both training and testing, around 36-37% of the vectors
corresponded to a linked word. The multicategory data
yielded 625,826 training feature vectors and 214,912 test
feature vectors. For both training and testing, around 31-
32% of vectors corresponded to a linked word.

Fig. 2. Hypertext in original article (top) vs. predicted hyper-
text from model (bottom).

Our confusion matrix after running logistic regression

on the government category dataset was:

Predict NO Predict YES

True NO
True YES

0.9287
0.4433

0.07135
0.5567

Although the model very accurately predicted true non-
linked words, over 44% of predictions were false negatives,
or words predicted as non-links that should actually have
been links. A large majority of these false negatives were
common words, such as "music", that were linked in the
article as part of a multi-word phrase, such as "Vietnamese
music". We hypothesized that updating our model to

1

Predicting and Identifying Hypertext in

Wikipedia Articles

Neel Guha, Annie Hu, and Cindy Wang
{nguha, anniehu, ciwang}@stanford.edu

I. Introduction

II. Related Work

Wikipedia now has millions of articles online and
tens of millions of views each hour. In order to
maintain a high standard of organization across this
sheer volume of content, it is essential to ensure accu-
rate and helpful intra-article linking. The Wikipedia
Manual of Style states that links represent "major
connections with the subject of another article that
will help readers to understand the current article
more fully."1

In this project, we optimize article linking towards
this goal by applying machine learning to predict
hypertext in Wikipedia articles (see Figure 1 for
an example of hypertext). As inputs, our algorithm
processes tokens (words and n-grams) from articles.
We then use Naive Bayes, logistic regression, and a
SVM to output a predicted classiﬁcation: plain text
(no link) or hypertext (link). In this paper we describe
the process for our data collection, feature selection,
and the relative performance of various algorithms.
Note that we have limited the scope of this project
to predicting solely hypertext and not hyperlinks.
While there has been considerable work on predicting
hyperlinks (especially using context resolution based
techniques), we felt there had been signiﬁcantly less
work on predicting hypertext.

Our model enables large scale automated editing of
Wikipedia linking in a low cost but eﬀective way and
can also be used to make link suggestions to individual
contributors. Outside of Wikipedia, our system can
be generalized to any broad database of documents
or texts. For example, it could be applied to predict
hypertext in news corpora or academic literature.

There is considerable prior work involving using
existing Wikipedia links for entity disambiguation
[1]. Rather than general applications, the goal of our
paper is to identify semantically important hypertext
within the articles themselves.

Prior approaches to the problem of hypertext detec-
tion rely heavily on keyword identiﬁcation algorithms.
Generally, these approaches score potential keywords
by evaluating 1), word occurrence statistics or 2),
linking patterns on similar pages. The traditional
statistical approach, which depends solely on word
occurence counts, involves methods such as tf-idf, χ2
independence test, and "keyphraseness" (proportion
of occurrences that are hypertext) across the training
set [2]. These methods have moderate success, with
upper precision/recall/F1 scores in the 0.5-0.6 range.
The second approach involves semantic analysis to
calculate metrics such as contextual commonness and
relatedness. [3] and [4] outline speciﬁc formulas to
evaluate these measures. [5] introduces a novel al-
gorithm (LTRank) to identify similar pages, then
identify candidate links from those similar pages that
might be missing on the given page. The state-of-the-
art algorithm for link detection has a precision and
recall of approximately 0.75 [4].

We aim to develop a better model by incorporating
not only statistical and contextual features, but con-
sidering the absolute importance of each word itself;
i.e. proper nouns are more likely to be relevant links.
[6] suggests a computational linguistics approach to
keyword extraction involving part of speech (POS)
tagging in additional to statistical features. We ex-
plain our feature selection in more detail below.

Fig. 1.
cian’ are hypertext.

In this sentence, the terms ’American’ and ’academi-

1http://en.wikipedia.org/wiki/Wikipedia:Manual of Style

III. Data Set and Features

For clarity, we deﬁne "hypertext" as a word in
an article that appears, in the article content, as a
clickable link to another article. Our algorithm works
as follows:

1) Model Training: For each article ai in a training
set of articles A, we we extract each unique token
(word) ti from ai and represent ai as the set of unique

tokens it contains. For each token we then construct a
feature vector xi based on the attributes of ti relative
to ai (its parent article) and A (all the of articles). If
in ai, ti is a word with a link to another Wikipedia
article, we label ti’s output vector yi as hypertext
(y=1). Otherwise we label it as plaintext (y = 0). The
set of all xi, yi’s across all ai in A forms our training
set.

2) Model Testing: When testing this model on an
article aT , we follow a similar initial procedure. We
extract each unique token from aT and construct its
corresponding feature vector. We then use the model
we trained above to classify the set of feature vectors
corresponding to the tokens in aT . If a feature vector
xi is classiﬁed as hypertext, then its corresponding
token is predicted to be linked on in the article aT .
Because Wikipedia only links on the ﬁrst occur-
rence of a word (as opposed to every occurrence of
the word), note that we represent each article by the
list of unique tokens it contains.

A. Features

To determine whether a word should be linked, we
thus considered features pertaining to the word itself,
as well as the word in the context of the article and
data set as a whole. The four features included in our
model are:

1: Whether or not a word is proper noun, 0 or 1.
Proper nouns typically denote signiﬁcant peo-
ple, places, or things, which usually have their
own Wikipedia pages. If these words appear in
an article, they should be linked.

2: Length of the word, an integer value. Longer
words are more likely to be less common, names,
or technical terms. In any of these cases, the
word should be linked.

3: The tf-idf score of the word (a ﬂoat value
between 0 and 1) relative to the article in which
the word appears in. Tf-idf, or term frequency-
inverse document frequency,
is a numerical
statistic that denotes how important a word is
to a document. Speciﬁcally, for a term t in a
document d ∈ D, tﬁdf(t, d) = tf(t, d)× idf(t, D)
where tf(t, d) is the normalized frequency of
term t in document d and idf(t, D) is the log
of the proportion of the number of documents
in D containing the term t.

4: The proportion of [the number of articles in
which the word is linked] to [the number of
articles in which the word is mentioned but not
linked], a ﬂoat value. This gave us a measure of
how often a word was linked if it was mentioned.

B. Data set and data collection

2

The words that constitute links to other pages
diﬀer signiﬁcantly from page to page and are highly
dependent on the context of the page. Entities in an
article that are more closely related to the topic of
the article are more likely to constitute links then
entities that are mentioned in passing. For example,
in the Wikipedia article on Paul Ryan, the entity
"marathon" is mentioned but does not link to its
corresponding wikipedia page. However in the page
for "Running", "marathon" is mentioned and links to
its corresponding Wikipedia page. Because of this,
attempting to predict hypertext across a set of ar-
ticles covering diverse topics can be tricky. With such
diverse data, it can be hard to determine meaningful
conclusions on which terms should and shouldn’t
constitute links.

We hypothesize that accurately predicting hyper-
text must leverage an implicit understanding of the
article’s context and topical focus. We tested this by
running our model on two diﬀerent data sets.

1: Intuitively, articles from the same Wikipedia
category should have similar distributions of
hypertext. Limiting the articles in the training
and testing set to one category thus allows
the model to implicitly account for the broader
topic/context of the articles. In this project, we
selected random samples of 100 training articles
and 30 test articles from the Wikipedia "Forms
of Government" category. We called this our
government dataset.

2: In order to test our model

in a case where
the articles
in the dataset do not origi-
nate from the same context, we ran our
model on a 600 training articles and 200
test articles uniformly drawn from a mixture
of 4 Wikipedia categories ("Judaism","Ancient
Greece","Environmental science","Technology").
We called this our multicategory dataset.

We scraped and parsed each of these articles using
BeautifulSoup. For each article we extracted a list of
unigrams/bigrams as well as a list of terms that are
linked on.

We evaluated a variety of possible features to
model. We treated each unique word in a given article
as a separate training example.

Creating the feature vectors for the training set
required signiﬁcant pre-processing of our data. We
utilized the NLTK and scikit libraries to perform part
of speech classiﬁcation and tf-idf scoring, respectively.
We applied NLTK part of speech tagging to the
tokenized plaintext of each article, and ﬁltered the

words tagged as proper nouns. We then used scikit
tf-idf term weighting to vectorize and score each word
in the context of the article in which it appeared. We
wrote custom scripts to determine the other features.

IV. Methods

In our algorithm, we experimented with the follow-

ing models:

A. Dummy Model

In order to establish a baseline for our algorithm’s
performance we used a dummy stratiﬁed classiﬁer
from the sklearn library [6]. This classiﬁer establishes
a baseline performance by randomly predicting a
class for each training example. Because our data
set contains a fairly signiﬁcant class imbalance (the
number of terms that aren’t links far exceeds the
number that are), the probability of picking each class
was weighted by the class distribution in the training
set.

B. Gaussian Naive Bayes

Naive Bayes models are useful for quick and initial
evaluations of a dataset to explore the feasibility
of building some classiﬁer. Though they make the
"naive" assumption of conditional independence be-
tween features, they’ve proved to be surprisingly reli-
able. For a yi with with an associated set of features
x0, ...xn:

nY

p(y)

P(xi|y)
P(x1, ..., xn)

i=1

P(yi|x1, ..., xn) =

(1)

We ﬁrst implemented a Gaussian Naive Bayes model
where the feature likelihood is assumed to follow a
Gaussian distribution.

C. Logistic Regression

We used a logistic regression model with l2 regu-
larization [7]. In this type of classiﬁer the hypothesis
function takes the form of a logistic curve:

hθ(x) =

1

1+exp(−θx)

(2)

where hθ(x) can be viewed as the probability y = 1
given an x. We used an l2 penalty with logistic
regression.

D. Support Vector Machine

3

Support Vector Machines attempt to classify data
by constructing a multidimensional hyperplane to
separate data points of diﬀering classes (in eﬀect
identifying a decision boundary). The ideal decision
boundary is given by a hyperplane which is at the
maximum distance from the closest points of each
class (referred to as support vectors). In the case
where the data is not linearly separable, SVMs utilize
Kernel functions to project the data into a higher
dimension where a linear separator can be found.
Speciﬁcally, SVMs operate by solving the optimiza-
tion problem:

mX

i,j=1

mX

αi − 1
maxαW(α) =
2
s.t. αi ≥ 0, i = 1, . . . , m

i=1

mX

αiy(i) = 0

y(i)y(j)αiαjhx(i), x(j)i(3)

(4)

(5)

i=1

V. Results and Discussion

A. Data

The government category dataset yielded 148,303 train-
ing feature vectors and 43,310 test feature vectors. For
both training and testing, around 36-37% of the vectors
corresponded to a linked word. The multicategory data
yielded 625,826 training feature vectors and 214,912 test
feature vectors. For both training and testing, around 31-
32% of vectors corresponded to a linked word.

Fig. 2. Hypertext in original article (top) vs. predicted hyper-
text from model (bottom).

Our confusion matrix after running logistic regression

on the government category dataset was:

Predict NO Predict YES

True NO
True YES

0.9287
0.4433

0.07135
0.5567

Although the model very accurately predicted true non-
linked words, over 44% of predictions were false negatives,
or words predicted as non-links that should actually have
been links. A large majority of these false negatives were
common words, such as "music", that were linked in the
article as part of a multi-word phrase, such as "Vietnamese
music". We hypothesized that updating our model to

consider bigrams, and eventually general n-grams, should
eliminate much of these false negatives.

Our confusion matrix after running logistic regression

on the multicategory dataset was:

True NO
True YES

Predict NO Predict YES
0.06748913
0.93251087
0.53432279
0.46567721

Even though we expanded the size of our dataset and
diversiﬁed the categories we considered, the proportions
of true positives and true negatives were very similar
between the two datasets. Again, we hypothesized that
considering single word tokens instead of n-grams caused
much of this similarity. The imbalance in our linked and
non-linked classes may also have contributed to the high
false negative rate.

B. Metrics

Given the signiﬁcant class imbalance in our data set,
we evaluated our algorithms’ performance by measuring
their accuracy, precision, recall, F1, and AUC score. Pre-
cision is calculated as the number of correctly identiﬁed
hypertext tokens divided by the total number of hypertext
tokens proposed by the system; recall is deﬁned as the
number of correctly identiﬁed hypertext tokens divided
by the total number of hypertext tokens in the original
document; and F1 score is the harmonic mean of the
precision and recall. AUC is deﬁned as the area under
the ROC curve, which is created by plotting the recall
against the false positive rate (the number of incorrectly
identiﬁed hypertext tokens divided by the total number
of non-hypertext tokens) at various threshold settings.

4

Within a single category,
logistic regression and SVM
consistently outperformed Naive Bayes across all metrics.
Naive Bayes likely performs poorly because of the inde-
pendence assumptions it makes on the features. Whether
a word is a proper noun is likely strongly correlated
with the proportion of times it is linked when men-
tioned, particularly within a set of related articles. For
instance, the proper noun "Vietnam" was linked in most
of the articles in which it appears. Given our feature
set, logistic regression and SVM seem to have reached an
upper threshold in terms of performance. Using articles
across several categories, accuracy and AUC values for all
models were roughly the same. However, SVM performed
considerably worse, while logistic regression performed
considerably better across most metrics. SVM most likely
suﬀered in the multi-category case since the classes be-
came less separable. Consider important technical words
that are mentioned few times in one article but many
times in articles from other categories, resulting in high
word length and low tf-idf score. Now consider short
names of important historical ﬁgures, which have low
word length but high tf-idf score. Since both types of
terms are good candidates for hypertext, this illustrates
how the classes are diﬃcult to separate with a (n-1)-
dimensional hyperplane. Since these instances are more
likely to occur in the multi-category case, SVM has a
much lower F1 score and recall. Thus, logistic regression
clearly performs best when the data is not restricted to
semantically related articles.

Fig. 3. A comparison of the performance of various models for
the mixed categories data set

C. Comparative model performance

The results of the various models along each of the
metrics mentioned above for the government data set and
the multi-category data set can be seen in Figure 4 and
Figure 3 respectively. As expected, the dummy model
performed very poorly and was surpassed by every other
model.

Fig. 4. A comparison of the performance of various models for
the government data set

D. Feature Analysis

To analyze which of our features were most relevant
to our predictions, we performed a simple Leave-One-
Out feature analysis on each of our main models. For
each feature, we retrained and retested the model with
that feature excluded, and compared the resulting metrics
across features.

For the government dataset, word length was actually
our most relevant feature, with tf-idf values and hypertext

1

Predicting and Identifying Hypertext in

Wikipedia Articles

Neel Guha, Annie Hu, and Cindy Wang
{nguha, anniehu, ciwang}@stanford.edu

I. Introduction

II. Related Work

Wikipedia now has millions of articles online and
tens of millions of views each hour. In order to
maintain a high standard of organization across this
sheer volume of content, it is essential to ensure accu-
rate and helpful intra-article linking. The Wikipedia
Manual of Style states that links represent "major
connections with the subject of another article that
will help readers to understand the current article
more fully."1

In this project, we optimize article linking towards
this goal by applying machine learning to predict
hypertext in Wikipedia articles (see Figure 1 for
an example of hypertext). As inputs, our algorithm
processes tokens (words and n-grams) from articles.
We then use Naive Bayes, logistic regression, and a
SVM to output a predicted classiﬁcation: plain text
(no link) or hypertext (link). In this paper we describe
the process for our data collection, feature selection,
and the relative performance of various algorithms.
Note that we have limited the scope of this project
to predicting solely hypertext and not hyperlinks.
While there has been considerable work on predicting
hyperlinks (especially using context resolution based
techniques), we felt there had been signiﬁcantly less
work on predicting hypertext.

Our model enables large scale automated editing of
Wikipedia linking in a low cost but eﬀective way and
can also be used to make link suggestions to individual
contributors. Outside of Wikipedia, our system can
be generalized to any broad database of documents
or texts. For example, it could be applied to predict
hypertext in news corpora or academic literature.

There is considerable prior work involving using
existing Wikipedia links for entity disambiguation
[1]. Rather than general applications, the goal of our
paper is to identify semantically important hypertext
within the articles themselves.

Prior approaches to the problem of hypertext detec-
tion rely heavily on keyword identiﬁcation algorithms.
Generally, these approaches score potential keywords
by evaluating 1), word occurrence statistics or 2),
linking patterns on similar pages. The traditional
statistical approach, which depends solely on word
occurence counts, involves methods such as tf-idf, χ2
independence test, and "keyphraseness" (proportion
of occurrences that are hypertext) across the training
set [2]. These methods have moderate success, with
upper precision/recall/F1 scores in the 0.5-0.6 range.
The second approach involves semantic analysis to
calculate metrics such as contextual commonness and
relatedness. [3] and [4] outline speciﬁc formulas to
evaluate these measures. [5] introduces a novel al-
gorithm (LTRank) to identify similar pages, then
identify candidate links from those similar pages that
might be missing on the given page. The state-of-the-
art algorithm for link detection has a precision and
recall of approximately 0.75 [4].

We aim to develop a better model by incorporating
not only statistical and contextual features, but con-
sidering the absolute importance of each word itself;
i.e. proper nouns are more likely to be relevant links.
[6] suggests a computational linguistics approach to
keyword extraction involving part of speech (POS)
tagging in additional to statistical features. We ex-
plain our feature selection in more detail below.

Fig. 1.
cian’ are hypertext.

In this sentence, the terms ’American’ and ’academi-

1http://en.wikipedia.org/wiki/Wikipedia:Manual of Style

III. Data Set and Features

For clarity, we deﬁne "hypertext" as a word in
an article that appears, in the article content, as a
clickable link to another article. Our algorithm works
as follows:

1) Model Training: For each article ai in a training
set of articles A, we we extract each unique token
(word) ti from ai and represent ai as the set of unique

tokens it contains. For each token we then construct a
feature vector xi based on the attributes of ti relative
to ai (its parent article) and A (all the of articles). If
in ai, ti is a word with a link to another Wikipedia
article, we label ti’s output vector yi as hypertext
(y=1). Otherwise we label it as plaintext (y = 0). The
set of all xi, yi’s across all ai in A forms our training
set.

2) Model Testing: When testing this model on an
article aT , we follow a similar initial procedure. We
extract each unique token from aT and construct its
corresponding feature vector. We then use the model
we trained above to classify the set of feature vectors
corresponding to the tokens in aT . If a feature vector
xi is classiﬁed as hypertext, then its corresponding
token is predicted to be linked on in the article aT .
Because Wikipedia only links on the ﬁrst occur-
rence of a word (as opposed to every occurrence of
the word), note that we represent each article by the
list of unique tokens it contains.

A. Features

To determine whether a word should be linked, we
thus considered features pertaining to the word itself,
as well as the word in the context of the article and
data set as a whole. The four features included in our
model are:

1: Whether or not a word is proper noun, 0 or 1.
Proper nouns typically denote signiﬁcant peo-
ple, places, or things, which usually have their
own Wikipedia pages. If these words appear in
an article, they should be linked.

2: Length of the word, an integer value. Longer
words are more likely to be less common, names,
or technical terms. In any of these cases, the
word should be linked.

3: The tf-idf score of the word (a ﬂoat value
between 0 and 1) relative to the article in which
the word appears in. Tf-idf, or term frequency-
inverse document frequency,
is a numerical
statistic that denotes how important a word is
to a document. Speciﬁcally, for a term t in a
document d ∈ D, tﬁdf(t, d) = tf(t, d)× idf(t, D)
where tf(t, d) is the normalized frequency of
term t in document d and idf(t, D) is the log
of the proportion of the number of documents
in D containing the term t.

4: The proportion of [the number of articles in
which the word is linked] to [the number of
articles in which the word is mentioned but not
linked], a ﬂoat value. This gave us a measure of
how often a word was linked if it was mentioned.

B. Data set and data collection

2

The words that constitute links to other pages
diﬀer signiﬁcantly from page to page and are highly
dependent on the context of the page. Entities in an
article that are more closely related to the topic of
the article are more likely to constitute links then
entities that are mentioned in passing. For example,
in the Wikipedia article on Paul Ryan, the entity
"marathon" is mentioned but does not link to its
corresponding wikipedia page. However in the page
for "Running", "marathon" is mentioned and links to
its corresponding Wikipedia page. Because of this,
attempting to predict hypertext across a set of ar-
ticles covering diverse topics can be tricky. With such
diverse data, it can be hard to determine meaningful
conclusions on which terms should and shouldn’t
constitute links.

We hypothesize that accurately predicting hyper-
text must leverage an implicit understanding of the
article’s context and topical focus. We tested this by
running our model on two diﬀerent data sets.

1: Intuitively, articles from the same Wikipedia
category should have similar distributions of
hypertext. Limiting the articles in the training
and testing set to one category thus allows
the model to implicitly account for the broader
topic/context of the articles. In this project, we
selected random samples of 100 training articles
and 30 test articles from the Wikipedia "Forms
of Government" category. We called this our
government dataset.

2: In order to test our model

in a case where
the articles
in the dataset do not origi-
nate from the same context, we ran our
model on a 600 training articles and 200
test articles uniformly drawn from a mixture
of 4 Wikipedia categories ("Judaism","Ancient
Greece","Environmental science","Technology").
We called this our multicategory dataset.

We scraped and parsed each of these articles using
BeautifulSoup. For each article we extracted a list of
unigrams/bigrams as well as a list of terms that are
linked on.

We evaluated a variety of possible features to
model. We treated each unique word in a given article
as a separate training example.

Creating the feature vectors for the training set
required signiﬁcant pre-processing of our data. We
utilized the NLTK and scikit libraries to perform part
of speech classiﬁcation and tf-idf scoring, respectively.
We applied NLTK part of speech tagging to the
tokenized plaintext of each article, and ﬁltered the

words tagged as proper nouns. We then used scikit
tf-idf term weighting to vectorize and score each word
in the context of the article in which it appeared. We
wrote custom scripts to determine the other features.

IV. Methods

In our algorithm, we experimented with the follow-

ing models:

A. Dummy Model

In order to establish a baseline for our algorithm’s
performance we used a dummy stratiﬁed classiﬁer
from the sklearn library [6]. This classiﬁer establishes
a baseline performance by randomly predicting a
class for each training example. Because our data
set contains a fairly signiﬁcant class imbalance (the
number of terms that aren’t links far exceeds the
number that are), the probability of picking each class
was weighted by the class distribution in the training
set.

B. Gaussian Naive Bayes

Naive Bayes models are useful for quick and initial
evaluations of a dataset to explore the feasibility
of building some classiﬁer. Though they make the
"naive" assumption of conditional independence be-
tween features, they’ve proved to be surprisingly reli-
able. For a yi with with an associated set of features
x0, ...xn:

nY

p(y)

P(xi|y)
P(x1, ..., xn)

i=1

P(yi|x1, ..., xn) =

(1)

We ﬁrst implemented a Gaussian Naive Bayes model
where the feature likelihood is assumed to follow a
Gaussian distribution.

C. Logistic Regression

We used a logistic regression model with l2 regu-
larization [7]. In this type of classiﬁer the hypothesis
function takes the form of a logistic curve:

hθ(x) =

1

1+exp(−θx)

(2)

where hθ(x) can be viewed as the probability y = 1
given an x. We used an l2 penalty with logistic
regression.

D. Support Vector Machine

3

Support Vector Machines attempt to classify data
by constructing a multidimensional hyperplane to
separate data points of diﬀering classes (in eﬀect
identifying a decision boundary). The ideal decision
boundary is given by a hyperplane which is at the
maximum distance from the closest points of each
class (referred to as support vectors). In the case
where the data is not linearly separable, SVMs utilize
Kernel functions to project the data into a higher
dimension where a linear separator can be found.
Speciﬁcally, SVMs operate by solving the optimiza-
tion problem:

mX

i,j=1

mX

αi − 1
maxαW(α) =
2
s.t. αi ≥ 0, i = 1, . . . , m

i=1

mX

αiy(i) = 0

y(i)y(j)αiαjhx(i), x(j)i(3)

(4)

(5)

i=1

V. Results and Discussion

A. Data

The government category dataset yielded 148,303 train-
ing feature vectors and 43,310 test feature vectors. For
both training and testing, around 36-37% of the vectors
corresponded to a linked word. The multicategory data
yielded 625,826 training feature vectors and 214,912 test
feature vectors. For both training and testing, around 31-
32% of vectors corresponded to a linked word.

Fig. 2. Hypertext in original article (top) vs. predicted hyper-
text from model (bottom).

Our confusion matrix after running logistic regression

on the government category dataset was:

Predict NO Predict YES

True NO
True YES

0.9287
0.4433

0.07135
0.5567

Although the model very accurately predicted true non-
linked words, over 44% of predictions were false negatives,
or words predicted as non-links that should actually have
been links. A large majority of these false negatives were
common words, such as "music", that were linked in the
article as part of a multi-word phrase, such as "Vietnamese
music". We hypothesized that updating our model to

consider bigrams, and eventually general n-grams, should
eliminate much of these false negatives.

Our confusion matrix after running logistic regression

on the multicategory dataset was:

True NO
True YES

Predict NO Predict YES
0.06748913
0.93251087
0.53432279
0.46567721

Even though we expanded the size of our dataset and
diversiﬁed the categories we considered, the proportions
of true positives and true negatives were very similar
between the two datasets. Again, we hypothesized that
considering single word tokens instead of n-grams caused
much of this similarity. The imbalance in our linked and
non-linked classes may also have contributed to the high
false negative rate.

B. Metrics

Given the signiﬁcant class imbalance in our data set,
we evaluated our algorithms’ performance by measuring
their accuracy, precision, recall, F1, and AUC score. Pre-
cision is calculated as the number of correctly identiﬁed
hypertext tokens divided by the total number of hypertext
tokens proposed by the system; recall is deﬁned as the
number of correctly identiﬁed hypertext tokens divided
by the total number of hypertext tokens in the original
document; and F1 score is the harmonic mean of the
precision and recall. AUC is deﬁned as the area under
the ROC curve, which is created by plotting the recall
against the false positive rate (the number of incorrectly
identiﬁed hypertext tokens divided by the total number
of non-hypertext tokens) at various threshold settings.

4

Within a single category,
logistic regression and SVM
consistently outperformed Naive Bayes across all metrics.
Naive Bayes likely performs poorly because of the inde-
pendence assumptions it makes on the features. Whether
a word is a proper noun is likely strongly correlated
with the proportion of times it is linked when men-
tioned, particularly within a set of related articles. For
instance, the proper noun "Vietnam" was linked in most
of the articles in which it appears. Given our feature
set, logistic regression and SVM seem to have reached an
upper threshold in terms of performance. Using articles
across several categories, accuracy and AUC values for all
models were roughly the same. However, SVM performed
considerably worse, while logistic regression performed
considerably better across most metrics. SVM most likely
suﬀered in the multi-category case since the classes be-
came less separable. Consider important technical words
that are mentioned few times in one article but many
times in articles from other categories, resulting in high
word length and low tf-idf score. Now consider short
names of important historical ﬁgures, which have low
word length but high tf-idf score. Since both types of
terms are good candidates for hypertext, this illustrates
how the classes are diﬃcult to separate with a (n-1)-
dimensional hyperplane. Since these instances are more
likely to occur in the multi-category case, SVM has a
much lower F1 score and recall. Thus, logistic regression
clearly performs best when the data is not restricted to
semantically related articles.

Fig. 3. A comparison of the performance of various models for
the mixed categories data set

C. Comparative model performance

The results of the various models along each of the
metrics mentioned above for the government data set and
the multi-category data set can be seen in Figure 4 and
Figure 3 respectively. As expected, the dummy model
performed very poorly and was surpassed by every other
model.

Fig. 4. A comparison of the performance of various models for
the government data set

D. Feature Analysis

To analyze which of our features were most relevant
to our predictions, we performed a simple Leave-One-
Out feature analysis on each of our main models. For
each feature, we retrained and retested the model with
that feature excluded, and compared the resulting metrics
across features.

For the government dataset, word length was actually
our most relevant feature, with tf-idf values and hypertext

5

E. Experimenting with Bigrams

For the government data set we also experimented with
running our algorithm on bigrams. For each article - in
addition to creating feature vectors for single word tokens
- we create feature vectors for bigrams (pairs of adjacent
words). Bigrams are frequently used in many natural
language applications and have been shown to be eﬀective
in a wide range of problems. Though all of our models
outperformed the dummy model when run on bigrams,
there was signiﬁcantly less variance between the models
(see 7). In fact, all of the models performed signiﬁcantly
poorer on the bigram dataset then on the single word
dataset.

Fig. 7. A comparison of the performance of various models for
the government data set using bigrams

However, our resulting confusion matrix from running
logistic regression did signiﬁcantly reduce our number of
false negatives, as predicted:

Fig. 5. LOO Feature Metrics for Government Dataset

proportions much less relevant. This seemed to suggest
that hypertext depended only loosely on article context,
directly contradicting our earlier hypothesis. Since we
limited input vectors for this dataset to one category of
articles, the articles may have been too similar, limiting
the usefulness of these two features in diﬀerentiating
context. The similarity of article context combined with
our relatively small sample size may have also caused
overﬁtting to our training set, and an inﬂated accuracy
of prediction. To further explore these two observations,
we then ran Leave-One-Out feature analysis on our mul-
ticategory dataset.

True NO
True YES

0.8579
0.0804

Predict NO Predict YES

0.1421
0.9196

Fig. 6. LOO Feature Metrics for Multicategory Dataset

Surprisingly, our results did not change much even
after considering multiple categories – in fact, word length
became even more relevant to our results. Though prior
approaches to predicting hypertext tended to focus on
statistical measures of word importance, such as tf-idf,
our results when mixing these measures with linguistic
features such as word length seem to suggest that linguis-
tic features correlate much more strongly to predictions.

VI. Future Work

In this project we described an algorithm for predicting
and identifying hypertext in Wikipedia articles. The lo-
gistic regression and SVM algorithms perform best in the
case of a single-category data set. However, across multi-
ple categories, logistic regression deﬁnitively outperforms
all other algorithms, reaching .77 accuracy and .81 AUC.
Possible next steps could include expanding the data
set and identifying more features to help decrease false-
negatives. We had computational diﬃculties (limited
RAM) in implementing n-grams in this project, and were
only able to achieve mediocre results with bigrams on a
small data set. With greater resources we would hopefully
be able to complete a more thorough implementation of
n-grams. This too could improve our results.

1

Predicting and Identifying Hypertext in

Wikipedia Articles

Neel Guha, Annie Hu, and Cindy Wang
{nguha, anniehu, ciwang}@stanford.edu

I. Introduction

II. Related Work

Wikipedia now has millions of articles online and
tens of millions of views each hour. In order to
maintain a high standard of organization across this
sheer volume of content, it is essential to ensure accu-
rate and helpful intra-article linking. The Wikipedia
Manual of Style states that links represent "major
connections with the subject of another article that
will help readers to understand the current article
more fully."1

In this project, we optimize article linking towards
this goal by applying machine learning to predict
hypertext in Wikipedia articles (see Figure 1 for
an example of hypertext). As inputs, our algorithm
processes tokens (words and n-grams) from articles.
We then use Naive Bayes, logistic regression, and a
SVM to output a predicted classiﬁcation: plain text
(no link) or hypertext (link). In this paper we describe
the process for our data collection, feature selection,
and the relative performance of various algorithms.
Note that we have limited the scope of this project
to predicting solely hypertext and not hyperlinks.
While there has been considerable work on predicting
hyperlinks (especially using context resolution based
techniques), we felt there had been signiﬁcantly less
work on predicting hypertext.

Our model enables large scale automated editing of
Wikipedia linking in a low cost but eﬀective way and
can also be used to make link suggestions to individual
contributors. Outside of Wikipedia, our system can
be generalized to any broad database of documents
or texts. For example, it could be applied to predict
hypertext in news corpora or academic literature.

There is considerable prior work involving using
existing Wikipedia links for entity disambiguation
[1]. Rather than general applications, the goal of our
paper is to identify semantically important hypertext
within the articles themselves.

Prior approaches to the problem of hypertext detec-
tion rely heavily on keyword identiﬁcation algorithms.
Generally, these approaches score potential keywords
by evaluating 1), word occurrence statistics or 2),
linking patterns on similar pages. The traditional
statistical approach, which depends solely on word
occurence counts, involves methods such as tf-idf, χ2
independence test, and "keyphraseness" (proportion
of occurrences that are hypertext) across the training
set [2]. These methods have moderate success, with
upper precision/recall/F1 scores in the 0.5-0.6 range.
The second approach involves semantic analysis to
calculate metrics such as contextual commonness and
relatedness. [3] and [4] outline speciﬁc formulas to
evaluate these measures. [5] introduces a novel al-
gorithm (LTRank) to identify similar pages, then
identify candidate links from those similar pages that
might be missing on the given page. The state-of-the-
art algorithm for link detection has a precision and
recall of approximately 0.75 [4].

We aim to develop a better model by incorporating
not only statistical and contextual features, but con-
sidering the absolute importance of each word itself;
i.e. proper nouns are more likely to be relevant links.
[6] suggests a computational linguistics approach to
keyword extraction involving part of speech (POS)
tagging in additional to statistical features. We ex-
plain our feature selection in more detail below.

Fig. 1.
cian’ are hypertext.

In this sentence, the terms ’American’ and ’academi-

1http://en.wikipedia.org/wiki/Wikipedia:Manual of Style

III. Data Set and Features

For clarity, we deﬁne "hypertext" as a word in
an article that appears, in the article content, as a
clickable link to another article. Our algorithm works
as follows:

1) Model Training: For each article ai in a training
set of articles A, we we extract each unique token
(word) ti from ai and represent ai as the set of unique

tokens it contains. For each token we then construct a
feature vector xi based on the attributes of ti relative
to ai (its parent article) and A (all the of articles). If
in ai, ti is a word with a link to another Wikipedia
article, we label ti’s output vector yi as hypertext
(y=1). Otherwise we label it as plaintext (y = 0). The
set of all xi, yi’s across all ai in A forms our training
set.

2) Model Testing: When testing this model on an
article aT , we follow a similar initial procedure. We
extract each unique token from aT and construct its
corresponding feature vector. We then use the model
we trained above to classify the set of feature vectors
corresponding to the tokens in aT . If a feature vector
xi is classiﬁed as hypertext, then its corresponding
token is predicted to be linked on in the article aT .
Because Wikipedia only links on the ﬁrst occur-
rence of a word (as opposed to every occurrence of
the word), note that we represent each article by the
list of unique tokens it contains.

A. Features

To determine whether a word should be linked, we
thus considered features pertaining to the word itself,
as well as the word in the context of the article and
data set as a whole. The four features included in our
model are:

1: Whether or not a word is proper noun, 0 or 1.
Proper nouns typically denote signiﬁcant peo-
ple, places, or things, which usually have their
own Wikipedia pages. If these words appear in
an article, they should be linked.

2: Length of the word, an integer value. Longer
words are more likely to be less common, names,
or technical terms. In any of these cases, the
word should be linked.

3: The tf-idf score of the word (a ﬂoat value
between 0 and 1) relative to the article in which
the word appears in. Tf-idf, or term frequency-
inverse document frequency,
is a numerical
statistic that denotes how important a word is
to a document. Speciﬁcally, for a term t in a
document d ∈ D, tﬁdf(t, d) = tf(t, d)× idf(t, D)
where tf(t, d) is the normalized frequency of
term t in document d and idf(t, D) is the log
of the proportion of the number of documents
in D containing the term t.

4: The proportion of [the number of articles in
which the word is linked] to [the number of
articles in which the word is mentioned but not
linked], a ﬂoat value. This gave us a measure of
how often a word was linked if it was mentioned.

B. Data set and data collection

2

The words that constitute links to other pages
diﬀer signiﬁcantly from page to page and are highly
dependent on the context of the page. Entities in an
article that are more closely related to the topic of
the article are more likely to constitute links then
entities that are mentioned in passing. For example,
in the Wikipedia article on Paul Ryan, the entity
"marathon" is mentioned but does not link to its
corresponding wikipedia page. However in the page
for "Running", "marathon" is mentioned and links to
its corresponding Wikipedia page. Because of this,
attempting to predict hypertext across a set of ar-
ticles covering diverse topics can be tricky. With such
diverse data, it can be hard to determine meaningful
conclusions on which terms should and shouldn’t
constitute links.

We hypothesize that accurately predicting hyper-
text must leverage an implicit understanding of the
article’s context and topical focus. We tested this by
running our model on two diﬀerent data sets.

1: Intuitively, articles from the same Wikipedia
category should have similar distributions of
hypertext. Limiting the articles in the training
and testing set to one category thus allows
the model to implicitly account for the broader
topic/context of the articles. In this project, we
selected random samples of 100 training articles
and 30 test articles from the Wikipedia "Forms
of Government" category. We called this our
government dataset.

2: In order to test our model

in a case where
the articles
in the dataset do not origi-
nate from the same context, we ran our
model on a 600 training articles and 200
test articles uniformly drawn from a mixture
of 4 Wikipedia categories ("Judaism","Ancient
Greece","Environmental science","Technology").
We called this our multicategory dataset.

We scraped and parsed each of these articles using
BeautifulSoup. For each article we extracted a list of
unigrams/bigrams as well as a list of terms that are
linked on.

We evaluated a variety of possible features to
model. We treated each unique word in a given article
as a separate training example.

Creating the feature vectors for the training set
required signiﬁcant pre-processing of our data. We
utilized the NLTK and scikit libraries to perform part
of speech classiﬁcation and tf-idf scoring, respectively.
We applied NLTK part of speech tagging to the
tokenized plaintext of each article, and ﬁltered the

words tagged as proper nouns. We then used scikit
tf-idf term weighting to vectorize and score each word
in the context of the article in which it appeared. We
wrote custom scripts to determine the other features.

IV. Methods

In our algorithm, we experimented with the follow-

ing models:

A. Dummy Model

In order to establish a baseline for our algorithm’s
performance we used a dummy stratiﬁed classiﬁer
from the sklearn library [6]. This classiﬁer establishes
a baseline performance by randomly predicting a
class for each training example. Because our data
set contains a fairly signiﬁcant class imbalance (the
number of terms that aren’t links far exceeds the
number that are), the probability of picking each class
was weighted by the class distribution in the training
set.

B. Gaussian Naive Bayes

Naive Bayes models are useful for quick and initial
evaluations of a dataset to explore the feasibility
of building some classiﬁer. Though they make the
"naive" assumption of conditional independence be-
tween features, they’ve proved to be surprisingly reli-
able. For a yi with with an associated set of features
x0, ...xn:

nY

p(y)

P(xi|y)
P(x1, ..., xn)

i=1

P(yi|x1, ..., xn) =

(1)

We ﬁrst implemented a Gaussian Naive Bayes model
where the feature likelihood is assumed to follow a
Gaussian distribution.

C. Logistic Regression

We used a logistic regression model with l2 regu-
larization [7]. In this type of classiﬁer the hypothesis
function takes the form of a logistic curve:

hθ(x) =

1

1+exp(−θx)

(2)

where hθ(x) can be viewed as the probability y = 1
given an x. We used an l2 penalty with logistic
regression.

D. Support Vector Machine

3

Support Vector Machines attempt to classify data
by constructing a multidimensional hyperplane to
separate data points of diﬀering classes (in eﬀect
identifying a decision boundary). The ideal decision
boundary is given by a hyperplane which is at the
maximum distance from the closest points of each
class (referred to as support vectors). In the case
where the data is not linearly separable, SVMs utilize
Kernel functions to project the data into a higher
dimension where a linear separator can be found.
Speciﬁcally, SVMs operate by solving the optimiza-
tion problem:

mX

i,j=1

mX

αi − 1
maxαW(α) =
2
s.t. αi ≥ 0, i = 1, . . . , m

i=1

mX

αiy(i) = 0

y(i)y(j)αiαjhx(i), x(j)i(3)

(4)

(5)

i=1

V. Results and Discussion

A. Data

The government category dataset yielded 148,303 train-
ing feature vectors and 43,310 test feature vectors. For
both training and testing, around 36-37% of the vectors
corresponded to a linked word. The multicategory data
yielded 625,826 training feature vectors and 214,912 test
feature vectors. For both training and testing, around 31-
32% of vectors corresponded to a linked word.

Fig. 2. Hypertext in original article (top) vs. predicted hyper-
text from model (bottom).

Our confusion matrix after running logistic regression

on the government category dataset was:

Predict NO Predict YES

True NO
True YES

0.9287
0.4433

0.07135
0.5567

Although the model very accurately predicted true non-
linked words, over 44% of predictions were false negatives,
or words predicted as non-links that should actually have
been links. A large majority of these false negatives were
common words, such as "music", that were linked in the
article as part of a multi-word phrase, such as "Vietnamese
music". We hypothesized that updating our model to

consider bigrams, and eventually general n-grams, should
eliminate much of these false negatives.

Our confusion matrix after running logistic regression

on the multicategory dataset was:

True NO
True YES

Predict NO Predict YES
0.06748913
0.93251087
0.53432279
0.46567721

Even though we expanded the size of our dataset and
diversiﬁed the categories we considered, the proportions
of true positives and true negatives were very similar
between the two datasets. Again, we hypothesized that
considering single word tokens instead of n-grams caused
much of this similarity. The imbalance in our linked and
non-linked classes may also have contributed to the high
false negative rate.

B. Metrics

Given the signiﬁcant class imbalance in our data set,
we evaluated our algorithms’ performance by measuring
their accuracy, precision, recall, F1, and AUC score. Pre-
cision is calculated as the number of correctly identiﬁed
hypertext tokens divided by the total number of hypertext
tokens proposed by the system; recall is deﬁned as the
number of correctly identiﬁed hypertext tokens divided
by the total number of hypertext tokens in the original
document; and F1 score is the harmonic mean of the
precision and recall. AUC is deﬁned as the area under
the ROC curve, which is created by plotting the recall
against the false positive rate (the number of incorrectly
identiﬁed hypertext tokens divided by the total number
of non-hypertext tokens) at various threshold settings.

4

Within a single category,
logistic regression and SVM
consistently outperformed Naive Bayes across all metrics.
Naive Bayes likely performs poorly because of the inde-
pendence assumptions it makes on the features. Whether
a word is a proper noun is likely strongly correlated
with the proportion of times it is linked when men-
tioned, particularly within a set of related articles. For
instance, the proper noun "Vietnam" was linked in most
of the articles in which it appears. Given our feature
set, logistic regression and SVM seem to have reached an
upper threshold in terms of performance. Using articles
across several categories, accuracy and AUC values for all
models were roughly the same. However, SVM performed
considerably worse, while logistic regression performed
considerably better across most metrics. SVM most likely
suﬀered in the multi-category case since the classes be-
came less separable. Consider important technical words
that are mentioned few times in one article but many
times in articles from other categories, resulting in high
word length and low tf-idf score. Now consider short
names of important historical ﬁgures, which have low
word length but high tf-idf score. Since both types of
terms are good candidates for hypertext, this illustrates
how the classes are diﬃcult to separate with a (n-1)-
dimensional hyperplane. Since these instances are more
likely to occur in the multi-category case, SVM has a
much lower F1 score and recall. Thus, logistic regression
clearly performs best when the data is not restricted to
semantically related articles.

Fig. 3. A comparison of the performance of various models for
the mixed categories data set

C. Comparative model performance

The results of the various models along each of the
metrics mentioned above for the government data set and
the multi-category data set can be seen in Figure 4 and
Figure 3 respectively. As expected, the dummy model
performed very poorly and was surpassed by every other
model.

Fig. 4. A comparison of the performance of various models for
the government data set

D. Feature Analysis

To analyze which of our features were most relevant
to our predictions, we performed a simple Leave-One-
Out feature analysis on each of our main models. For
each feature, we retrained and retested the model with
that feature excluded, and compared the resulting metrics
across features.

For the government dataset, word length was actually
our most relevant feature, with tf-idf values and hypertext

5

E. Experimenting with Bigrams

For the government data set we also experimented with
running our algorithm on bigrams. For each article - in
addition to creating feature vectors for single word tokens
- we create feature vectors for bigrams (pairs of adjacent
words). Bigrams are frequently used in many natural
language applications and have been shown to be eﬀective
in a wide range of problems. Though all of our models
outperformed the dummy model when run on bigrams,
there was signiﬁcantly less variance between the models
(see 7). In fact, all of the models performed signiﬁcantly
poorer on the bigram dataset then on the single word
dataset.

Fig. 7. A comparison of the performance of various models for
the government data set using bigrams

However, our resulting confusion matrix from running
logistic regression did signiﬁcantly reduce our number of
false negatives, as predicted:

Fig. 5. LOO Feature Metrics for Government Dataset

proportions much less relevant. This seemed to suggest
that hypertext depended only loosely on article context,
directly contradicting our earlier hypothesis. Since we
limited input vectors for this dataset to one category of
articles, the articles may have been too similar, limiting
the usefulness of these two features in diﬀerentiating
context. The similarity of article context combined with
our relatively small sample size may have also caused
overﬁtting to our training set, and an inﬂated accuracy
of prediction. To further explore these two observations,
we then ran Leave-One-Out feature analysis on our mul-
ticategory dataset.

True NO
True YES

0.8579
0.0804

Predict NO Predict YES

0.1421
0.9196

Fig. 6. LOO Feature Metrics for Multicategory Dataset

Surprisingly, our results did not change much even
after considering multiple categories – in fact, word length
became even more relevant to our results. Though prior
approaches to predicting hypertext tended to focus on
statistical measures of word importance, such as tf-idf,
our results when mixing these measures with linguistic
features such as word length seem to suggest that linguis-
tic features correlate much more strongly to predictions.

VI. Future Work

In this project we described an algorithm for predicting
and identifying hypertext in Wikipedia articles. The lo-
gistic regression and SVM algorithms perform best in the
case of a single-category data set. However, across multi-
ple categories, logistic regression deﬁnitively outperforms
all other algorithms, reaching .77 accuracy and .81 AUC.
Possible next steps could include expanding the data
set and identifying more features to help decrease false-
negatives. We had computational diﬃculties (limited
RAM) in implementing n-grams in this project, and were
only able to achieve mediocre results with bigrams on a
small data set. With greater resources we would hopefully
be able to complete a more thorough implementation of
n-grams. This too could improve our results.

VII. References

6

[1] Ratinov, Lev, et al.

"Local and global algo-
rithms for disambiguation to wikipedia." Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1. Association for Computa-
tional Linguistics, 2011.

[2] Mihalcea, Rada, and Andras Csomai. "Wikify!: link-
ing documents to encyclopedic knowledge." Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management.
ACM, 2007.

[3] Gabrilovich, Evgeniy, and Shaul Markovitch. "Com-
puting Semantic Relatedness Using Wikipedia-
based Explicit Semantic Analysis." IJCAI. Vol. 7.
2007.

[4] Milne, David, and Ian H. Witten. "Learning to link
with wikipedia." Proceedings of the 17th ACM con-
ference on Information and knowledge management.
ACM, 2008.

[5] Adafre, Sisay Fissaha, and Maarten de Rijke. "Dis-
covering missing links in Wikipedia." Proceedings of
the 3rd international workshop on Link discovery.
ACM, 2005.

[6] Hulth, Anette. "Improved automatic keyword ex-
traction given more linguistic knowledge." Proceed-
ings of the 2003 conference on Empirical methods in
natural language processing. Association for Com-
putational Linguistics, 2003.

VIII. Tools

[1] NTLK Library. Bird, Steven, Edward Loper and
Ewan Klein. Natural Language Processing with
Python. O’Reilly Media Inc. 2009.

[2] scikit-learn. Pedregosa et al. Scikit-learn: Machine
Learning in Python. JMLR 12, pp. 2825-2830, 2011.

