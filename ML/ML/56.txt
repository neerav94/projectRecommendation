Preprint typeset using LATEX style emulateapj v. 12/16/11

AUTOMATED IMAGE ARTIFACT IDENTIFICATION IN DARK ENERGY SURVEY CCD EXPOSURES

Kavli Institute for Particle Astrophysics and Cosmology & Physics Department, Stanford University, Stanford, CA 94305, USA

Joseph W. DeRose and Warren R. Morningstar

ABSTRACT

In this work, we present an implementation of a machine learning algorithm to identify and classify
image artifacts in observations taken by the Dark Energy Survey (DES). Speciﬁcally, we downsample
background subtracted CCD exposures and treat the downsampled pixel values as features in both a
Support Vector Machine (SVM) and a Convolutional Neural Network (CNN) to classify full exposures.
In general the CNN outperforms the SVM. In the two-class problem and the 29-class problem, our SVM
implementation does not perform better than random guessing on our test set for binary classiﬁcation,
and predicts no artifact for nearly everything in the 29-class problem. CNNs give a best test accuracy
of 67% on the binary classiﬁcation problem and 52% on the 29-class problem. We discuss sources of
systematic error in our models and plans to account for these in future work.

1. INTRODUCTION

In modern cosmology, much attention has been turned
toward using large photometric sky surveys to provide
constraints on structure formation and the composition
of our universe. An important component of modern sky
surveys is the management and processing of the large
volumes of data produced by the survey instruments.
Current sky surveys, such as the Dark Energy Survey
whose data is used in this work, generate on the order
of several gigabytes of images and metadata per night.
Future sky surveys will produce TB of data in the same
time frame. Because of the vast quantities of data pro-
duced, the much of the data reduction and analysis is
automated.

While this provides signiﬁcant advantages in perfor-
mance, an obvious disadvantage is that it is prone to
overlooking subtle features in the data which may cause
systematic errors in measurements of important quanti-
ties.
In particular, since the goal of many photomet-
ric surveys is to provide precise measurements of the
brightness of astronomical objects, any spurious bright-
ness variations within a detector have the potential to be
interpreted as a real signal, and thus may interfere with
the accuracy of measurements.

Brightness variations in a detector can be caused in
many diﬀerent ways, and thus have a variety of appear-
ances. A few notable examples that have been observed
by ongoing sky surveys include airplanes and satellites
passing across the ﬁeld of view, cosmic rays striking the
detector, stray scattered light and improper subtraction
of background noise. Some of these are detected and
masked away by the survey data reduction pipelines, but
a large number are missed. These missed aberrations are
usually referred to as image artifacts. Reliable identiﬁ-
cation of artifacts is an essential component of preparing
data for analysis.

Currently, the identiﬁcation of artifacts is carried out
by members of survey collaborations, a process that uses
a signiﬁcant number of human hours and that could be
better spent devoted to more scientiﬁcally interesting
problems. Therefore an automated means of identifying
and classifying image artifacts is desirable.

In this paper, we discuss our implementations of both
Support Vector Machines and Convolutional Neural Net-

works trained to identify image artifacts found in obser-
vations performed by DES. In section 2 we review at-
tempts in the literature to perform similar tasks. We de-
scribe our data set and choice of features in Section 3. In
Section 4, we discuss the theoretical underpinnings and
implementations of our models. In Section 5, we present
the results of our SVM and CNN on both a binary and
29-class problem. In Section 6, we discuss these results,
including ways in which our methods could be improved.

2. RELATED WORK

Automated image classiﬁcation in astronomy is only
just becoming a problem worth solving.
In the past,
many collaborations focused on taking a small number
of very long exposures over a small area of the night
sky. In these cases, great care was taken to gather data
only in the best observational conditions, and these data
were not numerous enough to require automated check-
ing. A few surveys such as the Sloan Digital Sky Survey
(SDSS) (Lupton et al. 2005), the 2-Micron All Sky Sur-
vey (2MASS) (Skrutskie et al. 2006) and the Palomar-
Quest (PQ) Survey (Donalek et al. 2008) have imple-
mented automated means of dealing with image artifacts.
The ﬁrst step in artifact identiﬁcation in all of these sur-
veys are background subtraction and object identiﬁca-
tion pipelines. Additionally, the surveys all attempt to
classify the objects which are extracted in these ﬁrst steps
as being real astronomical objects or artifacts, as opposed
to our approach of classifying artifacts on the CCD pix-
els themselves so that corrections can be applied before
object catalogs are created. SDSS and 2MASS both ac-
complish their classiﬁcations by making cuts in measured
parameters of the objects such as color and morphology,
something akin to a manually trained linear regression
algorithm. SDSS does not provide quantiﬁcation of their
performance on artifact detection, while 2MASS com-
ments that they achieve their requirement of having less
than 0.05% artifact contamination in their source cata-
logs, which is much less stringent than future require-
ments will be. PQ employed a Multi-Layer Perceptron
for its classiﬁcation, also using object parameters such
as shape and size as input variables. They report a 90%
classiﬁcation accuracy, but we again emphasize that this
is classiﬁcation done after objects have already been ex-

Preprint typeset using LATEX style emulateapj v. 12/16/11

AUTOMATED IMAGE ARTIFACT IDENTIFICATION IN DARK ENERGY SURVEY CCD EXPOSURES

Kavli Institute for Particle Astrophysics and Cosmology & Physics Department, Stanford University, Stanford, CA 94305, USA

Joseph W. DeRose and Warren R. Morningstar

ABSTRACT

In this work, we present an implementation of a machine learning algorithm to identify and classify
image artifacts in observations taken by the Dark Energy Survey (DES). Speciﬁcally, we downsample
background subtracted CCD exposures and treat the downsampled pixel values as features in both a
Support Vector Machine (SVM) and a Convolutional Neural Network (CNN) to classify full exposures.
In general the CNN outperforms the SVM. In the two-class problem and the 29-class problem, our SVM
implementation does not perform better than random guessing on our test set for binary classiﬁcation,
and predicts no artifact for nearly everything in the 29-class problem. CNNs give a best test accuracy
of 67% on the binary classiﬁcation problem and 52% on the 29-class problem. We discuss sources of
systematic error in our models and plans to account for these in future work.

1. INTRODUCTION

In modern cosmology, much attention has been turned
toward using large photometric sky surveys to provide
constraints on structure formation and the composition
of our universe. An important component of modern sky
surveys is the management and processing of the large
volumes of data produced by the survey instruments.
Current sky surveys, such as the Dark Energy Survey
whose data is used in this work, generate on the order
of several gigabytes of images and metadata per night.
Future sky surveys will produce TB of data in the same
time frame. Because of the vast quantities of data pro-
duced, the much of the data reduction and analysis is
automated.

While this provides signiﬁcant advantages in perfor-
mance, an obvious disadvantage is that it is prone to
overlooking subtle features in the data which may cause
systematic errors in measurements of important quanti-
ties.
In particular, since the goal of many photomet-
ric surveys is to provide precise measurements of the
brightness of astronomical objects, any spurious bright-
ness variations within a detector have the potential to be
interpreted as a real signal, and thus may interfere with
the accuracy of measurements.

Brightness variations in a detector can be caused in
many diﬀerent ways, and thus have a variety of appear-
ances. A few notable examples that have been observed
by ongoing sky surveys include airplanes and satellites
passing across the ﬁeld of view, cosmic rays striking the
detector, stray scattered light and improper subtraction
of background noise. Some of these are detected and
masked away by the survey data reduction pipelines, but
a large number are missed. These missed aberrations are
usually referred to as image artifacts. Reliable identiﬁ-
cation of artifacts is an essential component of preparing
data for analysis.

Currently, the identiﬁcation of artifacts is carried out
by members of survey collaborations, a process that uses
a signiﬁcant number of human hours and that could be
better spent devoted to more scientiﬁcally interesting
problems. Therefore an automated means of identifying
and classifying image artifacts is desirable.

In this paper, we discuss our implementations of both
Support Vector Machines and Convolutional Neural Net-

works trained to identify image artifacts found in obser-
vations performed by DES. In section 2 we review at-
tempts in the literature to perform similar tasks. We de-
scribe our data set and choice of features in Section 3. In
Section 4, we discuss the theoretical underpinnings and
implementations of our models. In Section 5, we present
the results of our SVM and CNN on both a binary and
29-class problem. In Section 6, we discuss these results,
including ways in which our methods could be improved.

2. RELATED WORK

Automated image classiﬁcation in astronomy is only
just becoming a problem worth solving.
In the past,
many collaborations focused on taking a small number
of very long exposures over a small area of the night
sky. In these cases, great care was taken to gather data
only in the best observational conditions, and these data
were not numerous enough to require automated check-
ing. A few surveys such as the Sloan Digital Sky Survey
(SDSS) (Lupton et al. 2005), the 2-Micron All Sky Sur-
vey (2MASS) (Skrutskie et al. 2006) and the Palomar-
Quest (PQ) Survey (Donalek et al. 2008) have imple-
mented automated means of dealing with image artifacts.
The ﬁrst step in artifact identiﬁcation in all of these sur-
veys are background subtraction and object identiﬁca-
tion pipelines. Additionally, the surveys all attempt to
classify the objects which are extracted in these ﬁrst steps
as being real astronomical objects or artifacts, as opposed
to our approach of classifying artifacts on the CCD pix-
els themselves so that corrections can be applied before
object catalogs are created. SDSS and 2MASS both ac-
complish their classiﬁcations by making cuts in measured
parameters of the objects such as color and morphology,
something akin to a manually trained linear regression
algorithm. SDSS does not provide quantiﬁcation of their
performance on artifact detection, while 2MASS com-
ments that they achieve their requirement of having less
than 0.05% artifact contamination in their source cata-
logs, which is much less stringent than future require-
ments will be. PQ employed a Multi-Layer Perceptron
for its classiﬁcation, also using object parameters such
as shape and size as input variables. They report a 90%
classiﬁcation accuracy, but we again emphasize that this
is classiﬁcation done after objects have already been ex-

2

tracted, not on the actual CCD pixels.

While image classiﬁcation is just making its way into
astronomy, it has been the subject of much work in the
ﬁeld of machine learning for a number of years. Two
approaches that have gained notoriety are the use of
Support Vector Machines and especially Convolutional
Neural Networks. In Chapelle et al. (1999), the authors
process images into color histograms and use these his-
togram values as inputs to a Support Vector Machine
trying a number of kernels. Their work reports an er-
ror rate of 11.5% on the Corel training set of RGB im-
ages. While this is an interesting result, it relies on color
images in order to construct features, an option that is
not available to us. Convolutional Neural Networks have
been used for a number of years in image classiﬁcation
problems. An early attempt in this direction was Le-
Cun et al. (1995) reporting a classiﬁcation error rate of
1.6% on a test set of written digits. Another very in-
teresting machine learning result in recent years was the
ImageNet (Krizhevsky et al. 2012) implementation of a
CNN, achieving 37.5% accuracy on a 1000 class image
classiﬁcation problem. Both of these papers used much
larger training sets than we had available, with many
more examples per class, but they are good examples of
the successes of CNNs on image classiﬁcation problems.
Additionally, most image classiﬁcation in the ﬁeld of Ma-
chine Learning is done on relatively high signal-to-noise
images.
It has been reported that when adding small
perturbations to images classiﬁed with high conﬁdence
by CNNs it is possible to fool them entirely (Goodfellow
et al. 2014). This may be relevant to our problem, since
the varying noise properties of individual exposures can
be interpreted as adding such perturbations to artifacts
that may otherwise be classiﬁed correctly.

3. DATA

The Dark Energy Survey (DES) is a sky survey being
conducted across many institutions. DES is being carried
out using the 4m Blanco Telescope located at the Cerro
Tololo Inter-American Observatory in Chile. The survey
saw ﬁrst light in 2012, and is currently in its 3rd year
of data taking. The DES camera (DECam) is made up
of 62 Charge Coupled Devices (CCDs) each composed of
4098 × 2048 pixels. Each pixel measures the ﬂux on the
sky as a number of electrons freed from the valence band
of the CCD silicon per second. The number of exposures
that DES takes per night varies widely depending on the
time of year and observing conditions, but a typical night
yields approximately 200 new exposures.

Given that the survey has been active for several years
taking data at the rates given above, the total volume
of data is on the order of 10 TB. This makes training a
model prohibitively expensive without reducing the size
of the data, even if only using a subset of the total data.
As a means of making our methods computationally fea-
sible, we decided to downsample the image data. We did
this by replacing squares of pixels of a characteristic side
length with the mean of all the pixels within the square.
This type of coarse-graining will not hide any artifacts
as long as the side lengths of the squares are signiﬁcantly
smaller than the sizes of the smallest artifacts. We found
that binning the data into 8x8 pixel bins (reducing our
data volume by a factor of 64), suﬃciently reduced the
size of our training set, while keeping the pixels signiﬁ-

cantly smaller than the observed sizes of artifacts.

Each CCD exposure also has associated with it an es-
timation of the background light not grouped into any
of the identiﬁed objects, as well as a mask identifying
the regions of the CCD containing bright stars, cosmic
rays and other spurious sources identiﬁed by the current
DES image reduction pipeline. Before downsampling the
exposures we subtract the estimated background and ap-
ply the mask. As a ﬁnal pre-processing step we apply the
transformation

(1)

ν = 255

arcsinh I − m

M − m

where I is the ﬂux of the pixel, M = max(I) and
m = min(I). This transformation emphasizes low level
noise in the image, where many of the artifacts have their
strongest signature.

Currently DES identiﬁes unmasked image artifacts
manually via a web based application (Melchior et al.
2015). Users inspect single CCD exposures, and upon
ﬁnding an artifact label a single pixel of the exposure
with the artifact type. Thus, there is no information
about the extent of the artifact, making classiﬁcation on
a pixel by pixel basis unfeasible. Artifacts are grouped
into 28 classes, including some obvious defects such as
Cosmic Ray, Airplane, and Satellite, and some more sub-
tle and obscure such as Haze, Dark Halo, and Wavy Sky.
We add an additional label, ’No Artifact’ as a null result
for our classiﬁer.

We used the DES web utility to provide us with our
training examples. Speciﬁcally, our training data consists
of all of the exposures obtained in the ﬁrst year of DES,
which have at least one CCD that was manually classiﬁed
as having an artifact. The total size of this data set
is 60856 CCD exposures, obtained from approximately
1000 observations. The number of artifacts classiﬁed in
the data is approximately 8000, spread across roughly
6% of the CCD exposures. Figure 1 contains examples
of 26 of the 28 artifact classes.

We labeled our training data based on which artifact
class it contained. Many of our exposures contain mul-
tiple classes of artifacts. For these, we randomly set the
artifact label to one of the artifacts present. When we
are attempting a binary classiﬁcation this is not an is-
sue, but when we attempt to classify into the full 29-class
space this may be a source of error.

Finally, we created an augmented training set consist-
ing of 3000 CCD exposures, half of which contained arti-
facts. We did this as a means of reducing the run times
of our algorithms, as well as emphasizing artifacts in our
training sets. This will likely bias our estimations of the
probabilities of artifacts high, as this training set con-
tains a much higher proportion of CCDs with artifacts
than the actual DES dataset, but a bias in this direction
is desirable. It is the opposite bias, one against classi-
fying CCDs as containing artifacts, that is detrimental
since this means that spurious objects make it into cat-
alogs being used for science.

4. METHODS

4.1. Support Vector Machine

Our ﬁrst attempt at an image artifact classiﬁer was a
Support Vector Machine. This was not the ideal choice

Preprint typeset using LATEX style emulateapj v. 12/16/11

AUTOMATED IMAGE ARTIFACT IDENTIFICATION IN DARK ENERGY SURVEY CCD EXPOSURES

Kavli Institute for Particle Astrophysics and Cosmology & Physics Department, Stanford University, Stanford, CA 94305, USA

Joseph W. DeRose and Warren R. Morningstar

ABSTRACT

In this work, we present an implementation of a machine learning algorithm to identify and classify
image artifacts in observations taken by the Dark Energy Survey (DES). Speciﬁcally, we downsample
background subtracted CCD exposures and treat the downsampled pixel values as features in both a
Support Vector Machine (SVM) and a Convolutional Neural Network (CNN) to classify full exposures.
In general the CNN outperforms the SVM. In the two-class problem and the 29-class problem, our SVM
implementation does not perform better than random guessing on our test set for binary classiﬁcation,
and predicts no artifact for nearly everything in the 29-class problem. CNNs give a best test accuracy
of 67% on the binary classiﬁcation problem and 52% on the 29-class problem. We discuss sources of
systematic error in our models and plans to account for these in future work.

1. INTRODUCTION

In modern cosmology, much attention has been turned
toward using large photometric sky surveys to provide
constraints on structure formation and the composition
of our universe. An important component of modern sky
surveys is the management and processing of the large
volumes of data produced by the survey instruments.
Current sky surveys, such as the Dark Energy Survey
whose data is used in this work, generate on the order
of several gigabytes of images and metadata per night.
Future sky surveys will produce TB of data in the same
time frame. Because of the vast quantities of data pro-
duced, the much of the data reduction and analysis is
automated.

While this provides signiﬁcant advantages in perfor-
mance, an obvious disadvantage is that it is prone to
overlooking subtle features in the data which may cause
systematic errors in measurements of important quanti-
ties.
In particular, since the goal of many photomet-
ric surveys is to provide precise measurements of the
brightness of astronomical objects, any spurious bright-
ness variations within a detector have the potential to be
interpreted as a real signal, and thus may interfere with
the accuracy of measurements.

Brightness variations in a detector can be caused in
many diﬀerent ways, and thus have a variety of appear-
ances. A few notable examples that have been observed
by ongoing sky surveys include airplanes and satellites
passing across the ﬁeld of view, cosmic rays striking the
detector, stray scattered light and improper subtraction
of background noise. Some of these are detected and
masked away by the survey data reduction pipelines, but
a large number are missed. These missed aberrations are
usually referred to as image artifacts. Reliable identiﬁ-
cation of artifacts is an essential component of preparing
data for analysis.

Currently, the identiﬁcation of artifacts is carried out
by members of survey collaborations, a process that uses
a signiﬁcant number of human hours and that could be
better spent devoted to more scientiﬁcally interesting
problems. Therefore an automated means of identifying
and classifying image artifacts is desirable.

In this paper, we discuss our implementations of both
Support Vector Machines and Convolutional Neural Net-

works trained to identify image artifacts found in obser-
vations performed by DES. In section 2 we review at-
tempts in the literature to perform similar tasks. We de-
scribe our data set and choice of features in Section 3. In
Section 4, we discuss the theoretical underpinnings and
implementations of our models. In Section 5, we present
the results of our SVM and CNN on both a binary and
29-class problem. In Section 6, we discuss these results,
including ways in which our methods could be improved.

2. RELATED WORK

Automated image classiﬁcation in astronomy is only
just becoming a problem worth solving.
In the past,
many collaborations focused on taking a small number
of very long exposures over a small area of the night
sky. In these cases, great care was taken to gather data
only in the best observational conditions, and these data
were not numerous enough to require automated check-
ing. A few surveys such as the Sloan Digital Sky Survey
(SDSS) (Lupton et al. 2005), the 2-Micron All Sky Sur-
vey (2MASS) (Skrutskie et al. 2006) and the Palomar-
Quest (PQ) Survey (Donalek et al. 2008) have imple-
mented automated means of dealing with image artifacts.
The ﬁrst step in artifact identiﬁcation in all of these sur-
veys are background subtraction and object identiﬁca-
tion pipelines. Additionally, the surveys all attempt to
classify the objects which are extracted in these ﬁrst steps
as being real astronomical objects or artifacts, as opposed
to our approach of classifying artifacts on the CCD pix-
els themselves so that corrections can be applied before
object catalogs are created. SDSS and 2MASS both ac-
complish their classiﬁcations by making cuts in measured
parameters of the objects such as color and morphology,
something akin to a manually trained linear regression
algorithm. SDSS does not provide quantiﬁcation of their
performance on artifact detection, while 2MASS com-
ments that they achieve their requirement of having less
than 0.05% artifact contamination in their source cata-
logs, which is much less stringent than future require-
ments will be. PQ employed a Multi-Layer Perceptron
for its classiﬁcation, also using object parameters such
as shape and size as input variables. They report a 90%
classiﬁcation accuracy, but we again emphasize that this
is classiﬁcation done after objects have already been ex-

2

tracted, not on the actual CCD pixels.

While image classiﬁcation is just making its way into
astronomy, it has been the subject of much work in the
ﬁeld of machine learning for a number of years. Two
approaches that have gained notoriety are the use of
Support Vector Machines and especially Convolutional
Neural Networks. In Chapelle et al. (1999), the authors
process images into color histograms and use these his-
togram values as inputs to a Support Vector Machine
trying a number of kernels. Their work reports an er-
ror rate of 11.5% on the Corel training set of RGB im-
ages. While this is an interesting result, it relies on color
images in order to construct features, an option that is
not available to us. Convolutional Neural Networks have
been used for a number of years in image classiﬁcation
problems. An early attempt in this direction was Le-
Cun et al. (1995) reporting a classiﬁcation error rate of
1.6% on a test set of written digits. Another very in-
teresting machine learning result in recent years was the
ImageNet (Krizhevsky et al. 2012) implementation of a
CNN, achieving 37.5% accuracy on a 1000 class image
classiﬁcation problem. Both of these papers used much
larger training sets than we had available, with many
more examples per class, but they are good examples of
the successes of CNNs on image classiﬁcation problems.
Additionally, most image classiﬁcation in the ﬁeld of Ma-
chine Learning is done on relatively high signal-to-noise
images.
It has been reported that when adding small
perturbations to images classiﬁed with high conﬁdence
by CNNs it is possible to fool them entirely (Goodfellow
et al. 2014). This may be relevant to our problem, since
the varying noise properties of individual exposures can
be interpreted as adding such perturbations to artifacts
that may otherwise be classiﬁed correctly.

3. DATA

The Dark Energy Survey (DES) is a sky survey being
conducted across many institutions. DES is being carried
out using the 4m Blanco Telescope located at the Cerro
Tololo Inter-American Observatory in Chile. The survey
saw ﬁrst light in 2012, and is currently in its 3rd year
of data taking. The DES camera (DECam) is made up
of 62 Charge Coupled Devices (CCDs) each composed of
4098 × 2048 pixels. Each pixel measures the ﬂux on the
sky as a number of electrons freed from the valence band
of the CCD silicon per second. The number of exposures
that DES takes per night varies widely depending on the
time of year and observing conditions, but a typical night
yields approximately 200 new exposures.

Given that the survey has been active for several years
taking data at the rates given above, the total volume
of data is on the order of 10 TB. This makes training a
model prohibitively expensive without reducing the size
of the data, even if only using a subset of the total data.
As a means of making our methods computationally fea-
sible, we decided to downsample the image data. We did
this by replacing squares of pixels of a characteristic side
length with the mean of all the pixels within the square.
This type of coarse-graining will not hide any artifacts
as long as the side lengths of the squares are signiﬁcantly
smaller than the sizes of the smallest artifacts. We found
that binning the data into 8x8 pixel bins (reducing our
data volume by a factor of 64), suﬃciently reduced the
size of our training set, while keeping the pixels signiﬁ-

cantly smaller than the observed sizes of artifacts.

Each CCD exposure also has associated with it an es-
timation of the background light not grouped into any
of the identiﬁed objects, as well as a mask identifying
the regions of the CCD containing bright stars, cosmic
rays and other spurious sources identiﬁed by the current
DES image reduction pipeline. Before downsampling the
exposures we subtract the estimated background and ap-
ply the mask. As a ﬁnal pre-processing step we apply the
transformation

(1)

ν = 255

arcsinh I − m

M − m

where I is the ﬂux of the pixel, M = max(I) and
m = min(I). This transformation emphasizes low level
noise in the image, where many of the artifacts have their
strongest signature.

Currently DES identiﬁes unmasked image artifacts
manually via a web based application (Melchior et al.
2015). Users inspect single CCD exposures, and upon
ﬁnding an artifact label a single pixel of the exposure
with the artifact type. Thus, there is no information
about the extent of the artifact, making classiﬁcation on
a pixel by pixel basis unfeasible. Artifacts are grouped
into 28 classes, including some obvious defects such as
Cosmic Ray, Airplane, and Satellite, and some more sub-
tle and obscure such as Haze, Dark Halo, and Wavy Sky.
We add an additional label, ’No Artifact’ as a null result
for our classiﬁer.

We used the DES web utility to provide us with our
training examples. Speciﬁcally, our training data consists
of all of the exposures obtained in the ﬁrst year of DES,
which have at least one CCD that was manually classiﬁed
as having an artifact. The total size of this data set
is 60856 CCD exposures, obtained from approximately
1000 observations. The number of artifacts classiﬁed in
the data is approximately 8000, spread across roughly
6% of the CCD exposures. Figure 1 contains examples
of 26 of the 28 artifact classes.

We labeled our training data based on which artifact
class it contained. Many of our exposures contain mul-
tiple classes of artifacts. For these, we randomly set the
artifact label to one of the artifacts present. When we
are attempting a binary classiﬁcation this is not an is-
sue, but when we attempt to classify into the full 29-class
space this may be a source of error.

Finally, we created an augmented training set consist-
ing of 3000 CCD exposures, half of which contained arti-
facts. We did this as a means of reducing the run times
of our algorithms, as well as emphasizing artifacts in our
training sets. This will likely bias our estimations of the
probabilities of artifacts high, as this training set con-
tains a much higher proportion of CCDs with artifacts
than the actual DES dataset, but a bias in this direction
is desirable. It is the opposite bias, one against classi-
fying CCDs as containing artifacts, that is detrimental
since this means that spurious objects make it into cat-
alogs being used for science.

4. METHODS

4.1. Support Vector Machine

Our ﬁrst attempt at an image artifact classiﬁer was a
Support Vector Machine. This was not the ideal choice

3

4.2. Convolutional Neural Network

Convolutional Neural Networks (CNNs) are a class of
learning algorithms tailored toward recognition of ab-
stract image features. CNNs accomplish this feat by
creating a number of convolutional ﬁlters which data
is passed through before reaching one or more densely
connected layers which operate more like conventional
machine learning methods. Because of the convolutional
layers, these networks are better suited for use with im-
ages than SVMs, because they are able to overcome prob-
lems like translations or rotations of image features and
can construct their own set of robust features out of raw
pixel data. CNNs are relatively diﬃcult to tune due to
the large number of components that can be optimized,
but when used correctly can lead to high image classiﬁ-
cation accuracies.

We have implemented a CNN using the python
wrapped library TensorFlow (Abadi et al. 2015). We
have constructed our CNN with the following architec-

ture:• A primary convolutional layer, followed by a max

pooling step to reduce the dimensionality of the
output features and a ReLU activation layer.
• A secondary convolutional layer with a similar max
pooling step, followed by a dropout layer to reduce
overﬁtting and a ReLU activation layer.
• The image is ﬂattened, and then passed to a pri-
mary fully connected layer followed by an addi-
tional dropout layer and a ReLU activation layer.
• A secondary fully connected layer followed by a
• A ﬁnal fully connected layer which produces the
probabilities of each class via a softmax activation.
In the convolutional layers, we convolve our images with
a series of ﬁlters and add a bias constant to each con-
volved image. The result of the convolution is given as

dropout layer and a ReLU activation layer.

(cid:88)

yi(cid:48),j(cid:48),k(cid:48) =

wijkk(cid:48)xi+i(cid:48),j+j(cid:48),k + bi(cid:48),j(cid:48)

(6)

i,j,k

where wijkk(cid:48), bi(cid:48),j(cid:48) are the learned convolution weights
and biases respectively. We then apply a Rectiﬁed Linear
Unit (ReLU) activation function

ReLU (x) = max (0, x)

(7)

to determine the output of each layer.
In the densely
connected layers, we perform a matrix multiplication us-
ing the outputs of the previous nodes as well as weights
matrices and constant bias terms. We also use ReLUs
for activation after these layers. The dropout layers ex-
ist to randomly deactivate the connections between neu-
rons in order to prevent overﬁtting, since if neurons are
randomly deactivated they cannot learn complex inter-
dependencies. Finally, a softmax activation on the last
densely connected layer produces the probabilities of
each class:

exp(cid:0)W (j)T x + b(j)(cid:1)
j=1 exp(cid:0)W (j)T x + b(j)(cid:1) (8)
(cid:80)n

Using the probabilities of each class, we compute the
cross entropy, deﬁned as

W,b(x) = P (y = j|x) =
hj
n(cid:88)

S = − m(cid:88)

i=1

j=1

Ptrue,ij log Ppred,ij

(9)

Figure 1. Example artifacts.

for such a high dimensional feature space, as we show
in 5 but SVMs require little ﬁne tuning in order to op-
timize their performance and are thus ideal for setting
performance baselines.

SVMs classify objects by assuming the hypothesis

takes the form

hw,b(x) = g(wT x + b)

(2)

where w and b are the weights and biases of the classiﬁer
and x is a feature vector. The SVM is trained by min-
imizing the geometric margin of the training set given
by

m(cid:88)

(cid:107)w(cid:107)2 + C

1
2

minγ,w,b

ξi
s.t. yi(wT xi + b) >= 1 − ξi, i = 1, . . . , m
ξi ≥ 0, i = 1, . . . , m

i=1

(3)

(4)
(5)

where γ = yi( w(cid:107)w(cid:107) T xi + b(cid:107)b(cid:107) ) is the geometric margin,
ξi are parameters allowing the training set to deviate
from linear separability and C is a regularization pa-
rameter controlling the sizes of the ξis. In practice this
minimization is performed by maximizing the dual ob-
jective, which allows us to replace inner products with
arbitrary kernels, implicitly using a higher dimensional
feature space.
In our implementation, we tried using
both a linear kernel as well as a gaussian kernel, ﬁnd-
ing no signiﬁcant diﬀerences between the two in terms of
generalization error.

Preprint typeset using LATEX style emulateapj v. 12/16/11

AUTOMATED IMAGE ARTIFACT IDENTIFICATION IN DARK ENERGY SURVEY CCD EXPOSURES

Kavli Institute for Particle Astrophysics and Cosmology & Physics Department, Stanford University, Stanford, CA 94305, USA

Joseph W. DeRose and Warren R. Morningstar

ABSTRACT

In this work, we present an implementation of a machine learning algorithm to identify and classify
image artifacts in observations taken by the Dark Energy Survey (DES). Speciﬁcally, we downsample
background subtracted CCD exposures and treat the downsampled pixel values as features in both a
Support Vector Machine (SVM) and a Convolutional Neural Network (CNN) to classify full exposures.
In general the CNN outperforms the SVM. In the two-class problem and the 29-class problem, our SVM
implementation does not perform better than random guessing on our test set for binary classiﬁcation,
and predicts no artifact for nearly everything in the 29-class problem. CNNs give a best test accuracy
of 67% on the binary classiﬁcation problem and 52% on the 29-class problem. We discuss sources of
systematic error in our models and plans to account for these in future work.

1. INTRODUCTION

In modern cosmology, much attention has been turned
toward using large photometric sky surveys to provide
constraints on structure formation and the composition
of our universe. An important component of modern sky
surveys is the management and processing of the large
volumes of data produced by the survey instruments.
Current sky surveys, such as the Dark Energy Survey
whose data is used in this work, generate on the order
of several gigabytes of images and metadata per night.
Future sky surveys will produce TB of data in the same
time frame. Because of the vast quantities of data pro-
duced, the much of the data reduction and analysis is
automated.

While this provides signiﬁcant advantages in perfor-
mance, an obvious disadvantage is that it is prone to
overlooking subtle features in the data which may cause
systematic errors in measurements of important quanti-
ties.
In particular, since the goal of many photomet-
ric surveys is to provide precise measurements of the
brightness of astronomical objects, any spurious bright-
ness variations within a detector have the potential to be
interpreted as a real signal, and thus may interfere with
the accuracy of measurements.

Brightness variations in a detector can be caused in
many diﬀerent ways, and thus have a variety of appear-
ances. A few notable examples that have been observed
by ongoing sky surveys include airplanes and satellites
passing across the ﬁeld of view, cosmic rays striking the
detector, stray scattered light and improper subtraction
of background noise. Some of these are detected and
masked away by the survey data reduction pipelines, but
a large number are missed. These missed aberrations are
usually referred to as image artifacts. Reliable identiﬁ-
cation of artifacts is an essential component of preparing
data for analysis.

Currently, the identiﬁcation of artifacts is carried out
by members of survey collaborations, a process that uses
a signiﬁcant number of human hours and that could be
better spent devoted to more scientiﬁcally interesting
problems. Therefore an automated means of identifying
and classifying image artifacts is desirable.

In this paper, we discuss our implementations of both
Support Vector Machines and Convolutional Neural Net-

works trained to identify image artifacts found in obser-
vations performed by DES. In section 2 we review at-
tempts in the literature to perform similar tasks. We de-
scribe our data set and choice of features in Section 3. In
Section 4, we discuss the theoretical underpinnings and
implementations of our models. In Section 5, we present
the results of our SVM and CNN on both a binary and
29-class problem. In Section 6, we discuss these results,
including ways in which our methods could be improved.

2. RELATED WORK

Automated image classiﬁcation in astronomy is only
just becoming a problem worth solving.
In the past,
many collaborations focused on taking a small number
of very long exposures over a small area of the night
sky. In these cases, great care was taken to gather data
only in the best observational conditions, and these data
were not numerous enough to require automated check-
ing. A few surveys such as the Sloan Digital Sky Survey
(SDSS) (Lupton et al. 2005), the 2-Micron All Sky Sur-
vey (2MASS) (Skrutskie et al. 2006) and the Palomar-
Quest (PQ) Survey (Donalek et al. 2008) have imple-
mented automated means of dealing with image artifacts.
The ﬁrst step in artifact identiﬁcation in all of these sur-
veys are background subtraction and object identiﬁca-
tion pipelines. Additionally, the surveys all attempt to
classify the objects which are extracted in these ﬁrst steps
as being real astronomical objects or artifacts, as opposed
to our approach of classifying artifacts on the CCD pix-
els themselves so that corrections can be applied before
object catalogs are created. SDSS and 2MASS both ac-
complish their classiﬁcations by making cuts in measured
parameters of the objects such as color and morphology,
something akin to a manually trained linear regression
algorithm. SDSS does not provide quantiﬁcation of their
performance on artifact detection, while 2MASS com-
ments that they achieve their requirement of having less
than 0.05% artifact contamination in their source cata-
logs, which is much less stringent than future require-
ments will be. PQ employed a Multi-Layer Perceptron
for its classiﬁcation, also using object parameters such
as shape and size as input variables. They report a 90%
classiﬁcation accuracy, but we again emphasize that this
is classiﬁcation done after objects have already been ex-

2

tracted, not on the actual CCD pixels.

While image classiﬁcation is just making its way into
astronomy, it has been the subject of much work in the
ﬁeld of machine learning for a number of years. Two
approaches that have gained notoriety are the use of
Support Vector Machines and especially Convolutional
Neural Networks. In Chapelle et al. (1999), the authors
process images into color histograms and use these his-
togram values as inputs to a Support Vector Machine
trying a number of kernels. Their work reports an er-
ror rate of 11.5% on the Corel training set of RGB im-
ages. While this is an interesting result, it relies on color
images in order to construct features, an option that is
not available to us. Convolutional Neural Networks have
been used for a number of years in image classiﬁcation
problems. An early attempt in this direction was Le-
Cun et al. (1995) reporting a classiﬁcation error rate of
1.6% on a test set of written digits. Another very in-
teresting machine learning result in recent years was the
ImageNet (Krizhevsky et al. 2012) implementation of a
CNN, achieving 37.5% accuracy on a 1000 class image
classiﬁcation problem. Both of these papers used much
larger training sets than we had available, with many
more examples per class, but they are good examples of
the successes of CNNs on image classiﬁcation problems.
Additionally, most image classiﬁcation in the ﬁeld of Ma-
chine Learning is done on relatively high signal-to-noise
images.
It has been reported that when adding small
perturbations to images classiﬁed with high conﬁdence
by CNNs it is possible to fool them entirely (Goodfellow
et al. 2014). This may be relevant to our problem, since
the varying noise properties of individual exposures can
be interpreted as adding such perturbations to artifacts
that may otherwise be classiﬁed correctly.

3. DATA

The Dark Energy Survey (DES) is a sky survey being
conducted across many institutions. DES is being carried
out using the 4m Blanco Telescope located at the Cerro
Tololo Inter-American Observatory in Chile. The survey
saw ﬁrst light in 2012, and is currently in its 3rd year
of data taking. The DES camera (DECam) is made up
of 62 Charge Coupled Devices (CCDs) each composed of
4098 × 2048 pixels. Each pixel measures the ﬂux on the
sky as a number of electrons freed from the valence band
of the CCD silicon per second. The number of exposures
that DES takes per night varies widely depending on the
time of year and observing conditions, but a typical night
yields approximately 200 new exposures.

Given that the survey has been active for several years
taking data at the rates given above, the total volume
of data is on the order of 10 TB. This makes training a
model prohibitively expensive without reducing the size
of the data, even if only using a subset of the total data.
As a means of making our methods computationally fea-
sible, we decided to downsample the image data. We did
this by replacing squares of pixels of a characteristic side
length with the mean of all the pixels within the square.
This type of coarse-graining will not hide any artifacts
as long as the side lengths of the squares are signiﬁcantly
smaller than the sizes of the smallest artifacts. We found
that binning the data into 8x8 pixel bins (reducing our
data volume by a factor of 64), suﬃciently reduced the
size of our training set, while keeping the pixels signiﬁ-

cantly smaller than the observed sizes of artifacts.

Each CCD exposure also has associated with it an es-
timation of the background light not grouped into any
of the identiﬁed objects, as well as a mask identifying
the regions of the CCD containing bright stars, cosmic
rays and other spurious sources identiﬁed by the current
DES image reduction pipeline. Before downsampling the
exposures we subtract the estimated background and ap-
ply the mask. As a ﬁnal pre-processing step we apply the
transformation

(1)

ν = 255

arcsinh I − m

M − m

where I is the ﬂux of the pixel, M = max(I) and
m = min(I). This transformation emphasizes low level
noise in the image, where many of the artifacts have their
strongest signature.

Currently DES identiﬁes unmasked image artifacts
manually via a web based application (Melchior et al.
2015). Users inspect single CCD exposures, and upon
ﬁnding an artifact label a single pixel of the exposure
with the artifact type. Thus, there is no information
about the extent of the artifact, making classiﬁcation on
a pixel by pixel basis unfeasible. Artifacts are grouped
into 28 classes, including some obvious defects such as
Cosmic Ray, Airplane, and Satellite, and some more sub-
tle and obscure such as Haze, Dark Halo, and Wavy Sky.
We add an additional label, ’No Artifact’ as a null result
for our classiﬁer.

We used the DES web utility to provide us with our
training examples. Speciﬁcally, our training data consists
of all of the exposures obtained in the ﬁrst year of DES,
which have at least one CCD that was manually classiﬁed
as having an artifact. The total size of this data set
is 60856 CCD exposures, obtained from approximately
1000 observations. The number of artifacts classiﬁed in
the data is approximately 8000, spread across roughly
6% of the CCD exposures. Figure 1 contains examples
of 26 of the 28 artifact classes.

We labeled our training data based on which artifact
class it contained. Many of our exposures contain mul-
tiple classes of artifacts. For these, we randomly set the
artifact label to one of the artifacts present. When we
are attempting a binary classiﬁcation this is not an is-
sue, but when we attempt to classify into the full 29-class
space this may be a source of error.

Finally, we created an augmented training set consist-
ing of 3000 CCD exposures, half of which contained arti-
facts. We did this as a means of reducing the run times
of our algorithms, as well as emphasizing artifacts in our
training sets. This will likely bias our estimations of the
probabilities of artifacts high, as this training set con-
tains a much higher proportion of CCDs with artifacts
than the actual DES dataset, but a bias in this direction
is desirable. It is the opposite bias, one against classi-
fying CCDs as containing artifacts, that is detrimental
since this means that spurious objects make it into cat-
alogs being used for science.

4. METHODS

4.1. Support Vector Machine

Our ﬁrst attempt at an image artifact classiﬁer was a
Support Vector Machine. This was not the ideal choice

3

4.2. Convolutional Neural Network

Convolutional Neural Networks (CNNs) are a class of
learning algorithms tailored toward recognition of ab-
stract image features. CNNs accomplish this feat by
creating a number of convolutional ﬁlters which data
is passed through before reaching one or more densely
connected layers which operate more like conventional
machine learning methods. Because of the convolutional
layers, these networks are better suited for use with im-
ages than SVMs, because they are able to overcome prob-
lems like translations or rotations of image features and
can construct their own set of robust features out of raw
pixel data. CNNs are relatively diﬃcult to tune due to
the large number of components that can be optimized,
but when used correctly can lead to high image classiﬁ-
cation accuracies.

We have implemented a CNN using the python
wrapped library TensorFlow (Abadi et al. 2015). We
have constructed our CNN with the following architec-

ture:• A primary convolutional layer, followed by a max

pooling step to reduce the dimensionality of the
output features and a ReLU activation layer.
• A secondary convolutional layer with a similar max
pooling step, followed by a dropout layer to reduce
overﬁtting and a ReLU activation layer.
• The image is ﬂattened, and then passed to a pri-
mary fully connected layer followed by an addi-
tional dropout layer and a ReLU activation layer.
• A secondary fully connected layer followed by a
• A ﬁnal fully connected layer which produces the
probabilities of each class via a softmax activation.
In the convolutional layers, we convolve our images with
a series of ﬁlters and add a bias constant to each con-
volved image. The result of the convolution is given as

dropout layer and a ReLU activation layer.

(cid:88)

yi(cid:48),j(cid:48),k(cid:48) =

wijkk(cid:48)xi+i(cid:48),j+j(cid:48),k + bi(cid:48),j(cid:48)

(6)

i,j,k

where wijkk(cid:48), bi(cid:48),j(cid:48) are the learned convolution weights
and biases respectively. We then apply a Rectiﬁed Linear
Unit (ReLU) activation function

ReLU (x) = max (0, x)

(7)

to determine the output of each layer.
In the densely
connected layers, we perform a matrix multiplication us-
ing the outputs of the previous nodes as well as weights
matrices and constant bias terms. We also use ReLUs
for activation after these layers. The dropout layers ex-
ist to randomly deactivate the connections between neu-
rons in order to prevent overﬁtting, since if neurons are
randomly deactivated they cannot learn complex inter-
dependencies. Finally, a softmax activation on the last
densely connected layer produces the probabilities of
each class:

exp(cid:0)W (j)T x + b(j)(cid:1)
j=1 exp(cid:0)W (j)T x + b(j)(cid:1) (8)
(cid:80)n

Using the probabilities of each class, we compute the
cross entropy, deﬁned as

W,b(x) = P (y = j|x) =
hj
n(cid:88)

S = − m(cid:88)

i=1

j=1

Ptrue,ij log Ppred,ij

(9)

Figure 1. Example artifacts.

for such a high dimensional feature space, as we show
in 5 but SVMs require little ﬁne tuning in order to op-
timize their performance and are thus ideal for setting
performance baselines.

SVMs classify objects by assuming the hypothesis

takes the form

hw,b(x) = g(wT x + b)

(2)

where w and b are the weights and biases of the classiﬁer
and x is a feature vector. The SVM is trained by min-
imizing the geometric margin of the training set given
by

m(cid:88)

(cid:107)w(cid:107)2 + C

1
2

minγ,w,b

ξi
s.t. yi(wT xi + b) >= 1 − ξi, i = 1, . . . , m
ξi ≥ 0, i = 1, . . . , m

i=1

(3)

(4)
(5)

where γ = yi( w(cid:107)w(cid:107) T xi + b(cid:107)b(cid:107) ) is the geometric margin,
ξi are parameters allowing the training set to deviate
from linear separability and C is a regularization pa-
rameter controlling the sizes of the ξis. In practice this
minimization is performed by maximizing the dual ob-
jective, which allows us to replace inner products with
arbitrary kernels, implicitly using a higher dimensional
feature space.
In our implementation, we tried using
both a linear kernel as well as a gaussian kernel, ﬁnd-
ing no signiﬁcant diﬀerences between the two in terms of
generalization error.

4

where the sums are over the training examples and dif-
ferent classiﬁcations respectively. The cross entropy is a
measure of how similar two probability distributions are,
in this case the true probability of each class, and the
predicted probability. Our objective with the CNN is to
minimize S with respect to the parameters of all the dif-
ferent layers. In practice, we perform this minimization
using an Adam optimized (Kingma & Ba 2014) batch
gradient descent. Adam optimization is a variant of gra-
dient descent where the parameter updates are given as
follows

(cid:112)

θt = θt−1 + α · ˆmt/(

ˆvt + )

(10)

and ˆmt and ˆvt are estimates of the ﬁrst and second mo-
ments of the gradient of the optimization objective with
respect to the parameters θ. This update adaptively ad-
justs the eﬀective learning rate, such that the learning
rate is higher or lower based on the ratio of the ﬁrst and
second moments of the gradient. In practice, this imple-
ments a sort of annealing, where updates become smaller
when approaching minima. We varied the batch sizes
that we used in various training runs eventually using a
batch size of 5 images for our best run.

5. RESULTS

We have decided to examine our models using two dif-
ferent classiﬁcation schemes: The 29 class problem in
which we try to classify images based on their labels as
given by the Exposure Checker, and the binary problem
in which images are ﬂagged as either ”artifact” or ”no
artifact”. This allows us to explore multiple metrics for
the success of our method, including its eﬀectiveness at
classifying certain artifacts. For these exercises we con-
structed a test set by holding out 10% of the training
examples from the training set constructed as detailed
in Section 3.

To diagnose the eﬀectiveness of the SVM we computed
training and test error as a function of number of train-
ing examples. We ﬁnd that the test error of the SVM
consistently remains at 50% for both the binary and 29
class problem independent of the number of training ex-
amples, while the training error is always 0 as can be
seen in Figure 2. This behavior is indicative of over-
ﬁtting manifested as low training error. This is to be
expected as the high dimensionality of the feature space
allows the training set to be linearly separable, but this
linear boundary is not representative of the true decision
boundary due to the small number of training examples
with respect to the number of features and classes. We
tried using larger coarse graining factors in order to re-
duce the dimensionality of the feature space, but this
does not result in any improvement in the test error.
The 29-class problem always has 50% error because be-
cause our classiﬁer is labeling all test examples as label
29, the number corresponding to no artifact.

Our training and test errors for the CNN as a function
of number of training epochs for both the two and 29
class problems are shown in ﬁgure 3. It is apparent that,
although the CNN signiﬁcantly outperforms the SVM
(achieving a maximum test accuracy of 67%), it is still
overﬁtting the data. We discuss means of resolving this
issue in Section 6.

Figure 2. Training and Cross-validation error plotted against
number of training examples with a coarse graining factor of 8.

Figure 3. Training and test error as a function of the number of
training epochs. Dashed lines indicate training error, while solid
lines indicate test error

Figure 4. False Positive rate Vs. False Negative rate for the
Binary classiﬁcation problem. The colors indicate the threshold
probability necessary to be classiﬁed as an artifact.

When identifying artifacts, our main objective is to
minimize the rate of false negatives, because we do not
want artifacts to make it into object catalogs being used
for science. This means that it is useful to re-adjust our
decision boundary to only accept data if the model is very
conﬁdent that there are no artifacts. However, there is

Preprint typeset using LATEX style emulateapj v. 12/16/11

AUTOMATED IMAGE ARTIFACT IDENTIFICATION IN DARK ENERGY SURVEY CCD EXPOSURES

Kavli Institute for Particle Astrophysics and Cosmology & Physics Department, Stanford University, Stanford, CA 94305, USA

Joseph W. DeRose and Warren R. Morningstar

ABSTRACT

In this work, we present an implementation of a machine learning algorithm to identify and classify
image artifacts in observations taken by the Dark Energy Survey (DES). Speciﬁcally, we downsample
background subtracted CCD exposures and treat the downsampled pixel values as features in both a
Support Vector Machine (SVM) and a Convolutional Neural Network (CNN) to classify full exposures.
In general the CNN outperforms the SVM. In the two-class problem and the 29-class problem, our SVM
implementation does not perform better than random guessing on our test set for binary classiﬁcation,
and predicts no artifact for nearly everything in the 29-class problem. CNNs give a best test accuracy
of 67% on the binary classiﬁcation problem and 52% on the 29-class problem. We discuss sources of
systematic error in our models and plans to account for these in future work.

1. INTRODUCTION

In modern cosmology, much attention has been turned
toward using large photometric sky surveys to provide
constraints on structure formation and the composition
of our universe. An important component of modern sky
surveys is the management and processing of the large
volumes of data produced by the survey instruments.
Current sky surveys, such as the Dark Energy Survey
whose data is used in this work, generate on the order
of several gigabytes of images and metadata per night.
Future sky surveys will produce TB of data in the same
time frame. Because of the vast quantities of data pro-
duced, the much of the data reduction and analysis is
automated.

While this provides signiﬁcant advantages in perfor-
mance, an obvious disadvantage is that it is prone to
overlooking subtle features in the data which may cause
systematic errors in measurements of important quanti-
ties.
In particular, since the goal of many photomet-
ric surveys is to provide precise measurements of the
brightness of astronomical objects, any spurious bright-
ness variations within a detector have the potential to be
interpreted as a real signal, and thus may interfere with
the accuracy of measurements.

Brightness variations in a detector can be caused in
many diﬀerent ways, and thus have a variety of appear-
ances. A few notable examples that have been observed
by ongoing sky surveys include airplanes and satellites
passing across the ﬁeld of view, cosmic rays striking the
detector, stray scattered light and improper subtraction
of background noise. Some of these are detected and
masked away by the survey data reduction pipelines, but
a large number are missed. These missed aberrations are
usually referred to as image artifacts. Reliable identiﬁ-
cation of artifacts is an essential component of preparing
data for analysis.

Currently, the identiﬁcation of artifacts is carried out
by members of survey collaborations, a process that uses
a signiﬁcant number of human hours and that could be
better spent devoted to more scientiﬁcally interesting
problems. Therefore an automated means of identifying
and classifying image artifacts is desirable.

In this paper, we discuss our implementations of both
Support Vector Machines and Convolutional Neural Net-

works trained to identify image artifacts found in obser-
vations performed by DES. In section 2 we review at-
tempts in the literature to perform similar tasks. We de-
scribe our data set and choice of features in Section 3. In
Section 4, we discuss the theoretical underpinnings and
implementations of our models. In Section 5, we present
the results of our SVM and CNN on both a binary and
29-class problem. In Section 6, we discuss these results,
including ways in which our methods could be improved.

2. RELATED WORK

Automated image classiﬁcation in astronomy is only
just becoming a problem worth solving.
In the past,
many collaborations focused on taking a small number
of very long exposures over a small area of the night
sky. In these cases, great care was taken to gather data
only in the best observational conditions, and these data
were not numerous enough to require automated check-
ing. A few surveys such as the Sloan Digital Sky Survey
(SDSS) (Lupton et al. 2005), the 2-Micron All Sky Sur-
vey (2MASS) (Skrutskie et al. 2006) and the Palomar-
Quest (PQ) Survey (Donalek et al. 2008) have imple-
mented automated means of dealing with image artifacts.
The ﬁrst step in artifact identiﬁcation in all of these sur-
veys are background subtraction and object identiﬁca-
tion pipelines. Additionally, the surveys all attempt to
classify the objects which are extracted in these ﬁrst steps
as being real astronomical objects or artifacts, as opposed
to our approach of classifying artifacts on the CCD pix-
els themselves so that corrections can be applied before
object catalogs are created. SDSS and 2MASS both ac-
complish their classiﬁcations by making cuts in measured
parameters of the objects such as color and morphology,
something akin to a manually trained linear regression
algorithm. SDSS does not provide quantiﬁcation of their
performance on artifact detection, while 2MASS com-
ments that they achieve their requirement of having less
than 0.05% artifact contamination in their source cata-
logs, which is much less stringent than future require-
ments will be. PQ employed a Multi-Layer Perceptron
for its classiﬁcation, also using object parameters such
as shape and size as input variables. They report a 90%
classiﬁcation accuracy, but we again emphasize that this
is classiﬁcation done after objects have already been ex-

2

tracted, not on the actual CCD pixels.

While image classiﬁcation is just making its way into
astronomy, it has been the subject of much work in the
ﬁeld of machine learning for a number of years. Two
approaches that have gained notoriety are the use of
Support Vector Machines and especially Convolutional
Neural Networks. In Chapelle et al. (1999), the authors
process images into color histograms and use these his-
togram values as inputs to a Support Vector Machine
trying a number of kernels. Their work reports an er-
ror rate of 11.5% on the Corel training set of RGB im-
ages. While this is an interesting result, it relies on color
images in order to construct features, an option that is
not available to us. Convolutional Neural Networks have
been used for a number of years in image classiﬁcation
problems. An early attempt in this direction was Le-
Cun et al. (1995) reporting a classiﬁcation error rate of
1.6% on a test set of written digits. Another very in-
teresting machine learning result in recent years was the
ImageNet (Krizhevsky et al. 2012) implementation of a
CNN, achieving 37.5% accuracy on a 1000 class image
classiﬁcation problem. Both of these papers used much
larger training sets than we had available, with many
more examples per class, but they are good examples of
the successes of CNNs on image classiﬁcation problems.
Additionally, most image classiﬁcation in the ﬁeld of Ma-
chine Learning is done on relatively high signal-to-noise
images.
It has been reported that when adding small
perturbations to images classiﬁed with high conﬁdence
by CNNs it is possible to fool them entirely (Goodfellow
et al. 2014). This may be relevant to our problem, since
the varying noise properties of individual exposures can
be interpreted as adding such perturbations to artifacts
that may otherwise be classiﬁed correctly.

3. DATA

The Dark Energy Survey (DES) is a sky survey being
conducted across many institutions. DES is being carried
out using the 4m Blanco Telescope located at the Cerro
Tololo Inter-American Observatory in Chile. The survey
saw ﬁrst light in 2012, and is currently in its 3rd year
of data taking. The DES camera (DECam) is made up
of 62 Charge Coupled Devices (CCDs) each composed of
4098 × 2048 pixels. Each pixel measures the ﬂux on the
sky as a number of electrons freed from the valence band
of the CCD silicon per second. The number of exposures
that DES takes per night varies widely depending on the
time of year and observing conditions, but a typical night
yields approximately 200 new exposures.

Given that the survey has been active for several years
taking data at the rates given above, the total volume
of data is on the order of 10 TB. This makes training a
model prohibitively expensive without reducing the size
of the data, even if only using a subset of the total data.
As a means of making our methods computationally fea-
sible, we decided to downsample the image data. We did
this by replacing squares of pixels of a characteristic side
length with the mean of all the pixels within the square.
This type of coarse-graining will not hide any artifacts
as long as the side lengths of the squares are signiﬁcantly
smaller than the sizes of the smallest artifacts. We found
that binning the data into 8x8 pixel bins (reducing our
data volume by a factor of 64), suﬃciently reduced the
size of our training set, while keeping the pixels signiﬁ-

cantly smaller than the observed sizes of artifacts.

Each CCD exposure also has associated with it an es-
timation of the background light not grouped into any
of the identiﬁed objects, as well as a mask identifying
the regions of the CCD containing bright stars, cosmic
rays and other spurious sources identiﬁed by the current
DES image reduction pipeline. Before downsampling the
exposures we subtract the estimated background and ap-
ply the mask. As a ﬁnal pre-processing step we apply the
transformation

(1)

ν = 255

arcsinh I − m

M − m

where I is the ﬂux of the pixel, M = max(I) and
m = min(I). This transformation emphasizes low level
noise in the image, where many of the artifacts have their
strongest signature.

Currently DES identiﬁes unmasked image artifacts
manually via a web based application (Melchior et al.
2015). Users inspect single CCD exposures, and upon
ﬁnding an artifact label a single pixel of the exposure
with the artifact type. Thus, there is no information
about the extent of the artifact, making classiﬁcation on
a pixel by pixel basis unfeasible. Artifacts are grouped
into 28 classes, including some obvious defects such as
Cosmic Ray, Airplane, and Satellite, and some more sub-
tle and obscure such as Haze, Dark Halo, and Wavy Sky.
We add an additional label, ’No Artifact’ as a null result
for our classiﬁer.

We used the DES web utility to provide us with our
training examples. Speciﬁcally, our training data consists
of all of the exposures obtained in the ﬁrst year of DES,
which have at least one CCD that was manually classiﬁed
as having an artifact. The total size of this data set
is 60856 CCD exposures, obtained from approximately
1000 observations. The number of artifacts classiﬁed in
the data is approximately 8000, spread across roughly
6% of the CCD exposures. Figure 1 contains examples
of 26 of the 28 artifact classes.

We labeled our training data based on which artifact
class it contained. Many of our exposures contain mul-
tiple classes of artifacts. For these, we randomly set the
artifact label to one of the artifacts present. When we
are attempting a binary classiﬁcation this is not an is-
sue, but when we attempt to classify into the full 29-class
space this may be a source of error.

Finally, we created an augmented training set consist-
ing of 3000 CCD exposures, half of which contained arti-
facts. We did this as a means of reducing the run times
of our algorithms, as well as emphasizing artifacts in our
training sets. This will likely bias our estimations of the
probabilities of artifacts high, as this training set con-
tains a much higher proportion of CCDs with artifacts
than the actual DES dataset, but a bias in this direction
is desirable. It is the opposite bias, one against classi-
fying CCDs as containing artifacts, that is detrimental
since this means that spurious objects make it into cat-
alogs being used for science.

4. METHODS

4.1. Support Vector Machine

Our ﬁrst attempt at an image artifact classiﬁer was a
Support Vector Machine. This was not the ideal choice

3

4.2. Convolutional Neural Network

Convolutional Neural Networks (CNNs) are a class of
learning algorithms tailored toward recognition of ab-
stract image features. CNNs accomplish this feat by
creating a number of convolutional ﬁlters which data
is passed through before reaching one or more densely
connected layers which operate more like conventional
machine learning methods. Because of the convolutional
layers, these networks are better suited for use with im-
ages than SVMs, because they are able to overcome prob-
lems like translations or rotations of image features and
can construct their own set of robust features out of raw
pixel data. CNNs are relatively diﬃcult to tune due to
the large number of components that can be optimized,
but when used correctly can lead to high image classiﬁ-
cation accuracies.

We have implemented a CNN using the python
wrapped library TensorFlow (Abadi et al. 2015). We
have constructed our CNN with the following architec-

ture:• A primary convolutional layer, followed by a max

pooling step to reduce the dimensionality of the
output features and a ReLU activation layer.
• A secondary convolutional layer with a similar max
pooling step, followed by a dropout layer to reduce
overﬁtting and a ReLU activation layer.
• The image is ﬂattened, and then passed to a pri-
mary fully connected layer followed by an addi-
tional dropout layer and a ReLU activation layer.
• A secondary fully connected layer followed by a
• A ﬁnal fully connected layer which produces the
probabilities of each class via a softmax activation.
In the convolutional layers, we convolve our images with
a series of ﬁlters and add a bias constant to each con-
volved image. The result of the convolution is given as

dropout layer and a ReLU activation layer.

(cid:88)

yi(cid:48),j(cid:48),k(cid:48) =

wijkk(cid:48)xi+i(cid:48),j+j(cid:48),k + bi(cid:48),j(cid:48)

(6)

i,j,k

where wijkk(cid:48), bi(cid:48),j(cid:48) are the learned convolution weights
and biases respectively. We then apply a Rectiﬁed Linear
Unit (ReLU) activation function

ReLU (x) = max (0, x)

(7)

to determine the output of each layer.
In the densely
connected layers, we perform a matrix multiplication us-
ing the outputs of the previous nodes as well as weights
matrices and constant bias terms. We also use ReLUs
for activation after these layers. The dropout layers ex-
ist to randomly deactivate the connections between neu-
rons in order to prevent overﬁtting, since if neurons are
randomly deactivated they cannot learn complex inter-
dependencies. Finally, a softmax activation on the last
densely connected layer produces the probabilities of
each class:

exp(cid:0)W (j)T x + b(j)(cid:1)
j=1 exp(cid:0)W (j)T x + b(j)(cid:1) (8)
(cid:80)n

Using the probabilities of each class, we compute the
cross entropy, deﬁned as

W,b(x) = P (y = j|x) =
hj
n(cid:88)

S = − m(cid:88)

i=1

j=1

Ptrue,ij log Ppred,ij

(9)

Figure 1. Example artifacts.

for such a high dimensional feature space, as we show
in 5 but SVMs require little ﬁne tuning in order to op-
timize their performance and are thus ideal for setting
performance baselines.

SVMs classify objects by assuming the hypothesis

takes the form

hw,b(x) = g(wT x + b)

(2)

where w and b are the weights and biases of the classiﬁer
and x is a feature vector. The SVM is trained by min-
imizing the geometric margin of the training set given
by

m(cid:88)

(cid:107)w(cid:107)2 + C

1
2

minγ,w,b

ξi
s.t. yi(wT xi + b) >= 1 − ξi, i = 1, . . . , m
ξi ≥ 0, i = 1, . . . , m

i=1

(3)

(4)
(5)

where γ = yi( w(cid:107)w(cid:107) T xi + b(cid:107)b(cid:107) ) is the geometric margin,
ξi are parameters allowing the training set to deviate
from linear separability and C is a regularization pa-
rameter controlling the sizes of the ξis. In practice this
minimization is performed by maximizing the dual ob-
jective, which allows us to replace inner products with
arbitrary kernels, implicitly using a higher dimensional
feature space.
In our implementation, we tried using
both a linear kernel as well as a gaussian kernel, ﬁnd-
ing no signiﬁcant diﬀerences between the two in terms of
generalization error.

4

where the sums are over the training examples and dif-
ferent classiﬁcations respectively. The cross entropy is a
measure of how similar two probability distributions are,
in this case the true probability of each class, and the
predicted probability. Our objective with the CNN is to
minimize S with respect to the parameters of all the dif-
ferent layers. In practice, we perform this minimization
using an Adam optimized (Kingma & Ba 2014) batch
gradient descent. Adam optimization is a variant of gra-
dient descent where the parameter updates are given as
follows

(cid:112)

θt = θt−1 + α · ˆmt/(

ˆvt + )

(10)

and ˆmt and ˆvt are estimates of the ﬁrst and second mo-
ments of the gradient of the optimization objective with
respect to the parameters θ. This update adaptively ad-
justs the eﬀective learning rate, such that the learning
rate is higher or lower based on the ratio of the ﬁrst and
second moments of the gradient. In practice, this imple-
ments a sort of annealing, where updates become smaller
when approaching minima. We varied the batch sizes
that we used in various training runs eventually using a
batch size of 5 images for our best run.

5. RESULTS

We have decided to examine our models using two dif-
ferent classiﬁcation schemes: The 29 class problem in
which we try to classify images based on their labels as
given by the Exposure Checker, and the binary problem
in which images are ﬂagged as either ”artifact” or ”no
artifact”. This allows us to explore multiple metrics for
the success of our method, including its eﬀectiveness at
classifying certain artifacts. For these exercises we con-
structed a test set by holding out 10% of the training
examples from the training set constructed as detailed
in Section 3.

To diagnose the eﬀectiveness of the SVM we computed
training and test error as a function of number of train-
ing examples. We ﬁnd that the test error of the SVM
consistently remains at 50% for both the binary and 29
class problem independent of the number of training ex-
amples, while the training error is always 0 as can be
seen in Figure 2. This behavior is indicative of over-
ﬁtting manifested as low training error. This is to be
expected as the high dimensionality of the feature space
allows the training set to be linearly separable, but this
linear boundary is not representative of the true decision
boundary due to the small number of training examples
with respect to the number of features and classes. We
tried using larger coarse graining factors in order to re-
duce the dimensionality of the feature space, but this
does not result in any improvement in the test error.
The 29-class problem always has 50% error because be-
cause our classiﬁer is labeling all test examples as label
29, the number corresponding to no artifact.

Our training and test errors for the CNN as a function
of number of training epochs for both the two and 29
class problems are shown in ﬁgure 3. It is apparent that,
although the CNN signiﬁcantly outperforms the SVM
(achieving a maximum test accuracy of 67%), it is still
overﬁtting the data. We discuss means of resolving this
issue in Section 6.

Figure 2. Training and Cross-validation error plotted against
number of training examples with a coarse graining factor of 8.

Figure 3. Training and test error as a function of the number of
training epochs. Dashed lines indicate training error, while solid
lines indicate test error

Figure 4. False Positive rate Vs. False Negative rate for the
Binary classiﬁcation problem. The colors indicate the threshold
probability necessary to be classiﬁed as an artifact.

When identifying artifacts, our main objective is to
minimize the rate of false negatives, because we do not
want artifacts to make it into object catalogs being used
for science. This means that it is useful to re-adjust our
decision boundary to only accept data if the model is very
conﬁdent that there are no artifacts. However, there is

a trade oﬀ, because adjusting the decision boundary in-
creases the rate of false positives. In order to maximize
the utility of our classiﬁer in terms of number of expo-
sures that do not need to be manually examined, this
rate should also be minimized. We have thus plotted an
ROC-like curve in which we attempt to determine the
best probability threshold to use as our decision bound-
ary. It is plotted in Figure 4.

Figure 5. Confusion matrix for our best CNN classiﬁer for the
29 class problem. The 29th label is ’no artifact’.

Figure 6. Confusion matrix for our best CNN classiﬁer for the
binary problem breaking down the true labels into their actual
classes. The 29th truth label and the top predicted label are ’no
artifact’.

6. DISCUSSION AND FUTURE DIRECTIONS

We have trained both a CNN and an SVM on a sam-
ple of data from the Dark Energy Survey, ﬁnding that
the CNN signiﬁcantly outperforms the SVM, achieving a
test accuracy of up to 67%, while the SVM is consistent
with random guessing. Since the major problem with the
CNN appears to be overﬁtting, the most straightforward
way to improve our classiﬁer will be to collect additional
training data. This will become available with the re-
lease of the data taken during additional DES observing
seasons. Overﬁtting can also be reduced in the CNN by
increasing the dropout probability in the dropout layers,
or by applying additional coarsegraining to the data. We
must be careful with the latter however, since excessive

5

coarsegraining will diminish our sensitivity to artifacts
such as cosmic rays, which only take up a few of the
original image pixels.

It is also possible that our CNN is mis-classifying arti-
facts because their features are too subtle or noisy to be
observed. Figure 1 shows the most noticeable examples
of all of the diﬀerent artifact classes. While several are
obvious, there are others that could be missed altogether
even by a human observer. Because of this, it is possible
that many of the false negatives found occur because the
features of that particular class of artifact are too subtle
to be noticed by the CNN. To examine this possibility,
we have plotted the test error for all the classes present
in our test set in Figure 6. We have found that Tree-
rings, Haze, and A/B jump artifacts were never classiﬁed
as being artifacts in our test set, which agrees with our
hypothesis since all three of these have very subtle sig-
natures in the data. The subtlety of the features of some
artifacts may also mean that some of the ’true’ train-
ing labels are incorrect, a possibility that we have not
attempted to account for.

Another source of classiﬁcation error in our data is our
requirement that each artifact has only 1 training label.
In the second from the top panel on the right column of
Figure 1, we see that both a satellite, and bright streaks
are present in a single exposure. This suggests that we
should adjust our model either to allow for multiple clas-
siﬁcations of a single image, or adjust our training set so
that only one class of artifacts is present in each image
section. The diﬃculty in doing this is that each arti-
fact label is only assigned to a single pixel, meaning that
we are currently limited to requiring that each artifact
also fully fall on a single CCD. We can overcome this in
principle by applying a region growing algorithm which
could be used to classify each pixel in our training set
with its appropriate artifact label. This is beyond the
scope of this project however, so we merely note its po-
tential as an improvement. We must also re-iterate that
this problem only exists when considering the 29 class
problem.

prone to overﬁtting.

From this work, we draw the following conclusions:
• Our CNN outperforms an SVM, although it is still
• We could improve our model by adding additional
training data, or by using smaller postage stamp
images with more extensive labeling.
• Our classiﬁer is less successful at identifying arti-

facts whose features are subtle.

We plan to continue this work with the hope that it will
eventually be integrated into the DES image processing
pipeline.

REFERENCES

Abadi, M., Agarwal, A., Barham, P., et al. 2015, TensorFlow:

Large-Scale Machine Learning on Heterogeneous Systems,
software available from tensorﬂow.org

Chapelle, O., Haﬀner, P., & Vapnik, V. N. 1999, Neural

Networks, IEEE Transactions on, 10, 1055

Donalek, C., Mahabal, A., Djorgovski, S., et al. 2008, arXiv

preprint arXiv:0810.4945

arXiv:1412.6572

Goodfellow, I. J., Shlens, J., & Szegedy, C. 2014, arXiv preprint

Kingma, D., & Ba, J. 2014, arXiv preprint arXiv:1412.6980
Krizhevsky, A., Sutskever, I., & Hinton, G. E. 2012, in Advances

in neural information processing systems, 1097–1105

LeCun, Y., Jackel, L., Bottou, L., et al. 1995, in International

conference on artiﬁcial neural networks, Vol. 60, 53–60

Preprint typeset using LATEX style emulateapj v. 12/16/11

AUTOMATED IMAGE ARTIFACT IDENTIFICATION IN DARK ENERGY SURVEY CCD EXPOSURES

Kavli Institute for Particle Astrophysics and Cosmology & Physics Department, Stanford University, Stanford, CA 94305, USA

Joseph W. DeRose and Warren R. Morningstar

ABSTRACT

In this work, we present an implementation of a machine learning algorithm to identify and classify
image artifacts in observations taken by the Dark Energy Survey (DES). Speciﬁcally, we downsample
background subtracted CCD exposures and treat the downsampled pixel values as features in both a
Support Vector Machine (SVM) and a Convolutional Neural Network (CNN) to classify full exposures.
In general the CNN outperforms the SVM. In the two-class problem and the 29-class problem, our SVM
implementation does not perform better than random guessing on our test set for binary classiﬁcation,
and predicts no artifact for nearly everything in the 29-class problem. CNNs give a best test accuracy
of 67% on the binary classiﬁcation problem and 52% on the 29-class problem. We discuss sources of
systematic error in our models and plans to account for these in future work.

1. INTRODUCTION

In modern cosmology, much attention has been turned
toward using large photometric sky surveys to provide
constraints on structure formation and the composition
of our universe. An important component of modern sky
surveys is the management and processing of the large
volumes of data produced by the survey instruments.
Current sky surveys, such as the Dark Energy Survey
whose data is used in this work, generate on the order
of several gigabytes of images and metadata per night.
Future sky surveys will produce TB of data in the same
time frame. Because of the vast quantities of data pro-
duced, the much of the data reduction and analysis is
automated.

While this provides signiﬁcant advantages in perfor-
mance, an obvious disadvantage is that it is prone to
overlooking subtle features in the data which may cause
systematic errors in measurements of important quanti-
ties.
In particular, since the goal of many photomet-
ric surveys is to provide precise measurements of the
brightness of astronomical objects, any spurious bright-
ness variations within a detector have the potential to be
interpreted as a real signal, and thus may interfere with
the accuracy of measurements.

Brightness variations in a detector can be caused in
many diﬀerent ways, and thus have a variety of appear-
ances. A few notable examples that have been observed
by ongoing sky surveys include airplanes and satellites
passing across the ﬁeld of view, cosmic rays striking the
detector, stray scattered light and improper subtraction
of background noise. Some of these are detected and
masked away by the survey data reduction pipelines, but
a large number are missed. These missed aberrations are
usually referred to as image artifacts. Reliable identiﬁ-
cation of artifacts is an essential component of preparing
data for analysis.

Currently, the identiﬁcation of artifacts is carried out
by members of survey collaborations, a process that uses
a signiﬁcant number of human hours and that could be
better spent devoted to more scientiﬁcally interesting
problems. Therefore an automated means of identifying
and classifying image artifacts is desirable.

In this paper, we discuss our implementations of both
Support Vector Machines and Convolutional Neural Net-

works trained to identify image artifacts found in obser-
vations performed by DES. In section 2 we review at-
tempts in the literature to perform similar tasks. We de-
scribe our data set and choice of features in Section 3. In
Section 4, we discuss the theoretical underpinnings and
implementations of our models. In Section 5, we present
the results of our SVM and CNN on both a binary and
29-class problem. In Section 6, we discuss these results,
including ways in which our methods could be improved.

2. RELATED WORK

Automated image classiﬁcation in astronomy is only
just becoming a problem worth solving.
In the past,
many collaborations focused on taking a small number
of very long exposures over a small area of the night
sky. In these cases, great care was taken to gather data
only in the best observational conditions, and these data
were not numerous enough to require automated check-
ing. A few surveys such as the Sloan Digital Sky Survey
(SDSS) (Lupton et al. 2005), the 2-Micron All Sky Sur-
vey (2MASS) (Skrutskie et al. 2006) and the Palomar-
Quest (PQ) Survey (Donalek et al. 2008) have imple-
mented automated means of dealing with image artifacts.
The ﬁrst step in artifact identiﬁcation in all of these sur-
veys are background subtraction and object identiﬁca-
tion pipelines. Additionally, the surveys all attempt to
classify the objects which are extracted in these ﬁrst steps
as being real astronomical objects or artifacts, as opposed
to our approach of classifying artifacts on the CCD pix-
els themselves so that corrections can be applied before
object catalogs are created. SDSS and 2MASS both ac-
complish their classiﬁcations by making cuts in measured
parameters of the objects such as color and morphology,
something akin to a manually trained linear regression
algorithm. SDSS does not provide quantiﬁcation of their
performance on artifact detection, while 2MASS com-
ments that they achieve their requirement of having less
than 0.05% artifact contamination in their source cata-
logs, which is much less stringent than future require-
ments will be. PQ employed a Multi-Layer Perceptron
for its classiﬁcation, also using object parameters such
as shape and size as input variables. They report a 90%
classiﬁcation accuracy, but we again emphasize that this
is classiﬁcation done after objects have already been ex-

2

tracted, not on the actual CCD pixels.

While image classiﬁcation is just making its way into
astronomy, it has been the subject of much work in the
ﬁeld of machine learning for a number of years. Two
approaches that have gained notoriety are the use of
Support Vector Machines and especially Convolutional
Neural Networks. In Chapelle et al. (1999), the authors
process images into color histograms and use these his-
togram values as inputs to a Support Vector Machine
trying a number of kernels. Their work reports an er-
ror rate of 11.5% on the Corel training set of RGB im-
ages. While this is an interesting result, it relies on color
images in order to construct features, an option that is
not available to us. Convolutional Neural Networks have
been used for a number of years in image classiﬁcation
problems. An early attempt in this direction was Le-
Cun et al. (1995) reporting a classiﬁcation error rate of
1.6% on a test set of written digits. Another very in-
teresting machine learning result in recent years was the
ImageNet (Krizhevsky et al. 2012) implementation of a
CNN, achieving 37.5% accuracy on a 1000 class image
classiﬁcation problem. Both of these papers used much
larger training sets than we had available, with many
more examples per class, but they are good examples of
the successes of CNNs on image classiﬁcation problems.
Additionally, most image classiﬁcation in the ﬁeld of Ma-
chine Learning is done on relatively high signal-to-noise
images.
It has been reported that when adding small
perturbations to images classiﬁed with high conﬁdence
by CNNs it is possible to fool them entirely (Goodfellow
et al. 2014). This may be relevant to our problem, since
the varying noise properties of individual exposures can
be interpreted as adding such perturbations to artifacts
that may otherwise be classiﬁed correctly.

3. DATA

The Dark Energy Survey (DES) is a sky survey being
conducted across many institutions. DES is being carried
out using the 4m Blanco Telescope located at the Cerro
Tololo Inter-American Observatory in Chile. The survey
saw ﬁrst light in 2012, and is currently in its 3rd year
of data taking. The DES camera (DECam) is made up
of 62 Charge Coupled Devices (CCDs) each composed of
4098 × 2048 pixels. Each pixel measures the ﬂux on the
sky as a number of electrons freed from the valence band
of the CCD silicon per second. The number of exposures
that DES takes per night varies widely depending on the
time of year and observing conditions, but a typical night
yields approximately 200 new exposures.

Given that the survey has been active for several years
taking data at the rates given above, the total volume
of data is on the order of 10 TB. This makes training a
model prohibitively expensive without reducing the size
of the data, even if only using a subset of the total data.
As a means of making our methods computationally fea-
sible, we decided to downsample the image data. We did
this by replacing squares of pixels of a characteristic side
length with the mean of all the pixels within the square.
This type of coarse-graining will not hide any artifacts
as long as the side lengths of the squares are signiﬁcantly
smaller than the sizes of the smallest artifacts. We found
that binning the data into 8x8 pixel bins (reducing our
data volume by a factor of 64), suﬃciently reduced the
size of our training set, while keeping the pixels signiﬁ-

cantly smaller than the observed sizes of artifacts.

Each CCD exposure also has associated with it an es-
timation of the background light not grouped into any
of the identiﬁed objects, as well as a mask identifying
the regions of the CCD containing bright stars, cosmic
rays and other spurious sources identiﬁed by the current
DES image reduction pipeline. Before downsampling the
exposures we subtract the estimated background and ap-
ply the mask. As a ﬁnal pre-processing step we apply the
transformation

(1)

ν = 255

arcsinh I − m

M − m

where I is the ﬂux of the pixel, M = max(I) and
m = min(I). This transformation emphasizes low level
noise in the image, where many of the artifacts have their
strongest signature.

Currently DES identiﬁes unmasked image artifacts
manually via a web based application (Melchior et al.
2015). Users inspect single CCD exposures, and upon
ﬁnding an artifact label a single pixel of the exposure
with the artifact type. Thus, there is no information
about the extent of the artifact, making classiﬁcation on
a pixel by pixel basis unfeasible. Artifacts are grouped
into 28 classes, including some obvious defects such as
Cosmic Ray, Airplane, and Satellite, and some more sub-
tle and obscure such as Haze, Dark Halo, and Wavy Sky.
We add an additional label, ’No Artifact’ as a null result
for our classiﬁer.

We used the DES web utility to provide us with our
training examples. Speciﬁcally, our training data consists
of all of the exposures obtained in the ﬁrst year of DES,
which have at least one CCD that was manually classiﬁed
as having an artifact. The total size of this data set
is 60856 CCD exposures, obtained from approximately
1000 observations. The number of artifacts classiﬁed in
the data is approximately 8000, spread across roughly
6% of the CCD exposures. Figure 1 contains examples
of 26 of the 28 artifact classes.

We labeled our training data based on which artifact
class it contained. Many of our exposures contain mul-
tiple classes of artifacts. For these, we randomly set the
artifact label to one of the artifacts present. When we
are attempting a binary classiﬁcation this is not an is-
sue, but when we attempt to classify into the full 29-class
space this may be a source of error.

Finally, we created an augmented training set consist-
ing of 3000 CCD exposures, half of which contained arti-
facts. We did this as a means of reducing the run times
of our algorithms, as well as emphasizing artifacts in our
training sets. This will likely bias our estimations of the
probabilities of artifacts high, as this training set con-
tains a much higher proportion of CCDs with artifacts
than the actual DES dataset, but a bias in this direction
is desirable. It is the opposite bias, one against classi-
fying CCDs as containing artifacts, that is detrimental
since this means that spurious objects make it into cat-
alogs being used for science.

4. METHODS

4.1. Support Vector Machine

Our ﬁrst attempt at an image artifact classiﬁer was a
Support Vector Machine. This was not the ideal choice

3

4.2. Convolutional Neural Network

Convolutional Neural Networks (CNNs) are a class of
learning algorithms tailored toward recognition of ab-
stract image features. CNNs accomplish this feat by
creating a number of convolutional ﬁlters which data
is passed through before reaching one or more densely
connected layers which operate more like conventional
machine learning methods. Because of the convolutional
layers, these networks are better suited for use with im-
ages than SVMs, because they are able to overcome prob-
lems like translations or rotations of image features and
can construct their own set of robust features out of raw
pixel data. CNNs are relatively diﬃcult to tune due to
the large number of components that can be optimized,
but when used correctly can lead to high image classiﬁ-
cation accuracies.

We have implemented a CNN using the python
wrapped library TensorFlow (Abadi et al. 2015). We
have constructed our CNN with the following architec-

ture:• A primary convolutional layer, followed by a max

pooling step to reduce the dimensionality of the
output features and a ReLU activation layer.
• A secondary convolutional layer with a similar max
pooling step, followed by a dropout layer to reduce
overﬁtting and a ReLU activation layer.
• The image is ﬂattened, and then passed to a pri-
mary fully connected layer followed by an addi-
tional dropout layer and a ReLU activation layer.
• A secondary fully connected layer followed by a
• A ﬁnal fully connected layer which produces the
probabilities of each class via a softmax activation.
In the convolutional layers, we convolve our images with
a series of ﬁlters and add a bias constant to each con-
volved image. The result of the convolution is given as

dropout layer and a ReLU activation layer.

(cid:88)

yi(cid:48),j(cid:48),k(cid:48) =

wijkk(cid:48)xi+i(cid:48),j+j(cid:48),k + bi(cid:48),j(cid:48)

(6)

i,j,k

where wijkk(cid:48), bi(cid:48),j(cid:48) are the learned convolution weights
and biases respectively. We then apply a Rectiﬁed Linear
Unit (ReLU) activation function

ReLU (x) = max (0, x)

(7)

to determine the output of each layer.
In the densely
connected layers, we perform a matrix multiplication us-
ing the outputs of the previous nodes as well as weights
matrices and constant bias terms. We also use ReLUs
for activation after these layers. The dropout layers ex-
ist to randomly deactivate the connections between neu-
rons in order to prevent overﬁtting, since if neurons are
randomly deactivated they cannot learn complex inter-
dependencies. Finally, a softmax activation on the last
densely connected layer produces the probabilities of
each class:

exp(cid:0)W (j)T x + b(j)(cid:1)
j=1 exp(cid:0)W (j)T x + b(j)(cid:1) (8)
(cid:80)n

Using the probabilities of each class, we compute the
cross entropy, deﬁned as

W,b(x) = P (y = j|x) =
hj
n(cid:88)

S = − m(cid:88)

i=1

j=1

Ptrue,ij log Ppred,ij

(9)

Figure 1. Example artifacts.

for such a high dimensional feature space, as we show
in 5 but SVMs require little ﬁne tuning in order to op-
timize their performance and are thus ideal for setting
performance baselines.

SVMs classify objects by assuming the hypothesis

takes the form

hw,b(x) = g(wT x + b)

(2)

where w and b are the weights and biases of the classiﬁer
and x is a feature vector. The SVM is trained by min-
imizing the geometric margin of the training set given
by

m(cid:88)

(cid:107)w(cid:107)2 + C

1
2

minγ,w,b

ξi
s.t. yi(wT xi + b) >= 1 − ξi, i = 1, . . . , m
ξi ≥ 0, i = 1, . . . , m

i=1

(3)

(4)
(5)

where γ = yi( w(cid:107)w(cid:107) T xi + b(cid:107)b(cid:107) ) is the geometric margin,
ξi are parameters allowing the training set to deviate
from linear separability and C is a regularization pa-
rameter controlling the sizes of the ξis. In practice this
minimization is performed by maximizing the dual ob-
jective, which allows us to replace inner products with
arbitrary kernels, implicitly using a higher dimensional
feature space.
In our implementation, we tried using
both a linear kernel as well as a gaussian kernel, ﬁnd-
ing no signiﬁcant diﬀerences between the two in terms of
generalization error.

4

where the sums are over the training examples and dif-
ferent classiﬁcations respectively. The cross entropy is a
measure of how similar two probability distributions are,
in this case the true probability of each class, and the
predicted probability. Our objective with the CNN is to
minimize S with respect to the parameters of all the dif-
ferent layers. In practice, we perform this minimization
using an Adam optimized (Kingma & Ba 2014) batch
gradient descent. Adam optimization is a variant of gra-
dient descent where the parameter updates are given as
follows

(cid:112)

θt = θt−1 + α · ˆmt/(

ˆvt + )

(10)

and ˆmt and ˆvt are estimates of the ﬁrst and second mo-
ments of the gradient of the optimization objective with
respect to the parameters θ. This update adaptively ad-
justs the eﬀective learning rate, such that the learning
rate is higher or lower based on the ratio of the ﬁrst and
second moments of the gradient. In practice, this imple-
ments a sort of annealing, where updates become smaller
when approaching minima. We varied the batch sizes
that we used in various training runs eventually using a
batch size of 5 images for our best run.

5. RESULTS

We have decided to examine our models using two dif-
ferent classiﬁcation schemes: The 29 class problem in
which we try to classify images based on their labels as
given by the Exposure Checker, and the binary problem
in which images are ﬂagged as either ”artifact” or ”no
artifact”. This allows us to explore multiple metrics for
the success of our method, including its eﬀectiveness at
classifying certain artifacts. For these exercises we con-
structed a test set by holding out 10% of the training
examples from the training set constructed as detailed
in Section 3.

To diagnose the eﬀectiveness of the SVM we computed
training and test error as a function of number of train-
ing examples. We ﬁnd that the test error of the SVM
consistently remains at 50% for both the binary and 29
class problem independent of the number of training ex-
amples, while the training error is always 0 as can be
seen in Figure 2. This behavior is indicative of over-
ﬁtting manifested as low training error. This is to be
expected as the high dimensionality of the feature space
allows the training set to be linearly separable, but this
linear boundary is not representative of the true decision
boundary due to the small number of training examples
with respect to the number of features and classes. We
tried using larger coarse graining factors in order to re-
duce the dimensionality of the feature space, but this
does not result in any improvement in the test error.
The 29-class problem always has 50% error because be-
cause our classiﬁer is labeling all test examples as label
29, the number corresponding to no artifact.

Our training and test errors for the CNN as a function
of number of training epochs for both the two and 29
class problems are shown in ﬁgure 3. It is apparent that,
although the CNN signiﬁcantly outperforms the SVM
(achieving a maximum test accuracy of 67%), it is still
overﬁtting the data. We discuss means of resolving this
issue in Section 6.

Figure 2. Training and Cross-validation error plotted against
number of training examples with a coarse graining factor of 8.

Figure 3. Training and test error as a function of the number of
training epochs. Dashed lines indicate training error, while solid
lines indicate test error

Figure 4. False Positive rate Vs. False Negative rate for the
Binary classiﬁcation problem. The colors indicate the threshold
probability necessary to be classiﬁed as an artifact.

When identifying artifacts, our main objective is to
minimize the rate of false negatives, because we do not
want artifacts to make it into object catalogs being used
for science. This means that it is useful to re-adjust our
decision boundary to only accept data if the model is very
conﬁdent that there are no artifacts. However, there is

a trade oﬀ, because adjusting the decision boundary in-
creases the rate of false positives. In order to maximize
the utility of our classiﬁer in terms of number of expo-
sures that do not need to be manually examined, this
rate should also be minimized. We have thus plotted an
ROC-like curve in which we attempt to determine the
best probability threshold to use as our decision bound-
ary. It is plotted in Figure 4.

Figure 5. Confusion matrix for our best CNN classiﬁer for the
29 class problem. The 29th label is ’no artifact’.

Figure 6. Confusion matrix for our best CNN classiﬁer for the
binary problem breaking down the true labels into their actual
classes. The 29th truth label and the top predicted label are ’no
artifact’.

6. DISCUSSION AND FUTURE DIRECTIONS

We have trained both a CNN and an SVM on a sam-
ple of data from the Dark Energy Survey, ﬁnding that
the CNN signiﬁcantly outperforms the SVM, achieving a
test accuracy of up to 67%, while the SVM is consistent
with random guessing. Since the major problem with the
CNN appears to be overﬁtting, the most straightforward
way to improve our classiﬁer will be to collect additional
training data. This will become available with the re-
lease of the data taken during additional DES observing
seasons. Overﬁtting can also be reduced in the CNN by
increasing the dropout probability in the dropout layers,
or by applying additional coarsegraining to the data. We
must be careful with the latter however, since excessive

5

coarsegraining will diminish our sensitivity to artifacts
such as cosmic rays, which only take up a few of the
original image pixels.

It is also possible that our CNN is mis-classifying arti-
facts because their features are too subtle or noisy to be
observed. Figure 1 shows the most noticeable examples
of all of the diﬀerent artifact classes. While several are
obvious, there are others that could be missed altogether
even by a human observer. Because of this, it is possible
that many of the false negatives found occur because the
features of that particular class of artifact are too subtle
to be noticed by the CNN. To examine this possibility,
we have plotted the test error for all the classes present
in our test set in Figure 6. We have found that Tree-
rings, Haze, and A/B jump artifacts were never classiﬁed
as being artifacts in our test set, which agrees with our
hypothesis since all three of these have very subtle sig-
natures in the data. The subtlety of the features of some
artifacts may also mean that some of the ’true’ train-
ing labels are incorrect, a possibility that we have not
attempted to account for.

Another source of classiﬁcation error in our data is our
requirement that each artifact has only 1 training label.
In the second from the top panel on the right column of
Figure 1, we see that both a satellite, and bright streaks
are present in a single exposure. This suggests that we
should adjust our model either to allow for multiple clas-
siﬁcations of a single image, or adjust our training set so
that only one class of artifacts is present in each image
section. The diﬃculty in doing this is that each arti-
fact label is only assigned to a single pixel, meaning that
we are currently limited to requiring that each artifact
also fully fall on a single CCD. We can overcome this in
principle by applying a region growing algorithm which
could be used to classify each pixel in our training set
with its appropriate artifact label. This is beyond the
scope of this project however, so we merely note its po-
tential as an improvement. We must also re-iterate that
this problem only exists when considering the 29 class
problem.

prone to overﬁtting.

From this work, we draw the following conclusions:
• Our CNN outperforms an SVM, although it is still
• We could improve our model by adding additional
training data, or by using smaller postage stamp
images with more extensive labeling.
• Our classiﬁer is less successful at identifying arti-

facts whose features are subtle.

We plan to continue this work with the hope that it will
eventually be integrated into the DES image processing
pipeline.

REFERENCES

Abadi, M., Agarwal, A., Barham, P., et al. 2015, TensorFlow:

Large-Scale Machine Learning on Heterogeneous Systems,
software available from tensorﬂow.org

Chapelle, O., Haﬀner, P., & Vapnik, V. N. 1999, Neural

Networks, IEEE Transactions on, 10, 1055

Donalek, C., Mahabal, A., Djorgovski, S., et al. 2008, arXiv

preprint arXiv:0810.4945

arXiv:1412.6572

Goodfellow, I. J., Shlens, J., & Szegedy, C. 2014, arXiv preprint

Kingma, D., & Ba, J. 2014, arXiv preprint arXiv:1412.6980
Krizhevsky, A., Sutskever, I., & Hinton, G. E. 2012, in Advances

in neural information processing systems, 1097–1105

LeCun, Y., Jackel, L., Bottou, L., et al. 1995, in International

conference on artiﬁcial neural networks, Vol. 60, 53–60

6

Lupton, R. H., Ivezic, Z., Gunn, J. E., et al. 2005, SDSS Image

Processing II: The Photo Pipelines, Tech. rep., Technical report

Melchior, P., Sheldon, E., Drlica-Wagner, A., et al. 2015, arXiv

preprint arXiv:1511.03391

Skrutskie, M., Cutri, R., Stiening, R., et al. 2006, The

Astronomical Journal, 131, 1163

