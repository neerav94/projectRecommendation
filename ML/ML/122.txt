Drug	 Â Store	 Â Sales	 Â Prediction	 Â 

Chenghao	 Â Wang,	 Â Yang	 Â Li	 Â 

1.	 Â INTRODUCTION	 Â 

2.	 Â RELATED	 Â WORK	 Â 	 Â 

Abstract	 Â -Â­â€	 Â In	 Â this	 Â paper	 Â we	 Â tried	 Â to	 Â apply	 Â machine	 Â learning	 Â algorithm	 Â into	 Â a	 Â real	 Â world	 Â problem	 Â â€“	 Â drug	 Â store	 Â sales	 Â 
forecasting.	 Â Given	 Â store	 Â information,	 Â and	 Â sales	 Â record	 Â we	 Â applied	 Â Linear	 Â Regression,	 Â Support	 Â Vector	 Â Regression(SVR)	 Â with	 Â 
Gaussian	 Â and	 Â Polynomial	 Â Kernels	 Â and	 Â Random	 Â Forest	 Â algorithm,	 Â and	 Â tried	 Â to	 Â predict	 Â sales	 Â for	 Â 1-Â­â€3	 Â weeks.	 Â Root	 Â Mean	 Â 
Square	 Â Percentage	 Â Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â the	 Â accuracy.	 Â As	 Â it	 Â turned	 Â out,	 Â Random	 Â Forest	 Â outshined	 Â all	 Â other	 Â 
models	 Â and	 Â reached	 Â RMSPE	 Â of	 Â 12.3%,	 Â which	 Â is	 Â a	 Â reliable	 Â forecast	 Â that	 Â enables	 Â store	 Â managers	 Â allocate	 Â staff	 Â and	 Â stock	 Â up	 Â 
effectively.	 Â  	 Â 
This	 Â problem	 Â is	 Â one	 Â of	 Â several	 Â Machine	 Â Learning	 Â problems	 Â on	 Â Kaggle1	 Â .	 Â The	 Â aim	 Â of	 Â this	 Â problem	 Â is	 Â to	 Â forecast	 Â future	 Â 
sales	 Â of	 Â 1,115	 Â Rossman	 Â drug	 Â stores	 Â located	 Â across	 Â Germany	 Â based	 Â on	 Â their	 Â historical	 Â sales	 Â data.	 Â The	 Â practical	 Â meaning	 Â of	 Â 
solving	 Â this	 Â problem	 Â lies	 Â in	 Â that	 Â reliable	 Â sales	 Â forecasts	 Â enables	 Â store	 Â managers	 Â to	 Â create	 Â effective	 Â staff	 Â schedules	 Â that	 Â 
increase	 Â productivity	 Â and	 Â motivation.	 Â Whatâ€™s	 Â more,	 Â for	 Â the	 Â purpose	 Â of	 Â practicing	 Â what	 Â we	 Â learnt	 Â from	 Â the	 Â Machine	 Â 
Learning	 Â class,	 Â this	 Â problem	 Â saves	 Â us	 Â the	 Â trouble	 Â of	 Â collecting	 Â data,	 Â and	 Â in	 Â the	 Â meanwhile	 Â provides	 Â a	 Â perfect	 Â real	 Â case	 Â to	 Â 
apply	 Â supervised	 Â learning	 Â algorithms.	 Â 
As	 Â a	 Â matter	 Â of	 Â fact,	 Â substantial	 Â effort	 Â has	 Â been	 Â put	 Â into	 Â sales	 Â prediction	 Â problems.	 Â Due	 Â to	 Â promising	 Â performance,	 Â 
	 Â 
artificial	 Â neural	 Â networks	 Â (ANNs)	 Â have	 Â been	 Â applied	 Â for	 Â sales	 Â forecasting	 Â in	 Â many	 Â scenarios.	 Â Thiesing,	 Â F.M.	 Â implemented	 Â a	 Â 
neural	 Â network	 Â forecasting	 Â system	 Â as	 Â a	 Â prototype	 Â to	 Â determine	 Â the	 Â expected	 Â sale	 Â figures[1].	 Â Whatâ€™s	 Â more,	 Â R.J.	 Â Kuo	 Â utilized	 Â 
a	 Â fuzzy	 Â neural	 Â network	 Â with	 Â initial	 Â weights	 Â generated	 Â by	 Â genetic	 Â algorithm	 Â (GFNN)	 Â and	 Â further	 Â integrated	 Â GFNN	 Â with	 Â ANN	 Â 
forecast	 Â using	 Â the	 Â time	 Â series	 Â data	 Â and	 Â promotion	 Â length[2].	 Â This	 Â is	 Â closely	 Â related	 Â to	 Â our	 Â problem	 Â because	 Â promotion	 Â has	 Â 
proved	 Â to	 Â be	 Â one	 Â of	 Â the	 Â most	 Â important	 Â features	 Â in	 Â our	 Â dataset.	 Â There	 Â are	 Â some	 Â interesting	 Â attempts	 Â too.	 Â For	 Â example,	 Â 
Xiaohui	 Â Yu	 Â tried	 Â to	 Â predict	 Â sales	 Â of	 Â products	 Â based	 Â on	 Â online	 Â reviews[3],	 Â and	 Â Michael	 Â Giering	 Â tried	 Â to	 Â correlate	 Â sales	 Â with	 Â 
customer	 Â demographics[4].	 Â As	 Â for	 Â beginners	 Â to	 Â get	 Â started	 Â with	 Â sales	 Â prediction	 Â problem,	 Â Smola	 Â described	 Â a	 Â regression	 Â 
technique	 Â similar	 Â to	 Â SVM	 Â called	 Â Support	 Â Vector	 Â Regression	 Â (SVR)[5].	 Â Breiman	 Â posed	 Â Random	 Â Forest	 Â algorithm[6]	 Â which	 Â is	 Â 
based	 Â on	 Â decision	 Â trees,	 Â but	 Â randomness	 Â is	 Â added.	 Â It	 Â performs	 Â very	 Â well	 Â compared	 Â to	 Â many	 Â other	 Â algorithms,	 Â including	 Â 
neural	 Â networks,	 Â discriminant	 Â analysis	 Â etc.	 Â and	 Â is	 Â robust	 Â against	 Â overfitting.	 Â SVR	 Â and	 Â Random	 Â Forest	 Â are	 Â both	 Â 
implemented	 Â in	 Â out	 Â project.	 Â 
The	 Â dataset	 Â of	 Â this	 Â problem	 Â can	 Â be	 Â found	 Â online2.	 Â The	 Â data	 Â comes	 Â in	 Â two	 Â sets	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â link	 Â to	 Â this	 Â problem:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales	 Â 
	 Â 
2	 Â The	 Â link	 Â to	 Â dataset:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales/data	 Â 	 Â 

3.	 Â DATASET	 Â AND	 Â FEATURES	 Â 

Drug	 Â Store	 Â Sales	 Â Prediction	 Â 

Chenghao	 Â Wang,	 Â Yang	 Â Li	 Â 

1.	 Â INTRODUCTION	 Â 

2.	 Â RELATED	 Â WORK	 Â 	 Â 

Abstract	 Â -Â­â€	 Â In	 Â this	 Â paper	 Â we	 Â tried	 Â to	 Â apply	 Â machine	 Â learning	 Â algorithm	 Â into	 Â a	 Â real	 Â world	 Â problem	 Â â€“	 Â drug	 Â store	 Â sales	 Â 
forecasting.	 Â Given	 Â store	 Â information,	 Â and	 Â sales	 Â record	 Â we	 Â applied	 Â Linear	 Â Regression,	 Â Support	 Â Vector	 Â Regression(SVR)	 Â with	 Â 
Gaussian	 Â and	 Â Polynomial	 Â Kernels	 Â and	 Â Random	 Â Forest	 Â algorithm,	 Â and	 Â tried	 Â to	 Â predict	 Â sales	 Â for	 Â 1-Â­â€3	 Â weeks.	 Â Root	 Â Mean	 Â 
Square	 Â Percentage	 Â Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â the	 Â accuracy.	 Â As	 Â it	 Â turned	 Â out,	 Â Random	 Â Forest	 Â outshined	 Â all	 Â other	 Â 
models	 Â and	 Â reached	 Â RMSPE	 Â of	 Â 12.3%,	 Â which	 Â is	 Â a	 Â reliable	 Â forecast	 Â that	 Â enables	 Â store	 Â managers	 Â allocate	 Â staff	 Â and	 Â stock	 Â up	 Â 
effectively.	 Â  	 Â 
This	 Â problem	 Â is	 Â one	 Â of	 Â several	 Â Machine	 Â Learning	 Â problems	 Â on	 Â Kaggle1	 Â .	 Â The	 Â aim	 Â of	 Â this	 Â problem	 Â is	 Â to	 Â forecast	 Â future	 Â 
sales	 Â of	 Â 1,115	 Â Rossman	 Â drug	 Â stores	 Â located	 Â across	 Â Germany	 Â based	 Â on	 Â their	 Â historical	 Â sales	 Â data.	 Â The	 Â practical	 Â meaning	 Â of	 Â 
solving	 Â this	 Â problem	 Â lies	 Â in	 Â that	 Â reliable	 Â sales	 Â forecasts	 Â enables	 Â store	 Â managers	 Â to	 Â create	 Â effective	 Â staff	 Â schedules	 Â that	 Â 
increase	 Â productivity	 Â and	 Â motivation.	 Â Whatâ€™s	 Â more,	 Â for	 Â the	 Â purpose	 Â of	 Â practicing	 Â what	 Â we	 Â learnt	 Â from	 Â the	 Â Machine	 Â 
Learning	 Â class,	 Â this	 Â problem	 Â saves	 Â us	 Â the	 Â trouble	 Â of	 Â collecting	 Â data,	 Â and	 Â in	 Â the	 Â meanwhile	 Â provides	 Â a	 Â perfect	 Â real	 Â case	 Â to	 Â 
apply	 Â supervised	 Â learning	 Â algorithms.	 Â 
As	 Â a	 Â matter	 Â of	 Â fact,	 Â substantial	 Â effort	 Â has	 Â been	 Â put	 Â into	 Â sales	 Â prediction	 Â problems.	 Â Due	 Â to	 Â promising	 Â performance,	 Â 
	 Â 
artificial	 Â neural	 Â networks	 Â (ANNs)	 Â have	 Â been	 Â applied	 Â for	 Â sales	 Â forecasting	 Â in	 Â many	 Â scenarios.	 Â Thiesing,	 Â F.M.	 Â implemented	 Â a	 Â 
neural	 Â network	 Â forecasting	 Â system	 Â as	 Â a	 Â prototype	 Â to	 Â determine	 Â the	 Â expected	 Â sale	 Â figures[1].	 Â Whatâ€™s	 Â more,	 Â R.J.	 Â Kuo	 Â utilized	 Â 
a	 Â fuzzy	 Â neural	 Â network	 Â with	 Â initial	 Â weights	 Â generated	 Â by	 Â genetic	 Â algorithm	 Â (GFNN)	 Â and	 Â further	 Â integrated	 Â GFNN	 Â with	 Â ANN	 Â 
forecast	 Â using	 Â the	 Â time	 Â series	 Â data	 Â and	 Â promotion	 Â length[2].	 Â This	 Â is	 Â closely	 Â related	 Â to	 Â our	 Â problem	 Â because	 Â promotion	 Â has	 Â 
proved	 Â to	 Â be	 Â one	 Â of	 Â the	 Â most	 Â important	 Â features	 Â in	 Â our	 Â dataset.	 Â There	 Â are	 Â some	 Â interesting	 Â attempts	 Â too.	 Â For	 Â example,	 Â 
Xiaohui	 Â Yu	 Â tried	 Â to	 Â predict	 Â sales	 Â of	 Â products	 Â based	 Â on	 Â online	 Â reviews[3],	 Â and	 Â Michael	 Â Giering	 Â tried	 Â to	 Â correlate	 Â sales	 Â with	 Â 
customer	 Â demographics[4].	 Â As	 Â for	 Â beginners	 Â to	 Â get	 Â started	 Â with	 Â sales	 Â prediction	 Â problem,	 Â Smola	 Â described	 Â a	 Â regression	 Â 
technique	 Â similar	 Â to	 Â SVM	 Â called	 Â Support	 Â Vector	 Â Regression	 Â (SVR)[5].	 Â Breiman	 Â posed	 Â Random	 Â Forest	 Â algorithm[6]	 Â which	 Â is	 Â 
based	 Â on	 Â decision	 Â trees,	 Â but	 Â randomness	 Â is	 Â added.	 Â It	 Â performs	 Â very	 Â well	 Â compared	 Â to	 Â many	 Â other	 Â algorithms,	 Â including	 Â 
neural	 Â networks,	 Â discriminant	 Â analysis	 Â etc.	 Â and	 Â is	 Â robust	 Â against	 Â overfitting.	 Â SVR	 Â and	 Â Random	 Â Forest	 Â are	 Â both	 Â 
implemented	 Â in	 Â out	 Â project.	 Â 
The	 Â dataset	 Â of	 Â this	 Â problem	 Â can	 Â be	 Â found	 Â online2.	 Â The	 Â data	 Â comes	 Â in	 Â two	 Â sets	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â link	 Â to	 Â this	 Â problem:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales	 Â 
	 Â 
2	 Â The	 Â link	 Â to	 Â dataset:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales/data	 Â 	 Â 

3.	 Â DATASET	 Â AND	 Â FEATURES	 Â 

!!

!!!!

4.	 Â METHODS	 Â 

1.	 Â Sales	 Â Dataset	 Â -Â­â€	 Â Historical	 Â sales	 Â data	 Â for	 Â 1,115	 Â Rossman	 Â stores	 Â from	 Â 2013/1/1	 Â to	 Â 2015/7/31.	 Â Features	 Â 
include	 Â store	 Â number,	 Â date,	 Â day	 Â of	 Â week,	 Â whether	 Â thereâ€™s	 Â a	 Â promotion,	 Â whether	 Â itâ€™s	 Â a	 Â school	 Â or	 Â state	 Â holiday	 Â and	 Â sales	 Â on	 Â 
that	 Â day.	 Â 
2.	 Â Store	 Â Dataset	 Â -Â­â€	 Â Storesâ€™	 Â individual	 Â characteristics.	 Â Features	 Â include	 Â store	 Â type,	 Â assortment	 Â level,	 Â nearest	 Â 
competitorâ€™s	 Â distance	 Â and	 Â when	 Â the	 Â competitor	 Â was	 Â opened,	 Â and	 Â whether	 Â thereâ€™s	 Â a	 Â consecutive	 Â promotion.	 Â 
Throughout	 Â our	 Â trial,	 Â weâ€™ve	 Â tried	 Â to	 Â take	 Â advantage	 Â of	 Â different	 Â subset	 Â of	 Â features.	 Â However,	 Â reducing	 Â number	 Â of	 Â 
features	 Â didnâ€™t	 Â increase	 Â accuracy	 Â for	 Â this	 Â problem.	 Â So	 Â all	 Â features	 Â are	 Â used	 Â for	 Â building	 Â models.	 Â 
70%/30%	 Â and	 Â k-Â­â€fold	 Â cross	 Â validations	 Â are	 Â used	 Â in	 Â this	 Â problem	 Â for	 Â training	 Â and	 Â testing.	 Â Root	 Â Mean	 Â Square	 Â Percentage	 Â 
	 Â 
(!!!!!!! )!
Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â accuracy,	 Â which	 Â is	 Â defined	 Â as: Â ğ‘…ğ‘€ğ‘†ğ‘ƒğ¸ Â = Â 
There	 Â are	 Â two	 Â methods	 Â to	 Â train	 Â the	 Â data.	 Â One	 Â is	 Â to	 Â train	 Â each	 Â store	 Â separately,	 Â which	 Â means	 Â forecasting	 Â sales	 Â of	 Â a	 Â 
single	 Â store	 Â based	 Â on	 Â its	 Â own	 Â sales	 Â record,	 Â regardless	 Â of	 Â store	 Â attributes.	 Â The	 Â other	 Â one	 Â is	 Â to	 Â train	 Â all	 Â stores	 Â together,	 Â 
considering	 Â store	 Â attributes	 Â as	 Â parameters.	 Â 
To	 Â train	 Â each	 Â store	 Â separately,	 Â one	 Â straightforward	 Â idea	 Â is	 Â to	 Â apply	 Â linear	 Â regression.	 Â According	 Â to	 Â the	 Â normal	 Â 
equation,	 Â ğœƒ Â = Â (ğ‘‹!ğ‘‹)!!ğ‘‹!ğ‘¦,	 Â we	 Â can	 Â easily	 Â predict	 Â sales	 Â by	 Â ğ»!(ğ‘¥)= Â ğœƒ!ğ‘¥	 Â 
Further	 Â more,	 Â we	 Â figured	 Â that	 Â this	 Â problem	 Â can	 Â actually	 Â be	 Â kernelized.	 Â Here	 Â consider	 Â that	 Â case	 Â of	 Â applying	 Â MAP	 Â 
estimate	 Â for	 Â ğœƒ	 Â to	 Â avoid	 Â overfitting,	 Â which	 Â results	 Â in	 Â the	 Â following	 Â primal	 Â problem	 Â 
ğœƒ= Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›||ğ‘¦âˆ’ğœƒ!ğ‘‹|| Â + Â ğœ†||ğœƒ||!	 Â 
ğ›¼!!!!! <ğ‘¥,ğ‘¥(!)>,	 Â we	 Â can	 Â see	 Â that	 Â this	 Â problem	 Â 
	 Â If	 Â we	 Â calculate	 Â ğ›¼	 Â as	 Â ğ›¼ Â = Â (<ğ‘‹,ğ‘‹>+ğœ†ğ¼)!!ğ‘¦.	 Â And	 Â define	 Â ğ»(ğ‘¥)=
can	 Â actually	 Â be	 Â kernalized,	 Â thus	 Â we	 Â can	 Â apply	 Â the	 Â kernel	 Â trick.	 Â We	 Â tried	 Â Gaussian	 Â Kernel	 Â and	 Â Polynomial	 Â Kernel	 Â in	 Â this	 Â case,	 Â 
which	 Â is	 Â illustrated	 Â as	 Â following.	 Â 
(a)	 Â Gaussian	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =exp âˆ’||!!!||!
!!!
(b)	 Â Polynomial	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =(ğ‘¥âˆ—ğ‘§+1)! Â 	 Â (Polynomials	 Â of	 Â degree	 Â up	 Â to	 Â d)	 Â 
Our	 Â next	 Â model	 Â for	 Â this	 Â project	 Â is	 Â Random	 Â Forest	 Â Regression.	 Â We	 Â tried	 Â this	 Â model	 Â because	 Â itâ€™s	 Â fast	 Â and	 Â can	 Â 
accommodate	 Â categorical	 Â data.	 Â RF	 Â first	 Â picked	 Â a	 Â certain	 Â amount	 Â of	 Â data	 Â from	 Â the	 Â dataset	 Â randomly	 Â (ie.	 Â bootstrap)	 Â and	 Â then	 Â 
picked	 Â a	 Â certain	 Â amount	 Â of	 Â features	 Â out	 Â of	 Â the	 Â total	 Â features	 Â randomly	 Â to	 Â build	 Â decision	 Â trees.	 Â The	 Â final	 Â result	 Â for	 Â each	 Â test	 Â 
data	 Â is	 Â average	 Â of	 Â results	 Â obtained	 Â by	 Â all	 Â these	 Â decision	 Â trees.	 Â Decision	 Â trees	 Â usually	 Â overfit	 Â the	 Â data;	 Â however	 Â randomness	 Â 
will	 Â average	 Â out	 Â the	 Â high	 Â variance	 Â 
Linear	 Â regression	 Â is	 Â used	 Â as	 Â our	 Â baseline	 Â model.	 Â 70%/30%	 Â cross	 Â validation	 Â is	 Â used	 Â here	 Â to	 Â divide	 Â the	 Â data	 Â set	 Â into	 Â 
training	 Â set	 Â and	 Â test	 Â set.	 Â As	 Â it	 Â turns	 Â out,	 Â linear	 Â regression	 Â gives	 Â us	 Â a	 Â RMSPE	 Â of	 Â 52.8%.	 Â 
	 Â 

5.	 Â EXPERIMENTS	 Â AND	 Â RESULTS	 Â 
Linear	 Â Regression	 Â 

Support	 Â Vector	 Â Regression	 Â 

	 Â 

Drug	 Â Store	 Â Sales	 Â Prediction	 Â 

Chenghao	 Â Wang,	 Â Yang	 Â Li	 Â 

1.	 Â INTRODUCTION	 Â 

2.	 Â RELATED	 Â WORK	 Â 	 Â 

Abstract	 Â -Â­â€	 Â In	 Â this	 Â paper	 Â we	 Â tried	 Â to	 Â apply	 Â machine	 Â learning	 Â algorithm	 Â into	 Â a	 Â real	 Â world	 Â problem	 Â â€“	 Â drug	 Â store	 Â sales	 Â 
forecasting.	 Â Given	 Â store	 Â information,	 Â and	 Â sales	 Â record	 Â we	 Â applied	 Â Linear	 Â Regression,	 Â Support	 Â Vector	 Â Regression(SVR)	 Â with	 Â 
Gaussian	 Â and	 Â Polynomial	 Â Kernels	 Â and	 Â Random	 Â Forest	 Â algorithm,	 Â and	 Â tried	 Â to	 Â predict	 Â sales	 Â for	 Â 1-Â­â€3	 Â weeks.	 Â Root	 Â Mean	 Â 
Square	 Â Percentage	 Â Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â the	 Â accuracy.	 Â As	 Â it	 Â turned	 Â out,	 Â Random	 Â Forest	 Â outshined	 Â all	 Â other	 Â 
models	 Â and	 Â reached	 Â RMSPE	 Â of	 Â 12.3%,	 Â which	 Â is	 Â a	 Â reliable	 Â forecast	 Â that	 Â enables	 Â store	 Â managers	 Â allocate	 Â staff	 Â and	 Â stock	 Â up	 Â 
effectively.	 Â  	 Â 
This	 Â problem	 Â is	 Â one	 Â of	 Â several	 Â Machine	 Â Learning	 Â problems	 Â on	 Â Kaggle1	 Â .	 Â The	 Â aim	 Â of	 Â this	 Â problem	 Â is	 Â to	 Â forecast	 Â future	 Â 
sales	 Â of	 Â 1,115	 Â Rossman	 Â drug	 Â stores	 Â located	 Â across	 Â Germany	 Â based	 Â on	 Â their	 Â historical	 Â sales	 Â data.	 Â The	 Â practical	 Â meaning	 Â of	 Â 
solving	 Â this	 Â problem	 Â lies	 Â in	 Â that	 Â reliable	 Â sales	 Â forecasts	 Â enables	 Â store	 Â managers	 Â to	 Â create	 Â effective	 Â staff	 Â schedules	 Â that	 Â 
increase	 Â productivity	 Â and	 Â motivation.	 Â Whatâ€™s	 Â more,	 Â for	 Â the	 Â purpose	 Â of	 Â practicing	 Â what	 Â we	 Â learnt	 Â from	 Â the	 Â Machine	 Â 
Learning	 Â class,	 Â this	 Â problem	 Â saves	 Â us	 Â the	 Â trouble	 Â of	 Â collecting	 Â data,	 Â and	 Â in	 Â the	 Â meanwhile	 Â provides	 Â a	 Â perfect	 Â real	 Â case	 Â to	 Â 
apply	 Â supervised	 Â learning	 Â algorithms.	 Â 
As	 Â a	 Â matter	 Â of	 Â fact,	 Â substantial	 Â effort	 Â has	 Â been	 Â put	 Â into	 Â sales	 Â prediction	 Â problems.	 Â Due	 Â to	 Â promising	 Â performance,	 Â 
	 Â 
artificial	 Â neural	 Â networks	 Â (ANNs)	 Â have	 Â been	 Â applied	 Â for	 Â sales	 Â forecasting	 Â in	 Â many	 Â scenarios.	 Â Thiesing,	 Â F.M.	 Â implemented	 Â a	 Â 
neural	 Â network	 Â forecasting	 Â system	 Â as	 Â a	 Â prototype	 Â to	 Â determine	 Â the	 Â expected	 Â sale	 Â figures[1].	 Â Whatâ€™s	 Â more,	 Â R.J.	 Â Kuo	 Â utilized	 Â 
a	 Â fuzzy	 Â neural	 Â network	 Â with	 Â initial	 Â weights	 Â generated	 Â by	 Â genetic	 Â algorithm	 Â (GFNN)	 Â and	 Â further	 Â integrated	 Â GFNN	 Â with	 Â ANN	 Â 
forecast	 Â using	 Â the	 Â time	 Â series	 Â data	 Â and	 Â promotion	 Â length[2].	 Â This	 Â is	 Â closely	 Â related	 Â to	 Â our	 Â problem	 Â because	 Â promotion	 Â has	 Â 
proved	 Â to	 Â be	 Â one	 Â of	 Â the	 Â most	 Â important	 Â features	 Â in	 Â our	 Â dataset.	 Â There	 Â are	 Â some	 Â interesting	 Â attempts	 Â too.	 Â For	 Â example,	 Â 
Xiaohui	 Â Yu	 Â tried	 Â to	 Â predict	 Â sales	 Â of	 Â products	 Â based	 Â on	 Â online	 Â reviews[3],	 Â and	 Â Michael	 Â Giering	 Â tried	 Â to	 Â correlate	 Â sales	 Â with	 Â 
customer	 Â demographics[4].	 Â As	 Â for	 Â beginners	 Â to	 Â get	 Â started	 Â with	 Â sales	 Â prediction	 Â problem,	 Â Smola	 Â described	 Â a	 Â regression	 Â 
technique	 Â similar	 Â to	 Â SVM	 Â called	 Â Support	 Â Vector	 Â Regression	 Â (SVR)[5].	 Â Breiman	 Â posed	 Â Random	 Â Forest	 Â algorithm[6]	 Â which	 Â is	 Â 
based	 Â on	 Â decision	 Â trees,	 Â but	 Â randomness	 Â is	 Â added.	 Â It	 Â performs	 Â very	 Â well	 Â compared	 Â to	 Â many	 Â other	 Â algorithms,	 Â including	 Â 
neural	 Â networks,	 Â discriminant	 Â analysis	 Â etc.	 Â and	 Â is	 Â robust	 Â against	 Â overfitting.	 Â SVR	 Â and	 Â Random	 Â Forest	 Â are	 Â both	 Â 
implemented	 Â in	 Â out	 Â project.	 Â 
The	 Â dataset	 Â of	 Â this	 Â problem	 Â can	 Â be	 Â found	 Â online2.	 Â The	 Â data	 Â comes	 Â in	 Â two	 Â sets	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â link	 Â to	 Â this	 Â problem:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales	 Â 
	 Â 
2	 Â The	 Â link	 Â to	 Â dataset:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales/data	 Â 	 Â 

3.	 Â DATASET	 Â AND	 Â FEATURES	 Â 

!!

!!!!

4.	 Â METHODS	 Â 

1.	 Â Sales	 Â Dataset	 Â -Â­â€	 Â Historical	 Â sales	 Â data	 Â for	 Â 1,115	 Â Rossman	 Â stores	 Â from	 Â 2013/1/1	 Â to	 Â 2015/7/31.	 Â Features	 Â 
include	 Â store	 Â number,	 Â date,	 Â day	 Â of	 Â week,	 Â whether	 Â thereâ€™s	 Â a	 Â promotion,	 Â whether	 Â itâ€™s	 Â a	 Â school	 Â or	 Â state	 Â holiday	 Â and	 Â sales	 Â on	 Â 
that	 Â day.	 Â 
2.	 Â Store	 Â Dataset	 Â -Â­â€	 Â Storesâ€™	 Â individual	 Â characteristics.	 Â Features	 Â include	 Â store	 Â type,	 Â assortment	 Â level,	 Â nearest	 Â 
competitorâ€™s	 Â distance	 Â and	 Â when	 Â the	 Â competitor	 Â was	 Â opened,	 Â and	 Â whether	 Â thereâ€™s	 Â a	 Â consecutive	 Â promotion.	 Â 
Throughout	 Â our	 Â trial,	 Â weâ€™ve	 Â tried	 Â to	 Â take	 Â advantage	 Â of	 Â different	 Â subset	 Â of	 Â features.	 Â However,	 Â reducing	 Â number	 Â of	 Â 
features	 Â didnâ€™t	 Â increase	 Â accuracy	 Â for	 Â this	 Â problem.	 Â So	 Â all	 Â features	 Â are	 Â used	 Â for	 Â building	 Â models.	 Â 
70%/30%	 Â and	 Â k-Â­â€fold	 Â cross	 Â validations	 Â are	 Â used	 Â in	 Â this	 Â problem	 Â for	 Â training	 Â and	 Â testing.	 Â Root	 Â Mean	 Â Square	 Â Percentage	 Â 
	 Â 
(!!!!!!! )!
Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â accuracy,	 Â which	 Â is	 Â defined	 Â as: Â ğ‘…ğ‘€ğ‘†ğ‘ƒğ¸ Â = Â 
There	 Â are	 Â two	 Â methods	 Â to	 Â train	 Â the	 Â data.	 Â One	 Â is	 Â to	 Â train	 Â each	 Â store	 Â separately,	 Â which	 Â means	 Â forecasting	 Â sales	 Â of	 Â a	 Â 
single	 Â store	 Â based	 Â on	 Â its	 Â own	 Â sales	 Â record,	 Â regardless	 Â of	 Â store	 Â attributes.	 Â The	 Â other	 Â one	 Â is	 Â to	 Â train	 Â all	 Â stores	 Â together,	 Â 
considering	 Â store	 Â attributes	 Â as	 Â parameters.	 Â 
To	 Â train	 Â each	 Â store	 Â separately,	 Â one	 Â straightforward	 Â idea	 Â is	 Â to	 Â apply	 Â linear	 Â regression.	 Â According	 Â to	 Â the	 Â normal	 Â 
equation,	 Â ğœƒ Â = Â (ğ‘‹!ğ‘‹)!!ğ‘‹!ğ‘¦,	 Â we	 Â can	 Â easily	 Â predict	 Â sales	 Â by	 Â ğ»!(ğ‘¥)= Â ğœƒ!ğ‘¥	 Â 
Further	 Â more,	 Â we	 Â figured	 Â that	 Â this	 Â problem	 Â can	 Â actually	 Â be	 Â kernelized.	 Â Here	 Â consider	 Â that	 Â case	 Â of	 Â applying	 Â MAP	 Â 
estimate	 Â for	 Â ğœƒ	 Â to	 Â avoid	 Â overfitting,	 Â which	 Â results	 Â in	 Â the	 Â following	 Â primal	 Â problem	 Â 
ğœƒ= Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›||ğ‘¦âˆ’ğœƒ!ğ‘‹|| Â + Â ğœ†||ğœƒ||!	 Â 
ğ›¼!!!!! <ğ‘¥,ğ‘¥(!)>,	 Â we	 Â can	 Â see	 Â that	 Â this	 Â problem	 Â 
	 Â If	 Â we	 Â calculate	 Â ğ›¼	 Â as	 Â ğ›¼ Â = Â (<ğ‘‹,ğ‘‹>+ğœ†ğ¼)!!ğ‘¦.	 Â And	 Â define	 Â ğ»(ğ‘¥)=
can	 Â actually	 Â be	 Â kernalized,	 Â thus	 Â we	 Â can	 Â apply	 Â the	 Â kernel	 Â trick.	 Â We	 Â tried	 Â Gaussian	 Â Kernel	 Â and	 Â Polynomial	 Â Kernel	 Â in	 Â this	 Â case,	 Â 
which	 Â is	 Â illustrated	 Â as	 Â following.	 Â 
(a)	 Â Gaussian	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =exp âˆ’||!!!||!
!!!
(b)	 Â Polynomial	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =(ğ‘¥âˆ—ğ‘§+1)! Â 	 Â (Polynomials	 Â of	 Â degree	 Â up	 Â to	 Â d)	 Â 
Our	 Â next	 Â model	 Â for	 Â this	 Â project	 Â is	 Â Random	 Â Forest	 Â Regression.	 Â We	 Â tried	 Â this	 Â model	 Â because	 Â itâ€™s	 Â fast	 Â and	 Â can	 Â 
accommodate	 Â categorical	 Â data.	 Â RF	 Â first	 Â picked	 Â a	 Â certain	 Â amount	 Â of	 Â data	 Â from	 Â the	 Â dataset	 Â randomly	 Â (ie.	 Â bootstrap)	 Â and	 Â then	 Â 
picked	 Â a	 Â certain	 Â amount	 Â of	 Â features	 Â out	 Â of	 Â the	 Â total	 Â features	 Â randomly	 Â to	 Â build	 Â decision	 Â trees.	 Â The	 Â final	 Â result	 Â for	 Â each	 Â test	 Â 
data	 Â is	 Â average	 Â of	 Â results	 Â obtained	 Â by	 Â all	 Â these	 Â decision	 Â trees.	 Â Decision	 Â trees	 Â usually	 Â overfit	 Â the	 Â data;	 Â however	 Â randomness	 Â 
will	 Â average	 Â out	 Â the	 Â high	 Â variance	 Â 
Linear	 Â regression	 Â is	 Â used	 Â as	 Â our	 Â baseline	 Â model.	 Â 70%/30%	 Â cross	 Â validation	 Â is	 Â used	 Â here	 Â to	 Â divide	 Â the	 Â data	 Â set	 Â into	 Â 
training	 Â set	 Â and	 Â test	 Â set.	 Â As	 Â it	 Â turns	 Â out,	 Â linear	 Â regression	 Â gives	 Â us	 Â a	 Â RMSPE	 Â of	 Â 52.8%.	 Â 
	 Â 

5.	 Â EXPERIMENTS	 Â AND	 Â RESULTS	 Â 
Linear	 Â Regression	 Â 

Support	 Â Vector	 Â Regression	 Â 

	 Â 

	 Â One	 Â thing	 Â special	 Â on	 Â the	 Â implementation	 Â of	 Â SVR	 Â is	 Â that,	 Â it	 Â need	 Â to	 Â build	 Â an	 Â m*m	 Â matrix,	 Â where	 Â m	 Â indicates	 Â the	 Â number	 Â 
of	 Â training	 Â samples.	 Â Since	 Â the	 Â size	 Â of	 Â our	 Â training	 Â set	 Â is	 Â ~700,000	 Â ,	 Â itâ€™s	 Â unrealistic	 Â to	 Â operate	 Â on	 Â the	 Â whole	 Â dataset.	 Â To	 Â take	 Â 
use	 Â of	 Â the	 Â abundant	 Â dataset	 Â practically,	 Â we	 Â build	 Â a	 Â SVR	 Â model	 Â for	 Â each	 Â store,	 Â and	 Â compute	 Â the	 Â mean	 Â of	 Â each	 Â storeâ€™s	 Â RMSPE	 Â 
as	 Â our	 Â final	 Â error	 Â rate.	 Â 	 Â 
Firstly,	 Â we	 Â applied	 Â Polynomial	 Â Kernel	 Â and	 Â Gaussian	 Â Kernel	 Â for	 Â a	 Â single	 Â store,	 Â Store	 Â 1.	 Â By	 Â trying	 Â different	 Â pairs	 Â of	 Â 
ğœ† Â & Â ğœ Â for	 Â Gaussian	 Â Kernel,	 Â and	 Â different	 Â pairs	 Â of	 Â ğœ† Â & Â ğ‘‘ Â for	 Â Polynomial	 Â Kernel,	 Â we	 Â found	 Â that	 Â when	 Â ğœ† Â  Â = Â 140,ğœ Â =
 Â 45, Â Gaussian	 Â Kernel	 Â gives	 Â the	 Â best	 Â RMSPE	 Â of	 Â 13.6%,	 Â when	 Â ğœ† Â  Â = Â 0.1,ğ‘‘ Â = Â 2	 Â Polynomial	 Â Kernel	 Â gives	 Â the	 Â best	 Â RMSPE	 Â of	 Â 
12.8%.	 Â Two	 Â kernels	 Â are	 Â comparable	 Â in	 Â this	 Â scenario. Â 
Secondly,	 Â using	 Â the	 Â method	 Â of	 Â finding	 Â optimal	 Â pairs	 Â of	 Â parameters	 Â discussed	 Â above,	 Â we	 Â applied	 Â Gaussian	 Â Kernel	 Â and	 Â 
Polynomial	 Â Kernel	 Â to	 Â all	 Â stores.	 Â As	 Â we	 Â dig	 Â deeper	 Â into	 Â the	 Â dataset,	 Â we	 Â found	 Â that	 Â accuracies	 Â vary	 Â on	 Â different	 Â time	 Â period	 Â 
of	 Â prediction.	 Â Below	 Â are	 Â figures	 Â of	 Â how	 Â RMSPE	 Â varies	 Â with	 Â different	 Â time	 Â period	 Â for	 Â prediction.	 Â 	 Â 

(a)	 Â Gaussian	 Â Kernel	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â (b)	 Â Polynomial	 Â Kernel	 Â 
Figure	 Â 1.	 Â How	 Â RMSPE	 Â Varies	 Â with	 Â Different	 Â Time	 Â Period	 Â to	 Â Predict	 Â 

	 Â 
As	 Â shown	 Â above,	 Â RMSPEs	 Â for	 Â both	 Â kernels	 Â first	 Â increase	 Â and	 Â then	 Â decrease	 Â as	 Â time	 Â period	 Â of	 Â prediction	 Â gets	 Â larger.	 Â 
For	 Â Gaussian	 Â Kernel,	 Â RMSPE	 Â reaches	 Â minimum	 Â when	 Â predicting	 Â for	 Â just	 Â one	 Â week,	 Â however,	 Â for	 Â Polynomial	 Â Kernel,	 Â RMSPE	 Â 
reaches	 Â minimum	 Â when	 Â predicting	 Â for	 Â 3	 Â weeks.	 Â Given	 Â this	 Â result,	 Â we	 Â draw	 Â figures	 Â of	 Â RMSPE	 Â for	 Â all	 Â stores	 Â using	 Â Gaussian	 Â 
Kernel	 Â and	 Â Polynomial	 Â Kernel,	 Â predicting	 Â for	 Â 1	 Â week	 Â and	 Â 3	 Â weeks	 Â respectively.	 Â 	 Â 

	 Â 

	 Â 

Drug	 Â Store	 Â Sales	 Â Prediction	 Â 

Chenghao	 Â Wang,	 Â Yang	 Â Li	 Â 

1.	 Â INTRODUCTION	 Â 

2.	 Â RELATED	 Â WORK	 Â 	 Â 

Abstract	 Â -Â­â€	 Â In	 Â this	 Â paper	 Â we	 Â tried	 Â to	 Â apply	 Â machine	 Â learning	 Â algorithm	 Â into	 Â a	 Â real	 Â world	 Â problem	 Â â€“	 Â drug	 Â store	 Â sales	 Â 
forecasting.	 Â Given	 Â store	 Â information,	 Â and	 Â sales	 Â record	 Â we	 Â applied	 Â Linear	 Â Regression,	 Â Support	 Â Vector	 Â Regression(SVR)	 Â with	 Â 
Gaussian	 Â and	 Â Polynomial	 Â Kernels	 Â and	 Â Random	 Â Forest	 Â algorithm,	 Â and	 Â tried	 Â to	 Â predict	 Â sales	 Â for	 Â 1-Â­â€3	 Â weeks.	 Â Root	 Â Mean	 Â 
Square	 Â Percentage	 Â Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â the	 Â accuracy.	 Â As	 Â it	 Â turned	 Â out,	 Â Random	 Â Forest	 Â outshined	 Â all	 Â other	 Â 
models	 Â and	 Â reached	 Â RMSPE	 Â of	 Â 12.3%,	 Â which	 Â is	 Â a	 Â reliable	 Â forecast	 Â that	 Â enables	 Â store	 Â managers	 Â allocate	 Â staff	 Â and	 Â stock	 Â up	 Â 
effectively.	 Â  	 Â 
This	 Â problem	 Â is	 Â one	 Â of	 Â several	 Â Machine	 Â Learning	 Â problems	 Â on	 Â Kaggle1	 Â .	 Â The	 Â aim	 Â of	 Â this	 Â problem	 Â is	 Â to	 Â forecast	 Â future	 Â 
sales	 Â of	 Â 1,115	 Â Rossman	 Â drug	 Â stores	 Â located	 Â across	 Â Germany	 Â based	 Â on	 Â their	 Â historical	 Â sales	 Â data.	 Â The	 Â practical	 Â meaning	 Â of	 Â 
solving	 Â this	 Â problem	 Â lies	 Â in	 Â that	 Â reliable	 Â sales	 Â forecasts	 Â enables	 Â store	 Â managers	 Â to	 Â create	 Â effective	 Â staff	 Â schedules	 Â that	 Â 
increase	 Â productivity	 Â and	 Â motivation.	 Â Whatâ€™s	 Â more,	 Â for	 Â the	 Â purpose	 Â of	 Â practicing	 Â what	 Â we	 Â learnt	 Â from	 Â the	 Â Machine	 Â 
Learning	 Â class,	 Â this	 Â problem	 Â saves	 Â us	 Â the	 Â trouble	 Â of	 Â collecting	 Â data,	 Â and	 Â in	 Â the	 Â meanwhile	 Â provides	 Â a	 Â perfect	 Â real	 Â case	 Â to	 Â 
apply	 Â supervised	 Â learning	 Â algorithms.	 Â 
As	 Â a	 Â matter	 Â of	 Â fact,	 Â substantial	 Â effort	 Â has	 Â been	 Â put	 Â into	 Â sales	 Â prediction	 Â problems.	 Â Due	 Â to	 Â promising	 Â performance,	 Â 
	 Â 
artificial	 Â neural	 Â networks	 Â (ANNs)	 Â have	 Â been	 Â applied	 Â for	 Â sales	 Â forecasting	 Â in	 Â many	 Â scenarios.	 Â Thiesing,	 Â F.M.	 Â implemented	 Â a	 Â 
neural	 Â network	 Â forecasting	 Â system	 Â as	 Â a	 Â prototype	 Â to	 Â determine	 Â the	 Â expected	 Â sale	 Â figures[1].	 Â Whatâ€™s	 Â more,	 Â R.J.	 Â Kuo	 Â utilized	 Â 
a	 Â fuzzy	 Â neural	 Â network	 Â with	 Â initial	 Â weights	 Â generated	 Â by	 Â genetic	 Â algorithm	 Â (GFNN)	 Â and	 Â further	 Â integrated	 Â GFNN	 Â with	 Â ANN	 Â 
forecast	 Â using	 Â the	 Â time	 Â series	 Â data	 Â and	 Â promotion	 Â length[2].	 Â This	 Â is	 Â closely	 Â related	 Â to	 Â our	 Â problem	 Â because	 Â promotion	 Â has	 Â 
proved	 Â to	 Â be	 Â one	 Â of	 Â the	 Â most	 Â important	 Â features	 Â in	 Â our	 Â dataset.	 Â There	 Â are	 Â some	 Â interesting	 Â attempts	 Â too.	 Â For	 Â example,	 Â 
Xiaohui	 Â Yu	 Â tried	 Â to	 Â predict	 Â sales	 Â of	 Â products	 Â based	 Â on	 Â online	 Â reviews[3],	 Â and	 Â Michael	 Â Giering	 Â tried	 Â to	 Â correlate	 Â sales	 Â with	 Â 
customer	 Â demographics[4].	 Â As	 Â for	 Â beginners	 Â to	 Â get	 Â started	 Â with	 Â sales	 Â prediction	 Â problem,	 Â Smola	 Â described	 Â a	 Â regression	 Â 
technique	 Â similar	 Â to	 Â SVM	 Â called	 Â Support	 Â Vector	 Â Regression	 Â (SVR)[5].	 Â Breiman	 Â posed	 Â Random	 Â Forest	 Â algorithm[6]	 Â which	 Â is	 Â 
based	 Â on	 Â decision	 Â trees,	 Â but	 Â randomness	 Â is	 Â added.	 Â It	 Â performs	 Â very	 Â well	 Â compared	 Â to	 Â many	 Â other	 Â algorithms,	 Â including	 Â 
neural	 Â networks,	 Â discriminant	 Â analysis	 Â etc.	 Â and	 Â is	 Â robust	 Â against	 Â overfitting.	 Â SVR	 Â and	 Â Random	 Â Forest	 Â are	 Â both	 Â 
implemented	 Â in	 Â out	 Â project.	 Â 
The	 Â dataset	 Â of	 Â this	 Â problem	 Â can	 Â be	 Â found	 Â online2.	 Â The	 Â data	 Â comes	 Â in	 Â two	 Â sets	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â link	 Â to	 Â this	 Â problem:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales	 Â 
	 Â 
2	 Â The	 Â link	 Â to	 Â dataset:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales/data	 Â 	 Â 

3.	 Â DATASET	 Â AND	 Â FEATURES	 Â 

!!

!!!!

4.	 Â METHODS	 Â 

1.	 Â Sales	 Â Dataset	 Â -Â­â€	 Â Historical	 Â sales	 Â data	 Â for	 Â 1,115	 Â Rossman	 Â stores	 Â from	 Â 2013/1/1	 Â to	 Â 2015/7/31.	 Â Features	 Â 
include	 Â store	 Â number,	 Â date,	 Â day	 Â of	 Â week,	 Â whether	 Â thereâ€™s	 Â a	 Â promotion,	 Â whether	 Â itâ€™s	 Â a	 Â school	 Â or	 Â state	 Â holiday	 Â and	 Â sales	 Â on	 Â 
that	 Â day.	 Â 
2.	 Â Store	 Â Dataset	 Â -Â­â€	 Â Storesâ€™	 Â individual	 Â characteristics.	 Â Features	 Â include	 Â store	 Â type,	 Â assortment	 Â level,	 Â nearest	 Â 
competitorâ€™s	 Â distance	 Â and	 Â when	 Â the	 Â competitor	 Â was	 Â opened,	 Â and	 Â whether	 Â thereâ€™s	 Â a	 Â consecutive	 Â promotion.	 Â 
Throughout	 Â our	 Â trial,	 Â weâ€™ve	 Â tried	 Â to	 Â take	 Â advantage	 Â of	 Â different	 Â subset	 Â of	 Â features.	 Â However,	 Â reducing	 Â number	 Â of	 Â 
features	 Â didnâ€™t	 Â increase	 Â accuracy	 Â for	 Â this	 Â problem.	 Â So	 Â all	 Â features	 Â are	 Â used	 Â for	 Â building	 Â models.	 Â 
70%/30%	 Â and	 Â k-Â­â€fold	 Â cross	 Â validations	 Â are	 Â used	 Â in	 Â this	 Â problem	 Â for	 Â training	 Â and	 Â testing.	 Â Root	 Â Mean	 Â Square	 Â Percentage	 Â 
	 Â 
(!!!!!!! )!
Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â accuracy,	 Â which	 Â is	 Â defined	 Â as: Â ğ‘…ğ‘€ğ‘†ğ‘ƒğ¸ Â = Â 
There	 Â are	 Â two	 Â methods	 Â to	 Â train	 Â the	 Â data.	 Â One	 Â is	 Â to	 Â train	 Â each	 Â store	 Â separately,	 Â which	 Â means	 Â forecasting	 Â sales	 Â of	 Â a	 Â 
single	 Â store	 Â based	 Â on	 Â its	 Â own	 Â sales	 Â record,	 Â regardless	 Â of	 Â store	 Â attributes.	 Â The	 Â other	 Â one	 Â is	 Â to	 Â train	 Â all	 Â stores	 Â together,	 Â 
considering	 Â store	 Â attributes	 Â as	 Â parameters.	 Â 
To	 Â train	 Â each	 Â store	 Â separately,	 Â one	 Â straightforward	 Â idea	 Â is	 Â to	 Â apply	 Â linear	 Â regression.	 Â According	 Â to	 Â the	 Â normal	 Â 
equation,	 Â ğœƒ Â = Â (ğ‘‹!ğ‘‹)!!ğ‘‹!ğ‘¦,	 Â we	 Â can	 Â easily	 Â predict	 Â sales	 Â by	 Â ğ»!(ğ‘¥)= Â ğœƒ!ğ‘¥	 Â 
Further	 Â more,	 Â we	 Â figured	 Â that	 Â this	 Â problem	 Â can	 Â actually	 Â be	 Â kernelized.	 Â Here	 Â consider	 Â that	 Â case	 Â of	 Â applying	 Â MAP	 Â 
estimate	 Â for	 Â ğœƒ	 Â to	 Â avoid	 Â overfitting,	 Â which	 Â results	 Â in	 Â the	 Â following	 Â primal	 Â problem	 Â 
ğœƒ= Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›||ğ‘¦âˆ’ğœƒ!ğ‘‹|| Â + Â ğœ†||ğœƒ||!	 Â 
ğ›¼!!!!! <ğ‘¥,ğ‘¥(!)>,	 Â we	 Â can	 Â see	 Â that	 Â this	 Â problem	 Â 
	 Â If	 Â we	 Â calculate	 Â ğ›¼	 Â as	 Â ğ›¼ Â = Â (<ğ‘‹,ğ‘‹>+ğœ†ğ¼)!!ğ‘¦.	 Â And	 Â define	 Â ğ»(ğ‘¥)=
can	 Â actually	 Â be	 Â kernalized,	 Â thus	 Â we	 Â can	 Â apply	 Â the	 Â kernel	 Â trick.	 Â We	 Â tried	 Â Gaussian	 Â Kernel	 Â and	 Â Polynomial	 Â Kernel	 Â in	 Â this	 Â case,	 Â 
which	 Â is	 Â illustrated	 Â as	 Â following.	 Â 
(a)	 Â Gaussian	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =exp âˆ’||!!!||!
!!!
(b)	 Â Polynomial	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =(ğ‘¥âˆ—ğ‘§+1)! Â 	 Â (Polynomials	 Â of	 Â degree	 Â up	 Â to	 Â d)	 Â 
Our	 Â next	 Â model	 Â for	 Â this	 Â project	 Â is	 Â Random	 Â Forest	 Â Regression.	 Â We	 Â tried	 Â this	 Â model	 Â because	 Â itâ€™s	 Â fast	 Â and	 Â can	 Â 
accommodate	 Â categorical	 Â data.	 Â RF	 Â first	 Â picked	 Â a	 Â certain	 Â amount	 Â of	 Â data	 Â from	 Â the	 Â dataset	 Â randomly	 Â (ie.	 Â bootstrap)	 Â and	 Â then	 Â 
picked	 Â a	 Â certain	 Â amount	 Â of	 Â features	 Â out	 Â of	 Â the	 Â total	 Â features	 Â randomly	 Â to	 Â build	 Â decision	 Â trees.	 Â The	 Â final	 Â result	 Â for	 Â each	 Â test	 Â 
data	 Â is	 Â average	 Â of	 Â results	 Â obtained	 Â by	 Â all	 Â these	 Â decision	 Â trees.	 Â Decision	 Â trees	 Â usually	 Â overfit	 Â the	 Â data;	 Â however	 Â randomness	 Â 
will	 Â average	 Â out	 Â the	 Â high	 Â variance	 Â 
Linear	 Â regression	 Â is	 Â used	 Â as	 Â our	 Â baseline	 Â model.	 Â 70%/30%	 Â cross	 Â validation	 Â is	 Â used	 Â here	 Â to	 Â divide	 Â the	 Â data	 Â set	 Â into	 Â 
training	 Â set	 Â and	 Â test	 Â set.	 Â As	 Â it	 Â turns	 Â out,	 Â linear	 Â regression	 Â gives	 Â us	 Â a	 Â RMSPE	 Â of	 Â 52.8%.	 Â 
	 Â 

5.	 Â EXPERIMENTS	 Â AND	 Â RESULTS	 Â 
Linear	 Â Regression	 Â 

Support	 Â Vector	 Â Regression	 Â 

	 Â 

	 Â One	 Â thing	 Â special	 Â on	 Â the	 Â implementation	 Â of	 Â SVR	 Â is	 Â that,	 Â it	 Â need	 Â to	 Â build	 Â an	 Â m*m	 Â matrix,	 Â where	 Â m	 Â indicates	 Â the	 Â number	 Â 
of	 Â training	 Â samples.	 Â Since	 Â the	 Â size	 Â of	 Â our	 Â training	 Â set	 Â is	 Â ~700,000	 Â ,	 Â itâ€™s	 Â unrealistic	 Â to	 Â operate	 Â on	 Â the	 Â whole	 Â dataset.	 Â To	 Â take	 Â 
use	 Â of	 Â the	 Â abundant	 Â dataset	 Â practically,	 Â we	 Â build	 Â a	 Â SVR	 Â model	 Â for	 Â each	 Â store,	 Â and	 Â compute	 Â the	 Â mean	 Â of	 Â each	 Â storeâ€™s	 Â RMSPE	 Â 
as	 Â our	 Â final	 Â error	 Â rate.	 Â 	 Â 
Firstly,	 Â we	 Â applied	 Â Polynomial	 Â Kernel	 Â and	 Â Gaussian	 Â Kernel	 Â for	 Â a	 Â single	 Â store,	 Â Store	 Â 1.	 Â By	 Â trying	 Â different	 Â pairs	 Â of	 Â 
ğœ† Â & Â ğœ Â for	 Â Gaussian	 Â Kernel,	 Â and	 Â different	 Â pairs	 Â of	 Â ğœ† Â & Â ğ‘‘ Â for	 Â Polynomial	 Â Kernel,	 Â we	 Â found	 Â that	 Â when	 Â ğœ† Â  Â = Â 140,ğœ Â =
 Â 45, Â Gaussian	 Â Kernel	 Â gives	 Â the	 Â best	 Â RMSPE	 Â of	 Â 13.6%,	 Â when	 Â ğœ† Â  Â = Â 0.1,ğ‘‘ Â = Â 2	 Â Polynomial	 Â Kernel	 Â gives	 Â the	 Â best	 Â RMSPE	 Â of	 Â 
12.8%.	 Â Two	 Â kernels	 Â are	 Â comparable	 Â in	 Â this	 Â scenario. Â 
Secondly,	 Â using	 Â the	 Â method	 Â of	 Â finding	 Â optimal	 Â pairs	 Â of	 Â parameters	 Â discussed	 Â above,	 Â we	 Â applied	 Â Gaussian	 Â Kernel	 Â and	 Â 
Polynomial	 Â Kernel	 Â to	 Â all	 Â stores.	 Â As	 Â we	 Â dig	 Â deeper	 Â into	 Â the	 Â dataset,	 Â we	 Â found	 Â that	 Â accuracies	 Â vary	 Â on	 Â different	 Â time	 Â period	 Â 
of	 Â prediction.	 Â Below	 Â are	 Â figures	 Â of	 Â how	 Â RMSPE	 Â varies	 Â with	 Â different	 Â time	 Â period	 Â for	 Â prediction.	 Â 	 Â 

(a)	 Â Gaussian	 Â Kernel	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â (b)	 Â Polynomial	 Â Kernel	 Â 
Figure	 Â 1.	 Â How	 Â RMSPE	 Â Varies	 Â with	 Â Different	 Â Time	 Â Period	 Â to	 Â Predict	 Â 

	 Â 
As	 Â shown	 Â above,	 Â RMSPEs	 Â for	 Â both	 Â kernels	 Â first	 Â increase	 Â and	 Â then	 Â decrease	 Â as	 Â time	 Â period	 Â of	 Â prediction	 Â gets	 Â larger.	 Â 
For	 Â Gaussian	 Â Kernel,	 Â RMSPE	 Â reaches	 Â minimum	 Â when	 Â predicting	 Â for	 Â just	 Â one	 Â week,	 Â however,	 Â for	 Â Polynomial	 Â Kernel,	 Â RMSPE	 Â 
reaches	 Â minimum	 Â when	 Â predicting	 Â for	 Â 3	 Â weeks.	 Â Given	 Â this	 Â result,	 Â we	 Â draw	 Â figures	 Â of	 Â RMSPE	 Â for	 Â all	 Â stores	 Â using	 Â Gaussian	 Â 
Kernel	 Â and	 Â Polynomial	 Â Kernel,	 Â predicting	 Â for	 Â 1	 Â week	 Â and	 Â 3	 Â weeks	 Â respectively.	 Â 	 Â 

	 Â 

	 Â 

Figure	 Â 2.	 Â RMSPE	 Â for	 Â Each	 Â Store	 Â 

(a)	 Â Gaussian	 Â Kernel(Predict	 Â for	 Â 1	 Â week)	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â (b)	 Â Polynomial	 Â Kernel(Predict	 Â for	 Â 3	 Â weeks)	 Â 
In	 Â terms	 Â of	 Â average	 Â RMSPE,	 Â Polynomial	 Â Kernel(16.3%)	 Â beats	 Â Gaussian	 Â Kernel	 Â (26.8%)	 Â significantly.	 Â In	 Â the	 Â meantime	 Â 
Polynomial	 Â Kernel	 Â is	 Â also	 Â more	 Â robust	 Â than	 Â Gaussian	 Â Kernel,	 Â given	 Â that	 Â there	 Â are	 Â fewer	 Â outliers	 Â and	 Â no	 Â extreme	 Â 
outliers(RMSPE>1)	 Â in	 Â the	 Â figure	 Â of	 Â Polynomial	 Â Kernel.	 Â So	 Â overall,	 Â Polynomial	 Â Kernel	 Â suits	 Â the	 Â dataset	 Â better	 Â and	 Â provides	 Â 
more	 Â reliable	 Â results.	 Â 
	 Â 
We	 Â applied	 Â Random	 Â Forest	 Â after	 Â merging	 Â all	 Â data	 Â including	 Â all	 Â the	 Â categorical	 Â data.	 Â We	 Â used	 Â scikit-Â­â€learn	 Â package	 Â of	 Â 
python	 Â for	 Â implementing	 Â the	 Â algorithm[7].	 Â 
The	 Â two	 Â main	 Â parameters	 Â we	 Â tuned	 Â for	 Â RF	 Â is	 Â the	 Â number	 Â of	 Â trees	 Â and	 Â the	 Â size	 Â of	 Â the	 Â random	 Â subsets	 Â of	 Â features	 Â to	 Â 
consider	 Â when	 Â splitting	 Â a	 Â node.	 Â We	 Â used	 Â 5	 Â fold	 Â cross	 Â validation	 Â to	 Â get	 Â RMSPE	 Â while	 Â varying	 Â these	 Â parameters.	 Â Two	 Â plots	 Â 
are	 Â shown	 Â below.	 Â From	 Â these	 Â plots,	 Â we	 Â could	 Â see	 Â that	 Â RMSPE	 Â doesnâ€™t	 Â change	 Â too	 Â much	 Â after	 Â tree	 Â number	 Â reaches	 Â 30	 Â and	 Â 
after	 Â feature	 Â number	 Â reaches	 Â 20.	 Â 	 Â 

Random	 Â Forest	 Â 

	 Â 	 Â 

	 Â 
Fig.	 Â 3	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Feature	 Â number	 Â 	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â Fig.	 Â 4	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Tree	 Â number	 Â 
	 Â After	 Â tuning	 Â and	 Â fixing	 Â the	 Â optimal	 Â parameters,	 Â we	 Â tried	 Â to	 Â change	 Â the	 Â size	 Â of	 Â the	 Â training	 Â data	 Â to	 Â fit	 Â the	 Â test	 Â data	 Â 
better.	 Â We	 Â used	 Â the	 Â last	 Â two	 Â weeks	 Â 7/14/2015	 Â â€“	 Â 7/31/2015	 Â as	 Â our	 Â test	 Â data	 Â to	 Â get	 Â our	 Â final	 Â prediction	 Â RMSPE	 Â result.	 Â We	 Â 
got	 Â a	 Â plot	 Â RMSPE	 Â vs.	 Â Number	 Â of	 Â month	 Â before	 Â the	 Â test	 Â period	 Â shown	 Â below.	 Â We	 Â could	 Â see	 Â RMSPE	 Â almost	 Â doesnâ€™t	 Â change	 Â 
after	 Â month	 Â number	 Â reaches	 Â around	 Â 20.	 Â Our	 Â best	 Â result	 Â for	 Â RF	 Â is	 Â 12.3%.	 Â Figure	 Â 6	 Â is	 Â the	 Â importance	 Â ranking	 Â bar	 Â plot	 Â for	 Â the	 Â 
most	 Â important	 Â 10	 Â features	 Â shown	 Â below.	 Â Competitors	 Â and	 Â promotions	 Â prove	 Â to	 Â have	 Â the	 Â biggest	 Â impact	 Â on	 Â sales,	 Â whereas	 Â 
features.	 Â We	 Â also	 Â plotted	 Â how	 Â RMSPE	 Â changes	 Â as	 Â the	 Â duration	 Â for	 Â test	 Â data	 Â increases	 Â as	 Â shown	 Â below.	 Â We	 Â can	 Â see	 Â our	 Â RF	 Â 
model	 Â is	 Â still	 Â relatively	 Â accurate	 Â even	 Â for	 Â a	 Â long	 Â duration,	 Â up	 Â to	 Â 6	 Â months.	 Â 

	 Â 

Drug	 Â Store	 Â Sales	 Â Prediction	 Â 

Chenghao	 Â Wang,	 Â Yang	 Â Li	 Â 

1.	 Â INTRODUCTION	 Â 

2.	 Â RELATED	 Â WORK	 Â 	 Â 

Abstract	 Â -Â­â€	 Â In	 Â this	 Â paper	 Â we	 Â tried	 Â to	 Â apply	 Â machine	 Â learning	 Â algorithm	 Â into	 Â a	 Â real	 Â world	 Â problem	 Â â€“	 Â drug	 Â store	 Â sales	 Â 
forecasting.	 Â Given	 Â store	 Â information,	 Â and	 Â sales	 Â record	 Â we	 Â applied	 Â Linear	 Â Regression,	 Â Support	 Â Vector	 Â Regression(SVR)	 Â with	 Â 
Gaussian	 Â and	 Â Polynomial	 Â Kernels	 Â and	 Â Random	 Â Forest	 Â algorithm,	 Â and	 Â tried	 Â to	 Â predict	 Â sales	 Â for	 Â 1-Â­â€3	 Â weeks.	 Â Root	 Â Mean	 Â 
Square	 Â Percentage	 Â Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â the	 Â accuracy.	 Â As	 Â it	 Â turned	 Â out,	 Â Random	 Â Forest	 Â outshined	 Â all	 Â other	 Â 
models	 Â and	 Â reached	 Â RMSPE	 Â of	 Â 12.3%,	 Â which	 Â is	 Â a	 Â reliable	 Â forecast	 Â that	 Â enables	 Â store	 Â managers	 Â allocate	 Â staff	 Â and	 Â stock	 Â up	 Â 
effectively.	 Â  	 Â 
This	 Â problem	 Â is	 Â one	 Â of	 Â several	 Â Machine	 Â Learning	 Â problems	 Â on	 Â Kaggle1	 Â .	 Â The	 Â aim	 Â of	 Â this	 Â problem	 Â is	 Â to	 Â forecast	 Â future	 Â 
sales	 Â of	 Â 1,115	 Â Rossman	 Â drug	 Â stores	 Â located	 Â across	 Â Germany	 Â based	 Â on	 Â their	 Â historical	 Â sales	 Â data.	 Â The	 Â practical	 Â meaning	 Â of	 Â 
solving	 Â this	 Â problem	 Â lies	 Â in	 Â that	 Â reliable	 Â sales	 Â forecasts	 Â enables	 Â store	 Â managers	 Â to	 Â create	 Â effective	 Â staff	 Â schedules	 Â that	 Â 
increase	 Â productivity	 Â and	 Â motivation.	 Â Whatâ€™s	 Â more,	 Â for	 Â the	 Â purpose	 Â of	 Â practicing	 Â what	 Â we	 Â learnt	 Â from	 Â the	 Â Machine	 Â 
Learning	 Â class,	 Â this	 Â problem	 Â saves	 Â us	 Â the	 Â trouble	 Â of	 Â collecting	 Â data,	 Â and	 Â in	 Â the	 Â meanwhile	 Â provides	 Â a	 Â perfect	 Â real	 Â case	 Â to	 Â 
apply	 Â supervised	 Â learning	 Â algorithms.	 Â 
As	 Â a	 Â matter	 Â of	 Â fact,	 Â substantial	 Â effort	 Â has	 Â been	 Â put	 Â into	 Â sales	 Â prediction	 Â problems.	 Â Due	 Â to	 Â promising	 Â performance,	 Â 
	 Â 
artificial	 Â neural	 Â networks	 Â (ANNs)	 Â have	 Â been	 Â applied	 Â for	 Â sales	 Â forecasting	 Â in	 Â many	 Â scenarios.	 Â Thiesing,	 Â F.M.	 Â implemented	 Â a	 Â 
neural	 Â network	 Â forecasting	 Â system	 Â as	 Â a	 Â prototype	 Â to	 Â determine	 Â the	 Â expected	 Â sale	 Â figures[1].	 Â Whatâ€™s	 Â more,	 Â R.J.	 Â Kuo	 Â utilized	 Â 
a	 Â fuzzy	 Â neural	 Â network	 Â with	 Â initial	 Â weights	 Â generated	 Â by	 Â genetic	 Â algorithm	 Â (GFNN)	 Â and	 Â further	 Â integrated	 Â GFNN	 Â with	 Â ANN	 Â 
forecast	 Â using	 Â the	 Â time	 Â series	 Â data	 Â and	 Â promotion	 Â length[2].	 Â This	 Â is	 Â closely	 Â related	 Â to	 Â our	 Â problem	 Â because	 Â promotion	 Â has	 Â 
proved	 Â to	 Â be	 Â one	 Â of	 Â the	 Â most	 Â important	 Â features	 Â in	 Â our	 Â dataset.	 Â There	 Â are	 Â some	 Â interesting	 Â attempts	 Â too.	 Â For	 Â example,	 Â 
Xiaohui	 Â Yu	 Â tried	 Â to	 Â predict	 Â sales	 Â of	 Â products	 Â based	 Â on	 Â online	 Â reviews[3],	 Â and	 Â Michael	 Â Giering	 Â tried	 Â to	 Â correlate	 Â sales	 Â with	 Â 
customer	 Â demographics[4].	 Â As	 Â for	 Â beginners	 Â to	 Â get	 Â started	 Â with	 Â sales	 Â prediction	 Â problem,	 Â Smola	 Â described	 Â a	 Â regression	 Â 
technique	 Â similar	 Â to	 Â SVM	 Â called	 Â Support	 Â Vector	 Â Regression	 Â (SVR)[5].	 Â Breiman	 Â posed	 Â Random	 Â Forest	 Â algorithm[6]	 Â which	 Â is	 Â 
based	 Â on	 Â decision	 Â trees,	 Â but	 Â randomness	 Â is	 Â added.	 Â It	 Â performs	 Â very	 Â well	 Â compared	 Â to	 Â many	 Â other	 Â algorithms,	 Â including	 Â 
neural	 Â networks,	 Â discriminant	 Â analysis	 Â etc.	 Â and	 Â is	 Â robust	 Â against	 Â overfitting.	 Â SVR	 Â and	 Â Random	 Â Forest	 Â are	 Â both	 Â 
implemented	 Â in	 Â out	 Â project.	 Â 
The	 Â dataset	 Â of	 Â this	 Â problem	 Â can	 Â be	 Â found	 Â online2.	 Â The	 Â data	 Â comes	 Â in	 Â two	 Â sets	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â link	 Â to	 Â this	 Â problem:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales	 Â 
	 Â 
2	 Â The	 Â link	 Â to	 Â dataset:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales/data	 Â 	 Â 

3.	 Â DATASET	 Â AND	 Â FEATURES	 Â 

!!

!!!!

4.	 Â METHODS	 Â 

1.	 Â Sales	 Â Dataset	 Â -Â­â€	 Â Historical	 Â sales	 Â data	 Â for	 Â 1,115	 Â Rossman	 Â stores	 Â from	 Â 2013/1/1	 Â to	 Â 2015/7/31.	 Â Features	 Â 
include	 Â store	 Â number,	 Â date,	 Â day	 Â of	 Â week,	 Â whether	 Â thereâ€™s	 Â a	 Â promotion,	 Â whether	 Â itâ€™s	 Â a	 Â school	 Â or	 Â state	 Â holiday	 Â and	 Â sales	 Â on	 Â 
that	 Â day.	 Â 
2.	 Â Store	 Â Dataset	 Â -Â­â€	 Â Storesâ€™	 Â individual	 Â characteristics.	 Â Features	 Â include	 Â store	 Â type,	 Â assortment	 Â level,	 Â nearest	 Â 
competitorâ€™s	 Â distance	 Â and	 Â when	 Â the	 Â competitor	 Â was	 Â opened,	 Â and	 Â whether	 Â thereâ€™s	 Â a	 Â consecutive	 Â promotion.	 Â 
Throughout	 Â our	 Â trial,	 Â weâ€™ve	 Â tried	 Â to	 Â take	 Â advantage	 Â of	 Â different	 Â subset	 Â of	 Â features.	 Â However,	 Â reducing	 Â number	 Â of	 Â 
features	 Â didnâ€™t	 Â increase	 Â accuracy	 Â for	 Â this	 Â problem.	 Â So	 Â all	 Â features	 Â are	 Â used	 Â for	 Â building	 Â models.	 Â 
70%/30%	 Â and	 Â k-Â­â€fold	 Â cross	 Â validations	 Â are	 Â used	 Â in	 Â this	 Â problem	 Â for	 Â training	 Â and	 Â testing.	 Â Root	 Â Mean	 Â Square	 Â Percentage	 Â 
	 Â 
(!!!!!!! )!
Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â accuracy,	 Â which	 Â is	 Â defined	 Â as: Â ğ‘…ğ‘€ğ‘†ğ‘ƒğ¸ Â = Â 
There	 Â are	 Â two	 Â methods	 Â to	 Â train	 Â the	 Â data.	 Â One	 Â is	 Â to	 Â train	 Â each	 Â store	 Â separately,	 Â which	 Â means	 Â forecasting	 Â sales	 Â of	 Â a	 Â 
single	 Â store	 Â based	 Â on	 Â its	 Â own	 Â sales	 Â record,	 Â regardless	 Â of	 Â store	 Â attributes.	 Â The	 Â other	 Â one	 Â is	 Â to	 Â train	 Â all	 Â stores	 Â together,	 Â 
considering	 Â store	 Â attributes	 Â as	 Â parameters.	 Â 
To	 Â train	 Â each	 Â store	 Â separately,	 Â one	 Â straightforward	 Â idea	 Â is	 Â to	 Â apply	 Â linear	 Â regression.	 Â According	 Â to	 Â the	 Â normal	 Â 
equation,	 Â ğœƒ Â = Â (ğ‘‹!ğ‘‹)!!ğ‘‹!ğ‘¦,	 Â we	 Â can	 Â easily	 Â predict	 Â sales	 Â by	 Â ğ»!(ğ‘¥)= Â ğœƒ!ğ‘¥	 Â 
Further	 Â more,	 Â we	 Â figured	 Â that	 Â this	 Â problem	 Â can	 Â actually	 Â be	 Â kernelized.	 Â Here	 Â consider	 Â that	 Â case	 Â of	 Â applying	 Â MAP	 Â 
estimate	 Â for	 Â ğœƒ	 Â to	 Â avoid	 Â overfitting,	 Â which	 Â results	 Â in	 Â the	 Â following	 Â primal	 Â problem	 Â 
ğœƒ= Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›||ğ‘¦âˆ’ğœƒ!ğ‘‹|| Â + Â ğœ†||ğœƒ||!	 Â 
ğ›¼!!!!! <ğ‘¥,ğ‘¥(!)>,	 Â we	 Â can	 Â see	 Â that	 Â this	 Â problem	 Â 
	 Â If	 Â we	 Â calculate	 Â ğ›¼	 Â as	 Â ğ›¼ Â = Â (<ğ‘‹,ğ‘‹>+ğœ†ğ¼)!!ğ‘¦.	 Â And	 Â define	 Â ğ»(ğ‘¥)=
can	 Â actually	 Â be	 Â kernalized,	 Â thus	 Â we	 Â can	 Â apply	 Â the	 Â kernel	 Â trick.	 Â We	 Â tried	 Â Gaussian	 Â Kernel	 Â and	 Â Polynomial	 Â Kernel	 Â in	 Â this	 Â case,	 Â 
which	 Â is	 Â illustrated	 Â as	 Â following.	 Â 
(a)	 Â Gaussian	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =exp âˆ’||!!!||!
!!!
(b)	 Â Polynomial	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =(ğ‘¥âˆ—ğ‘§+1)! Â 	 Â (Polynomials	 Â of	 Â degree	 Â up	 Â to	 Â d)	 Â 
Our	 Â next	 Â model	 Â for	 Â this	 Â project	 Â is	 Â Random	 Â Forest	 Â Regression.	 Â We	 Â tried	 Â this	 Â model	 Â because	 Â itâ€™s	 Â fast	 Â and	 Â can	 Â 
accommodate	 Â categorical	 Â data.	 Â RF	 Â first	 Â picked	 Â a	 Â certain	 Â amount	 Â of	 Â data	 Â from	 Â the	 Â dataset	 Â randomly	 Â (ie.	 Â bootstrap)	 Â and	 Â then	 Â 
picked	 Â a	 Â certain	 Â amount	 Â of	 Â features	 Â out	 Â of	 Â the	 Â total	 Â features	 Â randomly	 Â to	 Â build	 Â decision	 Â trees.	 Â The	 Â final	 Â result	 Â for	 Â each	 Â test	 Â 
data	 Â is	 Â average	 Â of	 Â results	 Â obtained	 Â by	 Â all	 Â these	 Â decision	 Â trees.	 Â Decision	 Â trees	 Â usually	 Â overfit	 Â the	 Â data;	 Â however	 Â randomness	 Â 
will	 Â average	 Â out	 Â the	 Â high	 Â variance	 Â 
Linear	 Â regression	 Â is	 Â used	 Â as	 Â our	 Â baseline	 Â model.	 Â 70%/30%	 Â cross	 Â validation	 Â is	 Â used	 Â here	 Â to	 Â divide	 Â the	 Â data	 Â set	 Â into	 Â 
training	 Â set	 Â and	 Â test	 Â set.	 Â As	 Â it	 Â turns	 Â out,	 Â linear	 Â regression	 Â gives	 Â us	 Â a	 Â RMSPE	 Â of	 Â 52.8%.	 Â 
	 Â 

5.	 Â EXPERIMENTS	 Â AND	 Â RESULTS	 Â 
Linear	 Â Regression	 Â 

Support	 Â Vector	 Â Regression	 Â 

	 Â 

	 Â One	 Â thing	 Â special	 Â on	 Â the	 Â implementation	 Â of	 Â SVR	 Â is	 Â that,	 Â it	 Â need	 Â to	 Â build	 Â an	 Â m*m	 Â matrix,	 Â where	 Â m	 Â indicates	 Â the	 Â number	 Â 
of	 Â training	 Â samples.	 Â Since	 Â the	 Â size	 Â of	 Â our	 Â training	 Â set	 Â is	 Â ~700,000	 Â ,	 Â itâ€™s	 Â unrealistic	 Â to	 Â operate	 Â on	 Â the	 Â whole	 Â dataset.	 Â To	 Â take	 Â 
use	 Â of	 Â the	 Â abundant	 Â dataset	 Â practically,	 Â we	 Â build	 Â a	 Â SVR	 Â model	 Â for	 Â each	 Â store,	 Â and	 Â compute	 Â the	 Â mean	 Â of	 Â each	 Â storeâ€™s	 Â RMSPE	 Â 
as	 Â our	 Â final	 Â error	 Â rate.	 Â 	 Â 
Firstly,	 Â we	 Â applied	 Â Polynomial	 Â Kernel	 Â and	 Â Gaussian	 Â Kernel	 Â for	 Â a	 Â single	 Â store,	 Â Store	 Â 1.	 Â By	 Â trying	 Â different	 Â pairs	 Â of	 Â 
ğœ† Â & Â ğœ Â for	 Â Gaussian	 Â Kernel,	 Â and	 Â different	 Â pairs	 Â of	 Â ğœ† Â & Â ğ‘‘ Â for	 Â Polynomial	 Â Kernel,	 Â we	 Â found	 Â that	 Â when	 Â ğœ† Â  Â = Â 140,ğœ Â =
 Â 45, Â Gaussian	 Â Kernel	 Â gives	 Â the	 Â best	 Â RMSPE	 Â of	 Â 13.6%,	 Â when	 Â ğœ† Â  Â = Â 0.1,ğ‘‘ Â = Â 2	 Â Polynomial	 Â Kernel	 Â gives	 Â the	 Â best	 Â RMSPE	 Â of	 Â 
12.8%.	 Â Two	 Â kernels	 Â are	 Â comparable	 Â in	 Â this	 Â scenario. Â 
Secondly,	 Â using	 Â the	 Â method	 Â of	 Â finding	 Â optimal	 Â pairs	 Â of	 Â parameters	 Â discussed	 Â above,	 Â we	 Â applied	 Â Gaussian	 Â Kernel	 Â and	 Â 
Polynomial	 Â Kernel	 Â to	 Â all	 Â stores.	 Â As	 Â we	 Â dig	 Â deeper	 Â into	 Â the	 Â dataset,	 Â we	 Â found	 Â that	 Â accuracies	 Â vary	 Â on	 Â different	 Â time	 Â period	 Â 
of	 Â prediction.	 Â Below	 Â are	 Â figures	 Â of	 Â how	 Â RMSPE	 Â varies	 Â with	 Â different	 Â time	 Â period	 Â for	 Â prediction.	 Â 	 Â 

(a)	 Â Gaussian	 Â Kernel	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â (b)	 Â Polynomial	 Â Kernel	 Â 
Figure	 Â 1.	 Â How	 Â RMSPE	 Â Varies	 Â with	 Â Different	 Â Time	 Â Period	 Â to	 Â Predict	 Â 

	 Â 
As	 Â shown	 Â above,	 Â RMSPEs	 Â for	 Â both	 Â kernels	 Â first	 Â increase	 Â and	 Â then	 Â decrease	 Â as	 Â time	 Â period	 Â of	 Â prediction	 Â gets	 Â larger.	 Â 
For	 Â Gaussian	 Â Kernel,	 Â RMSPE	 Â reaches	 Â minimum	 Â when	 Â predicting	 Â for	 Â just	 Â one	 Â week,	 Â however,	 Â for	 Â Polynomial	 Â Kernel,	 Â RMSPE	 Â 
reaches	 Â minimum	 Â when	 Â predicting	 Â for	 Â 3	 Â weeks.	 Â Given	 Â this	 Â result,	 Â we	 Â draw	 Â figures	 Â of	 Â RMSPE	 Â for	 Â all	 Â stores	 Â using	 Â Gaussian	 Â 
Kernel	 Â and	 Â Polynomial	 Â Kernel,	 Â predicting	 Â for	 Â 1	 Â week	 Â and	 Â 3	 Â weeks	 Â respectively.	 Â 	 Â 

	 Â 

	 Â 

Figure	 Â 2.	 Â RMSPE	 Â for	 Â Each	 Â Store	 Â 

(a)	 Â Gaussian	 Â Kernel(Predict	 Â for	 Â 1	 Â week)	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â (b)	 Â Polynomial	 Â Kernel(Predict	 Â for	 Â 3	 Â weeks)	 Â 
In	 Â terms	 Â of	 Â average	 Â RMSPE,	 Â Polynomial	 Â Kernel(16.3%)	 Â beats	 Â Gaussian	 Â Kernel	 Â (26.8%)	 Â significantly.	 Â In	 Â the	 Â meantime	 Â 
Polynomial	 Â Kernel	 Â is	 Â also	 Â more	 Â robust	 Â than	 Â Gaussian	 Â Kernel,	 Â given	 Â that	 Â there	 Â are	 Â fewer	 Â outliers	 Â and	 Â no	 Â extreme	 Â 
outliers(RMSPE>1)	 Â in	 Â the	 Â figure	 Â of	 Â Polynomial	 Â Kernel.	 Â So	 Â overall,	 Â Polynomial	 Â Kernel	 Â suits	 Â the	 Â dataset	 Â better	 Â and	 Â provides	 Â 
more	 Â reliable	 Â results.	 Â 
	 Â 
We	 Â applied	 Â Random	 Â Forest	 Â after	 Â merging	 Â all	 Â data	 Â including	 Â all	 Â the	 Â categorical	 Â data.	 Â We	 Â used	 Â scikit-Â­â€learn	 Â package	 Â of	 Â 
python	 Â for	 Â implementing	 Â the	 Â algorithm[7].	 Â 
The	 Â two	 Â main	 Â parameters	 Â we	 Â tuned	 Â for	 Â RF	 Â is	 Â the	 Â number	 Â of	 Â trees	 Â and	 Â the	 Â size	 Â of	 Â the	 Â random	 Â subsets	 Â of	 Â features	 Â to	 Â 
consider	 Â when	 Â splitting	 Â a	 Â node.	 Â We	 Â used	 Â 5	 Â fold	 Â cross	 Â validation	 Â to	 Â get	 Â RMSPE	 Â while	 Â varying	 Â these	 Â parameters.	 Â Two	 Â plots	 Â 
are	 Â shown	 Â below.	 Â From	 Â these	 Â plots,	 Â we	 Â could	 Â see	 Â that	 Â RMSPE	 Â doesnâ€™t	 Â change	 Â too	 Â much	 Â after	 Â tree	 Â number	 Â reaches	 Â 30	 Â and	 Â 
after	 Â feature	 Â number	 Â reaches	 Â 20.	 Â 	 Â 

Random	 Â Forest	 Â 

	 Â 	 Â 

	 Â 
Fig.	 Â 3	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Feature	 Â number	 Â 	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â Fig.	 Â 4	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Tree	 Â number	 Â 
	 Â After	 Â tuning	 Â and	 Â fixing	 Â the	 Â optimal	 Â parameters,	 Â we	 Â tried	 Â to	 Â change	 Â the	 Â size	 Â of	 Â the	 Â training	 Â data	 Â to	 Â fit	 Â the	 Â test	 Â data	 Â 
better.	 Â We	 Â used	 Â the	 Â last	 Â two	 Â weeks	 Â 7/14/2015	 Â â€“	 Â 7/31/2015	 Â as	 Â our	 Â test	 Â data	 Â to	 Â get	 Â our	 Â final	 Â prediction	 Â RMSPE	 Â result.	 Â We	 Â 
got	 Â a	 Â plot	 Â RMSPE	 Â vs.	 Â Number	 Â of	 Â month	 Â before	 Â the	 Â test	 Â period	 Â shown	 Â below.	 Â We	 Â could	 Â see	 Â RMSPE	 Â almost	 Â doesnâ€™t	 Â change	 Â 
after	 Â month	 Â number	 Â reaches	 Â around	 Â 20.	 Â Our	 Â best	 Â result	 Â for	 Â RF	 Â is	 Â 12.3%.	 Â Figure	 Â 6	 Â is	 Â the	 Â importance	 Â ranking	 Â bar	 Â plot	 Â for	 Â the	 Â 
most	 Â important	 Â 10	 Â features	 Â shown	 Â below.	 Â Competitors	 Â and	 Â promotions	 Â prove	 Â to	 Â have	 Â the	 Â biggest	 Â impact	 Â on	 Â sales,	 Â whereas	 Â 
features.	 Â We	 Â also	 Â plotted	 Â how	 Â RMSPE	 Â changes	 Â as	 Â the	 Â duration	 Â for	 Â test	 Â data	 Â increases	 Â as	 Â shown	 Â below.	 Â We	 Â can	 Â see	 Â our	 Â RF	 Â 
model	 Â is	 Â still	 Â relatively	 Â accurate	 Â even	 Â for	 Â a	 Â long	 Â duration,	 Â up	 Â to	 Â 6	 Â months.	 Â 

	 Â 

	 Â 	 Â Fig.	 Â 5	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Number	 Â of	 Â month	 Â 	 Â 	 Â 

	 Â 	 Â Fig.	 Â 6	 Â Feature	 Â importance	 Â ranking	 Â (10	 Â most	 Â important)	 Â 

	 Â 

Fig.	 Â 7	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Number	 Â of	 Â month	 Â for	 Â test	 Â duration	 Â 

	 Â  	 Â 

	 Â 

6.	 Â CONCLUSION	 Â AND	 Â FUTURE	 Â WORK	 Â 

The	 Â following	 Â table	 Â shows	 Â the	 Â results	 Â of	 Â our	 Â models.	 Â 

	 Â 

Random	 Â Forest	 Â 

SVR	 Â with	 Â Polynomial	 Â Kernel	 Â for	 Â All	 Â Stores	 Â 

As	 Â is	 Â shown	 Â in	 Â the	 Â result,	 Â among	 Â all	 Â models,	 Â Random	 Â Forest	 Â works	 Â the	 Â best,	 Â and	 Â provides	 Â a	 Â reliable	 Â prediction	 Â of	 Â the	 Â 
sales.	 Â Linear	 Â regression,	 Â SVR	 Â with	 Â Gaussian/Polynomial	 Â Kernels	 Â and	 Â RF	 Â all	 Â have	 Â their	 Â own	 Â strengths	 Â and	 Â limitations.	 Â By	 Â 
implementing	 Â these	 Â algorithms,	 Â weâ€™ve	 Â studies	 Â the	 Â properties	 Â of	 Â the	 Â dataset	 Â and	 Â made	 Â reasonable	 Â predictions.	 Â In	 Â the	 Â future,	 Â 
we	 Â wish	 Â to	 Â use	 Â the	 Â fact	 Â that	 Â sales	 Â records	 Â are	 Â consecutive	 Â in	 Â time,	 Â and	 Â see	 Â how	 Â time	 Â series	 Â affect	 Â prediction	 Â result.	 Â Also	 Â 
there	 Â are	 Â still	 Â many	 Â effective	 Â machine	 Â learning	 Â algorithms	 Â worth	 Â trying,	 Â so	 Â we	 Â would	 Â like	 Â to	 Â try	 Â more	 Â algorithms	 Â in	 Â the	 Â 
future,	 Â such	 Â as	 Â Gradient	 Â Boosting	 Â and	 Â k-Â­â€Nearest	 Â Neighbors	 Â algorithm.	 Â 
[1]	 Â Thiesing,	 Â Frank	 Â M.,	 Â and	 Â Oliver	 Â Vornberger.	 Â "Sales	 Â forecasting	 Â using	 Â neural	 Â networks."	 Â Neural	 Â Networks,	 Â 1997.,	 Â 
	 Â 

7.REFERENCES	 Â 

Model	 Â 

Linear	 Â Regression	 Â 

SVR	 Â with	 Â Polynomial	 Â Kernel	 Â for	 Â Store	 Â 1	 Â 

SVR	 Â with	 Â Gaussian	 Â Kernel	 Â for	 Â Store	 Â 1	 Â 

SVR	 Â with	 Â Gaussian	 Â Kernel	 Â for	 Â All	 Â Stores	 Â 

RMSPE	 Â 

52.7%	 Â 
12.8%	 Â 
13.6%	 Â 
Avg	 Â of	 Â 26.8%	 Â 
Avg	 Â of	 Â 16.3%	 Â 
12.3%	 Â 

Size	 Â of	 Â Test	 Â Set	 Â 

Remarks	 Â 

For	 Â any	 Â ğœ†	 Â 
ğœ† Â = Â 0.1 Â ğ‘‘ Â = Â 2	 Â 
ğœ† Â = Â 140 Â ğœ Â = Â 45	 Â 
Each	 Â store	 Â chooses	 Â its	 Â own	 Â optimal	 Â ğœ† Â & Â ğœ Â 	 Â 
Each	 Â store	 Â chooses	 Â its	 Â own	 Â optimal	 Â ğœ† Â & Â ğ‘‘ Â 	 Â 
7/14-Â­â€7/31/2015,	 Â 20	 Â max	 Â features,	 Â 30	 Â trees	 Â 

~300	 Â days	 Â 
~300	 Â days	 Â 
~300	 Â days	 Â 
1	 Â week	 Â 
3	 Â weeks	 Â 
2	 Â weeks	 Â 

Drug	 Â Store	 Â Sales	 Â Prediction	 Â 

Chenghao	 Â Wang,	 Â Yang	 Â Li	 Â 

1.	 Â INTRODUCTION	 Â 

2.	 Â RELATED	 Â WORK	 Â 	 Â 

Abstract	 Â -Â­â€	 Â In	 Â this	 Â paper	 Â we	 Â tried	 Â to	 Â apply	 Â machine	 Â learning	 Â algorithm	 Â into	 Â a	 Â real	 Â world	 Â problem	 Â â€“	 Â drug	 Â store	 Â sales	 Â 
forecasting.	 Â Given	 Â store	 Â information,	 Â and	 Â sales	 Â record	 Â we	 Â applied	 Â Linear	 Â Regression,	 Â Support	 Â Vector	 Â Regression(SVR)	 Â with	 Â 
Gaussian	 Â and	 Â Polynomial	 Â Kernels	 Â and	 Â Random	 Â Forest	 Â algorithm,	 Â and	 Â tried	 Â to	 Â predict	 Â sales	 Â for	 Â 1-Â­â€3	 Â weeks.	 Â Root	 Â Mean	 Â 
Square	 Â Percentage	 Â Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â the	 Â accuracy.	 Â As	 Â it	 Â turned	 Â out,	 Â Random	 Â Forest	 Â outshined	 Â all	 Â other	 Â 
models	 Â and	 Â reached	 Â RMSPE	 Â of	 Â 12.3%,	 Â which	 Â is	 Â a	 Â reliable	 Â forecast	 Â that	 Â enables	 Â store	 Â managers	 Â allocate	 Â staff	 Â and	 Â stock	 Â up	 Â 
effectively.	 Â  	 Â 
This	 Â problem	 Â is	 Â one	 Â of	 Â several	 Â Machine	 Â Learning	 Â problems	 Â on	 Â Kaggle1	 Â .	 Â The	 Â aim	 Â of	 Â this	 Â problem	 Â is	 Â to	 Â forecast	 Â future	 Â 
sales	 Â of	 Â 1,115	 Â Rossman	 Â drug	 Â stores	 Â located	 Â across	 Â Germany	 Â based	 Â on	 Â their	 Â historical	 Â sales	 Â data.	 Â The	 Â practical	 Â meaning	 Â of	 Â 
solving	 Â this	 Â problem	 Â lies	 Â in	 Â that	 Â reliable	 Â sales	 Â forecasts	 Â enables	 Â store	 Â managers	 Â to	 Â create	 Â effective	 Â staff	 Â schedules	 Â that	 Â 
increase	 Â productivity	 Â and	 Â motivation.	 Â Whatâ€™s	 Â more,	 Â for	 Â the	 Â purpose	 Â of	 Â practicing	 Â what	 Â we	 Â learnt	 Â from	 Â the	 Â Machine	 Â 
Learning	 Â class,	 Â this	 Â problem	 Â saves	 Â us	 Â the	 Â trouble	 Â of	 Â collecting	 Â data,	 Â and	 Â in	 Â the	 Â meanwhile	 Â provides	 Â a	 Â perfect	 Â real	 Â case	 Â to	 Â 
apply	 Â supervised	 Â learning	 Â algorithms.	 Â 
As	 Â a	 Â matter	 Â of	 Â fact,	 Â substantial	 Â effort	 Â has	 Â been	 Â put	 Â into	 Â sales	 Â prediction	 Â problems.	 Â Due	 Â to	 Â promising	 Â performance,	 Â 
	 Â 
artificial	 Â neural	 Â networks	 Â (ANNs)	 Â have	 Â been	 Â applied	 Â for	 Â sales	 Â forecasting	 Â in	 Â many	 Â scenarios.	 Â Thiesing,	 Â F.M.	 Â implemented	 Â a	 Â 
neural	 Â network	 Â forecasting	 Â system	 Â as	 Â a	 Â prototype	 Â to	 Â determine	 Â the	 Â expected	 Â sale	 Â figures[1].	 Â Whatâ€™s	 Â more,	 Â R.J.	 Â Kuo	 Â utilized	 Â 
a	 Â fuzzy	 Â neural	 Â network	 Â with	 Â initial	 Â weights	 Â generated	 Â by	 Â genetic	 Â algorithm	 Â (GFNN)	 Â and	 Â further	 Â integrated	 Â GFNN	 Â with	 Â ANN	 Â 
forecast	 Â using	 Â the	 Â time	 Â series	 Â data	 Â and	 Â promotion	 Â length[2].	 Â This	 Â is	 Â closely	 Â related	 Â to	 Â our	 Â problem	 Â because	 Â promotion	 Â has	 Â 
proved	 Â to	 Â be	 Â one	 Â of	 Â the	 Â most	 Â important	 Â features	 Â in	 Â our	 Â dataset.	 Â There	 Â are	 Â some	 Â interesting	 Â attempts	 Â too.	 Â For	 Â example,	 Â 
Xiaohui	 Â Yu	 Â tried	 Â to	 Â predict	 Â sales	 Â of	 Â products	 Â based	 Â on	 Â online	 Â reviews[3],	 Â and	 Â Michael	 Â Giering	 Â tried	 Â to	 Â correlate	 Â sales	 Â with	 Â 
customer	 Â demographics[4].	 Â As	 Â for	 Â beginners	 Â to	 Â get	 Â started	 Â with	 Â sales	 Â prediction	 Â problem,	 Â Smola	 Â described	 Â a	 Â regression	 Â 
technique	 Â similar	 Â to	 Â SVM	 Â called	 Â Support	 Â Vector	 Â Regression	 Â (SVR)[5].	 Â Breiman	 Â posed	 Â Random	 Â Forest	 Â algorithm[6]	 Â which	 Â is	 Â 
based	 Â on	 Â decision	 Â trees,	 Â but	 Â randomness	 Â is	 Â added.	 Â It	 Â performs	 Â very	 Â well	 Â compared	 Â to	 Â many	 Â other	 Â algorithms,	 Â including	 Â 
neural	 Â networks,	 Â discriminant	 Â analysis	 Â etc.	 Â and	 Â is	 Â robust	 Â against	 Â overfitting.	 Â SVR	 Â and	 Â Random	 Â Forest	 Â are	 Â both	 Â 
implemented	 Â in	 Â out	 Â project.	 Â 
The	 Â dataset	 Â of	 Â this	 Â problem	 Â can	 Â be	 Â found	 Â online2.	 Â The	 Â data	 Â comes	 Â in	 Â two	 Â sets	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 
1	 Â The	 Â link	 Â to	 Â this	 Â problem:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales	 Â 
	 Â 
2	 Â The	 Â link	 Â to	 Â dataset:	 Â https://www.kaggle.com/c/rossmann-Â­â€store-Â­â€sales/data	 Â 	 Â 

3.	 Â DATASET	 Â AND	 Â FEATURES	 Â 

!!

!!!!

4.	 Â METHODS	 Â 

1.	 Â Sales	 Â Dataset	 Â -Â­â€	 Â Historical	 Â sales	 Â data	 Â for	 Â 1,115	 Â Rossman	 Â stores	 Â from	 Â 2013/1/1	 Â to	 Â 2015/7/31.	 Â Features	 Â 
include	 Â store	 Â number,	 Â date,	 Â day	 Â of	 Â week,	 Â whether	 Â thereâ€™s	 Â a	 Â promotion,	 Â whether	 Â itâ€™s	 Â a	 Â school	 Â or	 Â state	 Â holiday	 Â and	 Â sales	 Â on	 Â 
that	 Â day.	 Â 
2.	 Â Store	 Â Dataset	 Â -Â­â€	 Â Storesâ€™	 Â individual	 Â characteristics.	 Â Features	 Â include	 Â store	 Â type,	 Â assortment	 Â level,	 Â nearest	 Â 
competitorâ€™s	 Â distance	 Â and	 Â when	 Â the	 Â competitor	 Â was	 Â opened,	 Â and	 Â whether	 Â thereâ€™s	 Â a	 Â consecutive	 Â promotion.	 Â 
Throughout	 Â our	 Â trial,	 Â weâ€™ve	 Â tried	 Â to	 Â take	 Â advantage	 Â of	 Â different	 Â subset	 Â of	 Â features.	 Â However,	 Â reducing	 Â number	 Â of	 Â 
features	 Â didnâ€™t	 Â increase	 Â accuracy	 Â for	 Â this	 Â problem.	 Â So	 Â all	 Â features	 Â are	 Â used	 Â for	 Â building	 Â models.	 Â 
70%/30%	 Â and	 Â k-Â­â€fold	 Â cross	 Â validations	 Â are	 Â used	 Â in	 Â this	 Â problem	 Â for	 Â training	 Â and	 Â testing.	 Â Root	 Â Mean	 Â Square	 Â Percentage	 Â 
	 Â 
(!!!!!!! )!
Error	 Â (RMSPE)	 Â is	 Â used	 Â to	 Â measure	 Â accuracy,	 Â which	 Â is	 Â defined	 Â as: Â ğ‘…ğ‘€ğ‘†ğ‘ƒğ¸ Â = Â 
There	 Â are	 Â two	 Â methods	 Â to	 Â train	 Â the	 Â data.	 Â One	 Â is	 Â to	 Â train	 Â each	 Â store	 Â separately,	 Â which	 Â means	 Â forecasting	 Â sales	 Â of	 Â a	 Â 
single	 Â store	 Â based	 Â on	 Â its	 Â own	 Â sales	 Â record,	 Â regardless	 Â of	 Â store	 Â attributes.	 Â The	 Â other	 Â one	 Â is	 Â to	 Â train	 Â all	 Â stores	 Â together,	 Â 
considering	 Â store	 Â attributes	 Â as	 Â parameters.	 Â 
To	 Â train	 Â each	 Â store	 Â separately,	 Â one	 Â straightforward	 Â idea	 Â is	 Â to	 Â apply	 Â linear	 Â regression.	 Â According	 Â to	 Â the	 Â normal	 Â 
equation,	 Â ğœƒ Â = Â (ğ‘‹!ğ‘‹)!!ğ‘‹!ğ‘¦,	 Â we	 Â can	 Â easily	 Â predict	 Â sales	 Â by	 Â ğ»!(ğ‘¥)= Â ğœƒ!ğ‘¥	 Â 
Further	 Â more,	 Â we	 Â figured	 Â that	 Â this	 Â problem	 Â can	 Â actually	 Â be	 Â kernelized.	 Â Here	 Â consider	 Â that	 Â case	 Â of	 Â applying	 Â MAP	 Â 
estimate	 Â for	 Â ğœƒ	 Â to	 Â avoid	 Â overfitting,	 Â which	 Â results	 Â in	 Â the	 Â following	 Â primal	 Â problem	 Â 
ğœƒ= Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›||ğ‘¦âˆ’ğœƒ!ğ‘‹|| Â + Â ğœ†||ğœƒ||!	 Â 
ğ›¼!!!!! <ğ‘¥,ğ‘¥(!)>,	 Â we	 Â can	 Â see	 Â that	 Â this	 Â problem	 Â 
	 Â If	 Â we	 Â calculate	 Â ğ›¼	 Â as	 Â ğ›¼ Â = Â (<ğ‘‹,ğ‘‹>+ğœ†ğ¼)!!ğ‘¦.	 Â And	 Â define	 Â ğ»(ğ‘¥)=
can	 Â actually	 Â be	 Â kernalized,	 Â thus	 Â we	 Â can	 Â apply	 Â the	 Â kernel	 Â trick.	 Â We	 Â tried	 Â Gaussian	 Â Kernel	 Â and	 Â Polynomial	 Â Kernel	 Â in	 Â this	 Â case,	 Â 
which	 Â is	 Â illustrated	 Â as	 Â following.	 Â 
(a)	 Â Gaussian	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =exp âˆ’||!!!||!
!!!
(b)	 Â Polynomial	 Â Kernel	 Â ğ¾ğ‘¥,ğ‘§  Â =(ğ‘¥âˆ—ğ‘§+1)! Â 	 Â (Polynomials	 Â of	 Â degree	 Â up	 Â to	 Â d)	 Â 
Our	 Â next	 Â model	 Â for	 Â this	 Â project	 Â is	 Â Random	 Â Forest	 Â Regression.	 Â We	 Â tried	 Â this	 Â model	 Â because	 Â itâ€™s	 Â fast	 Â and	 Â can	 Â 
accommodate	 Â categorical	 Â data.	 Â RF	 Â first	 Â picked	 Â a	 Â certain	 Â amount	 Â of	 Â data	 Â from	 Â the	 Â dataset	 Â randomly	 Â (ie.	 Â bootstrap)	 Â and	 Â then	 Â 
picked	 Â a	 Â certain	 Â amount	 Â of	 Â features	 Â out	 Â of	 Â the	 Â total	 Â features	 Â randomly	 Â to	 Â build	 Â decision	 Â trees.	 Â The	 Â final	 Â result	 Â for	 Â each	 Â test	 Â 
data	 Â is	 Â average	 Â of	 Â results	 Â obtained	 Â by	 Â all	 Â these	 Â decision	 Â trees.	 Â Decision	 Â trees	 Â usually	 Â overfit	 Â the	 Â data;	 Â however	 Â randomness	 Â 
will	 Â average	 Â out	 Â the	 Â high	 Â variance	 Â 
Linear	 Â regression	 Â is	 Â used	 Â as	 Â our	 Â baseline	 Â model.	 Â 70%/30%	 Â cross	 Â validation	 Â is	 Â used	 Â here	 Â to	 Â divide	 Â the	 Â data	 Â set	 Â into	 Â 
training	 Â set	 Â and	 Â test	 Â set.	 Â As	 Â it	 Â turns	 Â out,	 Â linear	 Â regression	 Â gives	 Â us	 Â a	 Â RMSPE	 Â of	 Â 52.8%.	 Â 
	 Â 

5.	 Â EXPERIMENTS	 Â AND	 Â RESULTS	 Â 
Linear	 Â Regression	 Â 

Support	 Â Vector	 Â Regression	 Â 

	 Â 

	 Â One	 Â thing	 Â special	 Â on	 Â the	 Â implementation	 Â of	 Â SVR	 Â is	 Â that,	 Â it	 Â need	 Â to	 Â build	 Â an	 Â m*m	 Â matrix,	 Â where	 Â m	 Â indicates	 Â the	 Â number	 Â 
of	 Â training	 Â samples.	 Â Since	 Â the	 Â size	 Â of	 Â our	 Â training	 Â set	 Â is	 Â ~700,000	 Â ,	 Â itâ€™s	 Â unrealistic	 Â to	 Â operate	 Â on	 Â the	 Â whole	 Â dataset.	 Â To	 Â take	 Â 
use	 Â of	 Â the	 Â abundant	 Â dataset	 Â practically,	 Â we	 Â build	 Â a	 Â SVR	 Â model	 Â for	 Â each	 Â store,	 Â and	 Â compute	 Â the	 Â mean	 Â of	 Â each	 Â storeâ€™s	 Â RMSPE	 Â 
as	 Â our	 Â final	 Â error	 Â rate.	 Â 	 Â 
Firstly,	 Â we	 Â applied	 Â Polynomial	 Â Kernel	 Â and	 Â Gaussian	 Â Kernel	 Â for	 Â a	 Â single	 Â store,	 Â Store	 Â 1.	 Â By	 Â trying	 Â different	 Â pairs	 Â of	 Â 
ğœ† Â & Â ğœ Â for	 Â Gaussian	 Â Kernel,	 Â and	 Â different	 Â pairs	 Â of	 Â ğœ† Â & Â ğ‘‘ Â for	 Â Polynomial	 Â Kernel,	 Â we	 Â found	 Â that	 Â when	 Â ğœ† Â  Â = Â 140,ğœ Â =
 Â 45, Â Gaussian	 Â Kernel	 Â gives	 Â the	 Â best	 Â RMSPE	 Â of	 Â 13.6%,	 Â when	 Â ğœ† Â  Â = Â 0.1,ğ‘‘ Â = Â 2	 Â Polynomial	 Â Kernel	 Â gives	 Â the	 Â best	 Â RMSPE	 Â of	 Â 
12.8%.	 Â Two	 Â kernels	 Â are	 Â comparable	 Â in	 Â this	 Â scenario. Â 
Secondly,	 Â using	 Â the	 Â method	 Â of	 Â finding	 Â optimal	 Â pairs	 Â of	 Â parameters	 Â discussed	 Â above,	 Â we	 Â applied	 Â Gaussian	 Â Kernel	 Â and	 Â 
Polynomial	 Â Kernel	 Â to	 Â all	 Â stores.	 Â As	 Â we	 Â dig	 Â deeper	 Â into	 Â the	 Â dataset,	 Â we	 Â found	 Â that	 Â accuracies	 Â vary	 Â on	 Â different	 Â time	 Â period	 Â 
of	 Â prediction.	 Â Below	 Â are	 Â figures	 Â of	 Â how	 Â RMSPE	 Â varies	 Â with	 Â different	 Â time	 Â period	 Â for	 Â prediction.	 Â 	 Â 

(a)	 Â Gaussian	 Â Kernel	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â (b)	 Â Polynomial	 Â Kernel	 Â 
Figure	 Â 1.	 Â How	 Â RMSPE	 Â Varies	 Â with	 Â Different	 Â Time	 Â Period	 Â to	 Â Predict	 Â 

	 Â 
As	 Â shown	 Â above,	 Â RMSPEs	 Â for	 Â both	 Â kernels	 Â first	 Â increase	 Â and	 Â then	 Â decrease	 Â as	 Â time	 Â period	 Â of	 Â prediction	 Â gets	 Â larger.	 Â 
For	 Â Gaussian	 Â Kernel,	 Â RMSPE	 Â reaches	 Â minimum	 Â when	 Â predicting	 Â for	 Â just	 Â one	 Â week,	 Â however,	 Â for	 Â Polynomial	 Â Kernel,	 Â RMSPE	 Â 
reaches	 Â minimum	 Â when	 Â predicting	 Â for	 Â 3	 Â weeks.	 Â Given	 Â this	 Â result,	 Â we	 Â draw	 Â figures	 Â of	 Â RMSPE	 Â for	 Â all	 Â stores	 Â using	 Â Gaussian	 Â 
Kernel	 Â and	 Â Polynomial	 Â Kernel,	 Â predicting	 Â for	 Â 1	 Â week	 Â and	 Â 3	 Â weeks	 Â respectively.	 Â 	 Â 

	 Â 

	 Â 

Figure	 Â 2.	 Â RMSPE	 Â for	 Â Each	 Â Store	 Â 

(a)	 Â Gaussian	 Â Kernel(Predict	 Â for	 Â 1	 Â week)	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â 	 Â (b)	 Â Polynomial	 Â Kernel(Predict	 Â for	 Â 3	 Â weeks)	 Â 
In	 Â terms	 Â of	 Â average	 Â RMSPE,	 Â Polynomial	 Â Kernel(16.3%)	 Â beats	 Â Gaussian	 Â Kernel	 Â (26.8%)	 Â significantly.	 Â In	 Â the	 Â meantime	 Â 
Polynomial	 Â Kernel	 Â is	 Â also	 Â more	 Â robust	 Â than	 Â Gaussian	 Â Kernel,	 Â given	 Â that	 Â there	 Â are	 Â fewer	 Â outliers	 Â and	 Â no	 Â extreme	 Â 
outliers(RMSPE>1)	 Â in	 Â the	 Â figure	 Â of	 Â Polynomial	 Â Kernel.	 Â So	 Â overall,	 Â Polynomial	 Â Kernel	 Â suits	 Â the	 Â dataset	 Â better	 Â and	 Â provides	 Â 
more	 Â reliable	 Â results.	 Â 
	 Â 
We	 Â applied	 Â Random	 Â Forest	 Â after	 Â merging	 Â all	 Â data	 Â including	 Â all	 Â the	 Â categorical	 Â data.	 Â We	 Â used	 Â scikit-Â­â€learn	 Â package	 Â of	 Â 
python	 Â for	 Â implementing	 Â the	 Â algorithm[7].	 Â 
The	 Â two	 Â main	 Â parameters	 Â we	 Â tuned	 Â for	 Â RF	 Â is	 Â the	 Â number	 Â of	 Â trees	 Â and	 Â the	 Â size	 Â of	 Â the	 Â random	 Â subsets	 Â of	 Â features	 Â to	 Â 
consider	 Â when	 Â splitting	 Â a	 Â node.	 Â We	 Â used	 Â 5	 Â fold	 Â cross	 Â validation	 Â to	 Â get	 Â RMSPE	 Â while	 Â varying	 Â these	 Â parameters.	 Â Two	 Â plots	 Â 
are	 Â shown	 Â below.	 Â From	 Â these	 Â plots,	 Â we	 Â could	 Â see	 Â that	 Â RMSPE	 Â doesnâ€™t	 Â change	 Â too	 Â much	 Â after	 Â tree	 Â number	 Â reaches	 Â 30	 Â and	 Â 
after	 Â feature	 Â number	 Â reaches	 Â 20.	 Â 	 Â 

Random	 Â Forest	 Â 

	 Â 	 Â 

	 Â 
Fig.	 Â 3	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Feature	 Â number	 Â 	 Â 
	 Â 	 Â 	 Â 	 Â 	 Â Fig.	 Â 4	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Tree	 Â number	 Â 
	 Â After	 Â tuning	 Â and	 Â fixing	 Â the	 Â optimal	 Â parameters,	 Â we	 Â tried	 Â to	 Â change	 Â the	 Â size	 Â of	 Â the	 Â training	 Â data	 Â to	 Â fit	 Â the	 Â test	 Â data	 Â 
better.	 Â We	 Â used	 Â the	 Â last	 Â two	 Â weeks	 Â 7/14/2015	 Â â€“	 Â 7/31/2015	 Â as	 Â our	 Â test	 Â data	 Â to	 Â get	 Â our	 Â final	 Â prediction	 Â RMSPE	 Â result.	 Â We	 Â 
got	 Â a	 Â plot	 Â RMSPE	 Â vs.	 Â Number	 Â of	 Â month	 Â before	 Â the	 Â test	 Â period	 Â shown	 Â below.	 Â We	 Â could	 Â see	 Â RMSPE	 Â almost	 Â doesnâ€™t	 Â change	 Â 
after	 Â month	 Â number	 Â reaches	 Â around	 Â 20.	 Â Our	 Â best	 Â result	 Â for	 Â RF	 Â is	 Â 12.3%.	 Â Figure	 Â 6	 Â is	 Â the	 Â importance	 Â ranking	 Â bar	 Â plot	 Â for	 Â the	 Â 
most	 Â important	 Â 10	 Â features	 Â shown	 Â below.	 Â Competitors	 Â and	 Â promotions	 Â prove	 Â to	 Â have	 Â the	 Â biggest	 Â impact	 Â on	 Â sales,	 Â whereas	 Â 
features.	 Â We	 Â also	 Â plotted	 Â how	 Â RMSPE	 Â changes	 Â as	 Â the	 Â duration	 Â for	 Â test	 Â data	 Â increases	 Â as	 Â shown	 Â below.	 Â We	 Â can	 Â see	 Â our	 Â RF	 Â 
model	 Â is	 Â still	 Â relatively	 Â accurate	 Â even	 Â for	 Â a	 Â long	 Â duration,	 Â up	 Â to	 Â 6	 Â months.	 Â 

	 Â 

	 Â 	 Â Fig.	 Â 5	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Number	 Â of	 Â month	 Â 	 Â 	 Â 

	 Â 	 Â Fig.	 Â 6	 Â Feature	 Â importance	 Â ranking	 Â (10	 Â most	 Â important)	 Â 

	 Â 

Fig.	 Â 7	 Â How	 Â RMSPE	 Â changes	 Â with	 Â Number	 Â of	 Â month	 Â for	 Â test	 Â duration	 Â 

	 Â  	 Â 

	 Â 

6.	 Â CONCLUSION	 Â AND	 Â FUTURE	 Â WORK	 Â 

The	 Â following	 Â table	 Â shows	 Â the	 Â results	 Â of	 Â our	 Â models.	 Â 

	 Â 

Random	 Â Forest	 Â 

SVR	 Â with	 Â Polynomial	 Â Kernel	 Â for	 Â All	 Â Stores	 Â 

As	 Â is	 Â shown	 Â in	 Â the	 Â result,	 Â among	 Â all	 Â models,	 Â Random	 Â Forest	 Â works	 Â the	 Â best,	 Â and	 Â provides	 Â a	 Â reliable	 Â prediction	 Â of	 Â the	 Â 
sales.	 Â Linear	 Â regression,	 Â SVR	 Â with	 Â Gaussian/Polynomial	 Â Kernels	 Â and	 Â RF	 Â all	 Â have	 Â their	 Â own	 Â strengths	 Â and	 Â limitations.	 Â By	 Â 
implementing	 Â these	 Â algorithms,	 Â weâ€™ve	 Â studies	 Â the	 Â properties	 Â of	 Â the	 Â dataset	 Â and	 Â made	 Â reasonable	 Â predictions.	 Â In	 Â the	 Â future,	 Â 
we	 Â wish	 Â to	 Â use	 Â the	 Â fact	 Â that	 Â sales	 Â records	 Â are	 Â consecutive	 Â in	 Â time,	 Â and	 Â see	 Â how	 Â time	 Â series	 Â affect	 Â prediction	 Â result.	 Â Also	 Â 
there	 Â are	 Â still	 Â many	 Â effective	 Â machine	 Â learning	 Â algorithms	 Â worth	 Â trying,	 Â so	 Â we	 Â would	 Â like	 Â to	 Â try	 Â more	 Â algorithms	 Â in	 Â the	 Â 
future,	 Â such	 Â as	 Â Gradient	 Â Boosting	 Â and	 Â k-Â­â€Nearest	 Â Neighbors	 Â algorithm.	 Â 
[1]	 Â Thiesing,	 Â Frank	 Â M.,	 Â and	 Â Oliver	 Â Vornberger.	 Â "Sales	 Â forecasting	 Â using	 Â neural	 Â networks."	 Â Neural	 Â Networks,	 Â 1997.,	 Â 
	 Â 

7.REFERENCES	 Â 

Model	 Â 

Linear	 Â Regression	 Â 

SVR	 Â with	 Â Polynomial	 Â Kernel	 Â for	 Â Store	 Â 1	 Â 

SVR	 Â with	 Â Gaussian	 Â Kernel	 Â for	 Â Store	 Â 1	 Â 

SVR	 Â with	 Â Gaussian	 Â Kernel	 Â for	 Â All	 Â Stores	 Â 

RMSPE	 Â 

52.7%	 Â 
12.8%	 Â 
13.6%	 Â 
Avg	 Â of	 Â 26.8%	 Â 
Avg	 Â of	 Â 16.3%	 Â 
12.3%	 Â 

Size	 Â of	 Â Test	 Â Set	 Â 

Remarks	 Â 

For	 Â any	 Â ğœ†	 Â 
ğœ† Â = Â 0.1 Â ğ‘‘ Â = Â 2	 Â 
ğœ† Â = Â 140 Â ğœ Â = Â 45	 Â 
Each	 Â store	 Â chooses	 Â its	 Â own	 Â optimal	 Â ğœ† Â & Â ğœ Â 	 Â 
Each	 Â store	 Â chooses	 Â its	 Â own	 Â optimal	 Â ğœ† Â & Â ğ‘‘ Â 	 Â 
7/14-Â­â€7/31/2015,	 Â 20	 Â max	 Â features,	 Â 30	 Â trees	 Â 

~300	 Â days	 Â 
~300	 Â days	 Â 
~300	 Â days	 Â 
1	 Â week	 Â 
3	 Â weeks	 Â 
2	 Â weeks	 Â 

International	 Â Conference	 Â on.	 Â Vol.	 Â 4.	 Â IEEE,	 Â 1997.	 Â 
[2]	 Â Kuo,	 Â R.	 Â J.	 Â "A	 Â sales	 Â forecasting	 Â system	 Â based	 Â on	 Â fuzzy	 Â neural	 Â network	 Â with	 Â initial	 Â weights	 Â generated	 Â by	 Â genetic	 Â 
algorithm."	 Â European	 Â Journal	 Â of	 Â Operational	 Â Research	 Â 129.3	 Â (2001):	 Â 496-Â­â€517.	 Â 
[3]	 Â Yu,	 Â Xiaohui,	 Â et	 Â al.	 Â "A	 Â quality-Â­â€aware	 Â model	 Â for	 Â sales	 Â prediction	 Â using	 Â reviews."	 Â Proceedings	 Â of	 Â the	 Â 19th	 Â international	 Â 
conference	 Â on	 Â World	 Â wide	 Â web.	 Â ACM,	 Â 2010.	 Â 
[4]	 Â Giering,	 Â Michael.	 Â "Retail	 Â sales	 Â prediction	 Â and	 Â item	 Â recommendations	 Â using	 Â customer	 Â demographics	 Â at	 Â store	 Â level."	 Â ACM	 Â 
SIGKDD	 Â Explorations	 Â Newsletter	 Â 10.2	 Â (2008):	 Â 84-Â­â€89.	 Â 
[5]	 Â Smola,	 Â Alex	 Â J.,	 Â and	 Â Bernhard	 Â SchÃ¶lkopf.	 Â "A	 Â tutorial	 Â on	 Â support	 Â vector	 Â regression."	 Â Statistics	 Â and	 Â computing	 Â 14.3	 Â (2004):	 Â 
199-Â­â€222.	 Â 
[6]	 Â Breiman,	 Â L.	 Â (2001).	 Â Random	 Â forests.	 Â Machine	 Â learning,	 Â 45(1),	 Â 5-Â­â€32.	 Â 
[7]	 Â Scikit-Â­â€learn:	 Â Machine	 Â Learning	 Â in	 Â Python,	 Â Pedregosa	 Â et	 Â al.,	 Â JMLR	 Â 12,	 Â pp.	 Â 2825-Â­â€2830,	 Â 2011.	 Â 

	 Â 

