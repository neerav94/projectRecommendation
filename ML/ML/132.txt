An Application of Machine Learning to Native Advertisements

Kevin Grogan, Quinlan Jung

Abstract
Machine learning algorithms are applied to data aggregated from the internet to classify webpages as native advertisements or
core content. Four diﬀerent classiﬁers are employed: logistic regression, Poisson naive Bayes, multinomial naive Bayes, and
support vector machine. The features are selected using a threshold based on the mutual information statistic; additionally, the
tf-idf and tf-bns transformations are applied to the training set. Principal component analysis is examined as an a priori feature
reduction algorithm. The reduction in the dimensionality of the training set through principal component analysis is found to
increase the test error of the logistic regression classiﬁer to an unacceptable level. The tf-idf and tf-bns transformations are
found to successfully improve the quality of the classiﬁers. The logistic regression classiﬁer is shown to produce the lowest test
error of all the classiﬁers examined.

1. Introduction

Native advertisements seek to mimic the core content of
websites in order to minimize the disruption to the users
viewing experience. Ideally, the advertisements are as fun
and informative as the website’s core content. However,
they are often intrusive and decrease the likelihood of re-
peat viewings. Therefore, advertisers would greatly beneﬁt
from a model that could determine which content is an ad-
vertisement and which is not. Tricking such a model post
facto would imply that the advertisement blends well with
the core content of the website. Additionally, identiﬁcation
of key parameters that betray advertisements would help ad-
vertisers develop more organic content.

Hence, this project applies machine learning algorithms
to classify articles as native advertisements or core con-
tent. For this project, the naive Bayes, support vector ma-
chine (SVM), and logistic regression algorithms as classi-
ﬁers are employed. Additionally, principal component anal-
ysis (PCA) to examine its utility in feature reduction.

2. Related Work

Naive Bayes and support vector machines have been the
traditional approaches to solving text classiﬁcation prob-
lems. When the dataset is extremely large, logistic regres-
sion is sometimes used to approximate an SVM because it is
computationally more eﬃcient [4].

Naive Bayes performs moderately well in text classi-
ﬁcation. Of the diﬀerent event models, the multivariate
Bernoulli performs well with small vocabulary sizes, but
the multinomial usually performs better at larger vocabulary
sizes, providing on average a 27% reduction in error over the
multivariate Bernoulli model at any vocabulary size [3].

SVMs consistently achieve good performance on text
classiﬁcation tasks, outperforming many existing meth-
ods [5]. Because most text classiﬁcation problems are lin-
early separable, a linear kernel is usually recommended, as

it is faster and easier to optimize [6]. Other popular kernels,
such as the string kernel, show positive results on modestly
sized datasets. For larger documents and datasets, approx-
imation techniques need to be used because non-linear ker-
nels are computationally expensive [7].

3. Data Set and Features

The training set consists of 300,000 HTML pages with
class labels provided by kaggle [2]. Since the training set
consited of raw html, the following algorithm is used to tok-
enize the training set:

for each HTML document:

extract the text in the HTML body,

stripping away tags

remove all punctuation
remove all stop words
merge similar words

While embedded javascript and links in the HTML head
along with punctuation in the body could provide more in-
sight to determine whether a document is an advertisement,
this hidden information is not apparent to the audience and is
unlikely to betray a native advertisement. Hence, this project
focuses on the text in the body.

Stop words such as “a”, “and”, and “the”, are removed as
an a priori method of feature reduction. Additionally, words
with the same roots were grouped by detecting common suf-
ﬁxes such as “-ly” and “ing”. Implicitly, this assumes that
the covariance between these words is suﬃciently high that
the dimensional reduction will have a negligible eﬀect on the
bias of the models.

The feature set, or dictionary, is the vocabulary found in
the training set. The collection of tokenized training docu-
ments is given to a CountVectorizer1, and it returns an n × m
frequency matrix, A, where n is the dictionary size, and m
is the number of documents. The entries Ai j denote the fre-
quency of the word at index i found in document j.

Email addresses: kgrogan@stanford.edu (Kevin Grogan),

quinlanj@cs.stanford.edu (Quinlan Jung)
CS229: Final Project

1A python class in the scikit-learn library that converts a collection of

text documents to a matrix of token counts

December 12, 2015

An Application of Machine Learning to Native Advertisements

Kevin Grogan, Quinlan Jung

Abstract
Machine learning algorithms are applied to data aggregated from the internet to classify webpages as native advertisements or
core content. Four diﬀerent classiﬁers are employed: logistic regression, Poisson naive Bayes, multinomial naive Bayes, and
support vector machine. The features are selected using a threshold based on the mutual information statistic; additionally, the
tf-idf and tf-bns transformations are applied to the training set. Principal component analysis is examined as an a priori feature
reduction algorithm. The reduction in the dimensionality of the training set through principal component analysis is found to
increase the test error of the logistic regression classiﬁer to an unacceptable level. The tf-idf and tf-bns transformations are
found to successfully improve the quality of the classiﬁers. The logistic regression classiﬁer is shown to produce the lowest test
error of all the classiﬁers examined.

1. Introduction

Native advertisements seek to mimic the core content of
websites in order to minimize the disruption to the users
viewing experience. Ideally, the advertisements are as fun
and informative as the website’s core content. However,
they are often intrusive and decrease the likelihood of re-
peat viewings. Therefore, advertisers would greatly beneﬁt
from a model that could determine which content is an ad-
vertisement and which is not. Tricking such a model post
facto would imply that the advertisement blends well with
the core content of the website. Additionally, identiﬁcation
of key parameters that betray advertisements would help ad-
vertisers develop more organic content.

Hence, this project applies machine learning algorithms
to classify articles as native advertisements or core con-
tent. For this project, the naive Bayes, support vector ma-
chine (SVM), and logistic regression algorithms as classi-
ﬁers are employed. Additionally, principal component anal-
ysis (PCA) to examine its utility in feature reduction.

2. Related Work

Naive Bayes and support vector machines have been the
traditional approaches to solving text classiﬁcation prob-
lems. When the dataset is extremely large, logistic regres-
sion is sometimes used to approximate an SVM because it is
computationally more eﬃcient [4].

Naive Bayes performs moderately well in text classi-
ﬁcation. Of the diﬀerent event models, the multivariate
Bernoulli performs well with small vocabulary sizes, but
the multinomial usually performs better at larger vocabulary
sizes, providing on average a 27% reduction in error over the
multivariate Bernoulli model at any vocabulary size [3].

SVMs consistently achieve good performance on text
classiﬁcation tasks, outperforming many existing meth-
ods [5]. Because most text classiﬁcation problems are lin-
early separable, a linear kernel is usually recommended, as

it is faster and easier to optimize [6]. Other popular kernels,
such as the string kernel, show positive results on modestly
sized datasets. For larger documents and datasets, approx-
imation techniques need to be used because non-linear ker-
nels are computationally expensive [7].

3. Data Set and Features

The training set consists of 300,000 HTML pages with
class labels provided by kaggle [2]. Since the training set
consited of raw html, the following algorithm is used to tok-
enize the training set:

for each HTML document:

extract the text in the HTML body,

stripping away tags

remove all punctuation
remove all stop words
merge similar words

While embedded javascript and links in the HTML head
along with punctuation in the body could provide more in-
sight to determine whether a document is an advertisement,
this hidden information is not apparent to the audience and is
unlikely to betray a native advertisement. Hence, this project
focuses on the text in the body.

Stop words such as “a”, “and”, and “the”, are removed as
an a priori method of feature reduction. Additionally, words
with the same roots were grouped by detecting common suf-
ﬁxes such as “-ly” and “ing”. Implicitly, this assumes that
the covariance between these words is suﬃciently high that
the dimensional reduction will have a negligible eﬀect on the
bias of the models.

The feature set, or dictionary, is the vocabulary found in
the training set. The collection of tokenized training docu-
ments is given to a CountVectorizer1, and it returns an n × m
frequency matrix, A, where n is the dictionary size, and m
is the number of documents. The entries Ai j denote the fre-
quency of the word at index i found in document j.

Email addresses: kgrogan@stanford.edu (Kevin Grogan),

quinlanj@cs.stanford.edu (Quinlan Jung)
CS229: Final Project

1A python class in the scikit-learn library that converts a collection of

text documents to a matrix of token counts

December 12, 2015

3.1. Term Frequency/Inverse Document Frequency

Term frequency/inverse document frequency (tf-idf) is a
heuristic metric used in text classiﬁcation to yield better re-
sults with classiﬁers. The functional form of the tf-idf statis-
tic used for this project is

tf-idfi j = tfi j × idfi

= log(1 + Ai, j) × log

(cid:33)

(cid:32) N

ni

(1)

,

4. Methods

However, the bounding of this metric indicates that it may
underpredict the contribution of a feature to the classiﬁcation
of a data set. Although, for large feature sets such as the
text classiﬁcation used for this project, mutual information
is posited to be a reasonable and computationally eﬃcient
criterion to eliminate features and reduce the variance error
for a classiﬁer.

where N is the size of the training set, and ni is overall fre-
quency of word i in the training set.
tf-idf makes the rea-
sonable assertion that the importance of a word in a docu-
ment must be monotonically dependent on the frequency of
that word (i.e., the tf statistic). However, this term should
be discounted when it is omnipresent in the corpus (i.e., the
idf statistic); therefore, words with little embedded mean-
ing and mostly syntactical relevance (such prepositions, con-
junctions, and articles) are neglected by the classiﬁers under
this transformation.

3.2. Term Frequency/Bi-Normal Separation

While tf-idf is the most widely used representation for
real-valued feature vectors for text classiﬁcation problems,
idf is oblivious to the training class labels and naturally
scales some features inappropriately. Alternatively, idf can
be replaced with bi-normal separation (bns), which has been
previously found to be successful at ranking words for fea-
ture selection ﬁltering [9]. The functional form of the tf-bns
statistic used is

tf-bnsi j = tfi j × bnsi

= log(1 + Ai, j) × |F−1(tpr) − F−1(fpr)|,

(2)

Where the true positive rate is given by tpr = P(word | pos-
itive class), the false positive rate is given by fpr= P(word
| positive class), and F−1 is the inverse Normal cumulative
distribution function.

3.3. Mutual Information

Mutual information provides a metric to assign an impor-
tance a given feature to the classiﬁcation of the document. It
is deﬁned as

MI(xi, y) =

p(xi, y) log

p(xi, y)
p(xi)p(y) .

(3)

(cid:88)

(cid:88)

xi∈{0,1}

y∈{0,1}

Rewriting Eq. 3 using Jensen’s inequality and applying the
relation p(xi, y) = p(y|xi)p(xi), the mutual information of a
feature, xi, is shown to be bounded by

MI(xi, y) ≤ log E

,

(4)

(cid:34) p(y|xi)

(cid:35)

p(y)

where the expectation is taken over the joint probability dis-
tribution of xi and y. This furthers the intuition that the mu-
tual information is a bounded and normalized metric related
to the expected contribution of a feature to the successful
prediction of a class label over simply the probability of the
class label.

4.1. Principal Component Analysis

Principal component analysis seeks to reduce the dimen-
sionality of a feature set. This is done by ﬁnding a unit vec-
tor u that maximizes the variance in the data when projected
onto the vector. Formally, this can be shown to be given by

u = argmax
u:(cid:107)u(cid:107)=1

uTΣu ,

(5)

where Σ is the covariance matrix. Through the method of La-
grange multipliers, this relation can be demonstrated to yield
the eigenvector corresponding to the maximum eigenvalue
of the covariance matrix: u = vi : λi = max{λ1, ..., λn}.

Hence, an eigenvalue decomposition of the covariance
matrix gives a basis of eigenvectors, where the variance of
the projection onto each eigenvector is in proportion to the
corresponding eigenvalue.

Furthermore, dimensional reduction of the training data
may be obtained by a projection onto the eigenvector basis:

Bk = VT

k C,

(6)
where Bk ∈ Rk×m is the reduced representation of the
columns of matrix C ∈ Rn×m to k ≤ n dimensions, and
Vk ∈ Rn×k is a matrix of the ﬁrst k principal eigenvectors.
For this project, the matrix C is built from the tf-idf trans-
formation of the frequency matrix detailed by Eq. 1, and the
rows of C are normalized to have zero mean and unit vari-
ance.
Additionally, since the initial set of features yield a covari-
ance matrix with O(109) elements, sparsity is enforced using
the Cauchy-Schwarz inequality

Cov(ci, c j)

0

Σi j =

Var(ci)Var(c j) > d

for [Cov(ci,c j)]2
o.t.w.

(7)

where ci, c j are row vectors in C, and d is a threshold parame-
ter set to 0.8. This parameter to yield a matrix with a density
just so that the matrix could be manipulated in a practical
manner on a laptop.

4.2. Logistic Regression

A logistic regression assumes that the conditional prob-
ability of the labeling follows a Bernoulli random variable
(i.e, y|x; θ ∼ Bernoulli(φ)). By deﬁning the hypothesis
function as the expectation of the conditional probability,
hθ(x) = E[y|x; θ] and assuming a linear relation between
the feature vector and parameter vecotor, θ, the hypothesis
function is correspondingly found to be given by the sigmoid
function:

hθ(x) =

2

1

1 − exp(θTx) ,

(8)

An Application of Machine Learning to Native Advertisements

Kevin Grogan, Quinlan Jung

Abstract
Machine learning algorithms are applied to data aggregated from the internet to classify webpages as native advertisements or
core content. Four diﬀerent classiﬁers are employed: logistic regression, Poisson naive Bayes, multinomial naive Bayes, and
support vector machine. The features are selected using a threshold based on the mutual information statistic; additionally, the
tf-idf and tf-bns transformations are applied to the training set. Principal component analysis is examined as an a priori feature
reduction algorithm. The reduction in the dimensionality of the training set through principal component analysis is found to
increase the test error of the logistic regression classiﬁer to an unacceptable level. The tf-idf and tf-bns transformations are
found to successfully improve the quality of the classiﬁers. The logistic regression classiﬁer is shown to produce the lowest test
error of all the classiﬁers examined.

1. Introduction

Native advertisements seek to mimic the core content of
websites in order to minimize the disruption to the users
viewing experience. Ideally, the advertisements are as fun
and informative as the website’s core content. However,
they are often intrusive and decrease the likelihood of re-
peat viewings. Therefore, advertisers would greatly beneﬁt
from a model that could determine which content is an ad-
vertisement and which is not. Tricking such a model post
facto would imply that the advertisement blends well with
the core content of the website. Additionally, identiﬁcation
of key parameters that betray advertisements would help ad-
vertisers develop more organic content.

Hence, this project applies machine learning algorithms
to classify articles as native advertisements or core con-
tent. For this project, the naive Bayes, support vector ma-
chine (SVM), and logistic regression algorithms as classi-
ﬁers are employed. Additionally, principal component anal-
ysis (PCA) to examine its utility in feature reduction.

2. Related Work

Naive Bayes and support vector machines have been the
traditional approaches to solving text classiﬁcation prob-
lems. When the dataset is extremely large, logistic regres-
sion is sometimes used to approximate an SVM because it is
computationally more eﬃcient [4].

Naive Bayes performs moderately well in text classi-
ﬁcation. Of the diﬀerent event models, the multivariate
Bernoulli performs well with small vocabulary sizes, but
the multinomial usually performs better at larger vocabulary
sizes, providing on average a 27% reduction in error over the
multivariate Bernoulli model at any vocabulary size [3].

SVMs consistently achieve good performance on text
classiﬁcation tasks, outperforming many existing meth-
ods [5]. Because most text classiﬁcation problems are lin-
early separable, a linear kernel is usually recommended, as

it is faster and easier to optimize [6]. Other popular kernels,
such as the string kernel, show positive results on modestly
sized datasets. For larger documents and datasets, approx-
imation techniques need to be used because non-linear ker-
nels are computationally expensive [7].

3. Data Set and Features

The training set consists of 300,000 HTML pages with
class labels provided by kaggle [2]. Since the training set
consited of raw html, the following algorithm is used to tok-
enize the training set:

for each HTML document:

extract the text in the HTML body,

stripping away tags

remove all punctuation
remove all stop words
merge similar words

While embedded javascript and links in the HTML head
along with punctuation in the body could provide more in-
sight to determine whether a document is an advertisement,
this hidden information is not apparent to the audience and is
unlikely to betray a native advertisement. Hence, this project
focuses on the text in the body.

Stop words such as “a”, “and”, and “the”, are removed as
an a priori method of feature reduction. Additionally, words
with the same roots were grouped by detecting common suf-
ﬁxes such as “-ly” and “ing”. Implicitly, this assumes that
the covariance between these words is suﬃciently high that
the dimensional reduction will have a negligible eﬀect on the
bias of the models.

The feature set, or dictionary, is the vocabulary found in
the training set. The collection of tokenized training docu-
ments is given to a CountVectorizer1, and it returns an n × m
frequency matrix, A, where n is the dictionary size, and m
is the number of documents. The entries Ai j denote the fre-
quency of the word at index i found in document j.

Email addresses: kgrogan@stanford.edu (Kevin Grogan),

quinlanj@cs.stanford.edu (Quinlan Jung)
CS229: Final Project

1A python class in the scikit-learn library that converts a collection of

text documents to a matrix of token counts

December 12, 2015

3.1. Term Frequency/Inverse Document Frequency

Term frequency/inverse document frequency (tf-idf) is a
heuristic metric used in text classiﬁcation to yield better re-
sults with classiﬁers. The functional form of the tf-idf statis-
tic used for this project is

tf-idfi j = tfi j × idfi

= log(1 + Ai, j) × log

(cid:33)

(cid:32) N

ni

(1)

,

4. Methods

However, the bounding of this metric indicates that it may
underpredict the contribution of a feature to the classiﬁcation
of a data set. Although, for large feature sets such as the
text classiﬁcation used for this project, mutual information
is posited to be a reasonable and computationally eﬃcient
criterion to eliminate features and reduce the variance error
for a classiﬁer.

where N is the size of the training set, and ni is overall fre-
quency of word i in the training set.
tf-idf makes the rea-
sonable assertion that the importance of a word in a docu-
ment must be monotonically dependent on the frequency of
that word (i.e., the tf statistic). However, this term should
be discounted when it is omnipresent in the corpus (i.e., the
idf statistic); therefore, words with little embedded mean-
ing and mostly syntactical relevance (such prepositions, con-
junctions, and articles) are neglected by the classiﬁers under
this transformation.

3.2. Term Frequency/Bi-Normal Separation

While tf-idf is the most widely used representation for
real-valued feature vectors for text classiﬁcation problems,
idf is oblivious to the training class labels and naturally
scales some features inappropriately. Alternatively, idf can
be replaced with bi-normal separation (bns), which has been
previously found to be successful at ranking words for fea-
ture selection ﬁltering [9]. The functional form of the tf-bns
statistic used is

tf-bnsi j = tfi j × bnsi

= log(1 + Ai, j) × |F−1(tpr) − F−1(fpr)|,

(2)

Where the true positive rate is given by tpr = P(word | pos-
itive class), the false positive rate is given by fpr= P(word
| positive class), and F−1 is the inverse Normal cumulative
distribution function.

3.3. Mutual Information

Mutual information provides a metric to assign an impor-
tance a given feature to the classiﬁcation of the document. It
is deﬁned as

MI(xi, y) =

p(xi, y) log

p(xi, y)
p(xi)p(y) .

(3)

(cid:88)

(cid:88)

xi∈{0,1}

y∈{0,1}

Rewriting Eq. 3 using Jensen’s inequality and applying the
relation p(xi, y) = p(y|xi)p(xi), the mutual information of a
feature, xi, is shown to be bounded by

MI(xi, y) ≤ log E

,

(4)

(cid:34) p(y|xi)

(cid:35)

p(y)

where the expectation is taken over the joint probability dis-
tribution of xi and y. This furthers the intuition that the mu-
tual information is a bounded and normalized metric related
to the expected contribution of a feature to the successful
prediction of a class label over simply the probability of the
class label.

4.1. Principal Component Analysis

Principal component analysis seeks to reduce the dimen-
sionality of a feature set. This is done by ﬁnding a unit vec-
tor u that maximizes the variance in the data when projected
onto the vector. Formally, this can be shown to be given by

u = argmax
u:(cid:107)u(cid:107)=1

uTΣu ,

(5)

where Σ is the covariance matrix. Through the method of La-
grange multipliers, this relation can be demonstrated to yield
the eigenvector corresponding to the maximum eigenvalue
of the covariance matrix: u = vi : λi = max{λ1, ..., λn}.

Hence, an eigenvalue decomposition of the covariance
matrix gives a basis of eigenvectors, where the variance of
the projection onto each eigenvector is in proportion to the
corresponding eigenvalue.

Furthermore, dimensional reduction of the training data
may be obtained by a projection onto the eigenvector basis:

Bk = VT

k C,

(6)
where Bk ∈ Rk×m is the reduced representation of the
columns of matrix C ∈ Rn×m to k ≤ n dimensions, and
Vk ∈ Rn×k is a matrix of the ﬁrst k principal eigenvectors.
For this project, the matrix C is built from the tf-idf trans-
formation of the frequency matrix detailed by Eq. 1, and the
rows of C are normalized to have zero mean and unit vari-
ance.
Additionally, since the initial set of features yield a covari-
ance matrix with O(109) elements, sparsity is enforced using
the Cauchy-Schwarz inequality

Cov(ci, c j)

0

Σi j =

Var(ci)Var(c j) > d

for [Cov(ci,c j)]2
o.t.w.

(7)

where ci, c j are row vectors in C, and d is a threshold parame-
ter set to 0.8. This parameter to yield a matrix with a density
just so that the matrix could be manipulated in a practical
manner on a laptop.

4.2. Logistic Regression

A logistic regression assumes that the conditional prob-
ability of the labeling follows a Bernoulli random variable
(i.e, y|x; θ ∼ Bernoulli(φ)). By deﬁning the hypothesis
function as the expectation of the conditional probability,
hθ(x) = E[y|x; θ] and assuming a linear relation between
the feature vector and parameter vecotor, θ, the hypothesis
function is correspondingly found to be given by the sigmoid
function:

hθ(x) =

2

1

1 − exp(θTx) ,

(8)

n(cid:89)

where the parameter vector is found directly through a max-
imum liklihood estimate (MLE). The MLE of the logistic
regression is found using batch gradient ascent where the
learning rate is found empirically after several trials; stochas-
tic gradient ascent was attempted, but the parameter vector
was found to be unacceptably uncertain.

4.3. Naive Bayes

The Naive Bayes algorithm utilizes Bayes rules to predict

a label given a feature set:
p(y = 1|x) =

(cid:80)
p(x|y = 1)p(y)
y∈{0,1} p(x|y)p(y) ,

(9)
where the prior distributions p(x|y = 1) and p(y) must be
modeled. The hypothesis function takes the form hθ(xi) =
p(y = 1|x) for the naive Bayes algorithm. Additionally, the
prior p(x|y) is assumed to be conditionally independent:

p(x|y) =

p(xi|y) .

(10)

i=1

The prior p(y) is modeled using a binomial distribution,
while p(x|y) is modeled using a multinomial distribution or
a Poisson distribution.

A Poisson distribution is considered under the rational that
the number of appearances of a word in a ﬁxed length doc-
ument is well-approximated by a Poisson process. The prior
for a Poisson distribution is given by

i j exp(−λi j)
λxi

p(xi|y = j) =

xi!

(11)
where λi j is E[xi|y = j]. Additionally, it can be shown that
Eq. 9 can be written in the form of a sigmoid function given
by Eq. 8, making implementation straightforward. The pa-
+(cid:80)n
rameter vector in this case is given by
i=1 λi0 − λi1

log φy

φy−1

log(λk1/λk0)

for k = 0
o.t.w.

θk =

(12)

where φy = p(y). The MLE of λi j is given by the sample
mean of each feature corresponding to the labeling y = j.

Assuming a Poisson distribution increases the computa-
tional eﬃciency of the classiﬁer but also embeds stronger
assumptions into the model. The comparative performance
of the classiﬁer will be examined in the Sec. 5.

4.4. Support Vector Machine

The SVM attempts to ﬁnd the maximum-margin hyper-
plane that separates the dataset in a higher dimensional fea-
ture space. Finding this optimal margin reduces to solving
the following convex optimization problem:

||ω||2 + C m(cid:88)

ξi

i=1

minγ,ω,b

1
2

Subject to the constraints:

s.t. y(i)(ωT x(i)) + b) ≥ 1 − ξi, i = 1, ..., m
ξi ≥ 0, i = 1, ..., m

(13)

(14)

Where ξi allows for the ’slack’ in the event that the data is
not linearly separable. Originally, an unoptimized L2-norm

3

linear SVM was employed, but it proved to be too slow due
the high cardinality of the training set. The SVM was modi-
ﬁed to use a dual coordinate descent method [8] that deﬁnes
the primal problem as follows:

minωm,ξi

1
2

m

(cid:88)

||ωm||2 + C(cid:88)

ξi
i − ξi ∀m, i

i

(15)

Where em
i

s.t. ωT x(i) − ωT x(i) ≥ em
= 1 − σm and σm = 1 if y = m, σm = 0
if yi (cid:44) m. This method reaches an -accurate solution in
O(log(1/) iterations. Similar to what is done with Naive
Bayes, tf-bns transformation is performed on the vocabulary
found in the training set. The SVM is set to have parameters
C = 0.23 and ξ = 1.0. The optimal C and ξ are found using
k-fold cross validation, where k = 10.

5. Discussion

5.1. Feature Reduction using Principal Component Analysis
As described in Sec. 4.1, principal component analysis re-
duces the dimensionality of the feature set by maximizing
the variance of the projection of the data onto a set of basis
vectors. Figure 1 shows the eﬀect of the reduced feature set
on the test and training errors.

Figure 1: The eﬀect of the reduction in dimensionality through PCA on
the test error. The error is normalized by that of the full-rank representation
of the data set. Error is found using the logistic regression classiﬁer and
k-folds cross validation.

The ﬁgure shows that error is signiﬁcantly increased when
the dimensionality of the data is reduced through principal
component analysis. Hence, while PCA reduces the feature
set, the substantial increase in bias overshadows any reduc-
tion in variance. Additionally, PCA is an unsupervised learn-
ing algorithm that is agnostic to the class labels. Hence, the
belief that a reduction of dimensionality through this algo-
rithm will provide lower test error may be Pollyannaish.

The threshold to enforce the sparsity may have been overly
aggressive to yield a successful implementation of PCA;
however, the reduction of this threshold would have been
impractical for this project. Additionally, the normalization

050100150200k05101520εTEST,PCA/εTESTAn Application of Machine Learning to Native Advertisements

Kevin Grogan, Quinlan Jung

Abstract
Machine learning algorithms are applied to data aggregated from the internet to classify webpages as native advertisements or
core content. Four diﬀerent classiﬁers are employed: logistic regression, Poisson naive Bayes, multinomial naive Bayes, and
support vector machine. The features are selected using a threshold based on the mutual information statistic; additionally, the
tf-idf and tf-bns transformations are applied to the training set. Principal component analysis is examined as an a priori feature
reduction algorithm. The reduction in the dimensionality of the training set through principal component analysis is found to
increase the test error of the logistic regression classiﬁer to an unacceptable level. The tf-idf and tf-bns transformations are
found to successfully improve the quality of the classiﬁers. The logistic regression classiﬁer is shown to produce the lowest test
error of all the classiﬁers examined.

1. Introduction

Native advertisements seek to mimic the core content of
websites in order to minimize the disruption to the users
viewing experience. Ideally, the advertisements are as fun
and informative as the website’s core content. However,
they are often intrusive and decrease the likelihood of re-
peat viewings. Therefore, advertisers would greatly beneﬁt
from a model that could determine which content is an ad-
vertisement and which is not. Tricking such a model post
facto would imply that the advertisement blends well with
the core content of the website. Additionally, identiﬁcation
of key parameters that betray advertisements would help ad-
vertisers develop more organic content.

Hence, this project applies machine learning algorithms
to classify articles as native advertisements or core con-
tent. For this project, the naive Bayes, support vector ma-
chine (SVM), and logistic regression algorithms as classi-
ﬁers are employed. Additionally, principal component anal-
ysis (PCA) to examine its utility in feature reduction.

2. Related Work

Naive Bayes and support vector machines have been the
traditional approaches to solving text classiﬁcation prob-
lems. When the dataset is extremely large, logistic regres-
sion is sometimes used to approximate an SVM because it is
computationally more eﬃcient [4].

Naive Bayes performs moderately well in text classi-
ﬁcation. Of the diﬀerent event models, the multivariate
Bernoulli performs well with small vocabulary sizes, but
the multinomial usually performs better at larger vocabulary
sizes, providing on average a 27% reduction in error over the
multivariate Bernoulli model at any vocabulary size [3].

SVMs consistently achieve good performance on text
classiﬁcation tasks, outperforming many existing meth-
ods [5]. Because most text classiﬁcation problems are lin-
early separable, a linear kernel is usually recommended, as

it is faster and easier to optimize [6]. Other popular kernels,
such as the string kernel, show positive results on modestly
sized datasets. For larger documents and datasets, approx-
imation techniques need to be used because non-linear ker-
nels are computationally expensive [7].

3. Data Set and Features

The training set consists of 300,000 HTML pages with
class labels provided by kaggle [2]. Since the training set
consited of raw html, the following algorithm is used to tok-
enize the training set:

for each HTML document:

extract the text in the HTML body,

stripping away tags

remove all punctuation
remove all stop words
merge similar words

While embedded javascript and links in the HTML head
along with punctuation in the body could provide more in-
sight to determine whether a document is an advertisement,
this hidden information is not apparent to the audience and is
unlikely to betray a native advertisement. Hence, this project
focuses on the text in the body.

Stop words such as “a”, “and”, and “the”, are removed as
an a priori method of feature reduction. Additionally, words
with the same roots were grouped by detecting common suf-
ﬁxes such as “-ly” and “ing”. Implicitly, this assumes that
the covariance between these words is suﬃciently high that
the dimensional reduction will have a negligible eﬀect on the
bias of the models.

The feature set, or dictionary, is the vocabulary found in
the training set. The collection of tokenized training docu-
ments is given to a CountVectorizer1, and it returns an n × m
frequency matrix, A, where n is the dictionary size, and m
is the number of documents. The entries Ai j denote the fre-
quency of the word at index i found in document j.

Email addresses: kgrogan@stanford.edu (Kevin Grogan),

quinlanj@cs.stanford.edu (Quinlan Jung)
CS229: Final Project

1A python class in the scikit-learn library that converts a collection of

text documents to a matrix of token counts

December 12, 2015

3.1. Term Frequency/Inverse Document Frequency

Term frequency/inverse document frequency (tf-idf) is a
heuristic metric used in text classiﬁcation to yield better re-
sults with classiﬁers. The functional form of the tf-idf statis-
tic used for this project is

tf-idfi j = tfi j × idfi

= log(1 + Ai, j) × log

(cid:33)

(cid:32) N

ni

(1)

,

4. Methods

However, the bounding of this metric indicates that it may
underpredict the contribution of a feature to the classiﬁcation
of a data set. Although, for large feature sets such as the
text classiﬁcation used for this project, mutual information
is posited to be a reasonable and computationally eﬃcient
criterion to eliminate features and reduce the variance error
for a classiﬁer.

where N is the size of the training set, and ni is overall fre-
quency of word i in the training set.
tf-idf makes the rea-
sonable assertion that the importance of a word in a docu-
ment must be monotonically dependent on the frequency of
that word (i.e., the tf statistic). However, this term should
be discounted when it is omnipresent in the corpus (i.e., the
idf statistic); therefore, words with little embedded mean-
ing and mostly syntactical relevance (such prepositions, con-
junctions, and articles) are neglected by the classiﬁers under
this transformation.

3.2. Term Frequency/Bi-Normal Separation

While tf-idf is the most widely used representation for
real-valued feature vectors for text classiﬁcation problems,
idf is oblivious to the training class labels and naturally
scales some features inappropriately. Alternatively, idf can
be replaced with bi-normal separation (bns), which has been
previously found to be successful at ranking words for fea-
ture selection ﬁltering [9]. The functional form of the tf-bns
statistic used is

tf-bnsi j = tfi j × bnsi

= log(1 + Ai, j) × |F−1(tpr) − F−1(fpr)|,

(2)

Where the true positive rate is given by tpr = P(word | pos-
itive class), the false positive rate is given by fpr= P(word
| positive class), and F−1 is the inverse Normal cumulative
distribution function.

3.3. Mutual Information

Mutual information provides a metric to assign an impor-
tance a given feature to the classiﬁcation of the document. It
is deﬁned as

MI(xi, y) =

p(xi, y) log

p(xi, y)
p(xi)p(y) .

(3)

(cid:88)

(cid:88)

xi∈{0,1}

y∈{0,1}

Rewriting Eq. 3 using Jensen’s inequality and applying the
relation p(xi, y) = p(y|xi)p(xi), the mutual information of a
feature, xi, is shown to be bounded by

MI(xi, y) ≤ log E

,

(4)

(cid:34) p(y|xi)

(cid:35)

p(y)

where the expectation is taken over the joint probability dis-
tribution of xi and y. This furthers the intuition that the mu-
tual information is a bounded and normalized metric related
to the expected contribution of a feature to the successful
prediction of a class label over simply the probability of the
class label.

4.1. Principal Component Analysis

Principal component analysis seeks to reduce the dimen-
sionality of a feature set. This is done by ﬁnding a unit vec-
tor u that maximizes the variance in the data when projected
onto the vector. Formally, this can be shown to be given by

u = argmax
u:(cid:107)u(cid:107)=1

uTΣu ,

(5)

where Σ is the covariance matrix. Through the method of La-
grange multipliers, this relation can be demonstrated to yield
the eigenvector corresponding to the maximum eigenvalue
of the covariance matrix: u = vi : λi = max{λ1, ..., λn}.

Hence, an eigenvalue decomposition of the covariance
matrix gives a basis of eigenvectors, where the variance of
the projection onto each eigenvector is in proportion to the
corresponding eigenvalue.

Furthermore, dimensional reduction of the training data
may be obtained by a projection onto the eigenvector basis:

Bk = VT

k C,

(6)
where Bk ∈ Rk×m is the reduced representation of the
columns of matrix C ∈ Rn×m to k ≤ n dimensions, and
Vk ∈ Rn×k is a matrix of the ﬁrst k principal eigenvectors.
For this project, the matrix C is built from the tf-idf trans-
formation of the frequency matrix detailed by Eq. 1, and the
rows of C are normalized to have zero mean and unit vari-
ance.
Additionally, since the initial set of features yield a covari-
ance matrix with O(109) elements, sparsity is enforced using
the Cauchy-Schwarz inequality

Cov(ci, c j)

0

Σi j =

Var(ci)Var(c j) > d

for [Cov(ci,c j)]2
o.t.w.

(7)

where ci, c j are row vectors in C, and d is a threshold parame-
ter set to 0.8. This parameter to yield a matrix with a density
just so that the matrix could be manipulated in a practical
manner on a laptop.

4.2. Logistic Regression

A logistic regression assumes that the conditional prob-
ability of the labeling follows a Bernoulli random variable
(i.e, y|x; θ ∼ Bernoulli(φ)). By deﬁning the hypothesis
function as the expectation of the conditional probability,
hθ(x) = E[y|x; θ] and assuming a linear relation between
the feature vector and parameter vecotor, θ, the hypothesis
function is correspondingly found to be given by the sigmoid
function:

hθ(x) =

2

1

1 − exp(θTx) ,

(8)

n(cid:89)

where the parameter vector is found directly through a max-
imum liklihood estimate (MLE). The MLE of the logistic
regression is found using batch gradient ascent where the
learning rate is found empirically after several trials; stochas-
tic gradient ascent was attempted, but the parameter vector
was found to be unacceptably uncertain.

4.3. Naive Bayes

The Naive Bayes algorithm utilizes Bayes rules to predict

a label given a feature set:
p(y = 1|x) =

(cid:80)
p(x|y = 1)p(y)
y∈{0,1} p(x|y)p(y) ,

(9)
where the prior distributions p(x|y = 1) and p(y) must be
modeled. The hypothesis function takes the form hθ(xi) =
p(y = 1|x) for the naive Bayes algorithm. Additionally, the
prior p(x|y) is assumed to be conditionally independent:

p(x|y) =

p(xi|y) .

(10)

i=1

The prior p(y) is modeled using a binomial distribution,
while p(x|y) is modeled using a multinomial distribution or
a Poisson distribution.

A Poisson distribution is considered under the rational that
the number of appearances of a word in a ﬁxed length doc-
ument is well-approximated by a Poisson process. The prior
for a Poisson distribution is given by

i j exp(−λi j)
λxi

p(xi|y = j) =

xi!

(11)
where λi j is E[xi|y = j]. Additionally, it can be shown that
Eq. 9 can be written in the form of a sigmoid function given
by Eq. 8, making implementation straightforward. The pa-
+(cid:80)n
rameter vector in this case is given by
i=1 λi0 − λi1

log φy

φy−1

log(λk1/λk0)

for k = 0
o.t.w.

θk =

(12)

where φy = p(y). The MLE of λi j is given by the sample
mean of each feature corresponding to the labeling y = j.

Assuming a Poisson distribution increases the computa-
tional eﬃciency of the classiﬁer but also embeds stronger
assumptions into the model. The comparative performance
of the classiﬁer will be examined in the Sec. 5.

4.4. Support Vector Machine

The SVM attempts to ﬁnd the maximum-margin hyper-
plane that separates the dataset in a higher dimensional fea-
ture space. Finding this optimal margin reduces to solving
the following convex optimization problem:

||ω||2 + C m(cid:88)

ξi

i=1

minγ,ω,b

1
2

Subject to the constraints:

s.t. y(i)(ωT x(i)) + b) ≥ 1 − ξi, i = 1, ..., m
ξi ≥ 0, i = 1, ..., m

(13)

(14)

Where ξi allows for the ’slack’ in the event that the data is
not linearly separable. Originally, an unoptimized L2-norm

3

linear SVM was employed, but it proved to be too slow due
the high cardinality of the training set. The SVM was modi-
ﬁed to use a dual coordinate descent method [8] that deﬁnes
the primal problem as follows:

minωm,ξi

1
2

m

(cid:88)

||ωm||2 + C(cid:88)

ξi
i − ξi ∀m, i

i

(15)

Where em
i

s.t. ωT x(i) − ωT x(i) ≥ em
= 1 − σm and σm = 1 if y = m, σm = 0
if yi (cid:44) m. This method reaches an -accurate solution in
O(log(1/) iterations. Similar to what is done with Naive
Bayes, tf-bns transformation is performed on the vocabulary
found in the training set. The SVM is set to have parameters
C = 0.23 and ξ = 1.0. The optimal C and ξ are found using
k-fold cross validation, where k = 10.

5. Discussion

5.1. Feature Reduction using Principal Component Analysis
As described in Sec. 4.1, principal component analysis re-
duces the dimensionality of the feature set by maximizing
the variance of the projection of the data onto a set of basis
vectors. Figure 1 shows the eﬀect of the reduced feature set
on the test and training errors.

Figure 1: The eﬀect of the reduction in dimensionality through PCA on
the test error. The error is normalized by that of the full-rank representation
of the data set. Error is found using the logistic regression classiﬁer and
k-folds cross validation.

The ﬁgure shows that error is signiﬁcantly increased when
the dimensionality of the data is reduced through principal
component analysis. Hence, while PCA reduces the feature
set, the substantial increase in bias overshadows any reduc-
tion in variance. Additionally, PCA is an unsupervised learn-
ing algorithm that is agnostic to the class labels. Hence, the
belief that a reduction of dimensionality through this algo-
rithm will provide lower test error may be Pollyannaish.

The threshold to enforce the sparsity may have been overly
aggressive to yield a successful implementation of PCA;
however, the reduction of this threshold would have been
impractical for this project. Additionally, the normalization

050100150200k05101520εTEST,PCA/εTESTFigure 2: Depiction of the tf-idf transformation. Words are colored and by the tf-idf value given by Eq. 1.

of the feature vectors may have reduced the useful informa-
tion contained within causing some of the high training er-
ror observed; however, not normalizing the data would have
certainly reduced the clustering of the data diminishing the
eﬃcacy of employing PCA.

As an interesting aside, inspection of the top words cor-
responding to diﬀerent eigenvectors seem to align with the
suspected orgin of the article; for instance, words with a
British spelling such as “organisation” and “stabiliser” corre-
sponded to the third principal eigenvector. Hence, PCA may
have more promise as a clustering algorithm.

5.2. Eﬀect of the tf-idf Transformation

The eﬀect of the tf-idf transformation discussed in Sec. 3.1
is shown in Fig. 2. As illustrated in the ﬁgure, the tf-idf
transformation naturally selects words with a high relevance
to the document. Words such as “this” and “have” are shown
to have a high idf, while the term frequency of the high
idf terms are document dependent.
Interestingly, “cialis”
is shown to have a large tf and idf in this matrix indicat-
ing a high importance; considering the connotation, this term
likely appeared in an advertisement. Additionally, the tf-idf
transformation is found to reduce the test error by 36% for
the logistic regression classiﬁer.

5.3. Application of the tf-bns Transformation to Multinomial

Naive Bayes

Originally, the tf-idf weighting scheme was used on fea-
tures of multinomial naive Bayes. However, the positive pre-
dicted value (ppv)

ppv =

True Positives

True Positives + False Positives

(16)

is 0, as it is unable to predict even a single native adver-
tisement correctly. This is because idf is oblivious to the
class labels in the training set, which can lead to inappropri-
ate scaling. Upon switching to tf-bns, an increase of average
ppv to 0.75, where n = 3000, is observed.

5.4. Feature Reduction through Mutual Information

As discussed in Sec. 3.3, the mutual information of a fea-
ture quantiﬁes the usefulness of a feature in predicting a clas-
siﬁcation. Figure 3 shows the eﬀect of feature reduction via

Figure 3: Comparison of the test and training error as a function of the
threshold parameter based on the mutual information. Error is determined
via k-folds cross validation. Error is shown for the logistic regression clas-
siﬁer. The tf-idf transformation is applied.

the mutual information on the error. The abscissa of the ﬁg-
ure corresponds to the threshold parameter t:

t(xi, y) =

MI(xi, y) − min j MI(x j, y)

max j MI(x j, y) − min j MI(x j, y)

(17)

where all features such that t < tTHRES are disregarded.

As shown in the ﬁgure, the error ﬁrst reduces for an in-
creasing threshold (i.e., as more features are eliminated), and
experiences a minimum near tTRES = 0.7; this is likely due
to the decrease in the variance as features are reduced. Sub-
sequently, the error begins to increase as bias error likely
becomes dominant. Note that at high thresholds, training er-
ror and test error becomes similar indicating a reduction in
variance. Hence, the metric of mutual information is shown
to reduce the test error in a computationally feasible manner.

5.5. Comparison of the Classiﬁers

A comparison of the test and training errors for the clas-
siﬁers used in this project is presented in Fig. 4. The logis-
tic regression is shown to yield the lowest test error of the
classiﬁers at the highest cardinality of the training set (i.e.,

4

100101102103idf100101102103tf234567atoncontactisusbutforithavethroughyeardowngreatstillloveworldwouldnowwhichimagephotowomenmonthsunitedsamelessreplyshecarehairyeshappensprobablytrendingyoureprogramcaeatstockmediumwalkingreligionpermanentlimiteluniversejanautounatlantataleexposeddivorcecakefactorsempireresolutiontonightdividedoughswinginitiativesroastednjskiingautuesumashesinstrumentalextendsoccupationaldelhinichtmysteriespdtﬁdigitslugliohostilitybriberykimchilagermonopolyquiltsmaddowvalenciacujahrevoltssequelsintegersgreenpeacebarefootspectralpentruthiscialisdashboardslaugh0.10.20.30.40.50.60.70.80.9tTHRES0.040.050.060.070.080.090.10.11εεTRAINεTESTAn Application of Machine Learning to Native Advertisements

Kevin Grogan, Quinlan Jung

Abstract
Machine learning algorithms are applied to data aggregated from the internet to classify webpages as native advertisements or
core content. Four diﬀerent classiﬁers are employed: logistic regression, Poisson naive Bayes, multinomial naive Bayes, and
support vector machine. The features are selected using a threshold based on the mutual information statistic; additionally, the
tf-idf and tf-bns transformations are applied to the training set. Principal component analysis is examined as an a priori feature
reduction algorithm. The reduction in the dimensionality of the training set through principal component analysis is found to
increase the test error of the logistic regression classiﬁer to an unacceptable level. The tf-idf and tf-bns transformations are
found to successfully improve the quality of the classiﬁers. The logistic regression classiﬁer is shown to produce the lowest test
error of all the classiﬁers examined.

1. Introduction

Native advertisements seek to mimic the core content of
websites in order to minimize the disruption to the users
viewing experience. Ideally, the advertisements are as fun
and informative as the website’s core content. However,
they are often intrusive and decrease the likelihood of re-
peat viewings. Therefore, advertisers would greatly beneﬁt
from a model that could determine which content is an ad-
vertisement and which is not. Tricking such a model post
facto would imply that the advertisement blends well with
the core content of the website. Additionally, identiﬁcation
of key parameters that betray advertisements would help ad-
vertisers develop more organic content.

Hence, this project applies machine learning algorithms
to classify articles as native advertisements or core con-
tent. For this project, the naive Bayes, support vector ma-
chine (SVM), and logistic regression algorithms as classi-
ﬁers are employed. Additionally, principal component anal-
ysis (PCA) to examine its utility in feature reduction.

2. Related Work

Naive Bayes and support vector machines have been the
traditional approaches to solving text classiﬁcation prob-
lems. When the dataset is extremely large, logistic regres-
sion is sometimes used to approximate an SVM because it is
computationally more eﬃcient [4].

Naive Bayes performs moderately well in text classi-
ﬁcation. Of the diﬀerent event models, the multivariate
Bernoulli performs well with small vocabulary sizes, but
the multinomial usually performs better at larger vocabulary
sizes, providing on average a 27% reduction in error over the
multivariate Bernoulli model at any vocabulary size [3].

SVMs consistently achieve good performance on text
classiﬁcation tasks, outperforming many existing meth-
ods [5]. Because most text classiﬁcation problems are lin-
early separable, a linear kernel is usually recommended, as

it is faster and easier to optimize [6]. Other popular kernels,
such as the string kernel, show positive results on modestly
sized datasets. For larger documents and datasets, approx-
imation techniques need to be used because non-linear ker-
nels are computationally expensive [7].

3. Data Set and Features

The training set consists of 300,000 HTML pages with
class labels provided by kaggle [2]. Since the training set
consited of raw html, the following algorithm is used to tok-
enize the training set:

for each HTML document:

extract the text in the HTML body,

stripping away tags

remove all punctuation
remove all stop words
merge similar words

While embedded javascript and links in the HTML head
along with punctuation in the body could provide more in-
sight to determine whether a document is an advertisement,
this hidden information is not apparent to the audience and is
unlikely to betray a native advertisement. Hence, this project
focuses on the text in the body.

Stop words such as “a”, “and”, and “the”, are removed as
an a priori method of feature reduction. Additionally, words
with the same roots were grouped by detecting common suf-
ﬁxes such as “-ly” and “ing”. Implicitly, this assumes that
the covariance between these words is suﬃciently high that
the dimensional reduction will have a negligible eﬀect on the
bias of the models.

The feature set, or dictionary, is the vocabulary found in
the training set. The collection of tokenized training docu-
ments is given to a CountVectorizer1, and it returns an n × m
frequency matrix, A, where n is the dictionary size, and m
is the number of documents. The entries Ai j denote the fre-
quency of the word at index i found in document j.

Email addresses: kgrogan@stanford.edu (Kevin Grogan),

quinlanj@cs.stanford.edu (Quinlan Jung)
CS229: Final Project

1A python class in the scikit-learn library that converts a collection of

text documents to a matrix of token counts

December 12, 2015

3.1. Term Frequency/Inverse Document Frequency

Term frequency/inverse document frequency (tf-idf) is a
heuristic metric used in text classiﬁcation to yield better re-
sults with classiﬁers. The functional form of the tf-idf statis-
tic used for this project is

tf-idfi j = tfi j × idfi

= log(1 + Ai, j) × log

(cid:33)

(cid:32) N

ni

(1)

,

4. Methods

However, the bounding of this metric indicates that it may
underpredict the contribution of a feature to the classiﬁcation
of a data set. Although, for large feature sets such as the
text classiﬁcation used for this project, mutual information
is posited to be a reasonable and computationally eﬃcient
criterion to eliminate features and reduce the variance error
for a classiﬁer.

where N is the size of the training set, and ni is overall fre-
quency of word i in the training set.
tf-idf makes the rea-
sonable assertion that the importance of a word in a docu-
ment must be monotonically dependent on the frequency of
that word (i.e., the tf statistic). However, this term should
be discounted when it is omnipresent in the corpus (i.e., the
idf statistic); therefore, words with little embedded mean-
ing and mostly syntactical relevance (such prepositions, con-
junctions, and articles) are neglected by the classiﬁers under
this transformation.

3.2. Term Frequency/Bi-Normal Separation

While tf-idf is the most widely used representation for
real-valued feature vectors for text classiﬁcation problems,
idf is oblivious to the training class labels and naturally
scales some features inappropriately. Alternatively, idf can
be replaced with bi-normal separation (bns), which has been
previously found to be successful at ranking words for fea-
ture selection ﬁltering [9]. The functional form of the tf-bns
statistic used is

tf-bnsi j = tfi j × bnsi

= log(1 + Ai, j) × |F−1(tpr) − F−1(fpr)|,

(2)

Where the true positive rate is given by tpr = P(word | pos-
itive class), the false positive rate is given by fpr= P(word
| positive class), and F−1 is the inverse Normal cumulative
distribution function.

3.3. Mutual Information

Mutual information provides a metric to assign an impor-
tance a given feature to the classiﬁcation of the document. It
is deﬁned as

MI(xi, y) =

p(xi, y) log

p(xi, y)
p(xi)p(y) .

(3)

(cid:88)

(cid:88)

xi∈{0,1}

y∈{0,1}

Rewriting Eq. 3 using Jensen’s inequality and applying the
relation p(xi, y) = p(y|xi)p(xi), the mutual information of a
feature, xi, is shown to be bounded by

MI(xi, y) ≤ log E

,

(4)

(cid:34) p(y|xi)

(cid:35)

p(y)

where the expectation is taken over the joint probability dis-
tribution of xi and y. This furthers the intuition that the mu-
tual information is a bounded and normalized metric related
to the expected contribution of a feature to the successful
prediction of a class label over simply the probability of the
class label.

4.1. Principal Component Analysis

Principal component analysis seeks to reduce the dimen-
sionality of a feature set. This is done by ﬁnding a unit vec-
tor u that maximizes the variance in the data when projected
onto the vector. Formally, this can be shown to be given by

u = argmax
u:(cid:107)u(cid:107)=1

uTΣu ,

(5)

where Σ is the covariance matrix. Through the method of La-
grange multipliers, this relation can be demonstrated to yield
the eigenvector corresponding to the maximum eigenvalue
of the covariance matrix: u = vi : λi = max{λ1, ..., λn}.

Hence, an eigenvalue decomposition of the covariance
matrix gives a basis of eigenvectors, where the variance of
the projection onto each eigenvector is in proportion to the
corresponding eigenvalue.

Furthermore, dimensional reduction of the training data
may be obtained by a projection onto the eigenvector basis:

Bk = VT

k C,

(6)
where Bk ∈ Rk×m is the reduced representation of the
columns of matrix C ∈ Rn×m to k ≤ n dimensions, and
Vk ∈ Rn×k is a matrix of the ﬁrst k principal eigenvectors.
For this project, the matrix C is built from the tf-idf trans-
formation of the frequency matrix detailed by Eq. 1, and the
rows of C are normalized to have zero mean and unit vari-
ance.
Additionally, since the initial set of features yield a covari-
ance matrix with O(109) elements, sparsity is enforced using
the Cauchy-Schwarz inequality

Cov(ci, c j)

0

Σi j =

Var(ci)Var(c j) > d

for [Cov(ci,c j)]2
o.t.w.

(7)

where ci, c j are row vectors in C, and d is a threshold parame-
ter set to 0.8. This parameter to yield a matrix with a density
just so that the matrix could be manipulated in a practical
manner on a laptop.

4.2. Logistic Regression

A logistic regression assumes that the conditional prob-
ability of the labeling follows a Bernoulli random variable
(i.e, y|x; θ ∼ Bernoulli(φ)). By deﬁning the hypothesis
function as the expectation of the conditional probability,
hθ(x) = E[y|x; θ] and assuming a linear relation between
the feature vector and parameter vecotor, θ, the hypothesis
function is correspondingly found to be given by the sigmoid
function:

hθ(x) =

2

1

1 − exp(θTx) ,

(8)

n(cid:89)

where the parameter vector is found directly through a max-
imum liklihood estimate (MLE). The MLE of the logistic
regression is found using batch gradient ascent where the
learning rate is found empirically after several trials; stochas-
tic gradient ascent was attempted, but the parameter vector
was found to be unacceptably uncertain.

4.3. Naive Bayes

The Naive Bayes algorithm utilizes Bayes rules to predict

a label given a feature set:
p(y = 1|x) =

(cid:80)
p(x|y = 1)p(y)
y∈{0,1} p(x|y)p(y) ,

(9)
where the prior distributions p(x|y = 1) and p(y) must be
modeled. The hypothesis function takes the form hθ(xi) =
p(y = 1|x) for the naive Bayes algorithm. Additionally, the
prior p(x|y) is assumed to be conditionally independent:

p(x|y) =

p(xi|y) .

(10)

i=1

The prior p(y) is modeled using a binomial distribution,
while p(x|y) is modeled using a multinomial distribution or
a Poisson distribution.

A Poisson distribution is considered under the rational that
the number of appearances of a word in a ﬁxed length doc-
ument is well-approximated by a Poisson process. The prior
for a Poisson distribution is given by

i j exp(−λi j)
λxi

p(xi|y = j) =

xi!

(11)
where λi j is E[xi|y = j]. Additionally, it can be shown that
Eq. 9 can be written in the form of a sigmoid function given
by Eq. 8, making implementation straightforward. The pa-
+(cid:80)n
rameter vector in this case is given by
i=1 λi0 − λi1

log φy

φy−1

log(λk1/λk0)

for k = 0
o.t.w.

θk =

(12)

where φy = p(y). The MLE of λi j is given by the sample
mean of each feature corresponding to the labeling y = j.

Assuming a Poisson distribution increases the computa-
tional eﬃciency of the classiﬁer but also embeds stronger
assumptions into the model. The comparative performance
of the classiﬁer will be examined in the Sec. 5.

4.4. Support Vector Machine

The SVM attempts to ﬁnd the maximum-margin hyper-
plane that separates the dataset in a higher dimensional fea-
ture space. Finding this optimal margin reduces to solving
the following convex optimization problem:

||ω||2 + C m(cid:88)

ξi

i=1

minγ,ω,b

1
2

Subject to the constraints:

s.t. y(i)(ωT x(i)) + b) ≥ 1 − ξi, i = 1, ..., m
ξi ≥ 0, i = 1, ..., m

(13)

(14)

Where ξi allows for the ’slack’ in the event that the data is
not linearly separable. Originally, an unoptimized L2-norm

3

linear SVM was employed, but it proved to be too slow due
the high cardinality of the training set. The SVM was modi-
ﬁed to use a dual coordinate descent method [8] that deﬁnes
the primal problem as follows:

minωm,ξi

1
2

m

(cid:88)

||ωm||2 + C(cid:88)

ξi
i − ξi ∀m, i

i

(15)

Where em
i

s.t. ωT x(i) − ωT x(i) ≥ em
= 1 − σm and σm = 1 if y = m, σm = 0
if yi (cid:44) m. This method reaches an -accurate solution in
O(log(1/) iterations. Similar to what is done with Naive
Bayes, tf-bns transformation is performed on the vocabulary
found in the training set. The SVM is set to have parameters
C = 0.23 and ξ = 1.0. The optimal C and ξ are found using
k-fold cross validation, where k = 10.

5. Discussion

5.1. Feature Reduction using Principal Component Analysis
As described in Sec. 4.1, principal component analysis re-
duces the dimensionality of the feature set by maximizing
the variance of the projection of the data onto a set of basis
vectors. Figure 1 shows the eﬀect of the reduced feature set
on the test and training errors.

Figure 1: The eﬀect of the reduction in dimensionality through PCA on
the test error. The error is normalized by that of the full-rank representation
of the data set. Error is found using the logistic regression classiﬁer and
k-folds cross validation.

The ﬁgure shows that error is signiﬁcantly increased when
the dimensionality of the data is reduced through principal
component analysis. Hence, while PCA reduces the feature
set, the substantial increase in bias overshadows any reduc-
tion in variance. Additionally, PCA is an unsupervised learn-
ing algorithm that is agnostic to the class labels. Hence, the
belief that a reduction of dimensionality through this algo-
rithm will provide lower test error may be Pollyannaish.

The threshold to enforce the sparsity may have been overly
aggressive to yield a successful implementation of PCA;
however, the reduction of this threshold would have been
impractical for this project. Additionally, the normalization

050100150200k05101520εTEST,PCA/εTESTFigure 2: Depiction of the tf-idf transformation. Words are colored and by the tf-idf value given by Eq. 1.

of the feature vectors may have reduced the useful informa-
tion contained within causing some of the high training er-
ror observed; however, not normalizing the data would have
certainly reduced the clustering of the data diminishing the
eﬃcacy of employing PCA.

As an interesting aside, inspection of the top words cor-
responding to diﬀerent eigenvectors seem to align with the
suspected orgin of the article; for instance, words with a
British spelling such as “organisation” and “stabiliser” corre-
sponded to the third principal eigenvector. Hence, PCA may
have more promise as a clustering algorithm.

5.2. Eﬀect of the tf-idf Transformation

The eﬀect of the tf-idf transformation discussed in Sec. 3.1
is shown in Fig. 2. As illustrated in the ﬁgure, the tf-idf
transformation naturally selects words with a high relevance
to the document. Words such as “this” and “have” are shown
to have a high idf, while the term frequency of the high
idf terms are document dependent.
Interestingly, “cialis”
is shown to have a large tf and idf in this matrix indicat-
ing a high importance; considering the connotation, this term
likely appeared in an advertisement. Additionally, the tf-idf
transformation is found to reduce the test error by 36% for
the logistic regression classiﬁer.

5.3. Application of the tf-bns Transformation to Multinomial

Naive Bayes

Originally, the tf-idf weighting scheme was used on fea-
tures of multinomial naive Bayes. However, the positive pre-
dicted value (ppv)

ppv =

True Positives

True Positives + False Positives

(16)

is 0, as it is unable to predict even a single native adver-
tisement correctly. This is because idf is oblivious to the
class labels in the training set, which can lead to inappropri-
ate scaling. Upon switching to tf-bns, an increase of average
ppv to 0.75, where n = 3000, is observed.

5.4. Feature Reduction through Mutual Information

As discussed in Sec. 3.3, the mutual information of a fea-
ture quantiﬁes the usefulness of a feature in predicting a clas-
siﬁcation. Figure 3 shows the eﬀect of feature reduction via

Figure 3: Comparison of the test and training error as a function of the
threshold parameter based on the mutual information. Error is determined
via k-folds cross validation. Error is shown for the logistic regression clas-
siﬁer. The tf-idf transformation is applied.

the mutual information on the error. The abscissa of the ﬁg-
ure corresponds to the threshold parameter t:

t(xi, y) =

MI(xi, y) − min j MI(x j, y)

max j MI(x j, y) − min j MI(x j, y)

(17)

where all features such that t < tTHRES are disregarded.

As shown in the ﬁgure, the error ﬁrst reduces for an in-
creasing threshold (i.e., as more features are eliminated), and
experiences a minimum near tTRES = 0.7; this is likely due
to the decrease in the variance as features are reduced. Sub-
sequently, the error begins to increase as bias error likely
becomes dominant. Note that at high thresholds, training er-
ror and test error becomes similar indicating a reduction in
variance. Hence, the metric of mutual information is shown
to reduce the test error in a computationally feasible manner.

5.5. Comparison of the Classiﬁers

A comparison of the test and training errors for the clas-
siﬁers used in this project is presented in Fig. 4. The logis-
tic regression is shown to yield the lowest test error of the
classiﬁers at the highest cardinality of the training set (i.e.,

4

100101102103idf100101102103tf234567atoncontactisusbutforithavethroughyeardowngreatstillloveworldwouldnowwhichimagephotowomenmonthsunitedsamelessreplyshecarehairyeshappensprobablytrendingyoureprogramcaeatstockmediumwalkingreligionpermanentlimiteluniversejanautounatlantataleexposeddivorcecakefactorsempireresolutiontonightdividedoughswinginitiativesroastednjskiingautuesumashesinstrumentalextendsoccupationaldelhinichtmysteriespdtﬁdigitslugliohostilitybriberykimchilagermonopolyquiltsmaddowvalenciacujahrevoltssequelsintegersgreenpeacebarefootspectralpentruthiscialisdashboardslaugh0.10.20.30.40.50.60.70.80.9tTHRES0.040.050.060.070.080.090.10.11εεTRAINεTESTfeature set.

2. The tf-idf transformation successfully augments words
that are pertinent to a document while discounting those
that are superﬂuous.

3. the tf-bns transformation showed an improvement over
tf-idf in the ppv metric when applied to the multinomial
naive Bayes model.

4. Feature reduction through the mutual information met-

ric improved the test error of the logistic model.

5. The logistic regression is found to yield the best training

error of the classiﬁers used in this project.

Additionally, the authors note that this report is not a na-
tive advertisement as determined by the logistic regression
classiﬁer.

Figure 4: Comparison of the test and training error as a function of cardi-
nality of the training set m. Solid lines denote the test error while dashed
lines denote the training error. The errors are deduced through a k-folds
cross validation.

7. Acknowledgment

The authors would like to thank Rakesh Ramesh for his

early contributions to this project.

References

[1] http://www.stumbleupon.com/
[2] https://www.kaggle.com/c/dato-native/data
[3] Andrew McCallum and Kamal Nigam. 1998. A comparison of event
models for Naive Bayes text classiﬁcation. In Proc. of the AAAI-98
Workshop on Learning for Text Categorization, pages 41–48.

[4] J. Zhang, R. Jin, Y. Yang, and A. Hauptmann. Modiﬁed logistic re-
gression: An approximation to SVM and its applications in large-scale
text categorization. In Twentieth International Conference on Machine
Learning, pages 472479, 2003.

[5] T. Joachims. Text categorization with support vector machines: Learn-
ing with many relevant features. In Claire Nedellec and Celine Rou-
veirol, editors, Proceedings of the European Conference on Machine
Learning, pages 137142, Berlin, 1998. Springer.

[6] http://www.svm-tutorial.com/2014/10/svm-linear-kernel-good-text-

classiﬁcation/

[7] H. Lodhi, J. Shawe-Taylor, N. Cristianini, and C. Watkins. Text clas-
siﬁcation using string kernels. In T. K. Leen, T. G. Dietterich, and V.
Tresp, editors, Adavances in Neural Information Processing Systems,
pages 563569, Cambridge, MA, 2001. MIT Press.

[8] S. Sathiya Keerthi, S. Sundararajan, Kai-Wei Chang, Cho-Jui Hsieh,
Chih-Jen Lin, A sequential dual method for large scale multi-class lin-
ear svms, Proceedings of the 14th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, August 24-27, 2008,
Las Vegas, Nevada, USA.

[9] George Forman, An extensive empirical study of feature selection met-
rics for text classiﬁcation, The Journal of Machine Learning Research,
3, 3/1/2003

m). This is posited to be due to the generality and there-
fore, adaptability of the classiﬁer. The logistic regression
is a generalization of the naive Bayes models and it is not
unexpected that it performs better since the parameters are
selected to directly maximize the liklihood function.

The Poisson naive Bayes model is shown to perform the
worst of the classiﬁes. This likely results from the strong
assumptions imposed by modeling each feature as a Pois-
son distributed random variable. Particularly for small car-
dinalities of the training set, the statistics are insuﬃciently
converged to provide a meaningful classiﬁer, and the error
is near 50%. However, the Poisson naive Bayes is found to
be the most computationally eﬃcient model performing 63×
faster than the logistic regression.

The support vector machine is not shown to generalize
particularly well for this problem. While the training error
is quite low (zero in some cases), the test error is moderate
- between that of the logistic regression and the multinomial
naive Bayes. This is likely due to the classiﬁer emphasizing
a few support vectors that determine the decision boundary.
All classiﬁers are shown to require at least 1000 training
examples to begin showing a regular slope in with respect to
the test error. In this regime, the logistic regression is shown
to have the steepest slope and can be seen to be approxi-
mately ﬁrst order.

6. Conclusions

Native advertisements are an eﬀective strategy to mar-
ket to consumers without damaging the core experience of
viewing a website.
It is posited that the quality of native
advertisements may be improved by using a machine learn-
ing model to diﬀerentiate advertisements from core content;
such would provide a metric for the quality of the adver-
tisement. Hence, several machine learning algorithms are
applied to the detection of native advertisements and the fol-
lowing conclusions are drawn:

1. Principal component analysis showed lackluster perfor-
mance as a means to reduce the dimensionality of the

5

102103104m10-410-310-210-1100εLogisticRegressionNaiveBayes:MultinomialNaiveBayes:PoissonSupportVectorMachine