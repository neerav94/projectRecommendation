Estimation of Causal Eﬀects from Observational Study

of Job Training Program

Dmitry Arkhangelsky and Rob Donnelly

darkhang@stanford.edu & rodonn@stanford.edu

1

Introduction

In the social sciences, researchers are often interested in measuring the eﬀect of a treatment
or intervention. For example, many labor economists are interested in measuring the eﬀect
of job training programs on increasing salaries or reducing unemployment. Measuring the
”causal eﬀect” of an intervention is relatively straight forward when the treatment is given
to randomly selected individuals. In this case the diﬀerence in the average outcome of the
treatment and control groups can be directly compared.

Unfortunately in many policy relevant situations economists do not have data from a ran-
domized controlled experiment. Instead they have data on the characteristics and outcomes
of a set of individuals who received the treatment, but no directly comparable control group.
We tried two approaches to estimating the eﬀect of a job training treatment on wages.
The ﬁrst approach predicts counterfactual wages for the treated individuals based on models
trained on a large sample from the general population. The second approach matches the
treated individuals to people from the general population based on propensity scores. We
ﬁnd that the second approach yields more plausible estimates of the eﬀect of job training.

2 Dataset, Features, and Preprocessing

We are using data from the National Supported Work Demonstration, which gave 12-18
months of job training to unemployed adults at 15 sites around the US. We have data on
185 men who participated in the job training, as well as 15,992 adults from the general
population1.
In order to be eligible to participate in the job training, an individual had
to meet certain eligibility requirements and had to volunteer. Because of this, the pool of
participants is very diﬀerent from a random sample of the population2. Previous work in
the economics literature has shown that treating the data as if it came from a randomized
controlled experiment leads to very biased estimates of the eﬀect of job training programs.
In particular since the average participant in a job training program has many disadvantages
in the job market relative to a randomly selected person from the whole population, naive
estimates often predict that the job training actually lowered wages.

Our features include the age, years of education, race, marriage status, annual income
1 and 2 years before training program, and unemployment status 1 and 2 years before job

1This data is from 1972, but is still relevant since it is a commonly used baseline in the economics literature

for methods of evaluating treatment eﬀects from non-experimental data.

2Participants in this job training program on average have less education and are more likely to have prior

criminal convictions or to be unemployed.

1

Estimation of Causal Eﬀects from Observational Study

of Job Training Program

Dmitry Arkhangelsky and Rob Donnelly

darkhang@stanford.edu & rodonn@stanford.edu

1

Introduction

In the social sciences, researchers are often interested in measuring the eﬀect of a treatment
or intervention. For example, many labor economists are interested in measuring the eﬀect
of job training programs on increasing salaries or reducing unemployment. Measuring the
”causal eﬀect” of an intervention is relatively straight forward when the treatment is given
to randomly selected individuals. In this case the diﬀerence in the average outcome of the
treatment and control groups can be directly compared.

Unfortunately in many policy relevant situations economists do not have data from a ran-
domized controlled experiment. Instead they have data on the characteristics and outcomes
of a set of individuals who received the treatment, but no directly comparable control group.
We tried two approaches to estimating the eﬀect of a job training treatment on wages.
The ﬁrst approach predicts counterfactual wages for the treated individuals based on models
trained on a large sample from the general population. The second approach matches the
treated individuals to people from the general population based on propensity scores. We
ﬁnd that the second approach yields more plausible estimates of the eﬀect of job training.

2 Dataset, Features, and Preprocessing

We are using data from the National Supported Work Demonstration, which gave 12-18
months of job training to unemployed adults at 15 sites around the US. We have data on
185 men who participated in the job training, as well as 15,992 adults from the general
population1.
In order to be eligible to participate in the job training, an individual had
to meet certain eligibility requirements and had to volunteer. Because of this, the pool of
participants is very diﬀerent from a random sample of the population2. Previous work in
the economics literature has shown that treating the data as if it came from a randomized
controlled experiment leads to very biased estimates of the eﬀect of job training programs.
In particular since the average participant in a job training program has many disadvantages
in the job market relative to a randomly selected person from the whole population, naive
estimates often predict that the job training actually lowered wages.

Our features include the age, years of education, race, marriage status, annual income
1 and 2 years before training program, and unemployment status 1 and 2 years before job

1This data is from 1972, but is still relevant since it is a commonly used baseline in the economics literature

for methods of evaluating treatment eﬀects from non-experimental data.

2Participants in this job training program on average have less education and are more likely to have prior

criminal convictions or to be unemployed.

1

training. We normalized all of our variables by subtracting the mean and dividing by the
standard deviation. To engineer additional features we also tried including cross products of
the base features in our dataset up to degree 3.

3 Models and Results

We used two distinct strategies for estimating the treatment eﬀect. Our ﬁrst approach is a
more straightforward one – we use several models to predict the ﬁnal wages of each individual
in the large sample from the general population. We then used these models to predict wages
each individual would have gotten if they hadn’t received training. The diﬀerence among the
individuals who received training between their actual ﬁnal wage and their predicted ﬁnal
wage, can be used as an estimate of the change in wage caused by the job training, i.e. a
casual eﬀect.

The second approach comes from the program evaluation literature. First, we estimate
the propensity score – conditional probability of being in the treatment group – and then
compare the treatment outcomes between the treated and untreated individuals who have
similar predicted probabilities of receiving the treatment.

3.1 Regression models

linear regression, lasso regression3,
We tried several types of models for predicting wages:
SVM (with linear, polynomial, and radial kernels), and random forest models. We choose
parameter values for each model using a grid search with 10-fold cross validation using Root
Mean Squared Error (RMSE) as our selection criteria. We evaluated performance on a 10%
hold-out set. The exponents following a model’s name indicate the degree of the polynomial
interactions of features that were included4.

By looking at the learning curves for these models it became clear that all of the models
had high bias. We attempted unsuccessfully to get access to more detailed data with a
larger number of features for each individual. Adding polynomial interaction terms did not
substantially reduce test error.

3Linear regression with L1 regularization to reduce overﬁtting.
4E.g. a 2 means that we added new features corresponding to the product of every pair of features in the

base dataset.

2

Linear (degree 2)Linear (degree 1)Elastic (degree 2)Elastic (degree 3)Linear (degree 3)Random ForestSVM RadialSVM LinearSVM Polynomial0.600.650.700.750.80500010000150005000100001500050001000015000500010000150005000100001500050001000015000500010000150005000100001500050001000015000NobsRMSETypetraintestEstimation of Causal Eﬀects from Observational Study

of Job Training Program

Dmitry Arkhangelsky and Rob Donnelly

darkhang@stanford.edu & rodonn@stanford.edu

1

Introduction

In the social sciences, researchers are often interested in measuring the eﬀect of a treatment
or intervention. For example, many labor economists are interested in measuring the eﬀect
of job training programs on increasing salaries or reducing unemployment. Measuring the
”causal eﬀect” of an intervention is relatively straight forward when the treatment is given
to randomly selected individuals. In this case the diﬀerence in the average outcome of the
treatment and control groups can be directly compared.

Unfortunately in many policy relevant situations economists do not have data from a ran-
domized controlled experiment. Instead they have data on the characteristics and outcomes
of a set of individuals who received the treatment, but no directly comparable control group.
We tried two approaches to estimating the eﬀect of a job training treatment on wages.
The ﬁrst approach predicts counterfactual wages for the treated individuals based on models
trained on a large sample from the general population. The second approach matches the
treated individuals to people from the general population based on propensity scores. We
ﬁnd that the second approach yields more plausible estimates of the eﬀect of job training.

2 Dataset, Features, and Preprocessing

We are using data from the National Supported Work Demonstration, which gave 12-18
months of job training to unemployed adults at 15 sites around the US. We have data on
185 men who participated in the job training, as well as 15,992 adults from the general
population1.
In order to be eligible to participate in the job training, an individual had
to meet certain eligibility requirements and had to volunteer. Because of this, the pool of
participants is very diﬀerent from a random sample of the population2. Previous work in
the economics literature has shown that treating the data as if it came from a randomized
controlled experiment leads to very biased estimates of the eﬀect of job training programs.
In particular since the average participant in a job training program has many disadvantages
in the job market relative to a randomly selected person from the whole population, naive
estimates often predict that the job training actually lowered wages.

Our features include the age, years of education, race, marriage status, annual income
1 and 2 years before training program, and unemployment status 1 and 2 years before job

1This data is from 1972, but is still relevant since it is a commonly used baseline in the economics literature

for methods of evaluating treatment eﬀects from non-experimental data.

2Participants in this job training program on average have less education and are more likely to have prior

criminal convictions or to be unemployed.

1

training. We normalized all of our variables by subtracting the mean and dividing by the
standard deviation. To engineer additional features we also tried including cross products of
the base features in our dataset up to degree 3.

3 Models and Results

We used two distinct strategies for estimating the treatment eﬀect. Our ﬁrst approach is a
more straightforward one – we use several models to predict the ﬁnal wages of each individual
in the large sample from the general population. We then used these models to predict wages
each individual would have gotten if they hadn’t received training. The diﬀerence among the
individuals who received training between their actual ﬁnal wage and their predicted ﬁnal
wage, can be used as an estimate of the change in wage caused by the job training, i.e. a
casual eﬀect.

The second approach comes from the program evaluation literature. First, we estimate
the propensity score – conditional probability of being in the treatment group – and then
compare the treatment outcomes between the treated and untreated individuals who have
similar predicted probabilities of receiving the treatment.

3.1 Regression models

linear regression, lasso regression3,
We tried several types of models for predicting wages:
SVM (with linear, polynomial, and radial kernels), and random forest models. We choose
parameter values for each model using a grid search with 10-fold cross validation using Root
Mean Squared Error (RMSE) as our selection criteria. We evaluated performance on a 10%
hold-out set. The exponents following a model’s name indicate the degree of the polynomial
interactions of features that were included4.

By looking at the learning curves for these models it became clear that all of the models
had high bias. We attempted unsuccessfully to get access to more detailed data with a
larger number of features for each individual. Adding polynomial interaction terms did not
substantially reduce test error.

3Linear regression with L1 regularization to reduce overﬁtting.
4E.g. a 2 means that we added new features corresponding to the product of every pair of features in the

base dataset.

2

Linear (degree 2)Linear (degree 1)Elastic (degree 2)Elastic (degree 3)Linear (degree 3)Random ForestSVM RadialSVM LinearSVM Polynomial0.600.650.700.750.80500010000150005000100001500050001000015000500010000150005000100001500050001000015000500010000150005000100001500050001000015000NobsRMSETypetraintestTraining RMSE Test RMSE Treatment Eﬀect

Linear2
Linear1
Lasso2
Lasso3
Linear3
Random Forest
SVM (radial)
SVM (linear)
SVM (polynomial)

0.71702
0.72407
0.72301
0.72281
0.71309
0.65542
0.73545
0.74588
0.72340

0.70945
0.71039
0.71319
0.71324
0.71365
0.72046
0.72446
0.72951
0.73667

-$1787
-$1242
-$1773
-$1655
-$1830
-$1034
-$2671
-$2658
-$2705

Linear regression with degree two polynomial interactions gave the lowest RMSE on the
hold-out set. The random forest’s low training error and higher test error suggests overﬁtting.
None of the SVM models were very successful, even after trying a wide selection of diﬀerent
values for the parameters. Since we are predicting a continuous outcome (salary) that is
normalized to unit variance, squaring the RMSE gives a measure of the fraction of variation
in salary that is unexplained by the model, roughly 50%. Since there are many determinants
of future wage (the industry you work in, work ethic, etc) that we do not observe, it is not
surprising that we are unable to explain all of the variation in wages.

We used these models to predict what wage the individuals in the job training would
have gotten if they had not participated. The diﬀerence between predicted wage and actual
wage for the individuals who received job training is the used as the estimate of the eﬀect of
the training. All of the models produced negative estimates of the eﬀect of the job training.
Measurements of the eﬀect of job training programs that come from randomized experiments
consistently ﬁnd positive eﬀects, which suggests our ﬁrst approach is not producing accurate
estimates of the eﬀect of the job training.

3.2 Propensity score models

Our second approach was propensity score stratiﬁcation. We ﬁrst train several models to
estimate a propensity score for each individual, the likelihood of he was given job training5.
Then, we cluster observations with similar estimated propensity score and estimate average
eﬀect within each group. This allows us to compare the treated individuals with individuals
from the general population who had a similar likelihood of being oﬀered the treatment.

We use several conceptually distinct models for estimating propensity scores: logistic re-
gression, lasso, decision tree, boosted tree and random forest. Again, tuning parameters were
chosen by 10-fold cross-validation. As an accuracy measure we use the squared diﬀerence
between our continuous estimate of the propensity score and the binary treatment indicator.
It’s worth emphasizing that perfectly predicting who received training would not advanta-
geous here, since then the model would suggest that no person from the general population
is comparable to anyone in the treated population. Despite this conceptual strangeness,
propensity score matching has a substantial theoretical grounding (Caliendo, 2008)

5I.e., we are predicting a continuous estimate of the binary outcome, ”Training/No Training”

3

Estimation of Causal Eﬀects from Observational Study

of Job Training Program

Dmitry Arkhangelsky and Rob Donnelly

darkhang@stanford.edu & rodonn@stanford.edu

1

Introduction

In the social sciences, researchers are often interested in measuring the eﬀect of a treatment
or intervention. For example, many labor economists are interested in measuring the eﬀect
of job training programs on increasing salaries or reducing unemployment. Measuring the
”causal eﬀect” of an intervention is relatively straight forward when the treatment is given
to randomly selected individuals. In this case the diﬀerence in the average outcome of the
treatment and control groups can be directly compared.

Unfortunately in many policy relevant situations economists do not have data from a ran-
domized controlled experiment. Instead they have data on the characteristics and outcomes
of a set of individuals who received the treatment, but no directly comparable control group.
We tried two approaches to estimating the eﬀect of a job training treatment on wages.
The ﬁrst approach predicts counterfactual wages for the treated individuals based on models
trained on a large sample from the general population. The second approach matches the
treated individuals to people from the general population based on propensity scores. We
ﬁnd that the second approach yields more plausible estimates of the eﬀect of job training.

2 Dataset, Features, and Preprocessing

We are using data from the National Supported Work Demonstration, which gave 12-18
months of job training to unemployed adults at 15 sites around the US. We have data on
185 men who participated in the job training, as well as 15,992 adults from the general
population1.
In order to be eligible to participate in the job training, an individual had
to meet certain eligibility requirements and had to volunteer. Because of this, the pool of
participants is very diﬀerent from a random sample of the population2. Previous work in
the economics literature has shown that treating the data as if it came from a randomized
controlled experiment leads to very biased estimates of the eﬀect of job training programs.
In particular since the average participant in a job training program has many disadvantages
in the job market relative to a randomly selected person from the whole population, naive
estimates often predict that the job training actually lowered wages.

Our features include the age, years of education, race, marriage status, annual income
1 and 2 years before training program, and unemployment status 1 and 2 years before job

1This data is from 1972, but is still relevant since it is a commonly used baseline in the economics literature

for methods of evaluating treatment eﬀects from non-experimental data.

2Participants in this job training program on average have less education and are more likely to have prior

criminal convictions or to be unemployed.

1

training. We normalized all of our variables by subtracting the mean and dividing by the
standard deviation. To engineer additional features we also tried including cross products of
the base features in our dataset up to degree 3.

3 Models and Results

We used two distinct strategies for estimating the treatment eﬀect. Our ﬁrst approach is a
more straightforward one – we use several models to predict the ﬁnal wages of each individual
in the large sample from the general population. We then used these models to predict wages
each individual would have gotten if they hadn’t received training. The diﬀerence among the
individuals who received training between their actual ﬁnal wage and their predicted ﬁnal
wage, can be used as an estimate of the change in wage caused by the job training, i.e. a
casual eﬀect.

The second approach comes from the program evaluation literature. First, we estimate
the propensity score – conditional probability of being in the treatment group – and then
compare the treatment outcomes between the treated and untreated individuals who have
similar predicted probabilities of receiving the treatment.

3.1 Regression models

linear regression, lasso regression3,
We tried several types of models for predicting wages:
SVM (with linear, polynomial, and radial kernels), and random forest models. We choose
parameter values for each model using a grid search with 10-fold cross validation using Root
Mean Squared Error (RMSE) as our selection criteria. We evaluated performance on a 10%
hold-out set. The exponents following a model’s name indicate the degree of the polynomial
interactions of features that were included4.

By looking at the learning curves for these models it became clear that all of the models
had high bias. We attempted unsuccessfully to get access to more detailed data with a
larger number of features for each individual. Adding polynomial interaction terms did not
substantially reduce test error.

3Linear regression with L1 regularization to reduce overﬁtting.
4E.g. a 2 means that we added new features corresponding to the product of every pair of features in the

base dataset.

2

Linear (degree 2)Linear (degree 1)Elastic (degree 2)Elastic (degree 3)Linear (degree 3)Random ForestSVM RadialSVM LinearSVM Polynomial0.600.650.700.750.80500010000150005000100001500050001000015000500010000150005000100001500050001000015000500010000150005000100001500050001000015000NobsRMSETypetraintestTraining RMSE Test RMSE Treatment Eﬀect

Linear2
Linear1
Lasso2
Lasso3
Linear3
Random Forest
SVM (radial)
SVM (linear)
SVM (polynomial)

0.71702
0.72407
0.72301
0.72281
0.71309
0.65542
0.73545
0.74588
0.72340

0.70945
0.71039
0.71319
0.71324
0.71365
0.72046
0.72446
0.72951
0.73667

-$1787
-$1242
-$1773
-$1655
-$1830
-$1034
-$2671
-$2658
-$2705

Linear regression with degree two polynomial interactions gave the lowest RMSE on the
hold-out set. The random forest’s low training error and higher test error suggests overﬁtting.
None of the SVM models were very successful, even after trying a wide selection of diﬀerent
values for the parameters. Since we are predicting a continuous outcome (salary) that is
normalized to unit variance, squaring the RMSE gives a measure of the fraction of variation
in salary that is unexplained by the model, roughly 50%. Since there are many determinants
of future wage (the industry you work in, work ethic, etc) that we do not observe, it is not
surprising that we are unable to explain all of the variation in wages.

We used these models to predict what wage the individuals in the job training would
have gotten if they had not participated. The diﬀerence between predicted wage and actual
wage for the individuals who received job training is the used as the estimate of the eﬀect of
the training. All of the models produced negative estimates of the eﬀect of the job training.
Measurements of the eﬀect of job training programs that come from randomized experiments
consistently ﬁnd positive eﬀects, which suggests our ﬁrst approach is not producing accurate
estimates of the eﬀect of the job training.

3.2 Propensity score models

Our second approach was propensity score stratiﬁcation. We ﬁrst train several models to
estimate a propensity score for each individual, the likelihood of he was given job training5.
Then, we cluster observations with similar estimated propensity score and estimate average
eﬀect within each group. This allows us to compare the treated individuals with individuals
from the general population who had a similar likelihood of being oﬀered the treatment.

We use several conceptually distinct models for estimating propensity scores: logistic re-
gression, lasso, decision tree, boosted tree and random forest. Again, tuning parameters were
chosen by 10-fold cross-validation. As an accuracy measure we use the squared diﬀerence
between our continuous estimate of the propensity score and the binary treatment indicator.
It’s worth emphasizing that perfectly predicting who received training would not advanta-
geous here, since then the model would suggest that no person from the general population
is comparable to anyone in the treated population. Despite this conceptual strangeness,
propensity score matching has a substantial theoretical grounding (Caliendo, 2008)

5I.e., we are predicting a continuous estimate of the binary outcome, ”Training/No Training”

3

Training and test errors for all procedures that we used are presented in the following
table. Since less than 1% of our data received the job training treatment, we also report
conditional errors – average error for treated observations.

Train
Error

0.0077
0.0058
0.0067
0.0066
0.0091
0.0044

Test
Error

0.0078
0.0074
0.0075
0.0096
0.0100
0.0077

Logistic1
Logistic2
Lasso3
Tree
Boosted Tree
Random Forest

Cond. Train Cond. Test Balance

#

Treatment

Error

0.5112
0.3475
0.4210
0.4236
0.6930
0.4166

Error

0.5252
0.4790
0.4903
0.6068
0.7317
0.5358

Groups

Eﬀect

0.87
1.54
0.84
1.72
1.49
0.56

6
8
6
7
5
4

$1855
$1069
$1417
$136
$1075
$1778

Due to the unbalanced distribution of the outcome variable, it’s unsurprising that the
overall accuracy is much higher than the accuracy on the treated individuals. Overall both
measures of test error seem to agree on which models were more accurate. Logistic regression
and Lasso seemed to outperform the tree based estimates. However, it’s unclear whether the
method with the lowest test error is the best in terms of estimating the overall treatment
eﬀect.

To cluster observation into several groups we took observed treatment status and ran
a simple decision tree using estimated propensity score as the only covariate. This proce-
dure results in groups of people with similar propensity score. Another possible procedure
is to cluster observations using some unsupervised algorithm like K-means. However, one
important advantage of using trees is that the number of classes is selected automatically.

For each model of propensity score we report the average treatment eﬀect, assessed balance

and the number of classes. Formally, we compute the following statistics:

Average Treatment Eﬀect =

(1)

(cid:80)K

(cid:1)

(cid:0)Y i1 − Y i0
(cid:80)K

i=1 N1i

i=1 N1i

here i is a generic class, N1i is the number of treated individuals in this class, Y ik is the
average outcome in i-th class, for k group (either treatment or control).

To assess balance we compute the following normalized sum of squares:

(cid:80)K

i=1

m(cid:88)

j=1

Balance =

(X ji1−X ij0)2
ˆV[Xjk1]+ˆV[Xjk0]

K

(2)

where ˆV[Xjik] is the estimated variance of the j-th covariate in the i-th class and k-th group.
A lower value corresponds to more similarity between the treated and untreated individuals
within a group, which is desirable.

The best model in terms of balance is the random forest model. There was considerable
variation in the estimated treatment eﬀect between diﬀerent models, but all of the models
predict positive eﬀects on wages from the job training and the magnitudes are plausible and

4

Estimation of Causal Eﬀects from Observational Study

of Job Training Program

Dmitry Arkhangelsky and Rob Donnelly

darkhang@stanford.edu & rodonn@stanford.edu

1

Introduction

In the social sciences, researchers are often interested in measuring the eﬀect of a treatment
or intervention. For example, many labor economists are interested in measuring the eﬀect
of job training programs on increasing salaries or reducing unemployment. Measuring the
”causal eﬀect” of an intervention is relatively straight forward when the treatment is given
to randomly selected individuals. In this case the diﬀerence in the average outcome of the
treatment and control groups can be directly compared.

Unfortunately in many policy relevant situations economists do not have data from a ran-
domized controlled experiment. Instead they have data on the characteristics and outcomes
of a set of individuals who received the treatment, but no directly comparable control group.
We tried two approaches to estimating the eﬀect of a job training treatment on wages.
The ﬁrst approach predicts counterfactual wages for the treated individuals based on models
trained on a large sample from the general population. The second approach matches the
treated individuals to people from the general population based on propensity scores. We
ﬁnd that the second approach yields more plausible estimates of the eﬀect of job training.

2 Dataset, Features, and Preprocessing

We are using data from the National Supported Work Demonstration, which gave 12-18
months of job training to unemployed adults at 15 sites around the US. We have data on
185 men who participated in the job training, as well as 15,992 adults from the general
population1.
In order to be eligible to participate in the job training, an individual had
to meet certain eligibility requirements and had to volunteer. Because of this, the pool of
participants is very diﬀerent from a random sample of the population2. Previous work in
the economics literature has shown that treating the data as if it came from a randomized
controlled experiment leads to very biased estimates of the eﬀect of job training programs.
In particular since the average participant in a job training program has many disadvantages
in the job market relative to a randomly selected person from the whole population, naive
estimates often predict that the job training actually lowered wages.

Our features include the age, years of education, race, marriage status, annual income
1 and 2 years before training program, and unemployment status 1 and 2 years before job

1This data is from 1972, but is still relevant since it is a commonly used baseline in the economics literature

for methods of evaluating treatment eﬀects from non-experimental data.

2Participants in this job training program on average have less education and are more likely to have prior

criminal convictions or to be unemployed.

1

training. We normalized all of our variables by subtracting the mean and dividing by the
standard deviation. To engineer additional features we also tried including cross products of
the base features in our dataset up to degree 3.

3 Models and Results

We used two distinct strategies for estimating the treatment eﬀect. Our ﬁrst approach is a
more straightforward one – we use several models to predict the ﬁnal wages of each individual
in the large sample from the general population. We then used these models to predict wages
each individual would have gotten if they hadn’t received training. The diﬀerence among the
individuals who received training between their actual ﬁnal wage and their predicted ﬁnal
wage, can be used as an estimate of the change in wage caused by the job training, i.e. a
casual eﬀect.

The second approach comes from the program evaluation literature. First, we estimate
the propensity score – conditional probability of being in the treatment group – and then
compare the treatment outcomes between the treated and untreated individuals who have
similar predicted probabilities of receiving the treatment.

3.1 Regression models

linear regression, lasso regression3,
We tried several types of models for predicting wages:
SVM (with linear, polynomial, and radial kernels), and random forest models. We choose
parameter values for each model using a grid search with 10-fold cross validation using Root
Mean Squared Error (RMSE) as our selection criteria. We evaluated performance on a 10%
hold-out set. The exponents following a model’s name indicate the degree of the polynomial
interactions of features that were included4.

By looking at the learning curves for these models it became clear that all of the models
had high bias. We attempted unsuccessfully to get access to more detailed data with a
larger number of features for each individual. Adding polynomial interaction terms did not
substantially reduce test error.

3Linear regression with L1 regularization to reduce overﬁtting.
4E.g. a 2 means that we added new features corresponding to the product of every pair of features in the

base dataset.

2

Linear (degree 2)Linear (degree 1)Elastic (degree 2)Elastic (degree 3)Linear (degree 3)Random ForestSVM RadialSVM LinearSVM Polynomial0.600.650.700.750.80500010000150005000100001500050001000015000500010000150005000100001500050001000015000500010000150005000100001500050001000015000NobsRMSETypetraintestTraining RMSE Test RMSE Treatment Eﬀect

Linear2
Linear1
Lasso2
Lasso3
Linear3
Random Forest
SVM (radial)
SVM (linear)
SVM (polynomial)

0.71702
0.72407
0.72301
0.72281
0.71309
0.65542
0.73545
0.74588
0.72340

0.70945
0.71039
0.71319
0.71324
0.71365
0.72046
0.72446
0.72951
0.73667

-$1787
-$1242
-$1773
-$1655
-$1830
-$1034
-$2671
-$2658
-$2705

Linear regression with degree two polynomial interactions gave the lowest RMSE on the
hold-out set. The random forest’s low training error and higher test error suggests overﬁtting.
None of the SVM models were very successful, even after trying a wide selection of diﬀerent
values for the parameters. Since we are predicting a continuous outcome (salary) that is
normalized to unit variance, squaring the RMSE gives a measure of the fraction of variation
in salary that is unexplained by the model, roughly 50%. Since there are many determinants
of future wage (the industry you work in, work ethic, etc) that we do not observe, it is not
surprising that we are unable to explain all of the variation in wages.

We used these models to predict what wage the individuals in the job training would
have gotten if they had not participated. The diﬀerence between predicted wage and actual
wage for the individuals who received job training is the used as the estimate of the eﬀect of
the training. All of the models produced negative estimates of the eﬀect of the job training.
Measurements of the eﬀect of job training programs that come from randomized experiments
consistently ﬁnd positive eﬀects, which suggests our ﬁrst approach is not producing accurate
estimates of the eﬀect of the job training.

3.2 Propensity score models

Our second approach was propensity score stratiﬁcation. We ﬁrst train several models to
estimate a propensity score for each individual, the likelihood of he was given job training5.
Then, we cluster observations with similar estimated propensity score and estimate average
eﬀect within each group. This allows us to compare the treated individuals with individuals
from the general population who had a similar likelihood of being oﬀered the treatment.

We use several conceptually distinct models for estimating propensity scores: logistic re-
gression, lasso, decision tree, boosted tree and random forest. Again, tuning parameters were
chosen by 10-fold cross-validation. As an accuracy measure we use the squared diﬀerence
between our continuous estimate of the propensity score and the binary treatment indicator.
It’s worth emphasizing that perfectly predicting who received training would not advanta-
geous here, since then the model would suggest that no person from the general population
is comparable to anyone in the treated population. Despite this conceptual strangeness,
propensity score matching has a substantial theoretical grounding (Caliendo, 2008)

5I.e., we are predicting a continuous estimate of the binary outcome, ”Training/No Training”

3

Training and test errors for all procedures that we used are presented in the following
table. Since less than 1% of our data received the job training treatment, we also report
conditional errors – average error for treated observations.

Train
Error

0.0077
0.0058
0.0067
0.0066
0.0091
0.0044

Test
Error

0.0078
0.0074
0.0075
0.0096
0.0100
0.0077

Logistic1
Logistic2
Lasso3
Tree
Boosted Tree
Random Forest

Cond. Train Cond. Test Balance

#

Treatment

Error

0.5112
0.3475
0.4210
0.4236
0.6930
0.4166

Error

0.5252
0.4790
0.4903
0.6068
0.7317
0.5358

Groups

Eﬀect

0.87
1.54
0.84
1.72
1.49
0.56

6
8
6
7
5
4

$1855
$1069
$1417
$136
$1075
$1778

Due to the unbalanced distribution of the outcome variable, it’s unsurprising that the
overall accuracy is much higher than the accuracy on the treated individuals. Overall both
measures of test error seem to agree on which models were more accurate. Logistic regression
and Lasso seemed to outperform the tree based estimates. However, it’s unclear whether the
method with the lowest test error is the best in terms of estimating the overall treatment
eﬀect.

To cluster observation into several groups we took observed treatment status and ran
a simple decision tree using estimated propensity score as the only covariate. This proce-
dure results in groups of people with similar propensity score. Another possible procedure
is to cluster observations using some unsupervised algorithm like K-means. However, one
important advantage of using trees is that the number of classes is selected automatically.

For each model of propensity score we report the average treatment eﬀect, assessed balance

and the number of classes. Formally, we compute the following statistics:

Average Treatment Eﬀect =

(1)

(cid:80)K

(cid:1)

(cid:0)Y i1 − Y i0
(cid:80)K

i=1 N1i

i=1 N1i

here i is a generic class, N1i is the number of treated individuals in this class, Y ik is the
average outcome in i-th class, for k group (either treatment or control).

To assess balance we compute the following normalized sum of squares:

(cid:80)K

i=1

m(cid:88)

j=1

Balance =

(X ji1−X ij0)2
ˆV[Xjk1]+ˆV[Xjk0]

K

(2)

where ˆV[Xjik] is the estimated variance of the j-th covariate in the i-th class and k-th group.
A lower value corresponds to more similarity between the treated and untreated individuals
within a group, which is desirable.

The best model in terms of balance is the random forest model. There was considerable
variation in the estimated treatment eﬀect between diﬀerent models, but all of the models
predict positive eﬀects on wages from the job training and the magnitudes are plausible and

4

increase in salary of roughly $1000 for a group of individuals whose previous salaries averaged
$63496.

4 Conclusions and Future Research

When researchers have used randomized controlled experiments to evaluate similar job train-
ing programs they generally ﬁnd the training leads to a wage increase of $800 to $1600.
With this as a baseline, the ﬁrst approach was wildly inaccurate since every model predicted
a substantial negative eﬀect. This approach might be more successful on a dataset with more
features, since all of the models tried had high bias. Having more information about each
individual might allow us to generate models that can explain more of the variation in wages.
If we were able to create highly accurate predictions of each individuals wage, then we would
be able to generate accurate predictions of what each individual in the job training program
would have earned if he didn’t participate. We would then have an accurate estimate of the
eﬀect of the job training even without running a randomized controlled experiment.

In contrast the propensity score based estimates all had the correct sign. With the
exception of the simple tree model, all of the estimate treatment eﬀects are consistent with
the $800 to $1600 range found in experiments of similar programs. The high variation
suggests that only limited conﬁdence can be had in the estimates from any single propensity
score model. Running several diﬀerent models, as we have done here, may give researchers
a sense of the general range of outcomes they should expect if the program were re run as a
true experiment.

For future research in this area, we are also interested in testing out unsupervised learning
algorithms as mechanisms for clustering individuals into discrete groups and then comparing
the treatment eﬀects within each group. Approaches like this might also allow researchers to
know not just the average aﬀect of the treatment across the whole population, but also have
predictions of which subsets of the population the treatment worked especially well on.

5 References

Caliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of
propensity score matching. Journal of economic surveys.

LaLonde, R. J. (1986). Evaluating the econometric evaluations of training programs with
experimental data. The American Economic Review, 604-620.

Smith, Jeﬀrey A., and Petra E. Todd. ”Reconciling conﬂicting evidence on the performance
of propensity-score matching methods.” American Economic Review (2001): 112-118.

6This data is from 1978, so adjusting for inﬂation, this corresponds to increasing an increase of $3,500

over a salary of $23,000 in 2014 dollars.

5

