CS229 Project: Identifying Regions of High Turbidity in San

Francisco Bay

Joe Adelson

December 11, 2014

Introduction

Suspended sediments in oceans, seas, and estuaries shape coastal geography, provide important
nutrients to ecosystems, and transport and bury harmful contaminants. Although the problem has
signiﬁcant interest in the scientiﬁc and environmental engineering communities, many of the the
mechanisms involved are poorly understood because of the diﬃculty and expense of measuring what
turns out to be a very complex system. The complexity arises from the fact that many estuaries
are dynamic regions, where the currents that move sediment depend on rainfall, wind, waves, tides,
salinity gradients, and anthropogenic manipulation (dams, weirs, etc.). On top of this the physics
of how sediment particles interact with the sea bed and one another is still poorly understood,
especially at the scales of interest to biologists and coastal engineers. Because of these challenges
the development of tools to study the problem is an active area of research.

The traditional tools of studying sediment transport are in situ ﬁeld measurements and numer-
ical models, and these each face challenges. Because of the small scale of in situ sampling methods,
it is often diﬃcult (and very expensive) to collect data over a large enough area to develop a strong
regional understanding of what is happening. Numerical models have thus gained popularity as a
way to predict suspended sediment concentrations (SSC), erosion, and deposition, but these con-
tend with their own problems of unknown boundary conditions and poorly understood transport
physics for silts and clays. To help ﬁll the gap left in these two technologies, there has been work
to develop remote sensing algorithms using available NASA and European Space Agency (ESA)
satellite imagery to map turbidity, an optical measure of the cloudiness of water, which serves as a
proxy for SSC. This is still a young technology and has substantial room for development.

San Francisco Bay is an excellent example of a system that showcases many of importances of
sediments and the challenges with understanding them. For one, it is a well studied estuary with
ample of measurements about the ﬂow and sediment conditions publicly available.

This project’s goal is to establish a relationship between available remote sensing data and in
situ measurements of turbidity in San Francisco Bay. This requires correlating the data at the
particular pixel of the measurement (and perhaps its neighbors) with the measurement itself. This
has been studied in other estuaries [1] and there are products available that can calculate turbidity,
but these are not calibrated for our area of interest.

Data Collection

There are two primary sources of data for this project: remote sensing satellite data and in situ
point measurements. Satellite images from the MERIS probe taken in the time period of 2006
to 2012 were downloaded from the CoastColour project [3], a data oﬀering from the ESA that
specializes in processing coastal images. Each image pixel contains information of the intensity of
discretized reﬂectance of both visual and infrared wavelengths, as well as precomputed estimates
of turbidity, suspended matter, pigment, and chlorophyll. Because the turbidity is of signiﬁcant

1

CS229 Project: Identifying Regions of High Turbidity in San

Francisco Bay

Joe Adelson

December 11, 2014

Introduction

Suspended sediments in oceans, seas, and estuaries shape coastal geography, provide important
nutrients to ecosystems, and transport and bury harmful contaminants. Although the problem has
signiﬁcant interest in the scientiﬁc and environmental engineering communities, many of the the
mechanisms involved are poorly understood because of the diﬃculty and expense of measuring what
turns out to be a very complex system. The complexity arises from the fact that many estuaries
are dynamic regions, where the currents that move sediment depend on rainfall, wind, waves, tides,
salinity gradients, and anthropogenic manipulation (dams, weirs, etc.). On top of this the physics
of how sediment particles interact with the sea bed and one another is still poorly understood,
especially at the scales of interest to biologists and coastal engineers. Because of these challenges
the development of tools to study the problem is an active area of research.

The traditional tools of studying sediment transport are in situ ﬁeld measurements and numer-
ical models, and these each face challenges. Because of the small scale of in situ sampling methods,
it is often diﬃcult (and very expensive) to collect data over a large enough area to develop a strong
regional understanding of what is happening. Numerical models have thus gained popularity as a
way to predict suspended sediment concentrations (SSC), erosion, and deposition, but these con-
tend with their own problems of unknown boundary conditions and poorly understood transport
physics for silts and clays. To help ﬁll the gap left in these two technologies, there has been work
to develop remote sensing algorithms using available NASA and European Space Agency (ESA)
satellite imagery to map turbidity, an optical measure of the cloudiness of water, which serves as a
proxy for SSC. This is still a young technology and has substantial room for development.

San Francisco Bay is an excellent example of a system that showcases many of importances of
sediments and the challenges with understanding them. For one, it is a well studied estuary with
ample of measurements about the ﬂow and sediment conditions publicly available.

This project’s goal is to establish a relationship between available remote sensing data and in
situ measurements of turbidity in San Francisco Bay. This requires correlating the data at the
particular pixel of the measurement (and perhaps its neighbors) with the measurement itself. This
has been studied in other estuaries [1] and there are products available that can calculate turbidity,
but these are not calibrated for our area of interest.

Data Collection

There are two primary sources of data for this project: remote sensing satellite data and in situ
point measurements. Satellite images from the MERIS probe taken in the time period of 2006
to 2012 were downloaded from the CoastColour project [3], a data oﬀering from the ESA that
specializes in processing coastal images. Each image pixel contains information of the intensity of
discretized reﬂectance of both visual and infrared wavelengths, as well as precomputed estimates
of turbidity, suspended matter, pigment, and chlorophyll. Because the turbidity is of signiﬁcant

1

December 11, 2014

CS 229 Project

Joe Adelson

interest, a spatially averaged turbidity is calculated in an attempt to create a turbidity feature with
reduced noise.

In situ measurements of turbidity are taken from three United States Geological Survey (USGS)
monitoring stations at Alcatraz Island, the Dumbarton Bridge, and the Richmond-San Rafael
Bridge each taken at a depth of 6 to 8 meters below the surface. These measurements are taken
every 15 minutes.

Preprocessing the data includes extracting useful information for both datasets at matching
times and locations. This includes removing cloudy images as well as ﬁnding the image pixel that
contains the USGS sampling station. In all there are at most three samples for each image, one
for each valid image pixel and station measurement combination, for a total of 679 samples of 44
features each.

Regression Methods

Linear regression with L1 and L2 shrinkage parameters as well as support vector regression were
tested. The examined parameters are: number of principal components, degree of polynomial
expansions of the feature space, and SVR kernels. Many tests were run on the dataset to ﬁnd
the optimal values of these numbers. For each test a parameter sweep for the optimal penalty
logarithmic parameter was done using K-fold cross validation with 5 folds.

In order to ensure that the turbidity levels are non-negative, all regressions are completed using
the log of the turbidity values. The performance metrics are given using the actual turbidity units
The computations use the Python library Scikit-Learn library [2]. The limiting factors for
testing polynomial expansions and number of principal components were both the run time of
the parameter sweeps and the trend towards overﬁtting with high polynomial degrees and many
features. Therefore, polynomial regression with high degrees was limited to using relatively few
principal components.

Results

The linear regressions perform better than support vector regression (table 1). The high order poly-
nomials are generally optimal with very large shrinkage parameters, which implies that high order
polynomials overﬁt the data. Below are mean RME associated with the optimal penalty parameter
for some of the tested PCA and polynomial combinations tested via 5-folds cross validation (ﬁgures
1, 2, 3). As a point of comparison the CoastColour turbidity measure has a root mean square
error (RME) of 108.5. Although this work shows substantial improvement CoastColour sediment
estimate, the R-Squared ﬁt is only 0.22.

PCA does not appear to be an eﬀective tool for eliminating overﬁtting of the data as the full
set of data performed best. Also, polynomial expansion of the features did not improved RME.
The regressions are sensitive to the shrinkage parameter and the search ﬁnds a smooth minimum
for the linear regressions (ﬁgure 4) but SVR is more erratic (ﬁgure 5). These regressions tend to
under predict the large measured turbidities (ﬁgure 6).

2

CS229 Project: Identifying Regions of High Turbidity in San

Francisco Bay

Joe Adelson

December 11, 2014

Introduction

Suspended sediments in oceans, seas, and estuaries shape coastal geography, provide important
nutrients to ecosystems, and transport and bury harmful contaminants. Although the problem has
signiﬁcant interest in the scientiﬁc and environmental engineering communities, many of the the
mechanisms involved are poorly understood because of the diﬃculty and expense of measuring what
turns out to be a very complex system. The complexity arises from the fact that many estuaries
are dynamic regions, where the currents that move sediment depend on rainfall, wind, waves, tides,
salinity gradients, and anthropogenic manipulation (dams, weirs, etc.). On top of this the physics
of how sediment particles interact with the sea bed and one another is still poorly understood,
especially at the scales of interest to biologists and coastal engineers. Because of these challenges
the development of tools to study the problem is an active area of research.

The traditional tools of studying sediment transport are in situ ﬁeld measurements and numer-
ical models, and these each face challenges. Because of the small scale of in situ sampling methods,
it is often diﬃcult (and very expensive) to collect data over a large enough area to develop a strong
regional understanding of what is happening. Numerical models have thus gained popularity as a
way to predict suspended sediment concentrations (SSC), erosion, and deposition, but these con-
tend with their own problems of unknown boundary conditions and poorly understood transport
physics for silts and clays. To help ﬁll the gap left in these two technologies, there has been work
to develop remote sensing algorithms using available NASA and European Space Agency (ESA)
satellite imagery to map turbidity, an optical measure of the cloudiness of water, which serves as a
proxy for SSC. This is still a young technology and has substantial room for development.

San Francisco Bay is an excellent example of a system that showcases many of importances of
sediments and the challenges with understanding them. For one, it is a well studied estuary with
ample of measurements about the ﬂow and sediment conditions publicly available.

This project’s goal is to establish a relationship between available remote sensing data and in
situ measurements of turbidity in San Francisco Bay. This requires correlating the data at the
particular pixel of the measurement (and perhaps its neighbors) with the measurement itself. This
has been studied in other estuaries [1] and there are products available that can calculate turbidity,
but these are not calibrated for our area of interest.

Data Collection

There are two primary sources of data for this project: remote sensing satellite data and in situ
point measurements. Satellite images from the MERIS probe taken in the time period of 2006
to 2012 were downloaded from the CoastColour project [3], a data oﬀering from the ESA that
specializes in processing coastal images. Each image pixel contains information of the intensity of
discretized reﬂectance of both visual and infrared wavelengths, as well as precomputed estimates
of turbidity, suspended matter, pigment, and chlorophyll. Because the turbidity is of signiﬁcant

1

December 11, 2014

CS 229 Project

Joe Adelson

interest, a spatially averaged turbidity is calculated in an attempt to create a turbidity feature with
reduced noise.

In situ measurements of turbidity are taken from three United States Geological Survey (USGS)
monitoring stations at Alcatraz Island, the Dumbarton Bridge, and the Richmond-San Rafael
Bridge each taken at a depth of 6 to 8 meters below the surface. These measurements are taken
every 15 minutes.

Preprocessing the data includes extracting useful information for both datasets at matching
times and locations. This includes removing cloudy images as well as ﬁnding the image pixel that
contains the USGS sampling station. In all there are at most three samples for each image, one
for each valid image pixel and station measurement combination, for a total of 679 samples of 44
features each.

Regression Methods

Linear regression with L1 and L2 shrinkage parameters as well as support vector regression were
tested. The examined parameters are: number of principal components, degree of polynomial
expansions of the feature space, and SVR kernels. Many tests were run on the dataset to ﬁnd
the optimal values of these numbers. For each test a parameter sweep for the optimal penalty
logarithmic parameter was done using K-fold cross validation with 5 folds.

In order to ensure that the turbidity levels are non-negative, all regressions are completed using
the log of the turbidity values. The performance metrics are given using the actual turbidity units
The computations use the Python library Scikit-Learn library [2]. The limiting factors for
testing polynomial expansions and number of principal components were both the run time of
the parameter sweeps and the trend towards overﬁtting with high polynomial degrees and many
features. Therefore, polynomial regression with high degrees was limited to using relatively few
principal components.

Results

The linear regressions perform better than support vector regression (table 1). The high order poly-
nomials are generally optimal with very large shrinkage parameters, which implies that high order
polynomials overﬁt the data. Below are mean RME associated with the optimal penalty parameter
for some of the tested PCA and polynomial combinations tested via 5-folds cross validation (ﬁgures
1, 2, 3). As a point of comparison the CoastColour turbidity measure has a root mean square
error (RME) of 108.5. Although this work shows substantial improvement CoastColour sediment
estimate, the R-Squared ﬁt is only 0.22.

PCA does not appear to be an eﬀective tool for eliminating overﬁtting of the data as the full
set of data performed best. Also, polynomial expansion of the features did not improved RME.
The regressions are sensitive to the shrinkage parameter and the search ﬁnds a smooth minimum
for the linear regressions (ﬁgure 4) but SVR is more erratic (ﬁgure 5). These regressions tend to
under predict the large measured turbidities (ﬁgure 6).

2

December 11, 2014

Discussion

CS 229 Project

Joe Adelson

We are able to make a signiﬁcant improvement over the “out-of-the-box” measure of turbidity for
San Francisco Bay. However, the optimal test set R-Squared value of 0.22 suggests that there is
much work to be done in predicting the turbidity. The reason for the poor performance likely
occurs for a variety of reasons. Most apparent is the disparity of scale in the data we use: Because
the satellite data has a spatial resolution of about 300 meters, it will inevitably not be able to pick
up the small scale feature that may aﬀect the in situ measurements. Secondly, there is a three
dimensionality to turbidity and the satellite reads the surfaces, while the in situ measurements are
sampled at a depth of 6-8 meters depending on the tide. There may also be non-linearities between
the satellite data and the actual turbidity that our regression models do not pick up. Future work
includes not only expanding this dataset to more features and testing the value of neural networks,
but also a proposal to conduct our own experiment of taking aerial photographs of the bay, while
measuring the turbidity levels of South San Francisco Bay using boat transects to get a wider
spatial baseline of measurements.

References
[1] R. L. Miller and B. A. McKee. Using {MODIS} terra 250 m imagery to map concentrations of total suspended

matter in coastal waters. Remote Sensing of Environment, 93(1):259 – 266, 2004.

[2] F. e. a. Pedregosa. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–

2830, 2011.

[3] K. Ruddick, C. Sa, S. Bernard, L. Robertson, M. Matthews, R. Doerﬀer, W. Schoenfeld, H. Z. G. HZG, M. S.

Salama, S. Budhiman, et al. Coastcolour round robin protocol in situ reﬂectance data set.

Table 1: Best performance for L1, L2, and Support Vector Regressions

Estimate

CoastColour Turbidity

L2 Regression
L1 Regression

Support Vector Regression

RME Optimal Parameter
108.5 N/A
4.00
2.20
7.87 ×10−4
2.20
0.054 (RBF Kernel, 5 Principal Components)
2.72

Figure 1: Optimal shrinkage parameter (via K-Folds optimization) and associated RME for measured and predicted
with L2 shrinkage for turbidity (FNU).

3

CS229 Project: Identifying Regions of High Turbidity in San

Francisco Bay

Joe Adelson

December 11, 2014

Introduction

Suspended sediments in oceans, seas, and estuaries shape coastal geography, provide important
nutrients to ecosystems, and transport and bury harmful contaminants. Although the problem has
signiﬁcant interest in the scientiﬁc and environmental engineering communities, many of the the
mechanisms involved are poorly understood because of the diﬃculty and expense of measuring what
turns out to be a very complex system. The complexity arises from the fact that many estuaries
are dynamic regions, where the currents that move sediment depend on rainfall, wind, waves, tides,
salinity gradients, and anthropogenic manipulation (dams, weirs, etc.). On top of this the physics
of how sediment particles interact with the sea bed and one another is still poorly understood,
especially at the scales of interest to biologists and coastal engineers. Because of these challenges
the development of tools to study the problem is an active area of research.

The traditional tools of studying sediment transport are in situ ﬁeld measurements and numer-
ical models, and these each face challenges. Because of the small scale of in situ sampling methods,
it is often diﬃcult (and very expensive) to collect data over a large enough area to develop a strong
regional understanding of what is happening. Numerical models have thus gained popularity as a
way to predict suspended sediment concentrations (SSC), erosion, and deposition, but these con-
tend with their own problems of unknown boundary conditions and poorly understood transport
physics for silts and clays. To help ﬁll the gap left in these two technologies, there has been work
to develop remote sensing algorithms using available NASA and European Space Agency (ESA)
satellite imagery to map turbidity, an optical measure of the cloudiness of water, which serves as a
proxy for SSC. This is still a young technology and has substantial room for development.

San Francisco Bay is an excellent example of a system that showcases many of importances of
sediments and the challenges with understanding them. For one, it is a well studied estuary with
ample of measurements about the ﬂow and sediment conditions publicly available.

This project’s goal is to establish a relationship between available remote sensing data and in
situ measurements of turbidity in San Francisco Bay. This requires correlating the data at the
particular pixel of the measurement (and perhaps its neighbors) with the measurement itself. This
has been studied in other estuaries [1] and there are products available that can calculate turbidity,
but these are not calibrated for our area of interest.

Data Collection

There are two primary sources of data for this project: remote sensing satellite data and in situ
point measurements. Satellite images from the MERIS probe taken in the time period of 2006
to 2012 were downloaded from the CoastColour project [3], a data oﬀering from the ESA that
specializes in processing coastal images. Each image pixel contains information of the intensity of
discretized reﬂectance of both visual and infrared wavelengths, as well as precomputed estimates
of turbidity, suspended matter, pigment, and chlorophyll. Because the turbidity is of signiﬁcant

1

December 11, 2014

CS 229 Project

Joe Adelson

interest, a spatially averaged turbidity is calculated in an attempt to create a turbidity feature with
reduced noise.

In situ measurements of turbidity are taken from three United States Geological Survey (USGS)
monitoring stations at Alcatraz Island, the Dumbarton Bridge, and the Richmond-San Rafael
Bridge each taken at a depth of 6 to 8 meters below the surface. These measurements are taken
every 15 minutes.

Preprocessing the data includes extracting useful information for both datasets at matching
times and locations. This includes removing cloudy images as well as ﬁnding the image pixel that
contains the USGS sampling station. In all there are at most three samples for each image, one
for each valid image pixel and station measurement combination, for a total of 679 samples of 44
features each.

Regression Methods

Linear regression with L1 and L2 shrinkage parameters as well as support vector regression were
tested. The examined parameters are: number of principal components, degree of polynomial
expansions of the feature space, and SVR kernels. Many tests were run on the dataset to ﬁnd
the optimal values of these numbers. For each test a parameter sweep for the optimal penalty
logarithmic parameter was done using K-fold cross validation with 5 folds.

In order to ensure that the turbidity levels are non-negative, all regressions are completed using
the log of the turbidity values. The performance metrics are given using the actual turbidity units
The computations use the Python library Scikit-Learn library [2]. The limiting factors for
testing polynomial expansions and number of principal components were both the run time of
the parameter sweeps and the trend towards overﬁtting with high polynomial degrees and many
features. Therefore, polynomial regression with high degrees was limited to using relatively few
principal components.

Results

The linear regressions perform better than support vector regression (table 1). The high order poly-
nomials are generally optimal with very large shrinkage parameters, which implies that high order
polynomials overﬁt the data. Below are mean RME associated with the optimal penalty parameter
for some of the tested PCA and polynomial combinations tested via 5-folds cross validation (ﬁgures
1, 2, 3). As a point of comparison the CoastColour turbidity measure has a root mean square
error (RME) of 108.5. Although this work shows substantial improvement CoastColour sediment
estimate, the R-Squared ﬁt is only 0.22.

PCA does not appear to be an eﬀective tool for eliminating overﬁtting of the data as the full
set of data performed best. Also, polynomial expansion of the features did not improved RME.
The regressions are sensitive to the shrinkage parameter and the search ﬁnds a smooth minimum
for the linear regressions (ﬁgure 4) but SVR is more erratic (ﬁgure 5). These regressions tend to
under predict the large measured turbidities (ﬁgure 6).

2

December 11, 2014

Discussion

CS 229 Project

Joe Adelson

We are able to make a signiﬁcant improvement over the “out-of-the-box” measure of turbidity for
San Francisco Bay. However, the optimal test set R-Squared value of 0.22 suggests that there is
much work to be done in predicting the turbidity. The reason for the poor performance likely
occurs for a variety of reasons. Most apparent is the disparity of scale in the data we use: Because
the satellite data has a spatial resolution of about 300 meters, it will inevitably not be able to pick
up the small scale feature that may aﬀect the in situ measurements. Secondly, there is a three
dimensionality to turbidity and the satellite reads the surfaces, while the in situ measurements are
sampled at a depth of 6-8 meters depending on the tide. There may also be non-linearities between
the satellite data and the actual turbidity that our regression models do not pick up. Future work
includes not only expanding this dataset to more features and testing the value of neural networks,
but also a proposal to conduct our own experiment of taking aerial photographs of the bay, while
measuring the turbidity levels of South San Francisco Bay using boat transects to get a wider
spatial baseline of measurements.

References
[1] R. L. Miller and B. A. McKee. Using {MODIS} terra 250 m imagery to map concentrations of total suspended

matter in coastal waters. Remote Sensing of Environment, 93(1):259 – 266, 2004.

[2] F. e. a. Pedregosa. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–

2830, 2011.

[3] K. Ruddick, C. Sa, S. Bernard, L. Robertson, M. Matthews, R. Doerﬀer, W. Schoenfeld, H. Z. G. HZG, M. S.

Salama, S. Budhiman, et al. Coastcolour round robin protocol in situ reﬂectance data set.

Table 1: Best performance for L1, L2, and Support Vector Regressions

Estimate

CoastColour Turbidity

L2 Regression
L1 Regression

Support Vector Regression

RME Optimal Parameter
108.5 N/A
4.00
2.20
7.87 ×10−4
2.20
0.054 (RBF Kernel, 5 Principal Components)
2.72

Figure 1: Optimal shrinkage parameter (via K-Folds optimization) and associated RME for measured and predicted
with L2 shrinkage for turbidity (FNU).

3

December 11, 2014

CS 229 Project

Joe Adelson

Figure 2: Optimal shrinkage parameter (via K-Folds optimization) and associated RME for measured and predicted
with L1 shrinkage for turbidity (FNU).

Figure 3: Optimal penalty parameter (via K-Folds optimization) and associated RME for SVR

Figure 4: Sweep for the L1 (Lasso) and L2 (Ridge) shrinkage parameters (β).

4

CS229 Project: Identifying Regions of High Turbidity in San

Francisco Bay

Joe Adelson

December 11, 2014

Introduction

Suspended sediments in oceans, seas, and estuaries shape coastal geography, provide important
nutrients to ecosystems, and transport and bury harmful contaminants. Although the problem has
signiﬁcant interest in the scientiﬁc and environmental engineering communities, many of the the
mechanisms involved are poorly understood because of the diﬃculty and expense of measuring what
turns out to be a very complex system. The complexity arises from the fact that many estuaries
are dynamic regions, where the currents that move sediment depend on rainfall, wind, waves, tides,
salinity gradients, and anthropogenic manipulation (dams, weirs, etc.). On top of this the physics
of how sediment particles interact with the sea bed and one another is still poorly understood,
especially at the scales of interest to biologists and coastal engineers. Because of these challenges
the development of tools to study the problem is an active area of research.

The traditional tools of studying sediment transport are in situ ﬁeld measurements and numer-
ical models, and these each face challenges. Because of the small scale of in situ sampling methods,
it is often diﬃcult (and very expensive) to collect data over a large enough area to develop a strong
regional understanding of what is happening. Numerical models have thus gained popularity as a
way to predict suspended sediment concentrations (SSC), erosion, and deposition, but these con-
tend with their own problems of unknown boundary conditions and poorly understood transport
physics for silts and clays. To help ﬁll the gap left in these two technologies, there has been work
to develop remote sensing algorithms using available NASA and European Space Agency (ESA)
satellite imagery to map turbidity, an optical measure of the cloudiness of water, which serves as a
proxy for SSC. This is still a young technology and has substantial room for development.

San Francisco Bay is an excellent example of a system that showcases many of importances of
sediments and the challenges with understanding them. For one, it is a well studied estuary with
ample of measurements about the ﬂow and sediment conditions publicly available.

This project’s goal is to establish a relationship between available remote sensing data and in
situ measurements of turbidity in San Francisco Bay. This requires correlating the data at the
particular pixel of the measurement (and perhaps its neighbors) with the measurement itself. This
has been studied in other estuaries [1] and there are products available that can calculate turbidity,
but these are not calibrated for our area of interest.

Data Collection

There are two primary sources of data for this project: remote sensing satellite data and in situ
point measurements. Satellite images from the MERIS probe taken in the time period of 2006
to 2012 were downloaded from the CoastColour project [3], a data oﬀering from the ESA that
specializes in processing coastal images. Each image pixel contains information of the intensity of
discretized reﬂectance of both visual and infrared wavelengths, as well as precomputed estimates
of turbidity, suspended matter, pigment, and chlorophyll. Because the turbidity is of signiﬁcant

1

December 11, 2014

CS 229 Project

Joe Adelson

interest, a spatially averaged turbidity is calculated in an attempt to create a turbidity feature with
reduced noise.

In situ measurements of turbidity are taken from three United States Geological Survey (USGS)
monitoring stations at Alcatraz Island, the Dumbarton Bridge, and the Richmond-San Rafael
Bridge each taken at a depth of 6 to 8 meters below the surface. These measurements are taken
every 15 minutes.

Preprocessing the data includes extracting useful information for both datasets at matching
times and locations. This includes removing cloudy images as well as ﬁnding the image pixel that
contains the USGS sampling station. In all there are at most three samples for each image, one
for each valid image pixel and station measurement combination, for a total of 679 samples of 44
features each.

Regression Methods

Linear regression with L1 and L2 shrinkage parameters as well as support vector regression were
tested. The examined parameters are: number of principal components, degree of polynomial
expansions of the feature space, and SVR kernels. Many tests were run on the dataset to ﬁnd
the optimal values of these numbers. For each test a parameter sweep for the optimal penalty
logarithmic parameter was done using K-fold cross validation with 5 folds.

In order to ensure that the turbidity levels are non-negative, all regressions are completed using
the log of the turbidity values. The performance metrics are given using the actual turbidity units
The computations use the Python library Scikit-Learn library [2]. The limiting factors for
testing polynomial expansions and number of principal components were both the run time of
the parameter sweeps and the trend towards overﬁtting with high polynomial degrees and many
features. Therefore, polynomial regression with high degrees was limited to using relatively few
principal components.

Results

The linear regressions perform better than support vector regression (table 1). The high order poly-
nomials are generally optimal with very large shrinkage parameters, which implies that high order
polynomials overﬁt the data. Below are mean RME associated with the optimal penalty parameter
for some of the tested PCA and polynomial combinations tested via 5-folds cross validation (ﬁgures
1, 2, 3). As a point of comparison the CoastColour turbidity measure has a root mean square
error (RME) of 108.5. Although this work shows substantial improvement CoastColour sediment
estimate, the R-Squared ﬁt is only 0.22.

PCA does not appear to be an eﬀective tool for eliminating overﬁtting of the data as the full
set of data performed best. Also, polynomial expansion of the features did not improved RME.
The regressions are sensitive to the shrinkage parameter and the search ﬁnds a smooth minimum
for the linear regressions (ﬁgure 4) but SVR is more erratic (ﬁgure 5). These regressions tend to
under predict the large measured turbidities (ﬁgure 6).

2

December 11, 2014

Discussion

CS 229 Project

Joe Adelson

We are able to make a signiﬁcant improvement over the “out-of-the-box” measure of turbidity for
San Francisco Bay. However, the optimal test set R-Squared value of 0.22 suggests that there is
much work to be done in predicting the turbidity. The reason for the poor performance likely
occurs for a variety of reasons. Most apparent is the disparity of scale in the data we use: Because
the satellite data has a spatial resolution of about 300 meters, it will inevitably not be able to pick
up the small scale feature that may aﬀect the in situ measurements. Secondly, there is a three
dimensionality to turbidity and the satellite reads the surfaces, while the in situ measurements are
sampled at a depth of 6-8 meters depending on the tide. There may also be non-linearities between
the satellite data and the actual turbidity that our regression models do not pick up. Future work
includes not only expanding this dataset to more features and testing the value of neural networks,
but also a proposal to conduct our own experiment of taking aerial photographs of the bay, while
measuring the turbidity levels of South San Francisco Bay using boat transects to get a wider
spatial baseline of measurements.

References
[1] R. L. Miller and B. A. McKee. Using {MODIS} terra 250 m imagery to map concentrations of total suspended

matter in coastal waters. Remote Sensing of Environment, 93(1):259 – 266, 2004.

[2] F. e. a. Pedregosa. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–

2830, 2011.

[3] K. Ruddick, C. Sa, S. Bernard, L. Robertson, M. Matthews, R. Doerﬀer, W. Schoenfeld, H. Z. G. HZG, M. S.

Salama, S. Budhiman, et al. Coastcolour round robin protocol in situ reﬂectance data set.

Table 1: Best performance for L1, L2, and Support Vector Regressions

Estimate

CoastColour Turbidity

L2 Regression
L1 Regression

Support Vector Regression

RME Optimal Parameter
108.5 N/A
4.00
2.20
7.87 ×10−4
2.20
0.054 (RBF Kernel, 5 Principal Components)
2.72

Figure 1: Optimal shrinkage parameter (via K-Folds optimization) and associated RME for measured and predicted
with L2 shrinkage for turbidity (FNU).

3

December 11, 2014

CS 229 Project

Joe Adelson

Figure 2: Optimal shrinkage parameter (via K-Folds optimization) and associated RME for measured and predicted
with L1 shrinkage for turbidity (FNU).

Figure 3: Optimal penalty parameter (via K-Folds optimization) and associated RME for SVR

Figure 4: Sweep for the L1 (Lasso) and L2 (Ridge) shrinkage parameters (β).

4

December 11, 2014

CS 229 Project

Joe Adelson

Figure 5: Sweep of the penalty parameter, β for SVR.

Figure 6: Measured turbidity vs. proposed turbidity for the optimal regression (L2 penalty parameter of 4)

5

