CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

1

Ofﬁce Appliance Classiﬁcation

From Disaggregated Plug-Load Data

Gerrit De Moor, Elissa Goldner, Brock Petersen

Department of Civil and Environmental Engineering, Atmosphere/Energy Program

Stanford University, Stanford, CA, 94305, USA

Abstract—As buildings consume 40% of US primary energy,
energy efﬁciency through smart plug-load management can have
a signiﬁcant impact on US carbon emissions. In this paper we
investigate learning algorithms to classify plug-loads based on
minutely energy plug-load data and hourly weather data. One
month of data is preprocessed into 31 daily features which are
used to classify 5660 training samples into 31 labels using a
range of multi-class classiﬁcation algorithms. Of all models tested,
random forest trees performed the best, with a test error of 17%.
This promising result can likely be further reﬁned with improved
feature selection, and a larger dataset. Future work will focus on
increasing the model performance and integrating it into a useful
product for plug-load management companies.

I.

INTRODUCTION

Energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change. While
transportation and industry contribute to this problem, build-
ings represent the largest energy consuming end use in the
United States at approximately 40% of primary energy.

Historically building energy reduction focused on HVAC
and lighting. As those loads have decreased, the importance
of appliance loads in buliding energy use has increased. Ofﬁce
buildings represent approximately 50% of the building energy
use in the United States and appliances consume 22% of that
energy.

Enmetric is a plug-load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in ofﬁce buildings. Recognizing the challenge
faced by increased energy use, Enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants.
Enmetric utilizes smart power strips that sense the appliance
load in virtual real-time. Using the smart power strips in con-
junction with a software platform, building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down.

Learning more about Enmetrics system, the group identiﬁed
two challenges that could be assisted by the use of machine
learning algorithms; classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type. This paper
will explore different techniques to accomplish those goals.

II. MODEL FUNCTION

The main goal of our model is classiﬁcation of disaggregated
loads. Currently, Enmetrics labels each disaggregated load
by hand during installation of the smart power strips. If the
devices are moved, it is the responsibility of the building
manager to report the move to Enmetric. Since Enmetrics data
analytics depend on accurate labeling of devices and because
it is often difﬁcult for the building manager to keep track of
individual employees moving loads around, Enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved. The model developed
in this project learns device signatures from past data and
classiﬁes each load as a particular device on a daily basis. By
comparing yesterdays device classiﬁcations to todays device
classiﬁcations, loads that have been classiﬁed as a new type
can be ﬂagged for review.

III. DATA

Enmetric System smart power strips system takes plug-load
measurements every second but aggregates the measurements
to be stored at the minute level. The energy data collected
included average power, minimum and maximum power during
the second, average frequency, average current, and average
power factor (see table below). For our project, we obtained
the minutely data for over 200 appliances in a 50,000 square
foot ofﬁce building in Silicon Valley. The data covered one
year from 12/1/2013 to 11/31/2014 ( 20 GB). Because of
computational limits, we were restricted to only analyzing one
month at a time ( 2 GB).

To capture the effects weather has on the appliance energy
use, hourly weather data was incorporated in the analysis
as well. The Meso West weather database provided hourly
temperature and relative humidity for Moffet Airﬁeld.

IV. FEATURES AND PREPROCESSING

All the numerical data provided by Enmetric was processed
into features for our models. Since minutely appliance energy
proﬁles vary signiﬁcantly during the day, all data was aggre-
gated and classiﬁed at the day-level for each unique socket.
Daily means and standard deviations of the minutely plug-load
data were calculated and serve as features.

Temporal features in the form of day of week, day of year,
month, and weekend served as additional features. All values
(except for weekend, which is binary) were normalized to
within the range [-1, 1 ] by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

1

Ofﬁce Appliance Classiﬁcation

From Disaggregated Plug-Load Data

Gerrit De Moor, Elissa Goldner, Brock Petersen

Department of Civil and Environmental Engineering, Atmosphere/Energy Program

Stanford University, Stanford, CA, 94305, USA

Abstract—As buildings consume 40% of US primary energy,
energy efﬁciency through smart plug-load management can have
a signiﬁcant impact on US carbon emissions. In this paper we
investigate learning algorithms to classify plug-loads based on
minutely energy plug-load data and hourly weather data. One
month of data is preprocessed into 31 daily features which are
used to classify 5660 training samples into 31 labels using a
range of multi-class classiﬁcation algorithms. Of all models tested,
random forest trees performed the best, with a test error of 17%.
This promising result can likely be further reﬁned with improved
feature selection, and a larger dataset. Future work will focus on
increasing the model performance and integrating it into a useful
product for plug-load management companies.

I.

INTRODUCTION

Energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change. While
transportation and industry contribute to this problem, build-
ings represent the largest energy consuming end use in the
United States at approximately 40% of primary energy.

Historically building energy reduction focused on HVAC
and lighting. As those loads have decreased, the importance
of appliance loads in buliding energy use has increased. Ofﬁce
buildings represent approximately 50% of the building energy
use in the United States and appliances consume 22% of that
energy.

Enmetric is a plug-load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in ofﬁce buildings. Recognizing the challenge
faced by increased energy use, Enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants.
Enmetric utilizes smart power strips that sense the appliance
load in virtual real-time. Using the smart power strips in con-
junction with a software platform, building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down.

Learning more about Enmetrics system, the group identiﬁed
two challenges that could be assisted by the use of machine
learning algorithms; classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type. This paper
will explore different techniques to accomplish those goals.

II. MODEL FUNCTION

The main goal of our model is classiﬁcation of disaggregated
loads. Currently, Enmetrics labels each disaggregated load
by hand during installation of the smart power strips. If the
devices are moved, it is the responsibility of the building
manager to report the move to Enmetric. Since Enmetrics data
analytics depend on accurate labeling of devices and because
it is often difﬁcult for the building manager to keep track of
individual employees moving loads around, Enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved. The model developed
in this project learns device signatures from past data and
classiﬁes each load as a particular device on a daily basis. By
comparing yesterdays device classiﬁcations to todays device
classiﬁcations, loads that have been classiﬁed as a new type
can be ﬂagged for review.

III. DATA

Enmetric System smart power strips system takes plug-load
measurements every second but aggregates the measurements
to be stored at the minute level. The energy data collected
included average power, minimum and maximum power during
the second, average frequency, average current, and average
power factor (see table below). For our project, we obtained
the minutely data for over 200 appliances in a 50,000 square
foot ofﬁce building in Silicon Valley. The data covered one
year from 12/1/2013 to 11/31/2014 ( 20 GB). Because of
computational limits, we were restricted to only analyzing one
month at a time ( 2 GB).

To capture the effects weather has on the appliance energy
use, hourly weather data was incorporated in the analysis
as well. The Meso West weather database provided hourly
temperature and relative humidity for Moffet Airﬁeld.

IV. FEATURES AND PREPROCESSING

All the numerical data provided by Enmetric was processed
into features for our models. Since minutely appliance energy
proﬁles vary signiﬁcantly during the day, all data was aggre-
gated and classiﬁed at the day-level for each unique socket.
Daily means and standard deviations of the minutely plug-load
data were calculated and serve as features.

Temporal features in the form of day of week, day of year,
month, and weekend served as additional features. All values
(except for weekend, which is binary) were normalized to
within the range [-1, 1 ] by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

2

Hourly weather data, temperature and humidity, were pro-
cessed in a similar manner to the Enmetric data. The daily
mean, minimum, maximum and standard deviation were cal-
culated for the temperature and humidity. This brings the total
feature count to 27 and the number of training samples to 5660
(for one month of data). An overview of the data used and how
it was processed into features is shown in the table above.

The plug-load data contained 31 different appliances, result-
ing in 31 labels to be used for classiﬁcation. An overview of
the labels is shown is in the table below.

up the data. In this sense, it is very similar to the classic 30%
test data, 70% training data rule, but with the difference that we
perform this 3 times with random permutations of the dataset.
The trade off between training and testing error for different
values of K is shown below for the Random Forest model. The
variance versus biased nature of the K value is shown. The
graph also supports our selection of a K value of 3.

Fig. 1: Training and testing error from random forest for
different k values.

VI. SUPPORT VECTOR MACHINES

Support vector machines (SVMs) are a set of supervised
learning algorithms commonly used for classiﬁcation. SVMs
are useful for this particular problem because they are very
effective in a high dimensional space. Because they allow
the user to choose the kernel function to act as the decision
function in the model, SVMs are also ﬂexible to what types
of classiﬁcation problems they are suited for. Despite their
effectiveness and versatility, SVMs become less effective when
the number of features is much greater than the number of
samples. Since this was not the case for our classiﬁcation
problem (31 labels, 5660 training samples), the group started
out by classifying using Support Vector Classiﬁcation (SVC)
with a linear kernel and varied different parameters until we
found our best results. The general algorithm for a support
vector machine is as follows:
Given training vectors xi ∈ Rp,i=1,...,n, in two classes, and
a vector y ∈ Rn such that yi ∈ {1,−1}, SVC solves the
following primal problem:

(cid:88)

min
w,b,ζ

1
2

wT w + C

ζi

i=1,n

subject to:

yi(wT φ(xi) + b) ≥ 1 − ζi,

Its dual is

ζi ≥ 0, i = 1, ..., n

min

α

1
2

αT Qα − eT α

(1)

(2)

(3)

(4)

V. CLASSIFICATION MODELS

All learning algorithms considered for use for our project
fell under the category of supervised learning methods for
classiﬁcation. Because the dataset included several unique la-
bels, the project is a multiclass classiﬁcation problem. Several
models were tested with varying parameters to ﬁnd which algo-
rithm resulted in the highest performance rates. The following
popular multiclass classiﬁcation algorithms were tested for our
purposes: multiclass support vector machine (SVM standard),
multi-class SVM one-vs-one (SVM OvO), multi-class SVM
one-vs-rest (SVM OvR), Random Forest, and Gradient Tree
Boosting. A more detailed description of each algorithm, as
well as its strengths and weaknesses with respect
to this
project, can be found below. All algorithms were implemented
in Python, using the sci-kit learn library.

The tuning of the parameters for each algorithm was done
by k-fold cross validation, with a k-value of 3. This rather
small value of k was chosen because we had sufﬁcient training
samples and we wanted to speed up processing times. The
training samples were ﬁrst randomly permuted before splitting

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

1

Ofﬁce Appliance Classiﬁcation

From Disaggregated Plug-Load Data

Gerrit De Moor, Elissa Goldner, Brock Petersen

Department of Civil and Environmental Engineering, Atmosphere/Energy Program

Stanford University, Stanford, CA, 94305, USA

Abstract—As buildings consume 40% of US primary energy,
energy efﬁciency through smart plug-load management can have
a signiﬁcant impact on US carbon emissions. In this paper we
investigate learning algorithms to classify plug-loads based on
minutely energy plug-load data and hourly weather data. One
month of data is preprocessed into 31 daily features which are
used to classify 5660 training samples into 31 labels using a
range of multi-class classiﬁcation algorithms. Of all models tested,
random forest trees performed the best, with a test error of 17%.
This promising result can likely be further reﬁned with improved
feature selection, and a larger dataset. Future work will focus on
increasing the model performance and integrating it into a useful
product for plug-load management companies.

I.

INTRODUCTION

Energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change. While
transportation and industry contribute to this problem, build-
ings represent the largest energy consuming end use in the
United States at approximately 40% of primary energy.

Historically building energy reduction focused on HVAC
and lighting. As those loads have decreased, the importance
of appliance loads in buliding energy use has increased. Ofﬁce
buildings represent approximately 50% of the building energy
use in the United States and appliances consume 22% of that
energy.

Enmetric is a plug-load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in ofﬁce buildings. Recognizing the challenge
faced by increased energy use, Enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants.
Enmetric utilizes smart power strips that sense the appliance
load in virtual real-time. Using the smart power strips in con-
junction with a software platform, building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down.

Learning more about Enmetrics system, the group identiﬁed
two challenges that could be assisted by the use of machine
learning algorithms; classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type. This paper
will explore different techniques to accomplish those goals.

II. MODEL FUNCTION

The main goal of our model is classiﬁcation of disaggregated
loads. Currently, Enmetrics labels each disaggregated load
by hand during installation of the smart power strips. If the
devices are moved, it is the responsibility of the building
manager to report the move to Enmetric. Since Enmetrics data
analytics depend on accurate labeling of devices and because
it is often difﬁcult for the building manager to keep track of
individual employees moving loads around, Enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved. The model developed
in this project learns device signatures from past data and
classiﬁes each load as a particular device on a daily basis. By
comparing yesterdays device classiﬁcations to todays device
classiﬁcations, loads that have been classiﬁed as a new type
can be ﬂagged for review.

III. DATA

Enmetric System smart power strips system takes plug-load
measurements every second but aggregates the measurements
to be stored at the minute level. The energy data collected
included average power, minimum and maximum power during
the second, average frequency, average current, and average
power factor (see table below). For our project, we obtained
the minutely data for over 200 appliances in a 50,000 square
foot ofﬁce building in Silicon Valley. The data covered one
year from 12/1/2013 to 11/31/2014 ( 20 GB). Because of
computational limits, we were restricted to only analyzing one
month at a time ( 2 GB).

To capture the effects weather has on the appliance energy
use, hourly weather data was incorporated in the analysis
as well. The Meso West weather database provided hourly
temperature and relative humidity for Moffet Airﬁeld.

IV. FEATURES AND PREPROCESSING

All the numerical data provided by Enmetric was processed
into features for our models. Since minutely appliance energy
proﬁles vary signiﬁcantly during the day, all data was aggre-
gated and classiﬁed at the day-level for each unique socket.
Daily means and standard deviations of the minutely plug-load
data were calculated and serve as features.

Temporal features in the form of day of week, day of year,
month, and weekend served as additional features. All values
(except for weekend, which is binary) were normalized to
within the range [-1, 1 ] by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

2

Hourly weather data, temperature and humidity, were pro-
cessed in a similar manner to the Enmetric data. The daily
mean, minimum, maximum and standard deviation were cal-
culated for the temperature and humidity. This brings the total
feature count to 27 and the number of training samples to 5660
(for one month of data). An overview of the data used and how
it was processed into features is shown in the table above.

The plug-load data contained 31 different appliances, result-
ing in 31 labels to be used for classiﬁcation. An overview of
the labels is shown is in the table below.

up the data. In this sense, it is very similar to the classic 30%
test data, 70% training data rule, but with the difference that we
perform this 3 times with random permutations of the dataset.
The trade off between training and testing error for different
values of K is shown below for the Random Forest model. The
variance versus biased nature of the K value is shown. The
graph also supports our selection of a K value of 3.

Fig. 1: Training and testing error from random forest for
different k values.

VI. SUPPORT VECTOR MACHINES

Support vector machines (SVMs) are a set of supervised
learning algorithms commonly used for classiﬁcation. SVMs
are useful for this particular problem because they are very
effective in a high dimensional space. Because they allow
the user to choose the kernel function to act as the decision
function in the model, SVMs are also ﬂexible to what types
of classiﬁcation problems they are suited for. Despite their
effectiveness and versatility, SVMs become less effective when
the number of features is much greater than the number of
samples. Since this was not the case for our classiﬁcation
problem (31 labels, 5660 training samples), the group started
out by classifying using Support Vector Classiﬁcation (SVC)
with a linear kernel and varied different parameters until we
found our best results. The general algorithm for a support
vector machine is as follows:
Given training vectors xi ∈ Rp,i=1,...,n, in two classes, and
a vector y ∈ Rn such that yi ∈ {1,−1}, SVC solves the
following primal problem:

(cid:88)

min
w,b,ζ

1
2

wT w + C

ζi

i=1,n

subject to:

yi(wT φ(xi) + b) ≥ 1 − ζi,

Its dual is

ζi ≥ 0, i = 1, ..., n

min

α

1
2

αT Qα − eT α

(1)

(2)

(3)

(4)

V. CLASSIFICATION MODELS

All learning algorithms considered for use for our project
fell under the category of supervised learning methods for
classiﬁcation. Because the dataset included several unique la-
bels, the project is a multiclass classiﬁcation problem. Several
models were tested with varying parameters to ﬁnd which algo-
rithm resulted in the highest performance rates. The following
popular multiclass classiﬁcation algorithms were tested for our
purposes: multiclass support vector machine (SVM standard),
multi-class SVM one-vs-one (SVM OvO), multi-class SVM
one-vs-rest (SVM OvR), Random Forest, and Gradient Tree
Boosting. A more detailed description of each algorithm, as
well as its strengths and weaknesses with respect
to this
project, can be found below. All algorithms were implemented
in Python, using the sci-kit learn library.

The tuning of the parameters for each algorithm was done
by k-fold cross validation, with a k-value of 3. This rather
small value of k was chosen because we had sufﬁcient training
samples and we wanted to speed up processing times. The
training samples were ﬁrst randomly permuted before splitting

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

3

subject to:

yT α = 0

0 ≤ αi ≤ C, i = 1, ..., l

(5)

(6)

where e is the vector of all ones, C ¿ 0 is the upper bound,
Q is an n by n positive semideﬁnite matrix, Qij ≡ K(xi, xj)
and φ(xi)T φ(x) is the kernel [4].

There are two types of approaches for multi-class SVMs:
One-vs-the-Rest (OvR) and One-vs-One (OvO). One-vs-the-
Rest considers training a single classiﬁer per class while the
One-vs-One trains N(N-1) binary classiﬁers for a N-way multi-
class problem [5]. A more detailed description of each method
can be found in the subsequent sections. Research has shown
that OvR and OvO are among the most suitable methods for
practical use [3].

A. One-Versus-One SVM

The ﬁrst type of SVM that was used for this project was
the standard SVC with a linear kernel found in the sci-kit
learn package. By default, SVC implements the one-versus-one
approach to multi-class classiﬁcation. OvO classiﬁcation will
build N(N-1) classiﬁers, one classiﬁer to distinguish between
each pair of classes i and j by marking i as the positive example
and j as the negative. When it is time to make a prediction
for a particular unseen sample, all classiﬁers are applied to
the sample and the class with the highest number of positive
predictions will be chosen.

rbf kernel : exp(−γ|x − x(cid:48)|2)

(7)

For the OvO SVM, we started by tuning the penalty pa-
rameter of the error term (C), which can also be seen as the
hyperplane separation term. While initially, increasing C lead
to smaller prediction errors, values larger than 10 resulted in
overﬁtting the training sample and hence an increase in testing
error. It was expected that small values of C would result
in more misclassiﬁcation, but we found that imporvements
reached their limit as C approaced a value of 10.The best
results for our model were found by using a radial basis
function (RBF) kernel, also know as the Gaussian kernel.
Varying values of gamma, the kernel coefﬁcient used with RBF
kernels, produced no improvement in predictions. A graphical
view of the effects of varying C values on the OvO support
vector classiﬁcation model can be found in Figure 2.

B. One-Versus-The-Rest SVM

As opposed to OvO where a sample is marked as in class
i and not in class j, One-vs-the-Rest will build N different
binary classiﬁers to determine if the example is either in class
i or not in class i [5]. For SVM OvR, the outputs and ideal
tuning parameters were very similar to standard SVM, as seen
in Figure 3. Similar to OvO SVM, the ideal parameters for the
OvR SVM were found to be C=10 with a RBF kernel.

Fig. 2: Training and testing error from One-versus-One support
vector classiﬁcation when varying C value.

Fig. 3: Training and testing error from One-versus-the-Rest
support vector classiﬁcation when varying C value.

VII. ENSEMBLE METHODS

Since SVM results were still showing signiﬁcant error
rates, the group decided to investigate ensemble methods for
classiﬁcation. The goal of ensemble methods is to combine
the predictions of several base estimators built with a given
learning algorithm to improve the robustness over a single
estimator [4]. By combining multiple hypotheses that may
produce weak results for a particular classiﬁcation problem,
ensembles methods aim to build a stronger, more accurate
hypothesis tailored for the individual problem. The two types
of ensemble methods used for our classiﬁcation problem were
random forest classiﬁcation and gradient tree boosting.

A. Random Forest

The random forest classiﬁcation method falls under the
category of an textitaveraging ensemble method. The driving
principle behind averaging methods is to build several esti-
mators independently and to average the resulting predictions
together in order to reduce the variance in predictions [4]. In
random forest classiﬁcation, a series of randomized decision
trees in the ensemble are built from a random subset of the
training set. Each subset used is a sample drawn from the
feature vector that is replaced after use for continued use in

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

1

Ofﬁce Appliance Classiﬁcation

From Disaggregated Plug-Load Data

Gerrit De Moor, Elissa Goldner, Brock Petersen

Department of Civil and Environmental Engineering, Atmosphere/Energy Program

Stanford University, Stanford, CA, 94305, USA

Abstract—As buildings consume 40% of US primary energy,
energy efﬁciency through smart plug-load management can have
a signiﬁcant impact on US carbon emissions. In this paper we
investigate learning algorithms to classify plug-loads based on
minutely energy plug-load data and hourly weather data. One
month of data is preprocessed into 31 daily features which are
used to classify 5660 training samples into 31 labels using a
range of multi-class classiﬁcation algorithms. Of all models tested,
random forest trees performed the best, with a test error of 17%.
This promising result can likely be further reﬁned with improved
feature selection, and a larger dataset. Future work will focus on
increasing the model performance and integrating it into a useful
product for plug-load management companies.

I.

INTRODUCTION

Energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change. While
transportation and industry contribute to this problem, build-
ings represent the largest energy consuming end use in the
United States at approximately 40% of primary energy.

Historically building energy reduction focused on HVAC
and lighting. As those loads have decreased, the importance
of appliance loads in buliding energy use has increased. Ofﬁce
buildings represent approximately 50% of the building energy
use in the United States and appliances consume 22% of that
energy.

Enmetric is a plug-load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in ofﬁce buildings. Recognizing the challenge
faced by increased energy use, Enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants.
Enmetric utilizes smart power strips that sense the appliance
load in virtual real-time. Using the smart power strips in con-
junction with a software platform, building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down.

Learning more about Enmetrics system, the group identiﬁed
two challenges that could be assisted by the use of machine
learning algorithms; classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type. This paper
will explore different techniques to accomplish those goals.

II. MODEL FUNCTION

The main goal of our model is classiﬁcation of disaggregated
loads. Currently, Enmetrics labels each disaggregated load
by hand during installation of the smart power strips. If the
devices are moved, it is the responsibility of the building
manager to report the move to Enmetric. Since Enmetrics data
analytics depend on accurate labeling of devices and because
it is often difﬁcult for the building manager to keep track of
individual employees moving loads around, Enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved. The model developed
in this project learns device signatures from past data and
classiﬁes each load as a particular device on a daily basis. By
comparing yesterdays device classiﬁcations to todays device
classiﬁcations, loads that have been classiﬁed as a new type
can be ﬂagged for review.

III. DATA

Enmetric System smart power strips system takes plug-load
measurements every second but aggregates the measurements
to be stored at the minute level. The energy data collected
included average power, minimum and maximum power during
the second, average frequency, average current, and average
power factor (see table below). For our project, we obtained
the minutely data for over 200 appliances in a 50,000 square
foot ofﬁce building in Silicon Valley. The data covered one
year from 12/1/2013 to 11/31/2014 ( 20 GB). Because of
computational limits, we were restricted to only analyzing one
month at a time ( 2 GB).

To capture the effects weather has on the appliance energy
use, hourly weather data was incorporated in the analysis
as well. The Meso West weather database provided hourly
temperature and relative humidity for Moffet Airﬁeld.

IV. FEATURES AND PREPROCESSING

All the numerical data provided by Enmetric was processed
into features for our models. Since minutely appliance energy
proﬁles vary signiﬁcantly during the day, all data was aggre-
gated and classiﬁed at the day-level for each unique socket.
Daily means and standard deviations of the minutely plug-load
data were calculated and serve as features.

Temporal features in the form of day of week, day of year,
month, and weekend served as additional features. All values
(except for weekend, which is binary) were normalized to
within the range [-1, 1 ] by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

2

Hourly weather data, temperature and humidity, were pro-
cessed in a similar manner to the Enmetric data. The daily
mean, minimum, maximum and standard deviation were cal-
culated for the temperature and humidity. This brings the total
feature count to 27 and the number of training samples to 5660
(for one month of data). An overview of the data used and how
it was processed into features is shown in the table above.

The plug-load data contained 31 different appliances, result-
ing in 31 labels to be used for classiﬁcation. An overview of
the labels is shown is in the table below.

up the data. In this sense, it is very similar to the classic 30%
test data, 70% training data rule, but with the difference that we
perform this 3 times with random permutations of the dataset.
The trade off between training and testing error for different
values of K is shown below for the Random Forest model. The
variance versus biased nature of the K value is shown. The
graph also supports our selection of a K value of 3.

Fig. 1: Training and testing error from random forest for
different k values.

VI. SUPPORT VECTOR MACHINES

Support vector machines (SVMs) are a set of supervised
learning algorithms commonly used for classiﬁcation. SVMs
are useful for this particular problem because they are very
effective in a high dimensional space. Because they allow
the user to choose the kernel function to act as the decision
function in the model, SVMs are also ﬂexible to what types
of classiﬁcation problems they are suited for. Despite their
effectiveness and versatility, SVMs become less effective when
the number of features is much greater than the number of
samples. Since this was not the case for our classiﬁcation
problem (31 labels, 5660 training samples), the group started
out by classifying using Support Vector Classiﬁcation (SVC)
with a linear kernel and varied different parameters until we
found our best results. The general algorithm for a support
vector machine is as follows:
Given training vectors xi ∈ Rp,i=1,...,n, in two classes, and
a vector y ∈ Rn such that yi ∈ {1,−1}, SVC solves the
following primal problem:

(cid:88)

min
w,b,ζ

1
2

wT w + C

ζi

i=1,n

subject to:

yi(wT φ(xi) + b) ≥ 1 − ζi,

Its dual is

ζi ≥ 0, i = 1, ..., n

min

α

1
2

αT Qα − eT α

(1)

(2)

(3)

(4)

V. CLASSIFICATION MODELS

All learning algorithms considered for use for our project
fell under the category of supervised learning methods for
classiﬁcation. Because the dataset included several unique la-
bels, the project is a multiclass classiﬁcation problem. Several
models were tested with varying parameters to ﬁnd which algo-
rithm resulted in the highest performance rates. The following
popular multiclass classiﬁcation algorithms were tested for our
purposes: multiclass support vector machine (SVM standard),
multi-class SVM one-vs-one (SVM OvO), multi-class SVM
one-vs-rest (SVM OvR), Random Forest, and Gradient Tree
Boosting. A more detailed description of each algorithm, as
well as its strengths and weaknesses with respect
to this
project, can be found below. All algorithms were implemented
in Python, using the sci-kit learn library.

The tuning of the parameters for each algorithm was done
by k-fold cross validation, with a k-value of 3. This rather
small value of k was chosen because we had sufﬁcient training
samples and we wanted to speed up processing times. The
training samples were ﬁrst randomly permuted before splitting

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

3

subject to:

yT α = 0

0 ≤ αi ≤ C, i = 1, ..., l

(5)

(6)

where e is the vector of all ones, C ¿ 0 is the upper bound,
Q is an n by n positive semideﬁnite matrix, Qij ≡ K(xi, xj)
and φ(xi)T φ(x) is the kernel [4].

There are two types of approaches for multi-class SVMs:
One-vs-the-Rest (OvR) and One-vs-One (OvO). One-vs-the-
Rest considers training a single classiﬁer per class while the
One-vs-One trains N(N-1) binary classiﬁers for a N-way multi-
class problem [5]. A more detailed description of each method
can be found in the subsequent sections. Research has shown
that OvR and OvO are among the most suitable methods for
practical use [3].

A. One-Versus-One SVM

The ﬁrst type of SVM that was used for this project was
the standard SVC with a linear kernel found in the sci-kit
learn package. By default, SVC implements the one-versus-one
approach to multi-class classiﬁcation. OvO classiﬁcation will
build N(N-1) classiﬁers, one classiﬁer to distinguish between
each pair of classes i and j by marking i as the positive example
and j as the negative. When it is time to make a prediction
for a particular unseen sample, all classiﬁers are applied to
the sample and the class with the highest number of positive
predictions will be chosen.

rbf kernel : exp(−γ|x − x(cid:48)|2)

(7)

For the OvO SVM, we started by tuning the penalty pa-
rameter of the error term (C), which can also be seen as the
hyperplane separation term. While initially, increasing C lead
to smaller prediction errors, values larger than 10 resulted in
overﬁtting the training sample and hence an increase in testing
error. It was expected that small values of C would result
in more misclassiﬁcation, but we found that imporvements
reached their limit as C approaced a value of 10.The best
results for our model were found by using a radial basis
function (RBF) kernel, also know as the Gaussian kernel.
Varying values of gamma, the kernel coefﬁcient used with RBF
kernels, produced no improvement in predictions. A graphical
view of the effects of varying C values on the OvO support
vector classiﬁcation model can be found in Figure 2.

B. One-Versus-The-Rest SVM

As opposed to OvO where a sample is marked as in class
i and not in class j, One-vs-the-Rest will build N different
binary classiﬁers to determine if the example is either in class
i or not in class i [5]. For SVM OvR, the outputs and ideal
tuning parameters were very similar to standard SVM, as seen
in Figure 3. Similar to OvO SVM, the ideal parameters for the
OvR SVM were found to be C=10 with a RBF kernel.

Fig. 2: Training and testing error from One-versus-One support
vector classiﬁcation when varying C value.

Fig. 3: Training and testing error from One-versus-the-Rest
support vector classiﬁcation when varying C value.

VII. ENSEMBLE METHODS

Since SVM results were still showing signiﬁcant error
rates, the group decided to investigate ensemble methods for
classiﬁcation. The goal of ensemble methods is to combine
the predictions of several base estimators built with a given
learning algorithm to improve the robustness over a single
estimator [4]. By combining multiple hypotheses that may
produce weak results for a particular classiﬁcation problem,
ensembles methods aim to build a stronger, more accurate
hypothesis tailored for the individual problem. The two types
of ensemble methods used for our classiﬁcation problem were
random forest classiﬁcation and gradient tree boosting.

A. Random Forest

The random forest classiﬁcation method falls under the
category of an textitaveraging ensemble method. The driving
principle behind averaging methods is to build several esti-
mators independently and to average the resulting predictions
together in order to reduce the variance in predictions [4]. In
random forest classiﬁcation, a series of randomized decision
trees in the ensemble are built from a random subset of the
training set. Each subset used is a sample drawn from the
feature vector that is replaced after use for continued use in

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

4

subsequent tests. When splitting a node in the construction of
the tree, instead of choosing the best split that is best ﬁt for
the entire training set, the split chosen is the best ﬁt among
the subset of data. Although the randomness of these subsets
usually produces results with higher bias, the nature of the
averaging ensemble method also decreases the variance among
results, typically much greater than the increase in bias.

When running random forest classiﬁcation, we varied the the
number of estimators and maximum depth used in the algo-
rithm until we found optimal results. Intuitively, we expected
the performance to increase as we increased the number of
estimators (i.e. the number of trees), but this came with the
trade-off of a longer computational time. As can be seen in
Figure 4, the improvement in training and testing error as we
increased the number of estimators was not as great as we had
expected. Surprisingly for the group, varying the max depth of
the trees had a much greater effect on performance, as seen in
Figure 5. The optimal values of max depth and n-estimators
found for our model were 17 and 100, respectively.
Given training vectors xi ∈ Rn,i=1,..., l and a label vector
y ∈ Rl, a decision tree recursively partitions the space such
that the samples with the same labels are grouped together. Let
the data at node m be represented by Q. For each candidate
split θ = (j, tm) consisting of a feature j and threshold tm,
partition the data into Qlef t(θ) and Qright(θ) subsets

Qlef t(θ) = (x, y)|xj <= tm

Qright(θ) = Q \ Qlef t(θ)

The impurity at m is computed using an impurity function
H(), the choice of which depends on the task being solved
(classiﬁcation or regression)

G(Q, θ) =

nlef t
Nm

H(Qlef t(θ)) +

nright
Nm

H(Qright(θ))

Select the parameters that minimises the impurity

θ∗ =θ G(Q, θ)

Recurse for subsets Qlef t(θ∗) and Qright(θ∗) until the maxi-
mum allowable depth is reached, Nm < minsamples orNm =
1.

Fig. 5: Training and testing error from random forest classiﬁ-
cation when varying max depth.

B. Gradient Tree Boosting

The ﬁrst of two algorithms that we looked at is called gra-
dient boosting classiﬁcation. Gradient tree boosting combines
weak learning methods in an iterative manner until a single,
stronger learning method is created. The gradient tree boosting
method was chosen for our classiﬁcation problem because it
is typically very good at handling data with heterogeneous
features. More concern with this algorithm was around the
scalability of its results; although it proved effective for our
data sets in month-long intervals, it was thought that combining
that data into a full year could cause issues due to the sequen-
tial nature of boosting limiting its ability to be parallelized
[4].

Since gradient tree boosting is another ensemble method, the
group expected that varying the number of estimators and the
max depth of the tree branches would act in similar manners as
they did in the random forest algorithm. As seen in Figures 6
and 7, our assumptions were largely correct. More interesting
however, was the effect that learning rate had on the model
results, shown in Figure 8. After varying the parameters in
the gradient tree boosting algorithm, the optimal values of n-
estimators, learning rate, and max depth found for our model
were 60, 0.1, and 10, respectively.

Fig. 4: Training and testing error from random forest classiﬁ-
cation when varying number of estimators.

Fig. 6: Training and testing error from gradient tree boosting
classiﬁcation when varying number of estimators.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

1

Ofﬁce Appliance Classiﬁcation

From Disaggregated Plug-Load Data

Gerrit De Moor, Elissa Goldner, Brock Petersen

Department of Civil and Environmental Engineering, Atmosphere/Energy Program

Stanford University, Stanford, CA, 94305, USA

Abstract—As buildings consume 40% of US primary energy,
energy efﬁciency through smart plug-load management can have
a signiﬁcant impact on US carbon emissions. In this paper we
investigate learning algorithms to classify plug-loads based on
minutely energy plug-load data and hourly weather data. One
month of data is preprocessed into 31 daily features which are
used to classify 5660 training samples into 31 labels using a
range of multi-class classiﬁcation algorithms. Of all models tested,
random forest trees performed the best, with a test error of 17%.
This promising result can likely be further reﬁned with improved
feature selection, and a larger dataset. Future work will focus on
increasing the model performance and integrating it into a useful
product for plug-load management companies.

I.

INTRODUCTION

Energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change. While
transportation and industry contribute to this problem, build-
ings represent the largest energy consuming end use in the
United States at approximately 40% of primary energy.

Historically building energy reduction focused on HVAC
and lighting. As those loads have decreased, the importance
of appliance loads in buliding energy use has increased. Ofﬁce
buildings represent approximately 50% of the building energy
use in the United States and appliances consume 22% of that
energy.

Enmetric is a plug-load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in ofﬁce buildings. Recognizing the challenge
faced by increased energy use, Enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants.
Enmetric utilizes smart power strips that sense the appliance
load in virtual real-time. Using the smart power strips in con-
junction with a software platform, building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down.

Learning more about Enmetrics system, the group identiﬁed
two challenges that could be assisted by the use of machine
learning algorithms; classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type. This paper
will explore different techniques to accomplish those goals.

II. MODEL FUNCTION

The main goal of our model is classiﬁcation of disaggregated
loads. Currently, Enmetrics labels each disaggregated load
by hand during installation of the smart power strips. If the
devices are moved, it is the responsibility of the building
manager to report the move to Enmetric. Since Enmetrics data
analytics depend on accurate labeling of devices and because
it is often difﬁcult for the building manager to keep track of
individual employees moving loads around, Enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved. The model developed
in this project learns device signatures from past data and
classiﬁes each load as a particular device on a daily basis. By
comparing yesterdays device classiﬁcations to todays device
classiﬁcations, loads that have been classiﬁed as a new type
can be ﬂagged for review.

III. DATA

Enmetric System smart power strips system takes plug-load
measurements every second but aggregates the measurements
to be stored at the minute level. The energy data collected
included average power, minimum and maximum power during
the second, average frequency, average current, and average
power factor (see table below). For our project, we obtained
the minutely data for over 200 appliances in a 50,000 square
foot ofﬁce building in Silicon Valley. The data covered one
year from 12/1/2013 to 11/31/2014 ( 20 GB). Because of
computational limits, we were restricted to only analyzing one
month at a time ( 2 GB).

To capture the effects weather has on the appliance energy
use, hourly weather data was incorporated in the analysis
as well. The Meso West weather database provided hourly
temperature and relative humidity for Moffet Airﬁeld.

IV. FEATURES AND PREPROCESSING

All the numerical data provided by Enmetric was processed
into features for our models. Since minutely appliance energy
proﬁles vary signiﬁcantly during the day, all data was aggre-
gated and classiﬁed at the day-level for each unique socket.
Daily means and standard deviations of the minutely plug-load
data were calculated and serve as features.

Temporal features in the form of day of week, day of year,
month, and weekend served as additional features. All values
(except for weekend, which is binary) were normalized to
within the range [-1, 1 ] by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

2

Hourly weather data, temperature and humidity, were pro-
cessed in a similar manner to the Enmetric data. The daily
mean, minimum, maximum and standard deviation were cal-
culated for the temperature and humidity. This brings the total
feature count to 27 and the number of training samples to 5660
(for one month of data). An overview of the data used and how
it was processed into features is shown in the table above.

The plug-load data contained 31 different appliances, result-
ing in 31 labels to be used for classiﬁcation. An overview of
the labels is shown is in the table below.

up the data. In this sense, it is very similar to the classic 30%
test data, 70% training data rule, but with the difference that we
perform this 3 times with random permutations of the dataset.
The trade off between training and testing error for different
values of K is shown below for the Random Forest model. The
variance versus biased nature of the K value is shown. The
graph also supports our selection of a K value of 3.

Fig. 1: Training and testing error from random forest for
different k values.

VI. SUPPORT VECTOR MACHINES

Support vector machines (SVMs) are a set of supervised
learning algorithms commonly used for classiﬁcation. SVMs
are useful for this particular problem because they are very
effective in a high dimensional space. Because they allow
the user to choose the kernel function to act as the decision
function in the model, SVMs are also ﬂexible to what types
of classiﬁcation problems they are suited for. Despite their
effectiveness and versatility, SVMs become less effective when
the number of features is much greater than the number of
samples. Since this was not the case for our classiﬁcation
problem (31 labels, 5660 training samples), the group started
out by classifying using Support Vector Classiﬁcation (SVC)
with a linear kernel and varied different parameters until we
found our best results. The general algorithm for a support
vector machine is as follows:
Given training vectors xi ∈ Rp,i=1,...,n, in two classes, and
a vector y ∈ Rn such that yi ∈ {1,−1}, SVC solves the
following primal problem:

(cid:88)

min
w,b,ζ

1
2

wT w + C

ζi

i=1,n

subject to:

yi(wT φ(xi) + b) ≥ 1 − ζi,

Its dual is

ζi ≥ 0, i = 1, ..., n

min

α

1
2

αT Qα − eT α

(1)

(2)

(3)

(4)

V. CLASSIFICATION MODELS

All learning algorithms considered for use for our project
fell under the category of supervised learning methods for
classiﬁcation. Because the dataset included several unique la-
bels, the project is a multiclass classiﬁcation problem. Several
models were tested with varying parameters to ﬁnd which algo-
rithm resulted in the highest performance rates. The following
popular multiclass classiﬁcation algorithms were tested for our
purposes: multiclass support vector machine (SVM standard),
multi-class SVM one-vs-one (SVM OvO), multi-class SVM
one-vs-rest (SVM OvR), Random Forest, and Gradient Tree
Boosting. A more detailed description of each algorithm, as
well as its strengths and weaknesses with respect
to this
project, can be found below. All algorithms were implemented
in Python, using the sci-kit learn library.

The tuning of the parameters for each algorithm was done
by k-fold cross validation, with a k-value of 3. This rather
small value of k was chosen because we had sufﬁcient training
samples and we wanted to speed up processing times. The
training samples were ﬁrst randomly permuted before splitting

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

3

subject to:

yT α = 0

0 ≤ αi ≤ C, i = 1, ..., l

(5)

(6)

where e is the vector of all ones, C ¿ 0 is the upper bound,
Q is an n by n positive semideﬁnite matrix, Qij ≡ K(xi, xj)
and φ(xi)T φ(x) is the kernel [4].

There are two types of approaches for multi-class SVMs:
One-vs-the-Rest (OvR) and One-vs-One (OvO). One-vs-the-
Rest considers training a single classiﬁer per class while the
One-vs-One trains N(N-1) binary classiﬁers for a N-way multi-
class problem [5]. A more detailed description of each method
can be found in the subsequent sections. Research has shown
that OvR and OvO are among the most suitable methods for
practical use [3].

A. One-Versus-One SVM

The ﬁrst type of SVM that was used for this project was
the standard SVC with a linear kernel found in the sci-kit
learn package. By default, SVC implements the one-versus-one
approach to multi-class classiﬁcation. OvO classiﬁcation will
build N(N-1) classiﬁers, one classiﬁer to distinguish between
each pair of classes i and j by marking i as the positive example
and j as the negative. When it is time to make a prediction
for a particular unseen sample, all classiﬁers are applied to
the sample and the class with the highest number of positive
predictions will be chosen.

rbf kernel : exp(−γ|x − x(cid:48)|2)

(7)

For the OvO SVM, we started by tuning the penalty pa-
rameter of the error term (C), which can also be seen as the
hyperplane separation term. While initially, increasing C lead
to smaller prediction errors, values larger than 10 resulted in
overﬁtting the training sample and hence an increase in testing
error. It was expected that small values of C would result
in more misclassiﬁcation, but we found that imporvements
reached their limit as C approaced a value of 10.The best
results for our model were found by using a radial basis
function (RBF) kernel, also know as the Gaussian kernel.
Varying values of gamma, the kernel coefﬁcient used with RBF
kernels, produced no improvement in predictions. A graphical
view of the effects of varying C values on the OvO support
vector classiﬁcation model can be found in Figure 2.

B. One-Versus-The-Rest SVM

As opposed to OvO where a sample is marked as in class
i and not in class j, One-vs-the-Rest will build N different
binary classiﬁers to determine if the example is either in class
i or not in class i [5]. For SVM OvR, the outputs and ideal
tuning parameters were very similar to standard SVM, as seen
in Figure 3. Similar to OvO SVM, the ideal parameters for the
OvR SVM were found to be C=10 with a RBF kernel.

Fig. 2: Training and testing error from One-versus-One support
vector classiﬁcation when varying C value.

Fig. 3: Training and testing error from One-versus-the-Rest
support vector classiﬁcation when varying C value.

VII. ENSEMBLE METHODS

Since SVM results were still showing signiﬁcant error
rates, the group decided to investigate ensemble methods for
classiﬁcation. The goal of ensemble methods is to combine
the predictions of several base estimators built with a given
learning algorithm to improve the robustness over a single
estimator [4]. By combining multiple hypotheses that may
produce weak results for a particular classiﬁcation problem,
ensembles methods aim to build a stronger, more accurate
hypothesis tailored for the individual problem. The two types
of ensemble methods used for our classiﬁcation problem were
random forest classiﬁcation and gradient tree boosting.

A. Random Forest

The random forest classiﬁcation method falls under the
category of an textitaveraging ensemble method. The driving
principle behind averaging methods is to build several esti-
mators independently and to average the resulting predictions
together in order to reduce the variance in predictions [4]. In
random forest classiﬁcation, a series of randomized decision
trees in the ensemble are built from a random subset of the
training set. Each subset used is a sample drawn from the
feature vector that is replaced after use for continued use in

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

4

subsequent tests. When splitting a node in the construction of
the tree, instead of choosing the best split that is best ﬁt for
the entire training set, the split chosen is the best ﬁt among
the subset of data. Although the randomness of these subsets
usually produces results with higher bias, the nature of the
averaging ensemble method also decreases the variance among
results, typically much greater than the increase in bias.

When running random forest classiﬁcation, we varied the the
number of estimators and maximum depth used in the algo-
rithm until we found optimal results. Intuitively, we expected
the performance to increase as we increased the number of
estimators (i.e. the number of trees), but this came with the
trade-off of a longer computational time. As can be seen in
Figure 4, the improvement in training and testing error as we
increased the number of estimators was not as great as we had
expected. Surprisingly for the group, varying the max depth of
the trees had a much greater effect on performance, as seen in
Figure 5. The optimal values of max depth and n-estimators
found for our model were 17 and 100, respectively.
Given training vectors xi ∈ Rn,i=1,..., l and a label vector
y ∈ Rl, a decision tree recursively partitions the space such
that the samples with the same labels are grouped together. Let
the data at node m be represented by Q. For each candidate
split θ = (j, tm) consisting of a feature j and threshold tm,
partition the data into Qlef t(θ) and Qright(θ) subsets

Qlef t(θ) = (x, y)|xj <= tm

Qright(θ) = Q \ Qlef t(θ)

The impurity at m is computed using an impurity function
H(), the choice of which depends on the task being solved
(classiﬁcation or regression)

G(Q, θ) =

nlef t
Nm

H(Qlef t(θ)) +

nright
Nm

H(Qright(θ))

Select the parameters that minimises the impurity

θ∗ =θ G(Q, θ)

Recurse for subsets Qlef t(θ∗) and Qright(θ∗) until the maxi-
mum allowable depth is reached, Nm < minsamples orNm =
1.

Fig. 5: Training and testing error from random forest classiﬁ-
cation when varying max depth.

B. Gradient Tree Boosting

The ﬁrst of two algorithms that we looked at is called gra-
dient boosting classiﬁcation. Gradient tree boosting combines
weak learning methods in an iterative manner until a single,
stronger learning method is created. The gradient tree boosting
method was chosen for our classiﬁcation problem because it
is typically very good at handling data with heterogeneous
features. More concern with this algorithm was around the
scalability of its results; although it proved effective for our
data sets in month-long intervals, it was thought that combining
that data into a full year could cause issues due to the sequen-
tial nature of boosting limiting its ability to be parallelized
[4].

Since gradient tree boosting is another ensemble method, the
group expected that varying the number of estimators and the
max depth of the tree branches would act in similar manners as
they did in the random forest algorithm. As seen in Figures 6
and 7, our assumptions were largely correct. More interesting
however, was the effect that learning rate had on the model
results, shown in Figure 8. After varying the parameters in
the gradient tree boosting algorithm, the optimal values of n-
estimators, learning rate, and max depth found for our model
were 60, 0.1, and 10, respectively.

Fig. 4: Training and testing error from random forest classiﬁ-
cation when varying number of estimators.

Fig. 6: Training and testing error from gradient tree boosting
classiﬁcation when varying number of estimators.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

5

vs-the-rest) did not improve the performance. The Random
Forest algorithm performed the best in terms of speed and test
error. The algorithm ran in under 15 seconds and resulted in
a test error of 17%. Gradient Tree Boosting resulted in the
lowest training error and performed only slightly worse than
the Random Forest algorithm in terms of test error.

Our work has shown that machine learning algorithms can
be a useful, efﬁcient tools in classiﬁcation of ofﬁce appliances
from plug load data. Future work should be able to further
improve the classiﬁcation to more acceptable levels of test
errors. In particular, improved feature extraction and selection,
and a larger dataset look promising.

X. CONTINUED WORK

The work presented in this paper shows that

learning
algorithms can be an effective tool for Enmetrics Systems
to use in their smart plug analytic services. The group will
be presenting our ﬁndings at Enmetric Systems next month
with the intentions of continuing further and producing a
useful tool for the company to possibly use. Based on the
successful results developed throughout
the course of this
project, the following are potential actions that may be taken
in the continuation of the project:

1) Run the ﬁve learning algorithms presented in this poster
on the larger dataset, varying the necessary parameters
to ﬁnd the best ﬁt model for classiﬁcation.

2) Develop an iterative algorithm that

learns based on
a collection of past data, predicts for the day, and
compares to known to identify any location where
deviced could have been moved or unplugged.

3) Explore the space of prediction to eliminate the need
for Enmetric to record by hand the type of device at
each plug at installation.

ACKNOWLEDGMENT

We would like to thank Jake Masters and Brad Degnan of
Enmentric Systems, Inc. for providing us with the data and
motivation for this project. We would also like to thank Romain
Juban of C3 Energy for his advice and guidance on the use of
learning algorithms with load disaggregation and classiﬁcation.
Finally, we would like to thank Professor Andrew Ng and the
CS229 teaching and project assistants for the help, guidance,
and encouragement.

REFERENCES

[1] H. Kopka and P. W. Daly, A Guide to LATEX, 3rd ed. Harlow, England:

Addison-Wesley, 1999.

[2] Machine Learning Applications for Load, Price and Wind Power Pre-
diction in Power Systems, Michael Negnevitsky, Senior Member, IEEE,
Paras Mandal, Member, IEEE, and Anurag K. Srivastava, Member, IEEE
[3] H. Lei and V. Govindaraju, Half-Against-Half Multi-class Support
Vector Machines, State University of New York at Buffalo, http://blue.
utb.edu/hlei/Research/papers/Half HalfSVM.pdf

[4] F. Pedregosa, et.al, Scikit-learn: Machine Learning in Python, Journal

of Machine Learning Research, vol. 12, pg. 2825-2830, 2011
http://www.mit.edu/∼9.520/spring09/Classes/multiclass.pdf
http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1583738&
url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs all.jsp%
3Farnumber%3D1583738

Fig. 7: Training and testing error from gradient tree boosting
classiﬁcation when varying max depth.

Fig. 8: Training and testing error from gradient tree boosting
classiﬁcation when varying learning rate.

VIII. RESULTS

All three SVM algorithms produced the greatest training and
test errors seen out of the ﬁve algorithms used. Trying different
binary SVM decompositions (OvO and OvR) did not improve
the performance. The Random Forest algorithm performed the
best in terms of speed and test error. The algorithm ran in under
15 seconds and resulted in a test error of 17%. Gradient Tree
Boosting resulted in the lowest training error and performed
only slightly worse than the Random Forest algorithm in terms
of test error.

The graphs show that tuning the main parameters of the
algorithms has a signiﬁcant impact on the test error. Biased-
variance trade off does show in some of the tuning, as can
be expected when tuning certain parameters such as the C
parameter in SVM models.

The performance of the Random Forest conﬁrmed the fact
that random forests usually work faster, but need a sufﬁcient
dataset size to work its randomization concept.

IX. CONCLUSION

SVM performed worst of the algorithms we tested. Trying
different binary SVM decompositions (one-vs-one and one-

[5]
[6]

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

1

Ofﬁce Appliance Classiﬁcation

From Disaggregated Plug-Load Data

Gerrit De Moor, Elissa Goldner, Brock Petersen

Department of Civil and Environmental Engineering, Atmosphere/Energy Program

Stanford University, Stanford, CA, 94305, USA

Abstract—As buildings consume 40% of US primary energy,
energy efﬁciency through smart plug-load management can have
a signiﬁcant impact on US carbon emissions. In this paper we
investigate learning algorithms to classify plug-loads based on
minutely energy plug-load data and hourly weather data. One
month of data is preprocessed into 31 daily features which are
used to classify 5660 training samples into 31 labels using a
range of multi-class classiﬁcation algorithms. Of all models tested,
random forest trees performed the best, with a test error of 17%.
This promising result can likely be further reﬁned with improved
feature selection, and a larger dataset. Future work will focus on
increasing the model performance and integrating it into a useful
product for plug-load management companies.

I.

INTRODUCTION

Energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change. While
transportation and industry contribute to this problem, build-
ings represent the largest energy consuming end use in the
United States at approximately 40% of primary energy.

Historically building energy reduction focused on HVAC
and lighting. As those loads have decreased, the importance
of appliance loads in buliding energy use has increased. Ofﬁce
buildings represent approximately 50% of the building energy
use in the United States and appliances consume 22% of that
energy.

Enmetric is a plug-load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in ofﬁce buildings. Recognizing the challenge
faced by increased energy use, Enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants.
Enmetric utilizes smart power strips that sense the appliance
load in virtual real-time. Using the smart power strips in con-
junction with a software platform, building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down.

Learning more about Enmetrics system, the group identiﬁed
two challenges that could be assisted by the use of machine
learning algorithms; classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type. This paper
will explore different techniques to accomplish those goals.

II. MODEL FUNCTION

The main goal of our model is classiﬁcation of disaggregated
loads. Currently, Enmetrics labels each disaggregated load
by hand during installation of the smart power strips. If the
devices are moved, it is the responsibility of the building
manager to report the move to Enmetric. Since Enmetrics data
analytics depend on accurate labeling of devices and because
it is often difﬁcult for the building manager to keep track of
individual employees moving loads around, Enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved. The model developed
in this project learns device signatures from past data and
classiﬁes each load as a particular device on a daily basis. By
comparing yesterdays device classiﬁcations to todays device
classiﬁcations, loads that have been classiﬁed as a new type
can be ﬂagged for review.

III. DATA

Enmetric System smart power strips system takes plug-load
measurements every second but aggregates the measurements
to be stored at the minute level. The energy data collected
included average power, minimum and maximum power during
the second, average frequency, average current, and average
power factor (see table below). For our project, we obtained
the minutely data for over 200 appliances in a 50,000 square
foot ofﬁce building in Silicon Valley. The data covered one
year from 12/1/2013 to 11/31/2014 ( 20 GB). Because of
computational limits, we were restricted to only analyzing one
month at a time ( 2 GB).

To capture the effects weather has on the appliance energy
use, hourly weather data was incorporated in the analysis
as well. The Meso West weather database provided hourly
temperature and relative humidity for Moffet Airﬁeld.

IV. FEATURES AND PREPROCESSING

All the numerical data provided by Enmetric was processed
into features for our models. Since minutely appliance energy
proﬁles vary signiﬁcantly during the day, all data was aggre-
gated and classiﬁed at the day-level for each unique socket.
Daily means and standard deviations of the minutely plug-load
data were calculated and serve as features.

Temporal features in the form of day of week, day of year,
month, and weekend served as additional features. All values
(except for weekend, which is binary) were normalized to
within the range [-1, 1 ] by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

2

Hourly weather data, temperature and humidity, were pro-
cessed in a similar manner to the Enmetric data. The daily
mean, minimum, maximum and standard deviation were cal-
culated for the temperature and humidity. This brings the total
feature count to 27 and the number of training samples to 5660
(for one month of data). An overview of the data used and how
it was processed into features is shown in the table above.

The plug-load data contained 31 different appliances, result-
ing in 31 labels to be used for classiﬁcation. An overview of
the labels is shown is in the table below.

up the data. In this sense, it is very similar to the classic 30%
test data, 70% training data rule, but with the difference that we
perform this 3 times with random permutations of the dataset.
The trade off between training and testing error for different
values of K is shown below for the Random Forest model. The
variance versus biased nature of the K value is shown. The
graph also supports our selection of a K value of 3.

Fig. 1: Training and testing error from random forest for
different k values.

VI. SUPPORT VECTOR MACHINES

Support vector machines (SVMs) are a set of supervised
learning algorithms commonly used for classiﬁcation. SVMs
are useful for this particular problem because they are very
effective in a high dimensional space. Because they allow
the user to choose the kernel function to act as the decision
function in the model, SVMs are also ﬂexible to what types
of classiﬁcation problems they are suited for. Despite their
effectiveness and versatility, SVMs become less effective when
the number of features is much greater than the number of
samples. Since this was not the case for our classiﬁcation
problem (31 labels, 5660 training samples), the group started
out by classifying using Support Vector Classiﬁcation (SVC)
with a linear kernel and varied different parameters until we
found our best results. The general algorithm for a support
vector machine is as follows:
Given training vectors xi ∈ Rp,i=1,...,n, in two classes, and
a vector y ∈ Rn such that yi ∈ {1,−1}, SVC solves the
following primal problem:

(cid:88)

min
w,b,ζ

1
2

wT w + C

ζi

i=1,n

subject to:

yi(wT φ(xi) + b) ≥ 1 − ζi,

Its dual is

ζi ≥ 0, i = 1, ..., n

min

α

1
2

αT Qα − eT α

(1)

(2)

(3)

(4)

V. CLASSIFICATION MODELS

All learning algorithms considered for use for our project
fell under the category of supervised learning methods for
classiﬁcation. Because the dataset included several unique la-
bels, the project is a multiclass classiﬁcation problem. Several
models were tested with varying parameters to ﬁnd which algo-
rithm resulted in the highest performance rates. The following
popular multiclass classiﬁcation algorithms were tested for our
purposes: multiclass support vector machine (SVM standard),
multi-class SVM one-vs-one (SVM OvO), multi-class SVM
one-vs-rest (SVM OvR), Random Forest, and Gradient Tree
Boosting. A more detailed description of each algorithm, as
well as its strengths and weaknesses with respect
to this
project, can be found below. All algorithms were implemented
in Python, using the sci-kit learn library.

The tuning of the parameters for each algorithm was done
by k-fold cross validation, with a k-value of 3. This rather
small value of k was chosen because we had sufﬁcient training
samples and we wanted to speed up processing times. The
training samples were ﬁrst randomly permuted before splitting

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

3

subject to:

yT α = 0

0 ≤ αi ≤ C, i = 1, ..., l

(5)

(6)

where e is the vector of all ones, C ¿ 0 is the upper bound,
Q is an n by n positive semideﬁnite matrix, Qij ≡ K(xi, xj)
and φ(xi)T φ(x) is the kernel [4].

There are two types of approaches for multi-class SVMs:
One-vs-the-Rest (OvR) and One-vs-One (OvO). One-vs-the-
Rest considers training a single classiﬁer per class while the
One-vs-One trains N(N-1) binary classiﬁers for a N-way multi-
class problem [5]. A more detailed description of each method
can be found in the subsequent sections. Research has shown
that OvR and OvO are among the most suitable methods for
practical use [3].

A. One-Versus-One SVM

The ﬁrst type of SVM that was used for this project was
the standard SVC with a linear kernel found in the sci-kit
learn package. By default, SVC implements the one-versus-one
approach to multi-class classiﬁcation. OvO classiﬁcation will
build N(N-1) classiﬁers, one classiﬁer to distinguish between
each pair of classes i and j by marking i as the positive example
and j as the negative. When it is time to make a prediction
for a particular unseen sample, all classiﬁers are applied to
the sample and the class with the highest number of positive
predictions will be chosen.

rbf kernel : exp(−γ|x − x(cid:48)|2)

(7)

For the OvO SVM, we started by tuning the penalty pa-
rameter of the error term (C), which can also be seen as the
hyperplane separation term. While initially, increasing C lead
to smaller prediction errors, values larger than 10 resulted in
overﬁtting the training sample and hence an increase in testing
error. It was expected that small values of C would result
in more misclassiﬁcation, but we found that imporvements
reached their limit as C approaced a value of 10.The best
results for our model were found by using a radial basis
function (RBF) kernel, also know as the Gaussian kernel.
Varying values of gamma, the kernel coefﬁcient used with RBF
kernels, produced no improvement in predictions. A graphical
view of the effects of varying C values on the OvO support
vector classiﬁcation model can be found in Figure 2.

B. One-Versus-The-Rest SVM

As opposed to OvO where a sample is marked as in class
i and not in class j, One-vs-the-Rest will build N different
binary classiﬁers to determine if the example is either in class
i or not in class i [5]. For SVM OvR, the outputs and ideal
tuning parameters were very similar to standard SVM, as seen
in Figure 3. Similar to OvO SVM, the ideal parameters for the
OvR SVM were found to be C=10 with a RBF kernel.

Fig. 2: Training and testing error from One-versus-One support
vector classiﬁcation when varying C value.

Fig. 3: Training and testing error from One-versus-the-Rest
support vector classiﬁcation when varying C value.

VII. ENSEMBLE METHODS

Since SVM results were still showing signiﬁcant error
rates, the group decided to investigate ensemble methods for
classiﬁcation. The goal of ensemble methods is to combine
the predictions of several base estimators built with a given
learning algorithm to improve the robustness over a single
estimator [4]. By combining multiple hypotheses that may
produce weak results for a particular classiﬁcation problem,
ensembles methods aim to build a stronger, more accurate
hypothesis tailored for the individual problem. The two types
of ensemble methods used for our classiﬁcation problem were
random forest classiﬁcation and gradient tree boosting.

A. Random Forest

The random forest classiﬁcation method falls under the
category of an textitaveraging ensemble method. The driving
principle behind averaging methods is to build several esti-
mators independently and to average the resulting predictions
together in order to reduce the variance in predictions [4]. In
random forest classiﬁcation, a series of randomized decision
trees in the ensemble are built from a random subset of the
training set. Each subset used is a sample drawn from the
feature vector that is replaced after use for continued use in

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

4

subsequent tests. When splitting a node in the construction of
the tree, instead of choosing the best split that is best ﬁt for
the entire training set, the split chosen is the best ﬁt among
the subset of data. Although the randomness of these subsets
usually produces results with higher bias, the nature of the
averaging ensemble method also decreases the variance among
results, typically much greater than the increase in bias.

When running random forest classiﬁcation, we varied the the
number of estimators and maximum depth used in the algo-
rithm until we found optimal results. Intuitively, we expected
the performance to increase as we increased the number of
estimators (i.e. the number of trees), but this came with the
trade-off of a longer computational time. As can be seen in
Figure 4, the improvement in training and testing error as we
increased the number of estimators was not as great as we had
expected. Surprisingly for the group, varying the max depth of
the trees had a much greater effect on performance, as seen in
Figure 5. The optimal values of max depth and n-estimators
found for our model were 17 and 100, respectively.
Given training vectors xi ∈ Rn,i=1,..., l and a label vector
y ∈ Rl, a decision tree recursively partitions the space such
that the samples with the same labels are grouped together. Let
the data at node m be represented by Q. For each candidate
split θ = (j, tm) consisting of a feature j and threshold tm,
partition the data into Qlef t(θ) and Qright(θ) subsets

Qlef t(θ) = (x, y)|xj <= tm

Qright(θ) = Q \ Qlef t(θ)

The impurity at m is computed using an impurity function
H(), the choice of which depends on the task being solved
(classiﬁcation or regression)

G(Q, θ) =

nlef t
Nm

H(Qlef t(θ)) +

nright
Nm

H(Qright(θ))

Select the parameters that minimises the impurity

θ∗ =θ G(Q, θ)

Recurse for subsets Qlef t(θ∗) and Qright(θ∗) until the maxi-
mum allowable depth is reached, Nm < minsamples orNm =
1.

Fig. 5: Training and testing error from random forest classiﬁ-
cation when varying max depth.

B. Gradient Tree Boosting

The ﬁrst of two algorithms that we looked at is called gra-
dient boosting classiﬁcation. Gradient tree boosting combines
weak learning methods in an iterative manner until a single,
stronger learning method is created. The gradient tree boosting
method was chosen for our classiﬁcation problem because it
is typically very good at handling data with heterogeneous
features. More concern with this algorithm was around the
scalability of its results; although it proved effective for our
data sets in month-long intervals, it was thought that combining
that data into a full year could cause issues due to the sequen-
tial nature of boosting limiting its ability to be parallelized
[4].

Since gradient tree boosting is another ensemble method, the
group expected that varying the number of estimators and the
max depth of the tree branches would act in similar manners as
they did in the random forest algorithm. As seen in Figures 6
and 7, our assumptions were largely correct. More interesting
however, was the effect that learning rate had on the model
results, shown in Figure 8. After varying the parameters in
the gradient tree boosting algorithm, the optimal values of n-
estimators, learning rate, and max depth found for our model
were 60, 0.1, and 10, respectively.

Fig. 4: Training and testing error from random forest classiﬁ-
cation when varying number of estimators.

Fig. 6: Training and testing error from gradient tree boosting
classiﬁcation when varying number of estimators.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

5

vs-the-rest) did not improve the performance. The Random
Forest algorithm performed the best in terms of speed and test
error. The algorithm ran in under 15 seconds and resulted in
a test error of 17%. Gradient Tree Boosting resulted in the
lowest training error and performed only slightly worse than
the Random Forest algorithm in terms of test error.

Our work has shown that machine learning algorithms can
be a useful, efﬁcient tools in classiﬁcation of ofﬁce appliances
from plug load data. Future work should be able to further
improve the classiﬁcation to more acceptable levels of test
errors. In particular, improved feature extraction and selection,
and a larger dataset look promising.

X. CONTINUED WORK

The work presented in this paper shows that

learning
algorithms can be an effective tool for Enmetrics Systems
to use in their smart plug analytic services. The group will
be presenting our ﬁndings at Enmetric Systems next month
with the intentions of continuing further and producing a
useful tool for the company to possibly use. Based on the
successful results developed throughout
the course of this
project, the following are potential actions that may be taken
in the continuation of the project:

1) Run the ﬁve learning algorithms presented in this poster
on the larger dataset, varying the necessary parameters
to ﬁnd the best ﬁt model for classiﬁcation.

2) Develop an iterative algorithm that

learns based on
a collection of past data, predicts for the day, and
compares to known to identify any location where
deviced could have been moved or unplugged.

3) Explore the space of prediction to eliminate the need
for Enmetric to record by hand the type of device at
each plug at installation.

ACKNOWLEDGMENT

We would like to thank Jake Masters and Brad Degnan of
Enmentric Systems, Inc. for providing us with the data and
motivation for this project. We would also like to thank Romain
Juban of C3 Energy for his advice and guidance on the use of
learning algorithms with load disaggregation and classiﬁcation.
Finally, we would like to thank Professor Andrew Ng and the
CS229 teaching and project assistants for the help, guidance,
and encouragement.

REFERENCES

[1] H. Kopka and P. W. Daly, A Guide to LATEX, 3rd ed. Harlow, England:

Addison-Wesley, 1999.

[2] Machine Learning Applications for Load, Price and Wind Power Pre-
diction in Power Systems, Michael Negnevitsky, Senior Member, IEEE,
Paras Mandal, Member, IEEE, and Anurag K. Srivastava, Member, IEEE
[3] H. Lei and V. Govindaraju, Half-Against-Half Multi-class Support
Vector Machines, State University of New York at Buffalo, http://blue.
utb.edu/hlei/Research/papers/Half HalfSVM.pdf

[4] F. Pedregosa, et.al, Scikit-learn: Machine Learning in Python, Journal

of Machine Learning Research, vol. 12, pg. 2825-2830, 2011
http://www.mit.edu/∼9.520/spring09/Classes/multiclass.pdf
http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1583738&
url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs all.jsp%
3Farnumber%3D1583738

Fig. 7: Training and testing error from gradient tree boosting
classiﬁcation when varying max depth.

Fig. 8: Training and testing error from gradient tree boosting
classiﬁcation when varying learning rate.

VIII. RESULTS

All three SVM algorithms produced the greatest training and
test errors seen out of the ﬁve algorithms used. Trying different
binary SVM decompositions (OvO and OvR) did not improve
the performance. The Random Forest algorithm performed the
best in terms of speed and test error. The algorithm ran in under
15 seconds and resulted in a test error of 17%. Gradient Tree
Boosting resulted in the lowest training error and performed
only slightly worse than the Random Forest algorithm in terms
of test error.

The graphs show that tuning the main parameters of the
algorithms has a signiﬁcant impact on the test error. Biased-
variance trade off does show in some of the tuning, as can
be expected when tuning certain parameters such as the C
parameter in SVM models.

The performance of the Random Forest conﬁrmed the fact
that random forests usually work faster, but need a sufﬁcient
dataset size to work its randomization concept.

IX. CONCLUSION

SVM performed worst of the algorithms we tested. Trying
different binary SVM decompositions (one-vs-one and one-

[5]
[6]

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

6

OFFICE APPLIANCE CLASSIFICATION MODEL

APPENDIX A

Different versions of this simulation were used throughout
this project. This version utilizes the most recent version
of the Ofﬁce Appliance Classiﬁcation algorithm. This script
was written in python, utilizing the pandas and sci-kit learn
libraries.

Listing 1: code 1

# c o d i n g : u t f −8

# ###DATA INPUT
# Read i n t h e d a t a and i n d e x by t i m e

stamps

#

# I n [ 1 ] :

# i m p o r t wa r ni n gs
# wa r ni n gs . f i l t e r w a r n i n g s (” i g n o r e ” ,

c a t e g o r y=D e p r e c a t i o n W a r n i n g )

# I n [ 2 1 ] :

import pandas a s pd
import numpy a s np
import
s y s
import
s t r i n g
from d a t e u t i l

import p a r s e r

i n p u t p a t h = ’ / Us er s / G e r r i t / Documents / ’

# read i n a l l d a t a f i l e s
from d i r e c t o r y
# f i l e n a m e s = [ 1 3 1 2 , 1401 , 1402 , 1403 ,
1404 , 1405 , 1406 , 1407 , 1408 , 1410 ,
1411]

# d f = pd . c o n c a t ( [ pd . r e a d c s v ( i n p u t p a t h +”

x sample ”+ s t r ( f i l e n a m e ) +”. c s v ” ,
low memory=F a l s e )
i n
f i l e n a m e s ] , 0 )

f i l e n a m e

f o r

# read i n one d a t a f i l e , o n l y
# f i l e n a m e = ’ x sample 1401 ’
f i l e n a m e = ’ x sample 1312 ’
d f = pd . r e a d c s v ( i n p u t p a t h + f i l e n a m e +” . csv

f i r s t nrows

” , low memory= F a l s e )

# ### Data m a n i p u l a t i o n
# Here we p l a y around w i t h t h e data , and

s e t up t h e X and Y v e c t o r

# I n [ 2 2 ] :

d f . head ( )

# I n [ 2 3 ] :

# c r e a t e one column o f u n i q u e IDs

from t h e

node Hid and c h a n n e l number

d f [ ’ channel number ’ ] = d f [ ’ channel number ’

] . apply ( s t r )

d f [ ’ node hid ’ ] = d f [ ’ node hid ’ ] . apply ( s t r )
d f [ ’ i d ’ ] = d f . apply ( lambda x : x [ ’ node hid ’

]+ x [ ’ channel number ’ ] , 1 )
d f [ ’ i d ’ ] = d f [ ’ i d ’ ] . apply ( i n t )
d f = d f . drop ( [ ’ node hid ’ ,

’ channel number ’

] , 1 )

d f . head ( )

# I n [ 2 4 ] :

# s t r i p t h e name column o f
i t new name

and c a l l

l o c a t i o n numbers

d f [ ’ new name ’ ] = d f . apply ( lambda x : x [ ’

name ’ ] , 1 )

d i c t d a t a = pd . r e a d c s v ( ’ / Us er s / G e r r i t /

Documents / D i c t i o n a r y . csv ’ )

d i c t i o n a r y = d i c t ( zi p ( d i c t d a t a . Old Name ,
d f = d f . r e p l a c e ({ ’ new name ’ : d i c t i o n a r y } )

d i c t d a t a . New Name ) )

# t a k e o u t u n l a b e l e d d a t a
d f = d f [ d f . new name != ’ Unknown ’ ]
d f . head ( )

# I n [ 2 5 ] :

d f [ ’ o c c u r r e d a t ’ ] = pd . t o d a t e t i m e ( d f [ ’

o c c u r r e d a t ’ ] )

x : x . t i m e t u p l e ( ) . tm yday )

d f [ ’DAY’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply ( lambda
d f [ ’COS DAY ’ ] = np . cos ( d f [ ’DAY’ ] / 3 6 5 . ∗ 2∗
d f [ ’SIN DAY ’ ] = np . s i n ( d f [ ’DAY’ ] / 3 6 5 . ∗ 2∗

np . p i )

np . p i )

d f [ ’WEEKDAY’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply (

d f [ ’MONTH’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply (
d f [ ’COS MONTH’ ] = np . cos ( d f [ ’MONTH’ ] / 1 2 . ∗
d f [ ’SIN MONTH ’ ] = np . s i n ( d f [ ’MONTH’ ] / 1 2 . ∗
d f [ ’WEEKEND’ ] = ( d f [ ’WEEKDAY’ ] > 4 ) ∗1

lambda x : x . month )
2∗ np . p i )
2∗ np . p i )

d f [ ’COS WEEKDAY’ ] = np . cos ( d f [ ’WEEKDAY’

d f [ ’SIN WEEKDAY ’ ] = np . s i n ( d f [ ’WEEKDAY’

lambda x : x . weekday ( ) )
] / 7 . ∗ 2∗ np . p i )
] / 7 . ∗ 2∗ np . p i )

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

1

Ofﬁce Appliance Classiﬁcation

From Disaggregated Plug-Load Data

Gerrit De Moor, Elissa Goldner, Brock Petersen

Department of Civil and Environmental Engineering, Atmosphere/Energy Program

Stanford University, Stanford, CA, 94305, USA

Abstract—As buildings consume 40% of US primary energy,
energy efﬁciency through smart plug-load management can have
a signiﬁcant impact on US carbon emissions. In this paper we
investigate learning algorithms to classify plug-loads based on
minutely energy plug-load data and hourly weather data. One
month of data is preprocessed into 31 daily features which are
used to classify 5660 training samples into 31 labels using a
range of multi-class classiﬁcation algorithms. Of all models tested,
random forest trees performed the best, with a test error of 17%.
This promising result can likely be further reﬁned with improved
feature selection, and a larger dataset. Future work will focus on
increasing the model performance and integrating it into a useful
product for plug-load management companies.

I.

INTRODUCTION

Energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change. While
transportation and industry contribute to this problem, build-
ings represent the largest energy consuming end use in the
United States at approximately 40% of primary energy.

Historically building energy reduction focused on HVAC
and lighting. As those loads have decreased, the importance
of appliance loads in buliding energy use has increased. Ofﬁce
buildings represent approximately 50% of the building energy
use in the United States and appliances consume 22% of that
energy.

Enmetric is a plug-load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in ofﬁce buildings. Recognizing the challenge
faced by increased energy use, Enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants.
Enmetric utilizes smart power strips that sense the appliance
load in virtual real-time. Using the smart power strips in con-
junction with a software platform, building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down.

Learning more about Enmetrics system, the group identiﬁed
two challenges that could be assisted by the use of machine
learning algorithms; classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type. This paper
will explore different techniques to accomplish those goals.

II. MODEL FUNCTION

The main goal of our model is classiﬁcation of disaggregated
loads. Currently, Enmetrics labels each disaggregated load
by hand during installation of the smart power strips. If the
devices are moved, it is the responsibility of the building
manager to report the move to Enmetric. Since Enmetrics data
analytics depend on accurate labeling of devices and because
it is often difﬁcult for the building manager to keep track of
individual employees moving loads around, Enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved. The model developed
in this project learns device signatures from past data and
classiﬁes each load as a particular device on a daily basis. By
comparing yesterdays device classiﬁcations to todays device
classiﬁcations, loads that have been classiﬁed as a new type
can be ﬂagged for review.

III. DATA

Enmetric System smart power strips system takes plug-load
measurements every second but aggregates the measurements
to be stored at the minute level. The energy data collected
included average power, minimum and maximum power during
the second, average frequency, average current, and average
power factor (see table below). For our project, we obtained
the minutely data for over 200 appliances in a 50,000 square
foot ofﬁce building in Silicon Valley. The data covered one
year from 12/1/2013 to 11/31/2014 ( 20 GB). Because of
computational limits, we were restricted to only analyzing one
month at a time ( 2 GB).

To capture the effects weather has on the appliance energy
use, hourly weather data was incorporated in the analysis
as well. The Meso West weather database provided hourly
temperature and relative humidity for Moffet Airﬁeld.

IV. FEATURES AND PREPROCESSING

All the numerical data provided by Enmetric was processed
into features for our models. Since minutely appliance energy
proﬁles vary signiﬁcantly during the day, all data was aggre-
gated and classiﬁed at the day-level for each unique socket.
Daily means and standard deviations of the minutely plug-load
data were calculated and serve as features.

Temporal features in the form of day of week, day of year,
month, and weekend served as additional features. All values
(except for weekend, which is binary) were normalized to
within the range [-1, 1 ] by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

2

Hourly weather data, temperature and humidity, were pro-
cessed in a similar manner to the Enmetric data. The daily
mean, minimum, maximum and standard deviation were cal-
culated for the temperature and humidity. This brings the total
feature count to 27 and the number of training samples to 5660
(for one month of data). An overview of the data used and how
it was processed into features is shown in the table above.

The plug-load data contained 31 different appliances, result-
ing in 31 labels to be used for classiﬁcation. An overview of
the labels is shown is in the table below.

up the data. In this sense, it is very similar to the classic 30%
test data, 70% training data rule, but with the difference that we
perform this 3 times with random permutations of the dataset.
The trade off between training and testing error for different
values of K is shown below for the Random Forest model. The
variance versus biased nature of the K value is shown. The
graph also supports our selection of a K value of 3.

Fig. 1: Training and testing error from random forest for
different k values.

VI. SUPPORT VECTOR MACHINES

Support vector machines (SVMs) are a set of supervised
learning algorithms commonly used for classiﬁcation. SVMs
are useful for this particular problem because they are very
effective in a high dimensional space. Because they allow
the user to choose the kernel function to act as the decision
function in the model, SVMs are also ﬂexible to what types
of classiﬁcation problems they are suited for. Despite their
effectiveness and versatility, SVMs become less effective when
the number of features is much greater than the number of
samples. Since this was not the case for our classiﬁcation
problem (31 labels, 5660 training samples), the group started
out by classifying using Support Vector Classiﬁcation (SVC)
with a linear kernel and varied different parameters until we
found our best results. The general algorithm for a support
vector machine is as follows:
Given training vectors xi ∈ Rp,i=1,...,n, in two classes, and
a vector y ∈ Rn such that yi ∈ {1,−1}, SVC solves the
following primal problem:

(cid:88)

min
w,b,ζ

1
2

wT w + C

ζi

i=1,n

subject to:

yi(wT φ(xi) + b) ≥ 1 − ζi,

Its dual is

ζi ≥ 0, i = 1, ..., n

min

α

1
2

αT Qα − eT α

(1)

(2)

(3)

(4)

V. CLASSIFICATION MODELS

All learning algorithms considered for use for our project
fell under the category of supervised learning methods for
classiﬁcation. Because the dataset included several unique la-
bels, the project is a multiclass classiﬁcation problem. Several
models were tested with varying parameters to ﬁnd which algo-
rithm resulted in the highest performance rates. The following
popular multiclass classiﬁcation algorithms were tested for our
purposes: multiclass support vector machine (SVM standard),
multi-class SVM one-vs-one (SVM OvO), multi-class SVM
one-vs-rest (SVM OvR), Random Forest, and Gradient Tree
Boosting. A more detailed description of each algorithm, as
well as its strengths and weaknesses with respect
to this
project, can be found below. All algorithms were implemented
in Python, using the sci-kit learn library.

The tuning of the parameters for each algorithm was done
by k-fold cross validation, with a k-value of 3. This rather
small value of k was chosen because we had sufﬁcient training
samples and we wanted to speed up processing times. The
training samples were ﬁrst randomly permuted before splitting

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

3

subject to:

yT α = 0

0 ≤ αi ≤ C, i = 1, ..., l

(5)

(6)

where e is the vector of all ones, C ¿ 0 is the upper bound,
Q is an n by n positive semideﬁnite matrix, Qij ≡ K(xi, xj)
and φ(xi)T φ(x) is the kernel [4].

There are two types of approaches for multi-class SVMs:
One-vs-the-Rest (OvR) and One-vs-One (OvO). One-vs-the-
Rest considers training a single classiﬁer per class while the
One-vs-One trains N(N-1) binary classiﬁers for a N-way multi-
class problem [5]. A more detailed description of each method
can be found in the subsequent sections. Research has shown
that OvR and OvO are among the most suitable methods for
practical use [3].

A. One-Versus-One SVM

The ﬁrst type of SVM that was used for this project was
the standard SVC with a linear kernel found in the sci-kit
learn package. By default, SVC implements the one-versus-one
approach to multi-class classiﬁcation. OvO classiﬁcation will
build N(N-1) classiﬁers, one classiﬁer to distinguish between
each pair of classes i and j by marking i as the positive example
and j as the negative. When it is time to make a prediction
for a particular unseen sample, all classiﬁers are applied to
the sample and the class with the highest number of positive
predictions will be chosen.

rbf kernel : exp(−γ|x − x(cid:48)|2)

(7)

For the OvO SVM, we started by tuning the penalty pa-
rameter of the error term (C), which can also be seen as the
hyperplane separation term. While initially, increasing C lead
to smaller prediction errors, values larger than 10 resulted in
overﬁtting the training sample and hence an increase in testing
error. It was expected that small values of C would result
in more misclassiﬁcation, but we found that imporvements
reached their limit as C approaced a value of 10.The best
results for our model were found by using a radial basis
function (RBF) kernel, also know as the Gaussian kernel.
Varying values of gamma, the kernel coefﬁcient used with RBF
kernels, produced no improvement in predictions. A graphical
view of the effects of varying C values on the OvO support
vector classiﬁcation model can be found in Figure 2.

B. One-Versus-The-Rest SVM

As opposed to OvO where a sample is marked as in class
i and not in class j, One-vs-the-Rest will build N different
binary classiﬁers to determine if the example is either in class
i or not in class i [5]. For SVM OvR, the outputs and ideal
tuning parameters were very similar to standard SVM, as seen
in Figure 3. Similar to OvO SVM, the ideal parameters for the
OvR SVM were found to be C=10 with a RBF kernel.

Fig. 2: Training and testing error from One-versus-One support
vector classiﬁcation when varying C value.

Fig. 3: Training and testing error from One-versus-the-Rest
support vector classiﬁcation when varying C value.

VII. ENSEMBLE METHODS

Since SVM results were still showing signiﬁcant error
rates, the group decided to investigate ensemble methods for
classiﬁcation. The goal of ensemble methods is to combine
the predictions of several base estimators built with a given
learning algorithm to improve the robustness over a single
estimator [4]. By combining multiple hypotheses that may
produce weak results for a particular classiﬁcation problem,
ensembles methods aim to build a stronger, more accurate
hypothesis tailored for the individual problem. The two types
of ensemble methods used for our classiﬁcation problem were
random forest classiﬁcation and gradient tree boosting.

A. Random Forest

The random forest classiﬁcation method falls under the
category of an textitaveraging ensemble method. The driving
principle behind averaging methods is to build several esti-
mators independently and to average the resulting predictions
together in order to reduce the variance in predictions [4]. In
random forest classiﬁcation, a series of randomized decision
trees in the ensemble are built from a random subset of the
training set. Each subset used is a sample drawn from the
feature vector that is replaced after use for continued use in

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

4

subsequent tests. When splitting a node in the construction of
the tree, instead of choosing the best split that is best ﬁt for
the entire training set, the split chosen is the best ﬁt among
the subset of data. Although the randomness of these subsets
usually produces results with higher bias, the nature of the
averaging ensemble method also decreases the variance among
results, typically much greater than the increase in bias.

When running random forest classiﬁcation, we varied the the
number of estimators and maximum depth used in the algo-
rithm until we found optimal results. Intuitively, we expected
the performance to increase as we increased the number of
estimators (i.e. the number of trees), but this came with the
trade-off of a longer computational time. As can be seen in
Figure 4, the improvement in training and testing error as we
increased the number of estimators was not as great as we had
expected. Surprisingly for the group, varying the max depth of
the trees had a much greater effect on performance, as seen in
Figure 5. The optimal values of max depth and n-estimators
found for our model were 17 and 100, respectively.
Given training vectors xi ∈ Rn,i=1,..., l and a label vector
y ∈ Rl, a decision tree recursively partitions the space such
that the samples with the same labels are grouped together. Let
the data at node m be represented by Q. For each candidate
split θ = (j, tm) consisting of a feature j and threshold tm,
partition the data into Qlef t(θ) and Qright(θ) subsets

Qlef t(θ) = (x, y)|xj <= tm

Qright(θ) = Q \ Qlef t(θ)

The impurity at m is computed using an impurity function
H(), the choice of which depends on the task being solved
(classiﬁcation or regression)

G(Q, θ) =

nlef t
Nm

H(Qlef t(θ)) +

nright
Nm

H(Qright(θ))

Select the parameters that minimises the impurity

θ∗ =θ G(Q, θ)

Recurse for subsets Qlef t(θ∗) and Qright(θ∗) until the maxi-
mum allowable depth is reached, Nm < minsamples orNm =
1.

Fig. 5: Training and testing error from random forest classiﬁ-
cation when varying max depth.

B. Gradient Tree Boosting

The ﬁrst of two algorithms that we looked at is called gra-
dient boosting classiﬁcation. Gradient tree boosting combines
weak learning methods in an iterative manner until a single,
stronger learning method is created. The gradient tree boosting
method was chosen for our classiﬁcation problem because it
is typically very good at handling data with heterogeneous
features. More concern with this algorithm was around the
scalability of its results; although it proved effective for our
data sets in month-long intervals, it was thought that combining
that data into a full year could cause issues due to the sequen-
tial nature of boosting limiting its ability to be parallelized
[4].

Since gradient tree boosting is another ensemble method, the
group expected that varying the number of estimators and the
max depth of the tree branches would act in similar manners as
they did in the random forest algorithm. As seen in Figures 6
and 7, our assumptions were largely correct. More interesting
however, was the effect that learning rate had on the model
results, shown in Figure 8. After varying the parameters in
the gradient tree boosting algorithm, the optimal values of n-
estimators, learning rate, and max depth found for our model
were 60, 0.1, and 10, respectively.

Fig. 4: Training and testing error from random forest classiﬁ-
cation when varying number of estimators.

Fig. 6: Training and testing error from gradient tree boosting
classiﬁcation when varying number of estimators.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

5

vs-the-rest) did not improve the performance. The Random
Forest algorithm performed the best in terms of speed and test
error. The algorithm ran in under 15 seconds and resulted in
a test error of 17%. Gradient Tree Boosting resulted in the
lowest training error and performed only slightly worse than
the Random Forest algorithm in terms of test error.

Our work has shown that machine learning algorithms can
be a useful, efﬁcient tools in classiﬁcation of ofﬁce appliances
from plug load data. Future work should be able to further
improve the classiﬁcation to more acceptable levels of test
errors. In particular, improved feature extraction and selection,
and a larger dataset look promising.

X. CONTINUED WORK

The work presented in this paper shows that

learning
algorithms can be an effective tool for Enmetrics Systems
to use in their smart plug analytic services. The group will
be presenting our ﬁndings at Enmetric Systems next month
with the intentions of continuing further and producing a
useful tool for the company to possibly use. Based on the
successful results developed throughout
the course of this
project, the following are potential actions that may be taken
in the continuation of the project:

1) Run the ﬁve learning algorithms presented in this poster
on the larger dataset, varying the necessary parameters
to ﬁnd the best ﬁt model for classiﬁcation.

2) Develop an iterative algorithm that

learns based on
a collection of past data, predicts for the day, and
compares to known to identify any location where
deviced could have been moved or unplugged.

3) Explore the space of prediction to eliminate the need
for Enmetric to record by hand the type of device at
each plug at installation.

ACKNOWLEDGMENT

We would like to thank Jake Masters and Brad Degnan of
Enmentric Systems, Inc. for providing us with the data and
motivation for this project. We would also like to thank Romain
Juban of C3 Energy for his advice and guidance on the use of
learning algorithms with load disaggregation and classiﬁcation.
Finally, we would like to thank Professor Andrew Ng and the
CS229 teaching and project assistants for the help, guidance,
and encouragement.

REFERENCES

[1] H. Kopka and P. W. Daly, A Guide to LATEX, 3rd ed. Harlow, England:

Addison-Wesley, 1999.

[2] Machine Learning Applications for Load, Price and Wind Power Pre-
diction in Power Systems, Michael Negnevitsky, Senior Member, IEEE,
Paras Mandal, Member, IEEE, and Anurag K. Srivastava, Member, IEEE
[3] H. Lei and V. Govindaraju, Half-Against-Half Multi-class Support
Vector Machines, State University of New York at Buffalo, http://blue.
utb.edu/hlei/Research/papers/Half HalfSVM.pdf

[4] F. Pedregosa, et.al, Scikit-learn: Machine Learning in Python, Journal

of Machine Learning Research, vol. 12, pg. 2825-2830, 2011
http://www.mit.edu/∼9.520/spring09/Classes/multiclass.pdf
http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1583738&
url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs all.jsp%
3Farnumber%3D1583738

Fig. 7: Training and testing error from gradient tree boosting
classiﬁcation when varying max depth.

Fig. 8: Training and testing error from gradient tree boosting
classiﬁcation when varying learning rate.

VIII. RESULTS

All three SVM algorithms produced the greatest training and
test errors seen out of the ﬁve algorithms used. Trying different
binary SVM decompositions (OvO and OvR) did not improve
the performance. The Random Forest algorithm performed the
best in terms of speed and test error. The algorithm ran in under
15 seconds and resulted in a test error of 17%. Gradient Tree
Boosting resulted in the lowest training error and performed
only slightly worse than the Random Forest algorithm in terms
of test error.

The graphs show that tuning the main parameters of the
algorithms has a signiﬁcant impact on the test error. Biased-
variance trade off does show in some of the tuning, as can
be expected when tuning certain parameters such as the C
parameter in SVM models.

The performance of the Random Forest conﬁrmed the fact
that random forests usually work faster, but need a sufﬁcient
dataset size to work its randomization concept.

IX. CONCLUSION

SVM performed worst of the algorithms we tested. Trying
different binary SVM decompositions (one-vs-one and one-

[5]
[6]

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

6

OFFICE APPLIANCE CLASSIFICATION MODEL

APPENDIX A

Different versions of this simulation were used throughout
this project. This version utilizes the most recent version
of the Ofﬁce Appliance Classiﬁcation algorithm. This script
was written in python, utilizing the pandas and sci-kit learn
libraries.

Listing 1: code 1

# c o d i n g : u t f −8

# ###DATA INPUT
# Read i n t h e d a t a and i n d e x by t i m e

stamps

#

# I n [ 1 ] :

# i m p o r t wa r ni n gs
# wa r ni n gs . f i l t e r w a r n i n g s (” i g n o r e ” ,

c a t e g o r y=D e p r e c a t i o n W a r n i n g )

# I n [ 2 1 ] :

import pandas a s pd
import numpy a s np
import
s y s
import
s t r i n g
from d a t e u t i l

import p a r s e r

i n p u t p a t h = ’ / Us er s / G e r r i t / Documents / ’

# read i n a l l d a t a f i l e s
from d i r e c t o r y
# f i l e n a m e s = [ 1 3 1 2 , 1401 , 1402 , 1403 ,
1404 , 1405 , 1406 , 1407 , 1408 , 1410 ,
1411]

# d f = pd . c o n c a t ( [ pd . r e a d c s v ( i n p u t p a t h +”

x sample ”+ s t r ( f i l e n a m e ) +”. c s v ” ,
low memory=F a l s e )
i n
f i l e n a m e s ] , 0 )

f i l e n a m e

f o r

# read i n one d a t a f i l e , o n l y
# f i l e n a m e = ’ x sample 1401 ’
f i l e n a m e = ’ x sample 1312 ’
d f = pd . r e a d c s v ( i n p u t p a t h + f i l e n a m e +” . csv

f i r s t nrows

” , low memory= F a l s e )

# ### Data m a n i p u l a t i o n
# Here we p l a y around w i t h t h e data , and

s e t up t h e X and Y v e c t o r

# I n [ 2 2 ] :

d f . head ( )

# I n [ 2 3 ] :

# c r e a t e one column o f u n i q u e IDs

from t h e

node Hid and c h a n n e l number

d f [ ’ channel number ’ ] = d f [ ’ channel number ’

] . apply ( s t r )

d f [ ’ node hid ’ ] = d f [ ’ node hid ’ ] . apply ( s t r )
d f [ ’ i d ’ ] = d f . apply ( lambda x : x [ ’ node hid ’

]+ x [ ’ channel number ’ ] , 1 )
d f [ ’ i d ’ ] = d f [ ’ i d ’ ] . apply ( i n t )
d f = d f . drop ( [ ’ node hid ’ ,

’ channel number ’

] , 1 )

d f . head ( )

# I n [ 2 4 ] :

# s t r i p t h e name column o f
i t new name

and c a l l

l o c a t i o n numbers

d f [ ’ new name ’ ] = d f . apply ( lambda x : x [ ’

name ’ ] , 1 )

d i c t d a t a = pd . r e a d c s v ( ’ / Us er s / G e r r i t /

Documents / D i c t i o n a r y . csv ’ )

d i c t i o n a r y = d i c t ( zi p ( d i c t d a t a . Old Name ,
d f = d f . r e p l a c e ({ ’ new name ’ : d i c t i o n a r y } )

d i c t d a t a . New Name ) )

# t a k e o u t u n l a b e l e d d a t a
d f = d f [ d f . new name != ’ Unknown ’ ]
d f . head ( )

# I n [ 2 5 ] :

d f [ ’ o c c u r r e d a t ’ ] = pd . t o d a t e t i m e ( d f [ ’

o c c u r r e d a t ’ ] )

x : x . t i m e t u p l e ( ) . tm yday )

d f [ ’DAY’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply ( lambda
d f [ ’COS DAY ’ ] = np . cos ( d f [ ’DAY’ ] / 3 6 5 . ∗ 2∗
d f [ ’SIN DAY ’ ] = np . s i n ( d f [ ’DAY’ ] / 3 6 5 . ∗ 2∗

np . p i )

np . p i )

d f [ ’WEEKDAY’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply (

d f [ ’MONTH’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply (
d f [ ’COS MONTH’ ] = np . cos ( d f [ ’MONTH’ ] / 1 2 . ∗
d f [ ’SIN MONTH ’ ] = np . s i n ( d f [ ’MONTH’ ] / 1 2 . ∗
d f [ ’WEEKEND’ ] = ( d f [ ’WEEKDAY’ ] > 4 ) ∗1

lambda x : x . month )
2∗ np . p i )
2∗ np . p i )

d f [ ’COS WEEKDAY’ ] = np . cos ( d f [ ’WEEKDAY’

d f [ ’SIN WEEKDAY ’ ] = np . s i n ( d f [ ’WEEKDAY’

lambda x : x . weekday ( ) )
] / 7 . ∗ 2∗ np . p i )
] / 7 . ∗ 2∗ np . p i )

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

7

# I n [ 2 6 ] :

c a l e n d a r

f e a t u r e s = [ ”COS DAY” , ”SIN DAY” , ”
COS WEEKDAY” , ”SIN WEEKDAY” , ”COS MONTH”
, ”SIN MONTH” , ”WEEKEND” ]

e l e c t r i c

f e a t u r e s = [ ” power avg ” , ”

power min ” , ” power max ” , ” e n e r g y u s e d ”
, ” f r e q u e n c y a v g ” , ” v o l t a g e a v g ” , ”
c u r r e n t a v g ” , ” p o w e r

f a c t o r a v g ” ]

# I n [ 2 7 ] :

df mean = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

e l e c t r i c
mean ( ) )

df mean . columns = [ x+” mean ” f o r x in d f [

d f

d f

d f

f e a t u r e s ] . columns ]

e l e c t r i c
s t d = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [
e l e c t r i c
s t d ( ) )
s t d . columns = [ x+” s t d ” f or x in d f [
e l e c t r i c

f e a t u r e s ] . apply ( lambda x : x .

f e a t u r e s ] . columns ]

d f c a l e n d a r = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

c a l e n d a r
mean ( ) )
l a b e l = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [ [ ’
new name ’ ] ] . apply ( lambda x : x . i l o c
[ 0 ] )

# I n [ 8 1 ] :

import numpy a s np

df avg day = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x

e l e c t r i c
[ 4 8 0 : 1 0 8 0 ] . mean ( ) )

d f a v g n i g h t = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [
f e a t u r e s ] . apply ( lambda x : x

e l e c t r i c
[ 0 : 4 8 0 and 1 0 8 1 : 1 3 3 9 ] . mean ( ) )

d f n i g h t d a y = df avg day / d f a v g n i g h t
d f n i g h t d a y . columns = [ x+”

n i g h t d a y r a t i o ” f o r x in d f [
e l e c t r i c

f e a t u r e s ] . columns ]

df max = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

e l e c t r i c
max ( ) )

df max . columns = [ x+” max” f or x in d f [

e l e c t r i c

f e a t u r e s ] . columns ]

df min = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

e l e c t r i c
max ( ) )

df min . columns = [ x+” min ” f or x in d f [

e l e c t r i c

f e a t u r e s ] . columns ]

# o t h e r
# H e a r t b e a t Time S e r i e s C l a s s i f i c a t i o n

s t u f f based on

With S u p p o r t

# V e c t o r Machines
# Argyro Kampouraki , George Manis , and

C h r i s t o p h o r o s Nikou , Member ,

IEEE

#

def

df

df

r o o t mean s q u a r e o f

s u c c e s s i v e

d i f f e r e n c e s
rmssd ( x ) :
N = l en ( x )
r e s u l t = np . s q r t ( sum ( np . e d i f f 1 d ( x ) ∗∗2)
/ ( N−1) )

return r e s u l t

f e a t u r e s ] . apply ( lambda x :

rmssd = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [
e l e c t r i c
rmssd ( x ) )
rmssd . columns = [ x+” rmssd ” f o r x in d f
[ e l e c t r i c

f e a t u r e s ] . columns ]

d f s d s d = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

e l e c t r i c
. s t d ( np . e d i f f 1 d ( x ) ) )

f e a t u r e s ] . apply ( lambda x : np

d f s d s d . columns = [ x+” sd sd ” f o r x in d f [

e l e c t r i c

f e a t u r e s ] . columns ]

# I n [ 8 2 ] :

df2 = pd . c o n c a t ( [ d f

df max , df min , d f n i g h t d a y , df
, d f s d s d , d f c a l e n d a r ] , 1 )

l a b e l , df mean , d f s t d ,
rmssd

# ### Te mp er at ur e Data

# I n [ 8 3 ] :

w e a t h e r d a t a = pd . r e a d c s v ( i n p u t p a t h +”

Weather Data Clean . csv ” )

# I n [ 8 4 ] :

w e a t h e r d a t a . columns = [ ’ Time ’ , ’Temp ’ , ’RH’

, ’ Hour ’ ]

w e a t h e r d a t a [ ’DAY’ ] = pd . t o d a t e t i m e (

w e a t h e r d a t a [ ’ Time ’ ] ) . apply ( lambda x :
x . t i m e t u p l e ( ) . tm yday )

mean weather = w e a t h e r d a t a . groupby ( [ ’DAY’

] ) [ [ ’Temp ’ , ’RH’ ] ] . apply ( lambda x : x .
mean ( ) )

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

1

Ofﬁce Appliance Classiﬁcation

From Disaggregated Plug-Load Data

Gerrit De Moor, Elissa Goldner, Brock Petersen

Department of Civil and Environmental Engineering, Atmosphere/Energy Program

Stanford University, Stanford, CA, 94305, USA

Abstract—As buildings consume 40% of US primary energy,
energy efﬁciency through smart plug-load management can have
a signiﬁcant impact on US carbon emissions. In this paper we
investigate learning algorithms to classify plug-loads based on
minutely energy plug-load data and hourly weather data. One
month of data is preprocessed into 31 daily features which are
used to classify 5660 training samples into 31 labels using a
range of multi-class classiﬁcation algorithms. Of all models tested,
random forest trees performed the best, with a test error of 17%.
This promising result can likely be further reﬁned with improved
feature selection, and a larger dataset. Future work will focus on
increasing the model performance and integrating it into a useful
product for plug-load management companies.

I.

INTRODUCTION

Energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change. While
transportation and industry contribute to this problem, build-
ings represent the largest energy consuming end use in the
United States at approximately 40% of primary energy.

Historically building energy reduction focused on HVAC
and lighting. As those loads have decreased, the importance
of appliance loads in buliding energy use has increased. Ofﬁce
buildings represent approximately 50% of the building energy
use in the United States and appliances consume 22% of that
energy.

Enmetric is a plug-load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in ofﬁce buildings. Recognizing the challenge
faced by increased energy use, Enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants.
Enmetric utilizes smart power strips that sense the appliance
load in virtual real-time. Using the smart power strips in con-
junction with a software platform, building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down.

Learning more about Enmetrics system, the group identiﬁed
two challenges that could be assisted by the use of machine
learning algorithms; classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type. This paper
will explore different techniques to accomplish those goals.

II. MODEL FUNCTION

The main goal of our model is classiﬁcation of disaggregated
loads. Currently, Enmetrics labels each disaggregated load
by hand during installation of the smart power strips. If the
devices are moved, it is the responsibility of the building
manager to report the move to Enmetric. Since Enmetrics data
analytics depend on accurate labeling of devices and because
it is often difﬁcult for the building manager to keep track of
individual employees moving loads around, Enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved. The model developed
in this project learns device signatures from past data and
classiﬁes each load as a particular device on a daily basis. By
comparing yesterdays device classiﬁcations to todays device
classiﬁcations, loads that have been classiﬁed as a new type
can be ﬂagged for review.

III. DATA

Enmetric System smart power strips system takes plug-load
measurements every second but aggregates the measurements
to be stored at the minute level. The energy data collected
included average power, minimum and maximum power during
the second, average frequency, average current, and average
power factor (see table below). For our project, we obtained
the minutely data for over 200 appliances in a 50,000 square
foot ofﬁce building in Silicon Valley. The data covered one
year from 12/1/2013 to 11/31/2014 ( 20 GB). Because of
computational limits, we were restricted to only analyzing one
month at a time ( 2 GB).

To capture the effects weather has on the appliance energy
use, hourly weather data was incorporated in the analysis
as well. The Meso West weather database provided hourly
temperature and relative humidity for Moffet Airﬁeld.

IV. FEATURES AND PREPROCESSING

All the numerical data provided by Enmetric was processed
into features for our models. Since minutely appliance energy
proﬁles vary signiﬁcantly during the day, all data was aggre-
gated and classiﬁed at the day-level for each unique socket.
Daily means and standard deviations of the minutely plug-load
data were calculated and serve as features.

Temporal features in the form of day of week, day of year,
month, and weekend served as additional features. All values
(except for weekend, which is binary) were normalized to
within the range [-1, 1 ] by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

2

Hourly weather data, temperature and humidity, were pro-
cessed in a similar manner to the Enmetric data. The daily
mean, minimum, maximum and standard deviation were cal-
culated for the temperature and humidity. This brings the total
feature count to 27 and the number of training samples to 5660
(for one month of data). An overview of the data used and how
it was processed into features is shown in the table above.

The plug-load data contained 31 different appliances, result-
ing in 31 labels to be used for classiﬁcation. An overview of
the labels is shown is in the table below.

up the data. In this sense, it is very similar to the classic 30%
test data, 70% training data rule, but with the difference that we
perform this 3 times with random permutations of the dataset.
The trade off between training and testing error for different
values of K is shown below for the Random Forest model. The
variance versus biased nature of the K value is shown. The
graph also supports our selection of a K value of 3.

Fig. 1: Training and testing error from random forest for
different k values.

VI. SUPPORT VECTOR MACHINES

Support vector machines (SVMs) are a set of supervised
learning algorithms commonly used for classiﬁcation. SVMs
are useful for this particular problem because they are very
effective in a high dimensional space. Because they allow
the user to choose the kernel function to act as the decision
function in the model, SVMs are also ﬂexible to what types
of classiﬁcation problems they are suited for. Despite their
effectiveness and versatility, SVMs become less effective when
the number of features is much greater than the number of
samples. Since this was not the case for our classiﬁcation
problem (31 labels, 5660 training samples), the group started
out by classifying using Support Vector Classiﬁcation (SVC)
with a linear kernel and varied different parameters until we
found our best results. The general algorithm for a support
vector machine is as follows:
Given training vectors xi ∈ Rp,i=1,...,n, in two classes, and
a vector y ∈ Rn such that yi ∈ {1,−1}, SVC solves the
following primal problem:

(cid:88)

min
w,b,ζ

1
2

wT w + C

ζi

i=1,n

subject to:

yi(wT φ(xi) + b) ≥ 1 − ζi,

Its dual is

ζi ≥ 0, i = 1, ..., n

min

α

1
2

αT Qα − eT α

(1)

(2)

(3)

(4)

V. CLASSIFICATION MODELS

All learning algorithms considered for use for our project
fell under the category of supervised learning methods for
classiﬁcation. Because the dataset included several unique la-
bels, the project is a multiclass classiﬁcation problem. Several
models were tested with varying parameters to ﬁnd which algo-
rithm resulted in the highest performance rates. The following
popular multiclass classiﬁcation algorithms were tested for our
purposes: multiclass support vector machine (SVM standard),
multi-class SVM one-vs-one (SVM OvO), multi-class SVM
one-vs-rest (SVM OvR), Random Forest, and Gradient Tree
Boosting. A more detailed description of each algorithm, as
well as its strengths and weaknesses with respect
to this
project, can be found below. All algorithms were implemented
in Python, using the sci-kit learn library.

The tuning of the parameters for each algorithm was done
by k-fold cross validation, with a k-value of 3. This rather
small value of k was chosen because we had sufﬁcient training
samples and we wanted to speed up processing times. The
training samples were ﬁrst randomly permuted before splitting

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

3

subject to:

yT α = 0

0 ≤ αi ≤ C, i = 1, ..., l

(5)

(6)

where e is the vector of all ones, C ¿ 0 is the upper bound,
Q is an n by n positive semideﬁnite matrix, Qij ≡ K(xi, xj)
and φ(xi)T φ(x) is the kernel [4].

There are two types of approaches for multi-class SVMs:
One-vs-the-Rest (OvR) and One-vs-One (OvO). One-vs-the-
Rest considers training a single classiﬁer per class while the
One-vs-One trains N(N-1) binary classiﬁers for a N-way multi-
class problem [5]. A more detailed description of each method
can be found in the subsequent sections. Research has shown
that OvR and OvO are among the most suitable methods for
practical use [3].

A. One-Versus-One SVM

The ﬁrst type of SVM that was used for this project was
the standard SVC with a linear kernel found in the sci-kit
learn package. By default, SVC implements the one-versus-one
approach to multi-class classiﬁcation. OvO classiﬁcation will
build N(N-1) classiﬁers, one classiﬁer to distinguish between
each pair of classes i and j by marking i as the positive example
and j as the negative. When it is time to make a prediction
for a particular unseen sample, all classiﬁers are applied to
the sample and the class with the highest number of positive
predictions will be chosen.

rbf kernel : exp(−γ|x − x(cid:48)|2)

(7)

For the OvO SVM, we started by tuning the penalty pa-
rameter of the error term (C), which can also be seen as the
hyperplane separation term. While initially, increasing C lead
to smaller prediction errors, values larger than 10 resulted in
overﬁtting the training sample and hence an increase in testing
error. It was expected that small values of C would result
in more misclassiﬁcation, but we found that imporvements
reached their limit as C approaced a value of 10.The best
results for our model were found by using a radial basis
function (RBF) kernel, also know as the Gaussian kernel.
Varying values of gamma, the kernel coefﬁcient used with RBF
kernels, produced no improvement in predictions. A graphical
view of the effects of varying C values on the OvO support
vector classiﬁcation model can be found in Figure 2.

B. One-Versus-The-Rest SVM

As opposed to OvO where a sample is marked as in class
i and not in class j, One-vs-the-Rest will build N different
binary classiﬁers to determine if the example is either in class
i or not in class i [5]. For SVM OvR, the outputs and ideal
tuning parameters were very similar to standard SVM, as seen
in Figure 3. Similar to OvO SVM, the ideal parameters for the
OvR SVM were found to be C=10 with a RBF kernel.

Fig. 2: Training and testing error from One-versus-One support
vector classiﬁcation when varying C value.

Fig. 3: Training and testing error from One-versus-the-Rest
support vector classiﬁcation when varying C value.

VII. ENSEMBLE METHODS

Since SVM results were still showing signiﬁcant error
rates, the group decided to investigate ensemble methods for
classiﬁcation. The goal of ensemble methods is to combine
the predictions of several base estimators built with a given
learning algorithm to improve the robustness over a single
estimator [4]. By combining multiple hypotheses that may
produce weak results for a particular classiﬁcation problem,
ensembles methods aim to build a stronger, more accurate
hypothesis tailored for the individual problem. The two types
of ensemble methods used for our classiﬁcation problem were
random forest classiﬁcation and gradient tree boosting.

A. Random Forest

The random forest classiﬁcation method falls under the
category of an textitaveraging ensemble method. The driving
principle behind averaging methods is to build several esti-
mators independently and to average the resulting predictions
together in order to reduce the variance in predictions [4]. In
random forest classiﬁcation, a series of randomized decision
trees in the ensemble are built from a random subset of the
training set. Each subset used is a sample drawn from the
feature vector that is replaced after use for continued use in

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

4

subsequent tests. When splitting a node in the construction of
the tree, instead of choosing the best split that is best ﬁt for
the entire training set, the split chosen is the best ﬁt among
the subset of data. Although the randomness of these subsets
usually produces results with higher bias, the nature of the
averaging ensemble method also decreases the variance among
results, typically much greater than the increase in bias.

When running random forest classiﬁcation, we varied the the
number of estimators and maximum depth used in the algo-
rithm until we found optimal results. Intuitively, we expected
the performance to increase as we increased the number of
estimators (i.e. the number of trees), but this came with the
trade-off of a longer computational time. As can be seen in
Figure 4, the improvement in training and testing error as we
increased the number of estimators was not as great as we had
expected. Surprisingly for the group, varying the max depth of
the trees had a much greater effect on performance, as seen in
Figure 5. The optimal values of max depth and n-estimators
found for our model were 17 and 100, respectively.
Given training vectors xi ∈ Rn,i=1,..., l and a label vector
y ∈ Rl, a decision tree recursively partitions the space such
that the samples with the same labels are grouped together. Let
the data at node m be represented by Q. For each candidate
split θ = (j, tm) consisting of a feature j and threshold tm,
partition the data into Qlef t(θ) and Qright(θ) subsets

Qlef t(θ) = (x, y)|xj <= tm

Qright(θ) = Q \ Qlef t(θ)

The impurity at m is computed using an impurity function
H(), the choice of which depends on the task being solved
(classiﬁcation or regression)

G(Q, θ) =

nlef t
Nm

H(Qlef t(θ)) +

nright
Nm

H(Qright(θ))

Select the parameters that minimises the impurity

θ∗ =θ G(Q, θ)

Recurse for subsets Qlef t(θ∗) and Qright(θ∗) until the maxi-
mum allowable depth is reached, Nm < minsamples orNm =
1.

Fig. 5: Training and testing error from random forest classiﬁ-
cation when varying max depth.

B. Gradient Tree Boosting

The ﬁrst of two algorithms that we looked at is called gra-
dient boosting classiﬁcation. Gradient tree boosting combines
weak learning methods in an iterative manner until a single,
stronger learning method is created. The gradient tree boosting
method was chosen for our classiﬁcation problem because it
is typically very good at handling data with heterogeneous
features. More concern with this algorithm was around the
scalability of its results; although it proved effective for our
data sets in month-long intervals, it was thought that combining
that data into a full year could cause issues due to the sequen-
tial nature of boosting limiting its ability to be parallelized
[4].

Since gradient tree boosting is another ensemble method, the
group expected that varying the number of estimators and the
max depth of the tree branches would act in similar manners as
they did in the random forest algorithm. As seen in Figures 6
and 7, our assumptions were largely correct. More interesting
however, was the effect that learning rate had on the model
results, shown in Figure 8. After varying the parameters in
the gradient tree boosting algorithm, the optimal values of n-
estimators, learning rate, and max depth found for our model
were 60, 0.1, and 10, respectively.

Fig. 4: Training and testing error from random forest classiﬁ-
cation when varying number of estimators.

Fig. 6: Training and testing error from gradient tree boosting
classiﬁcation when varying number of estimators.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

5

vs-the-rest) did not improve the performance. The Random
Forest algorithm performed the best in terms of speed and test
error. The algorithm ran in under 15 seconds and resulted in
a test error of 17%. Gradient Tree Boosting resulted in the
lowest training error and performed only slightly worse than
the Random Forest algorithm in terms of test error.

Our work has shown that machine learning algorithms can
be a useful, efﬁcient tools in classiﬁcation of ofﬁce appliances
from plug load data. Future work should be able to further
improve the classiﬁcation to more acceptable levels of test
errors. In particular, improved feature extraction and selection,
and a larger dataset look promising.

X. CONTINUED WORK

The work presented in this paper shows that

learning
algorithms can be an effective tool for Enmetrics Systems
to use in their smart plug analytic services. The group will
be presenting our ﬁndings at Enmetric Systems next month
with the intentions of continuing further and producing a
useful tool for the company to possibly use. Based on the
successful results developed throughout
the course of this
project, the following are potential actions that may be taken
in the continuation of the project:

1) Run the ﬁve learning algorithms presented in this poster
on the larger dataset, varying the necessary parameters
to ﬁnd the best ﬁt model for classiﬁcation.

2) Develop an iterative algorithm that

learns based on
a collection of past data, predicts for the day, and
compares to known to identify any location where
deviced could have been moved or unplugged.

3) Explore the space of prediction to eliminate the need
for Enmetric to record by hand the type of device at
each plug at installation.

ACKNOWLEDGMENT

We would like to thank Jake Masters and Brad Degnan of
Enmentric Systems, Inc. for providing us with the data and
motivation for this project. We would also like to thank Romain
Juban of C3 Energy for his advice and guidance on the use of
learning algorithms with load disaggregation and classiﬁcation.
Finally, we would like to thank Professor Andrew Ng and the
CS229 teaching and project assistants for the help, guidance,
and encouragement.

REFERENCES

[1] H. Kopka and P. W. Daly, A Guide to LATEX, 3rd ed. Harlow, England:

Addison-Wesley, 1999.

[2] Machine Learning Applications for Load, Price and Wind Power Pre-
diction in Power Systems, Michael Negnevitsky, Senior Member, IEEE,
Paras Mandal, Member, IEEE, and Anurag K. Srivastava, Member, IEEE
[3] H. Lei and V. Govindaraju, Half-Against-Half Multi-class Support
Vector Machines, State University of New York at Buffalo, http://blue.
utb.edu/hlei/Research/papers/Half HalfSVM.pdf

[4] F. Pedregosa, et.al, Scikit-learn: Machine Learning in Python, Journal

of Machine Learning Research, vol. 12, pg. 2825-2830, 2011
http://www.mit.edu/∼9.520/spring09/Classes/multiclass.pdf
http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1583738&
url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs all.jsp%
3Farnumber%3D1583738

Fig. 7: Training and testing error from gradient tree boosting
classiﬁcation when varying max depth.

Fig. 8: Training and testing error from gradient tree boosting
classiﬁcation when varying learning rate.

VIII. RESULTS

All three SVM algorithms produced the greatest training and
test errors seen out of the ﬁve algorithms used. Trying different
binary SVM decompositions (OvO and OvR) did not improve
the performance. The Random Forest algorithm performed the
best in terms of speed and test error. The algorithm ran in under
15 seconds and resulted in a test error of 17%. Gradient Tree
Boosting resulted in the lowest training error and performed
only slightly worse than the Random Forest algorithm in terms
of test error.

The graphs show that tuning the main parameters of the
algorithms has a signiﬁcant impact on the test error. Biased-
variance trade off does show in some of the tuning, as can
be expected when tuning certain parameters such as the C
parameter in SVM models.

The performance of the Random Forest conﬁrmed the fact
that random forests usually work faster, but need a sufﬁcient
dataset size to work its randomization concept.

IX. CONCLUSION

SVM performed worst of the algorithms we tested. Trying
different binary SVM decompositions (one-vs-one and one-

[5]
[6]

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

6

OFFICE APPLIANCE CLASSIFICATION MODEL

APPENDIX A

Different versions of this simulation were used throughout
this project. This version utilizes the most recent version
of the Ofﬁce Appliance Classiﬁcation algorithm. This script
was written in python, utilizing the pandas and sci-kit learn
libraries.

Listing 1: code 1

# c o d i n g : u t f −8

# ###DATA INPUT
# Read i n t h e d a t a and i n d e x by t i m e

stamps

#

# I n [ 1 ] :

# i m p o r t wa r ni n gs
# wa r ni n gs . f i l t e r w a r n i n g s (” i g n o r e ” ,

c a t e g o r y=D e p r e c a t i o n W a r n i n g )

# I n [ 2 1 ] :

import pandas a s pd
import numpy a s np
import
s y s
import
s t r i n g
from d a t e u t i l

import p a r s e r

i n p u t p a t h = ’ / Us er s / G e r r i t / Documents / ’

# read i n a l l d a t a f i l e s
from d i r e c t o r y
# f i l e n a m e s = [ 1 3 1 2 , 1401 , 1402 , 1403 ,
1404 , 1405 , 1406 , 1407 , 1408 , 1410 ,
1411]

# d f = pd . c o n c a t ( [ pd . r e a d c s v ( i n p u t p a t h +”

x sample ”+ s t r ( f i l e n a m e ) +”. c s v ” ,
low memory=F a l s e )
i n
f i l e n a m e s ] , 0 )

f i l e n a m e

f o r

# read i n one d a t a f i l e , o n l y
# f i l e n a m e = ’ x sample 1401 ’
f i l e n a m e = ’ x sample 1312 ’
d f = pd . r e a d c s v ( i n p u t p a t h + f i l e n a m e +” . csv

f i r s t nrows

” , low memory= F a l s e )

# ### Data m a n i p u l a t i o n
# Here we p l a y around w i t h t h e data , and

s e t up t h e X and Y v e c t o r

# I n [ 2 2 ] :

d f . head ( )

# I n [ 2 3 ] :

# c r e a t e one column o f u n i q u e IDs

from t h e

node Hid and c h a n n e l number

d f [ ’ channel number ’ ] = d f [ ’ channel number ’

] . apply ( s t r )

d f [ ’ node hid ’ ] = d f [ ’ node hid ’ ] . apply ( s t r )
d f [ ’ i d ’ ] = d f . apply ( lambda x : x [ ’ node hid ’

]+ x [ ’ channel number ’ ] , 1 )
d f [ ’ i d ’ ] = d f [ ’ i d ’ ] . apply ( i n t )
d f = d f . drop ( [ ’ node hid ’ ,

’ channel number ’

] , 1 )

d f . head ( )

# I n [ 2 4 ] :

# s t r i p t h e name column o f
i t new name

and c a l l

l o c a t i o n numbers

d f [ ’ new name ’ ] = d f . apply ( lambda x : x [ ’

name ’ ] , 1 )

d i c t d a t a = pd . r e a d c s v ( ’ / Us er s / G e r r i t /

Documents / D i c t i o n a r y . csv ’ )

d i c t i o n a r y = d i c t ( zi p ( d i c t d a t a . Old Name ,
d f = d f . r e p l a c e ({ ’ new name ’ : d i c t i o n a r y } )

d i c t d a t a . New Name ) )

# t a k e o u t u n l a b e l e d d a t a
d f = d f [ d f . new name != ’ Unknown ’ ]
d f . head ( )

# I n [ 2 5 ] :

d f [ ’ o c c u r r e d a t ’ ] = pd . t o d a t e t i m e ( d f [ ’

o c c u r r e d a t ’ ] )

x : x . t i m e t u p l e ( ) . tm yday )

d f [ ’DAY’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply ( lambda
d f [ ’COS DAY ’ ] = np . cos ( d f [ ’DAY’ ] / 3 6 5 . ∗ 2∗
d f [ ’SIN DAY ’ ] = np . s i n ( d f [ ’DAY’ ] / 3 6 5 . ∗ 2∗

np . p i )

np . p i )

d f [ ’WEEKDAY’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply (

d f [ ’MONTH’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply (
d f [ ’COS MONTH’ ] = np . cos ( d f [ ’MONTH’ ] / 1 2 . ∗
d f [ ’SIN MONTH ’ ] = np . s i n ( d f [ ’MONTH’ ] / 1 2 . ∗
d f [ ’WEEKEND’ ] = ( d f [ ’WEEKDAY’ ] > 4 ) ∗1

lambda x : x . month )
2∗ np . p i )
2∗ np . p i )

d f [ ’COS WEEKDAY’ ] = np . cos ( d f [ ’WEEKDAY’

d f [ ’SIN WEEKDAY ’ ] = np . s i n ( d f [ ’WEEKDAY’

lambda x : x . weekday ( ) )
] / 7 . ∗ 2∗ np . p i )
] / 7 . ∗ 2∗ np . p i )

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

7

# I n [ 2 6 ] :

c a l e n d a r

f e a t u r e s = [ ”COS DAY” , ”SIN DAY” , ”
COS WEEKDAY” , ”SIN WEEKDAY” , ”COS MONTH”
, ”SIN MONTH” , ”WEEKEND” ]

e l e c t r i c

f e a t u r e s = [ ” power avg ” , ”

power min ” , ” power max ” , ” e n e r g y u s e d ”
, ” f r e q u e n c y a v g ” , ” v o l t a g e a v g ” , ”
c u r r e n t a v g ” , ” p o w e r

f a c t o r a v g ” ]

# I n [ 2 7 ] :

df mean = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

e l e c t r i c
mean ( ) )

df mean . columns = [ x+” mean ” f o r x in d f [

d f

d f

d f

f e a t u r e s ] . columns ]

e l e c t r i c
s t d = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [
e l e c t r i c
s t d ( ) )
s t d . columns = [ x+” s t d ” f or x in d f [
e l e c t r i c

f e a t u r e s ] . apply ( lambda x : x .

f e a t u r e s ] . columns ]

d f c a l e n d a r = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

c a l e n d a r
mean ( ) )
l a b e l = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [ [ ’
new name ’ ] ] . apply ( lambda x : x . i l o c
[ 0 ] )

# I n [ 8 1 ] :

import numpy a s np

df avg day = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x

e l e c t r i c
[ 4 8 0 : 1 0 8 0 ] . mean ( ) )

d f a v g n i g h t = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [
f e a t u r e s ] . apply ( lambda x : x

e l e c t r i c
[ 0 : 4 8 0 and 1 0 8 1 : 1 3 3 9 ] . mean ( ) )

d f n i g h t d a y = df avg day / d f a v g n i g h t
d f n i g h t d a y . columns = [ x+”

n i g h t d a y r a t i o ” f o r x in d f [
e l e c t r i c

f e a t u r e s ] . columns ]

df max = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

e l e c t r i c
max ( ) )

df max . columns = [ x+” max” f or x in d f [

e l e c t r i c

f e a t u r e s ] . columns ]

df min = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

e l e c t r i c
max ( ) )

df min . columns = [ x+” min ” f or x in d f [

e l e c t r i c

f e a t u r e s ] . columns ]

# o t h e r
# H e a r t b e a t Time S e r i e s C l a s s i f i c a t i o n

s t u f f based on

With S u p p o r t

# V e c t o r Machines
# Argyro Kampouraki , George Manis , and

C h r i s t o p h o r o s Nikou , Member ,

IEEE

#

def

df

df

r o o t mean s q u a r e o f

s u c c e s s i v e

d i f f e r e n c e s
rmssd ( x ) :
N = l en ( x )
r e s u l t = np . s q r t ( sum ( np . e d i f f 1 d ( x ) ∗∗2)
/ ( N−1) )

return r e s u l t

f e a t u r e s ] . apply ( lambda x :

rmssd = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [
e l e c t r i c
rmssd ( x ) )
rmssd . columns = [ x+” rmssd ” f o r x in d f
[ e l e c t r i c

f e a t u r e s ] . columns ]

d f s d s d = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

e l e c t r i c
. s t d ( np . e d i f f 1 d ( x ) ) )

f e a t u r e s ] . apply ( lambda x : np

d f s d s d . columns = [ x+” sd sd ” f o r x in d f [

e l e c t r i c

f e a t u r e s ] . columns ]

# I n [ 8 2 ] :

df2 = pd . c o n c a t ( [ d f

df max , df min , d f n i g h t d a y , df
, d f s d s d , d f c a l e n d a r ] , 1 )

l a b e l , df mean , d f s t d ,
rmssd

# ### Te mp er at ur e Data

# I n [ 8 3 ] :

w e a t h e r d a t a = pd . r e a d c s v ( i n p u t p a t h +”

Weather Data Clean . csv ” )

# I n [ 8 4 ] :

w e a t h e r d a t a . columns = [ ’ Time ’ , ’Temp ’ , ’RH’

, ’ Hour ’ ]

w e a t h e r d a t a [ ’DAY’ ] = pd . t o d a t e t i m e (

w e a t h e r d a t a [ ’ Time ’ ] ) . apply ( lambda x :
x . t i m e t u p l e ( ) . tm yday )

mean weather = w e a t h e r d a t a . groupby ( [ ’DAY’

] ) [ [ ’Temp ’ , ’RH’ ] ] . apply ( lambda x : x .
mean ( ) )

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

8

s t d w e a t h e r = w e a t h e r d a t a . groupby ( [ ’DAY’
] ) [ [ ’Temp ’ , ’RH’ ] ] . apply ( lambda x : x .
s t d ( ) )

min weather = w e a t h e r d a t a . groupby ( [ ’DAY’
] ) [ [ ’Temp ’ , ’RH’ ] ] . apply ( lambda x : x .
min ( ) )

max weather = w e a t h e r d a t a . groupby ( [ ’DAY’
] ) [ [ ’Temp ’ , ’RH’ ] ] . apply ( lambda x : x .
max ( ) )

w e a t h e r = pd . c o n c a t ( [ mean weather ,

s t d w e a t h e r , min weather , max weather
] , 1 )

# I n [ 8 5 ] :

df3 = df2 . r e s e t

i n d e x ( ) . merge ( weather , how

l e f t o n = ’DAY’ ,

r i g h t

i n d e x =

= ’ l e f t ’ ,
True )

# I n [ 8 6 ] :

df3 . t o c s v ( i n p u t p a t h + f i l e n a m e +” p r o c e s s e d

. csv ” , i n d e x = F a l s e )

# I n [ 1 ] :

# i m p o r t pandas as pd
# i m p o r t numpy as np

# i n p u t p a t h = ’ / Users / G e r r i t / Documents / ’
# f i l e n a m e = ’ x sample 1312 ’
# d f 3 = pd . r e a d c s v ( i n p u t p a t h +”
x s a m p l e 1 3 1 2 p r o c e s s e d . c s v ”)

# d f 3 . s e t

i n d e x ( [ ’ i d ’ , ’DAY ’ ] , i n p l a c e=True )

# I n [ 8 7 ] :

# #### Kmeans

# I n [ 2 0 ] :

# from s k l e a r n . c l u s t e r
# i m p o r t numpy as np

i m p o r t KMeans

# kmeans = KMeans ( n i n i t = 10)
# kmeans . f i t ( X )
# l a b e l s = kmeans . l a b e l s
# c e n t r o i d s = kmeans . c l u s t e r c e n t e r s

# kmeans . c l u s t e r c e n t e r s
# max ( kmeans . l a b e l s

)

# #### S u p p o r t V e c t o r Machine C l a s s i f i e r

# I n [ 8 ] :

from s k l e a r n . svm import SVC
from s k l e a r n . ensemble import

R a n d o m F o r e s t C l a s s i f i e r

from s k l e a r n . ensemble import

G r a d i e n t B o o s t i n g C l a s s i f i e r
import

from s k l e a r n . m u l t i c l a s s

O n e V s O n e C l a s s i f i e r

from s k l e a r n . m u l t i c l a s s
O n e V s R e s t C l a s s i f i e r

import

# I n [ 7 ] :

k=3
Xcv = np . a r r a y s p l i t (X, k )
ycv = np . a r r a y s p l i t ( y , k )
Xcv

= [ np . v s t a c k ( t u p l e ( [ Xcv [ j ]

i f

j != i ] ) )

f or

df4 = np . random . p e r m u t a t i o n ( df3 )

ycv

= [ np . v s t a c k ( t u p l e ( [ ycv [ j ]

i f

j != i ] ) )

f or

# ### Using Machine L e a r n i n g A l g o r i t h m s
# Once we have read i n t h e d a t a p r o p e r l y ,
r i g h t machine l e a r n i n g
t h e s c i −k i t
l i b r a r y which has a l l

we can use t h e
a l g o r i t h m . F i r s t we i m p o r t

l e a r n ( s k l e a r n )
t h e good SVM s t u f f
t h i s
o t h e r a l g o r i t h m s as w e l l ! ) .

( and a l l

f o r

# I n [ 8 8 ] :

X = df4 [ : , 1 : ]
y = df4 [ : , : 1 ]

range ( k )
] ;

range ( k )
] ;

f o r

in
in range ( k )

j

f o r

in
in range ( k )

j

i

i

# I n [ 2 5 ] :

# S e l e c t one model

# C = [ 3 , 5 , 10 , 20 , 40 , 80]
C = [ 1 0 ]
# C = [ 0 . 1 , 5 . 0 , 1 0 . 0 , 5 0 . 0 , 1 0 0 . 0 , 5 0 0 . 0 ]

f o r c in C :
#

gamma = 0 . 0 )

model = SVC (C=50 , k e r n e l =’ r b f ’ ,

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

1

Ofﬁce Appliance Classiﬁcation

From Disaggregated Plug-Load Data

Gerrit De Moor, Elissa Goldner, Brock Petersen

Department of Civil and Environmental Engineering, Atmosphere/Energy Program

Stanford University, Stanford, CA, 94305, USA

Abstract—As buildings consume 40% of US primary energy,
energy efﬁciency through smart plug-load management can have
a signiﬁcant impact on US carbon emissions. In this paper we
investigate learning algorithms to classify plug-loads based on
minutely energy plug-load data and hourly weather data. One
month of data is preprocessed into 31 daily features which are
used to classify 5660 training samples into 31 labels using a
range of multi-class classiﬁcation algorithms. Of all models tested,
random forest trees performed the best, with a test error of 17%.
This promising result can likely be further reﬁned with improved
feature selection, and a larger dataset. Future work will focus on
increasing the model performance and integrating it into a useful
product for plug-load management companies.

I.

INTRODUCTION

Energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change. While
transportation and industry contribute to this problem, build-
ings represent the largest energy consuming end use in the
United States at approximately 40% of primary energy.

Historically building energy reduction focused on HVAC
and lighting. As those loads have decreased, the importance
of appliance loads in buliding energy use has increased. Ofﬁce
buildings represent approximately 50% of the building energy
use in the United States and appliances consume 22% of that
energy.

Enmetric is a plug-load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in ofﬁce buildings. Recognizing the challenge
faced by increased energy use, Enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants.
Enmetric utilizes smart power strips that sense the appliance
load in virtual real-time. Using the smart power strips in con-
junction with a software platform, building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down.

Learning more about Enmetrics system, the group identiﬁed
two challenges that could be assisted by the use of machine
learning algorithms; classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type. This paper
will explore different techniques to accomplish those goals.

II. MODEL FUNCTION

The main goal of our model is classiﬁcation of disaggregated
loads. Currently, Enmetrics labels each disaggregated load
by hand during installation of the smart power strips. If the
devices are moved, it is the responsibility of the building
manager to report the move to Enmetric. Since Enmetrics data
analytics depend on accurate labeling of devices and because
it is often difﬁcult for the building manager to keep track of
individual employees moving loads around, Enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved. The model developed
in this project learns device signatures from past data and
classiﬁes each load as a particular device on a daily basis. By
comparing yesterdays device classiﬁcations to todays device
classiﬁcations, loads that have been classiﬁed as a new type
can be ﬂagged for review.

III. DATA

Enmetric System smart power strips system takes plug-load
measurements every second but aggregates the measurements
to be stored at the minute level. The energy data collected
included average power, minimum and maximum power during
the second, average frequency, average current, and average
power factor (see table below). For our project, we obtained
the minutely data for over 200 appliances in a 50,000 square
foot ofﬁce building in Silicon Valley. The data covered one
year from 12/1/2013 to 11/31/2014 ( 20 GB). Because of
computational limits, we were restricted to only analyzing one
month at a time ( 2 GB).

To capture the effects weather has on the appliance energy
use, hourly weather data was incorporated in the analysis
as well. The Meso West weather database provided hourly
temperature and relative humidity for Moffet Airﬁeld.

IV. FEATURES AND PREPROCESSING

All the numerical data provided by Enmetric was processed
into features for our models. Since minutely appliance energy
proﬁles vary signiﬁcantly during the day, all data was aggre-
gated and classiﬁed at the day-level for each unique socket.
Daily means and standard deviations of the minutely plug-load
data were calculated and serve as features.

Temporal features in the form of day of week, day of year,
month, and weekend served as additional features. All values
(except for weekend, which is binary) were normalized to
within the range [-1, 1 ] by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

2

Hourly weather data, temperature and humidity, were pro-
cessed in a similar manner to the Enmetric data. The daily
mean, minimum, maximum and standard deviation were cal-
culated for the temperature and humidity. This brings the total
feature count to 27 and the number of training samples to 5660
(for one month of data). An overview of the data used and how
it was processed into features is shown in the table above.

The plug-load data contained 31 different appliances, result-
ing in 31 labels to be used for classiﬁcation. An overview of
the labels is shown is in the table below.

up the data. In this sense, it is very similar to the classic 30%
test data, 70% training data rule, but with the difference that we
perform this 3 times with random permutations of the dataset.
The trade off between training and testing error for different
values of K is shown below for the Random Forest model. The
variance versus biased nature of the K value is shown. The
graph also supports our selection of a K value of 3.

Fig. 1: Training and testing error from random forest for
different k values.

VI. SUPPORT VECTOR MACHINES

Support vector machines (SVMs) are a set of supervised
learning algorithms commonly used for classiﬁcation. SVMs
are useful for this particular problem because they are very
effective in a high dimensional space. Because they allow
the user to choose the kernel function to act as the decision
function in the model, SVMs are also ﬂexible to what types
of classiﬁcation problems they are suited for. Despite their
effectiveness and versatility, SVMs become less effective when
the number of features is much greater than the number of
samples. Since this was not the case for our classiﬁcation
problem (31 labels, 5660 training samples), the group started
out by classifying using Support Vector Classiﬁcation (SVC)
with a linear kernel and varied different parameters until we
found our best results. The general algorithm for a support
vector machine is as follows:
Given training vectors xi ∈ Rp,i=1,...,n, in two classes, and
a vector y ∈ Rn such that yi ∈ {1,−1}, SVC solves the
following primal problem:

(cid:88)

min
w,b,ζ

1
2

wT w + C

ζi

i=1,n

subject to:

yi(wT φ(xi) + b) ≥ 1 − ζi,

Its dual is

ζi ≥ 0, i = 1, ..., n

min

α

1
2

αT Qα − eT α

(1)

(2)

(3)

(4)

V. CLASSIFICATION MODELS

All learning algorithms considered for use for our project
fell under the category of supervised learning methods for
classiﬁcation. Because the dataset included several unique la-
bels, the project is a multiclass classiﬁcation problem. Several
models were tested with varying parameters to ﬁnd which algo-
rithm resulted in the highest performance rates. The following
popular multiclass classiﬁcation algorithms were tested for our
purposes: multiclass support vector machine (SVM standard),
multi-class SVM one-vs-one (SVM OvO), multi-class SVM
one-vs-rest (SVM OvR), Random Forest, and Gradient Tree
Boosting. A more detailed description of each algorithm, as
well as its strengths and weaknesses with respect
to this
project, can be found below. All algorithms were implemented
in Python, using the sci-kit learn library.

The tuning of the parameters for each algorithm was done
by k-fold cross validation, with a k-value of 3. This rather
small value of k was chosen because we had sufﬁcient training
samples and we wanted to speed up processing times. The
training samples were ﬁrst randomly permuted before splitting

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

3

subject to:

yT α = 0

0 ≤ αi ≤ C, i = 1, ..., l

(5)

(6)

where e is the vector of all ones, C ¿ 0 is the upper bound,
Q is an n by n positive semideﬁnite matrix, Qij ≡ K(xi, xj)
and φ(xi)T φ(x) is the kernel [4].

There are two types of approaches for multi-class SVMs:
One-vs-the-Rest (OvR) and One-vs-One (OvO). One-vs-the-
Rest considers training a single classiﬁer per class while the
One-vs-One trains N(N-1) binary classiﬁers for a N-way multi-
class problem [5]. A more detailed description of each method
can be found in the subsequent sections. Research has shown
that OvR and OvO are among the most suitable methods for
practical use [3].

A. One-Versus-One SVM

The ﬁrst type of SVM that was used for this project was
the standard SVC with a linear kernel found in the sci-kit
learn package. By default, SVC implements the one-versus-one
approach to multi-class classiﬁcation. OvO classiﬁcation will
build N(N-1) classiﬁers, one classiﬁer to distinguish between
each pair of classes i and j by marking i as the positive example
and j as the negative. When it is time to make a prediction
for a particular unseen sample, all classiﬁers are applied to
the sample and the class with the highest number of positive
predictions will be chosen.

rbf kernel : exp(−γ|x − x(cid:48)|2)

(7)

For the OvO SVM, we started by tuning the penalty pa-
rameter of the error term (C), which can also be seen as the
hyperplane separation term. While initially, increasing C lead
to smaller prediction errors, values larger than 10 resulted in
overﬁtting the training sample and hence an increase in testing
error. It was expected that small values of C would result
in more misclassiﬁcation, but we found that imporvements
reached their limit as C approaced a value of 10.The best
results for our model were found by using a radial basis
function (RBF) kernel, also know as the Gaussian kernel.
Varying values of gamma, the kernel coefﬁcient used with RBF
kernels, produced no improvement in predictions. A graphical
view of the effects of varying C values on the OvO support
vector classiﬁcation model can be found in Figure 2.

B. One-Versus-The-Rest SVM

As opposed to OvO where a sample is marked as in class
i and not in class j, One-vs-the-Rest will build N different
binary classiﬁers to determine if the example is either in class
i or not in class i [5]. For SVM OvR, the outputs and ideal
tuning parameters were very similar to standard SVM, as seen
in Figure 3. Similar to OvO SVM, the ideal parameters for the
OvR SVM were found to be C=10 with a RBF kernel.

Fig. 2: Training and testing error from One-versus-One support
vector classiﬁcation when varying C value.

Fig. 3: Training and testing error from One-versus-the-Rest
support vector classiﬁcation when varying C value.

VII. ENSEMBLE METHODS

Since SVM results were still showing signiﬁcant error
rates, the group decided to investigate ensemble methods for
classiﬁcation. The goal of ensemble methods is to combine
the predictions of several base estimators built with a given
learning algorithm to improve the robustness over a single
estimator [4]. By combining multiple hypotheses that may
produce weak results for a particular classiﬁcation problem,
ensembles methods aim to build a stronger, more accurate
hypothesis tailored for the individual problem. The two types
of ensemble methods used for our classiﬁcation problem were
random forest classiﬁcation and gradient tree boosting.

A. Random Forest

The random forest classiﬁcation method falls under the
category of an textitaveraging ensemble method. The driving
principle behind averaging methods is to build several esti-
mators independently and to average the resulting predictions
together in order to reduce the variance in predictions [4]. In
random forest classiﬁcation, a series of randomized decision
trees in the ensemble are built from a random subset of the
training set. Each subset used is a sample drawn from the
feature vector that is replaced after use for continued use in

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

4

subsequent tests. When splitting a node in the construction of
the tree, instead of choosing the best split that is best ﬁt for
the entire training set, the split chosen is the best ﬁt among
the subset of data. Although the randomness of these subsets
usually produces results with higher bias, the nature of the
averaging ensemble method also decreases the variance among
results, typically much greater than the increase in bias.

When running random forest classiﬁcation, we varied the the
number of estimators and maximum depth used in the algo-
rithm until we found optimal results. Intuitively, we expected
the performance to increase as we increased the number of
estimators (i.e. the number of trees), but this came with the
trade-off of a longer computational time. As can be seen in
Figure 4, the improvement in training and testing error as we
increased the number of estimators was not as great as we had
expected. Surprisingly for the group, varying the max depth of
the trees had a much greater effect on performance, as seen in
Figure 5. The optimal values of max depth and n-estimators
found for our model were 17 and 100, respectively.
Given training vectors xi ∈ Rn,i=1,..., l and a label vector
y ∈ Rl, a decision tree recursively partitions the space such
that the samples with the same labels are grouped together. Let
the data at node m be represented by Q. For each candidate
split θ = (j, tm) consisting of a feature j and threshold tm,
partition the data into Qlef t(θ) and Qright(θ) subsets

Qlef t(θ) = (x, y)|xj <= tm

Qright(θ) = Q \ Qlef t(θ)

The impurity at m is computed using an impurity function
H(), the choice of which depends on the task being solved
(classiﬁcation or regression)

G(Q, θ) =

nlef t
Nm

H(Qlef t(θ)) +

nright
Nm

H(Qright(θ))

Select the parameters that minimises the impurity

θ∗ =θ G(Q, θ)

Recurse for subsets Qlef t(θ∗) and Qright(θ∗) until the maxi-
mum allowable depth is reached, Nm < minsamples orNm =
1.

Fig. 5: Training and testing error from random forest classiﬁ-
cation when varying max depth.

B. Gradient Tree Boosting

The ﬁrst of two algorithms that we looked at is called gra-
dient boosting classiﬁcation. Gradient tree boosting combines
weak learning methods in an iterative manner until a single,
stronger learning method is created. The gradient tree boosting
method was chosen for our classiﬁcation problem because it
is typically very good at handling data with heterogeneous
features. More concern with this algorithm was around the
scalability of its results; although it proved effective for our
data sets in month-long intervals, it was thought that combining
that data into a full year could cause issues due to the sequen-
tial nature of boosting limiting its ability to be parallelized
[4].

Since gradient tree boosting is another ensemble method, the
group expected that varying the number of estimators and the
max depth of the tree branches would act in similar manners as
they did in the random forest algorithm. As seen in Figures 6
and 7, our assumptions were largely correct. More interesting
however, was the effect that learning rate had on the model
results, shown in Figure 8. After varying the parameters in
the gradient tree boosting algorithm, the optimal values of n-
estimators, learning rate, and max depth found for our model
were 60, 0.1, and 10, respectively.

Fig. 4: Training and testing error from random forest classiﬁ-
cation when varying number of estimators.

Fig. 6: Training and testing error from gradient tree boosting
classiﬁcation when varying number of estimators.

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

5

vs-the-rest) did not improve the performance. The Random
Forest algorithm performed the best in terms of speed and test
error. The algorithm ran in under 15 seconds and resulted in
a test error of 17%. Gradient Tree Boosting resulted in the
lowest training error and performed only slightly worse than
the Random Forest algorithm in terms of test error.

Our work has shown that machine learning algorithms can
be a useful, efﬁcient tools in classiﬁcation of ofﬁce appliances
from plug load data. Future work should be able to further
improve the classiﬁcation to more acceptable levels of test
errors. In particular, improved feature extraction and selection,
and a larger dataset look promising.

X. CONTINUED WORK

The work presented in this paper shows that

learning
algorithms can be an effective tool for Enmetrics Systems
to use in their smart plug analytic services. The group will
be presenting our ﬁndings at Enmetric Systems next month
with the intentions of continuing further and producing a
useful tool for the company to possibly use. Based on the
successful results developed throughout
the course of this
project, the following are potential actions that may be taken
in the continuation of the project:

1) Run the ﬁve learning algorithms presented in this poster
on the larger dataset, varying the necessary parameters
to ﬁnd the best ﬁt model for classiﬁcation.

2) Develop an iterative algorithm that

learns based on
a collection of past data, predicts for the day, and
compares to known to identify any location where
deviced could have been moved or unplugged.

3) Explore the space of prediction to eliminate the need
for Enmetric to record by hand the type of device at
each plug at installation.

ACKNOWLEDGMENT

We would like to thank Jake Masters and Brad Degnan of
Enmentric Systems, Inc. for providing us with the data and
motivation for this project. We would also like to thank Romain
Juban of C3 Energy for his advice and guidance on the use of
learning algorithms with load disaggregation and classiﬁcation.
Finally, we would like to thank Professor Andrew Ng and the
CS229 teaching and project assistants for the help, guidance,
and encouragement.

REFERENCES

[1] H. Kopka and P. W. Daly, A Guide to LATEX, 3rd ed. Harlow, England:

Addison-Wesley, 1999.

[2] Machine Learning Applications for Load, Price and Wind Power Pre-
diction in Power Systems, Michael Negnevitsky, Senior Member, IEEE,
Paras Mandal, Member, IEEE, and Anurag K. Srivastava, Member, IEEE
[3] H. Lei and V. Govindaraju, Half-Against-Half Multi-class Support
Vector Machines, State University of New York at Buffalo, http://blue.
utb.edu/hlei/Research/papers/Half HalfSVM.pdf

[4] F. Pedregosa, et.al, Scikit-learn: Machine Learning in Python, Journal

of Machine Learning Research, vol. 12, pg. 2825-2830, 2011
http://www.mit.edu/∼9.520/spring09/Classes/multiclass.pdf
http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1583738&
url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs all.jsp%
3Farnumber%3D1583738

Fig. 7: Training and testing error from gradient tree boosting
classiﬁcation when varying max depth.

Fig. 8: Training and testing error from gradient tree boosting
classiﬁcation when varying learning rate.

VIII. RESULTS

All three SVM algorithms produced the greatest training and
test errors seen out of the ﬁve algorithms used. Trying different
binary SVM decompositions (OvO and OvR) did not improve
the performance. The Random Forest algorithm performed the
best in terms of speed and test error. The algorithm ran in under
15 seconds and resulted in a test error of 17%. Gradient Tree
Boosting resulted in the lowest training error and performed
only slightly worse than the Random Forest algorithm in terms
of test error.

The graphs show that tuning the main parameters of the
algorithms has a signiﬁcant impact on the test error. Biased-
variance trade off does show in some of the tuning, as can
be expected when tuning certain parameters such as the C
parameter in SVM models.

The performance of the Random Forest conﬁrmed the fact
that random forests usually work faster, but need a sufﬁcient
dataset size to work its randomization concept.

IX. CONCLUSION

SVM performed worst of the algorithms we tested. Trying
different binary SVM decompositions (one-vs-one and one-

[5]
[6]

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

6

OFFICE APPLIANCE CLASSIFICATION MODEL

APPENDIX A

Different versions of this simulation were used throughout
this project. This version utilizes the most recent version
of the Ofﬁce Appliance Classiﬁcation algorithm. This script
was written in python, utilizing the pandas and sci-kit learn
libraries.

Listing 1: code 1

# c o d i n g : u t f −8

# ###DATA INPUT
# Read i n t h e d a t a and i n d e x by t i m e

stamps

#

# I n [ 1 ] :

# i m p o r t wa r ni n gs
# wa r ni n gs . f i l t e r w a r n i n g s (” i g n o r e ” ,

c a t e g o r y=D e p r e c a t i o n W a r n i n g )

# I n [ 2 1 ] :

import pandas a s pd
import numpy a s np
import
s y s
import
s t r i n g
from d a t e u t i l

import p a r s e r

i n p u t p a t h = ’ / Us er s / G e r r i t / Documents / ’

# read i n a l l d a t a f i l e s
from d i r e c t o r y
# f i l e n a m e s = [ 1 3 1 2 , 1401 , 1402 , 1403 ,
1404 , 1405 , 1406 , 1407 , 1408 , 1410 ,
1411]

# d f = pd . c o n c a t ( [ pd . r e a d c s v ( i n p u t p a t h +”

x sample ”+ s t r ( f i l e n a m e ) +”. c s v ” ,
low memory=F a l s e )
i n
f i l e n a m e s ] , 0 )

f i l e n a m e

f o r

# read i n one d a t a f i l e , o n l y
# f i l e n a m e = ’ x sample 1401 ’
f i l e n a m e = ’ x sample 1312 ’
d f = pd . r e a d c s v ( i n p u t p a t h + f i l e n a m e +” . csv

f i r s t nrows

” , low memory= F a l s e )

# ### Data m a n i p u l a t i o n
# Here we p l a y around w i t h t h e data , and

s e t up t h e X and Y v e c t o r

# I n [ 2 2 ] :

d f . head ( )

# I n [ 2 3 ] :

# c r e a t e one column o f u n i q u e IDs

from t h e

node Hid and c h a n n e l number

d f [ ’ channel number ’ ] = d f [ ’ channel number ’

] . apply ( s t r )

d f [ ’ node hid ’ ] = d f [ ’ node hid ’ ] . apply ( s t r )
d f [ ’ i d ’ ] = d f . apply ( lambda x : x [ ’ node hid ’

]+ x [ ’ channel number ’ ] , 1 )
d f [ ’ i d ’ ] = d f [ ’ i d ’ ] . apply ( i n t )
d f = d f . drop ( [ ’ node hid ’ ,

’ channel number ’

] , 1 )

d f . head ( )

# I n [ 2 4 ] :

# s t r i p t h e name column o f
i t new name

and c a l l

l o c a t i o n numbers

d f [ ’ new name ’ ] = d f . apply ( lambda x : x [ ’

name ’ ] , 1 )

d i c t d a t a = pd . r e a d c s v ( ’ / Us er s / G e r r i t /

Documents / D i c t i o n a r y . csv ’ )

d i c t i o n a r y = d i c t ( zi p ( d i c t d a t a . Old Name ,
d f = d f . r e p l a c e ({ ’ new name ’ : d i c t i o n a r y } )

d i c t d a t a . New Name ) )

# t a k e o u t u n l a b e l e d d a t a
d f = d f [ d f . new name != ’ Unknown ’ ]
d f . head ( )

# I n [ 2 5 ] :

d f [ ’ o c c u r r e d a t ’ ] = pd . t o d a t e t i m e ( d f [ ’

o c c u r r e d a t ’ ] )

x : x . t i m e t u p l e ( ) . tm yday )

d f [ ’DAY’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply ( lambda
d f [ ’COS DAY ’ ] = np . cos ( d f [ ’DAY’ ] / 3 6 5 . ∗ 2∗
d f [ ’SIN DAY ’ ] = np . s i n ( d f [ ’DAY’ ] / 3 6 5 . ∗ 2∗

np . p i )

np . p i )

d f [ ’WEEKDAY’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply (

d f [ ’MONTH’ ] = d f [ ’ o c c u r r e d a t ’ ] . apply (
d f [ ’COS MONTH’ ] = np . cos ( d f [ ’MONTH’ ] / 1 2 . ∗
d f [ ’SIN MONTH ’ ] = np . s i n ( d f [ ’MONTH’ ] / 1 2 . ∗
d f [ ’WEEKEND’ ] = ( d f [ ’WEEKDAY’ ] > 4 ) ∗1

lambda x : x . month )
2∗ np . p i )
2∗ np . p i )

d f [ ’COS WEEKDAY’ ] = np . cos ( d f [ ’WEEKDAY’

d f [ ’SIN WEEKDAY ’ ] = np . s i n ( d f [ ’WEEKDAY’

lambda x : x . weekday ( ) )
] / 7 . ∗ 2∗ np . p i )
] / 7 . ∗ 2∗ np . p i )

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

7

# I n [ 2 6 ] :

c a l e n d a r

f e a t u r e s = [ ”COS DAY” , ”SIN DAY” , ”
COS WEEKDAY” , ”SIN WEEKDAY” , ”COS MONTH”
, ”SIN MONTH” , ”WEEKEND” ]

e l e c t r i c

f e a t u r e s = [ ” power avg ” , ”

power min ” , ” power max ” , ” e n e r g y u s e d ”
, ” f r e q u e n c y a v g ” , ” v o l t a g e a v g ” , ”
c u r r e n t a v g ” , ” p o w e r

f a c t o r a v g ” ]

# I n [ 2 7 ] :

df mean = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

e l e c t r i c
mean ( ) )

df mean . columns = [ x+” mean ” f o r x in d f [

d f

d f

d f

f e a t u r e s ] . columns ]

e l e c t r i c
s t d = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [
e l e c t r i c
s t d ( ) )
s t d . columns = [ x+” s t d ” f or x in d f [
e l e c t r i c

f e a t u r e s ] . apply ( lambda x : x .

f e a t u r e s ] . columns ]

d f c a l e n d a r = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

c a l e n d a r
mean ( ) )
l a b e l = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [ [ ’
new name ’ ] ] . apply ( lambda x : x . i l o c
[ 0 ] )

# I n [ 8 1 ] :

import numpy a s np

df avg day = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x

e l e c t r i c
[ 4 8 0 : 1 0 8 0 ] . mean ( ) )

d f a v g n i g h t = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [
f e a t u r e s ] . apply ( lambda x : x

e l e c t r i c
[ 0 : 4 8 0 and 1 0 8 1 : 1 3 3 9 ] . mean ( ) )

d f n i g h t d a y = df avg day / d f a v g n i g h t
d f n i g h t d a y . columns = [ x+”

n i g h t d a y r a t i o ” f o r x in d f [
e l e c t r i c

f e a t u r e s ] . columns ]

df max = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

e l e c t r i c
max ( ) )

df max . columns = [ x+” max” f or x in d f [

e l e c t r i c

f e a t u r e s ] . columns ]

df min = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

f e a t u r e s ] . apply ( lambda x : x .

e l e c t r i c
max ( ) )

df min . columns = [ x+” min ” f or x in d f [

e l e c t r i c

f e a t u r e s ] . columns ]

# o t h e r
# H e a r t b e a t Time S e r i e s C l a s s i f i c a t i o n

s t u f f based on

With S u p p o r t

# V e c t o r Machines
# Argyro Kampouraki , George Manis , and

C h r i s t o p h o r o s Nikou , Member ,

IEEE

#

def

df

df

r o o t mean s q u a r e o f

s u c c e s s i v e

d i f f e r e n c e s
rmssd ( x ) :
N = l en ( x )
r e s u l t = np . s q r t ( sum ( np . e d i f f 1 d ( x ) ∗∗2)
/ ( N−1) )

return r e s u l t

f e a t u r e s ] . apply ( lambda x :

rmssd = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [
e l e c t r i c
rmssd ( x ) )
rmssd . columns = [ x+” rmssd ” f o r x in d f
[ e l e c t r i c

f e a t u r e s ] . columns ]

d f s d s d = d f . groupby ( [ ’ i d ’ , ’DAY’ ] ) [

e l e c t r i c
. s t d ( np . e d i f f 1 d ( x ) ) )

f e a t u r e s ] . apply ( lambda x : np

d f s d s d . columns = [ x+” sd sd ” f o r x in d f [

e l e c t r i c

f e a t u r e s ] . columns ]

# I n [ 8 2 ] :

df2 = pd . c o n c a t ( [ d f

df max , df min , d f n i g h t d a y , df
, d f s d s d , d f c a l e n d a r ] , 1 )

l a b e l , df mean , d f s t d ,
rmssd

# ### Te mp er at ur e Data

# I n [ 8 3 ] :

w e a t h e r d a t a = pd . r e a d c s v ( i n p u t p a t h +”

Weather Data Clean . csv ” )

# I n [ 8 4 ] :

w e a t h e r d a t a . columns = [ ’ Time ’ , ’Temp ’ , ’RH’

, ’ Hour ’ ]

w e a t h e r d a t a [ ’DAY’ ] = pd . t o d a t e t i m e (

w e a t h e r d a t a [ ’ Time ’ ] ) . apply ( lambda x :
x . t i m e t u p l e ( ) . tm yday )

mean weather = w e a t h e r d a t a . groupby ( [ ’DAY’

] ) [ [ ’Temp ’ , ’RH’ ] ] . apply ( lambda x : x .
mean ( ) )

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

8

s t d w e a t h e r = w e a t h e r d a t a . groupby ( [ ’DAY’
] ) [ [ ’Temp ’ , ’RH’ ] ] . apply ( lambda x : x .
s t d ( ) )

min weather = w e a t h e r d a t a . groupby ( [ ’DAY’
] ) [ [ ’Temp ’ , ’RH’ ] ] . apply ( lambda x : x .
min ( ) )

max weather = w e a t h e r d a t a . groupby ( [ ’DAY’
] ) [ [ ’Temp ’ , ’RH’ ] ] . apply ( lambda x : x .
max ( ) )

w e a t h e r = pd . c o n c a t ( [ mean weather ,

s t d w e a t h e r , min weather , max weather
] , 1 )

# I n [ 8 5 ] :

df3 = df2 . r e s e t

i n d e x ( ) . merge ( weather , how

l e f t o n = ’DAY’ ,

r i g h t

i n d e x =

= ’ l e f t ’ ,
True )

# I n [ 8 6 ] :

df3 . t o c s v ( i n p u t p a t h + f i l e n a m e +” p r o c e s s e d

. csv ” , i n d e x = F a l s e )

# I n [ 1 ] :

# i m p o r t pandas as pd
# i m p o r t numpy as np

# i n p u t p a t h = ’ / Users / G e r r i t / Documents / ’
# f i l e n a m e = ’ x sample 1312 ’
# d f 3 = pd . r e a d c s v ( i n p u t p a t h +”
x s a m p l e 1 3 1 2 p r o c e s s e d . c s v ”)

# d f 3 . s e t

i n d e x ( [ ’ i d ’ , ’DAY ’ ] , i n p l a c e=True )

# I n [ 8 7 ] :

# #### Kmeans

# I n [ 2 0 ] :

# from s k l e a r n . c l u s t e r
# i m p o r t numpy as np

i m p o r t KMeans

# kmeans = KMeans ( n i n i t = 10)
# kmeans . f i t ( X )
# l a b e l s = kmeans . l a b e l s
# c e n t r o i d s = kmeans . c l u s t e r c e n t e r s

# kmeans . c l u s t e r c e n t e r s
# max ( kmeans . l a b e l s

)

# #### S u p p o r t V e c t o r Machine C l a s s i f i e r

# I n [ 8 ] :

from s k l e a r n . svm import SVC
from s k l e a r n . ensemble import

R a n d o m F o r e s t C l a s s i f i e r

from s k l e a r n . ensemble import

G r a d i e n t B o o s t i n g C l a s s i f i e r
import

from s k l e a r n . m u l t i c l a s s

O n e V s O n e C l a s s i f i e r

from s k l e a r n . m u l t i c l a s s
O n e V s R e s t C l a s s i f i e r

import

# I n [ 7 ] :

k=3
Xcv = np . a r r a y s p l i t (X, k )
ycv = np . a r r a y s p l i t ( y , k )
Xcv

= [ np . v s t a c k ( t u p l e ( [ Xcv [ j ]

i f

j != i ] ) )

f or

df4 = np . random . p e r m u t a t i o n ( df3 )

ycv

= [ np . v s t a c k ( t u p l e ( [ ycv [ j ]

i f

j != i ] ) )

f or

# ### Using Machine L e a r n i n g A l g o r i t h m s
# Once we have read i n t h e d a t a p r o p e r l y ,
r i g h t machine l e a r n i n g
t h e s c i −k i t
l i b r a r y which has a l l

we can use t h e
a l g o r i t h m . F i r s t we i m p o r t

l e a r n ( s k l e a r n )
t h e good SVM s t u f f
t h i s
o t h e r a l g o r i t h m s as w e l l ! ) .

( and a l l

f o r

# I n [ 8 8 ] :

X = df4 [ : , 1 : ]
y = df4 [ : , : 1 ]

range ( k )
] ;

range ( k )
] ;

f o r

in
in range ( k )

j

f o r

in
in range ( k )

j

i

i

# I n [ 2 5 ] :

# S e l e c t one model

# C = [ 3 , 5 , 10 , 20 , 40 , 80]
C = [ 1 0 ]
# C = [ 0 . 1 , 5 . 0 , 1 0 . 0 , 5 0 . 0 , 1 0 0 . 0 , 5 0 0 . 0 ]

f o r c in C :
#

gamma = 0 . 0 )

model = SVC (C=50 , k e r n e l =’ r b f ’ ,

CS 229 MACHINE LEARNING, FALL 2014, STANFORD UNIVERSITY

t u n i n g : C = 50

model = R a n d o m F o r e s t C l a s s i f i e r (

n e s t i m a t o r s =100 , max depth =17 ,
m a x f e a t u r e s = ’ a u t o ’ ,
n j o b s =4)

t u n i n g : n e s t i m a t o r s = 100 ,

max depth = 17

model = G r a d i e n t B o o s t i n g C l a s s i f i e r (

l e a r n i n g r a t e =0.1 , n e s t i m a t o r s =60 ,
max depth =10 , m a x f e a t u r e s =15)
l e a r n i n g r a t e = 0 . 1 ,

t u n i n g :

e s t i m a t o r s = 60 , max depth = 5 ( or 10)
, m a x f e a t u r e s = 15

## To add t o SVC model

model = O n e V s O n e C l a s s i f i e r ( model ,

model = O n e V s R e s t C l a s s i f i e r ( model ,

n j o b s =4)

n j o b s =4)

t u n i n g : C = 500

e r r o r
e r r o r

t r a i n = [ [ ] ] ∗ c
t e s t = [ [ ] ] ∗ c

p r i n t ( ’ Number o f m i s p r e d i c t i o n s
sum ( p r e d i c t i o n != y [ : , 0 ] ) )

9

i s %d\n ’ %

# np . h s t a c k ( t u p l e ( p r e d i c t i o n , y [ : , 0 ] )
np . h s t a c k ( t u p l e ( [ p r e d i c t i o n , y [ : , 0 ] ] ) )

p r e d i c t i o n != y [ : 0 ]

# I n [ 6 0 ] :

# i m p o r t m a t p l o t l i b . p y p l o t as p l t
# %m a t p l o t l i b i n l i n e

# I n [ 6 5 ] :

# p l t . p l o t ( e r r o r s )
# p l t . x l a b e l (”C”)
# p l t . y l a b e l (” Cross−v a l i d a t e d e r r o r ”)
# p l t . t i t l e (” Tuning o f p a r a m e t e r C”)
# # p l t . s a v e f i g (” t e s t ”)

#

#

#

#

#

#

#

#
#
#

#

Xcv = np . a r r a y s p l i t ( X , c )
ycv = np . a r r a y s p l i t ( y , c )
Xcv = [ np . v s t a c k ( t u p l e ( [ Xcv [ j ]
i n
i n range ( c )

j != i ] ) )

f o r

i f

i

ycv = [ np . v s t a c k ( t u p l e ( [ ycv [ j ]
i n range ( c )
i n

j != i ] ) )

f o r

i f

i

j
range ( c ) ] ;

j
range ( c ) ] ;

# I n [

] :

f o r

f o r

f or

i

in range ( c ) :

model . f i t ( Xcv [ i ] , ycv [ i ] [ : , 0 ] )
# p r e d s
# p r i n t ” T r a i n i n g e r r o r

t r = model . p r e d i c t ( Xcv [ i ] )

f o l d
{}: {}”. f o r m a t ( i +1,1− model .
s c o r e ( Xcv [ i ] , ycv [ i ] ) )
t r a i n [ i ] = 1−model . s c o r e (

e r r o r

f o r

Xcv [ i ] , ycv [ i ] )

# p r e d s
# p r i n t ” T e s t

t e = model . p r e d i c t ( Xcv [ i ] )
f o l d {}:
{}”. f o r m a t ( i +1,1− model . s c o r e (
Xcv [ i ] , ycv [ i ] ) )

t e s t [ i ] = 1−model . s c o r e ( Xcv [

e r r o r

e r r o r

f o r

i ] , ycv [ i ] )

p r i n t c , ” T r a i n i n g e r r o r : {} , Cross−

v a l i d a t e d e r r o r : {} ” . format ( np .
mean ( e r r o r
e r r o r

t r a i n ) , np . mean (

t e s t ) )

# I n [ 4 6 ] :

p r e d i c t i o n = model . p r e d i c t (X)
p r i n t ( p r e d i c t i o n )

