Data Fusion for Predicting Breast Cancer Survival

Linbailu Jiang, Yufei Zhang, Siyi Peng

Mentor: Irene Kaplow

December 11, 2015

Introduction

1
1.1 Background
Cancer is more of a severe health issue than
ever in our current society. As severe as it is
lethal in general, there are many factors that
may aﬀect a patient’s survival. It is not easy
to ﬁnd a clear pattern to predict the survival
outcome of a cancer patient, which could be
a complex process involving diﬀerent biologic
conditions. Based on previous studies, many
features are considered to potentially aﬀect a
survival event – cancer type, age at diagno-
sis, treatment pathway, time since diagnosis,
and some speciﬁc genome patterns that may
relate to the progression of the cancer dis-
ease. While some of these features, such as
cancer type and age at diagnosis, are rela-
tively explicit and have more direct relations
with the survival outcome, some other fea-
tures, such as gene mutations and methyla-
tion levels, seem more complicated and re-
quire more eﬀort to analyze the potential in-
teractions among these features. In this pa-
per, we mainly focus on the pathological con-
ditions of the patients without examining dif-
ferent treatments of cancer.

1.2 Goal and Outline
In this paper, we want to understand a
patient’s survival rate given his/her genome
pattern and time interval since diagnosis.
Our ultimate goal is to predict the survival

rate of a patient with breast cancer chang-
ing over time based on some related genomic
data.

Our ﬁrst step is to understand how genomic
data is inﬂuenced by diﬀerent factors, such as
gene expression for diﬀerent RNA, copy num-
ber value, and the methylation level for diﬀer-
ent DNA. By doing feature correlation analy-
sis and feature selection in genomic data, we
identify cancer-related genes and their tar-
gets. The most challenging task here is to
distinguish the “driver” mutations, as a sub-
set that truly contributes to the tumor’s pro-
gression, from a large number of neutral “pas-
senger” mutations that characterize the can-
cer. Based on some previous studies, we guess
that a support vector machine method might
be helpful during this process.

At the second stage, we have relatively
fewer features for genomic data that may af-
fect the cancer survivals, so it could be easier
for us to conduct a merge based on the pa-
tient ID in the survival dataset and the sam-
ple ID in the genomic dataset. This would
help us to relate all the genomic information
to the survival results so we could combine
them with other potential features and start
to train our model. After that, we would es-
timate and compare the performance of the
models by using some cross validation meth-
ods.

1

Data Fusion for Predicting Breast Cancer Survival

Linbailu Jiang, Yufei Zhang, Siyi Peng

Mentor: Irene Kaplow

December 11, 2015

Introduction

1
1.1 Background
Cancer is more of a severe health issue than
ever in our current society. As severe as it is
lethal in general, there are many factors that
may aﬀect a patient’s survival. It is not easy
to ﬁnd a clear pattern to predict the survival
outcome of a cancer patient, which could be
a complex process involving diﬀerent biologic
conditions. Based on previous studies, many
features are considered to potentially aﬀect a
survival event – cancer type, age at diagno-
sis, treatment pathway, time since diagnosis,
and some speciﬁc genome patterns that may
relate to the progression of the cancer dis-
ease. While some of these features, such as
cancer type and age at diagnosis, are rela-
tively explicit and have more direct relations
with the survival outcome, some other fea-
tures, such as gene mutations and methyla-
tion levels, seem more complicated and re-
quire more eﬀort to analyze the potential in-
teractions among these features. In this pa-
per, we mainly focus on the pathological con-
ditions of the patients without examining dif-
ferent treatments of cancer.

1.2 Goal and Outline
In this paper, we want to understand a
patient’s survival rate given his/her genome
pattern and time interval since diagnosis.
Our ultimate goal is to predict the survival

rate of a patient with breast cancer chang-
ing over time based on some related genomic
data.

Our ﬁrst step is to understand how genomic
data is inﬂuenced by diﬀerent factors, such as
gene expression for diﬀerent RNA, copy num-
ber value, and the methylation level for diﬀer-
ent DNA. By doing feature correlation analy-
sis and feature selection in genomic data, we
identify cancer-related genes and their tar-
gets. The most challenging task here is to
distinguish the “driver” mutations, as a sub-
set that truly contributes to the tumor’s pro-
gression, from a large number of neutral “pas-
senger” mutations that characterize the can-
cer. Based on some previous studies, we guess
that a support vector machine method might
be helpful during this process.

At the second stage, we have relatively
fewer features for genomic data that may af-
fect the cancer survivals, so it could be easier
for us to conduct a merge based on the pa-
tient ID in the survival dataset and the sam-
ple ID in the genomic dataset. This would
help us to relate all the genomic information
to the survival results so we could combine
them with other potential features and start
to train our model. After that, we would es-
timate and compare the performance of the
models by using some cross validation meth-
ods.

1

1.3 Data
data
The

is

of

our

project

from
NIH(National Institutes of Health) Project,
and we obtained them from Professor Olivier
Gevaert. The data are all pre-possessed,
log-transformed, and well-separated based
on cancer type into 11 dataset.

The whole dataset is comprised of two
parts. First part is a dataset of patient ID,
cancer type, time since diagnosis (“TimeTo-
LastContactOrVisit” in days), and his/her
survival status (normally 0=alive, 1=dead).
Second part are some large lists about ge-
nomic data.
Each cancer type has one
large list.
In each large list, there are 3
datasets which separately contain gene ex-
pression data, copy number data, and methy-
lation data. All the data are pre-processed so
there can be some negative values in these
charts.

The ﬁrst genomic data we used in this
project is the gene expression data. Basi-
cally, for each sample, a high (large positive)
gene expression value for a speciﬁc gene code
means the information encoded in this gene
has been highly “interpreted”, while a low
value indicates that most of its information
has been “hidden”. The second genomic data
is called copy number variations data. This
data represents the structural variation of a
speciﬁc gene. A larger copy number implies
that the gene might be duplicated so that it
becomes more than the normal number while
a smaller copy number denotes a deletion in
a speciﬁc region so that the number is less
than the normal number. During these years,
methylation also becomes an important con-
cept in the cancer research, so our third
dataset records the pattern of methylation for
each sample. An aberrant DNA methylation
pattern, such as hyper-methylation or hypo-
methylation pattern, can usually be associ-
ated with many types of human malignancies.
In this project, due to time limit, we would
only focus on breast cancer(BRCA). The
sample sizes of the BRCA dataset is relatively

large compared with others, so we believe it’s
more likely to get valuable results when train-
ing on this dataset.

2 Methods
2.1 Feature Selection
For most of our datasets, the numbers of
features are much larger than the numbers of
sample (p » n), which can cause some diﬃ-
culties when training the data. For instance,
the sample size of the gene expression dataset
for breast cancer is only 985, while the corre-
sponding feature size is 16020, which is much
larger than the sample size here. A potential
problem that can be caused by this is over-
ﬁtting. To avoid this problem, the ﬁrst thing
needed to be handled is to reduce the number
of variables in these datasets.

The ﬁrst step we did here is to ﬁnd vari-
ance of each feature and get rid of the features
with low variance. For example, the variance
range of gene expression features for breast
cancer is from 0.03 to 25.50 and we ﬁnd most
of these features have relatively low variances.
We assume that the low-variance predictors
have less predictive power (which is not al-
ways true, we just use this simple method
to obtain a ﬁrst impression of the data), so
we tried removing some predictors with small
variance and use 10-fold cross validation to
see how this process may aﬀect the model
performance. Table 1 shows the model per-
formance based on diﬀerent variance thresh-
olds.

# of Selected

Features Based on

the Threshold

5909

1673

598

78

13

3

5

1

0.110037
0.352112

Variance Threshold
SVM
0.110061
Naive Bayes
0.121026
Table 1: Error estimations of SVM & Naive
Bayes using 10-fold cross validation

0.109963
0.347705

0.109976
0.332234

0.109975
0.269524

15

10

As we see, for an SVM model, there’s no ob-
vious improvement by reducing feature size;

2

Data Fusion for Predicting Breast Cancer Survival

Linbailu Jiang, Yufei Zhang, Siyi Peng

Mentor: Irene Kaplow

December 11, 2015

Introduction

1
1.1 Background
Cancer is more of a severe health issue than
ever in our current society. As severe as it is
lethal in general, there are many factors that
may aﬀect a patient’s survival. It is not easy
to ﬁnd a clear pattern to predict the survival
outcome of a cancer patient, which could be
a complex process involving diﬀerent biologic
conditions. Based on previous studies, many
features are considered to potentially aﬀect a
survival event – cancer type, age at diagno-
sis, treatment pathway, time since diagnosis,
and some speciﬁc genome patterns that may
relate to the progression of the cancer dis-
ease. While some of these features, such as
cancer type and age at diagnosis, are rela-
tively explicit and have more direct relations
with the survival outcome, some other fea-
tures, such as gene mutations and methyla-
tion levels, seem more complicated and re-
quire more eﬀort to analyze the potential in-
teractions among these features. In this pa-
per, we mainly focus on the pathological con-
ditions of the patients without examining dif-
ferent treatments of cancer.

1.2 Goal and Outline
In this paper, we want to understand a
patient’s survival rate given his/her genome
pattern and time interval since diagnosis.
Our ultimate goal is to predict the survival

rate of a patient with breast cancer chang-
ing over time based on some related genomic
data.

Our ﬁrst step is to understand how genomic
data is inﬂuenced by diﬀerent factors, such as
gene expression for diﬀerent RNA, copy num-
ber value, and the methylation level for diﬀer-
ent DNA. By doing feature correlation analy-
sis and feature selection in genomic data, we
identify cancer-related genes and their tar-
gets. The most challenging task here is to
distinguish the “driver” mutations, as a sub-
set that truly contributes to the tumor’s pro-
gression, from a large number of neutral “pas-
senger” mutations that characterize the can-
cer. Based on some previous studies, we guess
that a support vector machine method might
be helpful during this process.

At the second stage, we have relatively
fewer features for genomic data that may af-
fect the cancer survivals, so it could be easier
for us to conduct a merge based on the pa-
tient ID in the survival dataset and the sam-
ple ID in the genomic dataset. This would
help us to relate all the genomic information
to the survival results so we could combine
them with other potential features and start
to train our model. After that, we would es-
timate and compare the performance of the
models by using some cross validation meth-
ods.

1

1.3 Data
data
The

is

of

our

project

from
NIH(National Institutes of Health) Project,
and we obtained them from Professor Olivier
Gevaert. The data are all pre-possessed,
log-transformed, and well-separated based
on cancer type into 11 dataset.

The whole dataset is comprised of two
parts. First part is a dataset of patient ID,
cancer type, time since diagnosis (“TimeTo-
LastContactOrVisit” in days), and his/her
survival status (normally 0=alive, 1=dead).
Second part are some large lists about ge-
nomic data.
Each cancer type has one
large list.
In each large list, there are 3
datasets which separately contain gene ex-
pression data, copy number data, and methy-
lation data. All the data are pre-processed so
there can be some negative values in these
charts.

The ﬁrst genomic data we used in this
project is the gene expression data. Basi-
cally, for each sample, a high (large positive)
gene expression value for a speciﬁc gene code
means the information encoded in this gene
has been highly “interpreted”, while a low
value indicates that most of its information
has been “hidden”. The second genomic data
is called copy number variations data. This
data represents the structural variation of a
speciﬁc gene. A larger copy number implies
that the gene might be duplicated so that it
becomes more than the normal number while
a smaller copy number denotes a deletion in
a speciﬁc region so that the number is less
than the normal number. During these years,
methylation also becomes an important con-
cept in the cancer research, so our third
dataset records the pattern of methylation for
each sample. An aberrant DNA methylation
pattern, such as hyper-methylation or hypo-
methylation pattern, can usually be associ-
ated with many types of human malignancies.
In this project, due to time limit, we would
only focus on breast cancer(BRCA). The
sample sizes of the BRCA dataset is relatively

large compared with others, so we believe it’s
more likely to get valuable results when train-
ing on this dataset.

2 Methods
2.1 Feature Selection
For most of our datasets, the numbers of
features are much larger than the numbers of
sample (p » n), which can cause some diﬃ-
culties when training the data. For instance,
the sample size of the gene expression dataset
for breast cancer is only 985, while the corre-
sponding feature size is 16020, which is much
larger than the sample size here. A potential
problem that can be caused by this is over-
ﬁtting. To avoid this problem, the ﬁrst thing
needed to be handled is to reduce the number
of variables in these datasets.

The ﬁrst step we did here is to ﬁnd vari-
ance of each feature and get rid of the features
with low variance. For example, the variance
range of gene expression features for breast
cancer is from 0.03 to 25.50 and we ﬁnd most
of these features have relatively low variances.
We assume that the low-variance predictors
have less predictive power (which is not al-
ways true, we just use this simple method
to obtain a ﬁrst impression of the data), so
we tried removing some predictors with small
variance and use 10-fold cross validation to
see how this process may aﬀect the model
performance. Table 1 shows the model per-
formance based on diﬀerent variance thresh-
olds.

# of Selected

Features Based on

the Threshold

5909

1673

598

78

13

3

5

1

0.110037
0.352112

Variance Threshold
SVM
0.110061
Naive Bayes
0.121026
Table 1: Error estimations of SVM & Naive
Bayes using 10-fold cross validation

0.109963
0.347705

0.109976
0.332234

0.109975
0.269524

15

10

As we see, for an SVM model, there’s no ob-
vious improvement by reducing feature size;

2

however, for a Naive Bayes model, the smaller
the number of variables is, the better ac-
curacy is achieved. One possible explana-
tion for this is that the actual model (with
raw data) violates the conditional indepen-
dence assumption of the Naive Bayes model,
which means some genes may work together,
and their expression values can probably be
highly correlated. The accuracies for both
SVM and Naive Bayes model are close to 90
percent, however we don’t think these mod-
els are actually good at this point since we
found that all of them tend to predict 0(alive)
rather than 1(dead), and the corresponding
ROC curves imply that the models are unin-
formative.

To solve this problem, we decided to im-
prove our feature selection process and use a
more reliable method to pick important fea-
tures. According to previous studies, training
logistic regression models on each feature sep-
arately and ranking them by lowest CV error
can be a good method to ﬁnd valuable fea-
tures. We picked the top 200 genes with the
best performance on individual training, and
then ranked them again by using cox model
to reduce the ﬁnal feature size to be 20 (de-
tails in next section).

2.2 Survﬁt and Cox Model
After signiﬁcantly reducing the number of
feature variables, we then combined all the
important features to the survival dataset.
One important point to notice is the feature
variable “TimeToLastContactOrVisit”. It in-
dicates the number of days from a patient was
ﬁrst diagnosed breast cancer to his/her last
visit date.
If the patient is dead, this vari-
able represents the duration to death; if not,
this variable becomes a censored data, as we
don’t know actually what the patient’s cur-
rent status is. We only know this patient was
alive x days after the beginning of study (x =
“TimeToLastContactOrVisit”).
In previous
part, to simplify the model, we treated the

“TimeToLastContactOrVisit” as a continu-
ous feature variable and trained SVM mod-
els on it. However, the performance of these
models seems unfavorable, which may be ex-
plained by us not handling this term in a right
way. An alternate method to deal with it is to
treat this term as a part of response instead of
a predictor. We will discuss why it could be
important to correctly handle this term later
when we analyze our sample results.

Based on previous study, a common way to
combine the timeline and status of an event
is to create a new type object, usually called
“Surv” or survival object, which is a small
data matrix that contains comprehensive in-
formation of an event.
In this project, we
used cox model to ﬁt the survival data. The
cox model has the form:

λ(t|X) = λ0(t) exp(β1X1 + ... + βpXp)

= λ0(t) exp(Xβ0)

We used functions “Survﬁt” and “coxph” in
the survival package in R to train the mod-
els with cox model and then ﬁt the survival
object.The next problem we met in this part
was the large time cost of ﬁtting cox models
on hundreds of features. To improve the ef-
ﬁciency of our models, we ran cox models on
each feature individually and pick the top 20
cox-ranked genes to be our ﬁnal features in
our model. After training a cox model on the
20 features, and predicted a survival curve
for each test sample and calculate the cross
validation error for our model.

3 Results and Discussion
3.1
Survival

Interpreting
Curves

Diﬀerent from other types of response, sur-
vival objects cannot be directly compared
with the original test data. Therefore, we
need to ﬁnd a way to transform our predic-
tions back into two-level (0/1) factors to cal-

3

Data Fusion for Predicting Breast Cancer Survival

Linbailu Jiang, Yufei Zhang, Siyi Peng

Mentor: Irene Kaplow

December 11, 2015

Introduction

1
1.1 Background
Cancer is more of a severe health issue than
ever in our current society. As severe as it is
lethal in general, there are many factors that
may aﬀect a patient’s survival. It is not easy
to ﬁnd a clear pattern to predict the survival
outcome of a cancer patient, which could be
a complex process involving diﬀerent biologic
conditions. Based on previous studies, many
features are considered to potentially aﬀect a
survival event – cancer type, age at diagno-
sis, treatment pathway, time since diagnosis,
and some speciﬁc genome patterns that may
relate to the progression of the cancer dis-
ease. While some of these features, such as
cancer type and age at diagnosis, are rela-
tively explicit and have more direct relations
with the survival outcome, some other fea-
tures, such as gene mutations and methyla-
tion levels, seem more complicated and re-
quire more eﬀort to analyze the potential in-
teractions among these features. In this pa-
per, we mainly focus on the pathological con-
ditions of the patients without examining dif-
ferent treatments of cancer.

1.2 Goal and Outline
In this paper, we want to understand a
patient’s survival rate given his/her genome
pattern and time interval since diagnosis.
Our ultimate goal is to predict the survival

rate of a patient with breast cancer chang-
ing over time based on some related genomic
data.

Our ﬁrst step is to understand how genomic
data is inﬂuenced by diﬀerent factors, such as
gene expression for diﬀerent RNA, copy num-
ber value, and the methylation level for diﬀer-
ent DNA. By doing feature correlation analy-
sis and feature selection in genomic data, we
identify cancer-related genes and their tar-
gets. The most challenging task here is to
distinguish the “driver” mutations, as a sub-
set that truly contributes to the tumor’s pro-
gression, from a large number of neutral “pas-
senger” mutations that characterize the can-
cer. Based on some previous studies, we guess
that a support vector machine method might
be helpful during this process.

At the second stage, we have relatively
fewer features for genomic data that may af-
fect the cancer survivals, so it could be easier
for us to conduct a merge based on the pa-
tient ID in the survival dataset and the sam-
ple ID in the genomic dataset. This would
help us to relate all the genomic information
to the survival results so we could combine
them with other potential features and start
to train our model. After that, we would es-
timate and compare the performance of the
models by using some cross validation meth-
ods.

1

1.3 Data
data
The

is

of

our

project

from
NIH(National Institutes of Health) Project,
and we obtained them from Professor Olivier
Gevaert. The data are all pre-possessed,
log-transformed, and well-separated based
on cancer type into 11 dataset.

The whole dataset is comprised of two
parts. First part is a dataset of patient ID,
cancer type, time since diagnosis (“TimeTo-
LastContactOrVisit” in days), and his/her
survival status (normally 0=alive, 1=dead).
Second part are some large lists about ge-
nomic data.
Each cancer type has one
large list.
In each large list, there are 3
datasets which separately contain gene ex-
pression data, copy number data, and methy-
lation data. All the data are pre-processed so
there can be some negative values in these
charts.

The ﬁrst genomic data we used in this
project is the gene expression data. Basi-
cally, for each sample, a high (large positive)
gene expression value for a speciﬁc gene code
means the information encoded in this gene
has been highly “interpreted”, while a low
value indicates that most of its information
has been “hidden”. The second genomic data
is called copy number variations data. This
data represents the structural variation of a
speciﬁc gene. A larger copy number implies
that the gene might be duplicated so that it
becomes more than the normal number while
a smaller copy number denotes a deletion in
a speciﬁc region so that the number is less
than the normal number. During these years,
methylation also becomes an important con-
cept in the cancer research, so our third
dataset records the pattern of methylation for
each sample. An aberrant DNA methylation
pattern, such as hyper-methylation or hypo-
methylation pattern, can usually be associ-
ated with many types of human malignancies.
In this project, due to time limit, we would
only focus on breast cancer(BRCA). The
sample sizes of the BRCA dataset is relatively

large compared with others, so we believe it’s
more likely to get valuable results when train-
ing on this dataset.

2 Methods
2.1 Feature Selection
For most of our datasets, the numbers of
features are much larger than the numbers of
sample (p » n), which can cause some diﬃ-
culties when training the data. For instance,
the sample size of the gene expression dataset
for breast cancer is only 985, while the corre-
sponding feature size is 16020, which is much
larger than the sample size here. A potential
problem that can be caused by this is over-
ﬁtting. To avoid this problem, the ﬁrst thing
needed to be handled is to reduce the number
of variables in these datasets.

The ﬁrst step we did here is to ﬁnd vari-
ance of each feature and get rid of the features
with low variance. For example, the variance
range of gene expression features for breast
cancer is from 0.03 to 25.50 and we ﬁnd most
of these features have relatively low variances.
We assume that the low-variance predictors
have less predictive power (which is not al-
ways true, we just use this simple method
to obtain a ﬁrst impression of the data), so
we tried removing some predictors with small
variance and use 10-fold cross validation to
see how this process may aﬀect the model
performance. Table 1 shows the model per-
formance based on diﬀerent variance thresh-
olds.

# of Selected

Features Based on

the Threshold

5909

1673

598

78

13

3

5

1

0.110037
0.352112

Variance Threshold
SVM
0.110061
Naive Bayes
0.121026
Table 1: Error estimations of SVM & Naive
Bayes using 10-fold cross validation

0.109963
0.347705

0.109976
0.332234

0.109975
0.269524

15

10

As we see, for an SVM model, there’s no ob-
vious improvement by reducing feature size;

2

however, for a Naive Bayes model, the smaller
the number of variables is, the better ac-
curacy is achieved. One possible explana-
tion for this is that the actual model (with
raw data) violates the conditional indepen-
dence assumption of the Naive Bayes model,
which means some genes may work together,
and their expression values can probably be
highly correlated. The accuracies for both
SVM and Naive Bayes model are close to 90
percent, however we don’t think these mod-
els are actually good at this point since we
found that all of them tend to predict 0(alive)
rather than 1(dead), and the corresponding
ROC curves imply that the models are unin-
formative.

To solve this problem, we decided to im-
prove our feature selection process and use a
more reliable method to pick important fea-
tures. According to previous studies, training
logistic regression models on each feature sep-
arately and ranking them by lowest CV error
can be a good method to ﬁnd valuable fea-
tures. We picked the top 200 genes with the
best performance on individual training, and
then ranked them again by using cox model
to reduce the ﬁnal feature size to be 20 (de-
tails in next section).

2.2 Survﬁt and Cox Model
After signiﬁcantly reducing the number of
feature variables, we then combined all the
important features to the survival dataset.
One important point to notice is the feature
variable “TimeToLastContactOrVisit”. It in-
dicates the number of days from a patient was
ﬁrst diagnosed breast cancer to his/her last
visit date.
If the patient is dead, this vari-
able represents the duration to death; if not,
this variable becomes a censored data, as we
don’t know actually what the patient’s cur-
rent status is. We only know this patient was
alive x days after the beginning of study (x =
“TimeToLastContactOrVisit”).
In previous
part, to simplify the model, we treated the

“TimeToLastContactOrVisit” as a continu-
ous feature variable and trained SVM mod-
els on it. However, the performance of these
models seems unfavorable, which may be ex-
plained by us not handling this term in a right
way. An alternate method to deal with it is to
treat this term as a part of response instead of
a predictor. We will discuss why it could be
important to correctly handle this term later
when we analyze our sample results.

Based on previous study, a common way to
combine the timeline and status of an event
is to create a new type object, usually called
“Surv” or survival object, which is a small
data matrix that contains comprehensive in-
formation of an event.
In this project, we
used cox model to ﬁt the survival data. The
cox model has the form:

λ(t|X) = λ0(t) exp(β1X1 + ... + βpXp)

= λ0(t) exp(Xβ0)

We used functions “Survﬁt” and “coxph” in
the survival package in R to train the mod-
els with cox model and then ﬁt the survival
object.The next problem we met in this part
was the large time cost of ﬁtting cox models
on hundreds of features. To improve the ef-
ﬁciency of our models, we ran cox models on
each feature individually and pick the top 20
cox-ranked genes to be our ﬁnal features in
our model. After training a cox model on the
20 features, and predicted a survival curve
for each test sample and calculate the cross
validation error for our model.

3 Results and Discussion
3.1
Survival

Interpreting
Curves

Diﬀerent from other types of response, sur-
vival objects cannot be directly compared
with the original test data. Therefore, we
need to ﬁnd a way to transform our predic-
tions back into two-level (0/1) factors to cal-

3

culate model errors. Figure 1,2 and 3 are sev-
eral examples of survival curves that can help
to illustrate this transform process. Each plot
represents one patient’s survival curve.

the ﬁrst patient’s survival curve is much ﬂat-
ter and higher than the second one. However,
since we are looking at diﬀerent timeline, the
two patients are both predicted dead at their
timeline, respectively.

(a) Ex1: Event 0 on the
1321th Day
Figure 1: Survival Curves for test samples
with Event 0(alive)

(b) Ex2: Event 0 on the
189th Day

Figure 1 compares two examples with sta-
tus 0 at last contact. Although the two sur-
vival curves look very diﬀerent, they can have
the same event status based on diﬀerent time-
lines. The survival curve of the 1st sample
seems much better than the curve of the 2nd
sample, so we may predict that the survival
rate of the 1st person is higher than the 2nd
person. However, after considering the time
information of the last contact, it’s easy to
ﬁnd that the survival rate of the 1st person
on the 1321th Day is around 0.75 while the
survival rate of the 2nd person on the 189th
Day is around 0.98, which is higher than the
survival rate of the 1st person in this case.
This example clearly illustrates that “Time-
ToLastContactOrVisit” is a crucial term that
can aﬀect our predictions to a great extent.

(a) Ex1: Event 0 on the
1321th Day
Figure 3: Survival Curves for test samples
with Events 0 and 1

(b) Ex5: Event 1 on the
1365th Day

Figure 3 gives us an example of two patients
with similar timelines and diﬀerent survival
curves. Since at the 1321th day, the ﬁrst
patient’s survival rate is around 0.7, while
the second patient’s is around 0.4, we predict
the ﬁrst patient alive and the second patient
dead.

From the example of three comparisons of
graphs, we can see that both survival curve
and and timeline determine how we predict
the living condition of a patient.

3.2 Comparing Models on Dif-

ferent Datasets

Gene Expression Copy Number Methylation

Breast Cancer Model
Trained on:
Top 20 Cox-ranked Genes
Top 40 Cox-ranked Genes
Top 60 Cox-ranked Genes
Table 2: Accuracy of BRCA Models on Dif-
ferent Datasets & Feature Size

83.6%
85.2%
86.1%

52.3%
54.1%
54.8%

65.3%
69.6%
70.0%

From the model comparison result, we
can see that both gene expression data and
methylation data have a decent prediction ac-
curacy on the patient’s survival rate. Top 20
Cox-rankedg genes are good enough to make
rather accurate predictions.

(a) Ex3: Event 1 on the
1920th Day
Figure 2: Survival Curves for test samples
with Event 1(dead)

(b) Ex4: Event 1 on the
825th Day

Figure 2 compares two exapmles with sta-
tus 1 at last contact. Again, it’s clear that
two survival curves are very diﬀerent, where

4

012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivaloData Fusion for Predicting Breast Cancer Survival

Linbailu Jiang, Yufei Zhang, Siyi Peng

Mentor: Irene Kaplow

December 11, 2015

Introduction

1
1.1 Background
Cancer is more of a severe health issue than
ever in our current society. As severe as it is
lethal in general, there are many factors that
may aﬀect a patient’s survival. It is not easy
to ﬁnd a clear pattern to predict the survival
outcome of a cancer patient, which could be
a complex process involving diﬀerent biologic
conditions. Based on previous studies, many
features are considered to potentially aﬀect a
survival event – cancer type, age at diagno-
sis, treatment pathway, time since diagnosis,
and some speciﬁc genome patterns that may
relate to the progression of the cancer dis-
ease. While some of these features, such as
cancer type and age at diagnosis, are rela-
tively explicit and have more direct relations
with the survival outcome, some other fea-
tures, such as gene mutations and methyla-
tion levels, seem more complicated and re-
quire more eﬀort to analyze the potential in-
teractions among these features. In this pa-
per, we mainly focus on the pathological con-
ditions of the patients without examining dif-
ferent treatments of cancer.

1.2 Goal and Outline
In this paper, we want to understand a
patient’s survival rate given his/her genome
pattern and time interval since diagnosis.
Our ultimate goal is to predict the survival

rate of a patient with breast cancer chang-
ing over time based on some related genomic
data.

Our ﬁrst step is to understand how genomic
data is inﬂuenced by diﬀerent factors, such as
gene expression for diﬀerent RNA, copy num-
ber value, and the methylation level for diﬀer-
ent DNA. By doing feature correlation analy-
sis and feature selection in genomic data, we
identify cancer-related genes and their tar-
gets. The most challenging task here is to
distinguish the “driver” mutations, as a sub-
set that truly contributes to the tumor’s pro-
gression, from a large number of neutral “pas-
senger” mutations that characterize the can-
cer. Based on some previous studies, we guess
that a support vector machine method might
be helpful during this process.

At the second stage, we have relatively
fewer features for genomic data that may af-
fect the cancer survivals, so it could be easier
for us to conduct a merge based on the pa-
tient ID in the survival dataset and the sam-
ple ID in the genomic dataset. This would
help us to relate all the genomic information
to the survival results so we could combine
them with other potential features and start
to train our model. After that, we would es-
timate and compare the performance of the
models by using some cross validation meth-
ods.

1

1.3 Data
data
The

is

of

our

project

from
NIH(National Institutes of Health) Project,
and we obtained them from Professor Olivier
Gevaert. The data are all pre-possessed,
log-transformed, and well-separated based
on cancer type into 11 dataset.

The whole dataset is comprised of two
parts. First part is a dataset of patient ID,
cancer type, time since diagnosis (“TimeTo-
LastContactOrVisit” in days), and his/her
survival status (normally 0=alive, 1=dead).
Second part are some large lists about ge-
nomic data.
Each cancer type has one
large list.
In each large list, there are 3
datasets which separately contain gene ex-
pression data, copy number data, and methy-
lation data. All the data are pre-processed so
there can be some negative values in these
charts.

The ﬁrst genomic data we used in this
project is the gene expression data. Basi-
cally, for each sample, a high (large positive)
gene expression value for a speciﬁc gene code
means the information encoded in this gene
has been highly “interpreted”, while a low
value indicates that most of its information
has been “hidden”. The second genomic data
is called copy number variations data. This
data represents the structural variation of a
speciﬁc gene. A larger copy number implies
that the gene might be duplicated so that it
becomes more than the normal number while
a smaller copy number denotes a deletion in
a speciﬁc region so that the number is less
than the normal number. During these years,
methylation also becomes an important con-
cept in the cancer research, so our third
dataset records the pattern of methylation for
each sample. An aberrant DNA methylation
pattern, such as hyper-methylation or hypo-
methylation pattern, can usually be associ-
ated with many types of human malignancies.
In this project, due to time limit, we would
only focus on breast cancer(BRCA). The
sample sizes of the BRCA dataset is relatively

large compared with others, so we believe it’s
more likely to get valuable results when train-
ing on this dataset.

2 Methods
2.1 Feature Selection
For most of our datasets, the numbers of
features are much larger than the numbers of
sample (p » n), which can cause some diﬃ-
culties when training the data. For instance,
the sample size of the gene expression dataset
for breast cancer is only 985, while the corre-
sponding feature size is 16020, which is much
larger than the sample size here. A potential
problem that can be caused by this is over-
ﬁtting. To avoid this problem, the ﬁrst thing
needed to be handled is to reduce the number
of variables in these datasets.

The ﬁrst step we did here is to ﬁnd vari-
ance of each feature and get rid of the features
with low variance. For example, the variance
range of gene expression features for breast
cancer is from 0.03 to 25.50 and we ﬁnd most
of these features have relatively low variances.
We assume that the low-variance predictors
have less predictive power (which is not al-
ways true, we just use this simple method
to obtain a ﬁrst impression of the data), so
we tried removing some predictors with small
variance and use 10-fold cross validation to
see how this process may aﬀect the model
performance. Table 1 shows the model per-
formance based on diﬀerent variance thresh-
olds.

# of Selected

Features Based on

the Threshold

5909

1673

598

78

13

3

5

1

0.110037
0.352112

Variance Threshold
SVM
0.110061
Naive Bayes
0.121026
Table 1: Error estimations of SVM & Naive
Bayes using 10-fold cross validation

0.109963
0.347705

0.109976
0.332234

0.109975
0.269524

15

10

As we see, for an SVM model, there’s no ob-
vious improvement by reducing feature size;

2

however, for a Naive Bayes model, the smaller
the number of variables is, the better ac-
curacy is achieved. One possible explana-
tion for this is that the actual model (with
raw data) violates the conditional indepen-
dence assumption of the Naive Bayes model,
which means some genes may work together,
and their expression values can probably be
highly correlated. The accuracies for both
SVM and Naive Bayes model are close to 90
percent, however we don’t think these mod-
els are actually good at this point since we
found that all of them tend to predict 0(alive)
rather than 1(dead), and the corresponding
ROC curves imply that the models are unin-
formative.

To solve this problem, we decided to im-
prove our feature selection process and use a
more reliable method to pick important fea-
tures. According to previous studies, training
logistic regression models on each feature sep-
arately and ranking them by lowest CV error
can be a good method to ﬁnd valuable fea-
tures. We picked the top 200 genes with the
best performance on individual training, and
then ranked them again by using cox model
to reduce the ﬁnal feature size to be 20 (de-
tails in next section).

2.2 Survﬁt and Cox Model
After signiﬁcantly reducing the number of
feature variables, we then combined all the
important features to the survival dataset.
One important point to notice is the feature
variable “TimeToLastContactOrVisit”. It in-
dicates the number of days from a patient was
ﬁrst diagnosed breast cancer to his/her last
visit date.
If the patient is dead, this vari-
able represents the duration to death; if not,
this variable becomes a censored data, as we
don’t know actually what the patient’s cur-
rent status is. We only know this patient was
alive x days after the beginning of study (x =
“TimeToLastContactOrVisit”).
In previous
part, to simplify the model, we treated the

“TimeToLastContactOrVisit” as a continu-
ous feature variable and trained SVM mod-
els on it. However, the performance of these
models seems unfavorable, which may be ex-
plained by us not handling this term in a right
way. An alternate method to deal with it is to
treat this term as a part of response instead of
a predictor. We will discuss why it could be
important to correctly handle this term later
when we analyze our sample results.

Based on previous study, a common way to
combine the timeline and status of an event
is to create a new type object, usually called
“Surv” or survival object, which is a small
data matrix that contains comprehensive in-
formation of an event.
In this project, we
used cox model to ﬁt the survival data. The
cox model has the form:

λ(t|X) = λ0(t) exp(β1X1 + ... + βpXp)

= λ0(t) exp(Xβ0)

We used functions “Survﬁt” and “coxph” in
the survival package in R to train the mod-
els with cox model and then ﬁt the survival
object.The next problem we met in this part
was the large time cost of ﬁtting cox models
on hundreds of features. To improve the ef-
ﬁciency of our models, we ran cox models on
each feature individually and pick the top 20
cox-ranked genes to be our ﬁnal features in
our model. After training a cox model on the
20 features, and predicted a survival curve
for each test sample and calculate the cross
validation error for our model.

3 Results and Discussion
3.1
Survival

Interpreting
Curves

Diﬀerent from other types of response, sur-
vival objects cannot be directly compared
with the original test data. Therefore, we
need to ﬁnd a way to transform our predic-
tions back into two-level (0/1) factors to cal-

3

culate model errors. Figure 1,2 and 3 are sev-
eral examples of survival curves that can help
to illustrate this transform process. Each plot
represents one patient’s survival curve.

the ﬁrst patient’s survival curve is much ﬂat-
ter and higher than the second one. However,
since we are looking at diﬀerent timeline, the
two patients are both predicted dead at their
timeline, respectively.

(a) Ex1: Event 0 on the
1321th Day
Figure 1: Survival Curves for test samples
with Event 0(alive)

(b) Ex2: Event 0 on the
189th Day

Figure 1 compares two examples with sta-
tus 0 at last contact. Although the two sur-
vival curves look very diﬀerent, they can have
the same event status based on diﬀerent time-
lines. The survival curve of the 1st sample
seems much better than the curve of the 2nd
sample, so we may predict that the survival
rate of the 1st person is higher than the 2nd
person. However, after considering the time
information of the last contact, it’s easy to
ﬁnd that the survival rate of the 1st person
on the 1321th Day is around 0.75 while the
survival rate of the 2nd person on the 189th
Day is around 0.98, which is higher than the
survival rate of the 1st person in this case.
This example clearly illustrates that “Time-
ToLastContactOrVisit” is a crucial term that
can aﬀect our predictions to a great extent.

(a) Ex1: Event 0 on the
1321th Day
Figure 3: Survival Curves for test samples
with Events 0 and 1

(b) Ex5: Event 1 on the
1365th Day

Figure 3 gives us an example of two patients
with similar timelines and diﬀerent survival
curves. Since at the 1321th day, the ﬁrst
patient’s survival rate is around 0.7, while
the second patient’s is around 0.4, we predict
the ﬁrst patient alive and the second patient
dead.

From the example of three comparisons of
graphs, we can see that both survival curve
and and timeline determine how we predict
the living condition of a patient.

3.2 Comparing Models on Dif-

ferent Datasets

Gene Expression Copy Number Methylation

Breast Cancer Model
Trained on:
Top 20 Cox-ranked Genes
Top 40 Cox-ranked Genes
Top 60 Cox-ranked Genes
Table 2: Accuracy of BRCA Models on Dif-
ferent Datasets & Feature Size

83.6%
85.2%
86.1%

52.3%
54.1%
54.8%

65.3%
69.6%
70.0%

From the model comparison result, we
can see that both gene expression data and
methylation data have a decent prediction ac-
curacy on the patient’s survival rate. Top 20
Cox-rankedg genes are good enough to make
rather accurate predictions.

(a) Ex3: Event 1 on the
1920th Day
Figure 2: Survival Curves for test samples
with Event 1(dead)

(b) Ex4: Event 1 on the
825th Day

Figure 2 compares two exapmles with sta-
tus 1 at last contact. Again, it’s clear that
two survival curves are very diﬀerent, where

4

012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivalo012345670.00.40.8YearsSurvivalo[5] Geaghan M, Cairns MJ (2015). “Mi-
croRNA and Posttranscriptional Dysreg-
ulation in Psychiatry”. Biol. Psychiatry
78 (4): 231-9.

[6] Zaidi SK, Young DW, Choi JY, Pratap
J, Javed A, Montecino M, Stein JL, Lian
JB, van Wijnen AJ, Stein GS (October
2004). “Intranuclear traﬃcking: organiza-
tion and assembly of regulatory machin-
ery for combinatorial biological control”.
J. Biol. Chem. 279 (42): 43363-6.

[7] Hegde RS, Kang SW (July 2008). “The
concept of translocational regulation”. J.
Cell Biol. 182 (2): 225-32.

[8] https://en.wikipedia.org/wiki/Copy-

number variation

[9] https://en.wikipedia.org/wiki/

Copy number analysis

4 Conclusion
From various method, we found that treat-
ing both “TimeToLastContactOrVisit” and
“event”(the survival status) as a survival ob-
ject and ﬁtting a cox model to it is a good ap-
proach to train and predict the survival sta-
tus of cancer patients.
It has much higher
accuracy on predicting the patients’ survival
status than simply treating the whole prob-
lem as a classiﬁcation model and implement-
ing support vector machine or Naive Bayes
model. Both gene expression and methyla-
tion dataset work well as feature variables,
and using the top 20 Cox-ranked features is
enough accurate to make good predictions.

5 Future Work
Currently, we set the threshold as 0.5 in the
cox model to predict the cancer patient’s sur-
vival status. In the future, we could raise the
threshold so that we will increase the speci-
ﬁcity while not decreasing the sensitivity too
much.

Moreover, we hope to ﬁnd a way to combine
gene expression and methylation data and use
combined dataset to have a better prediction
on cancer patient’s survival status.

Other than breast cancer, we could broaden

our research on other cancer types as well.

References
[1] https://en.wikipedia.org/wiki/

Gene expression

[2] https://en.wikipedia.org/wiki/

Methylation

[3] Magali Champion, Olivier Gevaert,Multi-

omics data fusion for cancer data

[4] https://en.wikipedia.org/wiki/

Proportional hazards model

5

