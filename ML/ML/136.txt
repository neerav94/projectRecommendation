Crime Prediction and Classiﬁcation in San Francisco City

Addarsh Chandrasekar, Abhilash Sunder Raj and Poorna Kumar

Abstract— To be better prepared to respond to criminal activity,
it is important to understand patterns in crime. In our project, we
analyze crime data from the city of San Francisco, drawn from
a publicly available dataset. At the outset, the task is to predict
which category of crime is most likely to occur given a time and
place in San Francisco. To overcome the limitations imposed by our
limited set of features, we enrich our data by adding information
from the United States Census to it. We also attempt to make our
classiﬁcation task more meaningful by merging multiple classes
into larger classes. Finally, we report and reﬂect on our results
with different classiﬁers, and dwell on avenues for future work.

I. INTRODUCTION

Many important questions in public safety and protection
relate to crime, and a better understanding of crime is beneﬁcial
in multiple ways: it can lead to targeted and sensitive practices
by law enforcement authorities to mitigate crime, and more
concerted efforts by citizens and authorities to create healthy
neighborhood environments. With the advent of the Big Data
era and the availability of fast, efﬁcient algorithms for data
analysis, understanding patterns in crime from data is an active
and growing ﬁeld of research.

In our project, we use spatio-temporal and demographic
data to predict which category of crime is most
likely to
have occurred, given a time, place and the demographics of
the place. The inputs to our algorithms are time (hour, day,
month, year), place (latitude, longitude, and police district),
and demographic data (population, median income, minority
population, and number of families, which we get from the
United States Census). The output is the category of crime that
is likely to have occurred. We try out multiple classiﬁcation
algorithms, such as Naive Bayes, Support Vector Machines,
Gradient Boosted Decision Trees, and Random Forests. We also
perform multiple classiﬁcation tasks – we ﬁrst try to predict
which of 39 classes of crimes are likely to have occurred, and
later try to differentiate between blue- and white-collar crimes,
as well as violent and non-violent crimes.

II. RELATED WORK

Much of the current work is focused in two major directions:
(i) predicting surges and hotspots of crime, and (ii) understand-
ing patterns of criminal behavior that could help in solving
criminal investigations.

Important contributions towards the former include [1] by
Bogomolov et al, who try to predict whether any particular area
in London will be a crime hotspot or not, using anonymized
behavioural data from mobile networks as well as demographic
data. In [2], Chung-Hsien Yu et al use classiﬁcation techniques
to classify neighbourhoods in a city as hotspots of residential
burglary, using a variety of classiﬁcation algorithms such as
Support Vector Machines, Naive Bayes, and Neural Networks.
(More work on the usefulness of Support Vector Machines for
hotspot detection can be found in [3]). Toole et al demonstrated
in [4], by analyzing crime records for the city of Philadelphia,
that signiﬁcant spatio-temporal correlations exist in crime data,
and they were able to identify clusters of neighbourhoods whose
crime rates were affected simultaneously by external forces.

They also noted signiﬁcant correlations in crime across weekly
time scales.

Towards the second objective of understanding patterns of
criminal behaviour, signiﬁcant contributions have been made
by Tong Wang et al in [5], in ﬁnding patterns in criminal
activity and identifying individuals or groups of individuals who
might have committed particular crimes. Their approach was to
identify a common modus operandi across crimes, which could
then be linked to groups or individuals who might commit the
crime. For this, the authors proposed a new machine learning
method called Series Finder, which was trained to recognize
patterns in housebreak incidents in Cambridge, Massachusetts.
Our approach shares certain similarities with some of the
work described above,
in that we use spatio-temporal and
demographic information to discover which types of crimes are
likely to have occurred. However, we are notably different in
that, given the data, we seek to predict which category of crime
is most likely to occur, and we are hence concerned principally
with understanding the differences between different types of
crime, which is relatively unexplored territory.

III. OUR DATASET

Our dataset is a publicly available dataset that we obtained
from Kaggle, which has information about 878,049 crimes
that took place in San Francisco city over a span of nearly
twelve years. Each crime is labeled as belonging to one of 39
categories.

A. Features

Every entry in our training data set is about a particular crime,

and contains the following information:

• Date and timestamp of the incident.
• Day of the week that the crime occurred.
• Name of the Police Department District.
• Address:
incident.
• Latitude.
• Longitude.
• Category: category of the crime incident. This is the target

the approximate street address of the crime

variable.

• Description: a brief note describing any pertinent details of
the crime. (This was not used as a feature in our classiﬁers.)
• Resolution: whether the crime was resolved (with the
perpetrator being, say, arrested or booked) or not. (This
was also not used as a feature in our classiﬁers.)

TABLE I: Some sample rows from our dataset

Category

Dates
13-05-2015 23:53 WARRANTS
13-05-2015 23:53
13-05-2015 23:33
13-05-2015 23:30

OTHER OFFENSES
OTHER OFFENSES
LARCENY

DayOfWeek
Descript
WARRANTS
Wednesday
TRAFFIC VIOLATION Wednesday
TRAFFIC VIOLATION Wednesday
GRAND THEFT
Wednesday

PdDistrict
NORTHERN
NORTHERN
NORTHERN
NORTHERN

Crime Prediction and Classiﬁcation in San Francisco City

Addarsh Chandrasekar, Abhilash Sunder Raj and Poorna Kumar

Abstract— To be better prepared to respond to criminal activity,
it is important to understand patterns in crime. In our project, we
analyze crime data from the city of San Francisco, drawn from
a publicly available dataset. At the outset, the task is to predict
which category of crime is most likely to occur given a time and
place in San Francisco. To overcome the limitations imposed by our
limited set of features, we enrich our data by adding information
from the United States Census to it. We also attempt to make our
classiﬁcation task more meaningful by merging multiple classes
into larger classes. Finally, we report and reﬂect on our results
with different classiﬁers, and dwell on avenues for future work.

I. INTRODUCTION

Many important questions in public safety and protection
relate to crime, and a better understanding of crime is beneﬁcial
in multiple ways: it can lead to targeted and sensitive practices
by law enforcement authorities to mitigate crime, and more
concerted efforts by citizens and authorities to create healthy
neighborhood environments. With the advent of the Big Data
era and the availability of fast, efﬁcient algorithms for data
analysis, understanding patterns in crime from data is an active
and growing ﬁeld of research.

In our project, we use spatio-temporal and demographic
data to predict which category of crime is most
likely to
have occurred, given a time, place and the demographics of
the place. The inputs to our algorithms are time (hour, day,
month, year), place (latitude, longitude, and police district),
and demographic data (population, median income, minority
population, and number of families, which we get from the
United States Census). The output is the category of crime that
is likely to have occurred. We try out multiple classiﬁcation
algorithms, such as Naive Bayes, Support Vector Machines,
Gradient Boosted Decision Trees, and Random Forests. We also
perform multiple classiﬁcation tasks – we ﬁrst try to predict
which of 39 classes of crimes are likely to have occurred, and
later try to differentiate between blue- and white-collar crimes,
as well as violent and non-violent crimes.

II. RELATED WORK

Much of the current work is focused in two major directions:
(i) predicting surges and hotspots of crime, and (ii) understand-
ing patterns of criminal behavior that could help in solving
criminal investigations.

Important contributions towards the former include [1] by
Bogomolov et al, who try to predict whether any particular area
in London will be a crime hotspot or not, using anonymized
behavioural data from mobile networks as well as demographic
data. In [2], Chung-Hsien Yu et al use classiﬁcation techniques
to classify neighbourhoods in a city as hotspots of residential
burglary, using a variety of classiﬁcation algorithms such as
Support Vector Machines, Naive Bayes, and Neural Networks.
(More work on the usefulness of Support Vector Machines for
hotspot detection can be found in [3]). Toole et al demonstrated
in [4], by analyzing crime records for the city of Philadelphia,
that signiﬁcant spatio-temporal correlations exist in crime data,
and they were able to identify clusters of neighbourhoods whose
crime rates were affected simultaneously by external forces.

They also noted signiﬁcant correlations in crime across weekly
time scales.

Towards the second objective of understanding patterns of
criminal behaviour, signiﬁcant contributions have been made
by Tong Wang et al in [5], in ﬁnding patterns in criminal
activity and identifying individuals or groups of individuals who
might have committed particular crimes. Their approach was to
identify a common modus operandi across crimes, which could
then be linked to groups or individuals who might commit the
crime. For this, the authors proposed a new machine learning
method called Series Finder, which was trained to recognize
patterns in housebreak incidents in Cambridge, Massachusetts.
Our approach shares certain similarities with some of the
work described above,
in that we use spatio-temporal and
demographic information to discover which types of crimes are
likely to have occurred. However, we are notably different in
that, given the data, we seek to predict which category of crime
is most likely to occur, and we are hence concerned principally
with understanding the differences between different types of
crime, which is relatively unexplored territory.

III. OUR DATASET

Our dataset is a publicly available dataset that we obtained
from Kaggle, which has information about 878,049 crimes
that took place in San Francisco city over a span of nearly
twelve years. Each crime is labeled as belonging to one of 39
categories.

A. Features

Every entry in our training data set is about a particular crime,

and contains the following information:

• Date and timestamp of the incident.
• Day of the week that the crime occurred.
• Name of the Police Department District.
• Address:
incident.
• Latitude.
• Longitude.
• Category: category of the crime incident. This is the target

the approximate street address of the crime

variable.

• Description: a brief note describing any pertinent details of
the crime. (This was not used as a feature in our classiﬁers.)
• Resolution: whether the crime was resolved (with the
perpetrator being, say, arrested or booked) or not. (This
was also not used as a feature in our classiﬁers.)

TABLE I: Some sample rows from our dataset

Category

Dates
13-05-2015 23:53 WARRANTS
13-05-2015 23:53
13-05-2015 23:33
13-05-2015 23:30

OTHER OFFENSES
OTHER OFFENSES
LARCENY

DayOfWeek
Descript
WARRANTS
Wednesday
TRAFFIC VIOLATION Wednesday
TRAFFIC VIOLATION Wednesday
GRAND THEFT
Wednesday

PdDistrict
NORTHERN
NORTHERN
NORTHERN
NORTHERN

Resolution
ARREST, BOOKED
ARREST, BOOKED
ARREST, BOOKED
NONE

Address
OAK ST / LAGUNA ST
OAK ST / LAGUNA ST
VANNESS AV / GREENWICH ST
1500 Block of LOMBARD ST

X
-122.4258917
-122.4258917
-122.424363
-122.4269953

Y
37.7745986
37.7745986
37.80041432
37.80087263

B. Preprocessing

Before implementing machine learning algorithms on our
data, we went through a series of preprocessing steps with our
classiﬁcation task in mind. These included:

• Dropping features such as Resolution, Description and
Address: The resolution and description of a crime are
only known once the crime has occurred, and have limited
signiﬁcance in a practical, real-world scenario where one is
trying to predict what kind of crime has occurred, and so,
these were omitted. The address was dropped because we
had information about the latitude and longitude, and, in
that context, the address did not add much marginal value.
• The days of the week, police categories and crime cate-

gories were indexed and replaced by numbers.

• The timestamp contained the year, date and time of oc-
currence of each crime. This was decomposed into ﬁve
features: Year (2003-2015), Month (1-12), Date (1-31),
Hour (0-23) and Minute (0-59).

Following these preprocessing steps, we ran some out-of-the-
box learning algorithms as a part of our initial exploratory steps.
Our new feature set consisted of 7 features, all of which were
now numeric in nature.

C. Feature Enrichment

As we plunged into solving our classiﬁcation problem, we
felt that our feature set was not adequate enough in terms of
the information it contained to predict crime. In order to improve
our feature set, we augmented our dataset with additional
features that we scraped from the United States Census data.
This included demographic data such as mean income level of
a neighbourhood, racial diversity and so on. We felt that the
addition of such information could improve our performance at
the task of crime prediction. The census dataset was matched
with our dataset using the location coordinates of the crime in
our original dataset, and increased our number of features to
19.

D. Collapsing Crime Categories

We also felt that the number of output classiﬁcation labels, i.e.
39, in the original dataset was too high for accurate prediction.
The labels were too ﬁne-grained, and we realized that several
of these crime categories were similar to one another, and could
therefore be collapsed into smaller classes for better prediction.
Further, such collapsing could be done in several ways. Two
speciﬁc ways we went ahead with this were:

• Blue Collar Crimes vs White Collar Crimes: Blue Collar
Crimes included crimes such as Larceny, Arson and Bur-
glary while White Collar crimes included crimes such as
Fraud, Forgery and Extortion.

• Violent vs Non-Violent Crimes: Violent crimes included
crimes such as Assault, Arson and Prostitution while Non-
Violent crimes included crimes such as Trafﬁc Violations
and Trespassing.

IV. METHODS

After the preprocessing described in the previous sections,
we had three different classiﬁcations problems to solve, which
we proceeded to attack with an assortment of classiﬁcation

TABLE II: Sample Rows From The Census Data.
Note: The column headers are as follows. TC: Tract Code,
TIL: Tract Income Level, TMFI: Tract Median Family Income,
MFI: Median Family Income, OOU: Owner Occupied Units,
FU: Family Units, Min.: Minority

TIL

TC
101.00 Moderate
102.00
103.00

Upper
Upper

Distressed
No
No
No

TMFI % MFI
69.03
154.01
136.32

$96,900
$96,900
$96,900

2015 TMFI
$66,890
$1,49,236
$1,32,094

2010 MFI
$64,886
$1,44,750
$1,28,125

Tract Pop.
3739
4143
3852

Tract Min. % Min. Pop.
53.92
18.95
39.25

2016
785
1512

OOU
260
943
505

1- to 4- FU
424
918
1282

algorithms. The following sections explain the models we used
in detail.

A. Naive Bayes

As part of our initial exploratory analysis, we implemented
a Naive Bayes classiﬁer based on a multi-variate event model
with Laplace smoothing. This is a multi-class classiﬁcation
problem: the target variable Y (crime category) can be one of
39 classes, represented by numbers from 1 to 39. Therefore, φy
was modeled as a multinomial distribution.

Y ∈ {1, 2, . . . , 39}
Y ∼ φy(Multinomial)

(1)
(2)

The latitude and longitude data were not used for classiﬁ-
cation, and all the remaining features are categorical variables.
Thus, our feature vector, X, is a 7-dimensional vector. Each
of the features takes a range of values: concretely, Month
∈ {1, 2, . . . , 12}, Day of Week ∈ {1, 2, . . . , 7}, and so on.
Therefore, each feature is modeled by a multinomial distribu-
tion:

Xi ∈ {1, 2, . . . , ki}

Xi|{Y = j} ∼ φi|y=j(Multinomial)

(3)
(4)

Assuming that there are m training examples, the parame-
ters {φy, φi|y=j} are estimated using the following (Laplace-
smoothed) equations:

φy(j) = P{Y = j} =

(cid:80)m
i=1 1{y(i) = j} + 1

m + 39

(5)

(6)

(7)

φj|y=l(k) = P{Xj = k|Y = l}

(cid:80)m
(cid:80)m
i=1 1{x(i)
i=1 1{y(i) = l} + kj

j = k ∧ y(i) = l} + 1

=

B. Random Forests

Random Forests is a very popular ensemble learning method
which builds a number of classiﬁers on the training data and
combines all their outputs to make the best predictions on the
test data. Thus, the Random Forests algorithm is a variance
minimizing algorithm that uses randomness when making split
decision to help avoid overﬁtting on the training data.
A random forests classiﬁer is an ensemble classiﬁer, which
aggregates a family of classiﬁers h(x|θ1), h(x|θ2), ..h(x|θk).
Each member of the family, h(x|θ), is a classiﬁcation tree and
k is the number of trees chosen from a model random vector.

Crime Prediction and Classiﬁcation in San Francisco City

Addarsh Chandrasekar, Abhilash Sunder Raj and Poorna Kumar

Abstract— To be better prepared to respond to criminal activity,
it is important to understand patterns in crime. In our project, we
analyze crime data from the city of San Francisco, drawn from
a publicly available dataset. At the outset, the task is to predict
which category of crime is most likely to occur given a time and
place in San Francisco. To overcome the limitations imposed by our
limited set of features, we enrich our data by adding information
from the United States Census to it. We also attempt to make our
classiﬁcation task more meaningful by merging multiple classes
into larger classes. Finally, we report and reﬂect on our results
with different classiﬁers, and dwell on avenues for future work.

I. INTRODUCTION

Many important questions in public safety and protection
relate to crime, and a better understanding of crime is beneﬁcial
in multiple ways: it can lead to targeted and sensitive practices
by law enforcement authorities to mitigate crime, and more
concerted efforts by citizens and authorities to create healthy
neighborhood environments. With the advent of the Big Data
era and the availability of fast, efﬁcient algorithms for data
analysis, understanding patterns in crime from data is an active
and growing ﬁeld of research.

In our project, we use spatio-temporal and demographic
data to predict which category of crime is most
likely to
have occurred, given a time, place and the demographics of
the place. The inputs to our algorithms are time (hour, day,
month, year), place (latitude, longitude, and police district),
and demographic data (population, median income, minority
population, and number of families, which we get from the
United States Census). The output is the category of crime that
is likely to have occurred. We try out multiple classiﬁcation
algorithms, such as Naive Bayes, Support Vector Machines,
Gradient Boosted Decision Trees, and Random Forests. We also
perform multiple classiﬁcation tasks – we ﬁrst try to predict
which of 39 classes of crimes are likely to have occurred, and
later try to differentiate between blue- and white-collar crimes,
as well as violent and non-violent crimes.

II. RELATED WORK

Much of the current work is focused in two major directions:
(i) predicting surges and hotspots of crime, and (ii) understand-
ing patterns of criminal behavior that could help in solving
criminal investigations.

Important contributions towards the former include [1] by
Bogomolov et al, who try to predict whether any particular area
in London will be a crime hotspot or not, using anonymized
behavioural data from mobile networks as well as demographic
data. In [2], Chung-Hsien Yu et al use classiﬁcation techniques
to classify neighbourhoods in a city as hotspots of residential
burglary, using a variety of classiﬁcation algorithms such as
Support Vector Machines, Naive Bayes, and Neural Networks.
(More work on the usefulness of Support Vector Machines for
hotspot detection can be found in [3]). Toole et al demonstrated
in [4], by analyzing crime records for the city of Philadelphia,
that signiﬁcant spatio-temporal correlations exist in crime data,
and they were able to identify clusters of neighbourhoods whose
crime rates were affected simultaneously by external forces.

They also noted signiﬁcant correlations in crime across weekly
time scales.

Towards the second objective of understanding patterns of
criminal behaviour, signiﬁcant contributions have been made
by Tong Wang et al in [5], in ﬁnding patterns in criminal
activity and identifying individuals or groups of individuals who
might have committed particular crimes. Their approach was to
identify a common modus operandi across crimes, which could
then be linked to groups or individuals who might commit the
crime. For this, the authors proposed a new machine learning
method called Series Finder, which was trained to recognize
patterns in housebreak incidents in Cambridge, Massachusetts.
Our approach shares certain similarities with some of the
work described above,
in that we use spatio-temporal and
demographic information to discover which types of crimes are
likely to have occurred. However, we are notably different in
that, given the data, we seek to predict which category of crime
is most likely to occur, and we are hence concerned principally
with understanding the differences between different types of
crime, which is relatively unexplored territory.

III. OUR DATASET

Our dataset is a publicly available dataset that we obtained
from Kaggle, which has information about 878,049 crimes
that took place in San Francisco city over a span of nearly
twelve years. Each crime is labeled as belonging to one of 39
categories.

A. Features

Every entry in our training data set is about a particular crime,

and contains the following information:

• Date and timestamp of the incident.
• Day of the week that the crime occurred.
• Name of the Police Department District.
• Address:
incident.
• Latitude.
• Longitude.
• Category: category of the crime incident. This is the target

the approximate street address of the crime

variable.

• Description: a brief note describing any pertinent details of
the crime. (This was not used as a feature in our classiﬁers.)
• Resolution: whether the crime was resolved (with the
perpetrator being, say, arrested or booked) or not. (This
was also not used as a feature in our classiﬁers.)

TABLE I: Some sample rows from our dataset

Category

Dates
13-05-2015 23:53 WARRANTS
13-05-2015 23:53
13-05-2015 23:33
13-05-2015 23:30

OTHER OFFENSES
OTHER OFFENSES
LARCENY

DayOfWeek
Descript
WARRANTS
Wednesday
TRAFFIC VIOLATION Wednesday
TRAFFIC VIOLATION Wednesday
GRAND THEFT
Wednesday

PdDistrict
NORTHERN
NORTHERN
NORTHERN
NORTHERN

Resolution
ARREST, BOOKED
ARREST, BOOKED
ARREST, BOOKED
NONE

Address
OAK ST / LAGUNA ST
OAK ST / LAGUNA ST
VANNESS AV / GREENWICH ST
1500 Block of LOMBARD ST

X
-122.4258917
-122.4258917
-122.424363
-122.4269953

Y
37.7745986
37.7745986
37.80041432
37.80087263

B. Preprocessing

Before implementing machine learning algorithms on our
data, we went through a series of preprocessing steps with our
classiﬁcation task in mind. These included:

• Dropping features such as Resolution, Description and
Address: The resolution and description of a crime are
only known once the crime has occurred, and have limited
signiﬁcance in a practical, real-world scenario where one is
trying to predict what kind of crime has occurred, and so,
these were omitted. The address was dropped because we
had information about the latitude and longitude, and, in
that context, the address did not add much marginal value.
• The days of the week, police categories and crime cate-

gories were indexed and replaced by numbers.

• The timestamp contained the year, date and time of oc-
currence of each crime. This was decomposed into ﬁve
features: Year (2003-2015), Month (1-12), Date (1-31),
Hour (0-23) and Minute (0-59).

Following these preprocessing steps, we ran some out-of-the-
box learning algorithms as a part of our initial exploratory steps.
Our new feature set consisted of 7 features, all of which were
now numeric in nature.

C. Feature Enrichment

As we plunged into solving our classiﬁcation problem, we
felt that our feature set was not adequate enough in terms of
the information it contained to predict crime. In order to improve
our feature set, we augmented our dataset with additional
features that we scraped from the United States Census data.
This included demographic data such as mean income level of
a neighbourhood, racial diversity and so on. We felt that the
addition of such information could improve our performance at
the task of crime prediction. The census dataset was matched
with our dataset using the location coordinates of the crime in
our original dataset, and increased our number of features to
19.

D. Collapsing Crime Categories

We also felt that the number of output classiﬁcation labels, i.e.
39, in the original dataset was too high for accurate prediction.
The labels were too ﬁne-grained, and we realized that several
of these crime categories were similar to one another, and could
therefore be collapsed into smaller classes for better prediction.
Further, such collapsing could be done in several ways. Two
speciﬁc ways we went ahead with this were:

• Blue Collar Crimes vs White Collar Crimes: Blue Collar
Crimes included crimes such as Larceny, Arson and Bur-
glary while White Collar crimes included crimes such as
Fraud, Forgery and Extortion.

• Violent vs Non-Violent Crimes: Violent crimes included
crimes such as Assault, Arson and Prostitution while Non-
Violent crimes included crimes such as Trafﬁc Violations
and Trespassing.

IV. METHODS

After the preprocessing described in the previous sections,
we had three different classiﬁcations problems to solve, which
we proceeded to attack with an assortment of classiﬁcation

TABLE II: Sample Rows From The Census Data.
Note: The column headers are as follows. TC: Tract Code,
TIL: Tract Income Level, TMFI: Tract Median Family Income,
MFI: Median Family Income, OOU: Owner Occupied Units,
FU: Family Units, Min.: Minority

TIL

TC
101.00 Moderate
102.00
103.00

Upper
Upper

Distressed
No
No
No

TMFI % MFI
69.03
154.01
136.32

$96,900
$96,900
$96,900

2015 TMFI
$66,890
$1,49,236
$1,32,094

2010 MFI
$64,886
$1,44,750
$1,28,125

Tract Pop.
3739
4143
3852

Tract Min. % Min. Pop.
53.92
18.95
39.25

2016
785
1512

OOU
260
943
505

1- to 4- FU
424
918
1282

algorithms. The following sections explain the models we used
in detail.

A. Naive Bayes

As part of our initial exploratory analysis, we implemented
a Naive Bayes classiﬁer based on a multi-variate event model
with Laplace smoothing. This is a multi-class classiﬁcation
problem: the target variable Y (crime category) can be one of
39 classes, represented by numbers from 1 to 39. Therefore, φy
was modeled as a multinomial distribution.

Y ∈ {1, 2, . . . , 39}
Y ∼ φy(Multinomial)

(1)
(2)

The latitude and longitude data were not used for classiﬁ-
cation, and all the remaining features are categorical variables.
Thus, our feature vector, X, is a 7-dimensional vector. Each
of the features takes a range of values: concretely, Month
∈ {1, 2, . . . , 12}, Day of Week ∈ {1, 2, . . . , 7}, and so on.
Therefore, each feature is modeled by a multinomial distribu-
tion:

Xi ∈ {1, 2, . . . , ki}

Xi|{Y = j} ∼ φi|y=j(Multinomial)

(3)
(4)

Assuming that there are m training examples, the parame-
ters {φy, φi|y=j} are estimated using the following (Laplace-
smoothed) equations:

φy(j) = P{Y = j} =

(cid:80)m
i=1 1{y(i) = j} + 1

m + 39

(5)

(6)

(7)

φj|y=l(k) = P{Xj = k|Y = l}

(cid:80)m
(cid:80)m
i=1 1{x(i)
i=1 1{y(i) = l} + kj

j = k ∧ y(i) = l} + 1

=

B. Random Forests

Random Forests is a very popular ensemble learning method
which builds a number of classiﬁers on the training data and
combines all their outputs to make the best predictions on the
test data. Thus, the Random Forests algorithm is a variance
minimizing algorithm that uses randomness when making split
decision to help avoid overﬁtting on the training data.
A random forests classiﬁer is an ensemble classiﬁer, which
aggregates a family of classiﬁers h(x|θ1), h(x|θ2), ..h(x|θk).
Each member of the family, h(x|θ), is a classiﬁcation tree and
k is the number of trees chosen from a model random vector.

Also, each θk is a randomly chosen parameter vector. If D(x, y)
denotes the training dataset, each classiﬁcation tree in the
ensemble is built using a different subset Dθk (x, y) ⊂ D(x, y)
of the training dataset. Thus, h(x|θk) is the kth classiﬁcation tree
which uses a subset of features xθk ⊂ x to build a classiﬁcation
model. Each tree then works like regular decision trees:
it
partitions the data based on the value of a particular feature
(which is selected randomly from the subset), until the data is
fully partitioned, or the maximum allowed depth is reached.

The ﬁnal output y is obtained by aggregating the results thus:

(cid:8) k(cid:88)

(I(h(x|θj) = p))}

y = argmaxp∈{h(x1)..h(xk)}

j=1
where I denotes the indicator function.

C. Support Vector Machines

We used Support Vector Machines for binary classiﬁcation
in the latter part of the project, where we worked on the
classiﬁcation problems with collapsed categories. We ran SVMs
using the Gaussian (RBF) kernel to map the original features
to a high-dimensional feature space:

K(x, z) = exp

(cid:19)

(cid:18)

2σ2

−||x − z||2
m(cid:88)

(cid:107)w(cid:107)2 + C

ξi

i=1

min
γ,w,b

1
2

The optimal margin classiﬁer with l1 regularization was used.

s.t

yi(wT φ(xi) + b) ≥ 1 − ξi
ξi ≥ 0, i = 1 . . . m

D. Gradient Boosted Decision Trees

M(cid:88)

Gradient Tree Boosting[6]

is another popular ensemble
method used for regression and classiﬁcation. Given a training
sample (x, y), the goal is to ﬁnd a function F ∗(x) that maps x to
y such that the expected value of some loss function Ψ(y, F (x))
is minimized. Boosting approximates F ∗(x) by the following
equation:

F (x) =

βmh(x; am)

(13)

m=0

where the functions h(x; am) (called “base learners”) are simple
functions of x with parameters a. Starting with F0(x), the
parameters βm and am are found in a “stage-wise” manner and
the function Fm(x) is updated as:

Fm(x) = Fm−1(x) + βmh(x; am)

(14)

In tree boosting, the base learner h(x; a) is an L-terminal node
regression tree. At each iteration m, a regression tree partitions
the x-space into L-disjoint regions {Rlm}L
l=1and predicts a
separate constant value in each one. The update rules for
calculating Fm(x) given Fm−1(x) are as follows:

(cid:20) dΨ(yi, F (xi))

(cid:21)

˜yim = −

dF (xi)

F (x)=Fm−1(x)

γlm = argminγ

(cid:88)

xi∈Rlm

Ψ(yi, Fm−1(xi) + γ)

(18)

Fm(x) = Fm−1(x) + ν.γlm1(x ∈ Rlm)

(19)

V. EXPERIMENTAL RESULTS

In this section, we detail the results of running the classiﬁers
we described in the previous section on our data, on both the
full dataset, and on collapsed categories.

(8)

A. Performance on our Original Dataset

With our original dataset, we ran two different

learning
algorithms, i.e, Naive Bayes and Random Forests classiﬁers,
to get an initial understanding of the quality of our feature set,
and the amount of predictability in the data.

The Naive Bayes Model was tested using cross validation,
i.e, 70 percent of the data was used for training and the rest for
testing purposes. We got the following results:

• The classiﬁer gave 30% accuracy on the training set and
25% accuracy on the cross validation set. Hence, both
training and cross validation error were very high.

• The above trend was observed even on varying the size of
the training set. The accuracy did not go above 30%, even
on the training set. Note that this is still signiﬁcantly better
than random guessing, since we have not 2, but 39, output
classes.

We also implemented Random Forests for our classiﬁcation
problem. This was done keeping in mind that most machine
learning algorithms work with numerical features and that our
features are almost all categorical
in nature. The Random
Forests classiﬁer works well with categorical features and does
not need any preprocessing. We got the following results:

• After building the model, we ran it on the training set
itself to get a training error of 5%, which initially looked
too good to be true.

• However, on performing cross validation on the training
data using 10 folds, we got a test error of 84%, which
was huge compared to our training error, indicating high
variance.

B. Performance on Collapsed Classes:

After the feature enriching process detailed above, where
we augmented our training data with data from the Census,
we ran 3 different algorithms on two separate classiﬁcations of
the data. We split our crime categories into Blue Collar/White
Collar Crimes in one case and Violent/Non-Violent Crimes
in the other. Since blue-collar crimes far outnumbered white-
collar crimes, and non-violent crimes far exceeded violent
crimes, we decided to duplicate the minority class while
training our classiﬁer. This penalized the classiﬁer more for
mislabeling a training example in the minority class,
than
for mislabeling a training example in the majority class,
during the training phase, and over-rode the tendency of the
classiﬁer to minimize its error by simply labeling all data as
belonging to the majority class. By doing this, we were able
to improve our precision and recall values on the minority
class. Using this classiﬁcation, we ran the following algorithms:

Random Forests:
For random forests algorithms, the parameters to tune are the
number of trees and the maximum depth of each tree. In order

(9)

(10)

(11)

(12)

(15)

(16)

(17)

¯ylm = meanxi∈Rlm (˜yim)

h(x;{Rlm}L

1 ) =

¯ylm.1(x ∈ Rlm)

L(cid:88)

l=1

Crime Prediction and Classiﬁcation in San Francisco City

Addarsh Chandrasekar, Abhilash Sunder Raj and Poorna Kumar

Abstract— To be better prepared to respond to criminal activity,
it is important to understand patterns in crime. In our project, we
analyze crime data from the city of San Francisco, drawn from
a publicly available dataset. At the outset, the task is to predict
which category of crime is most likely to occur given a time and
place in San Francisco. To overcome the limitations imposed by our
limited set of features, we enrich our data by adding information
from the United States Census to it. We also attempt to make our
classiﬁcation task more meaningful by merging multiple classes
into larger classes. Finally, we report and reﬂect on our results
with different classiﬁers, and dwell on avenues for future work.

I. INTRODUCTION

Many important questions in public safety and protection
relate to crime, and a better understanding of crime is beneﬁcial
in multiple ways: it can lead to targeted and sensitive practices
by law enforcement authorities to mitigate crime, and more
concerted efforts by citizens and authorities to create healthy
neighborhood environments. With the advent of the Big Data
era and the availability of fast, efﬁcient algorithms for data
analysis, understanding patterns in crime from data is an active
and growing ﬁeld of research.

In our project, we use spatio-temporal and demographic
data to predict which category of crime is most
likely to
have occurred, given a time, place and the demographics of
the place. The inputs to our algorithms are time (hour, day,
month, year), place (latitude, longitude, and police district),
and demographic data (population, median income, minority
population, and number of families, which we get from the
United States Census). The output is the category of crime that
is likely to have occurred. We try out multiple classiﬁcation
algorithms, such as Naive Bayes, Support Vector Machines,
Gradient Boosted Decision Trees, and Random Forests. We also
perform multiple classiﬁcation tasks – we ﬁrst try to predict
which of 39 classes of crimes are likely to have occurred, and
later try to differentiate between blue- and white-collar crimes,
as well as violent and non-violent crimes.

II. RELATED WORK

Much of the current work is focused in two major directions:
(i) predicting surges and hotspots of crime, and (ii) understand-
ing patterns of criminal behavior that could help in solving
criminal investigations.

Important contributions towards the former include [1] by
Bogomolov et al, who try to predict whether any particular area
in London will be a crime hotspot or not, using anonymized
behavioural data from mobile networks as well as demographic
data. In [2], Chung-Hsien Yu et al use classiﬁcation techniques
to classify neighbourhoods in a city as hotspots of residential
burglary, using a variety of classiﬁcation algorithms such as
Support Vector Machines, Naive Bayes, and Neural Networks.
(More work on the usefulness of Support Vector Machines for
hotspot detection can be found in [3]). Toole et al demonstrated
in [4], by analyzing crime records for the city of Philadelphia,
that signiﬁcant spatio-temporal correlations exist in crime data,
and they were able to identify clusters of neighbourhoods whose
crime rates were affected simultaneously by external forces.

They also noted signiﬁcant correlations in crime across weekly
time scales.

Towards the second objective of understanding patterns of
criminal behaviour, signiﬁcant contributions have been made
by Tong Wang et al in [5], in ﬁnding patterns in criminal
activity and identifying individuals or groups of individuals who
might have committed particular crimes. Their approach was to
identify a common modus operandi across crimes, which could
then be linked to groups or individuals who might commit the
crime. For this, the authors proposed a new machine learning
method called Series Finder, which was trained to recognize
patterns in housebreak incidents in Cambridge, Massachusetts.
Our approach shares certain similarities with some of the
work described above,
in that we use spatio-temporal and
demographic information to discover which types of crimes are
likely to have occurred. However, we are notably different in
that, given the data, we seek to predict which category of crime
is most likely to occur, and we are hence concerned principally
with understanding the differences between different types of
crime, which is relatively unexplored territory.

III. OUR DATASET

Our dataset is a publicly available dataset that we obtained
from Kaggle, which has information about 878,049 crimes
that took place in San Francisco city over a span of nearly
twelve years. Each crime is labeled as belonging to one of 39
categories.

A. Features

Every entry in our training data set is about a particular crime,

and contains the following information:

• Date and timestamp of the incident.
• Day of the week that the crime occurred.
• Name of the Police Department District.
• Address:
incident.
• Latitude.
• Longitude.
• Category: category of the crime incident. This is the target

the approximate street address of the crime

variable.

• Description: a brief note describing any pertinent details of
the crime. (This was not used as a feature in our classiﬁers.)
• Resolution: whether the crime was resolved (with the
perpetrator being, say, arrested or booked) or not. (This
was also not used as a feature in our classiﬁers.)

TABLE I: Some sample rows from our dataset

Category

Dates
13-05-2015 23:53 WARRANTS
13-05-2015 23:53
13-05-2015 23:33
13-05-2015 23:30

OTHER OFFENSES
OTHER OFFENSES
LARCENY

DayOfWeek
Descript
WARRANTS
Wednesday
TRAFFIC VIOLATION Wednesday
TRAFFIC VIOLATION Wednesday
GRAND THEFT
Wednesday

PdDistrict
NORTHERN
NORTHERN
NORTHERN
NORTHERN

Resolution
ARREST, BOOKED
ARREST, BOOKED
ARREST, BOOKED
NONE

Address
OAK ST / LAGUNA ST
OAK ST / LAGUNA ST
VANNESS AV / GREENWICH ST
1500 Block of LOMBARD ST

X
-122.4258917
-122.4258917
-122.424363
-122.4269953

Y
37.7745986
37.7745986
37.80041432
37.80087263

B. Preprocessing

Before implementing machine learning algorithms on our
data, we went through a series of preprocessing steps with our
classiﬁcation task in mind. These included:

• Dropping features such as Resolution, Description and
Address: The resolution and description of a crime are
only known once the crime has occurred, and have limited
signiﬁcance in a practical, real-world scenario where one is
trying to predict what kind of crime has occurred, and so,
these were omitted. The address was dropped because we
had information about the latitude and longitude, and, in
that context, the address did not add much marginal value.
• The days of the week, police categories and crime cate-

gories were indexed and replaced by numbers.

• The timestamp contained the year, date and time of oc-
currence of each crime. This was decomposed into ﬁve
features: Year (2003-2015), Month (1-12), Date (1-31),
Hour (0-23) and Minute (0-59).

Following these preprocessing steps, we ran some out-of-the-
box learning algorithms as a part of our initial exploratory steps.
Our new feature set consisted of 7 features, all of which were
now numeric in nature.

C. Feature Enrichment

As we plunged into solving our classiﬁcation problem, we
felt that our feature set was not adequate enough in terms of
the information it contained to predict crime. In order to improve
our feature set, we augmented our dataset with additional
features that we scraped from the United States Census data.
This included demographic data such as mean income level of
a neighbourhood, racial diversity and so on. We felt that the
addition of such information could improve our performance at
the task of crime prediction. The census dataset was matched
with our dataset using the location coordinates of the crime in
our original dataset, and increased our number of features to
19.

D. Collapsing Crime Categories

We also felt that the number of output classiﬁcation labels, i.e.
39, in the original dataset was too high for accurate prediction.
The labels were too ﬁne-grained, and we realized that several
of these crime categories were similar to one another, and could
therefore be collapsed into smaller classes for better prediction.
Further, such collapsing could be done in several ways. Two
speciﬁc ways we went ahead with this were:

• Blue Collar Crimes vs White Collar Crimes: Blue Collar
Crimes included crimes such as Larceny, Arson and Bur-
glary while White Collar crimes included crimes such as
Fraud, Forgery and Extortion.

• Violent vs Non-Violent Crimes: Violent crimes included
crimes such as Assault, Arson and Prostitution while Non-
Violent crimes included crimes such as Trafﬁc Violations
and Trespassing.

IV. METHODS

After the preprocessing described in the previous sections,
we had three different classiﬁcations problems to solve, which
we proceeded to attack with an assortment of classiﬁcation

TABLE II: Sample Rows From The Census Data.
Note: The column headers are as follows. TC: Tract Code,
TIL: Tract Income Level, TMFI: Tract Median Family Income,
MFI: Median Family Income, OOU: Owner Occupied Units,
FU: Family Units, Min.: Minority

TIL

TC
101.00 Moderate
102.00
103.00

Upper
Upper

Distressed
No
No
No

TMFI % MFI
69.03
154.01
136.32

$96,900
$96,900
$96,900

2015 TMFI
$66,890
$1,49,236
$1,32,094

2010 MFI
$64,886
$1,44,750
$1,28,125

Tract Pop.
3739
4143
3852

Tract Min. % Min. Pop.
53.92
18.95
39.25

2016
785
1512

OOU
260
943
505

1- to 4- FU
424
918
1282

algorithms. The following sections explain the models we used
in detail.

A. Naive Bayes

As part of our initial exploratory analysis, we implemented
a Naive Bayes classiﬁer based on a multi-variate event model
with Laplace smoothing. This is a multi-class classiﬁcation
problem: the target variable Y (crime category) can be one of
39 classes, represented by numbers from 1 to 39. Therefore, φy
was modeled as a multinomial distribution.

Y ∈ {1, 2, . . . , 39}
Y ∼ φy(Multinomial)

(1)
(2)

The latitude and longitude data were not used for classiﬁ-
cation, and all the remaining features are categorical variables.
Thus, our feature vector, X, is a 7-dimensional vector. Each
of the features takes a range of values: concretely, Month
∈ {1, 2, . . . , 12}, Day of Week ∈ {1, 2, . . . , 7}, and so on.
Therefore, each feature is modeled by a multinomial distribu-
tion:

Xi ∈ {1, 2, . . . , ki}

Xi|{Y = j} ∼ φi|y=j(Multinomial)

(3)
(4)

Assuming that there are m training examples, the parame-
ters {φy, φi|y=j} are estimated using the following (Laplace-
smoothed) equations:

φy(j) = P{Y = j} =

(cid:80)m
i=1 1{y(i) = j} + 1

m + 39

(5)

(6)

(7)

φj|y=l(k) = P{Xj = k|Y = l}

(cid:80)m
(cid:80)m
i=1 1{x(i)
i=1 1{y(i) = l} + kj

j = k ∧ y(i) = l} + 1

=

B. Random Forests

Random Forests is a very popular ensemble learning method
which builds a number of classiﬁers on the training data and
combines all their outputs to make the best predictions on the
test data. Thus, the Random Forests algorithm is a variance
minimizing algorithm that uses randomness when making split
decision to help avoid overﬁtting on the training data.
A random forests classiﬁer is an ensemble classiﬁer, which
aggregates a family of classiﬁers h(x|θ1), h(x|θ2), ..h(x|θk).
Each member of the family, h(x|θ), is a classiﬁcation tree and
k is the number of trees chosen from a model random vector.

Also, each θk is a randomly chosen parameter vector. If D(x, y)
denotes the training dataset, each classiﬁcation tree in the
ensemble is built using a different subset Dθk (x, y) ⊂ D(x, y)
of the training dataset. Thus, h(x|θk) is the kth classiﬁcation tree
which uses a subset of features xθk ⊂ x to build a classiﬁcation
model. Each tree then works like regular decision trees:
it
partitions the data based on the value of a particular feature
(which is selected randomly from the subset), until the data is
fully partitioned, or the maximum allowed depth is reached.

The ﬁnal output y is obtained by aggregating the results thus:

(cid:8) k(cid:88)

(I(h(x|θj) = p))}

y = argmaxp∈{h(x1)..h(xk)}

j=1
where I denotes the indicator function.

C. Support Vector Machines

We used Support Vector Machines for binary classiﬁcation
in the latter part of the project, where we worked on the
classiﬁcation problems with collapsed categories. We ran SVMs
using the Gaussian (RBF) kernel to map the original features
to a high-dimensional feature space:

K(x, z) = exp

(cid:19)

(cid:18)

2σ2

−||x − z||2
m(cid:88)

(cid:107)w(cid:107)2 + C

ξi

i=1

min
γ,w,b

1
2

The optimal margin classiﬁer with l1 regularization was used.

s.t

yi(wT φ(xi) + b) ≥ 1 − ξi
ξi ≥ 0, i = 1 . . . m

D. Gradient Boosted Decision Trees

M(cid:88)

Gradient Tree Boosting[6]

is another popular ensemble
method used for regression and classiﬁcation. Given a training
sample (x, y), the goal is to ﬁnd a function F ∗(x) that maps x to
y such that the expected value of some loss function Ψ(y, F (x))
is minimized. Boosting approximates F ∗(x) by the following
equation:

F (x) =

βmh(x; am)

(13)

m=0

where the functions h(x; am) (called “base learners”) are simple
functions of x with parameters a. Starting with F0(x), the
parameters βm and am are found in a “stage-wise” manner and
the function Fm(x) is updated as:

Fm(x) = Fm−1(x) + βmh(x; am)

(14)

In tree boosting, the base learner h(x; a) is an L-terminal node
regression tree. At each iteration m, a regression tree partitions
the x-space into L-disjoint regions {Rlm}L
l=1and predicts a
separate constant value in each one. The update rules for
calculating Fm(x) given Fm−1(x) are as follows:

(cid:20) dΨ(yi, F (xi))

(cid:21)

˜yim = −

dF (xi)

F (x)=Fm−1(x)

γlm = argminγ

(cid:88)

xi∈Rlm

Ψ(yi, Fm−1(xi) + γ)

(18)

Fm(x) = Fm−1(x) + ν.γlm1(x ∈ Rlm)

(19)

V. EXPERIMENTAL RESULTS

In this section, we detail the results of running the classiﬁers
we described in the previous section on our data, on both the
full dataset, and on collapsed categories.

(8)

A. Performance on our Original Dataset

With our original dataset, we ran two different

learning
algorithms, i.e, Naive Bayes and Random Forests classiﬁers,
to get an initial understanding of the quality of our feature set,
and the amount of predictability in the data.

The Naive Bayes Model was tested using cross validation,
i.e, 70 percent of the data was used for training and the rest for
testing purposes. We got the following results:

• The classiﬁer gave 30% accuracy on the training set and
25% accuracy on the cross validation set. Hence, both
training and cross validation error were very high.

• The above trend was observed even on varying the size of
the training set. The accuracy did not go above 30%, even
on the training set. Note that this is still signiﬁcantly better
than random guessing, since we have not 2, but 39, output
classes.

We also implemented Random Forests for our classiﬁcation
problem. This was done keeping in mind that most machine
learning algorithms work with numerical features and that our
features are almost all categorical
in nature. The Random
Forests classiﬁer works well with categorical features and does
not need any preprocessing. We got the following results:

• After building the model, we ran it on the training set
itself to get a training error of 5%, which initially looked
too good to be true.

• However, on performing cross validation on the training
data using 10 folds, we got a test error of 84%, which
was huge compared to our training error, indicating high
variance.

B. Performance on Collapsed Classes:

After the feature enriching process detailed above, where
we augmented our training data with data from the Census,
we ran 3 different algorithms on two separate classiﬁcations of
the data. We split our crime categories into Blue Collar/White
Collar Crimes in one case and Violent/Non-Violent Crimes
in the other. Since blue-collar crimes far outnumbered white-
collar crimes, and non-violent crimes far exceeded violent
crimes, we decided to duplicate the minority class while
training our classiﬁer. This penalized the classiﬁer more for
mislabeling a training example in the minority class,
than
for mislabeling a training example in the majority class,
during the training phase, and over-rode the tendency of the
classiﬁer to minimize its error by simply labeling all data as
belonging to the majority class. By doing this, we were able
to improve our precision and recall values on the minority
class. Using this classiﬁcation, we ran the following algorithms:

Random Forests:
For random forests algorithms, the parameters to tune are the
number of trees and the maximum depth of each tree. In order

(9)

(10)

(11)

(12)

(15)

(16)

(17)

¯ylm = meanxi∈Rlm (˜yim)

h(x;{Rlm}L

1 ) =

¯ylm.1(x ∈ Rlm)

L(cid:88)

l=1

to pick optimum values for these, we tested the algorithm on
the data for different combinations of the parameters. Finally,
we picked the set of parameters that gave not only the overall
highest accuracy but also the highest precision and recall values
for both crime classes. The graphs below show the variation
of accuracy, precision and recall values for parameter values
for Blue/White Crime classiﬁcation. From this, the maximum
accuracy obtained was 79.18% for number of trees = 200 and
maximum depth = 15. Thus, random forests worked fairly well
on this problem, especially for blue collar crimes.

show the variation of accuracy, precision and recall values for
parameter values. From this, the maximum accuracy obtained
was 96.3% for number of estimators = 200 and maximum depth
= 13.

TABLE V: Precision and recall for gradient boosted trees on
Blue Collar/White Collar crime classiﬁcation

Precision Blue
0.963603527

Precision White
0.971364318

Recall Blue
0.996785378

Recall White
0.743345571

TABLE III: Precision and recall for random forests on Blue
Collar/White Collar crime classiﬁcation

Precision Blue
0.869700021

Precision White
0.163540533

Recall Blue
0.741358669

Recall White
0.312845342

The graphs below show the variation of accuracy, precision and
recall values for various conﬁgurations of parameter values for
Violent/Non-Violent Crime classiﬁcation. From this, the maxi-
mum accuracy obtained was 75.02% for number of estimators
= 200 and maximum depth = 11.

The graphs below show the variation of accuracy, precision
and recall values for parameter values for Violent/Non-Violent
Crime classiﬁcation. From this, the maximum accuracy obtained
was 61.75% for number of trees = 200 and maximum depth =
15.

TABLE IV: Precision and recall
for
Violent/Non-Violent crime classiﬁcation

random forests on

Precision Violent
0.589660627

Recall Violent
0.598377214

Precision Non Violent
0.762333658

Recall Non Violent
0.400292642

Gradient Boosted Decision Trees:
For the gradient boosted trees algorithm, the parameters of
interest are the number of trees and the maximum depth of
each tree. Once again, we tested the algorithm on the data for
different permutations of the parameters. Finally, we picked the
set of parameters that gave not only the overall highest accuracy
but also the highest precision and recall values for both crime
classes for Blue/White Crime classiﬁcation. The graphs below

TABLE VI: Precision and recall for gradient boosted trees on
Violent/Non-Violent crime classiﬁcation

Precision Violent
0.804631244

Recall Violent
0.899938198

Precision Non Violent
0.362965695

Recall Non Violent
0.206925557

Support Vector Machines:
We used Support Vector Machine classiﬁer with an “RBF”
kernel as our ﬁnal algorithm. The two parameters of interest
for the classiﬁer are the c and γ values. Once again, we
tested the algorithm on the data for different permutations of
the parameters and we picked the set of parameters that gave
the highest accuracy. The graphs below show the variation
of accuracy, precision and recall values for parameter values
for Blue/White Crime Classiﬁcation. From this, the maximum
accuracy obtained was 96% for c = 4 and γ = 0.2.
The graphs below show the variation of accuracy, precision
and recall values for parameter values for Violent/Non-Violent
Crime classiﬁcation. From this, the maximum accuracy obtained
was 62.80% for c = 1.1 and γ = 0.01.

Crime Prediction and Classiﬁcation in San Francisco City

Addarsh Chandrasekar, Abhilash Sunder Raj and Poorna Kumar

Abstract— To be better prepared to respond to criminal activity,
it is important to understand patterns in crime. In our project, we
analyze crime data from the city of San Francisco, drawn from
a publicly available dataset. At the outset, the task is to predict
which category of crime is most likely to occur given a time and
place in San Francisco. To overcome the limitations imposed by our
limited set of features, we enrich our data by adding information
from the United States Census to it. We also attempt to make our
classiﬁcation task more meaningful by merging multiple classes
into larger classes. Finally, we report and reﬂect on our results
with different classiﬁers, and dwell on avenues for future work.

I. INTRODUCTION

Many important questions in public safety and protection
relate to crime, and a better understanding of crime is beneﬁcial
in multiple ways: it can lead to targeted and sensitive practices
by law enforcement authorities to mitigate crime, and more
concerted efforts by citizens and authorities to create healthy
neighborhood environments. With the advent of the Big Data
era and the availability of fast, efﬁcient algorithms for data
analysis, understanding patterns in crime from data is an active
and growing ﬁeld of research.

In our project, we use spatio-temporal and demographic
data to predict which category of crime is most
likely to
have occurred, given a time, place and the demographics of
the place. The inputs to our algorithms are time (hour, day,
month, year), place (latitude, longitude, and police district),
and demographic data (population, median income, minority
population, and number of families, which we get from the
United States Census). The output is the category of crime that
is likely to have occurred. We try out multiple classiﬁcation
algorithms, such as Naive Bayes, Support Vector Machines,
Gradient Boosted Decision Trees, and Random Forests. We also
perform multiple classiﬁcation tasks – we ﬁrst try to predict
which of 39 classes of crimes are likely to have occurred, and
later try to differentiate between blue- and white-collar crimes,
as well as violent and non-violent crimes.

II. RELATED WORK

Much of the current work is focused in two major directions:
(i) predicting surges and hotspots of crime, and (ii) understand-
ing patterns of criminal behavior that could help in solving
criminal investigations.

Important contributions towards the former include [1] by
Bogomolov et al, who try to predict whether any particular area
in London will be a crime hotspot or not, using anonymized
behavioural data from mobile networks as well as demographic
data. In [2], Chung-Hsien Yu et al use classiﬁcation techniques
to classify neighbourhoods in a city as hotspots of residential
burglary, using a variety of classiﬁcation algorithms such as
Support Vector Machines, Naive Bayes, and Neural Networks.
(More work on the usefulness of Support Vector Machines for
hotspot detection can be found in [3]). Toole et al demonstrated
in [4], by analyzing crime records for the city of Philadelphia,
that signiﬁcant spatio-temporal correlations exist in crime data,
and they were able to identify clusters of neighbourhoods whose
crime rates were affected simultaneously by external forces.

They also noted signiﬁcant correlations in crime across weekly
time scales.

Towards the second objective of understanding patterns of
criminal behaviour, signiﬁcant contributions have been made
by Tong Wang et al in [5], in ﬁnding patterns in criminal
activity and identifying individuals or groups of individuals who
might have committed particular crimes. Their approach was to
identify a common modus operandi across crimes, which could
then be linked to groups or individuals who might commit the
crime. For this, the authors proposed a new machine learning
method called Series Finder, which was trained to recognize
patterns in housebreak incidents in Cambridge, Massachusetts.
Our approach shares certain similarities with some of the
work described above,
in that we use spatio-temporal and
demographic information to discover which types of crimes are
likely to have occurred. However, we are notably different in
that, given the data, we seek to predict which category of crime
is most likely to occur, and we are hence concerned principally
with understanding the differences between different types of
crime, which is relatively unexplored territory.

III. OUR DATASET

Our dataset is a publicly available dataset that we obtained
from Kaggle, which has information about 878,049 crimes
that took place in San Francisco city over a span of nearly
twelve years. Each crime is labeled as belonging to one of 39
categories.

A. Features

Every entry in our training data set is about a particular crime,

and contains the following information:

• Date and timestamp of the incident.
• Day of the week that the crime occurred.
• Name of the Police Department District.
• Address:
incident.
• Latitude.
• Longitude.
• Category: category of the crime incident. This is the target

the approximate street address of the crime

variable.

• Description: a brief note describing any pertinent details of
the crime. (This was not used as a feature in our classiﬁers.)
• Resolution: whether the crime was resolved (with the
perpetrator being, say, arrested or booked) or not. (This
was also not used as a feature in our classiﬁers.)

TABLE I: Some sample rows from our dataset

Category

Dates
13-05-2015 23:53 WARRANTS
13-05-2015 23:53
13-05-2015 23:33
13-05-2015 23:30

OTHER OFFENSES
OTHER OFFENSES
LARCENY

DayOfWeek
Descript
WARRANTS
Wednesday
TRAFFIC VIOLATION Wednesday
TRAFFIC VIOLATION Wednesday
GRAND THEFT
Wednesday

PdDistrict
NORTHERN
NORTHERN
NORTHERN
NORTHERN

Resolution
ARREST, BOOKED
ARREST, BOOKED
ARREST, BOOKED
NONE

Address
OAK ST / LAGUNA ST
OAK ST / LAGUNA ST
VANNESS AV / GREENWICH ST
1500 Block of LOMBARD ST

X
-122.4258917
-122.4258917
-122.424363
-122.4269953

Y
37.7745986
37.7745986
37.80041432
37.80087263

B. Preprocessing

Before implementing machine learning algorithms on our
data, we went through a series of preprocessing steps with our
classiﬁcation task in mind. These included:

• Dropping features such as Resolution, Description and
Address: The resolution and description of a crime are
only known once the crime has occurred, and have limited
signiﬁcance in a practical, real-world scenario where one is
trying to predict what kind of crime has occurred, and so,
these were omitted. The address was dropped because we
had information about the latitude and longitude, and, in
that context, the address did not add much marginal value.
• The days of the week, police categories and crime cate-

gories were indexed and replaced by numbers.

• The timestamp contained the year, date and time of oc-
currence of each crime. This was decomposed into ﬁve
features: Year (2003-2015), Month (1-12), Date (1-31),
Hour (0-23) and Minute (0-59).

Following these preprocessing steps, we ran some out-of-the-
box learning algorithms as a part of our initial exploratory steps.
Our new feature set consisted of 7 features, all of which were
now numeric in nature.

C. Feature Enrichment

As we plunged into solving our classiﬁcation problem, we
felt that our feature set was not adequate enough in terms of
the information it contained to predict crime. In order to improve
our feature set, we augmented our dataset with additional
features that we scraped from the United States Census data.
This included demographic data such as mean income level of
a neighbourhood, racial diversity and so on. We felt that the
addition of such information could improve our performance at
the task of crime prediction. The census dataset was matched
with our dataset using the location coordinates of the crime in
our original dataset, and increased our number of features to
19.

D. Collapsing Crime Categories

We also felt that the number of output classiﬁcation labels, i.e.
39, in the original dataset was too high for accurate prediction.
The labels were too ﬁne-grained, and we realized that several
of these crime categories were similar to one another, and could
therefore be collapsed into smaller classes for better prediction.
Further, such collapsing could be done in several ways. Two
speciﬁc ways we went ahead with this were:

• Blue Collar Crimes vs White Collar Crimes: Blue Collar
Crimes included crimes such as Larceny, Arson and Bur-
glary while White Collar crimes included crimes such as
Fraud, Forgery and Extortion.

• Violent vs Non-Violent Crimes: Violent crimes included
crimes such as Assault, Arson and Prostitution while Non-
Violent crimes included crimes such as Trafﬁc Violations
and Trespassing.

IV. METHODS

After the preprocessing described in the previous sections,
we had three different classiﬁcations problems to solve, which
we proceeded to attack with an assortment of classiﬁcation

TABLE II: Sample Rows From The Census Data.
Note: The column headers are as follows. TC: Tract Code,
TIL: Tract Income Level, TMFI: Tract Median Family Income,
MFI: Median Family Income, OOU: Owner Occupied Units,
FU: Family Units, Min.: Minority

TIL

TC
101.00 Moderate
102.00
103.00

Upper
Upper

Distressed
No
No
No

TMFI % MFI
69.03
154.01
136.32

$96,900
$96,900
$96,900

2015 TMFI
$66,890
$1,49,236
$1,32,094

2010 MFI
$64,886
$1,44,750
$1,28,125

Tract Pop.
3739
4143
3852

Tract Min. % Min. Pop.
53.92
18.95
39.25

2016
785
1512

OOU
260
943
505

1- to 4- FU
424
918
1282

algorithms. The following sections explain the models we used
in detail.

A. Naive Bayes

As part of our initial exploratory analysis, we implemented
a Naive Bayes classiﬁer based on a multi-variate event model
with Laplace smoothing. This is a multi-class classiﬁcation
problem: the target variable Y (crime category) can be one of
39 classes, represented by numbers from 1 to 39. Therefore, φy
was modeled as a multinomial distribution.

Y ∈ {1, 2, . . . , 39}
Y ∼ φy(Multinomial)

(1)
(2)

The latitude and longitude data were not used for classiﬁ-
cation, and all the remaining features are categorical variables.
Thus, our feature vector, X, is a 7-dimensional vector. Each
of the features takes a range of values: concretely, Month
∈ {1, 2, . . . , 12}, Day of Week ∈ {1, 2, . . . , 7}, and so on.
Therefore, each feature is modeled by a multinomial distribu-
tion:

Xi ∈ {1, 2, . . . , ki}

Xi|{Y = j} ∼ φi|y=j(Multinomial)

(3)
(4)

Assuming that there are m training examples, the parame-
ters {φy, φi|y=j} are estimated using the following (Laplace-
smoothed) equations:

φy(j) = P{Y = j} =

(cid:80)m
i=1 1{y(i) = j} + 1

m + 39

(5)

(6)

(7)

φj|y=l(k) = P{Xj = k|Y = l}

(cid:80)m
(cid:80)m
i=1 1{x(i)
i=1 1{y(i) = l} + kj

j = k ∧ y(i) = l} + 1

=

B. Random Forests

Random Forests is a very popular ensemble learning method
which builds a number of classiﬁers on the training data and
combines all their outputs to make the best predictions on the
test data. Thus, the Random Forests algorithm is a variance
minimizing algorithm that uses randomness when making split
decision to help avoid overﬁtting on the training data.
A random forests classiﬁer is an ensemble classiﬁer, which
aggregates a family of classiﬁers h(x|θ1), h(x|θ2), ..h(x|θk).
Each member of the family, h(x|θ), is a classiﬁcation tree and
k is the number of trees chosen from a model random vector.

Also, each θk is a randomly chosen parameter vector. If D(x, y)
denotes the training dataset, each classiﬁcation tree in the
ensemble is built using a different subset Dθk (x, y) ⊂ D(x, y)
of the training dataset. Thus, h(x|θk) is the kth classiﬁcation tree
which uses a subset of features xθk ⊂ x to build a classiﬁcation
model. Each tree then works like regular decision trees:
it
partitions the data based on the value of a particular feature
(which is selected randomly from the subset), until the data is
fully partitioned, or the maximum allowed depth is reached.

The ﬁnal output y is obtained by aggregating the results thus:

(cid:8) k(cid:88)

(I(h(x|θj) = p))}

y = argmaxp∈{h(x1)..h(xk)}

j=1
where I denotes the indicator function.

C. Support Vector Machines

We used Support Vector Machines for binary classiﬁcation
in the latter part of the project, where we worked on the
classiﬁcation problems with collapsed categories. We ran SVMs
using the Gaussian (RBF) kernel to map the original features
to a high-dimensional feature space:

K(x, z) = exp

(cid:19)

(cid:18)

2σ2

−||x − z||2
m(cid:88)

(cid:107)w(cid:107)2 + C

ξi

i=1

min
γ,w,b

1
2

The optimal margin classiﬁer with l1 regularization was used.

s.t

yi(wT φ(xi) + b) ≥ 1 − ξi
ξi ≥ 0, i = 1 . . . m

D. Gradient Boosted Decision Trees

M(cid:88)

Gradient Tree Boosting[6]

is another popular ensemble
method used for regression and classiﬁcation. Given a training
sample (x, y), the goal is to ﬁnd a function F ∗(x) that maps x to
y such that the expected value of some loss function Ψ(y, F (x))
is minimized. Boosting approximates F ∗(x) by the following
equation:

F (x) =

βmh(x; am)

(13)

m=0

where the functions h(x; am) (called “base learners”) are simple
functions of x with parameters a. Starting with F0(x), the
parameters βm and am are found in a “stage-wise” manner and
the function Fm(x) is updated as:

Fm(x) = Fm−1(x) + βmh(x; am)

(14)

In tree boosting, the base learner h(x; a) is an L-terminal node
regression tree. At each iteration m, a regression tree partitions
the x-space into L-disjoint regions {Rlm}L
l=1and predicts a
separate constant value in each one. The update rules for
calculating Fm(x) given Fm−1(x) are as follows:

(cid:20) dΨ(yi, F (xi))

(cid:21)

˜yim = −

dF (xi)

F (x)=Fm−1(x)

γlm = argminγ

(cid:88)

xi∈Rlm

Ψ(yi, Fm−1(xi) + γ)

(18)

Fm(x) = Fm−1(x) + ν.γlm1(x ∈ Rlm)

(19)

V. EXPERIMENTAL RESULTS

In this section, we detail the results of running the classiﬁers
we described in the previous section on our data, on both the
full dataset, and on collapsed categories.

(8)

A. Performance on our Original Dataset

With our original dataset, we ran two different

learning
algorithms, i.e, Naive Bayes and Random Forests classiﬁers,
to get an initial understanding of the quality of our feature set,
and the amount of predictability in the data.

The Naive Bayes Model was tested using cross validation,
i.e, 70 percent of the data was used for training and the rest for
testing purposes. We got the following results:

• The classiﬁer gave 30% accuracy on the training set and
25% accuracy on the cross validation set. Hence, both
training and cross validation error were very high.

• The above trend was observed even on varying the size of
the training set. The accuracy did not go above 30%, even
on the training set. Note that this is still signiﬁcantly better
than random guessing, since we have not 2, but 39, output
classes.

We also implemented Random Forests for our classiﬁcation
problem. This was done keeping in mind that most machine
learning algorithms work with numerical features and that our
features are almost all categorical
in nature. The Random
Forests classiﬁer works well with categorical features and does
not need any preprocessing. We got the following results:

• After building the model, we ran it on the training set
itself to get a training error of 5%, which initially looked
too good to be true.

• However, on performing cross validation on the training
data using 10 folds, we got a test error of 84%, which
was huge compared to our training error, indicating high
variance.

B. Performance on Collapsed Classes:

After the feature enriching process detailed above, where
we augmented our training data with data from the Census,
we ran 3 different algorithms on two separate classiﬁcations of
the data. We split our crime categories into Blue Collar/White
Collar Crimes in one case and Violent/Non-Violent Crimes
in the other. Since blue-collar crimes far outnumbered white-
collar crimes, and non-violent crimes far exceeded violent
crimes, we decided to duplicate the minority class while
training our classiﬁer. This penalized the classiﬁer more for
mislabeling a training example in the minority class,
than
for mislabeling a training example in the majority class,
during the training phase, and over-rode the tendency of the
classiﬁer to minimize its error by simply labeling all data as
belonging to the majority class. By doing this, we were able
to improve our precision and recall values on the minority
class. Using this classiﬁcation, we ran the following algorithms:

Random Forests:
For random forests algorithms, the parameters to tune are the
number of trees and the maximum depth of each tree. In order

(9)

(10)

(11)

(12)

(15)

(16)

(17)

¯ylm = meanxi∈Rlm (˜yim)

h(x;{Rlm}L

1 ) =

¯ylm.1(x ∈ Rlm)

L(cid:88)

l=1

to pick optimum values for these, we tested the algorithm on
the data for different combinations of the parameters. Finally,
we picked the set of parameters that gave not only the overall
highest accuracy but also the highest precision and recall values
for both crime classes. The graphs below show the variation
of accuracy, precision and recall values for parameter values
for Blue/White Crime classiﬁcation. From this, the maximum
accuracy obtained was 79.18% for number of trees = 200 and
maximum depth = 15. Thus, random forests worked fairly well
on this problem, especially for blue collar crimes.

show the variation of accuracy, precision and recall values for
parameter values. From this, the maximum accuracy obtained
was 96.3% for number of estimators = 200 and maximum depth
= 13.

TABLE V: Precision and recall for gradient boosted trees on
Blue Collar/White Collar crime classiﬁcation

Precision Blue
0.963603527

Precision White
0.971364318

Recall Blue
0.996785378

Recall White
0.743345571

TABLE III: Precision and recall for random forests on Blue
Collar/White Collar crime classiﬁcation

Precision Blue
0.869700021

Precision White
0.163540533

Recall Blue
0.741358669

Recall White
0.312845342

The graphs below show the variation of accuracy, precision and
recall values for various conﬁgurations of parameter values for
Violent/Non-Violent Crime classiﬁcation. From this, the maxi-
mum accuracy obtained was 75.02% for number of estimators
= 200 and maximum depth = 11.

The graphs below show the variation of accuracy, precision
and recall values for parameter values for Violent/Non-Violent
Crime classiﬁcation. From this, the maximum accuracy obtained
was 61.75% for number of trees = 200 and maximum depth =
15.

TABLE IV: Precision and recall
for
Violent/Non-Violent crime classiﬁcation

random forests on

Precision Violent
0.589660627

Recall Violent
0.598377214

Precision Non Violent
0.762333658

Recall Non Violent
0.400292642

Gradient Boosted Decision Trees:
For the gradient boosted trees algorithm, the parameters of
interest are the number of trees and the maximum depth of
each tree. Once again, we tested the algorithm on the data for
different permutations of the parameters. Finally, we picked the
set of parameters that gave not only the overall highest accuracy
but also the highest precision and recall values for both crime
classes for Blue/White Crime classiﬁcation. The graphs below

TABLE VI: Precision and recall for gradient boosted trees on
Violent/Non-Violent crime classiﬁcation

Precision Violent
0.804631244

Recall Violent
0.899938198

Precision Non Violent
0.362965695

Recall Non Violent
0.206925557

Support Vector Machines:
We used Support Vector Machine classiﬁer with an “RBF”
kernel as our ﬁnal algorithm. The two parameters of interest
for the classiﬁer are the c and γ values. Once again, we
tested the algorithm on the data for different permutations of
the parameters and we picked the set of parameters that gave
the highest accuracy. The graphs below show the variation
of accuracy, precision and recall values for parameter values
for Blue/White Crime Classiﬁcation. From this, the maximum
accuracy obtained was 96% for c = 4 and γ = 0.2.
The graphs below show the variation of accuracy, precision
and recall values for parameter values for Violent/Non-Violent
Crime classiﬁcation. From this, the maximum accuracy obtained
was 62.80% for c = 1.1 and γ = 0.01.

VI. CONCLUSION AND FUTURE WORK

The initial problem of classifying 39 different crime
categories was a challenging multi-class classiﬁcation problem,
and there was not enough predictability in our initial data-set
to obtain very high accuracy on it. We found that a more
meaningful approach was to collapse the crime categories into
fewer, larger groups, in order to ﬁnd structure in the data. We
got high accuracy and precision on the blue-collar/white-collar
crime classiﬁcation problem using Gradient Boosted trees
and Support Vector Machines (the former famously robust
and the latter well-suited to a 2-class classiﬁcation problem,
especially with an RBF Kernel
that can translate the data
to a high-dimensional space where it is linearly separable).
However, the Violent/Non-violent crime classiﬁcation did not
yield remarkable results with the same classiﬁers – this was
a signiﬁcantly harder classiﬁcation problem. Thus, collapsing
crime categories is not an obvious task and requires careful
choice and consideration.

Possible avenues through which to extend this work include
time-series modeling of the data to understand temporal cor-
relations in it, which can then be used to predict surges in
different categories of crime. It would also be interesting to
explore relationships between surges in different categories of
crimes – for example, it could be the case that two or more
classes of crimes surge and sink together, which would be
an interesting relationship to uncover. Other areas to work on
include implementing a more accurate multi-class classiﬁer, and
exploring better ways to visualize our results.

ACKNOWLEDGMENTS

We would like to thank our project TA Youssef Ahrez for
his thoughtful feedback and helpful ideas at every stage of our
project. We also owe a debt of gratitude to Viswajith Venugopal,
a student of the Department of Computer Science, for helping us
to understand and implement parallel computing. And of course,
our acknowledgements section would be incomplete without a
mention of Professor Andrew Ng, whose excellent class enabled
us to do this project in the ﬁrst place.

REFERENCES

[1] Bogomolov, Andrey and Lepri, Bruno and Staiano,
Jacopo and Oliver, Nuria and Pianesi, Fabio and Pentland,
Alex.2014. Once upon a crime: Towards crime prediction
from demographics and mobile data, Proceedings of the 16th
International Conference on Multimodal Interaction.
[2] Yu, Chung-Hsien and Ward, Max W and Morabito, Melissa
and Ding, Wei.2011. Crime forecasting using data mining
techniques, pages 779-786, IEEE 11th International Conference
on Data Mining Workshops (ICDMW)
[3] Kianmehr, Keivan and Alhajj, Reda. 2008. Effectiveness of
support vector machine for crime hot-spots prediction, pages
433-458, Applied Artiﬁcial Intelligence, volume 22, number 5.
[4] Toole, Jameson L and Eagle, Nathan and Plotkin, Joshua B.
2011 (TIST), volume 2, number 4, pages 38, ACM Transactions
on Intelligent Systems and Technology
[5] Wang, Tong and Rudin, Cynthia and Wagner, Daniel and
Sevieri, Rich. 2013. pages 515-530, Machine Learning and
Knowledge Discovery in Databases
[6] Friedman,
Jerome H. ”Stochastic gradient boosting.”
Computational Statistics and Data Analysis 38.4 (2002):
367-378.sts

TABLE VII: Precision and recall for SVMs on Blue Col-
lar/White Collar crime classiﬁcation

Precision Blue
0.9602589

Precision White
0.998788205

Recall Blue
0.988629185

Recall White
0.718219367

TABLE VIII: Precision and recall for SVMs on Violent/Non-
Violent crime classiﬁcation

Precision Violent
0.823943997

Recall Violent
0.668314728

Precision Non Violent
0.285785578

Recall Non Violent
0.481704753

C. Feature selection

More important than just our accuracy and precision is the
interpretation of our model: which of our features actually help
predict the category of crime? Analyzing the feature impor-
tances of our model gave us interesting insights.The following
are the most relevant features used by our SVM (which we
identiﬁed by running forward search) for classiﬁcation, for
the case of Blue Collar/White Collar Crimes: Hour, Minority
Population, Day Of Week, Tract Income Level, Tract Code,
Police District Number, Tract Population, Day of Month

Crime Prediction and Classiﬁcation in San Francisco City

Addarsh Chandrasekar, Abhilash Sunder Raj and Poorna Kumar

Abstract— To be better prepared to respond to criminal activity,
it is important to understand patterns in crime. In our project, we
analyze crime data from the city of San Francisco, drawn from
a publicly available dataset. At the outset, the task is to predict
which category of crime is most likely to occur given a time and
place in San Francisco. To overcome the limitations imposed by our
limited set of features, we enrich our data by adding information
from the United States Census to it. We also attempt to make our
classiﬁcation task more meaningful by merging multiple classes
into larger classes. Finally, we report and reﬂect on our results
with different classiﬁers, and dwell on avenues for future work.

I. INTRODUCTION

Many important questions in public safety and protection
relate to crime, and a better understanding of crime is beneﬁcial
in multiple ways: it can lead to targeted and sensitive practices
by law enforcement authorities to mitigate crime, and more
concerted efforts by citizens and authorities to create healthy
neighborhood environments. With the advent of the Big Data
era and the availability of fast, efﬁcient algorithms for data
analysis, understanding patterns in crime from data is an active
and growing ﬁeld of research.

In our project, we use spatio-temporal and demographic
data to predict which category of crime is most
likely to
have occurred, given a time, place and the demographics of
the place. The inputs to our algorithms are time (hour, day,
month, year), place (latitude, longitude, and police district),
and demographic data (population, median income, minority
population, and number of families, which we get from the
United States Census). The output is the category of crime that
is likely to have occurred. We try out multiple classiﬁcation
algorithms, such as Naive Bayes, Support Vector Machines,
Gradient Boosted Decision Trees, and Random Forests. We also
perform multiple classiﬁcation tasks – we ﬁrst try to predict
which of 39 classes of crimes are likely to have occurred, and
later try to differentiate between blue- and white-collar crimes,
as well as violent and non-violent crimes.

II. RELATED WORK

Much of the current work is focused in two major directions:
(i) predicting surges and hotspots of crime, and (ii) understand-
ing patterns of criminal behavior that could help in solving
criminal investigations.

Important contributions towards the former include [1] by
Bogomolov et al, who try to predict whether any particular area
in London will be a crime hotspot or not, using anonymized
behavioural data from mobile networks as well as demographic
data. In [2], Chung-Hsien Yu et al use classiﬁcation techniques
to classify neighbourhoods in a city as hotspots of residential
burglary, using a variety of classiﬁcation algorithms such as
Support Vector Machines, Naive Bayes, and Neural Networks.
(More work on the usefulness of Support Vector Machines for
hotspot detection can be found in [3]). Toole et al demonstrated
in [4], by analyzing crime records for the city of Philadelphia,
that signiﬁcant spatio-temporal correlations exist in crime data,
and they were able to identify clusters of neighbourhoods whose
crime rates were affected simultaneously by external forces.

They also noted signiﬁcant correlations in crime across weekly
time scales.

Towards the second objective of understanding patterns of
criminal behaviour, signiﬁcant contributions have been made
by Tong Wang et al in [5], in ﬁnding patterns in criminal
activity and identifying individuals or groups of individuals who
might have committed particular crimes. Their approach was to
identify a common modus operandi across crimes, which could
then be linked to groups or individuals who might commit the
crime. For this, the authors proposed a new machine learning
method called Series Finder, which was trained to recognize
patterns in housebreak incidents in Cambridge, Massachusetts.
Our approach shares certain similarities with some of the
work described above,
in that we use spatio-temporal and
demographic information to discover which types of crimes are
likely to have occurred. However, we are notably different in
that, given the data, we seek to predict which category of crime
is most likely to occur, and we are hence concerned principally
with understanding the differences between different types of
crime, which is relatively unexplored territory.

III. OUR DATASET

Our dataset is a publicly available dataset that we obtained
from Kaggle, which has information about 878,049 crimes
that took place in San Francisco city over a span of nearly
twelve years. Each crime is labeled as belonging to one of 39
categories.

A. Features

Every entry in our training data set is about a particular crime,

and contains the following information:

• Date and timestamp of the incident.
• Day of the week that the crime occurred.
• Name of the Police Department District.
• Address:
incident.
• Latitude.
• Longitude.
• Category: category of the crime incident. This is the target

the approximate street address of the crime

variable.

• Description: a brief note describing any pertinent details of
the crime. (This was not used as a feature in our classiﬁers.)
• Resolution: whether the crime was resolved (with the
perpetrator being, say, arrested or booked) or not. (This
was also not used as a feature in our classiﬁers.)

TABLE I: Some sample rows from our dataset

Category

Dates
13-05-2015 23:53 WARRANTS
13-05-2015 23:53
13-05-2015 23:33
13-05-2015 23:30

OTHER OFFENSES
OTHER OFFENSES
LARCENY

DayOfWeek
Descript
WARRANTS
Wednesday
TRAFFIC VIOLATION Wednesday
TRAFFIC VIOLATION Wednesday
GRAND THEFT
Wednesday

PdDistrict
NORTHERN
NORTHERN
NORTHERN
NORTHERN

Resolution
ARREST, BOOKED
ARREST, BOOKED
ARREST, BOOKED
NONE

Address
OAK ST / LAGUNA ST
OAK ST / LAGUNA ST
VANNESS AV / GREENWICH ST
1500 Block of LOMBARD ST

X
-122.4258917
-122.4258917
-122.424363
-122.4269953

Y
37.7745986
37.7745986
37.80041432
37.80087263

B. Preprocessing

Before implementing machine learning algorithms on our
data, we went through a series of preprocessing steps with our
classiﬁcation task in mind. These included:

• Dropping features such as Resolution, Description and
Address: The resolution and description of a crime are
only known once the crime has occurred, and have limited
signiﬁcance in a practical, real-world scenario where one is
trying to predict what kind of crime has occurred, and so,
these were omitted. The address was dropped because we
had information about the latitude and longitude, and, in
that context, the address did not add much marginal value.
• The days of the week, police categories and crime cate-

gories were indexed and replaced by numbers.

• The timestamp contained the year, date and time of oc-
currence of each crime. This was decomposed into ﬁve
features: Year (2003-2015), Month (1-12), Date (1-31),
Hour (0-23) and Minute (0-59).

Following these preprocessing steps, we ran some out-of-the-
box learning algorithms as a part of our initial exploratory steps.
Our new feature set consisted of 7 features, all of which were
now numeric in nature.

C. Feature Enrichment

As we plunged into solving our classiﬁcation problem, we
felt that our feature set was not adequate enough in terms of
the information it contained to predict crime. In order to improve
our feature set, we augmented our dataset with additional
features that we scraped from the United States Census data.
This included demographic data such as mean income level of
a neighbourhood, racial diversity and so on. We felt that the
addition of such information could improve our performance at
the task of crime prediction. The census dataset was matched
with our dataset using the location coordinates of the crime in
our original dataset, and increased our number of features to
19.

D. Collapsing Crime Categories

We also felt that the number of output classiﬁcation labels, i.e.
39, in the original dataset was too high for accurate prediction.
The labels were too ﬁne-grained, and we realized that several
of these crime categories were similar to one another, and could
therefore be collapsed into smaller classes for better prediction.
Further, such collapsing could be done in several ways. Two
speciﬁc ways we went ahead with this were:

• Blue Collar Crimes vs White Collar Crimes: Blue Collar
Crimes included crimes such as Larceny, Arson and Bur-
glary while White Collar crimes included crimes such as
Fraud, Forgery and Extortion.

• Violent vs Non-Violent Crimes: Violent crimes included
crimes such as Assault, Arson and Prostitution while Non-
Violent crimes included crimes such as Trafﬁc Violations
and Trespassing.

IV. METHODS

After the preprocessing described in the previous sections,
we had three different classiﬁcations problems to solve, which
we proceeded to attack with an assortment of classiﬁcation

TABLE II: Sample Rows From The Census Data.
Note: The column headers are as follows. TC: Tract Code,
TIL: Tract Income Level, TMFI: Tract Median Family Income,
MFI: Median Family Income, OOU: Owner Occupied Units,
FU: Family Units, Min.: Minority

TIL

TC
101.00 Moderate
102.00
103.00

Upper
Upper

Distressed
No
No
No

TMFI % MFI
69.03
154.01
136.32

$96,900
$96,900
$96,900

2015 TMFI
$66,890
$1,49,236
$1,32,094

2010 MFI
$64,886
$1,44,750
$1,28,125

Tract Pop.
3739
4143
3852

Tract Min. % Min. Pop.
53.92
18.95
39.25

2016
785
1512

OOU
260
943
505

1- to 4- FU
424
918
1282

algorithms. The following sections explain the models we used
in detail.

A. Naive Bayes

As part of our initial exploratory analysis, we implemented
a Naive Bayes classiﬁer based on a multi-variate event model
with Laplace smoothing. This is a multi-class classiﬁcation
problem: the target variable Y (crime category) can be one of
39 classes, represented by numbers from 1 to 39. Therefore, φy
was modeled as a multinomial distribution.

Y ∈ {1, 2, . . . , 39}
Y ∼ φy(Multinomial)

(1)
(2)

The latitude and longitude data were not used for classiﬁ-
cation, and all the remaining features are categorical variables.
Thus, our feature vector, X, is a 7-dimensional vector. Each
of the features takes a range of values: concretely, Month
∈ {1, 2, . . . , 12}, Day of Week ∈ {1, 2, . . . , 7}, and so on.
Therefore, each feature is modeled by a multinomial distribu-
tion:

Xi ∈ {1, 2, . . . , ki}

Xi|{Y = j} ∼ φi|y=j(Multinomial)

(3)
(4)

Assuming that there are m training examples, the parame-
ters {φy, φi|y=j} are estimated using the following (Laplace-
smoothed) equations:

φy(j) = P{Y = j} =

(cid:80)m
i=1 1{y(i) = j} + 1

m + 39

(5)

(6)

(7)

φj|y=l(k) = P{Xj = k|Y = l}

(cid:80)m
(cid:80)m
i=1 1{x(i)
i=1 1{y(i) = l} + kj

j = k ∧ y(i) = l} + 1

=

B. Random Forests

Random Forests is a very popular ensemble learning method
which builds a number of classiﬁers on the training data and
combines all their outputs to make the best predictions on the
test data. Thus, the Random Forests algorithm is a variance
minimizing algorithm that uses randomness when making split
decision to help avoid overﬁtting on the training data.
A random forests classiﬁer is an ensemble classiﬁer, which
aggregates a family of classiﬁers h(x|θ1), h(x|θ2), ..h(x|θk).
Each member of the family, h(x|θ), is a classiﬁcation tree and
k is the number of trees chosen from a model random vector.

Also, each θk is a randomly chosen parameter vector. If D(x, y)
denotes the training dataset, each classiﬁcation tree in the
ensemble is built using a different subset Dθk (x, y) ⊂ D(x, y)
of the training dataset. Thus, h(x|θk) is the kth classiﬁcation tree
which uses a subset of features xθk ⊂ x to build a classiﬁcation
model. Each tree then works like regular decision trees:
it
partitions the data based on the value of a particular feature
(which is selected randomly from the subset), until the data is
fully partitioned, or the maximum allowed depth is reached.

The ﬁnal output y is obtained by aggregating the results thus:

(cid:8) k(cid:88)

(I(h(x|θj) = p))}

y = argmaxp∈{h(x1)..h(xk)}

j=1
where I denotes the indicator function.

C. Support Vector Machines

We used Support Vector Machines for binary classiﬁcation
in the latter part of the project, where we worked on the
classiﬁcation problems with collapsed categories. We ran SVMs
using the Gaussian (RBF) kernel to map the original features
to a high-dimensional feature space:

K(x, z) = exp

(cid:19)

(cid:18)

2σ2

−||x − z||2
m(cid:88)

(cid:107)w(cid:107)2 + C

ξi

i=1

min
γ,w,b

1
2

The optimal margin classiﬁer with l1 regularization was used.

s.t

yi(wT φ(xi) + b) ≥ 1 − ξi
ξi ≥ 0, i = 1 . . . m

D. Gradient Boosted Decision Trees

M(cid:88)

Gradient Tree Boosting[6]

is another popular ensemble
method used for regression and classiﬁcation. Given a training
sample (x, y), the goal is to ﬁnd a function F ∗(x) that maps x to
y such that the expected value of some loss function Ψ(y, F (x))
is minimized. Boosting approximates F ∗(x) by the following
equation:

F (x) =

βmh(x; am)

(13)

m=0

where the functions h(x; am) (called “base learners”) are simple
functions of x with parameters a. Starting with F0(x), the
parameters βm and am are found in a “stage-wise” manner and
the function Fm(x) is updated as:

Fm(x) = Fm−1(x) + βmh(x; am)

(14)

In tree boosting, the base learner h(x; a) is an L-terminal node
regression tree. At each iteration m, a regression tree partitions
the x-space into L-disjoint regions {Rlm}L
l=1and predicts a
separate constant value in each one. The update rules for
calculating Fm(x) given Fm−1(x) are as follows:

(cid:20) dΨ(yi, F (xi))

(cid:21)

˜yim = −

dF (xi)

F (x)=Fm−1(x)

γlm = argminγ

(cid:88)

xi∈Rlm

Ψ(yi, Fm−1(xi) + γ)

(18)

Fm(x) = Fm−1(x) + ν.γlm1(x ∈ Rlm)

(19)

V. EXPERIMENTAL RESULTS

In this section, we detail the results of running the classiﬁers
we described in the previous section on our data, on both the
full dataset, and on collapsed categories.

(8)

A. Performance on our Original Dataset

With our original dataset, we ran two different

learning
algorithms, i.e, Naive Bayes and Random Forests classiﬁers,
to get an initial understanding of the quality of our feature set,
and the amount of predictability in the data.

The Naive Bayes Model was tested using cross validation,
i.e, 70 percent of the data was used for training and the rest for
testing purposes. We got the following results:

• The classiﬁer gave 30% accuracy on the training set and
25% accuracy on the cross validation set. Hence, both
training and cross validation error were very high.

• The above trend was observed even on varying the size of
the training set. The accuracy did not go above 30%, even
on the training set. Note that this is still signiﬁcantly better
than random guessing, since we have not 2, but 39, output
classes.

We also implemented Random Forests for our classiﬁcation
problem. This was done keeping in mind that most machine
learning algorithms work with numerical features and that our
features are almost all categorical
in nature. The Random
Forests classiﬁer works well with categorical features and does
not need any preprocessing. We got the following results:

• After building the model, we ran it on the training set
itself to get a training error of 5%, which initially looked
too good to be true.

• However, on performing cross validation on the training
data using 10 folds, we got a test error of 84%, which
was huge compared to our training error, indicating high
variance.

B. Performance on Collapsed Classes:

After the feature enriching process detailed above, where
we augmented our training data with data from the Census,
we ran 3 different algorithms on two separate classiﬁcations of
the data. We split our crime categories into Blue Collar/White
Collar Crimes in one case and Violent/Non-Violent Crimes
in the other. Since blue-collar crimes far outnumbered white-
collar crimes, and non-violent crimes far exceeded violent
crimes, we decided to duplicate the minority class while
training our classiﬁer. This penalized the classiﬁer more for
mislabeling a training example in the minority class,
than
for mislabeling a training example in the majority class,
during the training phase, and over-rode the tendency of the
classiﬁer to minimize its error by simply labeling all data as
belonging to the majority class. By doing this, we were able
to improve our precision and recall values on the minority
class. Using this classiﬁcation, we ran the following algorithms:

Random Forests:
For random forests algorithms, the parameters to tune are the
number of trees and the maximum depth of each tree. In order

(9)

(10)

(11)

(12)

(15)

(16)

(17)

¯ylm = meanxi∈Rlm (˜yim)

h(x;{Rlm}L

1 ) =

¯ylm.1(x ∈ Rlm)

L(cid:88)

l=1

to pick optimum values for these, we tested the algorithm on
the data for different combinations of the parameters. Finally,
we picked the set of parameters that gave not only the overall
highest accuracy but also the highest precision and recall values
for both crime classes. The graphs below show the variation
of accuracy, precision and recall values for parameter values
for Blue/White Crime classiﬁcation. From this, the maximum
accuracy obtained was 79.18% for number of trees = 200 and
maximum depth = 15. Thus, random forests worked fairly well
on this problem, especially for blue collar crimes.

show the variation of accuracy, precision and recall values for
parameter values. From this, the maximum accuracy obtained
was 96.3% for number of estimators = 200 and maximum depth
= 13.

TABLE V: Precision and recall for gradient boosted trees on
Blue Collar/White Collar crime classiﬁcation

Precision Blue
0.963603527

Precision White
0.971364318

Recall Blue
0.996785378

Recall White
0.743345571

TABLE III: Precision and recall for random forests on Blue
Collar/White Collar crime classiﬁcation

Precision Blue
0.869700021

Precision White
0.163540533

Recall Blue
0.741358669

Recall White
0.312845342

The graphs below show the variation of accuracy, precision and
recall values for various conﬁgurations of parameter values for
Violent/Non-Violent Crime classiﬁcation. From this, the maxi-
mum accuracy obtained was 75.02% for number of estimators
= 200 and maximum depth = 11.

The graphs below show the variation of accuracy, precision
and recall values for parameter values for Violent/Non-Violent
Crime classiﬁcation. From this, the maximum accuracy obtained
was 61.75% for number of trees = 200 and maximum depth =
15.

TABLE IV: Precision and recall
for
Violent/Non-Violent crime classiﬁcation

random forests on

Precision Violent
0.589660627

Recall Violent
0.598377214

Precision Non Violent
0.762333658

Recall Non Violent
0.400292642

Gradient Boosted Decision Trees:
For the gradient boosted trees algorithm, the parameters of
interest are the number of trees and the maximum depth of
each tree. Once again, we tested the algorithm on the data for
different permutations of the parameters. Finally, we picked the
set of parameters that gave not only the overall highest accuracy
but also the highest precision and recall values for both crime
classes for Blue/White Crime classiﬁcation. The graphs below

TABLE VI: Precision and recall for gradient boosted trees on
Violent/Non-Violent crime classiﬁcation

Precision Violent
0.804631244

Recall Violent
0.899938198

Precision Non Violent
0.362965695

Recall Non Violent
0.206925557

Support Vector Machines:
We used Support Vector Machine classiﬁer with an “RBF”
kernel as our ﬁnal algorithm. The two parameters of interest
for the classiﬁer are the c and γ values. Once again, we
tested the algorithm on the data for different permutations of
the parameters and we picked the set of parameters that gave
the highest accuracy. The graphs below show the variation
of accuracy, precision and recall values for parameter values
for Blue/White Crime Classiﬁcation. From this, the maximum
accuracy obtained was 96% for c = 4 and γ = 0.2.
The graphs below show the variation of accuracy, precision
and recall values for parameter values for Violent/Non-Violent
Crime classiﬁcation. From this, the maximum accuracy obtained
was 62.80% for c = 1.1 and γ = 0.01.

VI. CONCLUSION AND FUTURE WORK

The initial problem of classifying 39 different crime
categories was a challenging multi-class classiﬁcation problem,
and there was not enough predictability in our initial data-set
to obtain very high accuracy on it. We found that a more
meaningful approach was to collapse the crime categories into
fewer, larger groups, in order to ﬁnd structure in the data. We
got high accuracy and precision on the blue-collar/white-collar
crime classiﬁcation problem using Gradient Boosted trees
and Support Vector Machines (the former famously robust
and the latter well-suited to a 2-class classiﬁcation problem,
especially with an RBF Kernel
that can translate the data
to a high-dimensional space where it is linearly separable).
However, the Violent/Non-violent crime classiﬁcation did not
yield remarkable results with the same classiﬁers – this was
a signiﬁcantly harder classiﬁcation problem. Thus, collapsing
crime categories is not an obvious task and requires careful
choice and consideration.

Possible avenues through which to extend this work include
time-series modeling of the data to understand temporal cor-
relations in it, which can then be used to predict surges in
different categories of crime. It would also be interesting to
explore relationships between surges in different categories of
crimes – for example, it could be the case that two or more
classes of crimes surge and sink together, which would be
an interesting relationship to uncover. Other areas to work on
include implementing a more accurate multi-class classiﬁer, and
exploring better ways to visualize our results.

ACKNOWLEDGMENTS

We would like to thank our project TA Youssef Ahrez for
his thoughtful feedback and helpful ideas at every stage of our
project. We also owe a debt of gratitude to Viswajith Venugopal,
a student of the Department of Computer Science, for helping us
to understand and implement parallel computing. And of course,
our acknowledgements section would be incomplete without a
mention of Professor Andrew Ng, whose excellent class enabled
us to do this project in the ﬁrst place.

REFERENCES

[1] Bogomolov, Andrey and Lepri, Bruno and Staiano,
Jacopo and Oliver, Nuria and Pianesi, Fabio and Pentland,
Alex.2014. Once upon a crime: Towards crime prediction
from demographics and mobile data, Proceedings of the 16th
International Conference on Multimodal Interaction.
[2] Yu, Chung-Hsien and Ward, Max W and Morabito, Melissa
and Ding, Wei.2011. Crime forecasting using data mining
techniques, pages 779-786, IEEE 11th International Conference
on Data Mining Workshops (ICDMW)
[3] Kianmehr, Keivan and Alhajj, Reda. 2008. Effectiveness of
support vector machine for crime hot-spots prediction, pages
433-458, Applied Artiﬁcial Intelligence, volume 22, number 5.
[4] Toole, Jameson L and Eagle, Nathan and Plotkin, Joshua B.
2011 (TIST), volume 2, number 4, pages 38, ACM Transactions
on Intelligent Systems and Technology
[5] Wang, Tong and Rudin, Cynthia and Wagner, Daniel and
Sevieri, Rich. 2013. pages 515-530, Machine Learning and
Knowledge Discovery in Databases
[6] Friedman,
Jerome H. ”Stochastic gradient boosting.”
Computational Statistics and Data Analysis 38.4 (2002):
367-378.sts

TABLE VII: Precision and recall for SVMs on Blue Col-
lar/White Collar crime classiﬁcation

Precision Blue
0.9602589

Precision White
0.998788205

Recall Blue
0.988629185

Recall White
0.718219367

TABLE VIII: Precision and recall for SVMs on Violent/Non-
Violent crime classiﬁcation

Precision Violent
0.823943997

Recall Violent
0.668314728

Precision Non Violent
0.285785578

Recall Non Violent
0.481704753

C. Feature selection

More important than just our accuracy and precision is the
interpretation of our model: which of our features actually help
predict the category of crime? Analyzing the feature impor-
tances of our model gave us interesting insights.The following
are the most relevant features used by our SVM (which we
identiﬁed by running forward search) for classiﬁcation, for
the case of Blue Collar/White Collar Crimes: Hour, Minority
Population, Day Of Week, Tract Income Level, Tract Code,
Police District Number, Tract Population, Day of Month

[7]Leo Breiman, Random Forests, Machine Learning, 2001,
Volume 45, Number 1, Page 5

