Amazon Employee Access Control System

Shijian Tang, Jiang Han and Yue Zhang

Department of Electrical Engineering

Email: sjtang@stanford.edu

Abstract—In this work, based on the history data of 2010-
2011 from Amazon Inc., we build up a system which aims to
take place of resource administrators at Amazon. Our analysis
shows that the given dataset is highly imbalanced with categorical
values. Thus in the preprocessing step, we tried different sampling
methods, feature selection as well as one hot encoding to make
the data more suitable for prediction. In the prediction step,
initially we tried single models which are suitable for categorical
data like Naive Bayes, K-Nearest Neighbors (K-NN) and Decision
Tree. Then due to the performance limitation, we further applied
ensemble methods like random forest and gradient boosting. In
addition, with one-hot encoding which transforms the categorical
data into binary values, we are able to apply linear classiﬁer
and obtain satisﬁed performance. Finally, we ensemble three
best prediction results from random forest, gradient boosting and
logistic regression (with encoded data), and improved Area Under
Curve (AUC) from initial 0.7338 (Decision Tree) to 0.8903.

Keywords—Imbalanced classiﬁcation, categorical feature, one

hot encoding, random forest, gradient boosting.

I.

INTRODUCTION

An employee may need to apply for different resources
during his or her career at the company. For giants like Google
and Amazon, due to their highly complicated employee and
resource situations, the application review process is generally
done by different human administrators. In this project, based
on the history data of 2010 to 2011 done by human admin-
istrators at Amazon Inc., we aim to build up an employee
access control system, which automatically approve or reject
employee’s resource application.

Determining resource access privileges of employees is a
popular real-world challenge for many giant companies like
Amazon. When employees start to work, they ﬁrst need to
know what kinds of resources of the company they are or they
are not supposed to get access to. The resources maybe very
diverse, like computing resource and storage resource. It is
supposed that employees fulﬁlling the functions of the same
or similar roles should access the same or similar resources.
Here we have the data set from a knowledgeable supervisor
who takes time to manually grant the employee requests for
resource access. For the purpose of saving money and time
to allocate resources for the coming-and-going employees,
we built up a predication model that automatically determine
resource access privileges of employees.

the data set

In this problem,

is extremely imbalanced,
where one of the classes ACTION = 0 has a signiﬁcant
less number of occurrences than the other ACTION = 1 (5-
95 percent). Due to most of the learning algorithms design
principle of minimizing the overall error rate to which the
minority class contributes less, they always perform poorly in
problems with imbalanced data set.

This paper can be organized as follows. We will ﬁrst
describe and preprocess the data set in sections II and III.
Then, we will brieﬂy introduce several types of models such
as single classiﬁers described as well as ensemble models in
section IV. In section V, we will present the prediction results
for these models. At last we will summarize our work and
draw conclusions in section VI.

II. DATA DESCRIPTION

The data comes from Amazon Inc. collected from 2010-
2011 (published on Kaggle platform) with training set of
32769 samples and testing of 58922 samples. As shown in
table I, each of the data samples has one label attribute
called “ACTION”, where value “1” indicates this application is
approved and “0” indicates rejection. In addition, each samples
has eight features, which basically indicates different role or
group of one employee at Amazon.

TABLE I.

DATA FEATURE DESCRIPTION

Feature Name

ACTION

RESOURCE

MGR ID

ROLE ROLLUP 1

ROLE ROLLUP 2

ROLE DEPTNAME

ROLE TITLE

ROLE FAMILY

Feature Meaning

“1”: approved; “0”: rejected.

Resource ID

ID of the employee’s manager
Company role category ID1

(e.g. US Engineering)

Company role category ID2

(US Retail)

Department description
Business title description

Role family description
(e.g. Retail Manager)

(e.g. Manager)

ROLE FAMILY DESC

Role family extended description

ROLE CODE

Unique ID for each company role

As for the evaluation metric, Receiver Operating Character-
istic (ROC) curve is used to summarize classiﬁer performance
over tradeoffs between true positive and false positive error
rates. And we use Area Under the ROC Curve (AUC) as a
useful performance metric for imbalance classing problems.

III. DATA PREPROCESSING

In this section, we did three parts of work, which are:
balancing the dataset, one-hot encoding and feature selection.

A. Imbalanced dataset

As we described above, the given dataset is extremely
imbalanced with the number of one class signiﬁcantly lower
than the other. In real life, many issues can be described as
imbalanced classiﬁcation problem, such as medical diagnose,
text categorization, online resources management and so on.
Recently, the main approaches to solve the imbalanced
classiﬁcation is trying to balance the distribution between

Amazon Employee Access Control System

Shijian Tang, Jiang Han and Yue Zhang

Department of Electrical Engineering

Email: sjtang@stanford.edu

Abstract—In this work, based on the history data of 2010-
2011 from Amazon Inc., we build up a system which aims to
take place of resource administrators at Amazon. Our analysis
shows that the given dataset is highly imbalanced with categorical
values. Thus in the preprocessing step, we tried different sampling
methods, feature selection as well as one hot encoding to make
the data more suitable for prediction. In the prediction step,
initially we tried single models which are suitable for categorical
data like Naive Bayes, K-Nearest Neighbors (K-NN) and Decision
Tree. Then due to the performance limitation, we further applied
ensemble methods like random forest and gradient boosting. In
addition, with one-hot encoding which transforms the categorical
data into binary values, we are able to apply linear classiﬁer
and obtain satisﬁed performance. Finally, we ensemble three
best prediction results from random forest, gradient boosting and
logistic regression (with encoded data), and improved Area Under
Curve (AUC) from initial 0.7338 (Decision Tree) to 0.8903.

Keywords—Imbalanced classiﬁcation, categorical feature, one

hot encoding, random forest, gradient boosting.

I.

INTRODUCTION

An employee may need to apply for different resources
during his or her career at the company. For giants like Google
and Amazon, due to their highly complicated employee and
resource situations, the application review process is generally
done by different human administrators. In this project, based
on the history data of 2010 to 2011 done by human admin-
istrators at Amazon Inc., we aim to build up an employee
access control system, which automatically approve or reject
employee’s resource application.

Determining resource access privileges of employees is a
popular real-world challenge for many giant companies like
Amazon. When employees start to work, they ﬁrst need to
know what kinds of resources of the company they are or they
are not supposed to get access to. The resources maybe very
diverse, like computing resource and storage resource. It is
supposed that employees fulﬁlling the functions of the same
or similar roles should access the same or similar resources.
Here we have the data set from a knowledgeable supervisor
who takes time to manually grant the employee requests for
resource access. For the purpose of saving money and time
to allocate resources for the coming-and-going employees,
we built up a predication model that automatically determine
resource access privileges of employees.

the data set

In this problem,

is extremely imbalanced,
where one of the classes ACTION = 0 has a signiﬁcant
less number of occurrences than the other ACTION = 1 (5-
95 percent). Due to most of the learning algorithms design
principle of minimizing the overall error rate to which the
minority class contributes less, they always perform poorly in
problems with imbalanced data set.

This paper can be organized as follows. We will ﬁrst
describe and preprocess the data set in sections II and III.
Then, we will brieﬂy introduce several types of models such
as single classiﬁers described as well as ensemble models in
section IV. In section V, we will present the prediction results
for these models. At last we will summarize our work and
draw conclusions in section VI.

II. DATA DESCRIPTION

The data comes from Amazon Inc. collected from 2010-
2011 (published on Kaggle platform) with training set of
32769 samples and testing of 58922 samples. As shown in
table I, each of the data samples has one label attribute
called “ACTION”, where value “1” indicates this application is
approved and “0” indicates rejection. In addition, each samples
has eight features, which basically indicates different role or
group of one employee at Amazon.

TABLE I.

DATA FEATURE DESCRIPTION

Feature Name

ACTION

RESOURCE

MGR ID

ROLE ROLLUP 1

ROLE ROLLUP 2

ROLE DEPTNAME

ROLE TITLE

ROLE FAMILY

Feature Meaning

“1”: approved; “0”: rejected.

Resource ID

ID of the employee’s manager
Company role category ID1

(e.g. US Engineering)

Company role category ID2

(US Retail)

Department description
Business title description

Role family description
(e.g. Retail Manager)

(e.g. Manager)

ROLE FAMILY DESC

Role family extended description

ROLE CODE

Unique ID for each company role

As for the evaluation metric, Receiver Operating Character-
istic (ROC) curve is used to summarize classiﬁer performance
over tradeoffs between true positive and false positive error
rates. And we use Area Under the ROC Curve (AUC) as a
useful performance metric for imbalance classing problems.

III. DATA PREPROCESSING

In this section, we did three parts of work, which are:
balancing the dataset, one-hot encoding and feature selection.

A. Imbalanced dataset

As we described above, the given dataset is extremely
imbalanced with the number of one class signiﬁcantly lower
than the other. In real life, many issues can be described as
imbalanced classiﬁcation problem, such as medical diagnose,
text categorization, online resources management and so on.
Recently, the main approaches to solve the imbalanced
classiﬁcation is trying to balance the distribution between

TABLE II.

ONE-HOT ENCODING EXAMPLE

Category Data

119433
118321
118278

One-hot encoded eata

001
010
100

TABLE III.

FEATURE FREQUENCY VARIANCE

Feature Index
Variance(*106)
Feature Index
Variance(*106)

F1

0.0003

F6

F2

0.0001

F7

F3

3.5665

F8

F4

0.3191

F9

0.1244

0.0220

2.0396

0.1244

F5

0.0137

minority and majority classes in training set [1],[2] to make
the dataset suitable for standard machine learning models.
These techniques includes oversampling, undersampling [1]
and Synthetic Minority Over-sampling Technique (SMOTE)
[2].

1) Random oversampling and undersampling: Oversam-
pling typically refers to balance the data distribution by
sampling the minority class data with replacement. On the
other hand, undersampling changes the distribution of data by
randomly removing the data in majority class. Although the
performance will be improved by the above sampling methods,
the shortcomings of oversampling and undersampling are obvi-
ous. Oversampling will result in overﬁtting, and undersampling
may loss the importance information from dataset.

2) SMOTE: SMOTE is an oversampling method, which
will generate synthetic training samples [2] instead of remov-
ing or duplicating raw data. The basic idea of SMOTE is that
for each minority class sample, we create a synthetic example
from some of the k nearest neighbors of that sample. Based
on the number of new samples we need, we will randomly
choose some neighbors among all the nearest neighbors. This
process can be interpreted as choosing a random point in the
line between two feature vectors as our new samples.

B. One-hot encoding

Since the original features are discrete category values
which indicate different types. We can not directly apply linear
classiﬁer models on this kind of data. In order to apply linear
classiﬁer on this data, we need to use one-hot encoding.

One-hot encoding refers to bits that only have one single
active 1 while all remaining bits are inactive 0s. In the
given Amazon data, one feature may include multiple discrete
categorical values. In order to apply linear classiﬁer, it is
necessary to separate those values with only one active at a
time. Table II shows the example with one hot encoding on
a feature with three categories, which will be encoded into
“001”, “010” and “100” separately. We see that this encoding
method will expand the feature space from one to ﬁve. Sine
most of the sample values are “0”s, we use sparse matrix to
represent the newly encoded feature space.

C. Feature selection

Removing features with low variance is a common ap-
proach in feature selection [3]. In this work, we calculate the

(cid:88)

sj

V ar(Fi) =

(cid:18)

(cid:19)2

frequency variance of each feature based on Eq. (1):

f req(sj)

sum

f req(sj) − sum
|Fi,uniq|

(1)

where sj ∈ Fi,uniq, and Fi is the ith feature and Fi,uniq is the
set with all the unique feature values of Fi. Function f req(sj)
indicates the frequency of sj. |Fi,uniq| is the size of Fi,uniq,
i.e. the unique value number of feature Fi. In addition, sum =

f req(sj), which is the frequency sum of each sj.

(cid:80)

sj

Table III shows the analysis result of frequency variance
of all nine features, from which we can see that feature 1 and
feature 2 have obviously small variance. Hence, we remove
those two features in the further analysis steps.

IV. PREDICTIONS MODELS

A. Single classiﬁer

In this section, we brieﬂy introduce some single classi-
ﬁcation models we applied for initial testing. Here, “single”
corresponds to the “Ensembling” in the next section. They are
commonly used, thus we do not introduce the details here.

1) Naive Bayes: In this project problem, we treat all the
features to be mutually independent. This “naive” indepen-
dence assumption allows us to apply Naive Bayes algorithm in
the given categorical data. Also, laplace smoothing is applied
to those features never seen in the training. Thus, one sample
is labelled with 1 when P (1|F1...FN ) > P (0|F1...FN ) and
vice versa.

2) K-NN: K-NN algorithm is also suitable for categorical
data. It will label testing samples based on their nearest K
neighbors in the training set. Since the given data is categorical,
we use hamming distance instead of educlidean distance.
Smaller hamming distance between test sample and speciﬁc
training sample indicates they two are closer in the space.
Besides, we combine the labels of K neighbors based on their
hamming distance, closer neighbor will own higher inﬂuence.
3) Decision Tree: Decision tree builds classiﬁcation mod-
els in the form of a tree structure. It breaks down a dataset
into smaller subsets based on its features while an associated
decision tree is incrementally developed. The ﬁnal result is a
tree with decision nodes and leaf nodes. A leaf node represents
a classiﬁcation or decision.

4) Logistic Regression: Logistic regression is a commonly
used linear classiﬁer, which is simple but relatively efﬁcient.

B. Ensemble Methods

Ensemble method is a type of supervised learning, which
employs a set of classiﬁers and their decisions are combined in
certain way. Compared with single classiﬁer, in the ensemble
model, the output (decisions) depends on the vote of all the
individual classiﬁers. Generally, the ensemble model contain-
ing bagging is also known as bootstrap aggregating [4] and
boosting [6].

The main idea of bagging is that we generate several new
training data set by randomly uniform sampling the original
data set with replacement. Then use each new dataset to train

Amazon Employee Access Control System

Shijian Tang, Jiang Han and Yue Zhang

Department of Electrical Engineering

Email: sjtang@stanford.edu

Abstract—In this work, based on the history data of 2010-
2011 from Amazon Inc., we build up a system which aims to
take place of resource administrators at Amazon. Our analysis
shows that the given dataset is highly imbalanced with categorical
values. Thus in the preprocessing step, we tried different sampling
methods, feature selection as well as one hot encoding to make
the data more suitable for prediction. In the prediction step,
initially we tried single models which are suitable for categorical
data like Naive Bayes, K-Nearest Neighbors (K-NN) and Decision
Tree. Then due to the performance limitation, we further applied
ensemble methods like random forest and gradient boosting. In
addition, with one-hot encoding which transforms the categorical
data into binary values, we are able to apply linear classiﬁer
and obtain satisﬁed performance. Finally, we ensemble three
best prediction results from random forest, gradient boosting and
logistic regression (with encoded data), and improved Area Under
Curve (AUC) from initial 0.7338 (Decision Tree) to 0.8903.

Keywords—Imbalanced classiﬁcation, categorical feature, one

hot encoding, random forest, gradient boosting.

I.

INTRODUCTION

An employee may need to apply for different resources
during his or her career at the company. For giants like Google
and Amazon, due to their highly complicated employee and
resource situations, the application review process is generally
done by different human administrators. In this project, based
on the history data of 2010 to 2011 done by human admin-
istrators at Amazon Inc., we aim to build up an employee
access control system, which automatically approve or reject
employee’s resource application.

Determining resource access privileges of employees is a
popular real-world challenge for many giant companies like
Amazon. When employees start to work, they ﬁrst need to
know what kinds of resources of the company they are or they
are not supposed to get access to. The resources maybe very
diverse, like computing resource and storage resource. It is
supposed that employees fulﬁlling the functions of the same
or similar roles should access the same or similar resources.
Here we have the data set from a knowledgeable supervisor
who takes time to manually grant the employee requests for
resource access. For the purpose of saving money and time
to allocate resources for the coming-and-going employees,
we built up a predication model that automatically determine
resource access privileges of employees.

the data set

In this problem,

is extremely imbalanced,
where one of the classes ACTION = 0 has a signiﬁcant
less number of occurrences than the other ACTION = 1 (5-
95 percent). Due to most of the learning algorithms design
principle of minimizing the overall error rate to which the
minority class contributes less, they always perform poorly in
problems with imbalanced data set.

This paper can be organized as follows. We will ﬁrst
describe and preprocess the data set in sections II and III.
Then, we will brieﬂy introduce several types of models such
as single classiﬁers described as well as ensemble models in
section IV. In section V, we will present the prediction results
for these models. At last we will summarize our work and
draw conclusions in section VI.

II. DATA DESCRIPTION

The data comes from Amazon Inc. collected from 2010-
2011 (published on Kaggle platform) with training set of
32769 samples and testing of 58922 samples. As shown in
table I, each of the data samples has one label attribute
called “ACTION”, where value “1” indicates this application is
approved and “0” indicates rejection. In addition, each samples
has eight features, which basically indicates different role or
group of one employee at Amazon.

TABLE I.

DATA FEATURE DESCRIPTION

Feature Name

ACTION

RESOURCE

MGR ID

ROLE ROLLUP 1

ROLE ROLLUP 2

ROLE DEPTNAME

ROLE TITLE

ROLE FAMILY

Feature Meaning

“1”: approved; “0”: rejected.

Resource ID

ID of the employee’s manager
Company role category ID1

(e.g. US Engineering)

Company role category ID2

(US Retail)

Department description
Business title description

Role family description
(e.g. Retail Manager)

(e.g. Manager)

ROLE FAMILY DESC

Role family extended description

ROLE CODE

Unique ID for each company role

As for the evaluation metric, Receiver Operating Character-
istic (ROC) curve is used to summarize classiﬁer performance
over tradeoffs between true positive and false positive error
rates. And we use Area Under the ROC Curve (AUC) as a
useful performance metric for imbalance classing problems.

III. DATA PREPROCESSING

In this section, we did three parts of work, which are:
balancing the dataset, one-hot encoding and feature selection.

A. Imbalanced dataset

As we described above, the given dataset is extremely
imbalanced with the number of one class signiﬁcantly lower
than the other. In real life, many issues can be described as
imbalanced classiﬁcation problem, such as medical diagnose,
text categorization, online resources management and so on.
Recently, the main approaches to solve the imbalanced
classiﬁcation is trying to balance the distribution between

TABLE II.

ONE-HOT ENCODING EXAMPLE

Category Data

119433
118321
118278

One-hot encoded eata

001
010
100

TABLE III.

FEATURE FREQUENCY VARIANCE

Feature Index
Variance(*106)
Feature Index
Variance(*106)

F1

0.0003

F6

F2

0.0001

F7

F3

3.5665

F8

F4

0.3191

F9

0.1244

0.0220

2.0396

0.1244

F5

0.0137

minority and majority classes in training set [1],[2] to make
the dataset suitable for standard machine learning models.
These techniques includes oversampling, undersampling [1]
and Synthetic Minority Over-sampling Technique (SMOTE)
[2].

1) Random oversampling and undersampling: Oversam-
pling typically refers to balance the data distribution by
sampling the minority class data with replacement. On the
other hand, undersampling changes the distribution of data by
randomly removing the data in majority class. Although the
performance will be improved by the above sampling methods,
the shortcomings of oversampling and undersampling are obvi-
ous. Oversampling will result in overﬁtting, and undersampling
may loss the importance information from dataset.

2) SMOTE: SMOTE is an oversampling method, which
will generate synthetic training samples [2] instead of remov-
ing or duplicating raw data. The basic idea of SMOTE is that
for each minority class sample, we create a synthetic example
from some of the k nearest neighbors of that sample. Based
on the number of new samples we need, we will randomly
choose some neighbors among all the nearest neighbors. This
process can be interpreted as choosing a random point in the
line between two feature vectors as our new samples.

B. One-hot encoding

Since the original features are discrete category values
which indicate different types. We can not directly apply linear
classiﬁer models on this kind of data. In order to apply linear
classiﬁer on this data, we need to use one-hot encoding.

One-hot encoding refers to bits that only have one single
active 1 while all remaining bits are inactive 0s. In the
given Amazon data, one feature may include multiple discrete
categorical values. In order to apply linear classiﬁer, it is
necessary to separate those values with only one active at a
time. Table II shows the example with one hot encoding on
a feature with three categories, which will be encoded into
“001”, “010” and “100” separately. We see that this encoding
method will expand the feature space from one to ﬁve. Sine
most of the sample values are “0”s, we use sparse matrix to
represent the newly encoded feature space.

C. Feature selection

Removing features with low variance is a common ap-
proach in feature selection [3]. In this work, we calculate the

(cid:88)

sj

V ar(Fi) =

(cid:18)

(cid:19)2

frequency variance of each feature based on Eq. (1):

f req(sj)

sum

f req(sj) − sum
|Fi,uniq|

(1)

where sj ∈ Fi,uniq, and Fi is the ith feature and Fi,uniq is the
set with all the unique feature values of Fi. Function f req(sj)
indicates the frequency of sj. |Fi,uniq| is the size of Fi,uniq,
i.e. the unique value number of feature Fi. In addition, sum =

f req(sj), which is the frequency sum of each sj.

(cid:80)

sj

Table III shows the analysis result of frequency variance
of all nine features, from which we can see that feature 1 and
feature 2 have obviously small variance. Hence, we remove
those two features in the further analysis steps.

IV. PREDICTIONS MODELS

A. Single classiﬁer

In this section, we brieﬂy introduce some single classi-
ﬁcation models we applied for initial testing. Here, “single”
corresponds to the “Ensembling” in the next section. They are
commonly used, thus we do not introduce the details here.

1) Naive Bayes: In this project problem, we treat all the
features to be mutually independent. This “naive” indepen-
dence assumption allows us to apply Naive Bayes algorithm in
the given categorical data. Also, laplace smoothing is applied
to those features never seen in the training. Thus, one sample
is labelled with 1 when P (1|F1...FN ) > P (0|F1...FN ) and
vice versa.

2) K-NN: K-NN algorithm is also suitable for categorical
data. It will label testing samples based on their nearest K
neighbors in the training set. Since the given data is categorical,
we use hamming distance instead of educlidean distance.
Smaller hamming distance between test sample and speciﬁc
training sample indicates they two are closer in the space.
Besides, we combine the labels of K neighbors based on their
hamming distance, closer neighbor will own higher inﬂuence.
3) Decision Tree: Decision tree builds classiﬁcation mod-
els in the form of a tree structure. It breaks down a dataset
into smaller subsets based on its features while an associated
decision tree is incrementally developed. The ﬁnal result is a
tree with decision nodes and leaf nodes. A leaf node represents
a classiﬁcation or decision.

4) Logistic Regression: Logistic regression is a commonly
used linear classiﬁer, which is simple but relatively efﬁcient.

B. Ensemble Methods

Ensemble method is a type of supervised learning, which
employs a set of classiﬁers and their decisions are combined in
certain way. Compared with single classiﬁer, in the ensemble
model, the output (decisions) depends on the vote of all the
individual classiﬁers. Generally, the ensemble model contain-
ing bagging is also known as bootstrap aggregating [4] and
boosting [6].

The main idea of bagging is that we generate several new
training data set by randomly uniform sampling the original
data set with replacement. Then use each new dataset to train

a model, and the output equals to the dominate vote by all
the trained models. Typically, we choose decision tree as each
training model in many practical problems.

Boosting is an alternative ensemble method in machine
learning. Compared with bagging, the basic rule of boosting
is that by combining a set of weak learner properly, we can
obtain a strong learner. The boosting algorithms are typically
performed iteratively [6]. In each iteration, we add a new weak
learner to the set of learners.

Among several boosting algorithms, the adaptive boosting
(Ada Boosting) [6] and gradient boosting [7] are most popular.
In the subsection IV-B1, we will brieﬂy describe the
random forest method, which is developed based on bagging
and will be adopted in our paper. Then, in subsection IV-B2,
we will introduce the gradient boosting model in detail. We
employ the above two models to solve our problem.

1) Random Forest: Random forest [5] combines the meth-
ods of bagging with random subspace. The main idea is that
we build a set of decision trees not only depends on the
random sampling of training data, but also randomly selecting
the features when we building each tree. In prediction, we
will takes all the decisions made by each tree into account
and select the majority result as our prediction. In [5], each
tree in the forest can be generated in the following three rules:
First, randomly choose samples with replacement from original
data set to form a new data set (same as bagging). Second,
randomly choose a subset of features for each tree. And we
split the nodes during building the tree based on this subset of
features. The last is building the tree without pruning.

In [5], it reveals the fact that random forest model is able
to prevent overﬁtting. That is as the increase of tree number,
the generalization error will converge to an upper bound,

Pe ≤ ρ(1 − s2)/s2

(2)
where Pe is the generalization error. ρ as the mean value of
correlations and s is deﬁned as the classiﬁer set strength.

Based on Eq. (2) we ﬁnd that the performance of random
forest depends on the correlation between each tree. The
performance will degrades as the increase of correlations. And
the author of [5] also shows that the correlation of trees is
mainly determined by the number of the features we selected
when we split the nodes. The less number of features we
choose, the trees will become more uncorrelated, and then the
generalization error will decrease. The authors suggest that,
we choose the
N as the number of features we selected to
build the tree, with N as the number of features in training
dataset.

√

2) Gradient Boosting: The gradient boosting [7] is a type
of boosting algorithms. Boosting process can be described as
Fi = Fi−1 + γihi(x; ai) where Fi is the learner in the i th
iteration which may be a poor learner at this step. γi is a
parameter to weight the new estimator hi(x; ai), and ai is the
parameter in function h. In gradient boosting, the parameters
γi and ai are determined by minimizing the cost functions in
current iteration,

(γi, ai) = arg min
γ,a

L(yj, Fi−1(xj) + γhi(xj; a))

(3)

m(cid:88)

j=1

where xj, yj are the j th training samples with j = 1,··· , m.
h(x; a) is a simple functions added in the i th iteration
with a as the parameters to be determined. L(yj, Fi−1(xj) +
γhi(xj; a)) is the cost function between the output yj and the
model in the i th iteration which is Fi(xj).

Solving Eq. (3) consists of two steps. The ﬁrst step is

determining ai follows the expression as

ai = arg min
ai,ρ

ri,j − ρh(xj; a)

(4)

m(cid:88)

j=1

m(cid:88)

j=1

with rj,i is the pseudo residuals with details in [7].

After ﬁnding the current estimator h(x; ai), we can further

solve γi by,

γi = arg min

γ

L(yj, Fi−1(xj) + γhi(xj; ai))

(5)

From Eq. (5), [7] points out that in the case of cost function is
convex, the gradient boosting is actually equivalent to gradient
descent in function space.

Based on the description above, the most signiﬁcant param-
eters in gradient boosting are the selection of weak learners as
well as the cost function. There are several candidates for cost
function such as least squares and exponential functions. For
week learner, the most popular choice is decision tree, where
the gradient boosting model is often called gradient boosted
decision trees.

In the following section, we will discuss how to implement

the gradient boosting to solve our problem.

V. PREDICTION RESULT

In this section, we will show the simulation results with
different prediction models. Firstly, we try single predication
models which can be applied directly on categorical data like
Naive Bayes, K-NN and decision tree. Secondly, since we
found out that single classiﬁer does not well on the given data,
we further try ensemble models including random forest and
gradient boosting based on decision tree. In addition, we show
the AUC performance of logistic regression with one hot en-
coded data, which veriﬁes the advantage of one hot encoding.
In the ﬁnal, we combine the three best prediction results from
random forest, gradient boosting and one-hot encoding based
logistic regression to obtain further performance improvement.

A. Single Model with Categorical Data

In this subsection, we present the prediction results based
on different single models. In addition, different dataset with
raw data, under-sampling, over-sampling and SMOTE are also
taken into consideration for comparision.

Table IV shows the prediction results of Naive Bayes, KNN
and Decision Tree. We can see that since the data size is
big enough, the difference between training and test AUC
is generally quite small. For different sampling method, we
see that direct prediction on raw data can not work well
due to the imbalanced data type. While under-sampling and
SMOTE generally work better than over-sampling method,
which mathes the conclusion drawn by [1].

Amazon Employee Access Control System

Shijian Tang, Jiang Han and Yue Zhang

Department of Electrical Engineering

Email: sjtang@stanford.edu

Abstract—In this work, based on the history data of 2010-
2011 from Amazon Inc., we build up a system which aims to
take place of resource administrators at Amazon. Our analysis
shows that the given dataset is highly imbalanced with categorical
values. Thus in the preprocessing step, we tried different sampling
methods, feature selection as well as one hot encoding to make
the data more suitable for prediction. In the prediction step,
initially we tried single models which are suitable for categorical
data like Naive Bayes, K-Nearest Neighbors (K-NN) and Decision
Tree. Then due to the performance limitation, we further applied
ensemble methods like random forest and gradient boosting. In
addition, with one-hot encoding which transforms the categorical
data into binary values, we are able to apply linear classiﬁer
and obtain satisﬁed performance. Finally, we ensemble three
best prediction results from random forest, gradient boosting and
logistic regression (with encoded data), and improved Area Under
Curve (AUC) from initial 0.7338 (Decision Tree) to 0.8903.

Keywords—Imbalanced classiﬁcation, categorical feature, one

hot encoding, random forest, gradient boosting.

I.

INTRODUCTION

An employee may need to apply for different resources
during his or her career at the company. For giants like Google
and Amazon, due to their highly complicated employee and
resource situations, the application review process is generally
done by different human administrators. In this project, based
on the history data of 2010 to 2011 done by human admin-
istrators at Amazon Inc., we aim to build up an employee
access control system, which automatically approve or reject
employee’s resource application.

Determining resource access privileges of employees is a
popular real-world challenge for many giant companies like
Amazon. When employees start to work, they ﬁrst need to
know what kinds of resources of the company they are or they
are not supposed to get access to. The resources maybe very
diverse, like computing resource and storage resource. It is
supposed that employees fulﬁlling the functions of the same
or similar roles should access the same or similar resources.
Here we have the data set from a knowledgeable supervisor
who takes time to manually grant the employee requests for
resource access. For the purpose of saving money and time
to allocate resources for the coming-and-going employees,
we built up a predication model that automatically determine
resource access privileges of employees.

the data set

In this problem,

is extremely imbalanced,
where one of the classes ACTION = 0 has a signiﬁcant
less number of occurrences than the other ACTION = 1 (5-
95 percent). Due to most of the learning algorithms design
principle of minimizing the overall error rate to which the
minority class contributes less, they always perform poorly in
problems with imbalanced data set.

This paper can be organized as follows. We will ﬁrst
describe and preprocess the data set in sections II and III.
Then, we will brieﬂy introduce several types of models such
as single classiﬁers described as well as ensemble models in
section IV. In section V, we will present the prediction results
for these models. At last we will summarize our work and
draw conclusions in section VI.

II. DATA DESCRIPTION

The data comes from Amazon Inc. collected from 2010-
2011 (published on Kaggle platform) with training set of
32769 samples and testing of 58922 samples. As shown in
table I, each of the data samples has one label attribute
called “ACTION”, where value “1” indicates this application is
approved and “0” indicates rejection. In addition, each samples
has eight features, which basically indicates different role or
group of one employee at Amazon.

TABLE I.

DATA FEATURE DESCRIPTION

Feature Name

ACTION

RESOURCE

MGR ID

ROLE ROLLUP 1

ROLE ROLLUP 2

ROLE DEPTNAME

ROLE TITLE

ROLE FAMILY

Feature Meaning

“1”: approved; “0”: rejected.

Resource ID

ID of the employee’s manager
Company role category ID1

(e.g. US Engineering)

Company role category ID2

(US Retail)

Department description
Business title description

Role family description
(e.g. Retail Manager)

(e.g. Manager)

ROLE FAMILY DESC

Role family extended description

ROLE CODE

Unique ID for each company role

As for the evaluation metric, Receiver Operating Character-
istic (ROC) curve is used to summarize classiﬁer performance
over tradeoffs between true positive and false positive error
rates. And we use Area Under the ROC Curve (AUC) as a
useful performance metric for imbalance classing problems.

III. DATA PREPROCESSING

In this section, we did three parts of work, which are:
balancing the dataset, one-hot encoding and feature selection.

A. Imbalanced dataset

As we described above, the given dataset is extremely
imbalanced with the number of one class signiﬁcantly lower
than the other. In real life, many issues can be described as
imbalanced classiﬁcation problem, such as medical diagnose,
text categorization, online resources management and so on.
Recently, the main approaches to solve the imbalanced
classiﬁcation is trying to balance the distribution between

TABLE II.

ONE-HOT ENCODING EXAMPLE

Category Data

119433
118321
118278

One-hot encoded eata

001
010
100

TABLE III.

FEATURE FREQUENCY VARIANCE

Feature Index
Variance(*106)
Feature Index
Variance(*106)

F1

0.0003

F6

F2

0.0001

F7

F3

3.5665

F8

F4

0.3191

F9

0.1244

0.0220

2.0396

0.1244

F5

0.0137

minority and majority classes in training set [1],[2] to make
the dataset suitable for standard machine learning models.
These techniques includes oversampling, undersampling [1]
and Synthetic Minority Over-sampling Technique (SMOTE)
[2].

1) Random oversampling and undersampling: Oversam-
pling typically refers to balance the data distribution by
sampling the minority class data with replacement. On the
other hand, undersampling changes the distribution of data by
randomly removing the data in majority class. Although the
performance will be improved by the above sampling methods,
the shortcomings of oversampling and undersampling are obvi-
ous. Oversampling will result in overﬁtting, and undersampling
may loss the importance information from dataset.

2) SMOTE: SMOTE is an oversampling method, which
will generate synthetic training samples [2] instead of remov-
ing or duplicating raw data. The basic idea of SMOTE is that
for each minority class sample, we create a synthetic example
from some of the k nearest neighbors of that sample. Based
on the number of new samples we need, we will randomly
choose some neighbors among all the nearest neighbors. This
process can be interpreted as choosing a random point in the
line between two feature vectors as our new samples.

B. One-hot encoding

Since the original features are discrete category values
which indicate different types. We can not directly apply linear
classiﬁer models on this kind of data. In order to apply linear
classiﬁer on this data, we need to use one-hot encoding.

One-hot encoding refers to bits that only have one single
active 1 while all remaining bits are inactive 0s. In the
given Amazon data, one feature may include multiple discrete
categorical values. In order to apply linear classiﬁer, it is
necessary to separate those values with only one active at a
time. Table II shows the example with one hot encoding on
a feature with three categories, which will be encoded into
“001”, “010” and “100” separately. We see that this encoding
method will expand the feature space from one to ﬁve. Sine
most of the sample values are “0”s, we use sparse matrix to
represent the newly encoded feature space.

C. Feature selection

Removing features with low variance is a common ap-
proach in feature selection [3]. In this work, we calculate the

(cid:88)

sj

V ar(Fi) =

(cid:18)

(cid:19)2

frequency variance of each feature based on Eq. (1):

f req(sj)

sum

f req(sj) − sum
|Fi,uniq|

(1)

where sj ∈ Fi,uniq, and Fi is the ith feature and Fi,uniq is the
set with all the unique feature values of Fi. Function f req(sj)
indicates the frequency of sj. |Fi,uniq| is the size of Fi,uniq,
i.e. the unique value number of feature Fi. In addition, sum =

f req(sj), which is the frequency sum of each sj.

(cid:80)

sj

Table III shows the analysis result of frequency variance
of all nine features, from which we can see that feature 1 and
feature 2 have obviously small variance. Hence, we remove
those two features in the further analysis steps.

IV. PREDICTIONS MODELS

A. Single classiﬁer

In this section, we brieﬂy introduce some single classi-
ﬁcation models we applied for initial testing. Here, “single”
corresponds to the “Ensembling” in the next section. They are
commonly used, thus we do not introduce the details here.

1) Naive Bayes: In this project problem, we treat all the
features to be mutually independent. This “naive” indepen-
dence assumption allows us to apply Naive Bayes algorithm in
the given categorical data. Also, laplace smoothing is applied
to those features never seen in the training. Thus, one sample
is labelled with 1 when P (1|F1...FN ) > P (0|F1...FN ) and
vice versa.

2) K-NN: K-NN algorithm is also suitable for categorical
data. It will label testing samples based on their nearest K
neighbors in the training set. Since the given data is categorical,
we use hamming distance instead of educlidean distance.
Smaller hamming distance between test sample and speciﬁc
training sample indicates they two are closer in the space.
Besides, we combine the labels of K neighbors based on their
hamming distance, closer neighbor will own higher inﬂuence.
3) Decision Tree: Decision tree builds classiﬁcation mod-
els in the form of a tree structure. It breaks down a dataset
into smaller subsets based on its features while an associated
decision tree is incrementally developed. The ﬁnal result is a
tree with decision nodes and leaf nodes. A leaf node represents
a classiﬁcation or decision.

4) Logistic Regression: Logistic regression is a commonly
used linear classiﬁer, which is simple but relatively efﬁcient.

B. Ensemble Methods

Ensemble method is a type of supervised learning, which
employs a set of classiﬁers and their decisions are combined in
certain way. Compared with single classiﬁer, in the ensemble
model, the output (decisions) depends on the vote of all the
individual classiﬁers. Generally, the ensemble model contain-
ing bagging is also known as bootstrap aggregating [4] and
boosting [6].

The main idea of bagging is that we generate several new
training data set by randomly uniform sampling the original
data set with replacement. Then use each new dataset to train

a model, and the output equals to the dominate vote by all
the trained models. Typically, we choose decision tree as each
training model in many practical problems.

Boosting is an alternative ensemble method in machine
learning. Compared with bagging, the basic rule of boosting
is that by combining a set of weak learner properly, we can
obtain a strong learner. The boosting algorithms are typically
performed iteratively [6]. In each iteration, we add a new weak
learner to the set of learners.

Among several boosting algorithms, the adaptive boosting
(Ada Boosting) [6] and gradient boosting [7] are most popular.
In the subsection IV-B1, we will brieﬂy describe the
random forest method, which is developed based on bagging
and will be adopted in our paper. Then, in subsection IV-B2,
we will introduce the gradient boosting model in detail. We
employ the above two models to solve our problem.

1) Random Forest: Random forest [5] combines the meth-
ods of bagging with random subspace. The main idea is that
we build a set of decision trees not only depends on the
random sampling of training data, but also randomly selecting
the features when we building each tree. In prediction, we
will takes all the decisions made by each tree into account
and select the majority result as our prediction. In [5], each
tree in the forest can be generated in the following three rules:
First, randomly choose samples with replacement from original
data set to form a new data set (same as bagging). Second,
randomly choose a subset of features for each tree. And we
split the nodes during building the tree based on this subset of
features. The last is building the tree without pruning.

In [5], it reveals the fact that random forest model is able
to prevent overﬁtting. That is as the increase of tree number,
the generalization error will converge to an upper bound,

Pe ≤ ρ(1 − s2)/s2

(2)
where Pe is the generalization error. ρ as the mean value of
correlations and s is deﬁned as the classiﬁer set strength.

Based on Eq. (2) we ﬁnd that the performance of random
forest depends on the correlation between each tree. The
performance will degrades as the increase of correlations. And
the author of [5] also shows that the correlation of trees is
mainly determined by the number of the features we selected
when we split the nodes. The less number of features we
choose, the trees will become more uncorrelated, and then the
generalization error will decrease. The authors suggest that,
we choose the
N as the number of features we selected to
build the tree, with N as the number of features in training
dataset.

√

2) Gradient Boosting: The gradient boosting [7] is a type
of boosting algorithms. Boosting process can be described as
Fi = Fi−1 + γihi(x; ai) where Fi is the learner in the i th
iteration which may be a poor learner at this step. γi is a
parameter to weight the new estimator hi(x; ai), and ai is the
parameter in function h. In gradient boosting, the parameters
γi and ai are determined by minimizing the cost functions in
current iteration,

(γi, ai) = arg min
γ,a

L(yj, Fi−1(xj) + γhi(xj; a))

(3)

m(cid:88)

j=1

where xj, yj are the j th training samples with j = 1,··· , m.
h(x; a) is a simple functions added in the i th iteration
with a as the parameters to be determined. L(yj, Fi−1(xj) +
γhi(xj; a)) is the cost function between the output yj and the
model in the i th iteration which is Fi(xj).

Solving Eq. (3) consists of two steps. The ﬁrst step is

determining ai follows the expression as

ai = arg min
ai,ρ

ri,j − ρh(xj; a)

(4)

m(cid:88)

j=1

m(cid:88)

j=1

with rj,i is the pseudo residuals with details in [7].

After ﬁnding the current estimator h(x; ai), we can further

solve γi by,

γi = arg min

γ

L(yj, Fi−1(xj) + γhi(xj; ai))

(5)

From Eq. (5), [7] points out that in the case of cost function is
convex, the gradient boosting is actually equivalent to gradient
descent in function space.

Based on the description above, the most signiﬁcant param-
eters in gradient boosting are the selection of weak learners as
well as the cost function. There are several candidates for cost
function such as least squares and exponential functions. For
week learner, the most popular choice is decision tree, where
the gradient boosting model is often called gradient boosted
decision trees.

In the following section, we will discuss how to implement

the gradient boosting to solve our problem.

V. PREDICTION RESULT

In this section, we will show the simulation results with
different prediction models. Firstly, we try single predication
models which can be applied directly on categorical data like
Naive Bayes, K-NN and decision tree. Secondly, since we
found out that single classiﬁer does not well on the given data,
we further try ensemble models including random forest and
gradient boosting based on decision tree. In addition, we show
the AUC performance of logistic regression with one hot en-
coded data, which veriﬁes the advantage of one hot encoding.
In the ﬁnal, we combine the three best prediction results from
random forest, gradient boosting and one-hot encoding based
logistic regression to obtain further performance improvement.

A. Single Model with Categorical Data

In this subsection, we present the prediction results based
on different single models. In addition, different dataset with
raw data, under-sampling, over-sampling and SMOTE are also
taken into consideration for comparision.

Table IV shows the prediction results of Naive Bayes, KNN
and Decision Tree. We can see that since the data size is
big enough, the difference between training and test AUC
is generally quite small. For different sampling method, we
see that direct prediction on raw data can not work well
due to the imbalanced data type. While under-sampling and
SMOTE generally work better than over-sampling method,
which mathes the conclusion drawn by [1].

Fig. 1. The training and testing AUC for various tree numbers.

Fig. 2. The training and testing AUC for various training data set with tree
number as 200.

Another important information conveyed by Table IV is
that since the best test AUC performance is around 0.73 here,
we may conclude that one single prediction model can not
work well on the given problem and dataset. This makes us to
think about using some upgraded ensembel method.

TABLE IV.

AUC OF NAIVE BAYES, K-NN AND DECISION TREE

Training AUC

Test AUC

Training AUC

Test AUC

Training AUC

Test AUC

Raw Data

0.6186
0.6027

0.5971
0.5731

0.6538
0.6235

Under-sampling
Naive Bayes

Decision Tree

0.6926
0.6893
K-NN
0.6469
0.6365

0.7283
0.6963

Over-sampling

SMOTE

0.6497
0.6512

0.6205
0.6109

0.6870
0.6687

0.7037
0.7019

0.6590
0.6620

0.7323
0.7338

B. Ensemble model

1) Random forest: First, we will adopt the random forest
model for prediction. As suggested in [5], the number of
features we random selected for each tree is the root square
of the total number of features. We use CART trees without
prune, which is also implied in [5].

First we choose different number of trees to build our
model. The training and testing AUC for different tree numbers
from 10 to 300 are depicted in Fig. 1. The AUC for training
data is the average AUC calculated from 10-fold cross valida-
tion from training data.

Fig. 1 demonstrates that as the increase of tree number, the
performance of testing AUC gradually converges and does not

Fig. 3. The training AUC as a function of maximum depth of decision tree
with 500 iterations.

degrade too much as the tree number increases, i.e., the overﬁt
can be prevented. These results match with the conclusion we
described in subsection IV-B1.

From Fig. 1, we can observe that 200 is a good choice for
the tree numbers, which will not degrade the AUC performance
compared with the cases of less trees. On the other hand, it is
also not necessary to choose too large number of trees. The
time complexity will increase but the performance will change
sightly. Therefore we choose the tree number as 200 in our
further prediction.

In Fig. 2, we plot the learning curve characterized by AUC
versus the size of training data set. For the training AUC, we
use 10-fold cross validation to calculate the average AUC for
each size of training data. The learning curve implies that as
we enlarge the training data set, the training and testing AUC
converge to an expected value 0.8558. This means that random
forest is a good model for this problem and no overﬁtting
yields.

2) Gradient boosting: In this subsection, we will discuss
the results of gradient boosting. We choose the decision tree
as weak learner. Recall the discussion in subsection IV-B2,
we should select the parameters in the tree to specify the
weak learner. Typically there does not exit a general method
to determine the parameters such as maximum depth and
minimum sample splits in the decision tree. They depends on
the practical problems. In [7], the authors suggest that the depth
of tree is more important than other parameters.

Therefore, we go through several maximum depth from
2 to 15 over 500 iterations. Note that we have test that for
this speciﬁc data, after 500 iterations,
the performance of
model will converge. The training AUC based on 10-fold cross
validation for each maximum depth has been plotted in Fig.
3. Fig. 3 indicates that the optimal depth locates at 6. Next,
we consider the training and testing AUC based on the weak
learner we just found. By increasing the size of training data,
the training AUC based on 10-fold cross validation and the
testing AUC are illustrated in Fig. 4. Similar with other models
we have implemented. The AUC curves will converge to an
expected value (AUC=0.8514 for full training data set).

Finally,

let us compare the results from two ensemble
models: random forest and gradient boosting. We ﬁnd that
for large training data size, the performance of random forest
are competitive with gradient boosting, with testing AUC as

0501001502002503000.770.780.790.80.810.820.830.840.850.860.87Tree numberArea under curve (AUC)AUC performance of random forest  Test AUCTraining AUC0.511.522.53x 1040.650.70.750.80.850.9AUC curve of random forest with tree number equals 200Training data sizeArea under curve (AUC)  Training AUCTesting AUC2468101214160.760.770.780.790.80.810.820.830.840.850.86The maximum depth of decision tree as weak learnerArea under curve (AUC)The training AUC as a function of maximum depth of decision tree    Training AUCAmazon Employee Access Control System

Shijian Tang, Jiang Han and Yue Zhang

Department of Electrical Engineering

Email: sjtang@stanford.edu

Abstract—In this work, based on the history data of 2010-
2011 from Amazon Inc., we build up a system which aims to
take place of resource administrators at Amazon. Our analysis
shows that the given dataset is highly imbalanced with categorical
values. Thus in the preprocessing step, we tried different sampling
methods, feature selection as well as one hot encoding to make
the data more suitable for prediction. In the prediction step,
initially we tried single models which are suitable for categorical
data like Naive Bayes, K-Nearest Neighbors (K-NN) and Decision
Tree. Then due to the performance limitation, we further applied
ensemble methods like random forest and gradient boosting. In
addition, with one-hot encoding which transforms the categorical
data into binary values, we are able to apply linear classiﬁer
and obtain satisﬁed performance. Finally, we ensemble three
best prediction results from random forest, gradient boosting and
logistic regression (with encoded data), and improved Area Under
Curve (AUC) from initial 0.7338 (Decision Tree) to 0.8903.

Keywords—Imbalanced classiﬁcation, categorical feature, one

hot encoding, random forest, gradient boosting.

I.

INTRODUCTION

An employee may need to apply for different resources
during his or her career at the company. For giants like Google
and Amazon, due to their highly complicated employee and
resource situations, the application review process is generally
done by different human administrators. In this project, based
on the history data of 2010 to 2011 done by human admin-
istrators at Amazon Inc., we aim to build up an employee
access control system, which automatically approve or reject
employee’s resource application.

Determining resource access privileges of employees is a
popular real-world challenge for many giant companies like
Amazon. When employees start to work, they ﬁrst need to
know what kinds of resources of the company they are or they
are not supposed to get access to. The resources maybe very
diverse, like computing resource and storage resource. It is
supposed that employees fulﬁlling the functions of the same
or similar roles should access the same or similar resources.
Here we have the data set from a knowledgeable supervisor
who takes time to manually grant the employee requests for
resource access. For the purpose of saving money and time
to allocate resources for the coming-and-going employees,
we built up a predication model that automatically determine
resource access privileges of employees.

the data set

In this problem,

is extremely imbalanced,
where one of the classes ACTION = 0 has a signiﬁcant
less number of occurrences than the other ACTION = 1 (5-
95 percent). Due to most of the learning algorithms design
principle of minimizing the overall error rate to which the
minority class contributes less, they always perform poorly in
problems with imbalanced data set.

This paper can be organized as follows. We will ﬁrst
describe and preprocess the data set in sections II and III.
Then, we will brieﬂy introduce several types of models such
as single classiﬁers described as well as ensemble models in
section IV. In section V, we will present the prediction results
for these models. At last we will summarize our work and
draw conclusions in section VI.

II. DATA DESCRIPTION

The data comes from Amazon Inc. collected from 2010-
2011 (published on Kaggle platform) with training set of
32769 samples and testing of 58922 samples. As shown in
table I, each of the data samples has one label attribute
called “ACTION”, where value “1” indicates this application is
approved and “0” indicates rejection. In addition, each samples
has eight features, which basically indicates different role or
group of one employee at Amazon.

TABLE I.

DATA FEATURE DESCRIPTION

Feature Name

ACTION

RESOURCE

MGR ID

ROLE ROLLUP 1

ROLE ROLLUP 2

ROLE DEPTNAME

ROLE TITLE

ROLE FAMILY

Feature Meaning

“1”: approved; “0”: rejected.

Resource ID

ID of the employee’s manager
Company role category ID1

(e.g. US Engineering)

Company role category ID2

(US Retail)

Department description
Business title description

Role family description
(e.g. Retail Manager)

(e.g. Manager)

ROLE FAMILY DESC

Role family extended description

ROLE CODE

Unique ID for each company role

As for the evaluation metric, Receiver Operating Character-
istic (ROC) curve is used to summarize classiﬁer performance
over tradeoffs between true positive and false positive error
rates. And we use Area Under the ROC Curve (AUC) as a
useful performance metric for imbalance classing problems.

III. DATA PREPROCESSING

In this section, we did three parts of work, which are:
balancing the dataset, one-hot encoding and feature selection.

A. Imbalanced dataset

As we described above, the given dataset is extremely
imbalanced with the number of one class signiﬁcantly lower
than the other. In real life, many issues can be described as
imbalanced classiﬁcation problem, such as medical diagnose,
text categorization, online resources management and so on.
Recently, the main approaches to solve the imbalanced
classiﬁcation is trying to balance the distribution between

TABLE II.

ONE-HOT ENCODING EXAMPLE

Category Data

119433
118321
118278

One-hot encoded eata

001
010
100

TABLE III.

FEATURE FREQUENCY VARIANCE

Feature Index
Variance(*106)
Feature Index
Variance(*106)

F1

0.0003

F6

F2

0.0001

F7

F3

3.5665

F8

F4

0.3191

F9

0.1244

0.0220

2.0396

0.1244

F5

0.0137

minority and majority classes in training set [1],[2] to make
the dataset suitable for standard machine learning models.
These techniques includes oversampling, undersampling [1]
and Synthetic Minority Over-sampling Technique (SMOTE)
[2].

1) Random oversampling and undersampling: Oversam-
pling typically refers to balance the data distribution by
sampling the minority class data with replacement. On the
other hand, undersampling changes the distribution of data by
randomly removing the data in majority class. Although the
performance will be improved by the above sampling methods,
the shortcomings of oversampling and undersampling are obvi-
ous. Oversampling will result in overﬁtting, and undersampling
may loss the importance information from dataset.

2) SMOTE: SMOTE is an oversampling method, which
will generate synthetic training samples [2] instead of remov-
ing or duplicating raw data. The basic idea of SMOTE is that
for each minority class sample, we create a synthetic example
from some of the k nearest neighbors of that sample. Based
on the number of new samples we need, we will randomly
choose some neighbors among all the nearest neighbors. This
process can be interpreted as choosing a random point in the
line between two feature vectors as our new samples.

B. One-hot encoding

Since the original features are discrete category values
which indicate different types. We can not directly apply linear
classiﬁer models on this kind of data. In order to apply linear
classiﬁer on this data, we need to use one-hot encoding.

One-hot encoding refers to bits that only have one single
active 1 while all remaining bits are inactive 0s. In the
given Amazon data, one feature may include multiple discrete
categorical values. In order to apply linear classiﬁer, it is
necessary to separate those values with only one active at a
time. Table II shows the example with one hot encoding on
a feature with three categories, which will be encoded into
“001”, “010” and “100” separately. We see that this encoding
method will expand the feature space from one to ﬁve. Sine
most of the sample values are “0”s, we use sparse matrix to
represent the newly encoded feature space.

C. Feature selection

Removing features with low variance is a common ap-
proach in feature selection [3]. In this work, we calculate the

(cid:88)

sj

V ar(Fi) =

(cid:18)

(cid:19)2

frequency variance of each feature based on Eq. (1):

f req(sj)

sum

f req(sj) − sum
|Fi,uniq|

(1)

where sj ∈ Fi,uniq, and Fi is the ith feature and Fi,uniq is the
set with all the unique feature values of Fi. Function f req(sj)
indicates the frequency of sj. |Fi,uniq| is the size of Fi,uniq,
i.e. the unique value number of feature Fi. In addition, sum =

f req(sj), which is the frequency sum of each sj.

(cid:80)

sj

Table III shows the analysis result of frequency variance
of all nine features, from which we can see that feature 1 and
feature 2 have obviously small variance. Hence, we remove
those two features in the further analysis steps.

IV. PREDICTIONS MODELS

A. Single classiﬁer

In this section, we brieﬂy introduce some single classi-
ﬁcation models we applied for initial testing. Here, “single”
corresponds to the “Ensembling” in the next section. They are
commonly used, thus we do not introduce the details here.

1) Naive Bayes: In this project problem, we treat all the
features to be mutually independent. This “naive” indepen-
dence assumption allows us to apply Naive Bayes algorithm in
the given categorical data. Also, laplace smoothing is applied
to those features never seen in the training. Thus, one sample
is labelled with 1 when P (1|F1...FN ) > P (0|F1...FN ) and
vice versa.

2) K-NN: K-NN algorithm is also suitable for categorical
data. It will label testing samples based on their nearest K
neighbors in the training set. Since the given data is categorical,
we use hamming distance instead of educlidean distance.
Smaller hamming distance between test sample and speciﬁc
training sample indicates they two are closer in the space.
Besides, we combine the labels of K neighbors based on their
hamming distance, closer neighbor will own higher inﬂuence.
3) Decision Tree: Decision tree builds classiﬁcation mod-
els in the form of a tree structure. It breaks down a dataset
into smaller subsets based on its features while an associated
decision tree is incrementally developed. The ﬁnal result is a
tree with decision nodes and leaf nodes. A leaf node represents
a classiﬁcation or decision.

4) Logistic Regression: Logistic regression is a commonly
used linear classiﬁer, which is simple but relatively efﬁcient.

B. Ensemble Methods

Ensemble method is a type of supervised learning, which
employs a set of classiﬁers and their decisions are combined in
certain way. Compared with single classiﬁer, in the ensemble
model, the output (decisions) depends on the vote of all the
individual classiﬁers. Generally, the ensemble model contain-
ing bagging is also known as bootstrap aggregating [4] and
boosting [6].

The main idea of bagging is that we generate several new
training data set by randomly uniform sampling the original
data set with replacement. Then use each new dataset to train

a model, and the output equals to the dominate vote by all
the trained models. Typically, we choose decision tree as each
training model in many practical problems.

Boosting is an alternative ensemble method in machine
learning. Compared with bagging, the basic rule of boosting
is that by combining a set of weak learner properly, we can
obtain a strong learner. The boosting algorithms are typically
performed iteratively [6]. In each iteration, we add a new weak
learner to the set of learners.

Among several boosting algorithms, the adaptive boosting
(Ada Boosting) [6] and gradient boosting [7] are most popular.
In the subsection IV-B1, we will brieﬂy describe the
random forest method, which is developed based on bagging
and will be adopted in our paper. Then, in subsection IV-B2,
we will introduce the gradient boosting model in detail. We
employ the above two models to solve our problem.

1) Random Forest: Random forest [5] combines the meth-
ods of bagging with random subspace. The main idea is that
we build a set of decision trees not only depends on the
random sampling of training data, but also randomly selecting
the features when we building each tree. In prediction, we
will takes all the decisions made by each tree into account
and select the majority result as our prediction. In [5], each
tree in the forest can be generated in the following three rules:
First, randomly choose samples with replacement from original
data set to form a new data set (same as bagging). Second,
randomly choose a subset of features for each tree. And we
split the nodes during building the tree based on this subset of
features. The last is building the tree without pruning.

In [5], it reveals the fact that random forest model is able
to prevent overﬁtting. That is as the increase of tree number,
the generalization error will converge to an upper bound,

Pe ≤ ρ(1 − s2)/s2

(2)
where Pe is the generalization error. ρ as the mean value of
correlations and s is deﬁned as the classiﬁer set strength.

Based on Eq. (2) we ﬁnd that the performance of random
forest depends on the correlation between each tree. The
performance will degrades as the increase of correlations. And
the author of [5] also shows that the correlation of trees is
mainly determined by the number of the features we selected
when we split the nodes. The less number of features we
choose, the trees will become more uncorrelated, and then the
generalization error will decrease. The authors suggest that,
we choose the
N as the number of features we selected to
build the tree, with N as the number of features in training
dataset.

√

2) Gradient Boosting: The gradient boosting [7] is a type
of boosting algorithms. Boosting process can be described as
Fi = Fi−1 + γihi(x; ai) where Fi is the learner in the i th
iteration which may be a poor learner at this step. γi is a
parameter to weight the new estimator hi(x; ai), and ai is the
parameter in function h. In gradient boosting, the parameters
γi and ai are determined by minimizing the cost functions in
current iteration,

(γi, ai) = arg min
γ,a

L(yj, Fi−1(xj) + γhi(xj; a))

(3)

m(cid:88)

j=1

where xj, yj are the j th training samples with j = 1,··· , m.
h(x; a) is a simple functions added in the i th iteration
with a as the parameters to be determined. L(yj, Fi−1(xj) +
γhi(xj; a)) is the cost function between the output yj and the
model in the i th iteration which is Fi(xj).

Solving Eq. (3) consists of two steps. The ﬁrst step is

determining ai follows the expression as

ai = arg min
ai,ρ

ri,j − ρh(xj; a)

(4)

m(cid:88)

j=1

m(cid:88)

j=1

with rj,i is the pseudo residuals with details in [7].

After ﬁnding the current estimator h(x; ai), we can further

solve γi by,

γi = arg min

γ

L(yj, Fi−1(xj) + γhi(xj; ai))

(5)

From Eq. (5), [7] points out that in the case of cost function is
convex, the gradient boosting is actually equivalent to gradient
descent in function space.

Based on the description above, the most signiﬁcant param-
eters in gradient boosting are the selection of weak learners as
well as the cost function. There are several candidates for cost
function such as least squares and exponential functions. For
week learner, the most popular choice is decision tree, where
the gradient boosting model is often called gradient boosted
decision trees.

In the following section, we will discuss how to implement

the gradient boosting to solve our problem.

V. PREDICTION RESULT

In this section, we will show the simulation results with
different prediction models. Firstly, we try single predication
models which can be applied directly on categorical data like
Naive Bayes, K-NN and decision tree. Secondly, since we
found out that single classiﬁer does not well on the given data,
we further try ensemble models including random forest and
gradient boosting based on decision tree. In addition, we show
the AUC performance of logistic regression with one hot en-
coded data, which veriﬁes the advantage of one hot encoding.
In the ﬁnal, we combine the three best prediction results from
random forest, gradient boosting and one-hot encoding based
logistic regression to obtain further performance improvement.

A. Single Model with Categorical Data

In this subsection, we present the prediction results based
on different single models. In addition, different dataset with
raw data, under-sampling, over-sampling and SMOTE are also
taken into consideration for comparision.

Table IV shows the prediction results of Naive Bayes, KNN
and Decision Tree. We can see that since the data size is
big enough, the difference between training and test AUC
is generally quite small. For different sampling method, we
see that direct prediction on raw data can not work well
due to the imbalanced data type. While under-sampling and
SMOTE generally work better than over-sampling method,
which mathes the conclusion drawn by [1].

Fig. 1. The training and testing AUC for various tree numbers.

Fig. 2. The training and testing AUC for various training data set with tree
number as 200.

Another important information conveyed by Table IV is
that since the best test AUC performance is around 0.73 here,
we may conclude that one single prediction model can not
work well on the given problem and dataset. This makes us to
think about using some upgraded ensembel method.

TABLE IV.

AUC OF NAIVE BAYES, K-NN AND DECISION TREE

Training AUC

Test AUC

Training AUC

Test AUC

Training AUC

Test AUC

Raw Data

0.6186
0.6027

0.5971
0.5731

0.6538
0.6235

Under-sampling
Naive Bayes

Decision Tree

0.6926
0.6893
K-NN
0.6469
0.6365

0.7283
0.6963

Over-sampling

SMOTE

0.6497
0.6512

0.6205
0.6109

0.6870
0.6687

0.7037
0.7019

0.6590
0.6620

0.7323
0.7338

B. Ensemble model

1) Random forest: First, we will adopt the random forest
model for prediction. As suggested in [5], the number of
features we random selected for each tree is the root square
of the total number of features. We use CART trees without
prune, which is also implied in [5].

First we choose different number of trees to build our
model. The training and testing AUC for different tree numbers
from 10 to 300 are depicted in Fig. 1. The AUC for training
data is the average AUC calculated from 10-fold cross valida-
tion from training data.

Fig. 1 demonstrates that as the increase of tree number, the
performance of testing AUC gradually converges and does not

Fig. 3. The training AUC as a function of maximum depth of decision tree
with 500 iterations.

degrade too much as the tree number increases, i.e., the overﬁt
can be prevented. These results match with the conclusion we
described in subsection IV-B1.

From Fig. 1, we can observe that 200 is a good choice for
the tree numbers, which will not degrade the AUC performance
compared with the cases of less trees. On the other hand, it is
also not necessary to choose too large number of trees. The
time complexity will increase but the performance will change
sightly. Therefore we choose the tree number as 200 in our
further prediction.

In Fig. 2, we plot the learning curve characterized by AUC
versus the size of training data set. For the training AUC, we
use 10-fold cross validation to calculate the average AUC for
each size of training data. The learning curve implies that as
we enlarge the training data set, the training and testing AUC
converge to an expected value 0.8558. This means that random
forest is a good model for this problem and no overﬁtting
yields.

2) Gradient boosting: In this subsection, we will discuss
the results of gradient boosting. We choose the decision tree
as weak learner. Recall the discussion in subsection IV-B2,
we should select the parameters in the tree to specify the
weak learner. Typically there does not exit a general method
to determine the parameters such as maximum depth and
minimum sample splits in the decision tree. They depends on
the practical problems. In [7], the authors suggest that the depth
of tree is more important than other parameters.

Therefore, we go through several maximum depth from
2 to 15 over 500 iterations. Note that we have test that for
this speciﬁc data, after 500 iterations,
the performance of
model will converge. The training AUC based on 10-fold cross
validation for each maximum depth has been plotted in Fig.
3. Fig. 3 indicates that the optimal depth locates at 6. Next,
we consider the training and testing AUC based on the weak
learner we just found. By increasing the size of training data,
the training AUC based on 10-fold cross validation and the
testing AUC are illustrated in Fig. 4. Similar with other models
we have implemented. The AUC curves will converge to an
expected value (AUC=0.8514 for full training data set).

Finally,

let us compare the results from two ensemble
models: random forest and gradient boosting. We ﬁnd that
for large training data size, the performance of random forest
are competitive with gradient boosting, with testing AUC as

0501001502002503000.770.780.790.80.810.820.830.840.850.860.87Tree numberArea under curve (AUC)AUC performance of random forest  Test AUCTraining AUC0.511.522.53x 1040.650.70.750.80.850.9AUC curve of random forest with tree number equals 200Training data sizeArea under curve (AUC)  Training AUCTesting AUC2468101214160.760.770.780.790.80.810.820.830.840.850.86The maximum depth of decision tree as weak learnerArea under curve (AUC)The training AUC as a function of maximum depth of decision tree    Training AUCFig. 4. The training and testing AUC with gradient boosting with maximum
depth as 6 and 500 iterations.

Fig. 5. AUC of logistic regression with one-hot encoding

0.8558 for random forest and 0.8514 for gradient boosting.
This conclusion matches the experimental results from [5]
where the authors conclude that the performance of random
forest will be competitive as boosting. However, from the view
of practical problems, the random forest will beat the gradient
boosting in some aspects. First, the random forest is more
ﬂexible since we do not need to select the weak learner which
will cost additional time in boosting. Second, the random forest
runs much faster than gradient boosting especially when weak
learner depth is large. Therefore, to make our model more
ﬂexible and efﬁcient, random forest is a better choice.

C. Linear classiﬁer with one-hot encoding

In section III, we mentioned that linear classiﬁer can not be
applied directly to the raw data due to the categorical feature
values (with AUC for SVM around 0.5). In this subsection,
we apply one-hot encoding to transform the raw feature into
binary feature values and thus expand the feature space. After
one-hot encoding, with only one unique feature value active
in each column, we can apply linear classiﬁer in new feature
space.

Here, we choose the logistic regression to ﬁt the model.
Fig. 5 shows the prediction result of logistic regression with
one-hot encoding. Different data size is chosen here to show
the difference between training and test AUC performance.
From Fig.5, we can see that after one-hot encoding,
the
best test AUC value can achieve 0.8721, which shows the
great advantage of one-hot encoding. Also, note that with
training data size increase training and test AUC performance
converges to very close values.

D. Further Ensemble

Until now, we have three relatively better prediction results,
which are: random forest (test AUC = 0.8558 with tree number
of 100), gradient boosting (test AUC = 0.8514) and logistic
regression with one-hot encoding (test AUC = 0.8721). In this
subsection, we will ensemble those different prediction models
for further performance imporvement.

Roughly consider the above three models are mutually
independent, and since they have similar AUC performance,
let’s roughly take their test error as ε. For three classiﬁers with
binary classiﬁcation, their ensemble model will make incorrect
prediction only when more than two of them are incorrect.

(6)

εensemble = C 2

3 ε2(1 − ε) + C 3

3 ε3 = 3ε2 − 2ε3

here, εensemble is the prediction error rate of ensemble model.
Solve inequation εensemble > ε and get ε > 0.5. Thus by
ensembling those three models, we generally get better result.
In this last step, we combine the prediction results of
random forest, gradient boosting and logistic regression with
one-hot encoding. By getting the weighted (training AUC as
the weight) average value of the three models, we ﬁnally
improve the test AUC result to 0.8903.

VI. CONCLUSION AND FUTURE WORK

In this work, based on the history data from Amazon
Inc. We built up an automatic system to review employee’s
application on resources. In the preprocessing, low variance
feature remove method is used for feature selection. We also
applied different sampling methods and one-hot encoding to
make the data balanced and suitable for linear classiﬁer. In
the prediction step, we tried single models on categorical
data, veriﬁed one-hot encoding with logistic regression. After
that, we also applied ensemble methods like random forest,
gradient boosting and best three ensemble to further improve
AUC. The AUC performance ﬁnally achieves 0.8903 from the
initial 0.7338 by single decision tree. In the future, we plan to
apply feature grouping strategy, use more linear models with
one-hot encoding data and try different boosting models for
performance improvement.

REFERENCES

[1] N. Japkowicz. “Class imbalance: Are we focusing on the right issue,” in
Proceedings of the ICML’03 Workshop on Learning from Imbalanced
Data Sets, 2003.

[2] N. Chawla, L. Hall, K. Bowyer, and W. Kegelmeyer. “SMOTE: Syn-
thetic Minority Oversampling Technique,” Journal of Artiﬁcial Intelli-
gence Research, vol. 16, pp. 321-357, 2002.
I. Guyon and A. Elisseeff, ”An introduction to variable and feature
selection,” Journal of Machine Learning Research, vol. 3, pp. 1157-
1182, 2003.

[3]

[4] L. Breiman, ”Bagging predictors,” Machine Learning, vol. 26, no. 2,

pp. 123-140, 1996

[5] L. Breiman. “Random forests,” Machine Learning, vol. 45, no. 1, pp. 5-

32, 2001.

[6] M. Collins, R. Schapire and Y. Singer, “Logistic regression, AdaBoost
and Bregman distances,” Machine Learning, vol. 48, no. 1-3, pp. 253-
285, 2002.
J. Friedman, “Stochastic gradient boosting,” Computational Statistics &
Data Analysis, vol. 38, no. 4 pp. 367-378, 2002.

[7]

00.511.522.533.5x 1040.650.70.750.80.850.9Training data sizeArea under curve (AUC)AUC curve with gradient boosting with maximum depth as 6 and 500 iterations  Training AUCTesting AUC00.511.522.533.5x 1040.50.550.60.650.70.750.80.850.90.951Training data sizeArea under curve (AUC)  AUC of logistic regression with one−hot encodingTraining AUCTest AUC