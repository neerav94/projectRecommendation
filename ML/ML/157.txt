Predicting Wine Varietals from Professional Reviews

By Ron Tidhar, Eli Ben-Joseph, Kate Willison

11th December 2015

CS 229 - Machine Learning: Final Project - Stanford University

Abstract

This paper outlines the construction of a wine varietal classiﬁcation engine. Through
use of topic analysis, word stemming and ﬁltering, a Na¨ıve Bayes classiﬁcation algorithm
performed with a surprising degree of accuracy. This research, therefore, represents exciting
ﬁrst steps in applying Machine Learning techniques to an area not well studied in traditional
research.

1

Introduction

While many of us enjoy a good glass of wine, it can
be diﬃcult at times to put a ﬁnger on what exactly
draws us to any particular bottle. Given the qualita-
tive breadth and scope of hundreds of diﬀerent wine
varietals1 - ranging from the full-bodied Petit-Sirah,
to the light and sweet Chenin-Blanc - it’s no wonder
that Sommeliers and laypeople alike have striven to
share their experiences through developing a common
vocabulary around the qualities and aromas they ﬁnd
in each glass.2

Although this vocabulary may be diﬃcult to nav-
igate for the uninitiated, among professional wine
reviews, one often ﬁnds distinct and recurring de-
scriptors for each varietal. As such, in the follow-
ing study, we aim to use data from a large sample
of professional reviews in combination with various
Machine Learning techniques to build a classiﬁcation
model for a number of common wine varietals. This
would not only enable categorization based on pro-
vided wine-tasting terms (which has applications for
recommender models and blind-tasting3 education),

1‘Varietal’ refers to the type of grape primarily used in mak-
ing a wine, so that a wine labeled as ‘Chardonnay’ must be
made from at least 75% Chardonnay grapes. This is in con-
trast to classiﬁcation systems used widely in Europe, whereby
blends are labeled by region rather than the grape variety (e.g.
Bordeaux will commonly be a blend of Merlot, Cabernet Franc
and Cabernet Sauvignon)

2For examples of common wine-descriptive words used by
reviewers, see well known critic Robert Parker’s wine glossary
- https://www.erobertparker.com/info/glossary.asp

3’Blind tasting’ is the practice of tasting a wine without
knowing any information about its origin, varietal or produc-
tion, with the goal of guessing each from the qualities of the
wine itself.

but would also allow one to relate similar wines to
one another.

2 Data

Data were scraped from http://www.klwines.com/
using a BeautifulSoup4-based python script between
10-30-2015 and 11-04-2015. For each of 35 wine styles
categorized by the site, data for at most 2000 unique
examples was collected,
including varietal, profes-
sional and nonprofessional reviews, name, country,
region, appellation, alcohol content and persistent
web address. While at most ﬁve reviews were col-
lected for each wine, a large portion of the dataset
had no associated reviews - these were removed from
the ﬁnal dataset. In total, therefore, 32,892 reviews
were collected for use in the analysis.

3 Modeling

In order to measure a baseline performance, a simple
multi-class one-against-all classiﬁcation model was
built. This model was implemented across all 35 wine
varietals using Vowpal Wabbit5. Words were tok-
enized and grouped across reviews for a given wine,
and analyzed as a simple “bag-of-words”. Training
80% of the data in a single pass with a logistic loss
function, the resulting model correctly classiﬁed 61%
of the wines in the test set - far better than the ap-
proximate prior of 1/35 = 2.85%.

4Richardson,

L.,

“Beautiful

Soup”,

Crummy,

http://www.crummy.com/software/BeautifulSoup/, 2015

5Langford, J., “Vowpal Wabbit”, Microsoft Research,

https://github.com/JohnLangford/vowpal wabbit/wiki

1

Predicting Wine Varietals from Professional Reviews

By Ron Tidhar, Eli Ben-Joseph, Kate Willison

11th December 2015

CS 229 - Machine Learning: Final Project - Stanford University

Abstract

This paper outlines the construction of a wine varietal classiﬁcation engine. Through
use of topic analysis, word stemming and ﬁltering, a Na¨ıve Bayes classiﬁcation algorithm
performed with a surprising degree of accuracy. This research, therefore, represents exciting
ﬁrst steps in applying Machine Learning techniques to an area not well studied in traditional
research.

1

Introduction

While many of us enjoy a good glass of wine, it can
be diﬃcult at times to put a ﬁnger on what exactly
draws us to any particular bottle. Given the qualita-
tive breadth and scope of hundreds of diﬀerent wine
varietals1 - ranging from the full-bodied Petit-Sirah,
to the light and sweet Chenin-Blanc - it’s no wonder
that Sommeliers and laypeople alike have striven to
share their experiences through developing a common
vocabulary around the qualities and aromas they ﬁnd
in each glass.2

Although this vocabulary may be diﬃcult to nav-
igate for the uninitiated, among professional wine
reviews, one often ﬁnds distinct and recurring de-
scriptors for each varietal. As such, in the follow-
ing study, we aim to use data from a large sample
of professional reviews in combination with various
Machine Learning techniques to build a classiﬁcation
model for a number of common wine varietals. This
would not only enable categorization based on pro-
vided wine-tasting terms (which has applications for
recommender models and blind-tasting3 education),

1‘Varietal’ refers to the type of grape primarily used in mak-
ing a wine, so that a wine labeled as ‘Chardonnay’ must be
made from at least 75% Chardonnay grapes. This is in con-
trast to classiﬁcation systems used widely in Europe, whereby
blends are labeled by region rather than the grape variety (e.g.
Bordeaux will commonly be a blend of Merlot, Cabernet Franc
and Cabernet Sauvignon)

2For examples of common wine-descriptive words used by
reviewers, see well known critic Robert Parker’s wine glossary
- https://www.erobertparker.com/info/glossary.asp

3’Blind tasting’ is the practice of tasting a wine without
knowing any information about its origin, varietal or produc-
tion, with the goal of guessing each from the qualities of the
wine itself.

but would also allow one to relate similar wines to
one another.

2 Data

Data were scraped from http://www.klwines.com/
using a BeautifulSoup4-based python script between
10-30-2015 and 11-04-2015. For each of 35 wine styles
categorized by the site, data for at most 2000 unique
examples was collected,
including varietal, profes-
sional and nonprofessional reviews, name, country,
region, appellation, alcohol content and persistent
web address. While at most ﬁve reviews were col-
lected for each wine, a large portion of the dataset
had no associated reviews - these were removed from
the ﬁnal dataset. In total, therefore, 32,892 reviews
were collected for use in the analysis.

3 Modeling

In order to measure a baseline performance, a simple
multi-class one-against-all classiﬁcation model was
built. This model was implemented across all 35 wine
varietals using Vowpal Wabbit5. Words were tok-
enized and grouped across reviews for a given wine,
and analyzed as a simple “bag-of-words”. Training
80% of the data in a single pass with a logistic loss
function, the resulting model correctly classiﬁed 61%
of the wines in the test set - far better than the ap-
proximate prior of 1/35 = 2.85%.

4Richardson,

L.,

“Beautiful

Soup”,

Crummy,

http://www.crummy.com/software/BeautifulSoup/, 2015

5Langford, J., “Vowpal Wabbit”, Microsoft Research,

https://github.com/JohnLangford/vowpal wabbit/wiki

1

Figure 2: Table showing a sample of the 30 top wine-related
words, as classiﬁed by the unsupervised LDA algorithm.

Figure 1: Top 10 wine varietals by number of collected re-
views.

Given that the basic premise of the study was
validated by this simple analysis, further model re-
ﬁnement and strengthening was sought. As a ﬁrst
step, a list of wine-review-speciﬁc stop words was
created. The words listed were ones that indicated
the varietal directly or indirectly (such as ‘Chardon-
nay’ or ‘chateau’), or else represented information
that wouldn’t be available to a blind-taster (e.g.
‘hectare’), and so were removed them from the data
set.

In order to counter the high variance demonstrated
in the initial learning curves, model simpliﬁcation
was implemented, through both class and feature
set (i.e., input words) reduction. This was accom-
plished by building a 20-category topic model using
Latent Dirichlet Allocation (LDA)6 in MALLET7 -
a Java-based package for statistical natural language
processing - re-estimating Dirichlet parameters every
10 iterations. Following this, calculations were made
for the cumulative probabilities of each word across
all wine-related categories, as deﬁned by the model
output. By stemming both the resulting word list,
as well as the words contained in the training data
(so that, for example, ‘spice’, ‘spicey’ and ‘spiciness’
would map to the same feature), it was possible to
then ﬁlter the training features. In addition, in the
ﬁnal model, only varietals for which there were at
least 200 reviews were included; a total of 23 pre-
dicted categories.

With such data treatment techniques, a simple
Na¨ıve Bayes classiﬁcation algorithm was run on the

6Blei, David M., Ng, Andrew Y., Jordan, Michael I (Jan-
uary 2003). Laﬀerty, John, ed. ”Latent Dirichlet allocation”.
Journal of Machine Learning Research 3 (4–5): pp. 993–1022.
doi:10.1162/jmlr.2003.3.4-5.993.

7McCallum, Andrew Kachites.

”MALLET: A Machine
Learning for Language Toolkit.” http://mallet.cs.umass.edu.
2002.

Figure 3: Learning curves for 500, 1000, 3000, and 5000
feature word-based models.

data. Cross validation was used to assess the opti-
mal number of word features (selected from the LDA
topic analysis) to be used in the model.

4 Results

Learning Curves:

Given that the initial model exhibited high variance,
one clear strategy for improvement was to reduce the
feature set. To test this, cross validation was used
to ﬁnd the most suitable number of word features.
The ranked list shown in Figure 2 was used to ﬁl-
ter four Na¨ıve Bayes bag-of-words models ﬁt using
ten-fold cross validation (10% holdout); one each for
the top 500, 1000, 3000 and 5000 words. Using a set
of learning curves resulting from this analysis, algo-
rithm success rates were compared. As can be seen
in Figure 3, using 5,000 of the top descriptive words
to train and test the model yielded the best results.
Discussion of this somewhat surprising result follows
later.

2

Predicting Wine Varietals from Professional Reviews

By Ron Tidhar, Eli Ben-Joseph, Kate Willison

11th December 2015

CS 229 - Machine Learning: Final Project - Stanford University

Abstract

This paper outlines the construction of a wine varietal classiﬁcation engine. Through
use of topic analysis, word stemming and ﬁltering, a Na¨ıve Bayes classiﬁcation algorithm
performed with a surprising degree of accuracy. This research, therefore, represents exciting
ﬁrst steps in applying Machine Learning techniques to an area not well studied in traditional
research.

1

Introduction

While many of us enjoy a good glass of wine, it can
be diﬃcult at times to put a ﬁnger on what exactly
draws us to any particular bottle. Given the qualita-
tive breadth and scope of hundreds of diﬀerent wine
varietals1 - ranging from the full-bodied Petit-Sirah,
to the light and sweet Chenin-Blanc - it’s no wonder
that Sommeliers and laypeople alike have striven to
share their experiences through developing a common
vocabulary around the qualities and aromas they ﬁnd
in each glass.2

Although this vocabulary may be diﬃcult to nav-
igate for the uninitiated, among professional wine
reviews, one often ﬁnds distinct and recurring de-
scriptors for each varietal. As such, in the follow-
ing study, we aim to use data from a large sample
of professional reviews in combination with various
Machine Learning techniques to build a classiﬁcation
model for a number of common wine varietals. This
would not only enable categorization based on pro-
vided wine-tasting terms (which has applications for
recommender models and blind-tasting3 education),

1‘Varietal’ refers to the type of grape primarily used in mak-
ing a wine, so that a wine labeled as ‘Chardonnay’ must be
made from at least 75% Chardonnay grapes. This is in con-
trast to classiﬁcation systems used widely in Europe, whereby
blends are labeled by region rather than the grape variety (e.g.
Bordeaux will commonly be a blend of Merlot, Cabernet Franc
and Cabernet Sauvignon)

2For examples of common wine-descriptive words used by
reviewers, see well known critic Robert Parker’s wine glossary
- https://www.erobertparker.com/info/glossary.asp

3’Blind tasting’ is the practice of tasting a wine without
knowing any information about its origin, varietal or produc-
tion, with the goal of guessing each from the qualities of the
wine itself.

but would also allow one to relate similar wines to
one another.

2 Data

Data were scraped from http://www.klwines.com/
using a BeautifulSoup4-based python script between
10-30-2015 and 11-04-2015. For each of 35 wine styles
categorized by the site, data for at most 2000 unique
examples was collected,
including varietal, profes-
sional and nonprofessional reviews, name, country,
region, appellation, alcohol content and persistent
web address. While at most ﬁve reviews were col-
lected for each wine, a large portion of the dataset
had no associated reviews - these were removed from
the ﬁnal dataset. In total, therefore, 32,892 reviews
were collected for use in the analysis.

3 Modeling

In order to measure a baseline performance, a simple
multi-class one-against-all classiﬁcation model was
built. This model was implemented across all 35 wine
varietals using Vowpal Wabbit5. Words were tok-
enized and grouped across reviews for a given wine,
and analyzed as a simple “bag-of-words”. Training
80% of the data in a single pass with a logistic loss
function, the resulting model correctly classiﬁed 61%
of the wines in the test set - far better than the ap-
proximate prior of 1/35 = 2.85%.

4Richardson,

L.,

“Beautiful

Soup”,

Crummy,

http://www.crummy.com/software/BeautifulSoup/, 2015

5Langford, J., “Vowpal Wabbit”, Microsoft Research,

https://github.com/JohnLangford/vowpal wabbit/wiki

1

Figure 2: Table showing a sample of the 30 top wine-related
words, as classiﬁed by the unsupervised LDA algorithm.

Figure 1: Top 10 wine varietals by number of collected re-
views.

Given that the basic premise of the study was
validated by this simple analysis, further model re-
ﬁnement and strengthening was sought. As a ﬁrst
step, a list of wine-review-speciﬁc stop words was
created. The words listed were ones that indicated
the varietal directly or indirectly (such as ‘Chardon-
nay’ or ‘chateau’), or else represented information
that wouldn’t be available to a blind-taster (e.g.
‘hectare’), and so were removed them from the data
set.

In order to counter the high variance demonstrated
in the initial learning curves, model simpliﬁcation
was implemented, through both class and feature
set (i.e., input words) reduction. This was accom-
plished by building a 20-category topic model using
Latent Dirichlet Allocation (LDA)6 in MALLET7 -
a Java-based package for statistical natural language
processing - re-estimating Dirichlet parameters every
10 iterations. Following this, calculations were made
for the cumulative probabilities of each word across
all wine-related categories, as deﬁned by the model
output. By stemming both the resulting word list,
as well as the words contained in the training data
(so that, for example, ‘spice’, ‘spicey’ and ‘spiciness’
would map to the same feature), it was possible to
then ﬁlter the training features. In addition, in the
ﬁnal model, only varietals for which there were at
least 200 reviews were included; a total of 23 pre-
dicted categories.

With such data treatment techniques, a simple
Na¨ıve Bayes classiﬁcation algorithm was run on the

6Blei, David M., Ng, Andrew Y., Jordan, Michael I (Jan-
uary 2003). Laﬀerty, John, ed. ”Latent Dirichlet allocation”.
Journal of Machine Learning Research 3 (4–5): pp. 993–1022.
doi:10.1162/jmlr.2003.3.4-5.993.

7McCallum, Andrew Kachites.

”MALLET: A Machine
Learning for Language Toolkit.” http://mallet.cs.umass.edu.
2002.

Figure 3: Learning curves for 500, 1000, 3000, and 5000
feature word-based models.

data. Cross validation was used to assess the opti-
mal number of word features (selected from the LDA
topic analysis) to be used in the model.

4 Results

Learning Curves:

Given that the initial model exhibited high variance,
one clear strategy for improvement was to reduce the
feature set. To test this, cross validation was used
to ﬁnd the most suitable number of word features.
The ranked list shown in Figure 2 was used to ﬁl-
ter four Na¨ıve Bayes bag-of-words models ﬁt using
ten-fold cross validation (10% holdout); one each for
the top 500, 1000, 3000 and 5000 words. Using a set
of learning curves resulting from this analysis, algo-
rithm success rates were compared. As can be seen
in Figure 3, using 5,000 of the top descriptive words
to train and test the model yielded the best results.
Discussion of this somewhat surprising result follows
later.

2

Figure 5 demonstrates an example of this with a sub-
set of topics from the LDA analysis.

5 Applications

There are various applications of the wine-classiﬁer
model outside the realm of academia. The most
straightforward application is a simple wine recom-
mender: given a set of descriptors that represent one’s
general tastes (in terms of ﬂavors, textures, and aro-
mas), the model can recommend wines that best ﬁt
that proﬁle in a rank-ordered list (see Figure 6 for an
example of this in action).

This would allow someone to consider and gain ex-
posure to wines which they may not otherwise have
been acquainted. For many who are interested but
new to the world of wine, understanding the nuances
in tastes and aromas can seem like a daunting task
that presents a barrier to enjoying wine to its fullest.
Another application of the model is as a tool for
blind tasting. The wine classiﬁer could serve as a
decision guide: as the user inputs more descriptors,
the model would update the likely matches and use
the coeﬃcients to provide some motivation for why
a particular varietal is likely. Though the model is
not a perfect predictor, this would nevertheless be a
valuable educational tool.

Lastly, as many of the reviews analyzed also con-
tained recommended food pairings, the wine classi-
ﬁer model could be modiﬁed to recommend wine-
food pairings. During data preprocessing for our
main model, these food mentions were ﬁltered out,
as the associated words were relatively uncommon
(and therefore did not make the 5,000-word cut). By
modifying the feature inputs to include food-related
words, it would be possible to build a model that
would recommend top food pairings for a given vari-
etal. This could be useful for anyone looking to pair
a wine with a nice meal or vice versa, including both
restaurants and home chefs.

6 Conclusions

and

Future

Work

With a ﬁnal classiﬁcation accuracy of 68%, it is clear
that there is still room for improvement. The learn-
ing curve for the 5,000 word feature model indicates
a large separation between the training and testing
error rates. Given a desired performance on the or-
der of 80-90%, the curve implies that the model still
exhibits a high degree of overﬁtting (i.e., variance).
To rectify this problem, two strategies may help.

3

Figure 4: Test set confusion matrix. Rows represent true
varietal value, while columns correspond to the predicted cat-
egory for each example: cell values represent the total count
of examples that correspond to each true/predicted value.

Model quality:

The confusion matrix is useful to further assess the
performance of the most accurate model. In so doing,
it can be observed which varietals are most accurately
classiﬁed (i.e. have a higher proportion of their row
sum in the cell along the matrix diagonal), as well as
which varietals they are most often misclassiﬁed as
(cells that lie outside of the matrix diagonal).

The confusion matrix shows us that those wines
that are misclassiﬁed are more often than not as-
signed to a varietal that is descriptively similar. For
example: in the test set, Riesling is classiﬁed correctly
77% of the time (34/44), but misclassiﬁed as Sauvi-
gnon Blanc in 14% of examples. This is fairly under-
standable, as both are pale green-gold wines, mostly
unoaked, and often not very high alcohol, with (de-
pending where they are grown) ‘green’ fruit ﬂavors
and high acidity.89 Given this perspective, the 68%
accuracy that the model achieves on the test set is all
the more impressive.

Varietal proximity:

Another insight aﬀorded by the LDA topic analysis
involves varietals’ characteristic proximity to one an-
other: because the algorithm is unsupervised, topics
are each generally mapped to reviews of wines from
more than one varietal. Intuitively, a topic composed
primarily of two varietals indicates that those vari-
etals are likely similar in the dimensions indicated by
the highly-weighted words associated with that topic.

8Gregutt, Paul.

“White Wine Basics” Wine Enthusi-
ast (2011) http://www.winemag.com/2011/03/16/white-wine-
basics/

9Laube,

James;

Molesworth,

“Va-
rietal
(1996)
http://www.winespectator.com/webfeature/show/id/Varietal-
Characteristics 1001

James.
Spectator

Characteristics”

Wine

Predicting Wine Varietals from Professional Reviews

By Ron Tidhar, Eli Ben-Joseph, Kate Willison

11th December 2015

CS 229 - Machine Learning: Final Project - Stanford University

Abstract

This paper outlines the construction of a wine varietal classiﬁcation engine. Through
use of topic analysis, word stemming and ﬁltering, a Na¨ıve Bayes classiﬁcation algorithm
performed with a surprising degree of accuracy. This research, therefore, represents exciting
ﬁrst steps in applying Machine Learning techniques to an area not well studied in traditional
research.

1

Introduction

While many of us enjoy a good glass of wine, it can
be diﬃcult at times to put a ﬁnger on what exactly
draws us to any particular bottle. Given the qualita-
tive breadth and scope of hundreds of diﬀerent wine
varietals1 - ranging from the full-bodied Petit-Sirah,
to the light and sweet Chenin-Blanc - it’s no wonder
that Sommeliers and laypeople alike have striven to
share their experiences through developing a common
vocabulary around the qualities and aromas they ﬁnd
in each glass.2

Although this vocabulary may be diﬃcult to nav-
igate for the uninitiated, among professional wine
reviews, one often ﬁnds distinct and recurring de-
scriptors for each varietal. As such, in the follow-
ing study, we aim to use data from a large sample
of professional reviews in combination with various
Machine Learning techniques to build a classiﬁcation
model for a number of common wine varietals. This
would not only enable categorization based on pro-
vided wine-tasting terms (which has applications for
recommender models and blind-tasting3 education),

1‘Varietal’ refers to the type of grape primarily used in mak-
ing a wine, so that a wine labeled as ‘Chardonnay’ must be
made from at least 75% Chardonnay grapes. This is in con-
trast to classiﬁcation systems used widely in Europe, whereby
blends are labeled by region rather than the grape variety (e.g.
Bordeaux will commonly be a blend of Merlot, Cabernet Franc
and Cabernet Sauvignon)

2For examples of common wine-descriptive words used by
reviewers, see well known critic Robert Parker’s wine glossary
- https://www.erobertparker.com/info/glossary.asp

3’Blind tasting’ is the practice of tasting a wine without
knowing any information about its origin, varietal or produc-
tion, with the goal of guessing each from the qualities of the
wine itself.

but would also allow one to relate similar wines to
one another.

2 Data

Data were scraped from http://www.klwines.com/
using a BeautifulSoup4-based python script between
10-30-2015 and 11-04-2015. For each of 35 wine styles
categorized by the site, data for at most 2000 unique
examples was collected,
including varietal, profes-
sional and nonprofessional reviews, name, country,
region, appellation, alcohol content and persistent
web address. While at most ﬁve reviews were col-
lected for each wine, a large portion of the dataset
had no associated reviews - these were removed from
the ﬁnal dataset. In total, therefore, 32,892 reviews
were collected for use in the analysis.

3 Modeling

In order to measure a baseline performance, a simple
multi-class one-against-all classiﬁcation model was
built. This model was implemented across all 35 wine
varietals using Vowpal Wabbit5. Words were tok-
enized and grouped across reviews for a given wine,
and analyzed as a simple “bag-of-words”. Training
80% of the data in a single pass with a logistic loss
function, the resulting model correctly classiﬁed 61%
of the wines in the test set - far better than the ap-
proximate prior of 1/35 = 2.85%.

4Richardson,

L.,

“Beautiful

Soup”,

Crummy,

http://www.crummy.com/software/BeautifulSoup/, 2015

5Langford, J., “Vowpal Wabbit”, Microsoft Research,

https://github.com/JohnLangford/vowpal wabbit/wiki

1

Figure 2: Table showing a sample of the 30 top wine-related
words, as classiﬁed by the unsupervised LDA algorithm.

Figure 1: Top 10 wine varietals by number of collected re-
views.

Given that the basic premise of the study was
validated by this simple analysis, further model re-
ﬁnement and strengthening was sought. As a ﬁrst
step, a list of wine-review-speciﬁc stop words was
created. The words listed were ones that indicated
the varietal directly or indirectly (such as ‘Chardon-
nay’ or ‘chateau’), or else represented information
that wouldn’t be available to a blind-taster (e.g.
‘hectare’), and so were removed them from the data
set.

In order to counter the high variance demonstrated
in the initial learning curves, model simpliﬁcation
was implemented, through both class and feature
set (i.e., input words) reduction. This was accom-
plished by building a 20-category topic model using
Latent Dirichlet Allocation (LDA)6 in MALLET7 -
a Java-based package for statistical natural language
processing - re-estimating Dirichlet parameters every
10 iterations. Following this, calculations were made
for the cumulative probabilities of each word across
all wine-related categories, as deﬁned by the model
output. By stemming both the resulting word list,
as well as the words contained in the training data
(so that, for example, ‘spice’, ‘spicey’ and ‘spiciness’
would map to the same feature), it was possible to
then ﬁlter the training features. In addition, in the
ﬁnal model, only varietals for which there were at
least 200 reviews were included; a total of 23 pre-
dicted categories.

With such data treatment techniques, a simple
Na¨ıve Bayes classiﬁcation algorithm was run on the

6Blei, David M., Ng, Andrew Y., Jordan, Michael I (Jan-
uary 2003). Laﬀerty, John, ed. ”Latent Dirichlet allocation”.
Journal of Machine Learning Research 3 (4–5): pp. 993–1022.
doi:10.1162/jmlr.2003.3.4-5.993.

7McCallum, Andrew Kachites.

”MALLET: A Machine
Learning for Language Toolkit.” http://mallet.cs.umass.edu.
2002.

Figure 3: Learning curves for 500, 1000, 3000, and 5000
feature word-based models.

data. Cross validation was used to assess the opti-
mal number of word features (selected from the LDA
topic analysis) to be used in the model.

4 Results

Learning Curves:

Given that the initial model exhibited high variance,
one clear strategy for improvement was to reduce the
feature set. To test this, cross validation was used
to ﬁnd the most suitable number of word features.
The ranked list shown in Figure 2 was used to ﬁl-
ter four Na¨ıve Bayes bag-of-words models ﬁt using
ten-fold cross validation (10% holdout); one each for
the top 500, 1000, 3000 and 5000 words. Using a set
of learning curves resulting from this analysis, algo-
rithm success rates were compared. As can be seen
in Figure 3, using 5,000 of the top descriptive words
to train and test the model yielded the best results.
Discussion of this somewhat surprising result follows
later.

2

Figure 5 demonstrates an example of this with a sub-
set of topics from the LDA analysis.

5 Applications

There are various applications of the wine-classiﬁer
model outside the realm of academia. The most
straightforward application is a simple wine recom-
mender: given a set of descriptors that represent one’s
general tastes (in terms of ﬂavors, textures, and aro-
mas), the model can recommend wines that best ﬁt
that proﬁle in a rank-ordered list (see Figure 6 for an
example of this in action).

This would allow someone to consider and gain ex-
posure to wines which they may not otherwise have
been acquainted. For many who are interested but
new to the world of wine, understanding the nuances
in tastes and aromas can seem like a daunting task
that presents a barrier to enjoying wine to its fullest.
Another application of the model is as a tool for
blind tasting. The wine classiﬁer could serve as a
decision guide: as the user inputs more descriptors,
the model would update the likely matches and use
the coeﬃcients to provide some motivation for why
a particular varietal is likely. Though the model is
not a perfect predictor, this would nevertheless be a
valuable educational tool.

Lastly, as many of the reviews analyzed also con-
tained recommended food pairings, the wine classi-
ﬁer model could be modiﬁed to recommend wine-
food pairings. During data preprocessing for our
main model, these food mentions were ﬁltered out,
as the associated words were relatively uncommon
(and therefore did not make the 5,000-word cut). By
modifying the feature inputs to include food-related
words, it would be possible to build a model that
would recommend top food pairings for a given vari-
etal. This could be useful for anyone looking to pair
a wine with a nice meal or vice versa, including both
restaurants and home chefs.

6 Conclusions

and

Future

Work

With a ﬁnal classiﬁcation accuracy of 68%, it is clear
that there is still room for improvement. The learn-
ing curve for the 5,000 word feature model indicates
a large separation between the training and testing
error rates. Given a desired performance on the or-
der of 80-90%, the curve implies that the model still
exhibits a high degree of overﬁtting (i.e., variance).
To rectify this problem, two strategies may help.

3

Figure 4: Test set confusion matrix. Rows represent true
varietal value, while columns correspond to the predicted cat-
egory for each example: cell values represent the total count
of examples that correspond to each true/predicted value.

Model quality:

The confusion matrix is useful to further assess the
performance of the most accurate model. In so doing,
it can be observed which varietals are most accurately
classiﬁed (i.e. have a higher proportion of their row
sum in the cell along the matrix diagonal), as well as
which varietals they are most often misclassiﬁed as
(cells that lie outside of the matrix diagonal).

The confusion matrix shows us that those wines
that are misclassiﬁed are more often than not as-
signed to a varietal that is descriptively similar. For
example: in the test set, Riesling is classiﬁed correctly
77% of the time (34/44), but misclassiﬁed as Sauvi-
gnon Blanc in 14% of examples. This is fairly under-
standable, as both are pale green-gold wines, mostly
unoaked, and often not very high alcohol, with (de-
pending where they are grown) ‘green’ fruit ﬂavors
and high acidity.89 Given this perspective, the 68%
accuracy that the model achieves on the test set is all
the more impressive.

Varietal proximity:

Another insight aﬀorded by the LDA topic analysis
involves varietals’ characteristic proximity to one an-
other: because the algorithm is unsupervised, topics
are each generally mapped to reviews of wines from
more than one varietal. Intuitively, a topic composed
primarily of two varietals indicates that those vari-
etals are likely similar in the dimensions indicated by
the highly-weighted words associated with that topic.

8Gregutt, Paul.

“White Wine Basics” Wine Enthusi-
ast (2011) http://www.winemag.com/2011/03/16/white-wine-
basics/

9Laube,

James;

Molesworth,

“Va-
rietal
(1996)
http://www.winespectator.com/webfeature/show/id/Varietal-
Characteristics 1001

James.
Spectator

Characteristics”

Wine

Figure 6: A sample output predicting what a user may enjoy
based on their input.

First, increasing the size of the training data set
will help reduce variance, and will serve to increase
model robustness. This is a relatively straightforward
improvement, and can be done by ﬁnding other re-
view sites for which scraping is permissible.

Secondly, reducing the size of the feature set (i.e.,
training on less words) is also likely to improve the
model. Though this was tried (and rebuﬀed) with the
cross-validation analysis, there are still some further
optimisations to be considered.

Many descriptive words used in wine tasting re-
quire qualiﬁers or modiﬁers in order to be most mean-
ingful. For example, while “acidity” may be picked
up as a feature, it is most descriptive with a modi-
ﬁer. The diﬀerence between a “high acidity” and “low
acidity” wine is signiﬁcant. This is most likely the
cause of the counter-intuitive cross validation analy-
sis. As a result, a selected bigram analysis may serve
to reduce the feature set, by allowing for a smaller
set of more descriptive features.

Ultimately, this paper presents promising ﬁrst
steps towards building a robust wine varietal classi-
ﬁcation engine. By implementing the suggested fur-
ther improvements, many of the useful applications
can easily be realised.

4

Figure 5: Subset of topics demonstrating result of LDA
analysis. An interactive version of this ﬁgure can be found at
http://web.stanford.edu/∼kawi/wine model/category vis.html

Predicting Wine Varietals from Professional Reviews

By Ron Tidhar, Eli Ben-Joseph, Kate Willison

11th December 2015

CS 229 - Machine Learning: Final Project - Stanford University

Abstract

This paper outlines the construction of a wine varietal classiﬁcation engine. Through
use of topic analysis, word stemming and ﬁltering, a Na¨ıve Bayes classiﬁcation algorithm
performed with a surprising degree of accuracy. This research, therefore, represents exciting
ﬁrst steps in applying Machine Learning techniques to an area not well studied in traditional
research.

1

Introduction

While many of us enjoy a good glass of wine, it can
be diﬃcult at times to put a ﬁnger on what exactly
draws us to any particular bottle. Given the qualita-
tive breadth and scope of hundreds of diﬀerent wine
varietals1 - ranging from the full-bodied Petit-Sirah,
to the light and sweet Chenin-Blanc - it’s no wonder
that Sommeliers and laypeople alike have striven to
share their experiences through developing a common
vocabulary around the qualities and aromas they ﬁnd
in each glass.2

Although this vocabulary may be diﬃcult to nav-
igate for the uninitiated, among professional wine
reviews, one often ﬁnds distinct and recurring de-
scriptors for each varietal. As such, in the follow-
ing study, we aim to use data from a large sample
of professional reviews in combination with various
Machine Learning techniques to build a classiﬁcation
model for a number of common wine varietals. This
would not only enable categorization based on pro-
vided wine-tasting terms (which has applications for
recommender models and blind-tasting3 education),

1‘Varietal’ refers to the type of grape primarily used in mak-
ing a wine, so that a wine labeled as ‘Chardonnay’ must be
made from at least 75% Chardonnay grapes. This is in con-
trast to classiﬁcation systems used widely in Europe, whereby
blends are labeled by region rather than the grape variety (e.g.
Bordeaux will commonly be a blend of Merlot, Cabernet Franc
and Cabernet Sauvignon)

2For examples of common wine-descriptive words used by
reviewers, see well known critic Robert Parker’s wine glossary
- https://www.erobertparker.com/info/glossary.asp

3’Blind tasting’ is the practice of tasting a wine without
knowing any information about its origin, varietal or produc-
tion, with the goal of guessing each from the qualities of the
wine itself.

but would also allow one to relate similar wines to
one another.

2 Data

Data were scraped from http://www.klwines.com/
using a BeautifulSoup4-based python script between
10-30-2015 and 11-04-2015. For each of 35 wine styles
categorized by the site, data for at most 2000 unique
examples was collected,
including varietal, profes-
sional and nonprofessional reviews, name, country,
region, appellation, alcohol content and persistent
web address. While at most ﬁve reviews were col-
lected for each wine, a large portion of the dataset
had no associated reviews - these were removed from
the ﬁnal dataset. In total, therefore, 32,892 reviews
were collected for use in the analysis.

3 Modeling

In order to measure a baseline performance, a simple
multi-class one-against-all classiﬁcation model was
built. This model was implemented across all 35 wine
varietals using Vowpal Wabbit5. Words were tok-
enized and grouped across reviews for a given wine,
and analyzed as a simple “bag-of-words”. Training
80% of the data in a single pass with a logistic loss
function, the resulting model correctly classiﬁed 61%
of the wines in the test set - far better than the ap-
proximate prior of 1/35 = 2.85%.

4Richardson,

L.,

“Beautiful

Soup”,

Crummy,

http://www.crummy.com/software/BeautifulSoup/, 2015

5Langford, J., “Vowpal Wabbit”, Microsoft Research,

https://github.com/JohnLangford/vowpal wabbit/wiki

1

Figure 2: Table showing a sample of the 30 top wine-related
words, as classiﬁed by the unsupervised LDA algorithm.

Figure 1: Top 10 wine varietals by number of collected re-
views.

Given that the basic premise of the study was
validated by this simple analysis, further model re-
ﬁnement and strengthening was sought. As a ﬁrst
step, a list of wine-review-speciﬁc stop words was
created. The words listed were ones that indicated
the varietal directly or indirectly (such as ‘Chardon-
nay’ or ‘chateau’), or else represented information
that wouldn’t be available to a blind-taster (e.g.
‘hectare’), and so were removed them from the data
set.

In order to counter the high variance demonstrated
in the initial learning curves, model simpliﬁcation
was implemented, through both class and feature
set (i.e., input words) reduction. This was accom-
plished by building a 20-category topic model using
Latent Dirichlet Allocation (LDA)6 in MALLET7 -
a Java-based package for statistical natural language
processing - re-estimating Dirichlet parameters every
10 iterations. Following this, calculations were made
for the cumulative probabilities of each word across
all wine-related categories, as deﬁned by the model
output. By stemming both the resulting word list,
as well as the words contained in the training data
(so that, for example, ‘spice’, ‘spicey’ and ‘spiciness’
would map to the same feature), it was possible to
then ﬁlter the training features. In addition, in the
ﬁnal model, only varietals for which there were at
least 200 reviews were included; a total of 23 pre-
dicted categories.

With such data treatment techniques, a simple
Na¨ıve Bayes classiﬁcation algorithm was run on the

6Blei, David M., Ng, Andrew Y., Jordan, Michael I (Jan-
uary 2003). Laﬀerty, John, ed. ”Latent Dirichlet allocation”.
Journal of Machine Learning Research 3 (4–5): pp. 993–1022.
doi:10.1162/jmlr.2003.3.4-5.993.

7McCallum, Andrew Kachites.

”MALLET: A Machine
Learning for Language Toolkit.” http://mallet.cs.umass.edu.
2002.

Figure 3: Learning curves for 500, 1000, 3000, and 5000
feature word-based models.

data. Cross validation was used to assess the opti-
mal number of word features (selected from the LDA
topic analysis) to be used in the model.

4 Results

Learning Curves:

Given that the initial model exhibited high variance,
one clear strategy for improvement was to reduce the
feature set. To test this, cross validation was used
to ﬁnd the most suitable number of word features.
The ranked list shown in Figure 2 was used to ﬁl-
ter four Na¨ıve Bayes bag-of-words models ﬁt using
ten-fold cross validation (10% holdout); one each for
the top 500, 1000, 3000 and 5000 words. Using a set
of learning curves resulting from this analysis, algo-
rithm success rates were compared. As can be seen
in Figure 3, using 5,000 of the top descriptive words
to train and test the model yielded the best results.
Discussion of this somewhat surprising result follows
later.

2

Figure 5 demonstrates an example of this with a sub-
set of topics from the LDA analysis.

5 Applications

There are various applications of the wine-classiﬁer
model outside the realm of academia. The most
straightforward application is a simple wine recom-
mender: given a set of descriptors that represent one’s
general tastes (in terms of ﬂavors, textures, and aro-
mas), the model can recommend wines that best ﬁt
that proﬁle in a rank-ordered list (see Figure 6 for an
example of this in action).

This would allow someone to consider and gain ex-
posure to wines which they may not otherwise have
been acquainted. For many who are interested but
new to the world of wine, understanding the nuances
in tastes and aromas can seem like a daunting task
that presents a barrier to enjoying wine to its fullest.
Another application of the model is as a tool for
blind tasting. The wine classiﬁer could serve as a
decision guide: as the user inputs more descriptors,
the model would update the likely matches and use
the coeﬃcients to provide some motivation for why
a particular varietal is likely. Though the model is
not a perfect predictor, this would nevertheless be a
valuable educational tool.

Lastly, as many of the reviews analyzed also con-
tained recommended food pairings, the wine classi-
ﬁer model could be modiﬁed to recommend wine-
food pairings. During data preprocessing for our
main model, these food mentions were ﬁltered out,
as the associated words were relatively uncommon
(and therefore did not make the 5,000-word cut). By
modifying the feature inputs to include food-related
words, it would be possible to build a model that
would recommend top food pairings for a given vari-
etal. This could be useful for anyone looking to pair
a wine with a nice meal or vice versa, including both
restaurants and home chefs.

6 Conclusions

and

Future

Work

With a ﬁnal classiﬁcation accuracy of 68%, it is clear
that there is still room for improvement. The learn-
ing curve for the 5,000 word feature model indicates
a large separation between the training and testing
error rates. Given a desired performance on the or-
der of 80-90%, the curve implies that the model still
exhibits a high degree of overﬁtting (i.e., variance).
To rectify this problem, two strategies may help.

3

Figure 4: Test set confusion matrix. Rows represent true
varietal value, while columns correspond to the predicted cat-
egory for each example: cell values represent the total count
of examples that correspond to each true/predicted value.

Model quality:

The confusion matrix is useful to further assess the
performance of the most accurate model. In so doing,
it can be observed which varietals are most accurately
classiﬁed (i.e. have a higher proportion of their row
sum in the cell along the matrix diagonal), as well as
which varietals they are most often misclassiﬁed as
(cells that lie outside of the matrix diagonal).

The confusion matrix shows us that those wines
that are misclassiﬁed are more often than not as-
signed to a varietal that is descriptively similar. For
example: in the test set, Riesling is classiﬁed correctly
77% of the time (34/44), but misclassiﬁed as Sauvi-
gnon Blanc in 14% of examples. This is fairly under-
standable, as both are pale green-gold wines, mostly
unoaked, and often not very high alcohol, with (de-
pending where they are grown) ‘green’ fruit ﬂavors
and high acidity.89 Given this perspective, the 68%
accuracy that the model achieves on the test set is all
the more impressive.

Varietal proximity:

Another insight aﬀorded by the LDA topic analysis
involves varietals’ characteristic proximity to one an-
other: because the algorithm is unsupervised, topics
are each generally mapped to reviews of wines from
more than one varietal. Intuitively, a topic composed
primarily of two varietals indicates that those vari-
etals are likely similar in the dimensions indicated by
the highly-weighted words associated with that topic.

8Gregutt, Paul.

“White Wine Basics” Wine Enthusi-
ast (2011) http://www.winemag.com/2011/03/16/white-wine-
basics/

9Laube,

James;

Molesworth,

“Va-
rietal
(1996)
http://www.winespectator.com/webfeature/show/id/Varietal-
Characteristics 1001

James.
Spectator

Characteristics”

Wine

Figure 6: A sample output predicting what a user may enjoy
based on their input.

First, increasing the size of the training data set
will help reduce variance, and will serve to increase
model robustness. This is a relatively straightforward
improvement, and can be done by ﬁnding other re-
view sites for which scraping is permissible.

Secondly, reducing the size of the feature set (i.e.,
training on less words) is also likely to improve the
model. Though this was tried (and rebuﬀed) with the
cross-validation analysis, there are still some further
optimisations to be considered.

Many descriptive words used in wine tasting re-
quire qualiﬁers or modiﬁers in order to be most mean-
ingful. For example, while “acidity” may be picked
up as a feature, it is most descriptive with a modi-
ﬁer. The diﬀerence between a “high acidity” and “low
acidity” wine is signiﬁcant. This is most likely the
cause of the counter-intuitive cross validation analy-
sis. As a result, a selected bigram analysis may serve
to reduce the feature set, by allowing for a smaller
set of more descriptive features.

Ultimately, this paper presents promising ﬁrst
steps towards building a robust wine varietal classi-
ﬁcation engine. By implementing the suggested fur-
ther improvements, many of the useful applications
can easily be realised.

4

Figure 5: Subset of topics demonstrating result of LDA
analysis. An interactive version of this ﬁgure can be found at
http://web.stanford.edu/∼kawi/wine model/category vis.html

7 Bibliography

Blei, David M., Ng, Andrew Y., Jordan, Michael I
(January 2003). Laﬀerty, John, ed. “Latent
Dirichlet allocation”. Journal of Machine Learning
Research 3 (4–5): pp. 993–1022.
doi:10.1162/jmlr.2003.3.4-5.993.

Gregutt, Paul. “White Wine Basics” Wine
Enthusiast (2011)
http://www.winemag.com/2011/03/16/white-wine-
basics/

Langford, J., “Vowpal Wabbit”, Microsoft Research,
https://github.com/JohnLangford/vowpal wabbit/wiki

Laube, James; Molesworth, James. “Varietal
Characteristics” Wine Spectator (1996)
http://www.winespectator.com/webfeature/show/id/Varietal-
Characteristics 1001

McCallum, Andrew Kachites. “MALLET: A
Machine Learning for Language Toolkit.”
http://mallet.cs.umass.edu. 2002.

Parker, R., “A Glossary of Wine Terms”,
eRobertParker.com,
https://www.erobertparker.com/info/glossary.asp

Richardson, L., “Beautiful Soup”, Crummy,
http://www.crummy.com/software/BeautifulSoup/,
2015

5

