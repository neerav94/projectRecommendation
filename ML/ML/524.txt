Enhancing Cortana User Experience Using Machine Learning

Emad Elwany

elwany@stanford.edu
Microsoft Research

Siamak Shakeri

siamaks@stanford.edu

Microsoft

December 2014

Abstract

2 Datasets

Voice enabled personal assistants like Microsoft
Cortana are becoming better every day. As a re-
sult more users are relying on such software to ac-
complish more tasks. While these applications are
signiﬁcantly improving due to great advancements
in the underlying technologies, there are still short-
comings in their performance resulting in a class of
user queries that such assistants cannot yet handle
with satisfactory results.

We analyze the data from millions of user queries,
and build a machine learning system capable of
classifying user queries into two classes; a class of
queries that are addressable by Cortana with high
user satisfaction, and a class of queries that are not.
We then use unsupervised learning to cluster sim-
ilar queries and assign them to human assistants
who can complement Cortana functionality.

1 Introduction

Cortana is getting better at answering user queries
for more and more scenarios every day, from adding
reminders, to obtaining directions, tracking ﬂights,
and many more. However, the current technology
does not allow addressing more advanced scenar-
ios such as scheduling a doctor appointment or ar-
ranging dinner at a restaurant with a friend. These
more complex tasks could however be broken into
simpler tasks, some of which can be automated,
while others require human intervention. Human
intervention however is costly, and does not scale
as well as automated task execution, and thus it
should only be utilized as scarcely as possible.

In order to achieve that, we need to have a sys-
tem that can analyze user queries and make a de-
cision on whether a query should go through the
normal Cortana ﬂow, or the human enhanced one.
In addition, the system also needs to identify what
category of human assistant is best suited for this
type of query, so that the query can be directed and
handled eﬃciently.

To train our models, we extracted hundreds of
thousands of anonymized Cortana query log entries
through a Microsoft proprietary internal data ware-
house. Each log entry corresponds to a user query,
along with relevant information like time, location,
duration, as well as telemetry information indicat-
ing how the user interacted with the Cortana re-
sponses and the overall status of the query. In ad-
dition, we are able to group queries by user and
by session using anonymous user and session ID’s.
This allows us to view the progression of a certain
user’s queries through a single search session.

While the query logs contain several types of

data, we focused our attention on three:

• General Search: Queries which Cortana redi-

rects to Bing search.

• Command and Control (C&C): Queries
where the user is trying to execute a task us-
ing voice command, such as adding an appoint-
ment to the calendar, or sending a text mes-
sage.

• Enriched Search: Queries where Cortana is
able to provide enriched responses, such as
weather conditions, current traﬃc, telling a
joke and etc.

C&C and Enriched queries align very closely with
the types of queries the Cortana-enhanced platform
should target to provide an even better user expe-
rience. Due to computing constraints, we trained
the model using a 1 day window of data(table 1)

Table 1: Summary of Datasets

Data

Category

General Data

C&C

Enriched

Class 1 to 0

ratio
0.304
0.596
6.14

Train
size
302k
223k
28k

Test
size
33k
25k
3.1k

3 Data Labeling

We employed two diﬀerent heuristics to label the
data in a pre-processing step using the information

1

Enhancing Cortana User Experience Using Machine Learning

Emad Elwany

elwany@stanford.edu
Microsoft Research

Siamak Shakeri

siamaks@stanford.edu

Microsoft

December 2014

Abstract

2 Datasets

Voice enabled personal assistants like Microsoft
Cortana are becoming better every day. As a re-
sult more users are relying on such software to ac-
complish more tasks. While these applications are
signiﬁcantly improving due to great advancements
in the underlying technologies, there are still short-
comings in their performance resulting in a class of
user queries that such assistants cannot yet handle
with satisfactory results.

We analyze the data from millions of user queries,
and build a machine learning system capable of
classifying user queries into two classes; a class of
queries that are addressable by Cortana with high
user satisfaction, and a class of queries that are not.
We then use unsupervised learning to cluster sim-
ilar queries and assign them to human assistants
who can complement Cortana functionality.

1 Introduction

Cortana is getting better at answering user queries
for more and more scenarios every day, from adding
reminders, to obtaining directions, tracking ﬂights,
and many more. However, the current technology
does not allow addressing more advanced scenar-
ios such as scheduling a doctor appointment or ar-
ranging dinner at a restaurant with a friend. These
more complex tasks could however be broken into
simpler tasks, some of which can be automated,
while others require human intervention. Human
intervention however is costly, and does not scale
as well as automated task execution, and thus it
should only be utilized as scarcely as possible.

In order to achieve that, we need to have a sys-
tem that can analyze user queries and make a de-
cision on whether a query should go through the
normal Cortana ﬂow, or the human enhanced one.
In addition, the system also needs to identify what
category of human assistant is best suited for this
type of query, so that the query can be directed and
handled eﬃciently.

To train our models, we extracted hundreds of
thousands of anonymized Cortana query log entries
through a Microsoft proprietary internal data ware-
house. Each log entry corresponds to a user query,
along with relevant information like time, location,
duration, as well as telemetry information indicat-
ing how the user interacted with the Cortana re-
sponses and the overall status of the query. In ad-
dition, we are able to group queries by user and
by session using anonymous user and session ID’s.
This allows us to view the progression of a certain
user’s queries through a single search session.

While the query logs contain several types of

data, we focused our attention on three:

• General Search: Queries which Cortana redi-

rects to Bing search.

• Command and Control (C&C): Queries
where the user is trying to execute a task us-
ing voice command, such as adding an appoint-
ment to the calendar, or sending a text mes-
sage.

• Enriched Search: Queries where Cortana is
able to provide enriched responses, such as
weather conditions, current traﬃc, telling a
joke and etc.

C&C and Enriched queries align very closely with
the types of queries the Cortana-enhanced platform
should target to provide an even better user expe-
rience. Due to computing constraints, we trained
the model using a 1 day window of data(table 1)

Table 1: Summary of Datasets

Data

Category

General Data

C&C

Enriched

Class 1 to 0

ratio
0.304
0.596
6.14

Train
size
302k
223k
28k

Test
size
33k
25k
3.1k

3 Data Labeling

We employed two diﬀerent heuristics to label the
data in a pre-processing step using the information

1

in the logs. This allowed us to avoid manual la-
beling and to be able to work with large amounts
of data. Our heuristics separate the data into two
classes:
0: Suﬃciently Satisfactory Cortana response
1: Unsatisfactory Cortana response

Laplace smoothing. We test the classiﬁer perfor-
mance by using simple cross validation. We use
General Search data(table 1) for training and test-
ing. The results are summarized in the following
table:1 As it can be seen, the error rates are high

Table 2: Naive Bayes with Repetition Based Tagging

3.1 Repetition Based Tagging: RBT

The simple heuristic works on the weak assumption
that an unsatisﬁed user re-submits the query one
or more times with no or minor tweaks within the
same session. The labeling step goes through all the
log entries and labels an entry ‘1’ if a user submits a
slightly tweaked version of the query multiple times.
We detect such queries using Levenshtein distance
of 5 [3].

3.2 Feedback Based Tagging: FBT

This heuristic takes into account more explicit sig-
nals from the logs. This information is only avail-
able for Enriched and C&C queries. After Cortana
provides the user with answers or actions to these
queries, it asks the user if s/he has been satisﬁed.
We use this signal as the tagging in this heuristic.
However an issue that might cause the labeling, es-
pecially with Enriched data to be less reliable, is
that the users might just hit a button to move to
another screen without providing feedback to Cor-
tana. This will cause Cortana to tag that query as
unsuccessful. By looking at the data, we noticed
that for Enriched data, the provided feedback on
the status of the query does not truly reﬂect the
Cortana response satisfaction. We will discuss this
later in models performance analysis.

4 Feature Extraction

We use the raw query as our primary feature, and
represent queries using the event model. we con-
struct a dictionary of words by pre-processing all
the queries, eliminating noise, non-alphabetic and
non-English words, as well as applying basic stem-
ming to the dictionary terms. We then encode ev-
ery query into a bit vector of size k, where k is the
number of terms in our vocabulary. The ith bit of
each vector represents the number of occurrences
of the ith term in our vocabulary. The number of
features after all the query modiﬁcations turned up
to be 10,414.

Test error Training error Precision Recall
67.87

54.61

55

24.24

and precision is low, which is not promising. There
could be two issues causing this:

• NB is not a proper modeling for this problem.
Our intuition here is that the Naive Bayes as-
sumption does not work well. For example a
word could more likely cause a 0 classiﬁcation
if used with question words. This sort of in-
formation is ignored in NB by assuming the
features are independent give the output.

• The tagging heuristic used here does not truly
reﬂect if the user has been satisﬁed with the
Cortana response or not.

Before resorting to other models, we try NB with
FBT, and see if there are any performance enhance-
ments. As the table 3 shows, the errors are still high

Table 3: Naive Bayes with Feedback Based Tagging

Data

Category

C&C

Enriched

Test
error
60.59

15

Train
error
60.57

15

Precision Recall
93.31
97.62

37.49
86.74

for C&C data, even worse than General data tagged
with RBT. For Enriched data, as explained previ-
ously, due to high ratio of class 1 to class 0 sam-
ples(table 1) as well as unreliable feedback used by
FBT, algorithm tends to predict everything as class
1. That is why the errors are low, however this does
not mean the algorithm is performing well. We no-
ticed in the predicted test data, only 10 out of 420
class 0 samples are truly predicted. We will later
design a test to show this ﬂaw with General data
which also suﬀers from this issue. From now on,
we will focus on C&C data which has more reliable
query feedbacks.

Since using FBT did not produce any tangible
performance enhancements, we will use other mod-
els to train and classify the data.

5 Classiﬁcation Models

5.2 Support Vector Machine Classiﬁer

5.1 Naive Bayes

We now focus our attention on using SVM with
FBT and see how the results compare to NB. We

We start by using a Bayesian classiﬁer with the
multinomial event model for text classiﬁcation and

1The training and test error, precision and recall values

in tables are all in percent.

2

Enhancing Cortana User Experience Using Machine Learning

Emad Elwany

elwany@stanford.edu
Microsoft Research

Siamak Shakeri

siamaks@stanford.edu

Microsoft

December 2014

Abstract

2 Datasets

Voice enabled personal assistants like Microsoft
Cortana are becoming better every day. As a re-
sult more users are relying on such software to ac-
complish more tasks. While these applications are
signiﬁcantly improving due to great advancements
in the underlying technologies, there are still short-
comings in their performance resulting in a class of
user queries that such assistants cannot yet handle
with satisfactory results.

We analyze the data from millions of user queries,
and build a machine learning system capable of
classifying user queries into two classes; a class of
queries that are addressable by Cortana with high
user satisfaction, and a class of queries that are not.
We then use unsupervised learning to cluster sim-
ilar queries and assign them to human assistants
who can complement Cortana functionality.

1 Introduction

Cortana is getting better at answering user queries
for more and more scenarios every day, from adding
reminders, to obtaining directions, tracking ﬂights,
and many more. However, the current technology
does not allow addressing more advanced scenar-
ios such as scheduling a doctor appointment or ar-
ranging dinner at a restaurant with a friend. These
more complex tasks could however be broken into
simpler tasks, some of which can be automated,
while others require human intervention. Human
intervention however is costly, and does not scale
as well as automated task execution, and thus it
should only be utilized as scarcely as possible.

In order to achieve that, we need to have a sys-
tem that can analyze user queries and make a de-
cision on whether a query should go through the
normal Cortana ﬂow, or the human enhanced one.
In addition, the system also needs to identify what
category of human assistant is best suited for this
type of query, so that the query can be directed and
handled eﬃciently.

To train our models, we extracted hundreds of
thousands of anonymized Cortana query log entries
through a Microsoft proprietary internal data ware-
house. Each log entry corresponds to a user query,
along with relevant information like time, location,
duration, as well as telemetry information indicat-
ing how the user interacted with the Cortana re-
sponses and the overall status of the query. In ad-
dition, we are able to group queries by user and
by session using anonymous user and session ID’s.
This allows us to view the progression of a certain
user’s queries through a single search session.

While the query logs contain several types of

data, we focused our attention on three:

• General Search: Queries which Cortana redi-

rects to Bing search.

• Command and Control (C&C): Queries
where the user is trying to execute a task us-
ing voice command, such as adding an appoint-
ment to the calendar, or sending a text mes-
sage.

• Enriched Search: Queries where Cortana is
able to provide enriched responses, such as
weather conditions, current traﬃc, telling a
joke and etc.

C&C and Enriched queries align very closely with
the types of queries the Cortana-enhanced platform
should target to provide an even better user expe-
rience. Due to computing constraints, we trained
the model using a 1 day window of data(table 1)

Table 1: Summary of Datasets

Data

Category

General Data

C&C

Enriched

Class 1 to 0

ratio
0.304
0.596
6.14

Train
size
302k
223k
28k

Test
size
33k
25k
3.1k

3 Data Labeling

We employed two diﬀerent heuristics to label the
data in a pre-processing step using the information

1

in the logs. This allowed us to avoid manual la-
beling and to be able to work with large amounts
of data. Our heuristics separate the data into two
classes:
0: Suﬃciently Satisfactory Cortana response
1: Unsatisfactory Cortana response

Laplace smoothing. We test the classiﬁer perfor-
mance by using simple cross validation. We use
General Search data(table 1) for training and test-
ing. The results are summarized in the following
table:1 As it can be seen, the error rates are high

Table 2: Naive Bayes with Repetition Based Tagging

3.1 Repetition Based Tagging: RBT

The simple heuristic works on the weak assumption
that an unsatisﬁed user re-submits the query one
or more times with no or minor tweaks within the
same session. The labeling step goes through all the
log entries and labels an entry ‘1’ if a user submits a
slightly tweaked version of the query multiple times.
We detect such queries using Levenshtein distance
of 5 [3].

3.2 Feedback Based Tagging: FBT

This heuristic takes into account more explicit sig-
nals from the logs. This information is only avail-
able for Enriched and C&C queries. After Cortana
provides the user with answers or actions to these
queries, it asks the user if s/he has been satisﬁed.
We use this signal as the tagging in this heuristic.
However an issue that might cause the labeling, es-
pecially with Enriched data to be less reliable, is
that the users might just hit a button to move to
another screen without providing feedback to Cor-
tana. This will cause Cortana to tag that query as
unsuccessful. By looking at the data, we noticed
that for Enriched data, the provided feedback on
the status of the query does not truly reﬂect the
Cortana response satisfaction. We will discuss this
later in models performance analysis.

4 Feature Extraction

We use the raw query as our primary feature, and
represent queries using the event model. we con-
struct a dictionary of words by pre-processing all
the queries, eliminating noise, non-alphabetic and
non-English words, as well as applying basic stem-
ming to the dictionary terms. We then encode ev-
ery query into a bit vector of size k, where k is the
number of terms in our vocabulary. The ith bit of
each vector represents the number of occurrences
of the ith term in our vocabulary. The number of
features after all the query modiﬁcations turned up
to be 10,414.

Test error Training error Precision Recall
67.87

54.61

55

24.24

and precision is low, which is not promising. There
could be two issues causing this:

• NB is not a proper modeling for this problem.
Our intuition here is that the Naive Bayes as-
sumption does not work well. For example a
word could more likely cause a 0 classiﬁcation
if used with question words. This sort of in-
formation is ignored in NB by assuming the
features are independent give the output.

• The tagging heuristic used here does not truly
reﬂect if the user has been satisﬁed with the
Cortana response or not.

Before resorting to other models, we try NB with
FBT, and see if there are any performance enhance-
ments. As the table 3 shows, the errors are still high

Table 3: Naive Bayes with Feedback Based Tagging

Data

Category

C&C

Enriched

Test
error
60.59

15

Train
error
60.57

15

Precision Recall
93.31
97.62

37.49
86.74

for C&C data, even worse than General data tagged
with RBT. For Enriched data, as explained previ-
ously, due to high ratio of class 1 to class 0 sam-
ples(table 1) as well as unreliable feedback used by
FBT, algorithm tends to predict everything as class
1. That is why the errors are low, however this does
not mean the algorithm is performing well. We no-
ticed in the predicted test data, only 10 out of 420
class 0 samples are truly predicted. We will later
design a test to show this ﬂaw with General data
which also suﬀers from this issue. From now on,
we will focus on C&C data which has more reliable
query feedbacks.

Since using FBT did not produce any tangible
performance enhancements, we will use other mod-
els to train and classify the data.

5 Classiﬁcation Models

5.2 Support Vector Machine Classiﬁer

5.1 Naive Bayes

We now focus our attention on using SVM with
FBT and see how the results compare to NB. We

We start by using a Bayesian classiﬁer with the
multinomial event model for text classiﬁcation and

1The training and test error, precision and recall values

in tables are all in percent.

2

used SVM with linear kernel implemented in LIB-
LINEAR library [2]. Since both our test and train-
ing data are unbalanced, we also experimented with
changing the weights in SVM. Here is the C-SVM
problem used by LIBLINEAR to solve this prob-
lem:

ωT ω + C + (cid:88)

− (cid:88)

ξi

min
ω,b,ξ

subject to

ξi + C

1
2
yi=−1
yi(ωT φ(xi) + b) + b) ≥ 1 − ξi
ξi ≥ 0.

yi=1

(1)
We used C&C data for train and testing. Figures
1 and 2 show the results.

As it can be seen the result are more promising

Figure 1: Test and Training Errors for Linear Kernel
SVM with C&C Data using FBT

rors also increase. This is expected, as assign-
ing higher weights to class 1 will cause the clas-
siﬁer to be more biased towards class 1. This
will end up increasing the number of wrongly
tagged class 1’s(false positives).

• Precision and Recall have opposite trajecto-
ries in the ﬁgure. With increasing the weight,
a greater percentage of actual class 1 samples
are classiﬁed as class 1, however this will cause
a higher percentage of class 0 samples to be
wrongly also tagged as class 1. Thus we will
have higher recall, but with less precision.

5.2.1 Bias vs Variance

We will analyze the test and training error versus
training set size change to see whether we are run-
ning into over or under ﬁtting issues.

The number of training samples is more than 20
times the number of features for C&C and General.
For Enriched data, this value is 2.8. Considering
the test and train error rates are very close, we are
not facing variance or bias. Figure 3 shows this for
C&C with Linear SVM. The values of training and

Figure 3: Training and Test error Vs. Training set size
for Linear Kernel SVM with C&C Data using FBT

test error are staying very close to each other with
changes in the training size. Also, the amount of
the change is near 5% when changing the size from
10,000 to 200,000. Thus both our data sets and
number of features are large enough so that are
models are not suﬀering from either bias or vari-
ance.

5.3 Logistic Regression

As another popular classiﬁer, we will now use logis-
tic regression model to classify the C&C data. We
will be using L1 and L2 norm logistic regression
implemented in LIBLINEAR [2].

Figure 2: Precision and Recall for Linear Kernel SVM
with C&C Data using FBT

with SVM. We can get as low as 25.7% test error
with weight = 1.2 for class 1 with reasonable pre-
cision and recall. The optimal result considering
precision and recall besides the test error, is with
weight 1.5 for class 1: test error: 28.7%, recall:
60% and precision: 65.8%.
Intuitively, this value
for weight is close to the ratio of class 0 to class 1,
which is 1.69(table 1). Here are some observations:
• As the class 1 weight is increased over the opti-
mal point(∼1.4), both the training and test er-

3

Enhancing Cortana User Experience Using Machine Learning

Emad Elwany

elwany@stanford.edu
Microsoft Research

Siamak Shakeri

siamaks@stanford.edu

Microsoft

December 2014

Abstract

2 Datasets

Voice enabled personal assistants like Microsoft
Cortana are becoming better every day. As a re-
sult more users are relying on such software to ac-
complish more tasks. While these applications are
signiﬁcantly improving due to great advancements
in the underlying technologies, there are still short-
comings in their performance resulting in a class of
user queries that such assistants cannot yet handle
with satisfactory results.

We analyze the data from millions of user queries,
and build a machine learning system capable of
classifying user queries into two classes; a class of
queries that are addressable by Cortana with high
user satisfaction, and a class of queries that are not.
We then use unsupervised learning to cluster sim-
ilar queries and assign them to human assistants
who can complement Cortana functionality.

1 Introduction

Cortana is getting better at answering user queries
for more and more scenarios every day, from adding
reminders, to obtaining directions, tracking ﬂights,
and many more. However, the current technology
does not allow addressing more advanced scenar-
ios such as scheduling a doctor appointment or ar-
ranging dinner at a restaurant with a friend. These
more complex tasks could however be broken into
simpler tasks, some of which can be automated,
while others require human intervention. Human
intervention however is costly, and does not scale
as well as automated task execution, and thus it
should only be utilized as scarcely as possible.

In order to achieve that, we need to have a sys-
tem that can analyze user queries and make a de-
cision on whether a query should go through the
normal Cortana ﬂow, or the human enhanced one.
In addition, the system also needs to identify what
category of human assistant is best suited for this
type of query, so that the query can be directed and
handled eﬃciently.

To train our models, we extracted hundreds of
thousands of anonymized Cortana query log entries
through a Microsoft proprietary internal data ware-
house. Each log entry corresponds to a user query,
along with relevant information like time, location,
duration, as well as telemetry information indicat-
ing how the user interacted with the Cortana re-
sponses and the overall status of the query. In ad-
dition, we are able to group queries by user and
by session using anonymous user and session ID’s.
This allows us to view the progression of a certain
user’s queries through a single search session.

While the query logs contain several types of

data, we focused our attention on three:

• General Search: Queries which Cortana redi-

rects to Bing search.

• Command and Control (C&C): Queries
where the user is trying to execute a task us-
ing voice command, such as adding an appoint-
ment to the calendar, or sending a text mes-
sage.

• Enriched Search: Queries where Cortana is
able to provide enriched responses, such as
weather conditions, current traﬃc, telling a
joke and etc.

C&C and Enriched queries align very closely with
the types of queries the Cortana-enhanced platform
should target to provide an even better user expe-
rience. Due to computing constraints, we trained
the model using a 1 day window of data(table 1)

Table 1: Summary of Datasets

Data

Category

General Data

C&C

Enriched

Class 1 to 0

ratio
0.304
0.596
6.14

Train
size
302k
223k
28k

Test
size
33k
25k
3.1k

3 Data Labeling

We employed two diﬀerent heuristics to label the
data in a pre-processing step using the information

1

in the logs. This allowed us to avoid manual la-
beling and to be able to work with large amounts
of data. Our heuristics separate the data into two
classes:
0: Suﬃciently Satisfactory Cortana response
1: Unsatisfactory Cortana response

Laplace smoothing. We test the classiﬁer perfor-
mance by using simple cross validation. We use
General Search data(table 1) for training and test-
ing. The results are summarized in the following
table:1 As it can be seen, the error rates are high

Table 2: Naive Bayes with Repetition Based Tagging

3.1 Repetition Based Tagging: RBT

The simple heuristic works on the weak assumption
that an unsatisﬁed user re-submits the query one
or more times with no or minor tweaks within the
same session. The labeling step goes through all the
log entries and labels an entry ‘1’ if a user submits a
slightly tweaked version of the query multiple times.
We detect such queries using Levenshtein distance
of 5 [3].

3.2 Feedback Based Tagging: FBT

This heuristic takes into account more explicit sig-
nals from the logs. This information is only avail-
able for Enriched and C&C queries. After Cortana
provides the user with answers or actions to these
queries, it asks the user if s/he has been satisﬁed.
We use this signal as the tagging in this heuristic.
However an issue that might cause the labeling, es-
pecially with Enriched data to be less reliable, is
that the users might just hit a button to move to
another screen without providing feedback to Cor-
tana. This will cause Cortana to tag that query as
unsuccessful. By looking at the data, we noticed
that for Enriched data, the provided feedback on
the status of the query does not truly reﬂect the
Cortana response satisfaction. We will discuss this
later in models performance analysis.

4 Feature Extraction

We use the raw query as our primary feature, and
represent queries using the event model. we con-
struct a dictionary of words by pre-processing all
the queries, eliminating noise, non-alphabetic and
non-English words, as well as applying basic stem-
ming to the dictionary terms. We then encode ev-
ery query into a bit vector of size k, where k is the
number of terms in our vocabulary. The ith bit of
each vector represents the number of occurrences
of the ith term in our vocabulary. The number of
features after all the query modiﬁcations turned up
to be 10,414.

Test error Training error Precision Recall
67.87

54.61

55

24.24

and precision is low, which is not promising. There
could be two issues causing this:

• NB is not a proper modeling for this problem.
Our intuition here is that the Naive Bayes as-
sumption does not work well. For example a
word could more likely cause a 0 classiﬁcation
if used with question words. This sort of in-
formation is ignored in NB by assuming the
features are independent give the output.

• The tagging heuristic used here does not truly
reﬂect if the user has been satisﬁed with the
Cortana response or not.

Before resorting to other models, we try NB with
FBT, and see if there are any performance enhance-
ments. As the table 3 shows, the errors are still high

Table 3: Naive Bayes with Feedback Based Tagging

Data

Category

C&C

Enriched

Test
error
60.59

15

Train
error
60.57

15

Precision Recall
93.31
97.62

37.49
86.74

for C&C data, even worse than General data tagged
with RBT. For Enriched data, as explained previ-
ously, due to high ratio of class 1 to class 0 sam-
ples(table 1) as well as unreliable feedback used by
FBT, algorithm tends to predict everything as class
1. That is why the errors are low, however this does
not mean the algorithm is performing well. We no-
ticed in the predicted test data, only 10 out of 420
class 0 samples are truly predicted. We will later
design a test to show this ﬂaw with General data
which also suﬀers from this issue. From now on,
we will focus on C&C data which has more reliable
query feedbacks.

Since using FBT did not produce any tangible
performance enhancements, we will use other mod-
els to train and classify the data.

5 Classiﬁcation Models

5.2 Support Vector Machine Classiﬁer

5.1 Naive Bayes

We now focus our attention on using SVM with
FBT and see how the results compare to NB. We

We start by using a Bayesian classiﬁer with the
multinomial event model for text classiﬁcation and

1The training and test error, precision and recall values

in tables are all in percent.

2

used SVM with linear kernel implemented in LIB-
LINEAR library [2]. Since both our test and train-
ing data are unbalanced, we also experimented with
changing the weights in SVM. Here is the C-SVM
problem used by LIBLINEAR to solve this prob-
lem:

ωT ω + C + (cid:88)

− (cid:88)

ξi

min
ω,b,ξ

subject to

ξi + C

1
2
yi=−1
yi(ωT φ(xi) + b) + b) ≥ 1 − ξi
ξi ≥ 0.

yi=1

(1)
We used C&C data for train and testing. Figures
1 and 2 show the results.

As it can be seen the result are more promising

Figure 1: Test and Training Errors for Linear Kernel
SVM with C&C Data using FBT

rors also increase. This is expected, as assign-
ing higher weights to class 1 will cause the clas-
siﬁer to be more biased towards class 1. This
will end up increasing the number of wrongly
tagged class 1’s(false positives).

• Precision and Recall have opposite trajecto-
ries in the ﬁgure. With increasing the weight,
a greater percentage of actual class 1 samples
are classiﬁed as class 1, however this will cause
a higher percentage of class 0 samples to be
wrongly also tagged as class 1. Thus we will
have higher recall, but with less precision.

5.2.1 Bias vs Variance

We will analyze the test and training error versus
training set size change to see whether we are run-
ning into over or under ﬁtting issues.

The number of training samples is more than 20
times the number of features for C&C and General.
For Enriched data, this value is 2.8. Considering
the test and train error rates are very close, we are
not facing variance or bias. Figure 3 shows this for
C&C with Linear SVM. The values of training and

Figure 3: Training and Test error Vs. Training set size
for Linear Kernel SVM with C&C Data using FBT

test error are staying very close to each other with
changes in the training size. Also, the amount of
the change is near 5% when changing the size from
10,000 to 200,000. Thus both our data sets and
number of features are large enough so that are
models are not suﬀering from either bias or vari-
ance.

5.3 Logistic Regression

As another popular classiﬁer, we will now use logis-
tic regression model to classify the C&C data. We
will be using L1 and L2 norm logistic regression
implemented in LIBLINEAR [2].

Figure 2: Precision and Recall for Linear Kernel SVM
with C&C Data using FBT

with SVM. We can get as low as 25.7% test error
with weight = 1.2 for class 1 with reasonable pre-
cision and recall. The optimal result considering
precision and recall besides the test error, is with
weight 1.5 for class 1: test error: 28.7%, recall:
60% and precision: 65.8%.
Intuitively, this value
for weight is close to the ratio of class 0 to class 1,
which is 1.69(table 1). Here are some observations:
• As the class 1 weight is increased over the opti-
mal point(∼1.4), both the training and test er-

3

5.3.1 L1 Norm

Table 4: L2 Vs. L1 norm Logistic Regression with C&C
dataset

The unconstrained primal optimization problem
solved for this model is:
(cid:107)ω(cid:107)1 + C

−yiωT xi )

l(cid:88)

log(1 + e

min

(2)

ω

i=1

Model

L1
L2

Training

error
26.96
26.27

Test
error
28.46
28.3

Precision Recall
65.92
65.79

60.62

61

Where ω is the the vector of parameters. We will
also experiment with weights as with SVM. Figures
4 and 5 show the results: The model demonstrates

Figure 4: Test and Training Errors for L1 Logistic Re-
gression with C&C Data using FBT

class 1, and ran the model over C&C dataset. As
it can be seen from table 4, L2 norm model slightly
outperforms L1 norm in terms of test error, how-
ever the diﬀerence is very marginal.

5.3.3 Logistic Regression Vs. SVM

Both models perform very closely. Our intuition is
that they both are solving very similar optimiza-
tion problems, so it is expected that they perform
closely. The regularization done in Logistic Regres-
sion models were not very useful, as we saw that
due to the large size of our dataset, the algorithms
are not experience high variance.

5.4 Testing queries with aggressive class

ratio datasets

In order to see how well our model performs against
naive algorithms, as well as how resilient it is with
respect to the changes in the input query class ratio,
we choose three diﬀerent set of test samples:

• Test set containing of all 0’s: This is to test
against an algorithm(AlgBrute) that always
predicts 1. In this case AlgBrute will have 0%
accuracy.

• Test set containing of all 1’s: This is to see how
well our models perform agains an all positive
data set. AlgBrute will always predict 100%
here.

• Test set containing of 50% 1’s and 50% 0’s.

AlgBrute will have 50% accuracy.

We will be using Linear SVM with class 1 wight of
1.5. This model’s performance on the normal test
set has been: Test error: 28.72%, Recall:65.78%
Precision: 60.64%. Tables 5 shows the results with
C&C data. As it can be seen, the classiﬁer main-

Table 5: Linear SVM with aggressive C&C test data

Test set Test error Precision Recall
All 1’s
66.03
All 0’s
50-50

33.96
25.36
30.03

65.79

71.88

100

inf

0

Figure 5: Precision and Recall for L1 Logistic Regres-
sion with C&C Data using FBT

very close performance to Linear SVM. Here are
some notes:

• Weights of 1.4 to 1.6 yield the best results con-
sidering test and training errors as well as re-
call and precision.

• The similar trends in terms of training and test
error as well as precision and recall is seen com-
pared to SVM.

5.3.2 L2 Norm

min

ω

1
2

ωT ω + C

l(cid:88)

i=1

log(1 + e

−yiωT xi )

(3)

We experimented with L2 norm logistic regression,
and the results did not show any tangible diﬀer-
ences with L1 norm. We used 1.5 as weight for

tains its accuracy well. The precision and recall
are by deﬁnition expected to be 0 and inf when
there are no 1’s, and precision is 100% when there

4

Enhancing Cortana User Experience Using Machine Learning

Emad Elwany

elwany@stanford.edu
Microsoft Research

Siamak Shakeri

siamaks@stanford.edu

Microsoft

December 2014

Abstract

2 Datasets

Voice enabled personal assistants like Microsoft
Cortana are becoming better every day. As a re-
sult more users are relying on such software to ac-
complish more tasks. While these applications are
signiﬁcantly improving due to great advancements
in the underlying technologies, there are still short-
comings in their performance resulting in a class of
user queries that such assistants cannot yet handle
with satisfactory results.

We analyze the data from millions of user queries,
and build a machine learning system capable of
classifying user queries into two classes; a class of
queries that are addressable by Cortana with high
user satisfaction, and a class of queries that are not.
We then use unsupervised learning to cluster sim-
ilar queries and assign them to human assistants
who can complement Cortana functionality.

1 Introduction

Cortana is getting better at answering user queries
for more and more scenarios every day, from adding
reminders, to obtaining directions, tracking ﬂights,
and many more. However, the current technology
does not allow addressing more advanced scenar-
ios such as scheduling a doctor appointment or ar-
ranging dinner at a restaurant with a friend. These
more complex tasks could however be broken into
simpler tasks, some of which can be automated,
while others require human intervention. Human
intervention however is costly, and does not scale
as well as automated task execution, and thus it
should only be utilized as scarcely as possible.

In order to achieve that, we need to have a sys-
tem that can analyze user queries and make a de-
cision on whether a query should go through the
normal Cortana ﬂow, or the human enhanced one.
In addition, the system also needs to identify what
category of human assistant is best suited for this
type of query, so that the query can be directed and
handled eﬃciently.

To train our models, we extracted hundreds of
thousands of anonymized Cortana query log entries
through a Microsoft proprietary internal data ware-
house. Each log entry corresponds to a user query,
along with relevant information like time, location,
duration, as well as telemetry information indicat-
ing how the user interacted with the Cortana re-
sponses and the overall status of the query. In ad-
dition, we are able to group queries by user and
by session using anonymous user and session ID’s.
This allows us to view the progression of a certain
user’s queries through a single search session.

While the query logs contain several types of

data, we focused our attention on three:

• General Search: Queries which Cortana redi-

rects to Bing search.

• Command and Control (C&C): Queries
where the user is trying to execute a task us-
ing voice command, such as adding an appoint-
ment to the calendar, or sending a text mes-
sage.

• Enriched Search: Queries where Cortana is
able to provide enriched responses, such as
weather conditions, current traﬃc, telling a
joke and etc.

C&C and Enriched queries align very closely with
the types of queries the Cortana-enhanced platform
should target to provide an even better user expe-
rience. Due to computing constraints, we trained
the model using a 1 day window of data(table 1)

Table 1: Summary of Datasets

Data

Category

General Data

C&C

Enriched

Class 1 to 0

ratio
0.304
0.596
6.14

Train
size
302k
223k
28k

Test
size
33k
25k
3.1k

3 Data Labeling

We employed two diﬀerent heuristics to label the
data in a pre-processing step using the information

1

in the logs. This allowed us to avoid manual la-
beling and to be able to work with large amounts
of data. Our heuristics separate the data into two
classes:
0: Suﬃciently Satisfactory Cortana response
1: Unsatisfactory Cortana response

Laplace smoothing. We test the classiﬁer perfor-
mance by using simple cross validation. We use
General Search data(table 1) for training and test-
ing. The results are summarized in the following
table:1 As it can be seen, the error rates are high

Table 2: Naive Bayes with Repetition Based Tagging

3.1 Repetition Based Tagging: RBT

The simple heuristic works on the weak assumption
that an unsatisﬁed user re-submits the query one
or more times with no or minor tweaks within the
same session. The labeling step goes through all the
log entries and labels an entry ‘1’ if a user submits a
slightly tweaked version of the query multiple times.
We detect such queries using Levenshtein distance
of 5 [3].

3.2 Feedback Based Tagging: FBT

This heuristic takes into account more explicit sig-
nals from the logs. This information is only avail-
able for Enriched and C&C queries. After Cortana
provides the user with answers or actions to these
queries, it asks the user if s/he has been satisﬁed.
We use this signal as the tagging in this heuristic.
However an issue that might cause the labeling, es-
pecially with Enriched data to be less reliable, is
that the users might just hit a button to move to
another screen without providing feedback to Cor-
tana. This will cause Cortana to tag that query as
unsuccessful. By looking at the data, we noticed
that for Enriched data, the provided feedback on
the status of the query does not truly reﬂect the
Cortana response satisfaction. We will discuss this
later in models performance analysis.

4 Feature Extraction

We use the raw query as our primary feature, and
represent queries using the event model. we con-
struct a dictionary of words by pre-processing all
the queries, eliminating noise, non-alphabetic and
non-English words, as well as applying basic stem-
ming to the dictionary terms. We then encode ev-
ery query into a bit vector of size k, where k is the
number of terms in our vocabulary. The ith bit of
each vector represents the number of occurrences
of the ith term in our vocabulary. The number of
features after all the query modiﬁcations turned up
to be 10,414.

Test error Training error Precision Recall
67.87

54.61

55

24.24

and precision is low, which is not promising. There
could be two issues causing this:

• NB is not a proper modeling for this problem.
Our intuition here is that the Naive Bayes as-
sumption does not work well. For example a
word could more likely cause a 0 classiﬁcation
if used with question words. This sort of in-
formation is ignored in NB by assuming the
features are independent give the output.

• The tagging heuristic used here does not truly
reﬂect if the user has been satisﬁed with the
Cortana response or not.

Before resorting to other models, we try NB with
FBT, and see if there are any performance enhance-
ments. As the table 3 shows, the errors are still high

Table 3: Naive Bayes with Feedback Based Tagging

Data

Category

C&C

Enriched

Test
error
60.59

15

Train
error
60.57

15

Precision Recall
93.31
97.62

37.49
86.74

for C&C data, even worse than General data tagged
with RBT. For Enriched data, as explained previ-
ously, due to high ratio of class 1 to class 0 sam-
ples(table 1) as well as unreliable feedback used by
FBT, algorithm tends to predict everything as class
1. That is why the errors are low, however this does
not mean the algorithm is performing well. We no-
ticed in the predicted test data, only 10 out of 420
class 0 samples are truly predicted. We will later
design a test to show this ﬂaw with General data
which also suﬀers from this issue. From now on,
we will focus on C&C data which has more reliable
query feedbacks.

Since using FBT did not produce any tangible
performance enhancements, we will use other mod-
els to train and classify the data.

5 Classiﬁcation Models

5.2 Support Vector Machine Classiﬁer

5.1 Naive Bayes

We now focus our attention on using SVM with
FBT and see how the results compare to NB. We

We start by using a Bayesian classiﬁer with the
multinomial event model for text classiﬁcation and

1The training and test error, precision and recall values

in tables are all in percent.

2

used SVM with linear kernel implemented in LIB-
LINEAR library [2]. Since both our test and train-
ing data are unbalanced, we also experimented with
changing the weights in SVM. Here is the C-SVM
problem used by LIBLINEAR to solve this prob-
lem:

ωT ω + C + (cid:88)

− (cid:88)

ξi

min
ω,b,ξ

subject to

ξi + C

1
2
yi=−1
yi(ωT φ(xi) + b) + b) ≥ 1 − ξi
ξi ≥ 0.

yi=1

(1)
We used C&C data for train and testing. Figures
1 and 2 show the results.

As it can be seen the result are more promising

Figure 1: Test and Training Errors for Linear Kernel
SVM with C&C Data using FBT

rors also increase. This is expected, as assign-
ing higher weights to class 1 will cause the clas-
siﬁer to be more biased towards class 1. This
will end up increasing the number of wrongly
tagged class 1’s(false positives).

• Precision and Recall have opposite trajecto-
ries in the ﬁgure. With increasing the weight,
a greater percentage of actual class 1 samples
are classiﬁed as class 1, however this will cause
a higher percentage of class 0 samples to be
wrongly also tagged as class 1. Thus we will
have higher recall, but with less precision.

5.2.1 Bias vs Variance

We will analyze the test and training error versus
training set size change to see whether we are run-
ning into over or under ﬁtting issues.

The number of training samples is more than 20
times the number of features for C&C and General.
For Enriched data, this value is 2.8. Considering
the test and train error rates are very close, we are
not facing variance or bias. Figure 3 shows this for
C&C with Linear SVM. The values of training and

Figure 3: Training and Test error Vs. Training set size
for Linear Kernel SVM with C&C Data using FBT

test error are staying very close to each other with
changes in the training size. Also, the amount of
the change is near 5% when changing the size from
10,000 to 200,000. Thus both our data sets and
number of features are large enough so that are
models are not suﬀering from either bias or vari-
ance.

5.3 Logistic Regression

As another popular classiﬁer, we will now use logis-
tic regression model to classify the C&C data. We
will be using L1 and L2 norm logistic regression
implemented in LIBLINEAR [2].

Figure 2: Precision and Recall for Linear Kernel SVM
with C&C Data using FBT

with SVM. We can get as low as 25.7% test error
with weight = 1.2 for class 1 with reasonable pre-
cision and recall. The optimal result considering
precision and recall besides the test error, is with
weight 1.5 for class 1: test error: 28.7%, recall:
60% and precision: 65.8%.
Intuitively, this value
for weight is close to the ratio of class 0 to class 1,
which is 1.69(table 1). Here are some observations:
• As the class 1 weight is increased over the opti-
mal point(∼1.4), both the training and test er-

3

5.3.1 L1 Norm

Table 4: L2 Vs. L1 norm Logistic Regression with C&C
dataset

The unconstrained primal optimization problem
solved for this model is:
(cid:107)ω(cid:107)1 + C

−yiωT xi )

l(cid:88)

log(1 + e

min

(2)

ω

i=1

Model

L1
L2

Training

error
26.96
26.27

Test
error
28.46
28.3

Precision Recall
65.92
65.79

60.62

61

Where ω is the the vector of parameters. We will
also experiment with weights as with SVM. Figures
4 and 5 show the results: The model demonstrates

Figure 4: Test and Training Errors for L1 Logistic Re-
gression with C&C Data using FBT

class 1, and ran the model over C&C dataset. As
it can be seen from table 4, L2 norm model slightly
outperforms L1 norm in terms of test error, how-
ever the diﬀerence is very marginal.

5.3.3 Logistic Regression Vs. SVM

Both models perform very closely. Our intuition is
that they both are solving very similar optimiza-
tion problems, so it is expected that they perform
closely. The regularization done in Logistic Regres-
sion models were not very useful, as we saw that
due to the large size of our dataset, the algorithms
are not experience high variance.

5.4 Testing queries with aggressive class

ratio datasets

In order to see how well our model performs against
naive algorithms, as well as how resilient it is with
respect to the changes in the input query class ratio,
we choose three diﬀerent set of test samples:

• Test set containing of all 0’s: This is to test
against an algorithm(AlgBrute) that always
predicts 1. In this case AlgBrute will have 0%
accuracy.

• Test set containing of all 1’s: This is to see how
well our models perform agains an all positive
data set. AlgBrute will always predict 100%
here.

• Test set containing of 50% 1’s and 50% 0’s.

AlgBrute will have 50% accuracy.

We will be using Linear SVM with class 1 wight of
1.5. This model’s performance on the normal test
set has been: Test error: 28.72%, Recall:65.78%
Precision: 60.64%. Tables 5 shows the results with
C&C data. As it can be seen, the classiﬁer main-

Table 5: Linear SVM with aggressive C&C test data

Test set Test error Precision Recall
All 1’s
66.03
All 0’s
50-50

33.96
25.36
30.03

65.79

71.88

100

inf

0

Figure 5: Precision and Recall for L1 Logistic Regres-
sion with C&C Data using FBT

very close performance to Linear SVM. Here are
some notes:

• Weights of 1.4 to 1.6 yield the best results con-
sidering test and training errors as well as re-
call and precision.

• The similar trends in terms of training and test
error as well as precision and recall is seen com-
pared to SVM.

5.3.2 L2 Norm

min

ω

1
2

ωT ω + C

l(cid:88)

i=1

log(1 + e

−yiωT xi )

(3)

We experimented with L2 norm logistic regression,
and the results did not show any tangible diﬀer-
ences with L1 norm. We used 1.5 as weight for

tains its accuracy well. The precision and recall
are by deﬁnition expected to be 0 and inf when
there are no 1’s, and precision is 100% when there

4

are no 0’s. For the other cases, both precision and
recall maintain very close to normal training set
performance with aggressive test data. This has
the important implication that our model has not
merged to become naive algorithms where one class
is predicted all the time, or choosing one class with
a certain probability every time. These two naive
algorithms will perform very poorly with the ag-
gressive tests.

We performed this experiment with General
search data, which as mentioned before has the
problem of unreliable tagging. We used 3 as the
weight for class 1. Table 6 shows the results. As
it can be seen, speciﬁcally with all 1’s data, the
algorithm seems to be doing a coin ﬂip to predict
the samples. Thus the training labels have not pro-
vided any net valuable information to the model.
This proves our previous claim that the RBT is
not reliable. We saw similar results with Enriched
queries tagged by FBT.

Table 6: Linear SVM with aggressive General search
test data

Test set Test error Precision Recall
All 1’s
46.57
All 0’s
50-50

44.9
30.72
37.83

46.57

inf

100

0

56.04

6 Query Clustering

We start by using k-means [4] to cluster queries to-
gether. We used our own implementation of the
k-means algorithm and then compared it to the
Matlab build-in version for validation. Both imple-
mentation yielded very similar results for the same
value of k and the same similarity metric. We used
cosine similarity [5] as a similarity metric when per-
forming the clustering step. We tried to use the
EM algorithm as well, but it was a challenge to
use our sparse bit-vector data representations to
estimate Gaussian distributions, we plan on inves-
tigating using factor analysis to work around this
in the future.

7 Conclusion

The purpose of this project was to predict the
queries that existing Cortana system will not be
able to handle with satisfactory results. These un-
satisﬁed queries could later be handled by humans
as a premium service. We started by Naive Bayes
model and a repetition based tagging of General
search queries. Our results showed poor perfor-
mance with errors more than 55%. With improved
tagging of data by using FBT, and better models

5

such as SVM and Logistic Regression, we could re-
duce the test errors to as low as 28%. We also
saw improvements in precision from 24% to 60%.
Besides that, we noticed that SVM and regular-
ized logistic regression models have very close per-
formances. In addition, we showed that our SVM
model with proper tagging(FBT) is resilient with
respect to various ratios of class 1 to class 0 in the
test samples, diﬀerent from training set.

8 Future work

One of the main obstacles that we encountered was
how to properly tag the data for test and train-
ing of the model. Inferring user satisfaction is very
complex, and needs a more complex mechanism to
detect. A more advanced way is to follow the ac-
tions that user takes after the Cortana interaction
is over. This can give more accurate information to
see whether user has been satisﬁed or not.

Another path to expand this work is to run the
models over a larger set of data. We used only a
day of Cortana data in this project. Our main ob-
stacle was limited computation resources as well as
inability of the implemented models to digest data
sets larger than the available virtual memory of the
machine. For example, the SVM we used requires
loading the whole training sparse matrix, which be-
comes very large considering our large number of
features and queries.

One area that we touched slightly and could be
well investigated further is query clustering. We
did clustering on the data, however we did not see
meaningful clusters. Meaning that we were expect-
ing to see clusters of reminders, meetings, etc. This
was not observed by using k-means. More advanced
algorithms such as EM could yield better results.

References

[1] Chang, Chih-Chung and Lin, Chih-Jen, LIBSVM: A li-
brary for support vector machines. ACM Transactions
on Intelligent Systems and Technology, volume 2, issue
3, 2011.

[2] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang and Chih-Jen Lin LIBLINEAR: A Library
for Large Linear Classications. Journal of Machine
Learning Research, 2008.

[3] Levenshtein, Vladimir I Binary codes capable of cor-
insertions, and reversals. Soviet

recting deletions,
Physics Doklady, 1966.

[4] MacQueen, J. B Some Methods for classiﬁcation and
Analysis of Multivariate Observations.. Proceedings
of 5th Berkeley Symposium on Mathematical Statistics
and Probability : University of California Press 1967.

[5] Singhal, Amit Modern Information Retrieval: A Brief
Overview. Bulletin of the IEEE Computer Society Tech-
nical Committee on Data Engineering. 2001.

