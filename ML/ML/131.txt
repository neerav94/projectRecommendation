CS 229 Final report

A Study Of Ensemble Methods In Machine Learning

Kwhangho Kim, Jeha Yang

Abstract

The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is
well-known that ensemble methods can be used for improving prediction performance.
In this project we
provide novel methods of combining multiple learners in classiﬁcation tasks. We also present an empirical
study with wine quality score data set and demonstrate superior predictive power of the proposed ensemble
methods over simple voting or single model methods.

1 Introduction

An ensemble of classiﬁers is a set of classiﬁers whose individual decisions are combined in some way, typically
by weighted or unweighted voting, to classify new examples with the goal of improving accuracy and reliability.
As combining diverse, independent opinions in human decision-making to pursue a more protective mechanism,
the Ensemble model provides an eﬃcient way to improve accuracy and robustness over single model methods.
Ensemble methods provide a huge practical usefulness in that it has the promise of reducing and perhaps even
eliminating some key shortcomings of standard learning algorithms.
In this project, we focus on classiﬁer ensembles and propose novel algorithms of combining method. Our methods
provide very simple yet eﬀective way to determine optimal weight of each classiﬁer in a sense that it seeks not
only empirical risk minimization but diversiﬁcation of classiﬁers.
For application, we analyze Napa Valley Wine Quality data and show that all of our ensemble algorithms produce
way better predictive performance than any single method. Our methods also show superior performance than
simple voting method.

2 Related Work

In the recent years, experimental studies conducted by the machine-learning community show that combining the
outputs of multiple classiﬁers reduces the generalization error (Quinlan, 1996, Opitz and Maclin, 1999, Kuncheva
et al., 2004, Rokach, 2006). Ensemble methods are very eﬀective, mainly due to the phenomenon that various
types of classiﬁers have diﬀerent inductive biases (Geman et al., 1995, Mitchell, 1997). Indeed, ensemble methods
can eﬀectively make use of such diversity to reduce the variance-error (Tumer and Ghosh, 1999, Ali and Pazzani,
1996) without increasing the bias-error. In certain situations, an ensemble can also reduce bias-error, as shown
by the theory of large margin classiﬁers (Bartlett and Shawe-Taylor, 1998).
Given the potential usefulness of ensemble methods, it is not surprising that a vast number of methods is now
available to researchers and practitioners. There are several factors that diﬀerentiate between the various ensem-
bles methods. Rokach (2010) summarizes four factors as below:

1.
2.
3.
4.

Inter-classiﬁers relationship - How does each classiﬁer aﬀect the other classiﬁers
Combining method - The strategy of combining the classiﬁers
Diversity generator - How should we produce some sort of diversity between the classiﬁers
Ensemble size - The number of classiﬁers in the ensemble

In this project, we particularly focus on the combining method. Ali and Pazzani (1996) have compared several
combination methods. More theoretical analysis has been developed for estimating the classiﬁcation improvement
by Tumer and Ghosh (1999).

1

CS 229 Final report

A Study Of Ensemble Methods In Machine Learning

Kwhangho Kim, Jeha Yang

Abstract

The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is
well-known that ensemble methods can be used for improving prediction performance.
In this project we
provide novel methods of combining multiple learners in classiﬁcation tasks. We also present an empirical
study with wine quality score data set and demonstrate superior predictive power of the proposed ensemble
methods over simple voting or single model methods.

1 Introduction

An ensemble of classiﬁers is a set of classiﬁers whose individual decisions are combined in some way, typically
by weighted or unweighted voting, to classify new examples with the goal of improving accuracy and reliability.
As combining diverse, independent opinions in human decision-making to pursue a more protective mechanism,
the Ensemble model provides an eﬃcient way to improve accuracy and robustness over single model methods.
Ensemble methods provide a huge practical usefulness in that it has the promise of reducing and perhaps even
eliminating some key shortcomings of standard learning algorithms.
In this project, we focus on classiﬁer ensembles and propose novel algorithms of combining method. Our methods
provide very simple yet eﬀective way to determine optimal weight of each classiﬁer in a sense that it seeks not
only empirical risk minimization but diversiﬁcation of classiﬁers.
For application, we analyze Napa Valley Wine Quality data and show that all of our ensemble algorithms produce
way better predictive performance than any single method. Our methods also show superior performance than
simple voting method.

2 Related Work

In the recent years, experimental studies conducted by the machine-learning community show that combining the
outputs of multiple classiﬁers reduces the generalization error (Quinlan, 1996, Opitz and Maclin, 1999, Kuncheva
et al., 2004, Rokach, 2006). Ensemble methods are very eﬀective, mainly due to the phenomenon that various
types of classiﬁers have diﬀerent inductive biases (Geman et al., 1995, Mitchell, 1997). Indeed, ensemble methods
can eﬀectively make use of such diversity to reduce the variance-error (Tumer and Ghosh, 1999, Ali and Pazzani,
1996) without increasing the bias-error. In certain situations, an ensemble can also reduce bias-error, as shown
by the theory of large margin classiﬁers (Bartlett and Shawe-Taylor, 1998).
Given the potential usefulness of ensemble methods, it is not surprising that a vast number of methods is now
available to researchers and practitioners. There are several factors that diﬀerentiate between the various ensem-
bles methods. Rokach (2010) summarizes four factors as below:

1.
2.
3.
4.

Inter-classiﬁers relationship - How does each classiﬁer aﬀect the other classiﬁers
Combining method - The strategy of combining the classiﬁers
Diversity generator - How should we produce some sort of diversity between the classiﬁers
Ensemble size - The number of classiﬁers in the ensemble

In this project, we particularly focus on the combining method. Ali and Pazzani (1996) have compared several
combination methods. More theoretical analysis has been developed for estimating the classiﬁcation improvement
by Tumer and Ghosh (1999).

1

CS 229 Final report

Kwhangho Kim, Jeha Yang

3 Application : Napa Valley Wine Quality Score data

To test whether our proposed ensemble algorithms really work and to see how much improvement can be made,
we conduct an empirical study. For this empirical study we use Napa Valley Wine Quality Score data 1.

3.1 Purpose

A wine rating is a score assigned by one or more professional wine critics to a wine tasted as a summary of that
critic’s evaluation of that wine. This wine quality score is assigned after the wines have been released to the
market. Since it directly aﬀects the price and demand of the wine, for the wine companies predicting the wine
quality score is very important.
We want to predict the wine quality score with given soil conditions of the year in which the wine was produced.

3.2 Data Description

• We have 4800 (complete) data points.
• Wine Quality Score is integer-valued, and ranges from 3 to 9.
•
•

There are 13 predictors : X (labeling information), color, ﬁxed.acidity, volatile.acidity, citric.acid,
residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol
The training data is imbalanced ; that is, there are major (5,6,7) labels that appear most of the time
and minor (3,4,8,9) labels.

X

Color

2

Red:
1189
3234
3241 While:
3611
6497

Fixed
acidity

Volatile
acidity

Citric
acid

Residual

sugar

4.2
7.0
7.2
15.9

0.08
0.29
0.34
1.58

0

0.31
0.32
1.66

0.6
3.0
5.4
65.8

Cl−
0.01
0.05
0.06
0.61

Free
SO2

1
29
31
289

Total
SO2

6

118
115
440

Density

PH

2− Alcohol

SO4

0.987
0.995
0.994
1.039

2.72
3.20
3.22
4.01

0.22
0.50
0.53
2.00

8.0
10.3
10.5
14.9

Min.

Median
Mean
Max.

(a) Histogram of wine quality score

(b) Correlation matrix

(c) Correlation matrix

Figure 1: Basic data description

4 Ensemble Methodology

4.1 Single Model Methods and Basic Analysis

We ﬁt the following prediction models to our training data set :
1. Ordinary Least Square (OLS) Regression
2. Robust Linear Regression - M-estimation(RLM), Least Trimmed Squares (LTS)
3. Weighted Least Square (WLS) Regression - weights : reciprocal of the square root of the size of each score
4. L1 regression : backﬁtting + WLS (L1R)

1This dataset is given by Prof. T. Hastie and originally used in STATS 315A class. In this simulation, to keep conﬁdentiality of

the original data set, we only use a part of training set of the original dataset.

2

CS 229 Final report

A Study Of Ensemble Methods In Machine Learning

Kwhangho Kim, Jeha Yang

Abstract

The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is
well-known that ensemble methods can be used for improving prediction performance.
In this project we
provide novel methods of combining multiple learners in classiﬁcation tasks. We also present an empirical
study with wine quality score data set and demonstrate superior predictive power of the proposed ensemble
methods over simple voting or single model methods.

1 Introduction

An ensemble of classiﬁers is a set of classiﬁers whose individual decisions are combined in some way, typically
by weighted or unweighted voting, to classify new examples with the goal of improving accuracy and reliability.
As combining diverse, independent opinions in human decision-making to pursue a more protective mechanism,
the Ensemble model provides an eﬃcient way to improve accuracy and robustness over single model methods.
Ensemble methods provide a huge practical usefulness in that it has the promise of reducing and perhaps even
eliminating some key shortcomings of standard learning algorithms.
In this project, we focus on classiﬁer ensembles and propose novel algorithms of combining method. Our methods
provide very simple yet eﬀective way to determine optimal weight of each classiﬁer in a sense that it seeks not
only empirical risk minimization but diversiﬁcation of classiﬁers.
For application, we analyze Napa Valley Wine Quality data and show that all of our ensemble algorithms produce
way better predictive performance than any single method. Our methods also show superior performance than
simple voting method.

2 Related Work

In the recent years, experimental studies conducted by the machine-learning community show that combining the
outputs of multiple classiﬁers reduces the generalization error (Quinlan, 1996, Opitz and Maclin, 1999, Kuncheva
et al., 2004, Rokach, 2006). Ensemble methods are very eﬀective, mainly due to the phenomenon that various
types of classiﬁers have diﬀerent inductive biases (Geman et al., 1995, Mitchell, 1997). Indeed, ensemble methods
can eﬀectively make use of such diversity to reduce the variance-error (Tumer and Ghosh, 1999, Ali and Pazzani,
1996) without increasing the bias-error. In certain situations, an ensemble can also reduce bias-error, as shown
by the theory of large margin classiﬁers (Bartlett and Shawe-Taylor, 1998).
Given the potential usefulness of ensemble methods, it is not surprising that a vast number of methods is now
available to researchers and practitioners. There are several factors that diﬀerentiate between the various ensem-
bles methods. Rokach (2010) summarizes four factors as below:

1.
2.
3.
4.

Inter-classiﬁers relationship - How does each classiﬁer aﬀect the other classiﬁers
Combining method - The strategy of combining the classiﬁers
Diversity generator - How should we produce some sort of diversity between the classiﬁers
Ensemble size - The number of classiﬁers in the ensemble

In this project, we particularly focus on the combining method. Ali and Pazzani (1996) have compared several
combination methods. More theoretical analysis has been developed for estimating the classiﬁcation improvement
by Tumer and Ghosh (1999).

1

CS 229 Final report

Kwhangho Kim, Jeha Yang

3 Application : Napa Valley Wine Quality Score data

To test whether our proposed ensemble algorithms really work and to see how much improvement can be made,
we conduct an empirical study. For this empirical study we use Napa Valley Wine Quality Score data 1.

3.1 Purpose

A wine rating is a score assigned by one or more professional wine critics to a wine tasted as a summary of that
critic’s evaluation of that wine. This wine quality score is assigned after the wines have been released to the
market. Since it directly aﬀects the price and demand of the wine, for the wine companies predicting the wine
quality score is very important.
We want to predict the wine quality score with given soil conditions of the year in which the wine was produced.

3.2 Data Description

• We have 4800 (complete) data points.
• Wine Quality Score is integer-valued, and ranges from 3 to 9.
•
•

There are 13 predictors : X (labeling information), color, ﬁxed.acidity, volatile.acidity, citric.acid,
residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol
The training data is imbalanced ; that is, there are major (5,6,7) labels that appear most of the time
and minor (3,4,8,9) labels.

X

Color

2

Red:
1189
3234
3241 While:
3611
6497

Fixed
acidity

Volatile
acidity

Citric
acid

Residual

sugar

4.2
7.0
7.2
15.9

0.08
0.29
0.34
1.58

0

0.31
0.32
1.66

0.6
3.0
5.4
65.8

Cl−
0.01
0.05
0.06
0.61

Free
SO2

1
29
31
289

Total
SO2

6

118
115
440

Density

PH

2− Alcohol

SO4

0.987
0.995
0.994
1.039

2.72
3.20
3.22
4.01

0.22
0.50
0.53
2.00

8.0
10.3
10.5
14.9

Min.

Median
Mean
Max.

(a) Histogram of wine quality score

(b) Correlation matrix

(c) Correlation matrix

Figure 1: Basic data description

4 Ensemble Methodology

4.1 Single Model Methods and Basic Analysis

We ﬁt the following prediction models to our training data set :
1. Ordinary Least Square (OLS) Regression
2. Robust Linear Regression - M-estimation(RLM), Least Trimmed Squares (LTS)
3. Weighted Least Square (WLS) Regression - weights : reciprocal of the square root of the size of each score
4. L1 regression : backﬁtting + WLS (L1R)

1This dataset is given by Prof. T. Hastie and originally used in STATS 315A class. In this simulation, to keep conﬁdentiality of

the original data set, we only use a part of training set of the original dataset.

2

CS 229 Final report

Kwhangho Kim, Jeha Yang

5. Variable Selection - Forward Selection (FWD), Backward Selection (BWD), All-subset Selection (ALL)
6. Penalized Least Squares - Ridge, Lasso, Elastic Net
7. Principle Component Regression (PCR)
8. Partial Least Squares (PLS) Regression
9. Basis Expansions and Regularization - Natural Cubic Splines with Lasso (NCS)
10. Local Polynomial Regression (LPR) with selected variables from 5
11. Support Vector Machine (SVM) - Linear Kernel, Polynomial Kernel, Radial Kernel
12. Linear Discriminant Analysis (LDA)
13. Logistic Regression (LOG)
14. K-nearest neighbor (K-nn)
Note the following details of these methods :

Regression models, from 1 to 10, give real-valued scores, which are then rounded oﬀ to be integers.
In the WLS Regression model, weights are chosen to avoid the phenomenon that a model mostly
predicts major scores due to not predictors but its criterion.
SVM with k labels is the one-against-one-approach, in which k(k-1)/2 binary classiﬁers are trained
by SVM with 2 labels and majority vote determines the label of a data point.

We conducted 5-fold CVs for diﬀerent learning methods to estimate the generalization errors, using 4500 data
points randomly chosen from the training data set(the remaining 300 data points is set to be the test set).

Table 1: RMSEs for diﬀerent learning methods

L1R ElaNet PCR NCS
0.731
0.792
0.936
TEST 0.802

0.731
0.792

0.769
0.856

CV

LPR SVM LDA LOG
0.724
1.501
1.343
0.806

0.717
0.723

0.726
0.781

Note that 8 methods were chosen to be representatives of redundant methods and independent of each other.
Also, SVM denotes SVM with the radial kernel from now on.
From Table 1, we can see that all methods show mediocre performance, although classiﬁcation models are slightly
better than regressions. If we can pick up models which are good to predict the major labels and the others which
is good to predict the minor labels and ensemble them, it may contribute to the overall performance improvement.

4.2 Ensemble Algorithms

First we construct a simple heuristic ensemble classiﬁer using the idea described above. Let’s look at the per
class(score) misclassiﬁcation rates for typical methodologies selected above.

Table 2: Per class misclassiﬁcation rates

Score L1R ElaNet PCR NCS LPR SVM LDA LOG
100
98.8
48.7
35.1
78.3
91.4
100

77.3
87.7
41.5
31.2
69.7
99.3
100

100
94.5
42.2
20.3
55.0
73.1
100

100
98.2
44.6
26.1
75.5
100
100

95.5
100
43.7
28.1
75.5
100
100

100
98.8
47.9
23.7
75.3
100
100

3
4
5
6
7
8
9

100
98.8
46.8
25.0
74.3
100
100

100
98.8
47.6
23.6
75.2
100
100

•
•
•

•
•

From this table, we can observe that

SVM outperforms the others for major classes, and it is the only classiﬁer which successfully distin-
guishes some of the class 8 data points.
LDA classiﬁer performs much better job for class 3 and 4 than regressions and SVMs did.

Based on these observations, we decided to combine three classiﬁers SVM, LDA, and elastic-net (which is included
because it may have predictive power related to regression settings, although being dominated by SVM) in the

3

CS 229 Final report

A Study Of Ensemble Methods In Machine Learning

Kwhangho Kim, Jeha Yang

Abstract

The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is
well-known that ensemble methods can be used for improving prediction performance.
In this project we
provide novel methods of combining multiple learners in classiﬁcation tasks. We also present an empirical
study with wine quality score data set and demonstrate superior predictive power of the proposed ensemble
methods over simple voting or single model methods.

1 Introduction

An ensemble of classiﬁers is a set of classiﬁers whose individual decisions are combined in some way, typically
by weighted or unweighted voting, to classify new examples with the goal of improving accuracy and reliability.
As combining diverse, independent opinions in human decision-making to pursue a more protective mechanism,
the Ensemble model provides an eﬃcient way to improve accuracy and robustness over single model methods.
Ensemble methods provide a huge practical usefulness in that it has the promise of reducing and perhaps even
eliminating some key shortcomings of standard learning algorithms.
In this project, we focus on classiﬁer ensembles and propose novel algorithms of combining method. Our methods
provide very simple yet eﬀective way to determine optimal weight of each classiﬁer in a sense that it seeks not
only empirical risk minimization but diversiﬁcation of classiﬁers.
For application, we analyze Napa Valley Wine Quality data and show that all of our ensemble algorithms produce
way better predictive performance than any single method. Our methods also show superior performance than
simple voting method.

2 Related Work

In the recent years, experimental studies conducted by the machine-learning community show that combining the
outputs of multiple classiﬁers reduces the generalization error (Quinlan, 1996, Opitz and Maclin, 1999, Kuncheva
et al., 2004, Rokach, 2006). Ensemble methods are very eﬀective, mainly due to the phenomenon that various
types of classiﬁers have diﬀerent inductive biases (Geman et al., 1995, Mitchell, 1997). Indeed, ensemble methods
can eﬀectively make use of such diversity to reduce the variance-error (Tumer and Ghosh, 1999, Ali and Pazzani,
1996) without increasing the bias-error. In certain situations, an ensemble can also reduce bias-error, as shown
by the theory of large margin classiﬁers (Bartlett and Shawe-Taylor, 1998).
Given the potential usefulness of ensemble methods, it is not surprising that a vast number of methods is now
available to researchers and practitioners. There are several factors that diﬀerentiate between the various ensem-
bles methods. Rokach (2010) summarizes four factors as below:

1.
2.
3.
4.

Inter-classiﬁers relationship - How does each classiﬁer aﬀect the other classiﬁers
Combining method - The strategy of combining the classiﬁers
Diversity generator - How should we produce some sort of diversity between the classiﬁers
Ensemble size - The number of classiﬁers in the ensemble

In this project, we particularly focus on the combining method. Ali and Pazzani (1996) have compared several
combination methods. More theoretical analysis has been developed for estimating the classiﬁcation improvement
by Tumer and Ghosh (1999).

1

CS 229 Final report

Kwhangho Kim, Jeha Yang

3 Application : Napa Valley Wine Quality Score data

To test whether our proposed ensemble algorithms really work and to see how much improvement can be made,
we conduct an empirical study. For this empirical study we use Napa Valley Wine Quality Score data 1.

3.1 Purpose

A wine rating is a score assigned by one or more professional wine critics to a wine tasted as a summary of that
critic’s evaluation of that wine. This wine quality score is assigned after the wines have been released to the
market. Since it directly aﬀects the price and demand of the wine, for the wine companies predicting the wine
quality score is very important.
We want to predict the wine quality score with given soil conditions of the year in which the wine was produced.

3.2 Data Description

• We have 4800 (complete) data points.
• Wine Quality Score is integer-valued, and ranges from 3 to 9.
•
•

There are 13 predictors : X (labeling information), color, ﬁxed.acidity, volatile.acidity, citric.acid,
residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol
The training data is imbalanced ; that is, there are major (5,6,7) labels that appear most of the time
and minor (3,4,8,9) labels.

X

Color

2

Red:
1189
3234
3241 While:
3611
6497

Fixed
acidity

Volatile
acidity

Citric
acid

Residual

sugar

4.2
7.0
7.2
15.9

0.08
0.29
0.34
1.58

0

0.31
0.32
1.66

0.6
3.0
5.4
65.8

Cl−
0.01
0.05
0.06
0.61

Free
SO2

1
29
31
289

Total
SO2

6

118
115
440

Density

PH

2− Alcohol

SO4

0.987
0.995
0.994
1.039

2.72
3.20
3.22
4.01

0.22
0.50
0.53
2.00

8.0
10.3
10.5
14.9

Min.

Median
Mean
Max.

(a) Histogram of wine quality score

(b) Correlation matrix

(c) Correlation matrix

Figure 1: Basic data description

4 Ensemble Methodology

4.1 Single Model Methods and Basic Analysis

We ﬁt the following prediction models to our training data set :
1. Ordinary Least Square (OLS) Regression
2. Robust Linear Regression - M-estimation(RLM), Least Trimmed Squares (LTS)
3. Weighted Least Square (WLS) Regression - weights : reciprocal of the square root of the size of each score
4. L1 regression : backﬁtting + WLS (L1R)

1This dataset is given by Prof. T. Hastie and originally used in STATS 315A class. In this simulation, to keep conﬁdentiality of

the original data set, we only use a part of training set of the original dataset.

2

CS 229 Final report

Kwhangho Kim, Jeha Yang

5. Variable Selection - Forward Selection (FWD), Backward Selection (BWD), All-subset Selection (ALL)
6. Penalized Least Squares - Ridge, Lasso, Elastic Net
7. Principle Component Regression (PCR)
8. Partial Least Squares (PLS) Regression
9. Basis Expansions and Regularization - Natural Cubic Splines with Lasso (NCS)
10. Local Polynomial Regression (LPR) with selected variables from 5
11. Support Vector Machine (SVM) - Linear Kernel, Polynomial Kernel, Radial Kernel
12. Linear Discriminant Analysis (LDA)
13. Logistic Regression (LOG)
14. K-nearest neighbor (K-nn)
Note the following details of these methods :

Regression models, from 1 to 10, give real-valued scores, which are then rounded oﬀ to be integers.
In the WLS Regression model, weights are chosen to avoid the phenomenon that a model mostly
predicts major scores due to not predictors but its criterion.
SVM with k labels is the one-against-one-approach, in which k(k-1)/2 binary classiﬁers are trained
by SVM with 2 labels and majority vote determines the label of a data point.

We conducted 5-fold CVs for diﬀerent learning methods to estimate the generalization errors, using 4500 data
points randomly chosen from the training data set(the remaining 300 data points is set to be the test set).

Table 1: RMSEs for diﬀerent learning methods

L1R ElaNet PCR NCS
0.731
0.792
0.936
TEST 0.802

0.731
0.792

0.769
0.856

CV

LPR SVM LDA LOG
0.724
1.501
1.343
0.806

0.717
0.723

0.726
0.781

Note that 8 methods were chosen to be representatives of redundant methods and independent of each other.
Also, SVM denotes SVM with the radial kernel from now on.
From Table 1, we can see that all methods show mediocre performance, although classiﬁcation models are slightly
better than regressions. If we can pick up models which are good to predict the major labels and the others which
is good to predict the minor labels and ensemble them, it may contribute to the overall performance improvement.

4.2 Ensemble Algorithms

First we construct a simple heuristic ensemble classiﬁer using the idea described above. Let’s look at the per
class(score) misclassiﬁcation rates for typical methodologies selected above.

Table 2: Per class misclassiﬁcation rates

Score L1R ElaNet PCR NCS LPR SVM LDA LOG
100
98.8
48.7
35.1
78.3
91.4
100

77.3
87.7
41.5
31.2
69.7
99.3
100

100
94.5
42.2
20.3
55.0
73.1
100

100
98.2
44.6
26.1
75.5
100
100

95.5
100
43.7
28.1
75.5
100
100

100
98.8
47.9
23.7
75.3
100
100

3
4
5
6
7
8
9

100
98.8
46.8
25.0
74.3
100
100

100
98.8
47.6
23.6
75.2
100
100

•
•
•

•
•

From this table, we can observe that

SVM outperforms the others for major classes, and it is the only classiﬁer which successfully distin-
guishes some of the class 8 data points.
LDA classiﬁer performs much better job for class 3 and 4 than regressions and SVMs did.

Based on these observations, we decided to combine three classiﬁers SVM, LDA, and elastic-net (which is included
because it may have predictive power related to regression settings, although being dominated by SVM) in the

3

CS 229 Final report

Kwhangho Kim, Jeha Yang

way that strengthens the strengths and makes up for the weaknesses ; that is, put the weights decreasing in the
order of SVM, LDA, and elastic-net. When choosing weights under this scheme, we did several experiments with
diﬀerent combinations of the weights and empirically chose the best model among them without using any formal
optimization over the training data. The prediction rule obtained from our heuristics is as follows :
Algorithm 1 : Ensemble Method based on simple heuristics (Heuristic)

Prediction = 0.7 SVM + 0.25 LDA + 0.05 ElaNet

CV it follows that N =(cid:80)K
the kth CV fold, respectively. Finally let the matrix X (k) denote(cid:2)ˆy(−k,1) ˆy(−k,2) ··· ˆy(−k,M )(cid:3). Then we obtain a

Next, we discuss a reasonable process to ﬁnd the optimal weight vector more systematically. Suppose that we
have a ﬁnite set of classes {1,··· , C} and learning methods L1,··· , LM for classifying these classes (using the
given predictors and data set), and use K-fold CV. Let N denote the number of observations so with the K-fold
k=1 nk, where in our data nk = 900 for all k = 1, ..., K. For each (k, m), let y(k) and
ˆy(−k,m) denote the realized value of classes for kth CV fold and the predicted value of classes for kth CV fold
based on the learning method Lm trained on the rest of the data, i.e. trained on the data points taken out for
nk × 1 vector y(k), and nk × M matrix X (k) for our analysis. Now consider the following algorithm.
Algorithm 2 : Ensemble Method based on l2 Minimization of CV Errors (L2CV)
With given constructions {X (k)}k and {y(k)}k, we ﬁnd the M × 1 optimal ensemble weight vector ω∗ as

argmin
ω∈RM

||y(k) − X (k)ω||2

2 = argmin
ω∈RM

ωT

X (k)T

X (k)ω − 2

y(k)T

X (k)ω s.t. 1Tω = 1, ω ≥ 0

k=1

k=1

K(cid:88)

K(cid:88)

K(cid:88)

k=1

It can easily be shown that Algorithm 2 has lower CV RMSE (without rounding predictions, which is necessary
for the comparison and expected to have a little eﬀect on CV RMSE) than any of single learning methods, by
letting ω be columns of the M × M identity matrix. From this algorithm, we have the following ensemble:

Prediction = 0.092 NS + 0.170 LPR + 0.507 SVM + 0.231 LDA

Now recall that we used misclassiﬁcation rates per class to choose learning methods and give them weights.
Similarly, we could instead use Squared Errors Per Class(SSEPC), to be consistent with RMSE. Here we propose
2 automatic ensemble methods based only on SSEPC. The ﬁrst one is a modiﬁcation of Algorithm 2.
Algorithm 3 : Simple SSEPC Ensemble (SSEPC)
For each (k, m, c), let s(k)

cm)1≤c≤C,1≤m≤M ∈ RC×M . Simple

(cid:113)(cid:80)
SSEPC Ensemble is deﬁned by(cid:80)M

− y(−k)

)2 and S(k) := (s(k)

=c(ˆy(−k,m)
mMm, where α∗ is the solution of the quadratic programming problem
K(cid:88)

S(k)T S(k)α s.t. α ≥ 0, 1T α = 1

i:y(−k)
m=1 α∗

cm :=

αT

i

i

i

min
α∈RM

k=1

This optimizes an upper bound of the CV RMSE(without rounding), and has the same property as Algorithm 2 :

The CV RMSE of the simple SSEPC ensemble is smaller than those of L1,··· , LM ,

To see these, let’s consider the kth CV fold, and omit superscripts k and −k from y’s for simplicity. We can then

obtain an upper bound of SSE (from the kth CV fold) of an ensemble method(cid:80)M

m=1 αmLm as follows :

SSEk :=

i

(cid:88)
C(cid:88)
≤ C(cid:88)

c=1

(
m=1

M(cid:88)
M(cid:88)
M(cid:88)

[
m=1

=

[

c=1

m=1

(cid:88)

M(cid:88)

[

i

m=1

αm ˆy(m)

i − yi)2 =
(cid:88)

α2
m

i:yi=c

i − yi)2 + 2
(ˆy(m)

(cid:88)

ms(k)
α2
cm

2

+ 2

1≤l<m≤M

m(ˆy(m)
α2

(cid:88)

i − yi)2 + 2
(cid:88)
C(cid:88)
M(cid:88)

αlαm

i:yi=c

1≤l<m≤M

αlαms(k)

cl s(k)

cm] =

[

1≤l<m≤M
i − yi)(ˆy(m)
(ˆy(l)

i − yi)]

αms(k)

cm]2 = (cid:107)S(k)α(cid:107)2

2

(cid:88)

αlαm(ˆy(l)

i − yi)(ˆy(m)

i − yi)]

c=1

m=1

4

CS 229 Final report

A Study Of Ensemble Methods In Machine Learning

Kwhangho Kim, Jeha Yang

Abstract

The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is
well-known that ensemble methods can be used for improving prediction performance.
In this project we
provide novel methods of combining multiple learners in classiﬁcation tasks. We also present an empirical
study with wine quality score data set and demonstrate superior predictive power of the proposed ensemble
methods over simple voting or single model methods.

1 Introduction

An ensemble of classiﬁers is a set of classiﬁers whose individual decisions are combined in some way, typically
by weighted or unweighted voting, to classify new examples with the goal of improving accuracy and reliability.
As combining diverse, independent opinions in human decision-making to pursue a more protective mechanism,
the Ensemble model provides an eﬃcient way to improve accuracy and robustness over single model methods.
Ensemble methods provide a huge practical usefulness in that it has the promise of reducing and perhaps even
eliminating some key shortcomings of standard learning algorithms.
In this project, we focus on classiﬁer ensembles and propose novel algorithms of combining method. Our methods
provide very simple yet eﬀective way to determine optimal weight of each classiﬁer in a sense that it seeks not
only empirical risk minimization but diversiﬁcation of classiﬁers.
For application, we analyze Napa Valley Wine Quality data and show that all of our ensemble algorithms produce
way better predictive performance than any single method. Our methods also show superior performance than
simple voting method.

2 Related Work

In the recent years, experimental studies conducted by the machine-learning community show that combining the
outputs of multiple classiﬁers reduces the generalization error (Quinlan, 1996, Opitz and Maclin, 1999, Kuncheva
et al., 2004, Rokach, 2006). Ensemble methods are very eﬀective, mainly due to the phenomenon that various
types of classiﬁers have diﬀerent inductive biases (Geman et al., 1995, Mitchell, 1997). Indeed, ensemble methods
can eﬀectively make use of such diversity to reduce the variance-error (Tumer and Ghosh, 1999, Ali and Pazzani,
1996) without increasing the bias-error. In certain situations, an ensemble can also reduce bias-error, as shown
by the theory of large margin classiﬁers (Bartlett and Shawe-Taylor, 1998).
Given the potential usefulness of ensemble methods, it is not surprising that a vast number of methods is now
available to researchers and practitioners. There are several factors that diﬀerentiate between the various ensem-
bles methods. Rokach (2010) summarizes four factors as below:

1.
2.
3.
4.

Inter-classiﬁers relationship - How does each classiﬁer aﬀect the other classiﬁers
Combining method - The strategy of combining the classiﬁers
Diversity generator - How should we produce some sort of diversity between the classiﬁers
Ensemble size - The number of classiﬁers in the ensemble

In this project, we particularly focus on the combining method. Ali and Pazzani (1996) have compared several
combination methods. More theoretical analysis has been developed for estimating the classiﬁcation improvement
by Tumer and Ghosh (1999).

1

CS 229 Final report

Kwhangho Kim, Jeha Yang

3 Application : Napa Valley Wine Quality Score data

To test whether our proposed ensemble algorithms really work and to see how much improvement can be made,
we conduct an empirical study. For this empirical study we use Napa Valley Wine Quality Score data 1.

3.1 Purpose

A wine rating is a score assigned by one or more professional wine critics to a wine tasted as a summary of that
critic’s evaluation of that wine. This wine quality score is assigned after the wines have been released to the
market. Since it directly aﬀects the price and demand of the wine, for the wine companies predicting the wine
quality score is very important.
We want to predict the wine quality score with given soil conditions of the year in which the wine was produced.

3.2 Data Description

• We have 4800 (complete) data points.
• Wine Quality Score is integer-valued, and ranges from 3 to 9.
•
•

There are 13 predictors : X (labeling information), color, ﬁxed.acidity, volatile.acidity, citric.acid,
residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol
The training data is imbalanced ; that is, there are major (5,6,7) labels that appear most of the time
and minor (3,4,8,9) labels.

X

Color

2

Red:
1189
3234
3241 While:
3611
6497

Fixed
acidity

Volatile
acidity

Citric
acid

Residual

sugar

4.2
7.0
7.2
15.9

0.08
0.29
0.34
1.58

0

0.31
0.32
1.66

0.6
3.0
5.4
65.8

Cl−
0.01
0.05
0.06
0.61

Free
SO2

1
29
31
289

Total
SO2

6

118
115
440

Density

PH

2− Alcohol

SO4

0.987
0.995
0.994
1.039

2.72
3.20
3.22
4.01

0.22
0.50
0.53
2.00

8.0
10.3
10.5
14.9

Min.

Median
Mean
Max.

(a) Histogram of wine quality score

(b) Correlation matrix

(c) Correlation matrix

Figure 1: Basic data description

4 Ensemble Methodology

4.1 Single Model Methods and Basic Analysis

We ﬁt the following prediction models to our training data set :
1. Ordinary Least Square (OLS) Regression
2. Robust Linear Regression - M-estimation(RLM), Least Trimmed Squares (LTS)
3. Weighted Least Square (WLS) Regression - weights : reciprocal of the square root of the size of each score
4. L1 regression : backﬁtting + WLS (L1R)

1This dataset is given by Prof. T. Hastie and originally used in STATS 315A class. In this simulation, to keep conﬁdentiality of

the original data set, we only use a part of training set of the original dataset.

2

CS 229 Final report

Kwhangho Kim, Jeha Yang

5. Variable Selection - Forward Selection (FWD), Backward Selection (BWD), All-subset Selection (ALL)
6. Penalized Least Squares - Ridge, Lasso, Elastic Net
7. Principle Component Regression (PCR)
8. Partial Least Squares (PLS) Regression
9. Basis Expansions and Regularization - Natural Cubic Splines with Lasso (NCS)
10. Local Polynomial Regression (LPR) with selected variables from 5
11. Support Vector Machine (SVM) - Linear Kernel, Polynomial Kernel, Radial Kernel
12. Linear Discriminant Analysis (LDA)
13. Logistic Regression (LOG)
14. K-nearest neighbor (K-nn)
Note the following details of these methods :

Regression models, from 1 to 10, give real-valued scores, which are then rounded oﬀ to be integers.
In the WLS Regression model, weights are chosen to avoid the phenomenon that a model mostly
predicts major scores due to not predictors but its criterion.
SVM with k labels is the one-against-one-approach, in which k(k-1)/2 binary classiﬁers are trained
by SVM with 2 labels and majority vote determines the label of a data point.

We conducted 5-fold CVs for diﬀerent learning methods to estimate the generalization errors, using 4500 data
points randomly chosen from the training data set(the remaining 300 data points is set to be the test set).

Table 1: RMSEs for diﬀerent learning methods

L1R ElaNet PCR NCS
0.731
0.792
0.936
TEST 0.802

0.731
0.792

0.769
0.856

CV

LPR SVM LDA LOG
0.724
1.501
1.343
0.806

0.717
0.723

0.726
0.781

Note that 8 methods were chosen to be representatives of redundant methods and independent of each other.
Also, SVM denotes SVM with the radial kernel from now on.
From Table 1, we can see that all methods show mediocre performance, although classiﬁcation models are slightly
better than regressions. If we can pick up models which are good to predict the major labels and the others which
is good to predict the minor labels and ensemble them, it may contribute to the overall performance improvement.

4.2 Ensemble Algorithms

First we construct a simple heuristic ensemble classiﬁer using the idea described above. Let’s look at the per
class(score) misclassiﬁcation rates for typical methodologies selected above.

Table 2: Per class misclassiﬁcation rates

Score L1R ElaNet PCR NCS LPR SVM LDA LOG
100
98.8
48.7
35.1
78.3
91.4
100

77.3
87.7
41.5
31.2
69.7
99.3
100

100
94.5
42.2
20.3
55.0
73.1
100

100
98.2
44.6
26.1
75.5
100
100

95.5
100
43.7
28.1
75.5
100
100

100
98.8
47.9
23.7
75.3
100
100

3
4
5
6
7
8
9

100
98.8
46.8
25.0
74.3
100
100

100
98.8
47.6
23.6
75.2
100
100

•
•
•

•
•

From this table, we can observe that

SVM outperforms the others for major classes, and it is the only classiﬁer which successfully distin-
guishes some of the class 8 data points.
LDA classiﬁer performs much better job for class 3 and 4 than regressions and SVMs did.

Based on these observations, we decided to combine three classiﬁers SVM, LDA, and elastic-net (which is included
because it may have predictive power related to regression settings, although being dominated by SVM) in the

3

CS 229 Final report

Kwhangho Kim, Jeha Yang

way that strengthens the strengths and makes up for the weaknesses ; that is, put the weights decreasing in the
order of SVM, LDA, and elastic-net. When choosing weights under this scheme, we did several experiments with
diﬀerent combinations of the weights and empirically chose the best model among them without using any formal
optimization over the training data. The prediction rule obtained from our heuristics is as follows :
Algorithm 1 : Ensemble Method based on simple heuristics (Heuristic)

Prediction = 0.7 SVM + 0.25 LDA + 0.05 ElaNet

CV it follows that N =(cid:80)K
the kth CV fold, respectively. Finally let the matrix X (k) denote(cid:2)ˆy(−k,1) ˆy(−k,2) ··· ˆy(−k,M )(cid:3). Then we obtain a

Next, we discuss a reasonable process to ﬁnd the optimal weight vector more systematically. Suppose that we
have a ﬁnite set of classes {1,··· , C} and learning methods L1,··· , LM for classifying these classes (using the
given predictors and data set), and use K-fold CV. Let N denote the number of observations so with the K-fold
k=1 nk, where in our data nk = 900 for all k = 1, ..., K. For each (k, m), let y(k) and
ˆy(−k,m) denote the realized value of classes for kth CV fold and the predicted value of classes for kth CV fold
based on the learning method Lm trained on the rest of the data, i.e. trained on the data points taken out for
nk × 1 vector y(k), and nk × M matrix X (k) for our analysis. Now consider the following algorithm.
Algorithm 2 : Ensemble Method based on l2 Minimization of CV Errors (L2CV)
With given constructions {X (k)}k and {y(k)}k, we ﬁnd the M × 1 optimal ensemble weight vector ω∗ as

argmin
ω∈RM

||y(k) − X (k)ω||2

2 = argmin
ω∈RM

ωT

X (k)T

X (k)ω − 2

y(k)T

X (k)ω s.t. 1Tω = 1, ω ≥ 0

k=1

k=1

K(cid:88)

K(cid:88)

K(cid:88)

k=1

It can easily be shown that Algorithm 2 has lower CV RMSE (without rounding predictions, which is necessary
for the comparison and expected to have a little eﬀect on CV RMSE) than any of single learning methods, by
letting ω be columns of the M × M identity matrix. From this algorithm, we have the following ensemble:

Prediction = 0.092 NS + 0.170 LPR + 0.507 SVM + 0.231 LDA

Now recall that we used misclassiﬁcation rates per class to choose learning methods and give them weights.
Similarly, we could instead use Squared Errors Per Class(SSEPC), to be consistent with RMSE. Here we propose
2 automatic ensemble methods based only on SSEPC. The ﬁrst one is a modiﬁcation of Algorithm 2.
Algorithm 3 : Simple SSEPC Ensemble (SSEPC)
For each (k, m, c), let s(k)

cm)1≤c≤C,1≤m≤M ∈ RC×M . Simple

(cid:113)(cid:80)
SSEPC Ensemble is deﬁned by(cid:80)M

− y(−k)

)2 and S(k) := (s(k)

=c(ˆy(−k,m)
mMm, where α∗ is the solution of the quadratic programming problem
K(cid:88)

S(k)T S(k)α s.t. α ≥ 0, 1T α = 1

i:y(−k)
m=1 α∗

cm :=

αT

i

i

i

min
α∈RM

k=1

This optimizes an upper bound of the CV RMSE(without rounding), and has the same property as Algorithm 2 :

The CV RMSE of the simple SSEPC ensemble is smaller than those of L1,··· , LM ,

To see these, let’s consider the kth CV fold, and omit superscripts k and −k from y’s for simplicity. We can then

obtain an upper bound of SSE (from the kth CV fold) of an ensemble method(cid:80)M

m=1 αmLm as follows :

SSEk :=

i

(cid:88)
C(cid:88)
≤ C(cid:88)

c=1

(
m=1

M(cid:88)
M(cid:88)
M(cid:88)

[
m=1

=

[

c=1

m=1

(cid:88)

M(cid:88)

[

i

m=1

αm ˆy(m)

i − yi)2 =
(cid:88)

α2
m

i:yi=c

i − yi)2 + 2
(ˆy(m)

(cid:88)

ms(k)
α2
cm

2

+ 2

1≤l<m≤M

m(ˆy(m)
α2

(cid:88)

i − yi)2 + 2
(cid:88)
C(cid:88)
M(cid:88)

αlαm

i:yi=c

1≤l<m≤M

αlαms(k)

cl s(k)

cm] =

[

1≤l<m≤M
i − yi)(ˆy(m)
(ˆy(l)

i − yi)]

αms(k)

cm]2 = (cid:107)S(k)α(cid:107)2

2

(cid:88)

αlαm(ˆy(l)

i − yi)(ˆy(m)

i − yi)]

c=1

m=1

4

CS 229 Final report

Kwhangho Kim, Jeha Yang

S(k). Summing up these upper bounds for k = 1,··· , K gives SSE := (cid:80)K
k=1 SSEk ≤ (cid:80)K
αT(cid:80)K
(cid:80)C
2 = (cid:80)K
when αm = 1, αl = 0 ∀l (cid:54)= m, i.e., α∗T(cid:80)K

where the inequality is due to the Cauchy-Schwarz inequality, which makes the upper bound a function of
2 =
is the SSE of Lm
k=1 S(k)T S(k)α∗ ≤ (SSE’s of L1,··· , LM ) ··· (2). Combining (1) and

k=1 S(k)T S(k)α ··· (1). Furthurmore, note that (cid:80)K

k=1 (cid:107)S(k)α(cid:107)2

k=1 (cid:107)S(k)α(cid:107)2

2

c=1 s(k)

cm

k=1

(2) results in the claim above. Applying Algorithm 3 to our data resulted in the following sparse ensemble

Prediction = 0.050 LPR + 0.950 SVM

Finally, we revisited the ﬁrst heuristic idea and came up with the following algorithm.
Algorithm 4 : Forward Stepwise SSEPC Ensemble (FWD SSEPC)
Let R be the C × M matrix of CV SSE per class (hence its rows are classes).
(1) Deﬁne the best single model as the current model, and update R by removing the corresponding column.
(2) For each column of R, divide each element by the corresponding CV SSE per class of the current model, to
obtain the matrix of those ratios.
(3) If all of those ratios are larger than 1, stop ; otherwise, ﬁnd the column with the smallest sum of ratios,
include the corresponding single model to the current set of models, and update the model by using Algorithm 2
of the new set of models and R by removing the corresponding column.
(4) Repeat 2 and 3 until convergence (in terms of the supremum of the change in weight over all single models)
Let’s discuss the ideas of step 2 and 3. First of all, the smaller the ratio of the SSE of a class between a single
model and the current model is, the better the single model works in the corresponding class. Therefore, summing
up those ratios over all classes could be considered as a measure of how much a single model complements the
current ensemble model, treating all classes equally. Furthermore, if those ratios are greater than 1 for all classes,
than we don’t need to include the corresponding single model to the ensemble.
Implementing this algorithm gave the ensemble

Prediction = 0.520 SVM + 0.373 LDA + 0.107 L1R

5 Results and Discussion

Below are the experiment results of 4 algorithms we discussed so far.

Table 3: RMSEs for Ensemble Methods

Heuristic L2CV SSEPC FWD SSEPC

CV Set
Test Set

0.679
0.683

0.669
0.687

0.709
0.718

0.673
0.687

By comparing Table 3 with Table 1, it is found that

1.
2.

3.

All four ensemble methods perform better than any of single classiﬁers in terms of RMSE.
Ensemble methods show their strengths particularly when they combine the complementary predictive
powers of multiple models.
In particular, we can observe that the background ideas of FWD SSEPC worked in the sense that
LDA was picked following SVM as done in the heuristic ensemble, although the next step actually
increased the RMSE from 0.688 to 0.695.

6 Future Work

•

•

From the results, L2CV and FWD SSEPC showed similar performances. Therefore, we can further
compare them via experiments or theoretical analysis. Also, it can be examined if simple SSEPC
always guarantees sparse ensemble models, as shown in our experiment.
To improve FWD SSEPC, we can try to develop a measure for the third step as a function of the
ratios of SSEs, other than the one we used before.

5

CS 229 Final report

A Study Of Ensemble Methods In Machine Learning

Kwhangho Kim, Jeha Yang

Abstract

The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is
well-known that ensemble methods can be used for improving prediction performance.
In this project we
provide novel methods of combining multiple learners in classiﬁcation tasks. We also present an empirical
study with wine quality score data set and demonstrate superior predictive power of the proposed ensemble
methods over simple voting or single model methods.

1 Introduction

An ensemble of classiﬁers is a set of classiﬁers whose individual decisions are combined in some way, typically
by weighted or unweighted voting, to classify new examples with the goal of improving accuracy and reliability.
As combining diverse, independent opinions in human decision-making to pursue a more protective mechanism,
the Ensemble model provides an eﬃcient way to improve accuracy and robustness over single model methods.
Ensemble methods provide a huge practical usefulness in that it has the promise of reducing and perhaps even
eliminating some key shortcomings of standard learning algorithms.
In this project, we focus on classiﬁer ensembles and propose novel algorithms of combining method. Our methods
provide very simple yet eﬀective way to determine optimal weight of each classiﬁer in a sense that it seeks not
only empirical risk minimization but diversiﬁcation of classiﬁers.
For application, we analyze Napa Valley Wine Quality data and show that all of our ensemble algorithms produce
way better predictive performance than any single method. Our methods also show superior performance than
simple voting method.

2 Related Work

In the recent years, experimental studies conducted by the machine-learning community show that combining the
outputs of multiple classiﬁers reduces the generalization error (Quinlan, 1996, Opitz and Maclin, 1999, Kuncheva
et al., 2004, Rokach, 2006). Ensemble methods are very eﬀective, mainly due to the phenomenon that various
types of classiﬁers have diﬀerent inductive biases (Geman et al., 1995, Mitchell, 1997). Indeed, ensemble methods
can eﬀectively make use of such diversity to reduce the variance-error (Tumer and Ghosh, 1999, Ali and Pazzani,
1996) without increasing the bias-error. In certain situations, an ensemble can also reduce bias-error, as shown
by the theory of large margin classiﬁers (Bartlett and Shawe-Taylor, 1998).
Given the potential usefulness of ensemble methods, it is not surprising that a vast number of methods is now
available to researchers and practitioners. There are several factors that diﬀerentiate between the various ensem-
bles methods. Rokach (2010) summarizes four factors as below:

1.
2.
3.
4.

Inter-classiﬁers relationship - How does each classiﬁer aﬀect the other classiﬁers
Combining method - The strategy of combining the classiﬁers
Diversity generator - How should we produce some sort of diversity between the classiﬁers
Ensemble size - The number of classiﬁers in the ensemble

In this project, we particularly focus on the combining method. Ali and Pazzani (1996) have compared several
combination methods. More theoretical analysis has been developed for estimating the classiﬁcation improvement
by Tumer and Ghosh (1999).

1

CS 229 Final report

Kwhangho Kim, Jeha Yang

3 Application : Napa Valley Wine Quality Score data

To test whether our proposed ensemble algorithms really work and to see how much improvement can be made,
we conduct an empirical study. For this empirical study we use Napa Valley Wine Quality Score data 1.

3.1 Purpose

A wine rating is a score assigned by one or more professional wine critics to a wine tasted as a summary of that
critic’s evaluation of that wine. This wine quality score is assigned after the wines have been released to the
market. Since it directly aﬀects the price and demand of the wine, for the wine companies predicting the wine
quality score is very important.
We want to predict the wine quality score with given soil conditions of the year in which the wine was produced.

3.2 Data Description

• We have 4800 (complete) data points.
• Wine Quality Score is integer-valued, and ranges from 3 to 9.
•
•

There are 13 predictors : X (labeling information), color, ﬁxed.acidity, volatile.acidity, citric.acid,
residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, pH, sulphates, alcohol
The training data is imbalanced ; that is, there are major (5,6,7) labels that appear most of the time
and minor (3,4,8,9) labels.

X

Color

2

Red:
1189
3234
3241 While:
3611
6497

Fixed
acidity

Volatile
acidity

Citric
acid

Residual

sugar

4.2
7.0
7.2
15.9

0.08
0.29
0.34
1.58

0

0.31
0.32
1.66

0.6
3.0
5.4
65.8

Cl−
0.01
0.05
0.06
0.61

Free
SO2

1
29
31
289

Total
SO2

6

118
115
440

Density

PH

2− Alcohol

SO4

0.987
0.995
0.994
1.039

2.72
3.20
3.22
4.01

0.22
0.50
0.53
2.00

8.0
10.3
10.5
14.9

Min.

Median
Mean
Max.

(a) Histogram of wine quality score

(b) Correlation matrix

(c) Correlation matrix

Figure 1: Basic data description

4 Ensemble Methodology

4.1 Single Model Methods and Basic Analysis

We ﬁt the following prediction models to our training data set :
1. Ordinary Least Square (OLS) Regression
2. Robust Linear Regression - M-estimation(RLM), Least Trimmed Squares (LTS)
3. Weighted Least Square (WLS) Regression - weights : reciprocal of the square root of the size of each score
4. L1 regression : backﬁtting + WLS (L1R)

1This dataset is given by Prof. T. Hastie and originally used in STATS 315A class. In this simulation, to keep conﬁdentiality of

the original data set, we only use a part of training set of the original dataset.

2

CS 229 Final report

Kwhangho Kim, Jeha Yang

5. Variable Selection - Forward Selection (FWD), Backward Selection (BWD), All-subset Selection (ALL)
6. Penalized Least Squares - Ridge, Lasso, Elastic Net
7. Principle Component Regression (PCR)
8. Partial Least Squares (PLS) Regression
9. Basis Expansions and Regularization - Natural Cubic Splines with Lasso (NCS)
10. Local Polynomial Regression (LPR) with selected variables from 5
11. Support Vector Machine (SVM) - Linear Kernel, Polynomial Kernel, Radial Kernel
12. Linear Discriminant Analysis (LDA)
13. Logistic Regression (LOG)
14. K-nearest neighbor (K-nn)
Note the following details of these methods :

Regression models, from 1 to 10, give real-valued scores, which are then rounded oﬀ to be integers.
In the WLS Regression model, weights are chosen to avoid the phenomenon that a model mostly
predicts major scores due to not predictors but its criterion.
SVM with k labels is the one-against-one-approach, in which k(k-1)/2 binary classiﬁers are trained
by SVM with 2 labels and majority vote determines the label of a data point.

We conducted 5-fold CVs for diﬀerent learning methods to estimate the generalization errors, using 4500 data
points randomly chosen from the training data set(the remaining 300 data points is set to be the test set).

Table 1: RMSEs for diﬀerent learning methods

L1R ElaNet PCR NCS
0.731
0.792
0.936
TEST 0.802

0.731
0.792

0.769
0.856

CV

LPR SVM LDA LOG
0.724
1.501
1.343
0.806

0.717
0.723

0.726
0.781

Note that 8 methods were chosen to be representatives of redundant methods and independent of each other.
Also, SVM denotes SVM with the radial kernel from now on.
From Table 1, we can see that all methods show mediocre performance, although classiﬁcation models are slightly
better than regressions. If we can pick up models which are good to predict the major labels and the others which
is good to predict the minor labels and ensemble them, it may contribute to the overall performance improvement.

4.2 Ensemble Algorithms

First we construct a simple heuristic ensemble classiﬁer using the idea described above. Let’s look at the per
class(score) misclassiﬁcation rates for typical methodologies selected above.

Table 2: Per class misclassiﬁcation rates

Score L1R ElaNet PCR NCS LPR SVM LDA LOG
100
98.8
48.7
35.1
78.3
91.4
100

77.3
87.7
41.5
31.2
69.7
99.3
100

100
94.5
42.2
20.3
55.0
73.1
100

100
98.2
44.6
26.1
75.5
100
100

95.5
100
43.7
28.1
75.5
100
100

100
98.8
47.9
23.7
75.3
100
100

3
4
5
6
7
8
9

100
98.8
46.8
25.0
74.3
100
100

100
98.8
47.6
23.6
75.2
100
100

•
•
•

•
•

From this table, we can observe that

SVM outperforms the others for major classes, and it is the only classiﬁer which successfully distin-
guishes some of the class 8 data points.
LDA classiﬁer performs much better job for class 3 and 4 than regressions and SVMs did.

Based on these observations, we decided to combine three classiﬁers SVM, LDA, and elastic-net (which is included
because it may have predictive power related to regression settings, although being dominated by SVM) in the

3

CS 229 Final report

Kwhangho Kim, Jeha Yang

way that strengthens the strengths and makes up for the weaknesses ; that is, put the weights decreasing in the
order of SVM, LDA, and elastic-net. When choosing weights under this scheme, we did several experiments with
diﬀerent combinations of the weights and empirically chose the best model among them without using any formal
optimization over the training data. The prediction rule obtained from our heuristics is as follows :
Algorithm 1 : Ensemble Method based on simple heuristics (Heuristic)

Prediction = 0.7 SVM + 0.25 LDA + 0.05 ElaNet

CV it follows that N =(cid:80)K
the kth CV fold, respectively. Finally let the matrix X (k) denote(cid:2)ˆy(−k,1) ˆy(−k,2) ··· ˆy(−k,M )(cid:3). Then we obtain a

Next, we discuss a reasonable process to ﬁnd the optimal weight vector more systematically. Suppose that we
have a ﬁnite set of classes {1,··· , C} and learning methods L1,··· , LM for classifying these classes (using the
given predictors and data set), and use K-fold CV. Let N denote the number of observations so with the K-fold
k=1 nk, where in our data nk = 900 for all k = 1, ..., K. For each (k, m), let y(k) and
ˆy(−k,m) denote the realized value of classes for kth CV fold and the predicted value of classes for kth CV fold
based on the learning method Lm trained on the rest of the data, i.e. trained on the data points taken out for
nk × 1 vector y(k), and nk × M matrix X (k) for our analysis. Now consider the following algorithm.
Algorithm 2 : Ensemble Method based on l2 Minimization of CV Errors (L2CV)
With given constructions {X (k)}k and {y(k)}k, we ﬁnd the M × 1 optimal ensemble weight vector ω∗ as

argmin
ω∈RM

||y(k) − X (k)ω||2

2 = argmin
ω∈RM

ωT

X (k)T

X (k)ω − 2

y(k)T

X (k)ω s.t. 1Tω = 1, ω ≥ 0

k=1

k=1

K(cid:88)

K(cid:88)

K(cid:88)

k=1

It can easily be shown that Algorithm 2 has lower CV RMSE (without rounding predictions, which is necessary
for the comparison and expected to have a little eﬀect on CV RMSE) than any of single learning methods, by
letting ω be columns of the M × M identity matrix. From this algorithm, we have the following ensemble:

Prediction = 0.092 NS + 0.170 LPR + 0.507 SVM + 0.231 LDA

Now recall that we used misclassiﬁcation rates per class to choose learning methods and give them weights.
Similarly, we could instead use Squared Errors Per Class(SSEPC), to be consistent with RMSE. Here we propose
2 automatic ensemble methods based only on SSEPC. The ﬁrst one is a modiﬁcation of Algorithm 2.
Algorithm 3 : Simple SSEPC Ensemble (SSEPC)
For each (k, m, c), let s(k)

cm)1≤c≤C,1≤m≤M ∈ RC×M . Simple

(cid:113)(cid:80)
SSEPC Ensemble is deﬁned by(cid:80)M

− y(−k)

)2 and S(k) := (s(k)

=c(ˆy(−k,m)
mMm, where α∗ is the solution of the quadratic programming problem
K(cid:88)

S(k)T S(k)α s.t. α ≥ 0, 1T α = 1

i:y(−k)
m=1 α∗

cm :=

αT

i

i

i

min
α∈RM

k=1

This optimizes an upper bound of the CV RMSE(without rounding), and has the same property as Algorithm 2 :

The CV RMSE of the simple SSEPC ensemble is smaller than those of L1,··· , LM ,

To see these, let’s consider the kth CV fold, and omit superscripts k and −k from y’s for simplicity. We can then

obtain an upper bound of SSE (from the kth CV fold) of an ensemble method(cid:80)M

m=1 αmLm as follows :

SSEk :=

i

(cid:88)
C(cid:88)
≤ C(cid:88)

c=1

(
m=1

M(cid:88)
M(cid:88)
M(cid:88)

[
m=1

=

[

c=1

m=1

(cid:88)

M(cid:88)

[

i

m=1

αm ˆy(m)

i − yi)2 =
(cid:88)

α2
m

i:yi=c

i − yi)2 + 2
(ˆy(m)

(cid:88)

ms(k)
α2
cm

2

+ 2

1≤l<m≤M

m(ˆy(m)
α2

(cid:88)

i − yi)2 + 2
(cid:88)
C(cid:88)
M(cid:88)

αlαm

i:yi=c

1≤l<m≤M

αlαms(k)

cl s(k)

cm] =

[

1≤l<m≤M
i − yi)(ˆy(m)
(ˆy(l)

i − yi)]

αms(k)

cm]2 = (cid:107)S(k)α(cid:107)2

2

(cid:88)

αlαm(ˆy(l)

i − yi)(ˆy(m)

i − yi)]

c=1

m=1

4

CS 229 Final report

Kwhangho Kim, Jeha Yang

S(k). Summing up these upper bounds for k = 1,··· , K gives SSE := (cid:80)K
k=1 SSEk ≤ (cid:80)K
αT(cid:80)K
(cid:80)C
2 = (cid:80)K
when αm = 1, αl = 0 ∀l (cid:54)= m, i.e., α∗T(cid:80)K

where the inequality is due to the Cauchy-Schwarz inequality, which makes the upper bound a function of
2 =
is the SSE of Lm
k=1 S(k)T S(k)α∗ ≤ (SSE’s of L1,··· , LM ) ··· (2). Combining (1) and

k=1 S(k)T S(k)α ··· (1). Furthurmore, note that (cid:80)K

k=1 (cid:107)S(k)α(cid:107)2

k=1 (cid:107)S(k)α(cid:107)2

2

c=1 s(k)

cm

k=1

(2) results in the claim above. Applying Algorithm 3 to our data resulted in the following sparse ensemble

Prediction = 0.050 LPR + 0.950 SVM

Finally, we revisited the ﬁrst heuristic idea and came up with the following algorithm.
Algorithm 4 : Forward Stepwise SSEPC Ensemble (FWD SSEPC)
Let R be the C × M matrix of CV SSE per class (hence its rows are classes).
(1) Deﬁne the best single model as the current model, and update R by removing the corresponding column.
(2) For each column of R, divide each element by the corresponding CV SSE per class of the current model, to
obtain the matrix of those ratios.
(3) If all of those ratios are larger than 1, stop ; otherwise, ﬁnd the column with the smallest sum of ratios,
include the corresponding single model to the current set of models, and update the model by using Algorithm 2
of the new set of models and R by removing the corresponding column.
(4) Repeat 2 and 3 until convergence (in terms of the supremum of the change in weight over all single models)
Let’s discuss the ideas of step 2 and 3. First of all, the smaller the ratio of the SSE of a class between a single
model and the current model is, the better the single model works in the corresponding class. Therefore, summing
up those ratios over all classes could be considered as a measure of how much a single model complements the
current ensemble model, treating all classes equally. Furthermore, if those ratios are greater than 1 for all classes,
than we don’t need to include the corresponding single model to the ensemble.
Implementing this algorithm gave the ensemble

Prediction = 0.520 SVM + 0.373 LDA + 0.107 L1R

5 Results and Discussion

Below are the experiment results of 4 algorithms we discussed so far.

Table 3: RMSEs for Ensemble Methods

Heuristic L2CV SSEPC FWD SSEPC

CV Set
Test Set

0.679
0.683

0.669
0.687

0.709
0.718

0.673
0.687

By comparing Table 3 with Table 1, it is found that

1.
2.

3.

All four ensemble methods perform better than any of single classiﬁers in terms of RMSE.
Ensemble methods show their strengths particularly when they combine the complementary predictive
powers of multiple models.
In particular, we can observe that the background ideas of FWD SSEPC worked in the sense that
LDA was picked following SVM as done in the heuristic ensemble, although the next step actually
increased the RMSE from 0.688 to 0.695.

6 Future Work

•

•

From the results, L2CV and FWD SSEPC showed similar performances. Therefore, we can further
compare them via experiments or theoretical analysis. Also, it can be examined if simple SSEPC
always guarantees sparse ensemble models, as shown in our experiment.
To improve FWD SSEPC, we can try to develop a measure for the third step as a function of the
ratios of SSEs, other than the one we used before.

5

CS 229 Final report

7 Reference

Kwhangho Kim, Jeha Yang

1. Ali K. M., Pazzani M. J., Error Reduction through Learning Multiple Descriptions, Machine Learning, 24: 3,
173-202, 1996.
2. Bartlett P. and Shawe-Taylor J., Generalization Performance of Support Vector Machines.
and Other Pattern Classiﬁers, In Advances in Kernel Methods, Support Vector Learning, MIT Press, Cambridge,
USA, 1998.
3. Opitz, D. and Maclin, R., Popular Ensemble Methods: An Empirical Study, Journal of Artiﬁcial Research, 11:
169-198, 1999.
4. Mitchell, T., Machine Learning, McGraw-Hill, 1997.
5. Dietterich T., Ensemble methods in machine learning. In J. Kittler and F. Roll, editors, First International-
Workshop on Multiple Classiﬁer Systems, Lecture Notes in Computer Science, pages 1-15. Springer-Verlag, 2000
6. Dietterich, T.G., Machine learning research: Four current directions. AI Magazine 18(4) (1997) 97136.
7. Rokach, L. and Maimon, O., Clustering methods, Data Mining and Knowledge Discovery Handbook, pp.
321352, 2005, Springer.
8. Rokach, L., Decomposition methodology for classiﬁcation tasks: a meta decomposer framework, Pattern Anal-
ysis and Applications, 9(2006):257271.
9. Rokach, L., Ensemble Methods in Supervised Learning. Springer (2010).
10. Tumer. K. and Ghosh. J., Error correlation and error reduction in ensemble classiﬁers. Connection Science
(1997), 8(3-4), 385-404.
11. Bennett, K. P., Demiriz, A., Maclin, R., Exploiting unlabeled data in ensemble methods, KDD ’02 Proceed-
ings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, 289-296.
12. Rokach L., Genetic algorithm-based feature set partitioning for classiﬁcation problems, Pattern Recognition,
41(5):16761700, 2008.
13. Geman S., Bienenstock, E., and Doursat, R., Neural networks and the bias variance dilemma. Neural Com-
putation, 4:1-58, 1995.
14. Tan A. C., Gilbert D., Deville Y., Multi-class Protein Fold Classiﬁcation using a New Ensemble Machine
Learning Approach. Genome Informatics, 14:206217, 2003.
15. Quinlan, J. R., C4.5: Programs for Machine Learning, Morgan Kaufmann, Los Altos, 1993.
16. Quinlan, J. R., Bagging, Boosting, and C4.5.
Artiﬁcial Intelligence, pages 725-730, 1996.
17. Sohn S. Y., Choi, H., Ensemble based on Data Envelopment Analysis, ECML Meta Learning workshop, Sep.
4, 2001.

In Proceedings of the Thirteenth National Conference on

6

