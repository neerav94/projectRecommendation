Topic Analysis of the FCC’s Public Comments on Net Neutrality

Sachin Padmanabhan
spadman@stanford.edu

Leon Yao

Luda Zhao

Timothy Lee

leonyao@stanford.edu

ludazhao@stanford.edu

timothyl@stanford.edu

Department of Computer Science

Stanford University

Abstract

The FCC’s proposed net neutrality policy change in 2014 was
met with widespread public controversy and outrage. The
FCC recently released to the public millions of comments
that it received about the issue. It is abundantly clear that
the vast majority of citizens prefer to have net neutrality
intact, but what exactly are the people saying? What are
their main arguments and reasons for wanting to maintain
net neutrality? In this project, we use natural language pro-
cessing techniques to analyze the arguments in 800,000 of
the comments.

1. Introduction

1.1. Motivation

The Open Internet proceeding of the FCC (Federal Commu-
nication Commissions) is a critical regulatory eﬀort to de-
termine the future of the Internet. The proceeding concerns
“Net Neutrality”, the principle that all Internet traﬃc should
be treated equally, and no internet provider will be given
control over Internet traﬃc. Should the FCC decide not to
maintain net neutrality, ISPs will have more power to regu-
late Internet traﬃc and scrutinize data sent over the Internet.
In addition, ISPs will be able to discriminate between Inter-
net traﬃc to provide a “fast lane” for high-paying consumers.
Proponents of net neutrality argue that this will severely re-
strict free speech and privacy on the Internet. In addition,
they assert that giving ISPs diﬀerential control over Internet
traﬃc will ultimately result in extremely slow Internet for
average consumers, including individuals and corporations,
that cannot aﬀord to pay as much as large corporations. In
turn, this will hamper fair competition between businesses,
shifting the balance largely in the side of large companies
to the immense detriment of innovative startups.
Instead,
proponents maintain that the Internet should instead be re-
classiﬁed as a “common carrier”.

Project done in Stanford University’s CS 229 (Machine Learning)
course taught in Autumn 2014 by Professor Andrew Ng.

The debate on net neutrality has attracted a large response
from the American public. As of writing, the FCC proceed-
ings has attracted over 2 million comments. These comments
are important to the FCC’s decision making process, and are
usually read by people. However, given the unprecedented
number of comments, this is not possible. Under this con-
text, natural language processing is an eﬀective technique
that can help gain insight into the comments as a whole. Can
we automatically determine which issues were most pertinent
to proponents of net neutrality?

1.2. Our Work

After reading many of the comments, we saw that the argu-
ments were almost unanimously in favor of maintaining net
neutrality. However, the arguments presented varied greatly
in length, relevance, level of insight, and topic. We wanted to
determine what people’s arguments are and why they are in
favor of net neutrality. The majority of the comments made
at least one of the following arguments:

• Net neutrality is needed to protect freedom of ideas,
creativity, speech, and communication on the Internet
(ideafreedom)

• Net neutrality is needed to protect fair market competi-
tion for small businesses and startups (fairbusiness)
• Net neutrality is needed to protect the Internet from
further legislation and government intervention in the
future (fairgov)

Our goal was to classify the argument of each comment into
one or more of the above topics using supervised learning.

We decided against using an unsupervised learning approach
since it was attempted earlier by a team at Sunlight Labs.
The results they got were not very inspiring because the clus-
ters they found were only indicated by a few key words, but
were too vague to have any concrete topic behind it. Among
some very bad clusters, we did see some interesting ones like,
“small market,” “bidding,” “premium,” and “disadvantage.”
This exactly ﬁts our idea of free business. Instead of having
1 in nearly 100 clusters be interesting, we thought it would

Topic Analysis of the FCC’s Public Comments on Net Neutrality

Sachin Padmanabhan
spadman@stanford.edu

Leon Yao

Luda Zhao

Timothy Lee

leonyao@stanford.edu

ludazhao@stanford.edu

timothyl@stanford.edu

Department of Computer Science

Stanford University

Abstract

The FCC’s proposed net neutrality policy change in 2014 was
met with widespread public controversy and outrage. The
FCC recently released to the public millions of comments
that it received about the issue. It is abundantly clear that
the vast majority of citizens prefer to have net neutrality
intact, but what exactly are the people saying? What are
their main arguments and reasons for wanting to maintain
net neutrality? In this project, we use natural language pro-
cessing techniques to analyze the arguments in 800,000 of
the comments.

1. Introduction

1.1. Motivation

The Open Internet proceeding of the FCC (Federal Commu-
nication Commissions) is a critical regulatory eﬀort to de-
termine the future of the Internet. The proceeding concerns
“Net Neutrality”, the principle that all Internet traﬃc should
be treated equally, and no internet provider will be given
control over Internet traﬃc. Should the FCC decide not to
maintain net neutrality, ISPs will have more power to regu-
late Internet traﬃc and scrutinize data sent over the Internet.
In addition, ISPs will be able to discriminate between Inter-
net traﬃc to provide a “fast lane” for high-paying consumers.
Proponents of net neutrality argue that this will severely re-
strict free speech and privacy on the Internet. In addition,
they assert that giving ISPs diﬀerential control over Internet
traﬃc will ultimately result in extremely slow Internet for
average consumers, including individuals and corporations,
that cannot aﬀord to pay as much as large corporations. In
turn, this will hamper fair competition between businesses,
shifting the balance largely in the side of large companies
to the immense detriment of innovative startups.
Instead,
proponents maintain that the Internet should instead be re-
classiﬁed as a “common carrier”.

Project done in Stanford University’s CS 229 (Machine Learning)
course taught in Autumn 2014 by Professor Andrew Ng.

The debate on net neutrality has attracted a large response
from the American public. As of writing, the FCC proceed-
ings has attracted over 2 million comments. These comments
are important to the FCC’s decision making process, and are
usually read by people. However, given the unprecedented
number of comments, this is not possible. Under this con-
text, natural language processing is an eﬀective technique
that can help gain insight into the comments as a whole. Can
we automatically determine which issues were most pertinent
to proponents of net neutrality?

1.2. Our Work

After reading many of the comments, we saw that the argu-
ments were almost unanimously in favor of maintaining net
neutrality. However, the arguments presented varied greatly
in length, relevance, level of insight, and topic. We wanted to
determine what people’s arguments are and why they are in
favor of net neutrality. The majority of the comments made
at least one of the following arguments:

• Net neutrality is needed to protect freedom of ideas,
creativity, speech, and communication on the Internet
(ideafreedom)

• Net neutrality is needed to protect fair market competi-
tion for small businesses and startups (fairbusiness)
• Net neutrality is needed to protect the Internet from
further legislation and government intervention in the
future (fairgov)

Our goal was to classify the argument of each comment into
one or more of the above topics using supervised learning.

We decided against using an unsupervised learning approach
since it was attempted earlier by a team at Sunlight Labs.
The results they got were not very inspiring because the clus-
ters they found were only indicated by a few key words, but
were too vague to have any concrete topic behind it. Among
some very bad clusters, we did see some interesting ones like,
“small market,” “bidding,” “premium,” and “disadvantage.”
This exactly ﬁts our idea of free business. Instead of having
1 in nearly 100 clusters be interesting, we thought it would

Topic Analysis of the FCC’s Public Comments on Net Neutrality

be interesting to remove the noise and just focus on 3 topics
we knew were in the dataset.

2. Data

The original dataset released by the FCC consisted of 1.1
million raw comments along with metadata, many of which
were blank, unparseable, or too long (Les Mis´erables and
War and Peace were both submitted as comments). Fortu-
nately, the team at Sunlight Labs processed the dataset to
remove these unworkable comments and provided a cleaner
dataset of 800,959 comments with metadata in JSON format.

{

}

"applicant": "Kara J. Walton",
"dateRcpt": "2014-06-16T04:00:00Z",
"stateCd": "VA",
"zip": "20121"
"text": "Allowing the cable companies to

start charging companies for..."

To train and test our classiﬁer, we drew a random sample of
800 comments from the dataset. We manually read through
each comment to discern the arguments presented and cor-
respondingly labeled each comment.

{

}

...
"topiclabels": {

"ideafreedom": 1,
"fairbusiness": 1,
"freegov": 0

},
"formletter": 0,
"personal": 0

The rest of the comments were used after we built the clas-
siﬁer to glean interesting insight on the entire dataset.

3. Methodology

3.1. Form Letter Detection

By reading through the dataset, we noticed that a majority
(about 60%) of it was composed of “form letters,” which were
mostly identical comments written by third party organiza-
tion who had its supporters send in the same professionally
written messages.

To Chairman Tom Wheeler and the FCC Commis-
sioners To the FCC Please build any net neutrality
argument upon solid legal standing. Speciﬁcally,
this means reclassifying broadband under Title II
of the Telecommunications Act of 1934. 706 au-

thority from the Telecommunications Act has been
repeatedly struck down in court after legal chal-
lenges by telecom companies. Take the appropriate
steps to prevent this from happening again. Sin-
cerely, XXXX

Clearly, a form letter could often times sound exactly like
a regular comment. We decided to use form letters in our
topic classiﬁcation because, despite their spam-like nature,
they still signify the intentions of the individual sender who
agrees with this mass message, otherwise they wouldn’t have
taken the time to actually send it. We found that most, if
not nearly all, of these messages were from diﬀerent people.
Thus, one of the main problems we had to tune for was
overﬁtting the training set.

Instead, our goal was to use unsupervised learning to detect
exactly which comments were form letters so that we could
perform analysis on just the form letters themselves. We did
this using the Simhash algorithm, which is a generally fast
method to calculate the similarity between two documents,
and is eﬀective for near-duplicate detection.

Using the Simhash algorithm, we we found the near-
duplicates to the document being classiﬁed. If there existed
a signiﬁcant number of comments that were near duplicates,
then the comment was classiﬁed as a form letter. We used
a 64-bit hash size, shingle width of 4 letters, and hamming
distance threshold of 10 bits as parameters for the model.
Given a labelled data set of 800 comments, the model classi-
ﬁed form letters with 88% accuracy. On a data set containing
40,000 comments, the model showed that 63% of the com-
ments were form letters, similar to our initial observation’s
60% proportion.

3.2. Feature Selection

We ﬁrst preprocessed the data by removing stop words such
as “the,” “a,” “and,” etc. that appear in nearly all comments
but are essentially useless features. We also stemmed our
words, so that diﬀerent conjugations of the same word would
be counted as the same. We also tried using diﬀerent size
n-grams to increase our feature space and to capture more
of the word contexts.

We used several standard features for typical NLP datasets.
We ﬁrst found the word counts of our comments, then nor-
malized them and used TF–IDF (Term Frequency–Inverse
Document Frequency) features, which is a weighting factor
for each word that gives a value proportional to the frequency
of that word in the comment oﬀset by the frequency of the
word in the entire corpus. This allowed us to remove words
that appear in every comment, but are bad features to use
for training a classiﬁer. For example, words like “Internet”
and “FCC” were used in nearly every comment, but are not
helpful for determining if a comment is from a given class.
TF–IDF allows us to hone in on the most important features,

Topic Analysis of the FCC’s Public Comments on Net Neutrality

Sachin Padmanabhan
spadman@stanford.edu

Leon Yao

Luda Zhao

Timothy Lee

leonyao@stanford.edu

ludazhao@stanford.edu

timothyl@stanford.edu

Department of Computer Science

Stanford University

Abstract

The FCC’s proposed net neutrality policy change in 2014 was
met with widespread public controversy and outrage. The
FCC recently released to the public millions of comments
that it received about the issue. It is abundantly clear that
the vast majority of citizens prefer to have net neutrality
intact, but what exactly are the people saying? What are
their main arguments and reasons for wanting to maintain
net neutrality? In this project, we use natural language pro-
cessing techniques to analyze the arguments in 800,000 of
the comments.

1. Introduction

1.1. Motivation

The Open Internet proceeding of the FCC (Federal Commu-
nication Commissions) is a critical regulatory eﬀort to de-
termine the future of the Internet. The proceeding concerns
“Net Neutrality”, the principle that all Internet traﬃc should
be treated equally, and no internet provider will be given
control over Internet traﬃc. Should the FCC decide not to
maintain net neutrality, ISPs will have more power to regu-
late Internet traﬃc and scrutinize data sent over the Internet.
In addition, ISPs will be able to discriminate between Inter-
net traﬃc to provide a “fast lane” for high-paying consumers.
Proponents of net neutrality argue that this will severely re-
strict free speech and privacy on the Internet. In addition,
they assert that giving ISPs diﬀerential control over Internet
traﬃc will ultimately result in extremely slow Internet for
average consumers, including individuals and corporations,
that cannot aﬀord to pay as much as large corporations. In
turn, this will hamper fair competition between businesses,
shifting the balance largely in the side of large companies
to the immense detriment of innovative startups.
Instead,
proponents maintain that the Internet should instead be re-
classiﬁed as a “common carrier”.

Project done in Stanford University’s CS 229 (Machine Learning)
course taught in Autumn 2014 by Professor Andrew Ng.

The debate on net neutrality has attracted a large response
from the American public. As of writing, the FCC proceed-
ings has attracted over 2 million comments. These comments
are important to the FCC’s decision making process, and are
usually read by people. However, given the unprecedented
number of comments, this is not possible. Under this con-
text, natural language processing is an eﬀective technique
that can help gain insight into the comments as a whole. Can
we automatically determine which issues were most pertinent
to proponents of net neutrality?

1.2. Our Work

After reading many of the comments, we saw that the argu-
ments were almost unanimously in favor of maintaining net
neutrality. However, the arguments presented varied greatly
in length, relevance, level of insight, and topic. We wanted to
determine what people’s arguments are and why they are in
favor of net neutrality. The majority of the comments made
at least one of the following arguments:

• Net neutrality is needed to protect freedom of ideas,
creativity, speech, and communication on the Internet
(ideafreedom)

• Net neutrality is needed to protect fair market competi-
tion for small businesses and startups (fairbusiness)
• Net neutrality is needed to protect the Internet from
further legislation and government intervention in the
future (fairgov)

Our goal was to classify the argument of each comment into
one or more of the above topics using supervised learning.

We decided against using an unsupervised learning approach
since it was attempted earlier by a team at Sunlight Labs.
The results they got were not very inspiring because the clus-
ters they found were only indicated by a few key words, but
were too vague to have any concrete topic behind it. Among
some very bad clusters, we did see some interesting ones like,
“small market,” “bidding,” “premium,” and “disadvantage.”
This exactly ﬁts our idea of free business. Instead of having
1 in nearly 100 clusters be interesting, we thought it would

Topic Analysis of the FCC’s Public Comments on Net Neutrality

be interesting to remove the noise and just focus on 3 topics
we knew were in the dataset.

2. Data

The original dataset released by the FCC consisted of 1.1
million raw comments along with metadata, many of which
were blank, unparseable, or too long (Les Mis´erables and
War and Peace were both submitted as comments). Fortu-
nately, the team at Sunlight Labs processed the dataset to
remove these unworkable comments and provided a cleaner
dataset of 800,959 comments with metadata in JSON format.

{

}

"applicant": "Kara J. Walton",
"dateRcpt": "2014-06-16T04:00:00Z",
"stateCd": "VA",
"zip": "20121"
"text": "Allowing the cable companies to

start charging companies for..."

To train and test our classiﬁer, we drew a random sample of
800 comments from the dataset. We manually read through
each comment to discern the arguments presented and cor-
respondingly labeled each comment.

{

}

...
"topiclabels": {

"ideafreedom": 1,
"fairbusiness": 1,
"freegov": 0

},
"formletter": 0,
"personal": 0

The rest of the comments were used after we built the clas-
siﬁer to glean interesting insight on the entire dataset.

3. Methodology

3.1. Form Letter Detection

By reading through the dataset, we noticed that a majority
(about 60%) of it was composed of “form letters,” which were
mostly identical comments written by third party organiza-
tion who had its supporters send in the same professionally
written messages.

To Chairman Tom Wheeler and the FCC Commis-
sioners To the FCC Please build any net neutrality
argument upon solid legal standing. Speciﬁcally,
this means reclassifying broadband under Title II
of the Telecommunications Act of 1934. 706 au-

thority from the Telecommunications Act has been
repeatedly struck down in court after legal chal-
lenges by telecom companies. Take the appropriate
steps to prevent this from happening again. Sin-
cerely, XXXX

Clearly, a form letter could often times sound exactly like
a regular comment. We decided to use form letters in our
topic classiﬁcation because, despite their spam-like nature,
they still signify the intentions of the individual sender who
agrees with this mass message, otherwise they wouldn’t have
taken the time to actually send it. We found that most, if
not nearly all, of these messages were from diﬀerent people.
Thus, one of the main problems we had to tune for was
overﬁtting the training set.

Instead, our goal was to use unsupervised learning to detect
exactly which comments were form letters so that we could
perform analysis on just the form letters themselves. We did
this using the Simhash algorithm, which is a generally fast
method to calculate the similarity between two documents,
and is eﬀective for near-duplicate detection.

Using the Simhash algorithm, we we found the near-
duplicates to the document being classiﬁed. If there existed
a signiﬁcant number of comments that were near duplicates,
then the comment was classiﬁed as a form letter. We used
a 64-bit hash size, shingle width of 4 letters, and hamming
distance threshold of 10 bits as parameters for the model.
Given a labelled data set of 800 comments, the model classi-
ﬁed form letters with 88% accuracy. On a data set containing
40,000 comments, the model showed that 63% of the com-
ments were form letters, similar to our initial observation’s
60% proportion.

3.2. Feature Selection

We ﬁrst preprocessed the data by removing stop words such
as “the,” “a,” “and,” etc. that appear in nearly all comments
but are essentially useless features. We also stemmed our
words, so that diﬀerent conjugations of the same word would
be counted as the same. We also tried using diﬀerent size
n-grams to increase our feature space and to capture more
of the word contexts.

We used several standard features for typical NLP datasets.
We ﬁrst found the word counts of our comments, then nor-
malized them and used TF–IDF (Term Frequency–Inverse
Document Frequency) features, which is a weighting factor
for each word that gives a value proportional to the frequency
of that word in the comment oﬀset by the frequency of the
word in the entire corpus. This allowed us to remove words
that appear in every comment, but are bad features to use
for training a classiﬁer. For example, words like “Internet”
and “FCC” were used in nearly every comment, but are not
helpful for determining if a comment is from a given class.
TF–IDF allows us to hone in on the most important features,

Topic Analysis of the FCC’s Public Comments on Net Neutrality

which is one of the best methods for feature selection.

tf(t, d) = 0.5 +

0.5f (t, d)

max{f (w, d) : w ∈ d}

N

|{d ∈ D : t ∈ D}|
idf(t, D) = log
tﬁdf(t, d, D) = tf(t, d) · idf(t, D)

On top of TF–IDF we also used a min/max frequency prun-
ing. If a word only appears once or twice in the dataset or
in every single comment, TF–IDF will assign it a low score,
but we wanted to actually reduce our feature space so as to
not overﬁt. If a word has word frequency less than our min
or greater than our max, then we removed it.

3.3. Model Selection

For each of our classiﬁers we learned a one vs. all classiﬁer
because a particular comment could have multiple diﬀerent
topics. We used 10-fold cross validation for each model, so
each training/testing accuracy we report are the generaliza-
tion accuracies.

3.3.1. Bernoulli Na¨ıve Bayes Classifier

The ﬁrst classiﬁer we tried was just a simple Na¨ıve Bayes
with Laplace smoothing for data distributed according to the
Bernoulli distribution. By ﬁnding the maximum likelihood
estimates

φj|y=1 =

φj|y=0 =

(cid:80)m
(cid:80)m
j = 1 ∧ y(i) = 1}
i=1 1{x(i)
(cid:80)m
i=1 1{y(i) = 1}
(cid:80)m
i=1 1{x(i)
j = 1 ∧ y(i) = 0}
(cid:80)m
i=1 1{y(i) = 0}
i=1 1{y(i) = 1}

φy =

m

m(cid:88)

used logistic regression with (cid:96)2 regularization, which corre-
sponds to a Gaussian prior on the data. Thus, we imple-
mented a stochastic gradient descent classiﬁer to minimize
the cost function

θ = arg max

J(θ) =

θ

i=1

log p(y(i)|x(i); θ) − λ
2

(cid:107)θ(cid:107)2

2

The results are summarized in Table 2.

Table 2. Classiﬁcation accuracies for ell2-regularized logistic re-
gression

Topic

Training Testing

ideafreedom
fairbusiness
freegov

99.24%
99.59%
99.24%

90.89%
87.72%
96.46%

3.3.3. Support Vector Machine

We ﬁnally tried an (cid:96)1-norm soft margin SVM classiﬁer with
a Gaussian kernel.

y(i)y(j)αiαjK(x(i), x(j))

m(cid:88)

m(cid:88)

i=1

j=1

αi − 1
2

0 ≤ αi ≤ C, i = 1, . . . , m

min

α

s.t.

m(cid:88)
m(cid:88)

i=1

i=1

αiy(i) = 0

K(x, z) = exp

(cid:18)(cid:107)x − z(cid:107)2

(cid:19)

2τ

and then determining the class with the highest posterior
probability, we obtained the results in Table 1. We saw that

Table 1. Classiﬁcation accuracies for Na¨ıve Bayes classiﬁer

Topic

Training Testing

ideafreedom
fairbusiness
freegov

87.14%
86.60%
96.19%

85.70%
80.76%
96.46%

the Na¨ıve Bayes Classiﬁer suﬀered a lot from the form letters
and also overﬁtted the training set.

3.3.2. Regularized (Bayesian) Logistic Regression

Since overﬁtting was a problem for Na¨ıve Bayes, we decided
to use regularization to restrict the norm of the learned pa-
rameters to control the VC dimension of our classiﬁer. We

Although computationally more intensive, we felt it would
yield better results. Indeed, chosen with default parameters,
this gave better results than the previous methods. In or-
der to further improve the results, we ran a model selection
algorithm to search for the best parameters for the model,
and the resulting classiﬁer yielded even better results. The
results for the optimized classiﬁer are summarized in Table
3.

Table 3. Classiﬁcation accuracies for support vector machine with
Gaussian kernel

Topic

Training Testing

ideafreedom
fairbusiness
freegov

95.22%
97.37%
97.45%

90.92%
89.03%
97.48%

Topic Analysis of the FCC’s Public Comments on Net Neutrality

Sachin Padmanabhan
spadman@stanford.edu

Leon Yao

Luda Zhao

Timothy Lee

leonyao@stanford.edu

ludazhao@stanford.edu

timothyl@stanford.edu

Department of Computer Science

Stanford University

Abstract

The FCC’s proposed net neutrality policy change in 2014 was
met with widespread public controversy and outrage. The
FCC recently released to the public millions of comments
that it received about the issue. It is abundantly clear that
the vast majority of citizens prefer to have net neutrality
intact, but what exactly are the people saying? What are
their main arguments and reasons for wanting to maintain
net neutrality? In this project, we use natural language pro-
cessing techniques to analyze the arguments in 800,000 of
the comments.

1. Introduction

1.1. Motivation

The Open Internet proceeding of the FCC (Federal Commu-
nication Commissions) is a critical regulatory eﬀort to de-
termine the future of the Internet. The proceeding concerns
“Net Neutrality”, the principle that all Internet traﬃc should
be treated equally, and no internet provider will be given
control over Internet traﬃc. Should the FCC decide not to
maintain net neutrality, ISPs will have more power to regu-
late Internet traﬃc and scrutinize data sent over the Internet.
In addition, ISPs will be able to discriminate between Inter-
net traﬃc to provide a “fast lane” for high-paying consumers.
Proponents of net neutrality argue that this will severely re-
strict free speech and privacy on the Internet. In addition,
they assert that giving ISPs diﬀerential control over Internet
traﬃc will ultimately result in extremely slow Internet for
average consumers, including individuals and corporations,
that cannot aﬀord to pay as much as large corporations. In
turn, this will hamper fair competition between businesses,
shifting the balance largely in the side of large companies
to the immense detriment of innovative startups.
Instead,
proponents maintain that the Internet should instead be re-
classiﬁed as a “common carrier”.

Project done in Stanford University’s CS 229 (Machine Learning)
course taught in Autumn 2014 by Professor Andrew Ng.

The debate on net neutrality has attracted a large response
from the American public. As of writing, the FCC proceed-
ings has attracted over 2 million comments. These comments
are important to the FCC’s decision making process, and are
usually read by people. However, given the unprecedented
number of comments, this is not possible. Under this con-
text, natural language processing is an eﬀective technique
that can help gain insight into the comments as a whole. Can
we automatically determine which issues were most pertinent
to proponents of net neutrality?

1.2. Our Work

After reading many of the comments, we saw that the argu-
ments were almost unanimously in favor of maintaining net
neutrality. However, the arguments presented varied greatly
in length, relevance, level of insight, and topic. We wanted to
determine what people’s arguments are and why they are in
favor of net neutrality. The majority of the comments made
at least one of the following arguments:

• Net neutrality is needed to protect freedom of ideas,
creativity, speech, and communication on the Internet
(ideafreedom)

• Net neutrality is needed to protect fair market competi-
tion for small businesses and startups (fairbusiness)
• Net neutrality is needed to protect the Internet from
further legislation and government intervention in the
future (fairgov)

Our goal was to classify the argument of each comment into
one or more of the above topics using supervised learning.

We decided against using an unsupervised learning approach
since it was attempted earlier by a team at Sunlight Labs.
The results they got were not very inspiring because the clus-
ters they found were only indicated by a few key words, but
were too vague to have any concrete topic behind it. Among
some very bad clusters, we did see some interesting ones like,
“small market,” “bidding,” “premium,” and “disadvantage.”
This exactly ﬁts our idea of free business. Instead of having
1 in nearly 100 clusters be interesting, we thought it would

Topic Analysis of the FCC’s Public Comments on Net Neutrality

be interesting to remove the noise and just focus on 3 topics
we knew were in the dataset.

2. Data

The original dataset released by the FCC consisted of 1.1
million raw comments along with metadata, many of which
were blank, unparseable, or too long (Les Mis´erables and
War and Peace were both submitted as comments). Fortu-
nately, the team at Sunlight Labs processed the dataset to
remove these unworkable comments and provided a cleaner
dataset of 800,959 comments with metadata in JSON format.

{

}

"applicant": "Kara J. Walton",
"dateRcpt": "2014-06-16T04:00:00Z",
"stateCd": "VA",
"zip": "20121"
"text": "Allowing the cable companies to

start charging companies for..."

To train and test our classiﬁer, we drew a random sample of
800 comments from the dataset. We manually read through
each comment to discern the arguments presented and cor-
respondingly labeled each comment.

{

}

...
"topiclabels": {

"ideafreedom": 1,
"fairbusiness": 1,
"freegov": 0

},
"formletter": 0,
"personal": 0

The rest of the comments were used after we built the clas-
siﬁer to glean interesting insight on the entire dataset.

3. Methodology

3.1. Form Letter Detection

By reading through the dataset, we noticed that a majority
(about 60%) of it was composed of “form letters,” which were
mostly identical comments written by third party organiza-
tion who had its supporters send in the same professionally
written messages.

To Chairman Tom Wheeler and the FCC Commis-
sioners To the FCC Please build any net neutrality
argument upon solid legal standing. Speciﬁcally,
this means reclassifying broadband under Title II
of the Telecommunications Act of 1934. 706 au-

thority from the Telecommunications Act has been
repeatedly struck down in court after legal chal-
lenges by telecom companies. Take the appropriate
steps to prevent this from happening again. Sin-
cerely, XXXX

Clearly, a form letter could often times sound exactly like
a regular comment. We decided to use form letters in our
topic classiﬁcation because, despite their spam-like nature,
they still signify the intentions of the individual sender who
agrees with this mass message, otherwise they wouldn’t have
taken the time to actually send it. We found that most, if
not nearly all, of these messages were from diﬀerent people.
Thus, one of the main problems we had to tune for was
overﬁtting the training set.

Instead, our goal was to use unsupervised learning to detect
exactly which comments were form letters so that we could
perform analysis on just the form letters themselves. We did
this using the Simhash algorithm, which is a generally fast
method to calculate the similarity between two documents,
and is eﬀective for near-duplicate detection.

Using the Simhash algorithm, we we found the near-
duplicates to the document being classiﬁed. If there existed
a signiﬁcant number of comments that were near duplicates,
then the comment was classiﬁed as a form letter. We used
a 64-bit hash size, shingle width of 4 letters, and hamming
distance threshold of 10 bits as parameters for the model.
Given a labelled data set of 800 comments, the model classi-
ﬁed form letters with 88% accuracy. On a data set containing
40,000 comments, the model showed that 63% of the com-
ments were form letters, similar to our initial observation’s
60% proportion.

3.2. Feature Selection

We ﬁrst preprocessed the data by removing stop words such
as “the,” “a,” “and,” etc. that appear in nearly all comments
but are essentially useless features. We also stemmed our
words, so that diﬀerent conjugations of the same word would
be counted as the same. We also tried using diﬀerent size
n-grams to increase our feature space and to capture more
of the word contexts.

We used several standard features for typical NLP datasets.
We ﬁrst found the word counts of our comments, then nor-
malized them and used TF–IDF (Term Frequency–Inverse
Document Frequency) features, which is a weighting factor
for each word that gives a value proportional to the frequency
of that word in the comment oﬀset by the frequency of the
word in the entire corpus. This allowed us to remove words
that appear in every comment, but are bad features to use
for training a classiﬁer. For example, words like “Internet”
and “FCC” were used in nearly every comment, but are not
helpful for determining if a comment is from a given class.
TF–IDF allows us to hone in on the most important features,

Topic Analysis of the FCC’s Public Comments on Net Neutrality

which is one of the best methods for feature selection.

tf(t, d) = 0.5 +

0.5f (t, d)

max{f (w, d) : w ∈ d}

N

|{d ∈ D : t ∈ D}|
idf(t, D) = log
tﬁdf(t, d, D) = tf(t, d) · idf(t, D)

On top of TF–IDF we also used a min/max frequency prun-
ing. If a word only appears once or twice in the dataset or
in every single comment, TF–IDF will assign it a low score,
but we wanted to actually reduce our feature space so as to
not overﬁt. If a word has word frequency less than our min
or greater than our max, then we removed it.

3.3. Model Selection

For each of our classiﬁers we learned a one vs. all classiﬁer
because a particular comment could have multiple diﬀerent
topics. We used 10-fold cross validation for each model, so
each training/testing accuracy we report are the generaliza-
tion accuracies.

3.3.1. Bernoulli Na¨ıve Bayes Classifier

The ﬁrst classiﬁer we tried was just a simple Na¨ıve Bayes
with Laplace smoothing for data distributed according to the
Bernoulli distribution. By ﬁnding the maximum likelihood
estimates

φj|y=1 =

φj|y=0 =

(cid:80)m
(cid:80)m
j = 1 ∧ y(i) = 1}
i=1 1{x(i)
(cid:80)m
i=1 1{y(i) = 1}
(cid:80)m
i=1 1{x(i)
j = 1 ∧ y(i) = 0}
(cid:80)m
i=1 1{y(i) = 0}
i=1 1{y(i) = 1}

φy =

m

m(cid:88)

used logistic regression with (cid:96)2 regularization, which corre-
sponds to a Gaussian prior on the data. Thus, we imple-
mented a stochastic gradient descent classiﬁer to minimize
the cost function

θ = arg max

J(θ) =

θ

i=1

log p(y(i)|x(i); θ) − λ
2

(cid:107)θ(cid:107)2

2

The results are summarized in Table 2.

Table 2. Classiﬁcation accuracies for ell2-regularized logistic re-
gression

Topic

Training Testing

ideafreedom
fairbusiness
freegov

99.24%
99.59%
99.24%

90.89%
87.72%
96.46%

3.3.3. Support Vector Machine

We ﬁnally tried an (cid:96)1-norm soft margin SVM classiﬁer with
a Gaussian kernel.

y(i)y(j)αiαjK(x(i), x(j))

m(cid:88)

m(cid:88)

i=1

j=1

αi − 1
2

0 ≤ αi ≤ C, i = 1, . . . , m

min

α

s.t.

m(cid:88)
m(cid:88)

i=1

i=1

αiy(i) = 0

K(x, z) = exp

(cid:18)(cid:107)x − z(cid:107)2

(cid:19)

2τ

and then determining the class with the highest posterior
probability, we obtained the results in Table 1. We saw that

Table 1. Classiﬁcation accuracies for Na¨ıve Bayes classiﬁer

Topic

Training Testing

ideafreedom
fairbusiness
freegov

87.14%
86.60%
96.19%

85.70%
80.76%
96.46%

the Na¨ıve Bayes Classiﬁer suﬀered a lot from the form letters
and also overﬁtted the training set.

3.3.2. Regularized (Bayesian) Logistic Regression

Since overﬁtting was a problem for Na¨ıve Bayes, we decided
to use regularization to restrict the norm of the learned pa-
rameters to control the VC dimension of our classiﬁer. We

Although computationally more intensive, we felt it would
yield better results. Indeed, chosen with default parameters,
this gave better results than the previous methods. In or-
der to further improve the results, we ran a model selection
algorithm to search for the best parameters for the model,
and the resulting classiﬁer yielded even better results. The
results for the optimized classiﬁer are summarized in Table
3.

Table 3. Classiﬁcation accuracies for support vector machine with
Gaussian kernel

Topic

Training Testing

ideafreedom
fairbusiness
freegov

95.22%
97.37%
97.45%

90.92%
89.03%
97.48%

Topic Analysis of the FCC’s Public Comments on Net Neutrality

Figure 1. Training Accuracy

Figure 2. Testing Accuracy

3.4. Evaluation

After selecting our best classiﬁer, an SVM with Gaussian
kernel, we also looked at the precision and recall statistics
in terms of a confusion matrix, where each column of the
matrix represents the instances in a predicted class (nega-
tive, positive), while each row represents the instances in an
actual class (negative, positive). Splitting data half and half
for training and testing, we obtained the following confusion
matrix for each topic:

ideafreedom

fairbusiness

(cid:20)386 0

fairgov

(cid:21)

9

1

Precision: 100%
Recall: 97.7%

(cid:20)168

39

(cid:21)

10
179

(cid:20)178

25

(cid:21)

18
175

Precision: 81.2%
Recall: 94.4%

Precision: 90.8%
Recall: 87.7%

We see that for all of our topics, the classiﬁer achieved both
high precision and high recall. This result boosts our conﬁ-
dence that this particular classiﬁer will be able to obtain a
reasonable classiﬁcation on our unlabeled data.

4. Results & Analysis

Since the SVM with a Gaussian kernel was our best classiﬁer,
we used this classiﬁer to derive insights on the entire dataset
of comments.

Overall, we found that the vast majority of comments talked
about either the idea of freedom or fair business practices,
with the plurality of these comments mentioning both ar-

Figure 3. Distribution of topics among all comments

guments. These results are shown in the Venn diagram in
Figure 3.

In addition, we thought it would be interesting to analyze
the arguments that people in diﬀerent states made. Cali-
fornia had about 24% of the total comments, and they lead
the percentage in every topic as well. So in order to see
the the topic breakdown for speciﬁc states, we found the
percentage of each state that argued about each topic. For
example, only 0.06% of California talked about freedom from
government interference while 0.5% of Florida and 0.41% of
Texas talked about it. Although “free government” was not
a very common topic talked about, Florida and Texas still
had a signiﬁcant number of people talk about it compared to
the overall percentage of “free government” comments in the
dataset (0.17%), considering about 84% of comments that
argued about “free government” did not specify a state.

These results are interesting because Texas and Florida are
both predominantly Republican states compared to Califor-
nia, so people in these states would care more about the
traditionally Republican ideal of having a small government.
This directly corresponds to their distaste towards govern-
ment involvement with net neutrality.

Additionally, California accounts for about 24% of all the
comments, but accounts for about 39% of the “idea free-
dom” topic. This indicates that a majority of California
cares about topics relating to our ﬁrst amendment rights and
the ability to freely post things on the internet. This makes
sense because California is one of the most liberal states in
the country.

Other than these two anomalies, other states’ topic distribu-
tions were mostly the same as the overall topic distribution.

5. Conclusion

Through our investigation, we’ve gained a better understand-
ing of the issues that people have raised regarding net neu-

Topic Analysis of the FCC’s Public Comments on Net Neutrality

Sachin Padmanabhan
spadman@stanford.edu

Leon Yao

Luda Zhao

Timothy Lee

leonyao@stanford.edu

ludazhao@stanford.edu

timothyl@stanford.edu

Department of Computer Science

Stanford University

Abstract

The FCC’s proposed net neutrality policy change in 2014 was
met with widespread public controversy and outrage. The
FCC recently released to the public millions of comments
that it received about the issue. It is abundantly clear that
the vast majority of citizens prefer to have net neutrality
intact, but what exactly are the people saying? What are
their main arguments and reasons for wanting to maintain
net neutrality? In this project, we use natural language pro-
cessing techniques to analyze the arguments in 800,000 of
the comments.

1. Introduction

1.1. Motivation

The Open Internet proceeding of the FCC (Federal Commu-
nication Commissions) is a critical regulatory eﬀort to de-
termine the future of the Internet. The proceeding concerns
“Net Neutrality”, the principle that all Internet traﬃc should
be treated equally, and no internet provider will be given
control over Internet traﬃc. Should the FCC decide not to
maintain net neutrality, ISPs will have more power to regu-
late Internet traﬃc and scrutinize data sent over the Internet.
In addition, ISPs will be able to discriminate between Inter-
net traﬃc to provide a “fast lane” for high-paying consumers.
Proponents of net neutrality argue that this will severely re-
strict free speech and privacy on the Internet. In addition,
they assert that giving ISPs diﬀerential control over Internet
traﬃc will ultimately result in extremely slow Internet for
average consumers, including individuals and corporations,
that cannot aﬀord to pay as much as large corporations. In
turn, this will hamper fair competition between businesses,
shifting the balance largely in the side of large companies
to the immense detriment of innovative startups.
Instead,
proponents maintain that the Internet should instead be re-
classiﬁed as a “common carrier”.

Project done in Stanford University’s CS 229 (Machine Learning)
course taught in Autumn 2014 by Professor Andrew Ng.

The debate on net neutrality has attracted a large response
from the American public. As of writing, the FCC proceed-
ings has attracted over 2 million comments. These comments
are important to the FCC’s decision making process, and are
usually read by people. However, given the unprecedented
number of comments, this is not possible. Under this con-
text, natural language processing is an eﬀective technique
that can help gain insight into the comments as a whole. Can
we automatically determine which issues were most pertinent
to proponents of net neutrality?

1.2. Our Work

After reading many of the comments, we saw that the argu-
ments were almost unanimously in favor of maintaining net
neutrality. However, the arguments presented varied greatly
in length, relevance, level of insight, and topic. We wanted to
determine what people’s arguments are and why they are in
favor of net neutrality. The majority of the comments made
at least one of the following arguments:

• Net neutrality is needed to protect freedom of ideas,
creativity, speech, and communication on the Internet
(ideafreedom)

• Net neutrality is needed to protect fair market competi-
tion for small businesses and startups (fairbusiness)
• Net neutrality is needed to protect the Internet from
further legislation and government intervention in the
future (fairgov)

Our goal was to classify the argument of each comment into
one or more of the above topics using supervised learning.

We decided against using an unsupervised learning approach
since it was attempted earlier by a team at Sunlight Labs.
The results they got were not very inspiring because the clus-
ters they found were only indicated by a few key words, but
were too vague to have any concrete topic behind it. Among
some very bad clusters, we did see some interesting ones like,
“small market,” “bidding,” “premium,” and “disadvantage.”
This exactly ﬁts our idea of free business. Instead of having
1 in nearly 100 clusters be interesting, we thought it would

Topic Analysis of the FCC’s Public Comments on Net Neutrality

be interesting to remove the noise and just focus on 3 topics
we knew were in the dataset.

2. Data

The original dataset released by the FCC consisted of 1.1
million raw comments along with metadata, many of which
were blank, unparseable, or too long (Les Mis´erables and
War and Peace were both submitted as comments). Fortu-
nately, the team at Sunlight Labs processed the dataset to
remove these unworkable comments and provided a cleaner
dataset of 800,959 comments with metadata in JSON format.

{

}

"applicant": "Kara J. Walton",
"dateRcpt": "2014-06-16T04:00:00Z",
"stateCd": "VA",
"zip": "20121"
"text": "Allowing the cable companies to

start charging companies for..."

To train and test our classiﬁer, we drew a random sample of
800 comments from the dataset. We manually read through
each comment to discern the arguments presented and cor-
respondingly labeled each comment.

{

}

...
"topiclabels": {

"ideafreedom": 1,
"fairbusiness": 1,
"freegov": 0

},
"formletter": 0,
"personal": 0

The rest of the comments were used after we built the clas-
siﬁer to glean interesting insight on the entire dataset.

3. Methodology

3.1. Form Letter Detection

By reading through the dataset, we noticed that a majority
(about 60%) of it was composed of “form letters,” which were
mostly identical comments written by third party organiza-
tion who had its supporters send in the same professionally
written messages.

To Chairman Tom Wheeler and the FCC Commis-
sioners To the FCC Please build any net neutrality
argument upon solid legal standing. Speciﬁcally,
this means reclassifying broadband under Title II
of the Telecommunications Act of 1934. 706 au-

thority from the Telecommunications Act has been
repeatedly struck down in court after legal chal-
lenges by telecom companies. Take the appropriate
steps to prevent this from happening again. Sin-
cerely, XXXX

Clearly, a form letter could often times sound exactly like
a regular comment. We decided to use form letters in our
topic classiﬁcation because, despite their spam-like nature,
they still signify the intentions of the individual sender who
agrees with this mass message, otherwise they wouldn’t have
taken the time to actually send it. We found that most, if
not nearly all, of these messages were from diﬀerent people.
Thus, one of the main problems we had to tune for was
overﬁtting the training set.

Instead, our goal was to use unsupervised learning to detect
exactly which comments were form letters so that we could
perform analysis on just the form letters themselves. We did
this using the Simhash algorithm, which is a generally fast
method to calculate the similarity between two documents,
and is eﬀective for near-duplicate detection.

Using the Simhash algorithm, we we found the near-
duplicates to the document being classiﬁed. If there existed
a signiﬁcant number of comments that were near duplicates,
then the comment was classiﬁed as a form letter. We used
a 64-bit hash size, shingle width of 4 letters, and hamming
distance threshold of 10 bits as parameters for the model.
Given a labelled data set of 800 comments, the model classi-
ﬁed form letters with 88% accuracy. On a data set containing
40,000 comments, the model showed that 63% of the com-
ments were form letters, similar to our initial observation’s
60% proportion.

3.2. Feature Selection

We ﬁrst preprocessed the data by removing stop words such
as “the,” “a,” “and,” etc. that appear in nearly all comments
but are essentially useless features. We also stemmed our
words, so that diﬀerent conjugations of the same word would
be counted as the same. We also tried using diﬀerent size
n-grams to increase our feature space and to capture more
of the word contexts.

We used several standard features for typical NLP datasets.
We ﬁrst found the word counts of our comments, then nor-
malized them and used TF–IDF (Term Frequency–Inverse
Document Frequency) features, which is a weighting factor
for each word that gives a value proportional to the frequency
of that word in the comment oﬀset by the frequency of the
word in the entire corpus. This allowed us to remove words
that appear in every comment, but are bad features to use
for training a classiﬁer. For example, words like “Internet”
and “FCC” were used in nearly every comment, but are not
helpful for determining if a comment is from a given class.
TF–IDF allows us to hone in on the most important features,

Topic Analysis of the FCC’s Public Comments on Net Neutrality

which is one of the best methods for feature selection.

tf(t, d) = 0.5 +

0.5f (t, d)

max{f (w, d) : w ∈ d}

N

|{d ∈ D : t ∈ D}|
idf(t, D) = log
tﬁdf(t, d, D) = tf(t, d) · idf(t, D)

On top of TF–IDF we also used a min/max frequency prun-
ing. If a word only appears once or twice in the dataset or
in every single comment, TF–IDF will assign it a low score,
but we wanted to actually reduce our feature space so as to
not overﬁt. If a word has word frequency less than our min
or greater than our max, then we removed it.

3.3. Model Selection

For each of our classiﬁers we learned a one vs. all classiﬁer
because a particular comment could have multiple diﬀerent
topics. We used 10-fold cross validation for each model, so
each training/testing accuracy we report are the generaliza-
tion accuracies.

3.3.1. Bernoulli Na¨ıve Bayes Classifier

The ﬁrst classiﬁer we tried was just a simple Na¨ıve Bayes
with Laplace smoothing for data distributed according to the
Bernoulli distribution. By ﬁnding the maximum likelihood
estimates

φj|y=1 =

φj|y=0 =

(cid:80)m
(cid:80)m
j = 1 ∧ y(i) = 1}
i=1 1{x(i)
(cid:80)m
i=1 1{y(i) = 1}
(cid:80)m
i=1 1{x(i)
j = 1 ∧ y(i) = 0}
(cid:80)m
i=1 1{y(i) = 0}
i=1 1{y(i) = 1}

φy =

m

m(cid:88)

used logistic regression with (cid:96)2 regularization, which corre-
sponds to a Gaussian prior on the data. Thus, we imple-
mented a stochastic gradient descent classiﬁer to minimize
the cost function

θ = arg max

J(θ) =

θ

i=1

log p(y(i)|x(i); θ) − λ
2

(cid:107)θ(cid:107)2

2

The results are summarized in Table 2.

Table 2. Classiﬁcation accuracies for ell2-regularized logistic re-
gression

Topic

Training Testing

ideafreedom
fairbusiness
freegov

99.24%
99.59%
99.24%

90.89%
87.72%
96.46%

3.3.3. Support Vector Machine

We ﬁnally tried an (cid:96)1-norm soft margin SVM classiﬁer with
a Gaussian kernel.

y(i)y(j)αiαjK(x(i), x(j))

m(cid:88)

m(cid:88)

i=1

j=1

αi − 1
2

0 ≤ αi ≤ C, i = 1, . . . , m

min

α

s.t.

m(cid:88)
m(cid:88)

i=1

i=1

αiy(i) = 0

K(x, z) = exp

(cid:18)(cid:107)x − z(cid:107)2

(cid:19)

2τ

and then determining the class with the highest posterior
probability, we obtained the results in Table 1. We saw that

Table 1. Classiﬁcation accuracies for Na¨ıve Bayes classiﬁer

Topic

Training Testing

ideafreedom
fairbusiness
freegov

87.14%
86.60%
96.19%

85.70%
80.76%
96.46%

the Na¨ıve Bayes Classiﬁer suﬀered a lot from the form letters
and also overﬁtted the training set.

3.3.2. Regularized (Bayesian) Logistic Regression

Since overﬁtting was a problem for Na¨ıve Bayes, we decided
to use regularization to restrict the norm of the learned pa-
rameters to control the VC dimension of our classiﬁer. We

Although computationally more intensive, we felt it would
yield better results. Indeed, chosen with default parameters,
this gave better results than the previous methods. In or-
der to further improve the results, we ran a model selection
algorithm to search for the best parameters for the model,
and the resulting classiﬁer yielded even better results. The
results for the optimized classiﬁer are summarized in Table
3.

Table 3. Classiﬁcation accuracies for support vector machine with
Gaussian kernel

Topic

Training Testing

ideafreedom
fairbusiness
freegov

95.22%
97.37%
97.45%

90.92%
89.03%
97.48%

Topic Analysis of the FCC’s Public Comments on Net Neutrality

Figure 1. Training Accuracy

Figure 2. Testing Accuracy

3.4. Evaluation

After selecting our best classiﬁer, an SVM with Gaussian
kernel, we also looked at the precision and recall statistics
in terms of a confusion matrix, where each column of the
matrix represents the instances in a predicted class (nega-
tive, positive), while each row represents the instances in an
actual class (negative, positive). Splitting data half and half
for training and testing, we obtained the following confusion
matrix for each topic:

ideafreedom

fairbusiness

(cid:20)386 0

fairgov

(cid:21)

9

1

Precision: 100%
Recall: 97.7%

(cid:20)168

39

(cid:21)

10
179

(cid:20)178

25

(cid:21)

18
175

Precision: 81.2%
Recall: 94.4%

Precision: 90.8%
Recall: 87.7%

We see that for all of our topics, the classiﬁer achieved both
high precision and high recall. This result boosts our conﬁ-
dence that this particular classiﬁer will be able to obtain a
reasonable classiﬁcation on our unlabeled data.

4. Results & Analysis

Since the SVM with a Gaussian kernel was our best classiﬁer,
we used this classiﬁer to derive insights on the entire dataset
of comments.

Overall, we found that the vast majority of comments talked
about either the idea of freedom or fair business practices,
with the plurality of these comments mentioning both ar-

Figure 3. Distribution of topics among all comments

guments. These results are shown in the Venn diagram in
Figure 3.

In addition, we thought it would be interesting to analyze
the arguments that people in diﬀerent states made. Cali-
fornia had about 24% of the total comments, and they lead
the percentage in every topic as well. So in order to see
the the topic breakdown for speciﬁc states, we found the
percentage of each state that argued about each topic. For
example, only 0.06% of California talked about freedom from
government interference while 0.5% of Florida and 0.41% of
Texas talked about it. Although “free government” was not
a very common topic talked about, Florida and Texas still
had a signiﬁcant number of people talk about it compared to
the overall percentage of “free government” comments in the
dataset (0.17%), considering about 84% of comments that
argued about “free government” did not specify a state.

These results are interesting because Texas and Florida are
both predominantly Republican states compared to Califor-
nia, so people in these states would care more about the
traditionally Republican ideal of having a small government.
This directly corresponds to their distaste towards govern-
ment involvement with net neutrality.

Additionally, California accounts for about 24% of all the
comments, but accounts for about 39% of the “idea free-
dom” topic. This indicates that a majority of California
cares about topics relating to our ﬁrst amendment rights and
the ability to freely post things on the internet. This makes
sense because California is one of the most liberal states in
the country.

Other than these two anomalies, other states’ topic distribu-
tions were mostly the same as the overall topic distribution.

5. Conclusion

Through our investigation, we’ve gained a better understand-
ing of the issues that people have raised regarding net neu-

Topic Analysis of the FCC’s Public Comments on Net Neutrality

rounding algorithms.
In Proceedings of the thiry-fourth
annual ACM symposium on Theory of computing, pp. 380–
388. ACM, 2002.

Gong, Caichun, Huang, Yulan, Cheng, Xueqi, and Bai,
Shuo. Detecting near-duplicates in large-scale short text
databases. In Advances in Knowledge Discovery and Data
Mining, pp. 877–883. Springer, 2008.

Hastie, Trevor, Tibshirani, Robert, Friedman, Jerome,
Hastie, T, Friedman, J, and Tibshirani, R. The elements
of statistical learning, volume 2. Springer, 2009.

Lannon, Bob. What can we learn from 800,000 public com-
ments on the fcc’s net neutrality plan? Sunlight Founda-
tion Blog, 2014.

Manning, Christopher D, Raghavan, Prabhakar,

Sch¨utze, Hinrich.
volume 1. Cambridge university press Cambridge, 2008.

and
Introduction to information retrieval,

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of Ma-
chine Learning Research, 12:2825–2830, 2011.

Russell, Stuart and Norvig, Peter. Artiﬁcial intelligence: A
modern approach. Artiﬁcial Intelligence. Prentice-Hall,
Egnlewood Cliﬀs, 25, 1995.

trality in the FCC’s public comments. By identifying the
most prominent concerns and training a classiﬁer using a
pre-labeled training set, we were able to classify 800,000 com-
ments and capture their broad sentiments using a fraction of
time and manpower as traditional procedures of review. In
addition, we were able to apply our topic classiﬁcation labels
to make interesting observations about the geographical dis-
tribution of topics, in which we found out that distribution
of our topic seems to follow certain regional political trends,
an unexpected but fascinating result.

Furthermore, as with most publicly gathered comments, our
dataset contains a large percentage of largely identical form
letters. Since they provide a useful metric of the level of ac-
tive public participation, we found it a worthwhile endeavor
to identify them. We were fairly successful in this regard in
using the Simhash algorithm, as our predicted percentages
of 63% matched closely with the actual amount.

6. Future Work

From this project, there are many multiple directions we can
take to abstract information from the comments for a deeper
analysis. For example, the analysis of comments pertaining
to form letters could provide very useful information. Af-
ter ﬁnding the clusters of comments from form letters in the
dataset, we can observe the geographic origins of form letters.
In addition, we can apply the same model to perform topic
classiﬁcation on the set of comments of form letters only.
Also, we can group comments by time to see what events
cause form letters to be sent (for example, a television ad-
vertisement impels viewers to send the comment through a
website). Besides form letters, we can look at the comments
at a ﬁner granularity through the lens gender. We can apply
the same model to perform topic classiﬁcation to sets of com-
ments from diﬀerent genders. By continuing this work, we
hope to achieve more interesting results about the public’s
perception of net neutrality.

Acknowledgments

Special thanks to Professor Dan Jurafsky.

References

Andoni, Alexandr and Indyk, Piotr. Near-optimal hashing
algorithms for approximate nearest neighbor in high di-
mensions.
In Foundations of Computer Science, 2006.
FOCS’06. 47th Annual IEEE Symposium on, pp. 459–468.
IEEE, 2006.

Bishop, Christopher M et al. Pattern recognition and ma-

chine learning, volume 1. springer New York, 2006.

Charikar, Moses S. Similarity estimation techniques from

