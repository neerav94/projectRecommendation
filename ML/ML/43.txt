CS229 Machine Learning • Autumn 2015

Learning Instrument Identiﬁcation

Lewis Guignard and Greg Kehoe

Stanford University

lewisg@stanford.edu gpkehoe@stanford.edu

Abstract

In this project, we utilize machine learning techniques to construct a classiﬁer for automatic instrument
recognition given a mono-track audio recording. We focus on the classiﬁcation of eight instruments
commonly used in rock music bands. By examining the spectral content of each instrument, we propose a
set of features that can be used to accurately classify musical instruments and we present results from
both supervised and unsupervised learning algorithms applied to our generated data set.

1.

Introduction

The primary goal of this project is to classify
single-instrument recordings for eight instru-
ments: Acoustic Guitar, Electric Guitar, Electric
Bass Guitar, Tenor Drum, Bass Drum, Snare
Drum, Cymbals, and Hi-Hat. The motivation
for instrument identiﬁcation is to provide in-
sights and results that we hope to use in our
continued research of multiple-source separa-
tion from a single mono-track recording.

The input to our model consists of indi-
vidual audio recordings for each instrument
in MP3 format. We perform feature extraction
from the discrete fourier transform (DFT) of the
normalized signal and label each example with
a unique integer corresponding to the instru-
ments’ class. We then use this data for analysis
and train multiple SVMs to make predictions
on the test set.

2. Related Work

Recent work on instrument classiﬁcation for
music information retrieval (MIR) focuses on
designing feature sets to accurately identify
instruments on a monophonic or polyphonic
audio recording. Popular approaches include
manual feature set construction and evalua-
tion using classic machine learning algorithms
[1], [2], the use of neural networks to learn
the feature set [3], and combinations of man-
ual design techniques with neural networks for
learning [4]. In each of these studies a common
goal is to create or learn a set of features that
will improve classiﬁer accuracy. Additionally, a

small set of features are typically derived from
the DFT such that computational complexity is
reduced.

This project contributes a novel feature set
design that allows us to obtain 93.1% classiﬁca-
tion accuracy using an SVM. Similar to other
work, we manually designed the feature set
from the DFT. However, we do not use features
extracted from the time-domain signal.

3. Dataset

Our data set consists of 1,455 samples, obtained
from both online and live recordings. The ta-
ble below shows the distribution of samples.
We originaly sought to use samples from en-
tirely online sources for rapid data collection.
However, individual samples for guitar proved
difﬁcult to ﬁnd. In order to generate samples
for each guitar, we used an AudioBox USB and
Audacity to record individual notes along the
fretboard of each guitar and generate MP3 ﬁles
with a duration of approximately 3-4 seconds
per note. Samples for the percussion instru-
ments were extracted from YouTube videos
using [5].

Instrument

Acoustic Guitar
Electric Guitar

Electric Bass Guitar

Tenor Drum
Bass Drum
Snare Drum

Cymbals

Hihat

n Samples % of Data

146
362
270
153
125
156
127
116

10.0
24.9
18.6
10.5
8.6
10.7
8.7
8.0

Given that our Feature Selection depends

1

CS229 Machine Learning • Autumn 2015

Learning Instrument Identiﬁcation

Lewis Guignard and Greg Kehoe

Stanford University

lewisg@stanford.edu gpkehoe@stanford.edu

Abstract

In this project, we utilize machine learning techniques to construct a classiﬁer for automatic instrument
recognition given a mono-track audio recording. We focus on the classiﬁcation of eight instruments
commonly used in rock music bands. By examining the spectral content of each instrument, we propose a
set of features that can be used to accurately classify musical instruments and we present results from
both supervised and unsupervised learning algorithms applied to our generated data set.

1.

Introduction

The primary goal of this project is to classify
single-instrument recordings for eight instru-
ments: Acoustic Guitar, Electric Guitar, Electric
Bass Guitar, Tenor Drum, Bass Drum, Snare
Drum, Cymbals, and Hi-Hat. The motivation
for instrument identiﬁcation is to provide in-
sights and results that we hope to use in our
continued research of multiple-source separa-
tion from a single mono-track recording.

The input to our model consists of indi-
vidual audio recordings for each instrument
in MP3 format. We perform feature extraction
from the discrete fourier transform (DFT) of the
normalized signal and label each example with
a unique integer corresponding to the instru-
ments’ class. We then use this data for analysis
and train multiple SVMs to make predictions
on the test set.

2. Related Work

Recent work on instrument classiﬁcation for
music information retrieval (MIR) focuses on
designing feature sets to accurately identify
instruments on a monophonic or polyphonic
audio recording. Popular approaches include
manual feature set construction and evalua-
tion using classic machine learning algorithms
[1], [2], the use of neural networks to learn
the feature set [3], and combinations of man-
ual design techniques with neural networks for
learning [4]. In each of these studies a common
goal is to create or learn a set of features that
will improve classiﬁer accuracy. Additionally, a

small set of features are typically derived from
the DFT such that computational complexity is
reduced.

This project contributes a novel feature set
design that allows us to obtain 93.1% classiﬁca-
tion accuracy using an SVM. Similar to other
work, we manually designed the feature set
from the DFT. However, we do not use features
extracted from the time-domain signal.

3. Dataset

Our data set consists of 1,455 samples, obtained
from both online and live recordings. The ta-
ble below shows the distribution of samples.
We originaly sought to use samples from en-
tirely online sources for rapid data collection.
However, individual samples for guitar proved
difﬁcult to ﬁnd. In order to generate samples
for each guitar, we used an AudioBox USB and
Audacity to record individual notes along the
fretboard of each guitar and generate MP3 ﬁles
with a duration of approximately 3-4 seconds
per note. Samples for the percussion instru-
ments were extracted from YouTube videos
using [5].

Instrument

Acoustic Guitar
Electric Guitar

Electric Bass Guitar

Tenor Drum
Bass Drum
Snare Drum

Cymbals

Hihat

n Samples % of Data

146
362
270
153
125
156
127
116

10.0
24.9
18.6
10.5
8.6
10.7
8.7
8.0

Given that our Feature Selection depends

1

CS229 Machine Learning • Autumn 2015

on the spectral content of the training exam-
ples, we were concerned about how dissimi-
larites in quality between WAV audio, easily
produced by our live recordings, and MP3 au-
dio as output by [5] would bias the data set. A
brief analysis of the DFT for the same record-
ing encoded as both 16-bit WAV at 44.1 KHz
and MP3 at 32 kbps revealed that no signiﬁcant
information loss occurred that incumbured our
feature extraction. Therefore, we decided to
use MP3 as the common input format.

4. Features

Features for each sample are obtained by us-
ing the DFT and identifying the 10 most pre-
dominant frequencies with the greatest per-
centage contribution to the total power of the
signal. Given that our samples are not uniform
in length, we ﬁrst normalize each sample to
have unit energy. This also removes any effects
due to variance in volume levels among the
samples. After normalizing, we ﬁnd the 10 fre-
quencies with greatest amplitude and integrate
over a log range centered at each frequency
to compute the power contribution. Using a
log range accounts for a larger octave band [6]
for notes at higher frequencies and models our
assumption that the guitars are tuned using
twelve-tone equal temperament [7]. Thus, for
each of the 10 selected frequencies, we extract
a 3-tuple of frequency, power, and amplitude
such that the nth tuple represents the frequency
component with the nth largest power contri-
bution. An example of feature extraction is
shown in Figure 1 for the same note played on
the bass guitar and acoustic guitar. We observe
that each sample gives a unique signature of
each instrument in the frequency domain.

2

Figure 1: Feature Selection, Bass and Electric Guitar

Alternatively, we considered using the en-
tire FFT as the feature vector for each sample.
However, in order to avoid high variance, we
chose to select a small number of discrete fea-
tures relative to the size of our training set.
This stemmed primarily from the difﬁculty of
acquiring a large number of training exam-
ples. Furthermore, by selecting a small set
of features shown to be most relevant by our
preliminary analysis, we were able to obtain re-
duced computational complexity and achieve
run times of under a minute.

5. Methods

Several models were applied to the dataset,
ﬁrst in an exploratory manner (K-Means, Prin-
cipal Component Analysis (PCA)), and then
for instrument classiﬁcation, a Support Vector
Machine implementing a variety of kernels.

5.1. K-Means
K-Means lends itself as a quick model to at-
tempt on the data, and as its a relatively simple
model to implement, gave us good intuition as
to how separable the classes are, at least in a
linear regime.

In the K-Means classiﬁcation algorithm, K
centroids are assigned randomly in the space
of the data, centroids being the Euclidean cen-
ters of mass for each cluster. In our case, K =
8 for all the instruments, but we also clustered
on lower values of K to look at subsets of in-
struments. Once the centroids are randomly
assigned, the following algorithm is iterated
until centroid locations stop changing:

1. Assign each example to its closest cen-

troid

2. Update centroids to be the center of mass

for the examples assigned to it.

After the centroid locations stop updating,
the algorithm has completed (converged), and
K classes have been assigned to each datapoint.
This unsupervised learning procedure can then
be compared to our instrument classes to get
an idea of how well each instrument is sepa-
rated in the feature space chosen.

CS229 Machine Learning • Autumn 2015

Learning Instrument Identiﬁcation

Lewis Guignard and Greg Kehoe

Stanford University

lewisg@stanford.edu gpkehoe@stanford.edu

Abstract

In this project, we utilize machine learning techniques to construct a classiﬁer for automatic instrument
recognition given a mono-track audio recording. We focus on the classiﬁcation of eight instruments
commonly used in rock music bands. By examining the spectral content of each instrument, we propose a
set of features that can be used to accurately classify musical instruments and we present results from
both supervised and unsupervised learning algorithms applied to our generated data set.

1.

Introduction

The primary goal of this project is to classify
single-instrument recordings for eight instru-
ments: Acoustic Guitar, Electric Guitar, Electric
Bass Guitar, Tenor Drum, Bass Drum, Snare
Drum, Cymbals, and Hi-Hat. The motivation
for instrument identiﬁcation is to provide in-
sights and results that we hope to use in our
continued research of multiple-source separa-
tion from a single mono-track recording.

The input to our model consists of indi-
vidual audio recordings for each instrument
in MP3 format. We perform feature extraction
from the discrete fourier transform (DFT) of the
normalized signal and label each example with
a unique integer corresponding to the instru-
ments’ class. We then use this data for analysis
and train multiple SVMs to make predictions
on the test set.

2. Related Work

Recent work on instrument classiﬁcation for
music information retrieval (MIR) focuses on
designing feature sets to accurately identify
instruments on a monophonic or polyphonic
audio recording. Popular approaches include
manual feature set construction and evalua-
tion using classic machine learning algorithms
[1], [2], the use of neural networks to learn
the feature set [3], and combinations of man-
ual design techniques with neural networks for
learning [4]. In each of these studies a common
goal is to create or learn a set of features that
will improve classiﬁer accuracy. Additionally, a

small set of features are typically derived from
the DFT such that computational complexity is
reduced.

This project contributes a novel feature set
design that allows us to obtain 93.1% classiﬁca-
tion accuracy using an SVM. Similar to other
work, we manually designed the feature set
from the DFT. However, we do not use features
extracted from the time-domain signal.

3. Dataset

Our data set consists of 1,455 samples, obtained
from both online and live recordings. The ta-
ble below shows the distribution of samples.
We originaly sought to use samples from en-
tirely online sources for rapid data collection.
However, individual samples for guitar proved
difﬁcult to ﬁnd. In order to generate samples
for each guitar, we used an AudioBox USB and
Audacity to record individual notes along the
fretboard of each guitar and generate MP3 ﬁles
with a duration of approximately 3-4 seconds
per note. Samples for the percussion instru-
ments were extracted from YouTube videos
using [5].

Instrument

Acoustic Guitar
Electric Guitar

Electric Bass Guitar

Tenor Drum
Bass Drum
Snare Drum

Cymbals

Hihat

n Samples % of Data

146
362
270
153
125
156
127
116

10.0
24.9
18.6
10.5
8.6
10.7
8.7
8.0

Given that our Feature Selection depends

1

CS229 Machine Learning • Autumn 2015

on the spectral content of the training exam-
ples, we were concerned about how dissimi-
larites in quality between WAV audio, easily
produced by our live recordings, and MP3 au-
dio as output by [5] would bias the data set. A
brief analysis of the DFT for the same record-
ing encoded as both 16-bit WAV at 44.1 KHz
and MP3 at 32 kbps revealed that no signiﬁcant
information loss occurred that incumbured our
feature extraction. Therefore, we decided to
use MP3 as the common input format.

4. Features

Features for each sample are obtained by us-
ing the DFT and identifying the 10 most pre-
dominant frequencies with the greatest per-
centage contribution to the total power of the
signal. Given that our samples are not uniform
in length, we ﬁrst normalize each sample to
have unit energy. This also removes any effects
due to variance in volume levels among the
samples. After normalizing, we ﬁnd the 10 fre-
quencies with greatest amplitude and integrate
over a log range centered at each frequency
to compute the power contribution. Using a
log range accounts for a larger octave band [6]
for notes at higher frequencies and models our
assumption that the guitars are tuned using
twelve-tone equal temperament [7]. Thus, for
each of the 10 selected frequencies, we extract
a 3-tuple of frequency, power, and amplitude
such that the nth tuple represents the frequency
component with the nth largest power contri-
bution. An example of feature extraction is
shown in Figure 1 for the same note played on
the bass guitar and acoustic guitar. We observe
that each sample gives a unique signature of
each instrument in the frequency domain.

2

Figure 1: Feature Selection, Bass and Electric Guitar

Alternatively, we considered using the en-
tire FFT as the feature vector for each sample.
However, in order to avoid high variance, we
chose to select a small number of discrete fea-
tures relative to the size of our training set.
This stemmed primarily from the difﬁculty of
acquiring a large number of training exam-
ples. Furthermore, by selecting a small set
of features shown to be most relevant by our
preliminary analysis, we were able to obtain re-
duced computational complexity and achieve
run times of under a minute.

5. Methods

Several models were applied to the dataset,
ﬁrst in an exploratory manner (K-Means, Prin-
cipal Component Analysis (PCA)), and then
for instrument classiﬁcation, a Support Vector
Machine implementing a variety of kernels.

5.1. K-Means
K-Means lends itself as a quick model to at-
tempt on the data, and as its a relatively simple
model to implement, gave us good intuition as
to how separable the classes are, at least in a
linear regime.

In the K-Means classiﬁcation algorithm, K
centroids are assigned randomly in the space
of the data, centroids being the Euclidean cen-
ters of mass for each cluster. In our case, K =
8 for all the instruments, but we also clustered
on lower values of K to look at subsets of in-
struments. Once the centroids are randomly
assigned, the following algorithm is iterated
until centroid locations stop changing:

1. Assign each example to its closest cen-

troid

2. Update centroids to be the center of mass

for the examples assigned to it.

After the centroid locations stop updating,
the algorithm has completed (converged), and
K classes have been assigned to each datapoint.
This unsupervised learning procedure can then
be compared to our instrument classes to get
an idea of how well each instrument is sepa-
rated in the feature space chosen.

CS229 Machine Learning • Autumn 2015

5.2. Principal Component Analysis
PCA is a method of dimensionality reduction
that can be used for visual inspection of the
data, and can also be used to map large di-
mensional data to smaller dimensions (feature
sizes), while capturing as much of the variance
(information) of the data as possible. For our
purposes, PCA was used to visually interpret
any structure in the data, and again gain an
intuition of both how separable each class is,
and what models might perform better than
others in separating them.

PCA uses a Singular Value Decomposition
to ﬁnd the principal eigenvectors of the data.
One can then visualize the data in this new
space, using a subset of the ﬁrst n principal
components, ordered by amount of variance
of the data explained.
’n’ is usually picked
as 2 or 3 for visualization purposes. Finding
the direction of the ﬁrst principal component
is equivalent to ﬁnding u such that ||u||2 = 1
and:

1
m

m∑

i=1

(x(i)Tu)2

is maximized, where m is the number of
training examples, and x(i) is a speciﬁc training
example. The second principal component can
then be found by ﬁnding the direction v that
maximizes the same equation, but is orthogo-
nal to u, and so on for higher order principal
components. In this way, one can be sure to
describe the maximum amount of variance of
the data in the fewest directions.

The mean and variance of each feature are
usually normalized (to 0 and 1, respectively)
when performing PCA, so as not to give dis-
proportionate importance to speciﬁc features.

5.3. Support Vector Machine
Support Vector Machines (SVMs) are one of
the most popular ’out of the box’ machine
learning algorithms for classiﬁcation, for their
ease of use and wide success. The SVM works
by ﬁnding an optimal hyperplane separating
the data into two classes, that gives the largest
’geometric’ margin, i.e. has all data maximally
distant from the hyperplane. When data is not
linearly separable, one can include Lagrange

Multipliers that introduce error terms that al-
low some examples to be inside the geometric
margin, and even possibly on the wrong side
of the hyperplane. The maximization problem
is:

maxα

m∑

i=1

αi − 1
2

(cid:68)

x(i), x(j)(cid:69)

i,j=1

y(i)y(j)αiαj

m∑
s.t.0 ≤ αi ≤ C, i = 1, ..., m
αiy(i) = 0

m∑

i=1

(1)

(2)

(3)

These αi and inner products can then be

used to solve for the hyperplane:

αiy(i)(cid:68)

(cid:69)

+ b

x(i), x

ωTx + b =

m∑

i=1

With this theoretical motivation, one can
use the Sequential Minimal Optimization algo-
rithm to solve for the hyperplane of a given
dataset.

One of the great beneﬁts of SVM is the inner
product of datapoints in the optimization. This
can be replaced by a Kernel, which can map
the features into a higher dimensional space,
including mixing terms, etc.

When multiple classes are present, SVM can
still be used. There are two ways to tackle the
problem, namely one versus one or one versus
all. In one versus one, there are K(K − 1)/2
models trained, one for each pair of classes
(where K is the number of classes). The ﬁnal
classiﬁcation of a test point is given to the class
for which it gets the most votes from the above
models.

5.4. Regularization and Model Selec-
tion
For regularization, we used 10-fold cross vali-
dation (CV) to compare different models used.
10-fold cross validation trains on 90 % of the
data and tests on the remaining 10 %, and re-
peats for all 10 ways to divide the dataset in
this way. The test error from each hold-out set
is then averaged to ﬁnd the estimated test error
of the model overall. In this way, one can still
train on the whole dataset given, and achieve
a pseudo test error.

3

CS229 Machine Learning • Autumn 2015

Learning Instrument Identiﬁcation

Lewis Guignard and Greg Kehoe

Stanford University

lewisg@stanford.edu gpkehoe@stanford.edu

Abstract

In this project, we utilize machine learning techniques to construct a classiﬁer for automatic instrument
recognition given a mono-track audio recording. We focus on the classiﬁcation of eight instruments
commonly used in rock music bands. By examining the spectral content of each instrument, we propose a
set of features that can be used to accurately classify musical instruments and we present results from
both supervised and unsupervised learning algorithms applied to our generated data set.

1.

Introduction

The primary goal of this project is to classify
single-instrument recordings for eight instru-
ments: Acoustic Guitar, Electric Guitar, Electric
Bass Guitar, Tenor Drum, Bass Drum, Snare
Drum, Cymbals, and Hi-Hat. The motivation
for instrument identiﬁcation is to provide in-
sights and results that we hope to use in our
continued research of multiple-source separa-
tion from a single mono-track recording.

The input to our model consists of indi-
vidual audio recordings for each instrument
in MP3 format. We perform feature extraction
from the discrete fourier transform (DFT) of the
normalized signal and label each example with
a unique integer corresponding to the instru-
ments’ class. We then use this data for analysis
and train multiple SVMs to make predictions
on the test set.

2. Related Work

Recent work on instrument classiﬁcation for
music information retrieval (MIR) focuses on
designing feature sets to accurately identify
instruments on a monophonic or polyphonic
audio recording. Popular approaches include
manual feature set construction and evalua-
tion using classic machine learning algorithms
[1], [2], the use of neural networks to learn
the feature set [3], and combinations of man-
ual design techniques with neural networks for
learning [4]. In each of these studies a common
goal is to create or learn a set of features that
will improve classiﬁer accuracy. Additionally, a

small set of features are typically derived from
the DFT such that computational complexity is
reduced.

This project contributes a novel feature set
design that allows us to obtain 93.1% classiﬁca-
tion accuracy using an SVM. Similar to other
work, we manually designed the feature set
from the DFT. However, we do not use features
extracted from the time-domain signal.

3. Dataset

Our data set consists of 1,455 samples, obtained
from both online and live recordings. The ta-
ble below shows the distribution of samples.
We originaly sought to use samples from en-
tirely online sources for rapid data collection.
However, individual samples for guitar proved
difﬁcult to ﬁnd. In order to generate samples
for each guitar, we used an AudioBox USB and
Audacity to record individual notes along the
fretboard of each guitar and generate MP3 ﬁles
with a duration of approximately 3-4 seconds
per note. Samples for the percussion instru-
ments were extracted from YouTube videos
using [5].

Instrument

Acoustic Guitar
Electric Guitar

Electric Bass Guitar

Tenor Drum
Bass Drum
Snare Drum

Cymbals

Hihat

n Samples % of Data

146
362
270
153
125
156
127
116

10.0
24.9
18.6
10.5
8.6
10.7
8.7
8.0

Given that our Feature Selection depends

1

CS229 Machine Learning • Autumn 2015

on the spectral content of the training exam-
ples, we were concerned about how dissimi-
larites in quality between WAV audio, easily
produced by our live recordings, and MP3 au-
dio as output by [5] would bias the data set. A
brief analysis of the DFT for the same record-
ing encoded as both 16-bit WAV at 44.1 KHz
and MP3 at 32 kbps revealed that no signiﬁcant
information loss occurred that incumbured our
feature extraction. Therefore, we decided to
use MP3 as the common input format.

4. Features

Features for each sample are obtained by us-
ing the DFT and identifying the 10 most pre-
dominant frequencies with the greatest per-
centage contribution to the total power of the
signal. Given that our samples are not uniform
in length, we ﬁrst normalize each sample to
have unit energy. This also removes any effects
due to variance in volume levels among the
samples. After normalizing, we ﬁnd the 10 fre-
quencies with greatest amplitude and integrate
over a log range centered at each frequency
to compute the power contribution. Using a
log range accounts for a larger octave band [6]
for notes at higher frequencies and models our
assumption that the guitars are tuned using
twelve-tone equal temperament [7]. Thus, for
each of the 10 selected frequencies, we extract
a 3-tuple of frequency, power, and amplitude
such that the nth tuple represents the frequency
component with the nth largest power contri-
bution. An example of feature extraction is
shown in Figure 1 for the same note played on
the bass guitar and acoustic guitar. We observe
that each sample gives a unique signature of
each instrument in the frequency domain.

2

Figure 1: Feature Selection, Bass and Electric Guitar

Alternatively, we considered using the en-
tire FFT as the feature vector for each sample.
However, in order to avoid high variance, we
chose to select a small number of discrete fea-
tures relative to the size of our training set.
This stemmed primarily from the difﬁculty of
acquiring a large number of training exam-
ples. Furthermore, by selecting a small set
of features shown to be most relevant by our
preliminary analysis, we were able to obtain re-
duced computational complexity and achieve
run times of under a minute.

5. Methods

Several models were applied to the dataset,
ﬁrst in an exploratory manner (K-Means, Prin-
cipal Component Analysis (PCA)), and then
for instrument classiﬁcation, a Support Vector
Machine implementing a variety of kernels.

5.1. K-Means
K-Means lends itself as a quick model to at-
tempt on the data, and as its a relatively simple
model to implement, gave us good intuition as
to how separable the classes are, at least in a
linear regime.

In the K-Means classiﬁcation algorithm, K
centroids are assigned randomly in the space
of the data, centroids being the Euclidean cen-
ters of mass for each cluster. In our case, K =
8 for all the instruments, but we also clustered
on lower values of K to look at subsets of in-
struments. Once the centroids are randomly
assigned, the following algorithm is iterated
until centroid locations stop changing:

1. Assign each example to its closest cen-

troid

2. Update centroids to be the center of mass

for the examples assigned to it.

After the centroid locations stop updating,
the algorithm has completed (converged), and
K classes have been assigned to each datapoint.
This unsupervised learning procedure can then
be compared to our instrument classes to get
an idea of how well each instrument is sepa-
rated in the feature space chosen.

CS229 Machine Learning • Autumn 2015

5.2. Principal Component Analysis
PCA is a method of dimensionality reduction
that can be used for visual inspection of the
data, and can also be used to map large di-
mensional data to smaller dimensions (feature
sizes), while capturing as much of the variance
(information) of the data as possible. For our
purposes, PCA was used to visually interpret
any structure in the data, and again gain an
intuition of both how separable each class is,
and what models might perform better than
others in separating them.

PCA uses a Singular Value Decomposition
to ﬁnd the principal eigenvectors of the data.
One can then visualize the data in this new
space, using a subset of the ﬁrst n principal
components, ordered by amount of variance
of the data explained.
’n’ is usually picked
as 2 or 3 for visualization purposes. Finding
the direction of the ﬁrst principal component
is equivalent to ﬁnding u such that ||u||2 = 1
and:

1
m

m∑

i=1

(x(i)Tu)2

is maximized, where m is the number of
training examples, and x(i) is a speciﬁc training
example. The second principal component can
then be found by ﬁnding the direction v that
maximizes the same equation, but is orthogo-
nal to u, and so on for higher order principal
components. In this way, one can be sure to
describe the maximum amount of variance of
the data in the fewest directions.

The mean and variance of each feature are
usually normalized (to 0 and 1, respectively)
when performing PCA, so as not to give dis-
proportionate importance to speciﬁc features.

5.3. Support Vector Machine
Support Vector Machines (SVMs) are one of
the most popular ’out of the box’ machine
learning algorithms for classiﬁcation, for their
ease of use and wide success. The SVM works
by ﬁnding an optimal hyperplane separating
the data into two classes, that gives the largest
’geometric’ margin, i.e. has all data maximally
distant from the hyperplane. When data is not
linearly separable, one can include Lagrange

Multipliers that introduce error terms that al-
low some examples to be inside the geometric
margin, and even possibly on the wrong side
of the hyperplane. The maximization problem
is:

maxα

m∑

i=1

αi − 1
2

(cid:68)

x(i), x(j)(cid:69)

i,j=1

y(i)y(j)αiαj

m∑
s.t.0 ≤ αi ≤ C, i = 1, ..., m
αiy(i) = 0

m∑

i=1

(1)

(2)

(3)

These αi and inner products can then be

used to solve for the hyperplane:

αiy(i)(cid:68)

(cid:69)

+ b

x(i), x

ωTx + b =

m∑

i=1

With this theoretical motivation, one can
use the Sequential Minimal Optimization algo-
rithm to solve for the hyperplane of a given
dataset.

One of the great beneﬁts of SVM is the inner
product of datapoints in the optimization. This
can be replaced by a Kernel, which can map
the features into a higher dimensional space,
including mixing terms, etc.

When multiple classes are present, SVM can
still be used. There are two ways to tackle the
problem, namely one versus one or one versus
all. In one versus one, there are K(K − 1)/2
models trained, one for each pair of classes
(where K is the number of classes). The ﬁnal
classiﬁcation of a test point is given to the class
for which it gets the most votes from the above
models.

5.4. Regularization and Model Selec-
tion
For regularization, we used 10-fold cross vali-
dation (CV) to compare different models used.
10-fold cross validation trains on 90 % of the
data and tests on the remaining 10 %, and re-
peats for all 10 ways to divide the dataset in
this way. The test error from each hold-out set
is then averaged to ﬁnd the estimated test error
of the model overall. In this way, one can still
train on the whole dataset given, and achieve
a pseudo test error.

3

CS229 Machine Learning • Autumn 2015

In choosing which features to use in the
model, we implemented both forward search
and backward search and compared the results.
In forward search, one trains a model on each
feature alone and chooses the model with the
lowest CV error. Then, one iteratively checks
which of the remaining unused features will
lower the CV error when included in the model.
The algorithm ﬁnishes when adding any un-
used feature will only increase the CV error.
In backwards search, one starts with a model
using all features, then iteratively chooses to re-
move one feature that lowers the CV error the
most. The algorithm terminates when remov-
ing any more single features will only increase
the CV error.

6. Results / Discussion

6.1. K-Means
In implementing K-Means on all eight instru-
ments, we found no distinguishable mapping
from the classes generated by the clustering
to the classes of instruments in our dataset.
We assume this is because, at least in a linear
space, there are large overlaps of features from
each class. We then asked ourselves how well
K-Means would work on instruments whose
fourier transforms seemed far apart. We mea-
sured success of the algorithm by ratio of num-
ber of correctly classed datapoints to total num-
ber of datapoints.

In comparing the data of Electric Guitar and
Cymbals, K-Means with 2 clusters achieved
93.2 % accuracy, and when comparing Electric
Guitar to Tenor Drum, 58.4% accuracy was ob-
tained. With no good clustering on all eight
instruments, or on separating the Electric, Bass,
and Acoustic Guitar, we moved to other algo-
rithms.

6.2. Principal Component Analysis
The goal of running PCA on the dataset was
exploratory in nature; to look for any structure
in the dataset and gain insight into the separa-
bility of the classes of instruments. We found
when looking at all instruments in the ﬁrst
three components, a tetrahedral shape, where
the guitars were spread along one face, and

4

cymbals along another. All data converges to
one point in this space, and three boundaries of
the tetrahedron are sharp, in that data does not
move beyond them. We would be interested in
ﬁnding other instruments or sounds / effects
that might cross these boundaries.

Figure 2: PCA of all data

Figure 2 shows the complete dataset in the
ﬁrst three principal components, explaining
66% of the variance of the data, with propor-
tion of variance explained for each component
being:

PC #

1

2

% Var expn’d 38.1

21.4

3
6.8

Figure 3: Initial Feature Selection

Figure 3 illuminates the three guitars alone
in the ﬁrst three principal components. The
instruments are not linearly separable in this

CS229 Machine Learning • Autumn 2015

Learning Instrument Identiﬁcation

Lewis Guignard and Greg Kehoe

Stanford University

lewisg@stanford.edu gpkehoe@stanford.edu

Abstract

In this project, we utilize machine learning techniques to construct a classiﬁer for automatic instrument
recognition given a mono-track audio recording. We focus on the classiﬁcation of eight instruments
commonly used in rock music bands. By examining the spectral content of each instrument, we propose a
set of features that can be used to accurately classify musical instruments and we present results from
both supervised and unsupervised learning algorithms applied to our generated data set.

1.

Introduction

The primary goal of this project is to classify
single-instrument recordings for eight instru-
ments: Acoustic Guitar, Electric Guitar, Electric
Bass Guitar, Tenor Drum, Bass Drum, Snare
Drum, Cymbals, and Hi-Hat. The motivation
for instrument identiﬁcation is to provide in-
sights and results that we hope to use in our
continued research of multiple-source separa-
tion from a single mono-track recording.

The input to our model consists of indi-
vidual audio recordings for each instrument
in MP3 format. We perform feature extraction
from the discrete fourier transform (DFT) of the
normalized signal and label each example with
a unique integer corresponding to the instru-
ments’ class. We then use this data for analysis
and train multiple SVMs to make predictions
on the test set.

2. Related Work

Recent work on instrument classiﬁcation for
music information retrieval (MIR) focuses on
designing feature sets to accurately identify
instruments on a monophonic or polyphonic
audio recording. Popular approaches include
manual feature set construction and evalua-
tion using classic machine learning algorithms
[1], [2], the use of neural networks to learn
the feature set [3], and combinations of man-
ual design techniques with neural networks for
learning [4]. In each of these studies a common
goal is to create or learn a set of features that
will improve classiﬁer accuracy. Additionally, a

small set of features are typically derived from
the DFT such that computational complexity is
reduced.

This project contributes a novel feature set
design that allows us to obtain 93.1% classiﬁca-
tion accuracy using an SVM. Similar to other
work, we manually designed the feature set
from the DFT. However, we do not use features
extracted from the time-domain signal.

3. Dataset

Our data set consists of 1,455 samples, obtained
from both online and live recordings. The ta-
ble below shows the distribution of samples.
We originaly sought to use samples from en-
tirely online sources for rapid data collection.
However, individual samples for guitar proved
difﬁcult to ﬁnd. In order to generate samples
for each guitar, we used an AudioBox USB and
Audacity to record individual notes along the
fretboard of each guitar and generate MP3 ﬁles
with a duration of approximately 3-4 seconds
per note. Samples for the percussion instru-
ments were extracted from YouTube videos
using [5].

Instrument

Acoustic Guitar
Electric Guitar

Electric Bass Guitar

Tenor Drum
Bass Drum
Snare Drum

Cymbals

Hihat

n Samples % of Data

146
362
270
153
125
156
127
116

10.0
24.9
18.6
10.5
8.6
10.7
8.7
8.0

Given that our Feature Selection depends

1

CS229 Machine Learning • Autumn 2015

on the spectral content of the training exam-
ples, we were concerned about how dissimi-
larites in quality between WAV audio, easily
produced by our live recordings, and MP3 au-
dio as output by [5] would bias the data set. A
brief analysis of the DFT for the same record-
ing encoded as both 16-bit WAV at 44.1 KHz
and MP3 at 32 kbps revealed that no signiﬁcant
information loss occurred that incumbured our
feature extraction. Therefore, we decided to
use MP3 as the common input format.

4. Features

Features for each sample are obtained by us-
ing the DFT and identifying the 10 most pre-
dominant frequencies with the greatest per-
centage contribution to the total power of the
signal. Given that our samples are not uniform
in length, we ﬁrst normalize each sample to
have unit energy. This also removes any effects
due to variance in volume levels among the
samples. After normalizing, we ﬁnd the 10 fre-
quencies with greatest amplitude and integrate
over a log range centered at each frequency
to compute the power contribution. Using a
log range accounts for a larger octave band [6]
for notes at higher frequencies and models our
assumption that the guitars are tuned using
twelve-tone equal temperament [7]. Thus, for
each of the 10 selected frequencies, we extract
a 3-tuple of frequency, power, and amplitude
such that the nth tuple represents the frequency
component with the nth largest power contri-
bution. An example of feature extraction is
shown in Figure 1 for the same note played on
the bass guitar and acoustic guitar. We observe
that each sample gives a unique signature of
each instrument in the frequency domain.

2

Figure 1: Feature Selection, Bass and Electric Guitar

Alternatively, we considered using the en-
tire FFT as the feature vector for each sample.
However, in order to avoid high variance, we
chose to select a small number of discrete fea-
tures relative to the size of our training set.
This stemmed primarily from the difﬁculty of
acquiring a large number of training exam-
ples. Furthermore, by selecting a small set
of features shown to be most relevant by our
preliminary analysis, we were able to obtain re-
duced computational complexity and achieve
run times of under a minute.

5. Methods

Several models were applied to the dataset,
ﬁrst in an exploratory manner (K-Means, Prin-
cipal Component Analysis (PCA)), and then
for instrument classiﬁcation, a Support Vector
Machine implementing a variety of kernels.

5.1. K-Means
K-Means lends itself as a quick model to at-
tempt on the data, and as its a relatively simple
model to implement, gave us good intuition as
to how separable the classes are, at least in a
linear regime.

In the K-Means classiﬁcation algorithm, K
centroids are assigned randomly in the space
of the data, centroids being the Euclidean cen-
ters of mass for each cluster. In our case, K =
8 for all the instruments, but we also clustered
on lower values of K to look at subsets of in-
struments. Once the centroids are randomly
assigned, the following algorithm is iterated
until centroid locations stop changing:

1. Assign each example to its closest cen-

troid

2. Update centroids to be the center of mass

for the examples assigned to it.

After the centroid locations stop updating,
the algorithm has completed (converged), and
K classes have been assigned to each datapoint.
This unsupervised learning procedure can then
be compared to our instrument classes to get
an idea of how well each instrument is sepa-
rated in the feature space chosen.

CS229 Machine Learning • Autumn 2015

5.2. Principal Component Analysis
PCA is a method of dimensionality reduction
that can be used for visual inspection of the
data, and can also be used to map large di-
mensional data to smaller dimensions (feature
sizes), while capturing as much of the variance
(information) of the data as possible. For our
purposes, PCA was used to visually interpret
any structure in the data, and again gain an
intuition of both how separable each class is,
and what models might perform better than
others in separating them.

PCA uses a Singular Value Decomposition
to ﬁnd the principal eigenvectors of the data.
One can then visualize the data in this new
space, using a subset of the ﬁrst n principal
components, ordered by amount of variance
of the data explained.
’n’ is usually picked
as 2 or 3 for visualization purposes. Finding
the direction of the ﬁrst principal component
is equivalent to ﬁnding u such that ||u||2 = 1
and:

1
m

m∑

i=1

(x(i)Tu)2

is maximized, where m is the number of
training examples, and x(i) is a speciﬁc training
example. The second principal component can
then be found by ﬁnding the direction v that
maximizes the same equation, but is orthogo-
nal to u, and so on for higher order principal
components. In this way, one can be sure to
describe the maximum amount of variance of
the data in the fewest directions.

The mean and variance of each feature are
usually normalized (to 0 and 1, respectively)
when performing PCA, so as not to give dis-
proportionate importance to speciﬁc features.

5.3. Support Vector Machine
Support Vector Machines (SVMs) are one of
the most popular ’out of the box’ machine
learning algorithms for classiﬁcation, for their
ease of use and wide success. The SVM works
by ﬁnding an optimal hyperplane separating
the data into two classes, that gives the largest
’geometric’ margin, i.e. has all data maximally
distant from the hyperplane. When data is not
linearly separable, one can include Lagrange

Multipliers that introduce error terms that al-
low some examples to be inside the geometric
margin, and even possibly on the wrong side
of the hyperplane. The maximization problem
is:

maxα

m∑

i=1

αi − 1
2

(cid:68)

x(i), x(j)(cid:69)

i,j=1

y(i)y(j)αiαj

m∑
s.t.0 ≤ αi ≤ C, i = 1, ..., m
αiy(i) = 0

m∑

i=1

(1)

(2)

(3)

These αi and inner products can then be

used to solve for the hyperplane:

αiy(i)(cid:68)

(cid:69)

+ b

x(i), x

ωTx + b =

m∑

i=1

With this theoretical motivation, one can
use the Sequential Minimal Optimization algo-
rithm to solve for the hyperplane of a given
dataset.

One of the great beneﬁts of SVM is the inner
product of datapoints in the optimization. This
can be replaced by a Kernel, which can map
the features into a higher dimensional space,
including mixing terms, etc.

When multiple classes are present, SVM can
still be used. There are two ways to tackle the
problem, namely one versus one or one versus
all. In one versus one, there are K(K − 1)/2
models trained, one for each pair of classes
(where K is the number of classes). The ﬁnal
classiﬁcation of a test point is given to the class
for which it gets the most votes from the above
models.

5.4. Regularization and Model Selec-
tion
For regularization, we used 10-fold cross vali-
dation (CV) to compare different models used.
10-fold cross validation trains on 90 % of the
data and tests on the remaining 10 %, and re-
peats for all 10 ways to divide the dataset in
this way. The test error from each hold-out set
is then averaged to ﬁnd the estimated test error
of the model overall. In this way, one can still
train on the whole dataset given, and achieve
a pseudo test error.

3

CS229 Machine Learning • Autumn 2015

In choosing which features to use in the
model, we implemented both forward search
and backward search and compared the results.
In forward search, one trains a model on each
feature alone and chooses the model with the
lowest CV error. Then, one iteratively checks
which of the remaining unused features will
lower the CV error when included in the model.
The algorithm ﬁnishes when adding any un-
used feature will only increase the CV error.
In backwards search, one starts with a model
using all features, then iteratively chooses to re-
move one feature that lowers the CV error the
most. The algorithm terminates when remov-
ing any more single features will only increase
the CV error.

6. Results / Discussion

6.1. K-Means
In implementing K-Means on all eight instru-
ments, we found no distinguishable mapping
from the classes generated by the clustering
to the classes of instruments in our dataset.
We assume this is because, at least in a linear
space, there are large overlaps of features from
each class. We then asked ourselves how well
K-Means would work on instruments whose
fourier transforms seemed far apart. We mea-
sured success of the algorithm by ratio of num-
ber of correctly classed datapoints to total num-
ber of datapoints.

In comparing the data of Electric Guitar and
Cymbals, K-Means with 2 clusters achieved
93.2 % accuracy, and when comparing Electric
Guitar to Tenor Drum, 58.4% accuracy was ob-
tained. With no good clustering on all eight
instruments, or on separating the Electric, Bass,
and Acoustic Guitar, we moved to other algo-
rithms.

6.2. Principal Component Analysis
The goal of running PCA on the dataset was
exploratory in nature; to look for any structure
in the dataset and gain insight into the separa-
bility of the classes of instruments. We found
when looking at all instruments in the ﬁrst
three components, a tetrahedral shape, where
the guitars were spread along one face, and

4

cymbals along another. All data converges to
one point in this space, and three boundaries of
the tetrahedron are sharp, in that data does not
move beyond them. We would be interested in
ﬁnding other instruments or sounds / effects
that might cross these boundaries.

Figure 2: PCA of all data

Figure 2 shows the complete dataset in the
ﬁrst three principal components, explaining
66% of the variance of the data, with propor-
tion of variance explained for each component
being:

PC #

1

2

% Var expn’d 38.1

21.4

3
6.8

Figure 3: Initial Feature Selection

Figure 3 illuminates the three guitars alone
in the ﬁrst three principal components. The
instruments are not linearly separable in this

CS229 Machine Learning • Autumn 2015

subspace, but their respective centroids are dis-
tinct. We ﬁnd the mediocre results of K-Means
surprising, considering how distinct the data
is here, although we note that regularization
or model selection was not performed on the
feature set for K-Means. The three principal
components explain 53.9% of the variance, with
the proportion in each component being:

PC #

1

2

% Var expn’d 26.3

18.4

3
9.2

6.3. Support Vector Machine
For the classiﬁcation algorithm, we chose an
SVM algorithm to attempt to classify all 8 in-
struments, and to classify the subset of Electric,
Acoustic and Bass Guitars. Models of Linear,
Gaussian, and Polynomial kernels were trained
on a random 80 % subset of the data, and tested
on the remaining 20% for a range of applicable
parameters. We found the polynomial kernel
to have the best performance of the three ker-
nels, varying cost (C), Gamma (γ), and degree
(d) per the below kernel (for cost, see equation
(2)).

K(x(i), x(j)) = (γx(i)Tx(j))d

(4)

We attempted a model for every combina-

tion of the following values of parameters:

C ∈ {.1, 1, 10, 100}
γ ∈ {.5, 1, 2}
d ∈ {2, 3, 4}

The model with lowest test error when clas-
sifying all instruments used: C = .1, γ = 1, d =
2, and when classifying the three guitar instru-
ments, used: C = .1, γ = .5, d = 3.

Using these optimal parameters, we ran 10-
fold cross validation with each model on its
respective dataset, then applied both forward
and backward selection to attempt a better er-
ror rate using a subset of the features.

CV Error
Forward
Backward

All instruments Guitars
13.1%
8.9%
6.9%

17.8%
17.3%
15.4%

Using the backwards subset of the features
(29 of the 30 features), looking only at guitars,
we train on 80% of the data and test on the
remaining 20% to produce the following con-
fusion matrix.

Predict

Truth

Acou
Bass
Elec

Acou Bass Elec

23
0
1

1
43
3

0
2
76

7. Conclusions / Future Work

When separating musical instruments, choos-
ing an indicative subset of features to predict
on is extremely important. Using frequency,
amplitude and power of the top ten peaks of
the spectrum of the samples proved a good sub-
set of features, as shown in PCA visually and
SVM analytically. K-Means did not perform
well, and we ﬁnd this intriguing considering
how well SVM performed. We assume this is
because of the freedom that the SVM method
has in mapping to higher order spaces, and
including mixing terms.

Given more time and resources, we would
move onto source separation, possibly aug-
menting an Independent Component Analysis
method, or exploring neural networks.

5

CS229 Machine Learning • Autumn 2015

Learning Instrument Identiﬁcation

Lewis Guignard and Greg Kehoe

Stanford University

lewisg@stanford.edu gpkehoe@stanford.edu

Abstract

In this project, we utilize machine learning techniques to construct a classiﬁer for automatic instrument
recognition given a mono-track audio recording. We focus on the classiﬁcation of eight instruments
commonly used in rock music bands. By examining the spectral content of each instrument, we propose a
set of features that can be used to accurately classify musical instruments and we present results from
both supervised and unsupervised learning algorithms applied to our generated data set.

1.

Introduction

The primary goal of this project is to classify
single-instrument recordings for eight instru-
ments: Acoustic Guitar, Electric Guitar, Electric
Bass Guitar, Tenor Drum, Bass Drum, Snare
Drum, Cymbals, and Hi-Hat. The motivation
for instrument identiﬁcation is to provide in-
sights and results that we hope to use in our
continued research of multiple-source separa-
tion from a single mono-track recording.

The input to our model consists of indi-
vidual audio recordings for each instrument
in MP3 format. We perform feature extraction
from the discrete fourier transform (DFT) of the
normalized signal and label each example with
a unique integer corresponding to the instru-
ments’ class. We then use this data for analysis
and train multiple SVMs to make predictions
on the test set.

2. Related Work

Recent work on instrument classiﬁcation for
music information retrieval (MIR) focuses on
designing feature sets to accurately identify
instruments on a monophonic or polyphonic
audio recording. Popular approaches include
manual feature set construction and evalua-
tion using classic machine learning algorithms
[1], [2], the use of neural networks to learn
the feature set [3], and combinations of man-
ual design techniques with neural networks for
learning [4]. In each of these studies a common
goal is to create or learn a set of features that
will improve classiﬁer accuracy. Additionally, a

small set of features are typically derived from
the DFT such that computational complexity is
reduced.

This project contributes a novel feature set
design that allows us to obtain 93.1% classiﬁca-
tion accuracy using an SVM. Similar to other
work, we manually designed the feature set
from the DFT. However, we do not use features
extracted from the time-domain signal.

3. Dataset

Our data set consists of 1,455 samples, obtained
from both online and live recordings. The ta-
ble below shows the distribution of samples.
We originaly sought to use samples from en-
tirely online sources for rapid data collection.
However, individual samples for guitar proved
difﬁcult to ﬁnd. In order to generate samples
for each guitar, we used an AudioBox USB and
Audacity to record individual notes along the
fretboard of each guitar and generate MP3 ﬁles
with a duration of approximately 3-4 seconds
per note. Samples for the percussion instru-
ments were extracted from YouTube videos
using [5].

Instrument

Acoustic Guitar
Electric Guitar

Electric Bass Guitar

Tenor Drum
Bass Drum
Snare Drum

Cymbals

Hihat

n Samples % of Data

146
362
270
153
125
156
127
116

10.0
24.9
18.6
10.5
8.6
10.7
8.7
8.0

Given that our Feature Selection depends

1

CS229 Machine Learning • Autumn 2015

on the spectral content of the training exam-
ples, we were concerned about how dissimi-
larites in quality between WAV audio, easily
produced by our live recordings, and MP3 au-
dio as output by [5] would bias the data set. A
brief analysis of the DFT for the same record-
ing encoded as both 16-bit WAV at 44.1 KHz
and MP3 at 32 kbps revealed that no signiﬁcant
information loss occurred that incumbured our
feature extraction. Therefore, we decided to
use MP3 as the common input format.

4. Features

Features for each sample are obtained by us-
ing the DFT and identifying the 10 most pre-
dominant frequencies with the greatest per-
centage contribution to the total power of the
signal. Given that our samples are not uniform
in length, we ﬁrst normalize each sample to
have unit energy. This also removes any effects
due to variance in volume levels among the
samples. After normalizing, we ﬁnd the 10 fre-
quencies with greatest amplitude and integrate
over a log range centered at each frequency
to compute the power contribution. Using a
log range accounts for a larger octave band [6]
for notes at higher frequencies and models our
assumption that the guitars are tuned using
twelve-tone equal temperament [7]. Thus, for
each of the 10 selected frequencies, we extract
a 3-tuple of frequency, power, and amplitude
such that the nth tuple represents the frequency
component with the nth largest power contri-
bution. An example of feature extraction is
shown in Figure 1 for the same note played on
the bass guitar and acoustic guitar. We observe
that each sample gives a unique signature of
each instrument in the frequency domain.

2

Figure 1: Feature Selection, Bass and Electric Guitar

Alternatively, we considered using the en-
tire FFT as the feature vector for each sample.
However, in order to avoid high variance, we
chose to select a small number of discrete fea-
tures relative to the size of our training set.
This stemmed primarily from the difﬁculty of
acquiring a large number of training exam-
ples. Furthermore, by selecting a small set
of features shown to be most relevant by our
preliminary analysis, we were able to obtain re-
duced computational complexity and achieve
run times of under a minute.

5. Methods

Several models were applied to the dataset,
ﬁrst in an exploratory manner (K-Means, Prin-
cipal Component Analysis (PCA)), and then
for instrument classiﬁcation, a Support Vector
Machine implementing a variety of kernels.

5.1. K-Means
K-Means lends itself as a quick model to at-
tempt on the data, and as its a relatively simple
model to implement, gave us good intuition as
to how separable the classes are, at least in a
linear regime.

In the K-Means classiﬁcation algorithm, K
centroids are assigned randomly in the space
of the data, centroids being the Euclidean cen-
ters of mass for each cluster. In our case, K =
8 for all the instruments, but we also clustered
on lower values of K to look at subsets of in-
struments. Once the centroids are randomly
assigned, the following algorithm is iterated
until centroid locations stop changing:

1. Assign each example to its closest cen-

troid

2. Update centroids to be the center of mass

for the examples assigned to it.

After the centroid locations stop updating,
the algorithm has completed (converged), and
K classes have been assigned to each datapoint.
This unsupervised learning procedure can then
be compared to our instrument classes to get
an idea of how well each instrument is sepa-
rated in the feature space chosen.

CS229 Machine Learning • Autumn 2015

5.2. Principal Component Analysis
PCA is a method of dimensionality reduction
that can be used for visual inspection of the
data, and can also be used to map large di-
mensional data to smaller dimensions (feature
sizes), while capturing as much of the variance
(information) of the data as possible. For our
purposes, PCA was used to visually interpret
any structure in the data, and again gain an
intuition of both how separable each class is,
and what models might perform better than
others in separating them.

PCA uses a Singular Value Decomposition
to ﬁnd the principal eigenvectors of the data.
One can then visualize the data in this new
space, using a subset of the ﬁrst n principal
components, ordered by amount of variance
of the data explained.
’n’ is usually picked
as 2 or 3 for visualization purposes. Finding
the direction of the ﬁrst principal component
is equivalent to ﬁnding u such that ||u||2 = 1
and:

1
m

m∑

i=1

(x(i)Tu)2

is maximized, where m is the number of
training examples, and x(i) is a speciﬁc training
example. The second principal component can
then be found by ﬁnding the direction v that
maximizes the same equation, but is orthogo-
nal to u, and so on for higher order principal
components. In this way, one can be sure to
describe the maximum amount of variance of
the data in the fewest directions.

The mean and variance of each feature are
usually normalized (to 0 and 1, respectively)
when performing PCA, so as not to give dis-
proportionate importance to speciﬁc features.

5.3. Support Vector Machine
Support Vector Machines (SVMs) are one of
the most popular ’out of the box’ machine
learning algorithms for classiﬁcation, for their
ease of use and wide success. The SVM works
by ﬁnding an optimal hyperplane separating
the data into two classes, that gives the largest
’geometric’ margin, i.e. has all data maximally
distant from the hyperplane. When data is not
linearly separable, one can include Lagrange

Multipliers that introduce error terms that al-
low some examples to be inside the geometric
margin, and even possibly on the wrong side
of the hyperplane. The maximization problem
is:

maxα

m∑

i=1

αi − 1
2

(cid:68)

x(i), x(j)(cid:69)

i,j=1

y(i)y(j)αiαj

m∑
s.t.0 ≤ αi ≤ C, i = 1, ..., m
αiy(i) = 0

m∑

i=1

(1)

(2)

(3)

These αi and inner products can then be

used to solve for the hyperplane:

αiy(i)(cid:68)

(cid:69)

+ b

x(i), x

ωTx + b =

m∑

i=1

With this theoretical motivation, one can
use the Sequential Minimal Optimization algo-
rithm to solve for the hyperplane of a given
dataset.

One of the great beneﬁts of SVM is the inner
product of datapoints in the optimization. This
can be replaced by a Kernel, which can map
the features into a higher dimensional space,
including mixing terms, etc.

When multiple classes are present, SVM can
still be used. There are two ways to tackle the
problem, namely one versus one or one versus
all. In one versus one, there are K(K − 1)/2
models trained, one for each pair of classes
(where K is the number of classes). The ﬁnal
classiﬁcation of a test point is given to the class
for which it gets the most votes from the above
models.

5.4. Regularization and Model Selec-
tion
For regularization, we used 10-fold cross vali-
dation (CV) to compare different models used.
10-fold cross validation trains on 90 % of the
data and tests on the remaining 10 %, and re-
peats for all 10 ways to divide the dataset in
this way. The test error from each hold-out set
is then averaged to ﬁnd the estimated test error
of the model overall. In this way, one can still
train on the whole dataset given, and achieve
a pseudo test error.

3

CS229 Machine Learning • Autumn 2015

In choosing which features to use in the
model, we implemented both forward search
and backward search and compared the results.
In forward search, one trains a model on each
feature alone and chooses the model with the
lowest CV error. Then, one iteratively checks
which of the remaining unused features will
lower the CV error when included in the model.
The algorithm ﬁnishes when adding any un-
used feature will only increase the CV error.
In backwards search, one starts with a model
using all features, then iteratively chooses to re-
move one feature that lowers the CV error the
most. The algorithm terminates when remov-
ing any more single features will only increase
the CV error.

6. Results / Discussion

6.1. K-Means
In implementing K-Means on all eight instru-
ments, we found no distinguishable mapping
from the classes generated by the clustering
to the classes of instruments in our dataset.
We assume this is because, at least in a linear
space, there are large overlaps of features from
each class. We then asked ourselves how well
K-Means would work on instruments whose
fourier transforms seemed far apart. We mea-
sured success of the algorithm by ratio of num-
ber of correctly classed datapoints to total num-
ber of datapoints.

In comparing the data of Electric Guitar and
Cymbals, K-Means with 2 clusters achieved
93.2 % accuracy, and when comparing Electric
Guitar to Tenor Drum, 58.4% accuracy was ob-
tained. With no good clustering on all eight
instruments, or on separating the Electric, Bass,
and Acoustic Guitar, we moved to other algo-
rithms.

6.2. Principal Component Analysis
The goal of running PCA on the dataset was
exploratory in nature; to look for any structure
in the dataset and gain insight into the separa-
bility of the classes of instruments. We found
when looking at all instruments in the ﬁrst
three components, a tetrahedral shape, where
the guitars were spread along one face, and

4

cymbals along another. All data converges to
one point in this space, and three boundaries of
the tetrahedron are sharp, in that data does not
move beyond them. We would be interested in
ﬁnding other instruments or sounds / effects
that might cross these boundaries.

Figure 2: PCA of all data

Figure 2 shows the complete dataset in the
ﬁrst three principal components, explaining
66% of the variance of the data, with propor-
tion of variance explained for each component
being:

PC #

1

2

% Var expn’d 38.1

21.4

3
6.8

Figure 3: Initial Feature Selection

Figure 3 illuminates the three guitars alone
in the ﬁrst three principal components. The
instruments are not linearly separable in this

CS229 Machine Learning • Autumn 2015

subspace, but their respective centroids are dis-
tinct. We ﬁnd the mediocre results of K-Means
surprising, considering how distinct the data
is here, although we note that regularization
or model selection was not performed on the
feature set for K-Means. The three principal
components explain 53.9% of the variance, with
the proportion in each component being:

PC #

1

2

% Var expn’d 26.3

18.4

3
9.2

6.3. Support Vector Machine
For the classiﬁcation algorithm, we chose an
SVM algorithm to attempt to classify all 8 in-
struments, and to classify the subset of Electric,
Acoustic and Bass Guitars. Models of Linear,
Gaussian, and Polynomial kernels were trained
on a random 80 % subset of the data, and tested
on the remaining 20% for a range of applicable
parameters. We found the polynomial kernel
to have the best performance of the three ker-
nels, varying cost (C), Gamma (γ), and degree
(d) per the below kernel (for cost, see equation
(2)).

K(x(i), x(j)) = (γx(i)Tx(j))d

(4)

We attempted a model for every combina-

tion of the following values of parameters:

C ∈ {.1, 1, 10, 100}
γ ∈ {.5, 1, 2}
d ∈ {2, 3, 4}

The model with lowest test error when clas-
sifying all instruments used: C = .1, γ = 1, d =
2, and when classifying the three guitar instru-
ments, used: C = .1, γ = .5, d = 3.

Using these optimal parameters, we ran 10-
fold cross validation with each model on its
respective dataset, then applied both forward
and backward selection to attempt a better er-
ror rate using a subset of the features.

CV Error
Forward
Backward

All instruments Guitars
13.1%
8.9%
6.9%

17.8%
17.3%
15.4%

Using the backwards subset of the features
(29 of the 30 features), looking only at guitars,
we train on 80% of the data and test on the
remaining 20% to produce the following con-
fusion matrix.

Predict

Truth

Acou
Bass
Elec

Acou Bass Elec

23
0
1

1
43
3

0
2
76

7. Conclusions / Future Work

When separating musical instruments, choos-
ing an indicative subset of features to predict
on is extremely important. Using frequency,
amplitude and power of the top ten peaks of
the spectrum of the samples proved a good sub-
set of features, as shown in PCA visually and
SVM analytically. K-Means did not perform
well, and we ﬁnd this intriguing considering
how well SVM performed. We assume this is
because of the freedom that the SVM method
has in mapping to higher order spaces, and
including mixing terms.

Given more time and resources, we would
move onto source separation, possibly aug-
menting an Independent Component Analysis
method, or exploring neural networks.

5

CS229 Machine Learning • Autumn 2015

References

[1] Y Takahashi and K Kondo. Comparison of two classiﬁcation methods for musical instrument
identiﬁcation. In Consumer Electronics (GCCE), 2014 IEEE 3rd Global Conference on, pages 67–68.
IEEE, 2014.

[2] Elzbieta Kubera, Alicja A Wieczorkowska, and Magdalena Skrzypiec. Inﬂuence of feature sets
on precision, recall, and accuracy of identiﬁcation of musical instruments in audio recordings.
In ISMIS, pages 204–213. Springer, 2014.

[3] Peter Li, Jiyuan Qian, and Tian Wang. Automatic instrument recognition in polyphonic music

using convolutional neural networks. arXiv preprint arXiv:1511.05520, 2015.

[4] DG Bhalke, CB Rama Rao, and DS Bormane. Automatic musical instrument classiﬁcation using
fractional fourier transform based-mfcc features and counter propagation neural network.
Journal of Intelligent Information Systems, pages 1–22, 2015.

[5] Youtube to mp3 generator. www.youtube-mp3.org.

[6] Octave band. https://en.wikipedia.org/wiki/Octave_band.

[7] Equal temperament. https://en.wikipedia.org/wiki/Equal_temperament.

6

