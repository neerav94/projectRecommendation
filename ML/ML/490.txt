Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:19

Table II. Published Algorithms Related to the Design Space

Algorithm
Clauset et al. [2004] (CNM)
Danon et al. [2006]
Wakita and Tsurumi [2007]
Schuetz and Caﬂisch [2008a] (SC)
Pons and Latapy [2006] (PL)
Pujol et al. [2006] (PBD)
Ye et al. [2008] (YHY)
Liu and Murata [2010] (LPA)
Blondel et al. [2008] (BGLL)
L ¨u and Huang [2009] (LH)
Sun et al. [2009] (SDJB)
CJ0 ZS+ML50+LM
LM MI+ML100+LM

Coarsening
CJ0 MI
CJ0 GC
CJ0 WHN/WHE
CJx MI
CJ0 + random walk
prejoining, modiﬁed CJ0
mixed GM MI, CJ0 MI
mixed LM MI, CJx MI
LM MI
subdivision by tabu search SL
spectral bisection + KL MI SL
CJ0 ZS
LM MI

Multilevel Reﬁnement
-
-
-
SL
-
-
SL
SL
ML 100% no

no
no
no
LM MI
no
no
GM MI
LM MI

tabu search
KL MI
ML 50% LM MI
ML 100% LM MI

6.1. Overview of Algorithms
Algorithms for modularity clustering can be categorized into the following four types:
subdivision, cluster joining, vertex moving, and mathematical programming. Most
recent implementations actually use a combination of these approaches. Table II sum-
marizes how some of these roughly ﬁt into the design space.

Subdivision heuristics successively divide clusters. Some methods naturally produce
any number of clusters, for example by iteratively removing individual edges [Bader
and Madduri 2008; Newman and Girvan 2004]; other algorithms [Djidjev 2008; Duch
and Arenas 2005], in particular many eigenvector-based methods [Newman 2006b;
Richardson et al. 2009; Sun et al. 2009; White and Smyth 2005], essentially split the
graph into a ﬁxed number of clusters, but are extended to arbitrarily cluster counts
through recursive application.

CJ (or agglomeration) heuristics iteratively join clusters. Cluster pairs can be se-
lected based on random walks [Pons and Latapy 2006; Pujol et al. 2006], increase of
modularity [Clauset et al. 2004; Schuetz and Caﬂisch 2008a; Ye et al. 2008], or other
criteria [Danon et al. 2006; Donetti and Mu ˜noz 2005; Wakita and Tsurumi 2007].

VM heuristics move vertices between clusters, with KL style [Newman 2006b; Sun
et al. 2009] and locally greedy moving [Barber and Clark 2009; Blondel et al. 2008;
Liu and Murata 2010; Mei et al. 2009; Schuetz and Caﬂisch 2008a] being the most
prominent examples. Other approaches include tabu search [Arenas et al. 2008; L ¨u and
Huang 2009], extremal optimization [Duch and Arenas 2005], and simulated annealing
[Guimer`a and Amaral 2005; Massen and Doye 2005; Medus et al. 2005; Reichardt and
Bornholdt 2006].

Finally, mathematical programming approaches model modularity maximization as
a linear or quadratic programming problem, which can be solved with existing software
packages [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al. 2007]. For small
graphs, the integer linear programming methods allow the computation of optimal
modularity values.

6.2. Published Modularity Values
Table III compares the best modularity values from various publications with the
results of the heuristic CJ0 ZS+ML50+LM. For small graphs, all classes contain algo-
rithms that produce comparable or better clusterings. For larger graphs, the results of
CJ0 ZS+ML50+LM are better than any published value. This can be attributed to the
Multilevel Reﬁnement, which is not present in previous implementations.

On several small graphs, the label propagation algorithm LPA is able to ﬁnd better
clusterings than CJ0 ZS+ML50+LM. The algorithm basically produces a single coars-
ening level through LM MI and then applies CJx as postprocessing [Liu and Murata

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:19

Table II. Published Algorithms Related to the Design Space

Algorithm
Clauset et al. [2004] (CNM)
Danon et al. [2006]
Wakita and Tsurumi [2007]
Schuetz and Caﬂisch [2008a] (SC)
Pons and Latapy [2006] (PL)
Pujol et al. [2006] (PBD)
Ye et al. [2008] (YHY)
Liu and Murata [2010] (LPA)
Blondel et al. [2008] (BGLL)
L ¨u and Huang [2009] (LH)
Sun et al. [2009] (SDJB)
CJ0 ZS+ML50+LM
LM MI+ML100+LM

Coarsening
CJ0 MI
CJ0 GC
CJ0 WHN/WHE
CJx MI
CJ0 + random walk
prejoining, modiﬁed CJ0
mixed GM MI, CJ0 MI
mixed LM MI, CJx MI
LM MI
subdivision by tabu search SL
spectral bisection + KL MI SL
CJ0 ZS
LM MI

Multilevel Reﬁnement
-
-
-
SL
-
-
SL
SL
ML 100% no

no
no
no
LM MI
no
no
GM MI
LM MI

tabu search
KL MI
ML 50% LM MI
ML 100% LM MI

6.1. Overview of Algorithms
Algorithms for modularity clustering can be categorized into the following four types:
subdivision, cluster joining, vertex moving, and mathematical programming. Most
recent implementations actually use a combination of these approaches. Table II sum-
marizes how some of these roughly ﬁt into the design space.

Subdivision heuristics successively divide clusters. Some methods naturally produce
any number of clusters, for example by iteratively removing individual edges [Bader
and Madduri 2008; Newman and Girvan 2004]; other algorithms [Djidjev 2008; Duch
and Arenas 2005], in particular many eigenvector-based methods [Newman 2006b;
Richardson et al. 2009; Sun et al. 2009; White and Smyth 2005], essentially split the
graph into a ﬁxed number of clusters, but are extended to arbitrarily cluster counts
through recursive application.

CJ (or agglomeration) heuristics iteratively join clusters. Cluster pairs can be se-
lected based on random walks [Pons and Latapy 2006; Pujol et al. 2006], increase of
modularity [Clauset et al. 2004; Schuetz and Caﬂisch 2008a; Ye et al. 2008], or other
criteria [Danon et al. 2006; Donetti and Mu ˜noz 2005; Wakita and Tsurumi 2007].

VM heuristics move vertices between clusters, with KL style [Newman 2006b; Sun
et al. 2009] and locally greedy moving [Barber and Clark 2009; Blondel et al. 2008;
Liu and Murata 2010; Mei et al. 2009; Schuetz and Caﬂisch 2008a] being the most
prominent examples. Other approaches include tabu search [Arenas et al. 2008; L ¨u and
Huang 2009], extremal optimization [Duch and Arenas 2005], and simulated annealing
[Guimer`a and Amaral 2005; Massen and Doye 2005; Medus et al. 2005; Reichardt and
Bornholdt 2006].

Finally, mathematical programming approaches model modularity maximization as
a linear or quadratic programming problem, which can be solved with existing software
packages [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al. 2007]. For small
graphs, the integer linear programming methods allow the computation of optimal
modularity values.

6.2. Published Modularity Values
Table III compares the best modularity values from various publications with the
results of the heuristic CJ0 ZS+ML50+LM. For small graphs, all classes contain algo-
rithms that produce comparable or better clusterings. For larger graphs, the results of
CJ0 ZS+ML50+LM are better than any published value. This can be attributed to the
Multilevel Reﬁnement, which is not present in previous implementations.

On several small graphs, the label propagation algorithm LPA is able to ﬁnd better
clusterings than CJ0 ZS+ML50+LM. The algorithm basically produces a single coars-
ening level through LM MI and then applies CJx as postprocessing [Liu and Murata

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:20

R. Rotta and A. Noack

Table III. Best Published Modularity Values for Four Algorithm Classes and the CJ0 ZS+ML50+LM
Heuristic in the Last Column
Graph
Karate
Dolphins
PolBooks
AFootball
Jazz
Celeg metab
Email
Erdos02
PGP main
CMat03 main
ND edu

Moving
(DA) .4188
(LPA) .5285
(LPA) .527
(LPA) .605
(DA) .4452
(LPA) .452
(LPA) .582
(LH) .6902
(LPA) .884
(LPA) .755
(BGLL) .935

Subdivision
(N) .419
(N) .4893
(GN) .509
(WS) .602
(SDJB) .445
(SDJB) .452
(SDJB) .580
(N) .5969
(SDJB) .867
(SDJB) .737

(YHY) .4198
(PL) .5171
(SC) .5269
(YHY) .605
(SC) .445
(SC) .450
(SC) .575
(PBD) .6817
(SC) .878
(YHY) .761
(SC) .939

Size
34
62
105
115
198
453
1,133
6,927
11k
28k
325k

Joining

Math Prog
(AK) .4197
(XTP) .5285
(AK) .5272
(AK) .6046
(AK) .445
(AK) .450
(AK) .579

CJ0+LM
.41978
.52760
.52560
.60155
.44467
.44603
.57837
.71597
.88403
.81603
.95102

Where possible, missing values were substituted with results from published implementations,
which are shown in italics. For references to the literature see Section 6.2.

2010]. The reported high modularity values are mostly due to selecting the best values
out of 100 randomized runs. As can be seen, randomization is a useful tool to ﬁnd good
clusterings more reliably, though at a high computational expense.

Mathematical programming approaches consistently ﬁnd better clusterings than
CJ0 ZS+ML50+LM, though by a small margin; however, they are computationally
much more demanding and do not scale to large graphs [Agarwal and Kempe 2008].

References. The graphs are: Karate, the famous network of karate club members
[Zachary 1977]; Dolphins, a social network of bottlenose dolphins [Lusseau et al. 2003];
PolBooks, a co-purchasing graph of books about U.S. politics [Krebs 2008]; AFootball,
a network of college football matches [Girvan and Newman 2002]; Jazz, a collaboration
network of 198 Jazz musicians [Gleiser and Danon 2003]; Celeg metab, a metabolic
network of the worm C.elegans [Duch and Arenas 2005]; Email, a network of e-mail
interchanges between university members [Guimer`a et al. 2003]; Erdos02, the famous
co-authorship network around Paul Erds [Grossman 2007]; PGP main, a PGP key-
signing network [Bogu ˜n´a et al. 2004]; CMat03 main, a collaboration network of scientists
in condensed matter physics [Newman 2001]; and ND edu, a Web page network from
the nd.edu domain [Albert et al. 1999].

The subdivision methods are the betweenness-based edge removal algorithm GN by
Newman and Girvan [2004]; Newman’s [2006b] recursive spectral bisection N; SDJB, a
spectral clustering combined with KL Reﬁnement [Sun et al. 2009]; and the spectral
clustering WS by White and Smyth [2005].

The CJ algorithms comprise the walktrap algorithm PL of Pons and Latapy [2006]; the
multistep heuristic SC of Schuetz and Caﬂisch [2008a]; and two algorithms that combine
joining with some preprocessing by VM: PBD [Pujol et al. 2006] and YHY [Ye et al. 2008].
The VM algorithms are BGLL, the hierarchical LM algorithm of Blondel et al. [2008];
GC, an extremal optimization method combined with recursive subdivision [Duch and
Arenas 2005]; LPA, a label propagation algorithm [Liu and Murata 2010]; and LH, an
iterated tabu search [L ¨u and Huang 2009].

The results for mathematical programming algorithms were collected from the re-
laxed linear and quadratic programming AK of Agarwal and Kempe [2008] and the
mixed integer programming XTP of Xu et al. [2007].

6.3. Published Implementations on Graphs with Rounded Weights
Implementations that support graphs with integer edge weights are available for
(cid:7)
the multistep joining algorithm of Schuetz and Caﬂisch [2008a] (with parameter
l = 0.25
f (V , V )/2, as recommended by Schuetz and Caﬂisch [2008b]), the algorithm
of Pons and Latapy [2006] based on short random walks (here of length 4, the default

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:19

Table II. Published Algorithms Related to the Design Space

Algorithm
Clauset et al. [2004] (CNM)
Danon et al. [2006]
Wakita and Tsurumi [2007]
Schuetz and Caﬂisch [2008a] (SC)
Pons and Latapy [2006] (PL)
Pujol et al. [2006] (PBD)
Ye et al. [2008] (YHY)
Liu and Murata [2010] (LPA)
Blondel et al. [2008] (BGLL)
L ¨u and Huang [2009] (LH)
Sun et al. [2009] (SDJB)
CJ0 ZS+ML50+LM
LM MI+ML100+LM

Coarsening
CJ0 MI
CJ0 GC
CJ0 WHN/WHE
CJx MI
CJ0 + random walk
prejoining, modiﬁed CJ0
mixed GM MI, CJ0 MI
mixed LM MI, CJx MI
LM MI
subdivision by tabu search SL
spectral bisection + KL MI SL
CJ0 ZS
LM MI

Multilevel Reﬁnement
-
-
-
SL
-
-
SL
SL
ML 100% no

no
no
no
LM MI
no
no
GM MI
LM MI

tabu search
KL MI
ML 50% LM MI
ML 100% LM MI

6.1. Overview of Algorithms
Algorithms for modularity clustering can be categorized into the following four types:
subdivision, cluster joining, vertex moving, and mathematical programming. Most
recent implementations actually use a combination of these approaches. Table II sum-
marizes how some of these roughly ﬁt into the design space.

Subdivision heuristics successively divide clusters. Some methods naturally produce
any number of clusters, for example by iteratively removing individual edges [Bader
and Madduri 2008; Newman and Girvan 2004]; other algorithms [Djidjev 2008; Duch
and Arenas 2005], in particular many eigenvector-based methods [Newman 2006b;
Richardson et al. 2009; Sun et al. 2009; White and Smyth 2005], essentially split the
graph into a ﬁxed number of clusters, but are extended to arbitrarily cluster counts
through recursive application.

CJ (or agglomeration) heuristics iteratively join clusters. Cluster pairs can be se-
lected based on random walks [Pons and Latapy 2006; Pujol et al. 2006], increase of
modularity [Clauset et al. 2004; Schuetz and Caﬂisch 2008a; Ye et al. 2008], or other
criteria [Danon et al. 2006; Donetti and Mu ˜noz 2005; Wakita and Tsurumi 2007].

VM heuristics move vertices between clusters, with KL style [Newman 2006b; Sun
et al. 2009] and locally greedy moving [Barber and Clark 2009; Blondel et al. 2008;
Liu and Murata 2010; Mei et al. 2009; Schuetz and Caﬂisch 2008a] being the most
prominent examples. Other approaches include tabu search [Arenas et al. 2008; L ¨u and
Huang 2009], extremal optimization [Duch and Arenas 2005], and simulated annealing
[Guimer`a and Amaral 2005; Massen and Doye 2005; Medus et al. 2005; Reichardt and
Bornholdt 2006].

Finally, mathematical programming approaches model modularity maximization as
a linear or quadratic programming problem, which can be solved with existing software
packages [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al. 2007]. For small
graphs, the integer linear programming methods allow the computation of optimal
modularity values.

6.2. Published Modularity Values
Table III compares the best modularity values from various publications with the
results of the heuristic CJ0 ZS+ML50+LM. For small graphs, all classes contain algo-
rithms that produce comparable or better clusterings. For larger graphs, the results of
CJ0 ZS+ML50+LM are better than any published value. This can be attributed to the
Multilevel Reﬁnement, which is not present in previous implementations.

On several small graphs, the label propagation algorithm LPA is able to ﬁnd better
clusterings than CJ0 ZS+ML50+LM. The algorithm basically produces a single coars-
ening level through LM MI and then applies CJx as postprocessing [Liu and Murata

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:20

R. Rotta and A. Noack

Table III. Best Published Modularity Values for Four Algorithm Classes and the CJ0 ZS+ML50+LM
Heuristic in the Last Column
Graph
Karate
Dolphins
PolBooks
AFootball
Jazz
Celeg metab
Email
Erdos02
PGP main
CMat03 main
ND edu

Moving
(DA) .4188
(LPA) .5285
(LPA) .527
(LPA) .605
(DA) .4452
(LPA) .452
(LPA) .582
(LH) .6902
(LPA) .884
(LPA) .755
(BGLL) .935

Subdivision
(N) .419
(N) .4893
(GN) .509
(WS) .602
(SDJB) .445
(SDJB) .452
(SDJB) .580
(N) .5969
(SDJB) .867
(SDJB) .737

(YHY) .4198
(PL) .5171
(SC) .5269
(YHY) .605
(SC) .445
(SC) .450
(SC) .575
(PBD) .6817
(SC) .878
(YHY) .761
(SC) .939

Size
34
62
105
115
198
453
1,133
6,927
11k
28k
325k

Joining

Math Prog
(AK) .4197
(XTP) .5285
(AK) .5272
(AK) .6046
(AK) .445
(AK) .450
(AK) .579

CJ0+LM
.41978
.52760
.52560
.60155
.44467
.44603
.57837
.71597
.88403
.81603
.95102

Where possible, missing values were substituted with results from published implementations,
which are shown in italics. For references to the literature see Section 6.2.

2010]. The reported high modularity values are mostly due to selecting the best values
out of 100 randomized runs. As can be seen, randomization is a useful tool to ﬁnd good
clusterings more reliably, though at a high computational expense.

Mathematical programming approaches consistently ﬁnd better clusterings than
CJ0 ZS+ML50+LM, though by a small margin; however, they are computationally
much more demanding and do not scale to large graphs [Agarwal and Kempe 2008].

References. The graphs are: Karate, the famous network of karate club members
[Zachary 1977]; Dolphins, a social network of bottlenose dolphins [Lusseau et al. 2003];
PolBooks, a co-purchasing graph of books about U.S. politics [Krebs 2008]; AFootball,
a network of college football matches [Girvan and Newman 2002]; Jazz, a collaboration
network of 198 Jazz musicians [Gleiser and Danon 2003]; Celeg metab, a metabolic
network of the worm C.elegans [Duch and Arenas 2005]; Email, a network of e-mail
interchanges between university members [Guimer`a et al. 2003]; Erdos02, the famous
co-authorship network around Paul Erds [Grossman 2007]; PGP main, a PGP key-
signing network [Bogu ˜n´a et al. 2004]; CMat03 main, a collaboration network of scientists
in condensed matter physics [Newman 2001]; and ND edu, a Web page network from
the nd.edu domain [Albert et al. 1999].

The subdivision methods are the betweenness-based edge removal algorithm GN by
Newman and Girvan [2004]; Newman’s [2006b] recursive spectral bisection N; SDJB, a
spectral clustering combined with KL Reﬁnement [Sun et al. 2009]; and the spectral
clustering WS by White and Smyth [2005].

The CJ algorithms comprise the walktrap algorithm PL of Pons and Latapy [2006]; the
multistep heuristic SC of Schuetz and Caﬂisch [2008a]; and two algorithms that combine
joining with some preprocessing by VM: PBD [Pujol et al. 2006] and YHY [Ye et al. 2008].
The VM algorithms are BGLL, the hierarchical LM algorithm of Blondel et al. [2008];
GC, an extremal optimization method combined with recursive subdivision [Duch and
Arenas 2005]; LPA, a label propagation algorithm [Liu and Murata 2010]; and LH, an
iterated tabu search [L ¨u and Huang 2009].

The results for mathematical programming algorithms were collected from the re-
laxed linear and quadratic programming AK of Agarwal and Kempe [2008] and the
mixed integer programming XTP of Xu et al. [2007].

6.3. Published Implementations on Graphs with Rounded Weights
Implementations that support graphs with integer edge weights are available for
(cid:7)
the multistep joining algorithm of Schuetz and Caﬂisch [2008a] (with parameter
l = 0.25
f (V , V )/2, as recommended by Schuetz and Caﬂisch [2008b]), the algorithm
of Pons and Latapy [2006] based on short random walks (here of length 4, the default

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:21

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

7
5

.

0

5
5

.

0

3
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

8

7

6

5

4

3

2

1

with refinement
basic algorithm

with refinement
basic algorithm

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

Fig. 7. Mean modularity (left) and runtime ratio (right) from published implementations on graphs with
rounded weights. The reﬁnement bars refer to the standard reﬁnement of the respective implementations.
The modularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the
Louvain method (the fastest in this experiment).
value), and the hierarchical LM algorithm of Blondel et al. [2008], known as Louvain
method.

The graphs for this experiment were derived from the 62 benchmark graphs by
removing self-edges, extracting the largest connected component, and rounding the
edge weights to the next nonzero integer if necessary. In order to minimize rounding
errors, the weights were multiplied with (222 − 1)/ deg(V ) before rounding.

Figure 7 shows the obtained results. Coarsening by CJ0 ZS (with 50% reduction fac-
tor) and by LM MI (with 100% reduction factor) are signiﬁcantly more effective than
the other algorithms when combined with Multilevel Reﬁnement. Only the Louvain im-
plementation by Blondel et al. [2008] is slightly faster, but produces worse clusterings.
Coarsening by CJ0 MI (with 100% reduction factor) is included in the ﬁgure be-
cause it is equivalent to the widely used fast greedy joining of Clauset et al. [2004].
LM MI without reﬁnement is conceptually equivalent to the Louvain algorithm; accord-
ingly, both are similarly effective and efﬁcient. CJx MI corresponds to the algorithm of
Schuetz and Caﬂisch. Although the two implementations use a different formula for the
parameter l, both produce equally good clusterings. Thus, differences in the implemen-
tations or in the formulas for the parameter l do not affect the conclusions about CJx, in
particular its inferiority to the simpler and parameter-free CJ0 with the ZS prioritizer.
The absolute runtimes and their dependency on the graph size are shown in Figure 8.
Both the Louvain implementation and CJ0 ZS+ML50+LM scale well with graph size
and generally are very efﬁcient. The longest runtime of only 6.6 seconds was measured
on the graph EatRS, which has 23k vertices and 305k edges.

6.4. Published Implementations on Unweighted Graphs
Implementations for unweighted graphs are available for further clustering al-
gorithms: the fast greedy joining of Clauset et al. [2004], the spectral recursive
bisection of Newman [2006b], the spinglass simulated annealing of Reichardt and
Bornholdt [2006], and the fast joining algorithm of Wakita and Tsurumi [2007].

Because these implementations cannot process graphs with weighted edges, only 23
unweighted graphs of the benchmark collection were used. In some of these graphs,
negligible differences in edge weights and small amounts of self-edges were removed.
Figure 9 shows the results, compared to our implementation CJ0 ZS with optional
reﬁnement ML50+LM. Only Reichardt and Bornholdt’s implementation produces clus-
terings of similarly high modularity, but it is much slower, and again, only the Louvain
implementation is slightly faster, but produces worse clusterings. As can be seen in
Figure 10, CJ0 ZS+ML50+LM is consistently among the fastest implementations, in-
dependently of the graph size.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:19

Table II. Published Algorithms Related to the Design Space

Algorithm
Clauset et al. [2004] (CNM)
Danon et al. [2006]
Wakita and Tsurumi [2007]
Schuetz and Caﬂisch [2008a] (SC)
Pons and Latapy [2006] (PL)
Pujol et al. [2006] (PBD)
Ye et al. [2008] (YHY)
Liu and Murata [2010] (LPA)
Blondel et al. [2008] (BGLL)
L ¨u and Huang [2009] (LH)
Sun et al. [2009] (SDJB)
CJ0 ZS+ML50+LM
LM MI+ML100+LM

Coarsening
CJ0 MI
CJ0 GC
CJ0 WHN/WHE
CJx MI
CJ0 + random walk
prejoining, modiﬁed CJ0
mixed GM MI, CJ0 MI
mixed LM MI, CJx MI
LM MI
subdivision by tabu search SL
spectral bisection + KL MI SL
CJ0 ZS
LM MI

Multilevel Reﬁnement
-
-
-
SL
-
-
SL
SL
ML 100% no

no
no
no
LM MI
no
no
GM MI
LM MI

tabu search
KL MI
ML 50% LM MI
ML 100% LM MI

6.1. Overview of Algorithms
Algorithms for modularity clustering can be categorized into the following four types:
subdivision, cluster joining, vertex moving, and mathematical programming. Most
recent implementations actually use a combination of these approaches. Table II sum-
marizes how some of these roughly ﬁt into the design space.

Subdivision heuristics successively divide clusters. Some methods naturally produce
any number of clusters, for example by iteratively removing individual edges [Bader
and Madduri 2008; Newman and Girvan 2004]; other algorithms [Djidjev 2008; Duch
and Arenas 2005], in particular many eigenvector-based methods [Newman 2006b;
Richardson et al. 2009; Sun et al. 2009; White and Smyth 2005], essentially split the
graph into a ﬁxed number of clusters, but are extended to arbitrarily cluster counts
through recursive application.

CJ (or agglomeration) heuristics iteratively join clusters. Cluster pairs can be se-
lected based on random walks [Pons and Latapy 2006; Pujol et al. 2006], increase of
modularity [Clauset et al. 2004; Schuetz and Caﬂisch 2008a; Ye et al. 2008], or other
criteria [Danon et al. 2006; Donetti and Mu ˜noz 2005; Wakita and Tsurumi 2007].

VM heuristics move vertices between clusters, with KL style [Newman 2006b; Sun
et al. 2009] and locally greedy moving [Barber and Clark 2009; Blondel et al. 2008;
Liu and Murata 2010; Mei et al. 2009; Schuetz and Caﬂisch 2008a] being the most
prominent examples. Other approaches include tabu search [Arenas et al. 2008; L ¨u and
Huang 2009], extremal optimization [Duch and Arenas 2005], and simulated annealing
[Guimer`a and Amaral 2005; Massen and Doye 2005; Medus et al. 2005; Reichardt and
Bornholdt 2006].

Finally, mathematical programming approaches model modularity maximization as
a linear or quadratic programming problem, which can be solved with existing software
packages [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al. 2007]. For small
graphs, the integer linear programming methods allow the computation of optimal
modularity values.

6.2. Published Modularity Values
Table III compares the best modularity values from various publications with the
results of the heuristic CJ0 ZS+ML50+LM. For small graphs, all classes contain algo-
rithms that produce comparable or better clusterings. For larger graphs, the results of
CJ0 ZS+ML50+LM are better than any published value. This can be attributed to the
Multilevel Reﬁnement, which is not present in previous implementations.

On several small graphs, the label propagation algorithm LPA is able to ﬁnd better
clusterings than CJ0 ZS+ML50+LM. The algorithm basically produces a single coars-
ening level through LM MI and then applies CJx as postprocessing [Liu and Murata

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:20

R. Rotta and A. Noack

Table III. Best Published Modularity Values for Four Algorithm Classes and the CJ0 ZS+ML50+LM
Heuristic in the Last Column
Graph
Karate
Dolphins
PolBooks
AFootball
Jazz
Celeg metab
Email
Erdos02
PGP main
CMat03 main
ND edu

Moving
(DA) .4188
(LPA) .5285
(LPA) .527
(LPA) .605
(DA) .4452
(LPA) .452
(LPA) .582
(LH) .6902
(LPA) .884
(LPA) .755
(BGLL) .935

Subdivision
(N) .419
(N) .4893
(GN) .509
(WS) .602
(SDJB) .445
(SDJB) .452
(SDJB) .580
(N) .5969
(SDJB) .867
(SDJB) .737

(YHY) .4198
(PL) .5171
(SC) .5269
(YHY) .605
(SC) .445
(SC) .450
(SC) .575
(PBD) .6817
(SC) .878
(YHY) .761
(SC) .939

Size
34
62
105
115
198
453
1,133
6,927
11k
28k
325k

Joining

Math Prog
(AK) .4197
(XTP) .5285
(AK) .5272
(AK) .6046
(AK) .445
(AK) .450
(AK) .579

CJ0+LM
.41978
.52760
.52560
.60155
.44467
.44603
.57837
.71597
.88403
.81603
.95102

Where possible, missing values were substituted with results from published implementations,
which are shown in italics. For references to the literature see Section 6.2.

2010]. The reported high modularity values are mostly due to selecting the best values
out of 100 randomized runs. As can be seen, randomization is a useful tool to ﬁnd good
clusterings more reliably, though at a high computational expense.

Mathematical programming approaches consistently ﬁnd better clusterings than
CJ0 ZS+ML50+LM, though by a small margin; however, they are computationally
much more demanding and do not scale to large graphs [Agarwal and Kempe 2008].

References. The graphs are: Karate, the famous network of karate club members
[Zachary 1977]; Dolphins, a social network of bottlenose dolphins [Lusseau et al. 2003];
PolBooks, a co-purchasing graph of books about U.S. politics [Krebs 2008]; AFootball,
a network of college football matches [Girvan and Newman 2002]; Jazz, a collaboration
network of 198 Jazz musicians [Gleiser and Danon 2003]; Celeg metab, a metabolic
network of the worm C.elegans [Duch and Arenas 2005]; Email, a network of e-mail
interchanges between university members [Guimer`a et al. 2003]; Erdos02, the famous
co-authorship network around Paul Erds [Grossman 2007]; PGP main, a PGP key-
signing network [Bogu ˜n´a et al. 2004]; CMat03 main, a collaboration network of scientists
in condensed matter physics [Newman 2001]; and ND edu, a Web page network from
the nd.edu domain [Albert et al. 1999].

The subdivision methods are the betweenness-based edge removal algorithm GN by
Newman and Girvan [2004]; Newman’s [2006b] recursive spectral bisection N; SDJB, a
spectral clustering combined with KL Reﬁnement [Sun et al. 2009]; and the spectral
clustering WS by White and Smyth [2005].

The CJ algorithms comprise the walktrap algorithm PL of Pons and Latapy [2006]; the
multistep heuristic SC of Schuetz and Caﬂisch [2008a]; and two algorithms that combine
joining with some preprocessing by VM: PBD [Pujol et al. 2006] and YHY [Ye et al. 2008].
The VM algorithms are BGLL, the hierarchical LM algorithm of Blondel et al. [2008];
GC, an extremal optimization method combined with recursive subdivision [Duch and
Arenas 2005]; LPA, a label propagation algorithm [Liu and Murata 2010]; and LH, an
iterated tabu search [L ¨u and Huang 2009].

The results for mathematical programming algorithms were collected from the re-
laxed linear and quadratic programming AK of Agarwal and Kempe [2008] and the
mixed integer programming XTP of Xu et al. [2007].

6.3. Published Implementations on Graphs with Rounded Weights
Implementations that support graphs with integer edge weights are available for
(cid:7)
the multistep joining algorithm of Schuetz and Caﬂisch [2008a] (with parameter
l = 0.25
f (V , V )/2, as recommended by Schuetz and Caﬂisch [2008b]), the algorithm
of Pons and Latapy [2006] based on short random walks (here of length 4, the default

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:21

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

7
5

.

0

5
5

.

0

3
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

8

7

6

5

4

3

2

1

with refinement
basic algorithm

with refinement
basic algorithm

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

Fig. 7. Mean modularity (left) and runtime ratio (right) from published implementations on graphs with
rounded weights. The reﬁnement bars refer to the standard reﬁnement of the respective implementations.
The modularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the
Louvain method (the fastest in this experiment).
value), and the hierarchical LM algorithm of Blondel et al. [2008], known as Louvain
method.

The graphs for this experiment were derived from the 62 benchmark graphs by
removing self-edges, extracting the largest connected component, and rounding the
edge weights to the next nonzero integer if necessary. In order to minimize rounding
errors, the weights were multiplied with (222 − 1)/ deg(V ) before rounding.

Figure 7 shows the obtained results. Coarsening by CJ0 ZS (with 50% reduction fac-
tor) and by LM MI (with 100% reduction factor) are signiﬁcantly more effective than
the other algorithms when combined with Multilevel Reﬁnement. Only the Louvain im-
plementation by Blondel et al. [2008] is slightly faster, but produces worse clusterings.
Coarsening by CJ0 MI (with 100% reduction factor) is included in the ﬁgure be-
cause it is equivalent to the widely used fast greedy joining of Clauset et al. [2004].
LM MI without reﬁnement is conceptually equivalent to the Louvain algorithm; accord-
ingly, both are similarly effective and efﬁcient. CJx MI corresponds to the algorithm of
Schuetz and Caﬂisch. Although the two implementations use a different formula for the
parameter l, both produce equally good clusterings. Thus, differences in the implemen-
tations or in the formulas for the parameter l do not affect the conclusions about CJx, in
particular its inferiority to the simpler and parameter-free CJ0 with the ZS prioritizer.
The absolute runtimes and their dependency on the graph size are shown in Figure 8.
Both the Louvain implementation and CJ0 ZS+ML50+LM scale well with graph size
and generally are very efﬁcient. The longest runtime of only 6.6 seconds was measured
on the graph EatRS, which has 23k vertices and 305k edges.

6.4. Published Implementations on Unweighted Graphs
Implementations for unweighted graphs are available for further clustering al-
gorithms: the fast greedy joining of Clauset et al. [2004], the spectral recursive
bisection of Newman [2006b], the spinglass simulated annealing of Reichardt and
Bornholdt [2006], and the fast joining algorithm of Wakita and Tsurumi [2007].

Because these implementations cannot process graphs with weighted edges, only 23
unweighted graphs of the benchmark collection were used. In some of these graphs,
negligible differences in edge weights and small amounts of self-edges were removed.
Figure 9 shows the results, compared to our implementation CJ0 ZS with optional
reﬁnement ML50+LM. Only Reichardt and Bornholdt’s implementation produces clus-
terings of similarly high modularity, but it is much slower, and again, only the Louvain
implementation is slightly faster, but produces worse clusterings. As can be seen in
Figure 10, CJ0 ZS+ML50+LM is consistently among the fastest implementations, in-
dependently of the graph size.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:22

l

e
a
c
s
−
g
o

l
 
,
]
s
[
 

e
m

i
t

n
u
r

2
0
+
e
1

1
0
+
e
1

0
0
+
e
1

1
0
−
e
1

2
0
−
e
1

● Pons

Schuetz+refinement
CJ0_ZS+ML50+LM
Louvain

6.6 seconds

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●●

●●●

●

●
●●●

R. Rotta and A. Noack

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●●

● ●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 8. Runtime by graph size on graphs with rounded weights. The dashed line marks the longest runtime
of the Multilevel algorithms.

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

8
5

.

0

4
5

.

0

0
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

4
1

0
1

8

6

4

2

with refinement
basic algorithm

with refinement
basic algorithm

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

Fig. 9. Mean modularity (left) and runtime ratio (right) from published implementations on unweighted
graphs. The reﬁnement bars refer to the standard reﬁnement of the respective implementations. The mod-
ularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the Louvain
method (the fastest in this experiment). Newman’s spectral clustering is on average 150 times slower than
the Louvain method and Reichardt’s spinglass algorithm is around 2,400 times slower.

7. SUMMARY
The three primary contributions are (i) new heuristics for modularity clustering with
improved effectiveness and efﬁciency, (ii) a coherent organization of existing and new
heuristics and their combinations, and (iii) a systematic experimental comparison of
the heuristics.

Concerning the algorithmic contributions, experiments have shown that the new
criterion ZS for choosing joined clusters slightly outperforms the best existing criteria,
and clearly outperforms the most widely used criterion MI. Moreover, the new (for
modularity clustering) Multilevel Reﬁnement has turned out to be signiﬁcantly more
effective than the conventional Single-Level Reﬁnement and no reﬁnement.

Concerning organization and uniﬁcation, several existing and new heuristics and a
vast number of combinations are comprised in a design space with ﬁve dimensions: the
coarsening algorithm with its prioritizer (including Vertex Moving as well as Single-
Step and Multistep Cluster Joining), the reduction factor for controlling the number of
coarsening levels, and the reﬁnement algorithm with its prioritizer (including Single-
Level and Multilevel Reﬁnement by Vertex Moving).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:19

Table II. Published Algorithms Related to the Design Space

Algorithm
Clauset et al. [2004] (CNM)
Danon et al. [2006]
Wakita and Tsurumi [2007]
Schuetz and Caﬂisch [2008a] (SC)
Pons and Latapy [2006] (PL)
Pujol et al. [2006] (PBD)
Ye et al. [2008] (YHY)
Liu and Murata [2010] (LPA)
Blondel et al. [2008] (BGLL)
L ¨u and Huang [2009] (LH)
Sun et al. [2009] (SDJB)
CJ0 ZS+ML50+LM
LM MI+ML100+LM

Coarsening
CJ0 MI
CJ0 GC
CJ0 WHN/WHE
CJx MI
CJ0 + random walk
prejoining, modiﬁed CJ0
mixed GM MI, CJ0 MI
mixed LM MI, CJx MI
LM MI
subdivision by tabu search SL
spectral bisection + KL MI SL
CJ0 ZS
LM MI

Multilevel Reﬁnement
-
-
-
SL
-
-
SL
SL
ML 100% no

no
no
no
LM MI
no
no
GM MI
LM MI

tabu search
KL MI
ML 50% LM MI
ML 100% LM MI

6.1. Overview of Algorithms
Algorithms for modularity clustering can be categorized into the following four types:
subdivision, cluster joining, vertex moving, and mathematical programming. Most
recent implementations actually use a combination of these approaches. Table II sum-
marizes how some of these roughly ﬁt into the design space.

Subdivision heuristics successively divide clusters. Some methods naturally produce
any number of clusters, for example by iteratively removing individual edges [Bader
and Madduri 2008; Newman and Girvan 2004]; other algorithms [Djidjev 2008; Duch
and Arenas 2005], in particular many eigenvector-based methods [Newman 2006b;
Richardson et al. 2009; Sun et al. 2009; White and Smyth 2005], essentially split the
graph into a ﬁxed number of clusters, but are extended to arbitrarily cluster counts
through recursive application.

CJ (or agglomeration) heuristics iteratively join clusters. Cluster pairs can be se-
lected based on random walks [Pons and Latapy 2006; Pujol et al. 2006], increase of
modularity [Clauset et al. 2004; Schuetz and Caﬂisch 2008a; Ye et al. 2008], or other
criteria [Danon et al. 2006; Donetti and Mu ˜noz 2005; Wakita and Tsurumi 2007].

VM heuristics move vertices between clusters, with KL style [Newman 2006b; Sun
et al. 2009] and locally greedy moving [Barber and Clark 2009; Blondel et al. 2008;
Liu and Murata 2010; Mei et al. 2009; Schuetz and Caﬂisch 2008a] being the most
prominent examples. Other approaches include tabu search [Arenas et al. 2008; L ¨u and
Huang 2009], extremal optimization [Duch and Arenas 2005], and simulated annealing
[Guimer`a and Amaral 2005; Massen and Doye 2005; Medus et al. 2005; Reichardt and
Bornholdt 2006].

Finally, mathematical programming approaches model modularity maximization as
a linear or quadratic programming problem, which can be solved with existing software
packages [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al. 2007]. For small
graphs, the integer linear programming methods allow the computation of optimal
modularity values.

6.2. Published Modularity Values
Table III compares the best modularity values from various publications with the
results of the heuristic CJ0 ZS+ML50+LM. For small graphs, all classes contain algo-
rithms that produce comparable or better clusterings. For larger graphs, the results of
CJ0 ZS+ML50+LM are better than any published value. This can be attributed to the
Multilevel Reﬁnement, which is not present in previous implementations.

On several small graphs, the label propagation algorithm LPA is able to ﬁnd better
clusterings than CJ0 ZS+ML50+LM. The algorithm basically produces a single coars-
ening level through LM MI and then applies CJx as postprocessing [Liu and Murata

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:20

R. Rotta and A. Noack

Table III. Best Published Modularity Values for Four Algorithm Classes and the CJ0 ZS+ML50+LM
Heuristic in the Last Column
Graph
Karate
Dolphins
PolBooks
AFootball
Jazz
Celeg metab
Email
Erdos02
PGP main
CMat03 main
ND edu

Moving
(DA) .4188
(LPA) .5285
(LPA) .527
(LPA) .605
(DA) .4452
(LPA) .452
(LPA) .582
(LH) .6902
(LPA) .884
(LPA) .755
(BGLL) .935

Subdivision
(N) .419
(N) .4893
(GN) .509
(WS) .602
(SDJB) .445
(SDJB) .452
(SDJB) .580
(N) .5969
(SDJB) .867
(SDJB) .737

(YHY) .4198
(PL) .5171
(SC) .5269
(YHY) .605
(SC) .445
(SC) .450
(SC) .575
(PBD) .6817
(SC) .878
(YHY) .761
(SC) .939

Size
34
62
105
115
198
453
1,133
6,927
11k
28k
325k

Joining

Math Prog
(AK) .4197
(XTP) .5285
(AK) .5272
(AK) .6046
(AK) .445
(AK) .450
(AK) .579

CJ0+LM
.41978
.52760
.52560
.60155
.44467
.44603
.57837
.71597
.88403
.81603
.95102

Where possible, missing values were substituted with results from published implementations,
which are shown in italics. For references to the literature see Section 6.2.

2010]. The reported high modularity values are mostly due to selecting the best values
out of 100 randomized runs. As can be seen, randomization is a useful tool to ﬁnd good
clusterings more reliably, though at a high computational expense.

Mathematical programming approaches consistently ﬁnd better clusterings than
CJ0 ZS+ML50+LM, though by a small margin; however, they are computationally
much more demanding and do not scale to large graphs [Agarwal and Kempe 2008].

References. The graphs are: Karate, the famous network of karate club members
[Zachary 1977]; Dolphins, a social network of bottlenose dolphins [Lusseau et al. 2003];
PolBooks, a co-purchasing graph of books about U.S. politics [Krebs 2008]; AFootball,
a network of college football matches [Girvan and Newman 2002]; Jazz, a collaboration
network of 198 Jazz musicians [Gleiser and Danon 2003]; Celeg metab, a metabolic
network of the worm C.elegans [Duch and Arenas 2005]; Email, a network of e-mail
interchanges between university members [Guimer`a et al. 2003]; Erdos02, the famous
co-authorship network around Paul Erds [Grossman 2007]; PGP main, a PGP key-
signing network [Bogu ˜n´a et al. 2004]; CMat03 main, a collaboration network of scientists
in condensed matter physics [Newman 2001]; and ND edu, a Web page network from
the nd.edu domain [Albert et al. 1999].

The subdivision methods are the betweenness-based edge removal algorithm GN by
Newman and Girvan [2004]; Newman’s [2006b] recursive spectral bisection N; SDJB, a
spectral clustering combined with KL Reﬁnement [Sun et al. 2009]; and the spectral
clustering WS by White and Smyth [2005].

The CJ algorithms comprise the walktrap algorithm PL of Pons and Latapy [2006]; the
multistep heuristic SC of Schuetz and Caﬂisch [2008a]; and two algorithms that combine
joining with some preprocessing by VM: PBD [Pujol et al. 2006] and YHY [Ye et al. 2008].
The VM algorithms are BGLL, the hierarchical LM algorithm of Blondel et al. [2008];
GC, an extremal optimization method combined with recursive subdivision [Duch and
Arenas 2005]; LPA, a label propagation algorithm [Liu and Murata 2010]; and LH, an
iterated tabu search [L ¨u and Huang 2009].

The results for mathematical programming algorithms were collected from the re-
laxed linear and quadratic programming AK of Agarwal and Kempe [2008] and the
mixed integer programming XTP of Xu et al. [2007].

6.3. Published Implementations on Graphs with Rounded Weights
Implementations that support graphs with integer edge weights are available for
(cid:7)
the multistep joining algorithm of Schuetz and Caﬂisch [2008a] (with parameter
l = 0.25
f (V , V )/2, as recommended by Schuetz and Caﬂisch [2008b]), the algorithm
of Pons and Latapy [2006] based on short random walks (here of length 4, the default

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:21

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

7
5

.

0

5
5

.

0

3
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

8

7

6

5

4

3

2

1

with refinement
basic algorithm

with refinement
basic algorithm

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

Fig. 7. Mean modularity (left) and runtime ratio (right) from published implementations on graphs with
rounded weights. The reﬁnement bars refer to the standard reﬁnement of the respective implementations.
The modularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the
Louvain method (the fastest in this experiment).
value), and the hierarchical LM algorithm of Blondel et al. [2008], known as Louvain
method.

The graphs for this experiment were derived from the 62 benchmark graphs by
removing self-edges, extracting the largest connected component, and rounding the
edge weights to the next nonzero integer if necessary. In order to minimize rounding
errors, the weights were multiplied with (222 − 1)/ deg(V ) before rounding.

Figure 7 shows the obtained results. Coarsening by CJ0 ZS (with 50% reduction fac-
tor) and by LM MI (with 100% reduction factor) are signiﬁcantly more effective than
the other algorithms when combined with Multilevel Reﬁnement. Only the Louvain im-
plementation by Blondel et al. [2008] is slightly faster, but produces worse clusterings.
Coarsening by CJ0 MI (with 100% reduction factor) is included in the ﬁgure be-
cause it is equivalent to the widely used fast greedy joining of Clauset et al. [2004].
LM MI without reﬁnement is conceptually equivalent to the Louvain algorithm; accord-
ingly, both are similarly effective and efﬁcient. CJx MI corresponds to the algorithm of
Schuetz and Caﬂisch. Although the two implementations use a different formula for the
parameter l, both produce equally good clusterings. Thus, differences in the implemen-
tations or in the formulas for the parameter l do not affect the conclusions about CJx, in
particular its inferiority to the simpler and parameter-free CJ0 with the ZS prioritizer.
The absolute runtimes and their dependency on the graph size are shown in Figure 8.
Both the Louvain implementation and CJ0 ZS+ML50+LM scale well with graph size
and generally are very efﬁcient. The longest runtime of only 6.6 seconds was measured
on the graph EatRS, which has 23k vertices and 305k edges.

6.4. Published Implementations on Unweighted Graphs
Implementations for unweighted graphs are available for further clustering al-
gorithms: the fast greedy joining of Clauset et al. [2004], the spectral recursive
bisection of Newman [2006b], the spinglass simulated annealing of Reichardt and
Bornholdt [2006], and the fast joining algorithm of Wakita and Tsurumi [2007].

Because these implementations cannot process graphs with weighted edges, only 23
unweighted graphs of the benchmark collection were used. In some of these graphs,
negligible differences in edge weights and small amounts of self-edges were removed.
Figure 9 shows the results, compared to our implementation CJ0 ZS with optional
reﬁnement ML50+LM. Only Reichardt and Bornholdt’s implementation produces clus-
terings of similarly high modularity, but it is much slower, and again, only the Louvain
implementation is slightly faster, but produces worse clusterings. As can be seen in
Figure 10, CJ0 ZS+ML50+LM is consistently among the fastest implementations, in-
dependently of the graph size.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:22

l

e
a
c
s
−
g
o

l
 
,
]
s
[
 

e
m

i
t

n
u
r

2
0
+
e
1

1
0
+
e
1

0
0
+
e
1

1
0
−
e
1

2
0
−
e
1

● Pons

Schuetz+refinement
CJ0_ZS+ML50+LM
Louvain

6.6 seconds

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●●

●●●

●

●
●●●

R. Rotta and A. Noack

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●●

● ●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 8. Runtime by graph size on graphs with rounded weights. The dashed line marks the longest runtime
of the Multilevel algorithms.

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

8
5

.

0

4
5

.

0

0
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

4
1

0
1

8

6

4

2

with refinement
basic algorithm

with refinement
basic algorithm

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

Fig. 9. Mean modularity (left) and runtime ratio (right) from published implementations on unweighted
graphs. The reﬁnement bars refer to the standard reﬁnement of the respective implementations. The mod-
ularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the Louvain
method (the fastest in this experiment). Newman’s spectral clustering is on average 150 times slower than
the Louvain method and Reichardt’s spinglass algorithm is around 2,400 times slower.

7. SUMMARY
The three primary contributions are (i) new heuristics for modularity clustering with
improved effectiveness and efﬁciency, (ii) a coherent organization of existing and new
heuristics and their combinations, and (iii) a systematic experimental comparison of
the heuristics.

Concerning the algorithmic contributions, experiments have shown that the new
criterion ZS for choosing joined clusters slightly outperforms the best existing criteria,
and clearly outperforms the most widely used criterion MI. Moreover, the new (for
modularity clustering) Multilevel Reﬁnement has turned out to be signiﬁcantly more
effective than the conventional Single-Level Reﬁnement and no reﬁnement.

Concerning organization and uniﬁcation, several existing and new heuristics and a
vast number of combinations are comprised in a design space with ﬁve dimensions: the
coarsening algorithm with its prioritizer (including Vertex Moving as well as Single-
Step and Multistep Cluster Joining), the reduction factor for controlling the number of
coarsening levels, and the reﬁnement algorithm with its prioritizer (including Single-
Level and Multilevel Reﬁnement by Vertex Moving).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:23

● Reichardt
Newman
Clauset
Schuetz+refinement
CJ0_ZS+ML50+LM
Wakita−HN

●●

●

●●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

0
0
0
0
1

0
0
0
1

l

e
a
c
s
−
g
o

l
 
,

i

n
a
v
u
o
L
o

 

t
 

o

i
t

a
r
 

e
m

i
t

n
u
r

0
0
1

0
1

1

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 10. Runtime ratio by graph size on unweighted graphs. All ratios are relative to the Louvain method
(dashed line).

Table IV. Graph Sources

Source
AArenas
AClauset
ANoack
Cx-Nets
GraphDrawing
JFowler
MNewman
Pajek
UriAlon

Web Address
http://deim.urv.cat/∼aarenas/data/welcome.htm
http://www.santafe.edu/∼aaronc/hierarchy/
http://www-sst.informatik.tu-cottbus.de/∼an/GD/
http://cxnets.googlepages.com/
http://vlado.fmf.uni-lj.si/pub/networks/data/GD/GD.htm
http://jhfowler.ucsd.edu/judicial.htm
http://www-personal.umich.edu/∼mejn/netdata/
http://vlado.fmf.uni-lj.si/pub/networks/data/
http://www.weizmann.ac.il/mcb/UriAlon/

The experimental comparison of achieved modularities and required runtimes
yielded several signiﬁcant results. The two primary coarsening approaches—Cluster
Joining and Vertex Moving—turned out to perform fairly similar with their best param-
eter values and reﬁnement. Moreover, some widely used, complex, or computationally
expensive design alternatives (e.g., Multistep Cluster Joining, join prioritization by
MI, Single-Level Reﬁnement, and Global Vertex Moving)—were outperformed by new,
simpler, or more efﬁcient alternatives. Overall, Single-Step Cluster Joining with ZS
prioritizer combined with Multilevel Local by Local Vertex Moving Reﬁnement is one
of the most effective and efﬁcient algorithms for modularity clustering in the literature.

APPENDIX
Table V lists the graphs used for the experiments. The graphs postﬁxed with “ main”
just contain the largest connected component of the original graph. All graphs from
the subset “UW” were used without edge weights and self-edges for the experiments
in Section 6.4. For each graph the source collection is named in the last column. Web
addresses of these collections are listed in Table IV. For information about the original
authors, please visit the respective Web sites.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:19

Table II. Published Algorithms Related to the Design Space

Algorithm
Clauset et al. [2004] (CNM)
Danon et al. [2006]
Wakita and Tsurumi [2007]
Schuetz and Caﬂisch [2008a] (SC)
Pons and Latapy [2006] (PL)
Pujol et al. [2006] (PBD)
Ye et al. [2008] (YHY)
Liu and Murata [2010] (LPA)
Blondel et al. [2008] (BGLL)
L ¨u and Huang [2009] (LH)
Sun et al. [2009] (SDJB)
CJ0 ZS+ML50+LM
LM MI+ML100+LM

Coarsening
CJ0 MI
CJ0 GC
CJ0 WHN/WHE
CJx MI
CJ0 + random walk
prejoining, modiﬁed CJ0
mixed GM MI, CJ0 MI
mixed LM MI, CJx MI
LM MI
subdivision by tabu search SL
spectral bisection + KL MI SL
CJ0 ZS
LM MI

Multilevel Reﬁnement
-
-
-
SL
-
-
SL
SL
ML 100% no

no
no
no
LM MI
no
no
GM MI
LM MI

tabu search
KL MI
ML 50% LM MI
ML 100% LM MI

6.1. Overview of Algorithms
Algorithms for modularity clustering can be categorized into the following four types:
subdivision, cluster joining, vertex moving, and mathematical programming. Most
recent implementations actually use a combination of these approaches. Table II sum-
marizes how some of these roughly ﬁt into the design space.

Subdivision heuristics successively divide clusters. Some methods naturally produce
any number of clusters, for example by iteratively removing individual edges [Bader
and Madduri 2008; Newman and Girvan 2004]; other algorithms [Djidjev 2008; Duch
and Arenas 2005], in particular many eigenvector-based methods [Newman 2006b;
Richardson et al. 2009; Sun et al. 2009; White and Smyth 2005], essentially split the
graph into a ﬁxed number of clusters, but are extended to arbitrarily cluster counts
through recursive application.

CJ (or agglomeration) heuristics iteratively join clusters. Cluster pairs can be se-
lected based on random walks [Pons and Latapy 2006; Pujol et al. 2006], increase of
modularity [Clauset et al. 2004; Schuetz and Caﬂisch 2008a; Ye et al. 2008], or other
criteria [Danon et al. 2006; Donetti and Mu ˜noz 2005; Wakita and Tsurumi 2007].

VM heuristics move vertices between clusters, with KL style [Newman 2006b; Sun
et al. 2009] and locally greedy moving [Barber and Clark 2009; Blondel et al. 2008;
Liu and Murata 2010; Mei et al. 2009; Schuetz and Caﬂisch 2008a] being the most
prominent examples. Other approaches include tabu search [Arenas et al. 2008; L ¨u and
Huang 2009], extremal optimization [Duch and Arenas 2005], and simulated annealing
[Guimer`a and Amaral 2005; Massen and Doye 2005; Medus et al. 2005; Reichardt and
Bornholdt 2006].

Finally, mathematical programming approaches model modularity maximization as
a linear or quadratic programming problem, which can be solved with existing software
packages [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al. 2007]. For small
graphs, the integer linear programming methods allow the computation of optimal
modularity values.

6.2. Published Modularity Values
Table III compares the best modularity values from various publications with the
results of the heuristic CJ0 ZS+ML50+LM. For small graphs, all classes contain algo-
rithms that produce comparable or better clusterings. For larger graphs, the results of
CJ0 ZS+ML50+LM are better than any published value. This can be attributed to the
Multilevel Reﬁnement, which is not present in previous implementations.

On several small graphs, the label propagation algorithm LPA is able to ﬁnd better
clusterings than CJ0 ZS+ML50+LM. The algorithm basically produces a single coars-
ening level through LM MI and then applies CJx as postprocessing [Liu and Murata

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:20

R. Rotta and A. Noack

Table III. Best Published Modularity Values for Four Algorithm Classes and the CJ0 ZS+ML50+LM
Heuristic in the Last Column
Graph
Karate
Dolphins
PolBooks
AFootball
Jazz
Celeg metab
Email
Erdos02
PGP main
CMat03 main
ND edu

Moving
(DA) .4188
(LPA) .5285
(LPA) .527
(LPA) .605
(DA) .4452
(LPA) .452
(LPA) .582
(LH) .6902
(LPA) .884
(LPA) .755
(BGLL) .935

Subdivision
(N) .419
(N) .4893
(GN) .509
(WS) .602
(SDJB) .445
(SDJB) .452
(SDJB) .580
(N) .5969
(SDJB) .867
(SDJB) .737

(YHY) .4198
(PL) .5171
(SC) .5269
(YHY) .605
(SC) .445
(SC) .450
(SC) .575
(PBD) .6817
(SC) .878
(YHY) .761
(SC) .939

Size
34
62
105
115
198
453
1,133
6,927
11k
28k
325k

Joining

Math Prog
(AK) .4197
(XTP) .5285
(AK) .5272
(AK) .6046
(AK) .445
(AK) .450
(AK) .579

CJ0+LM
.41978
.52760
.52560
.60155
.44467
.44603
.57837
.71597
.88403
.81603
.95102

Where possible, missing values were substituted with results from published implementations,
which are shown in italics. For references to the literature see Section 6.2.

2010]. The reported high modularity values are mostly due to selecting the best values
out of 100 randomized runs. As can be seen, randomization is a useful tool to ﬁnd good
clusterings more reliably, though at a high computational expense.

Mathematical programming approaches consistently ﬁnd better clusterings than
CJ0 ZS+ML50+LM, though by a small margin; however, they are computationally
much more demanding and do not scale to large graphs [Agarwal and Kempe 2008].

References. The graphs are: Karate, the famous network of karate club members
[Zachary 1977]; Dolphins, a social network of bottlenose dolphins [Lusseau et al. 2003];
PolBooks, a co-purchasing graph of books about U.S. politics [Krebs 2008]; AFootball,
a network of college football matches [Girvan and Newman 2002]; Jazz, a collaboration
network of 198 Jazz musicians [Gleiser and Danon 2003]; Celeg metab, a metabolic
network of the worm C.elegans [Duch and Arenas 2005]; Email, a network of e-mail
interchanges between university members [Guimer`a et al. 2003]; Erdos02, the famous
co-authorship network around Paul Erds [Grossman 2007]; PGP main, a PGP key-
signing network [Bogu ˜n´a et al. 2004]; CMat03 main, a collaboration network of scientists
in condensed matter physics [Newman 2001]; and ND edu, a Web page network from
the nd.edu domain [Albert et al. 1999].

The subdivision methods are the betweenness-based edge removal algorithm GN by
Newman and Girvan [2004]; Newman’s [2006b] recursive spectral bisection N; SDJB, a
spectral clustering combined with KL Reﬁnement [Sun et al. 2009]; and the spectral
clustering WS by White and Smyth [2005].

The CJ algorithms comprise the walktrap algorithm PL of Pons and Latapy [2006]; the
multistep heuristic SC of Schuetz and Caﬂisch [2008a]; and two algorithms that combine
joining with some preprocessing by VM: PBD [Pujol et al. 2006] and YHY [Ye et al. 2008].
The VM algorithms are BGLL, the hierarchical LM algorithm of Blondel et al. [2008];
GC, an extremal optimization method combined with recursive subdivision [Duch and
Arenas 2005]; LPA, a label propagation algorithm [Liu and Murata 2010]; and LH, an
iterated tabu search [L ¨u and Huang 2009].

The results for mathematical programming algorithms were collected from the re-
laxed linear and quadratic programming AK of Agarwal and Kempe [2008] and the
mixed integer programming XTP of Xu et al. [2007].

6.3. Published Implementations on Graphs with Rounded Weights
Implementations that support graphs with integer edge weights are available for
(cid:7)
the multistep joining algorithm of Schuetz and Caﬂisch [2008a] (with parameter
l = 0.25
f (V , V )/2, as recommended by Schuetz and Caﬂisch [2008b]), the algorithm
of Pons and Latapy [2006] based on short random walks (here of length 4, the default

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:21

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

7
5

.

0

5
5

.

0

3
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

8

7

6

5

4

3

2

1

with refinement
basic algorithm

with refinement
basic algorithm

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

Fig. 7. Mean modularity (left) and runtime ratio (right) from published implementations on graphs with
rounded weights. The reﬁnement bars refer to the standard reﬁnement of the respective implementations.
The modularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the
Louvain method (the fastest in this experiment).
value), and the hierarchical LM algorithm of Blondel et al. [2008], known as Louvain
method.

The graphs for this experiment were derived from the 62 benchmark graphs by
removing self-edges, extracting the largest connected component, and rounding the
edge weights to the next nonzero integer if necessary. In order to minimize rounding
errors, the weights were multiplied with (222 − 1)/ deg(V ) before rounding.

Figure 7 shows the obtained results. Coarsening by CJ0 ZS (with 50% reduction fac-
tor) and by LM MI (with 100% reduction factor) are signiﬁcantly more effective than
the other algorithms when combined with Multilevel Reﬁnement. Only the Louvain im-
plementation by Blondel et al. [2008] is slightly faster, but produces worse clusterings.
Coarsening by CJ0 MI (with 100% reduction factor) is included in the ﬁgure be-
cause it is equivalent to the widely used fast greedy joining of Clauset et al. [2004].
LM MI without reﬁnement is conceptually equivalent to the Louvain algorithm; accord-
ingly, both are similarly effective and efﬁcient. CJx MI corresponds to the algorithm of
Schuetz and Caﬂisch. Although the two implementations use a different formula for the
parameter l, both produce equally good clusterings. Thus, differences in the implemen-
tations or in the formulas for the parameter l do not affect the conclusions about CJx, in
particular its inferiority to the simpler and parameter-free CJ0 with the ZS prioritizer.
The absolute runtimes and their dependency on the graph size are shown in Figure 8.
Both the Louvain implementation and CJ0 ZS+ML50+LM scale well with graph size
and generally are very efﬁcient. The longest runtime of only 6.6 seconds was measured
on the graph EatRS, which has 23k vertices and 305k edges.

6.4. Published Implementations on Unweighted Graphs
Implementations for unweighted graphs are available for further clustering al-
gorithms: the fast greedy joining of Clauset et al. [2004], the spectral recursive
bisection of Newman [2006b], the spinglass simulated annealing of Reichardt and
Bornholdt [2006], and the fast joining algorithm of Wakita and Tsurumi [2007].

Because these implementations cannot process graphs with weighted edges, only 23
unweighted graphs of the benchmark collection were used. In some of these graphs,
negligible differences in edge weights and small amounts of self-edges were removed.
Figure 9 shows the results, compared to our implementation CJ0 ZS with optional
reﬁnement ML50+LM. Only Reichardt and Bornholdt’s implementation produces clus-
terings of similarly high modularity, but it is much slower, and again, only the Louvain
implementation is slightly faster, but produces worse clusterings. As can be seen in
Figure 10, CJ0 ZS+ML50+LM is consistently among the fastest implementations, in-
dependently of the graph size.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:22

l

e
a
c
s
−
g
o

l
 
,
]
s
[
 

e
m

i
t

n
u
r

2
0
+
e
1

1
0
+
e
1

0
0
+
e
1

1
0
−
e
1

2
0
−
e
1

● Pons

Schuetz+refinement
CJ0_ZS+ML50+LM
Louvain

6.6 seconds

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●●

●●●

●

●
●●●

R. Rotta and A. Noack

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●●

● ●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 8. Runtime by graph size on graphs with rounded weights. The dashed line marks the longest runtime
of the Multilevel algorithms.

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

8
5

.

0

4
5

.

0

0
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

4
1

0
1

8

6

4

2

with refinement
basic algorithm

with refinement
basic algorithm

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

Fig. 9. Mean modularity (left) and runtime ratio (right) from published implementations on unweighted
graphs. The reﬁnement bars refer to the standard reﬁnement of the respective implementations. The mod-
ularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the Louvain
method (the fastest in this experiment). Newman’s spectral clustering is on average 150 times slower than
the Louvain method and Reichardt’s spinglass algorithm is around 2,400 times slower.

7. SUMMARY
The three primary contributions are (i) new heuristics for modularity clustering with
improved effectiveness and efﬁciency, (ii) a coherent organization of existing and new
heuristics and their combinations, and (iii) a systematic experimental comparison of
the heuristics.

Concerning the algorithmic contributions, experiments have shown that the new
criterion ZS for choosing joined clusters slightly outperforms the best existing criteria,
and clearly outperforms the most widely used criterion MI. Moreover, the new (for
modularity clustering) Multilevel Reﬁnement has turned out to be signiﬁcantly more
effective than the conventional Single-Level Reﬁnement and no reﬁnement.

Concerning organization and uniﬁcation, several existing and new heuristics and a
vast number of combinations are comprised in a design space with ﬁve dimensions: the
coarsening algorithm with its prioritizer (including Vertex Moving as well as Single-
Step and Multistep Cluster Joining), the reduction factor for controlling the number of
coarsening levels, and the reﬁnement algorithm with its prioritizer (including Single-
Level and Multilevel Reﬁnement by Vertex Moving).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:23

● Reichardt
Newman
Clauset
Schuetz+refinement
CJ0_ZS+ML50+LM
Wakita−HN

●●

●

●●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

0
0
0
0
1

0
0
0
1

l

e
a
c
s
−
g
o

l
 
,

i

n
a
v
u
o
L
o

 

t
 

o

i
t

a
r
 

e
m

i
t

n
u
r

0
0
1

0
1

1

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 10. Runtime ratio by graph size on unweighted graphs. All ratios are relative to the Louvain method
(dashed line).

Table IV. Graph Sources

Source
AArenas
AClauset
ANoack
Cx-Nets
GraphDrawing
JFowler
MNewman
Pajek
UriAlon

Web Address
http://deim.urv.cat/∼aarenas/data/welcome.htm
http://www.santafe.edu/∼aaronc/hierarchy/
http://www-sst.informatik.tu-cottbus.de/∼an/GD/
http://cxnets.googlepages.com/
http://vlado.fmf.uni-lj.si/pub/networks/data/GD/GD.htm
http://jhfowler.ucsd.edu/judicial.htm
http://www-personal.umich.edu/∼mejn/netdata/
http://vlado.fmf.uni-lj.si/pub/networks/data/
http://www.weizmann.ac.il/mcb/UriAlon/

The experimental comparison of achieved modularities and required runtimes
yielded several signiﬁcant results. The two primary coarsening approaches—Cluster
Joining and Vertex Moving—turned out to perform fairly similar with their best param-
eter values and reﬁnement. Moreover, some widely used, complex, or computationally
expensive design alternatives (e.g., Multistep Cluster Joining, join prioritization by
MI, Single-Level Reﬁnement, and Global Vertex Moving)—were outperformed by new,
simpler, or more efﬁcient alternatives. Overall, Single-Step Cluster Joining with ZS
prioritizer combined with Multilevel Local by Local Vertex Moving Reﬁnement is one
of the most effective and efﬁcient algorithms for modularity clustering in the literature.

APPENDIX
Table V lists the graphs used for the experiments. The graphs postﬁxed with “ main”
just contain the largest connected component of the original graph. All graphs from
the subset “UW” were used without edge weights and self-edges for the experiments
in Section 6.4. For each graph the source collection is named in the last column. Web
addresses of these collections are listed in Table IV. For information about the original
authors, please visit the respective Web sites.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:24

R. Rotta and A. Noack

SouthernWomen
Karate
Football
Morse
Food
Dolphins
Terrorist
WorldImport1999
Grass web
Lesmis
World trade
A00 main
PolBooks
AdjNoun
AFootball
Baywet
Jazz
SmallW main
A01 main
CelegansNeural
USAir97
A03 main
Netscience main
WorldCities main
Celeg metab
USAir500
s838
Roget main
SmaGri main
A96
Email
PolBlogs main
NDyeast main
Java
Yeast main
SciMet main
ODLIS main
DutchElite main
Geom main
Kohonen main
Epa main
Eva main
PPI SCerevisiae main
USpowerGrid
Hep-th main
California main
Zewail main
Erdos02
Lederberg main
PairsP
PGP main
DaysAll
Foldoc
Astro-ph main
AS-22july06
EatRS
DIC28 main
Judicial main
Hep-th-new main
CMat03 main
USSC main
Wordnet3 main

Table V. Graph Collection

Subset

UW
UW

UW

UW
UW
UW

UW
UW

UW

UW

UW
UW

UW

UW

UW

UW
UW

UW

UW

UW

UW

UW

UW

Vertices

32
34
35
36
45
62
62
66
75
77
80
83
105
112
115
128
198
233
249
297
332
328
379
413
453
500
512
994
1,024
1,096
1,133
12,22
1,458
1,538
2,224
2,678
2,898
3,621
3,621
3,704
4,253
4,475
4,626
4,941
5,835
5,925
6,640
6,927
8,212
10,617
10,680
13,308
13,356
14,845
22,963
23,219
24,831
25,389
27,400
27,519
34,428
75,606

Edges
89
78
118
666
990
159
152
2,145
113
254
875
135
441
425
613
2,075
2,742
994
635
2,148
2,126
497
914
7,518
2,040
2,980
819
3,641
4,917
1,677
5,451
16,717
1,993
7,817
7,049
10,369
16,381
4,310
9,461
12,675
8,897
4,654
14,801
6,594
13,815
15,770
54,174
11,850
41,436
63,786
24,316
148,035
91,471
119,652
48,436
305,501
71,014
216,436
352,059
116,181
201,078
120,472

Edge Weight
89.0
78.0
295.0
25,448.0
11,426.0
159.0
304.0
4367,930.4
113.0
820.0
65761,594.0
135.0
441.0
425.0
616.0
3,459.4
5,484.0
1,988.0
642.0
8,817.0
2,126.0
497.0
489.5
16,892.0
4,596.0
453914,166.0
819.0
5,059.0
4,922.0
1,691.0
10,902.0
19,089.0
1,993.0
8,032.0
7,049.0
10,385.0
18,417.0
4,311.0
19,770.0
12,685.0
8,953.0
4,664.0
29,602.0
13,188.0
13,674.6
15,946.0
54,244.0
11,850.0
41,507.0
612,563.0
24,340.0
338,706.0
125,207.0
33,372.3
48,436.0
788,876.0
71,014.0
216,718.0
352,542.0
60,793.1
202,053.0
131,780.0

Type

Source

Pajek
social
MNewman
social
Pajek
economy
ANoack
similarity
ANoack
similarity
Pajek
social
AClauset
social
ANoack
economy
AClauset
biology
MNewman
social
Pajek
economy
software GraphDrawing
Pajek
similarity
MNewman
linguistics
MNewman
social
Pajek
biology
social
AArenas
Pajek
citation
Pajek
citation
MNewman
biology
ﬂight
Pajek
biology GraphDrawing
MNewman
co-author
Pajek
social
AArenas
biology
Cx-Nets
ﬂight
UriAlon
technology
Pajek
linguistics
citation
Pajek
software GraphDrawing
AArenas
Pajek
Pajek
software GraphDrawing
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Cx-Nets
Pajek
MNewman
Pajek
Pajek
Pajek
Pajek
Pajek
AArenas
Pajek
Pajek
MNewman
MNewman
Pajek
Pajek
JFowler
Pajek
MNewman
JFowler
Pajek

biology
citation
linguistics
economy
co-author
citation
web
economy
biology
technology
citation
web
citation
co-author
citation
similarity
social
similarity
linguistics
co-author
web
linguistics
linguistics
citation
co-author
co-author
citation
linguistics

social
citation
biology

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:19

Table II. Published Algorithms Related to the Design Space

Algorithm
Clauset et al. [2004] (CNM)
Danon et al. [2006]
Wakita and Tsurumi [2007]
Schuetz and Caﬂisch [2008a] (SC)
Pons and Latapy [2006] (PL)
Pujol et al. [2006] (PBD)
Ye et al. [2008] (YHY)
Liu and Murata [2010] (LPA)
Blondel et al. [2008] (BGLL)
L ¨u and Huang [2009] (LH)
Sun et al. [2009] (SDJB)
CJ0 ZS+ML50+LM
LM MI+ML100+LM

Coarsening
CJ0 MI
CJ0 GC
CJ0 WHN/WHE
CJx MI
CJ0 + random walk
prejoining, modiﬁed CJ0
mixed GM MI, CJ0 MI
mixed LM MI, CJx MI
LM MI
subdivision by tabu search SL
spectral bisection + KL MI SL
CJ0 ZS
LM MI

Multilevel Reﬁnement
-
-
-
SL
-
-
SL
SL
ML 100% no

no
no
no
LM MI
no
no
GM MI
LM MI

tabu search
KL MI
ML 50% LM MI
ML 100% LM MI

6.1. Overview of Algorithms
Algorithms for modularity clustering can be categorized into the following four types:
subdivision, cluster joining, vertex moving, and mathematical programming. Most
recent implementations actually use a combination of these approaches. Table II sum-
marizes how some of these roughly ﬁt into the design space.

Subdivision heuristics successively divide clusters. Some methods naturally produce
any number of clusters, for example by iteratively removing individual edges [Bader
and Madduri 2008; Newman and Girvan 2004]; other algorithms [Djidjev 2008; Duch
and Arenas 2005], in particular many eigenvector-based methods [Newman 2006b;
Richardson et al. 2009; Sun et al. 2009; White and Smyth 2005], essentially split the
graph into a ﬁxed number of clusters, but are extended to arbitrarily cluster counts
through recursive application.

CJ (or agglomeration) heuristics iteratively join clusters. Cluster pairs can be se-
lected based on random walks [Pons and Latapy 2006; Pujol et al. 2006], increase of
modularity [Clauset et al. 2004; Schuetz and Caﬂisch 2008a; Ye et al. 2008], or other
criteria [Danon et al. 2006; Donetti and Mu ˜noz 2005; Wakita and Tsurumi 2007].

VM heuristics move vertices between clusters, with KL style [Newman 2006b; Sun
et al. 2009] and locally greedy moving [Barber and Clark 2009; Blondel et al. 2008;
Liu and Murata 2010; Mei et al. 2009; Schuetz and Caﬂisch 2008a] being the most
prominent examples. Other approaches include tabu search [Arenas et al. 2008; L ¨u and
Huang 2009], extremal optimization [Duch and Arenas 2005], and simulated annealing
[Guimer`a and Amaral 2005; Massen and Doye 2005; Medus et al. 2005; Reichardt and
Bornholdt 2006].

Finally, mathematical programming approaches model modularity maximization as
a linear or quadratic programming problem, which can be solved with existing software
packages [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al. 2007]. For small
graphs, the integer linear programming methods allow the computation of optimal
modularity values.

6.2. Published Modularity Values
Table III compares the best modularity values from various publications with the
results of the heuristic CJ0 ZS+ML50+LM. For small graphs, all classes contain algo-
rithms that produce comparable or better clusterings. For larger graphs, the results of
CJ0 ZS+ML50+LM are better than any published value. This can be attributed to the
Multilevel Reﬁnement, which is not present in previous implementations.

On several small graphs, the label propagation algorithm LPA is able to ﬁnd better
clusterings than CJ0 ZS+ML50+LM. The algorithm basically produces a single coars-
ening level through LM MI and then applies CJx as postprocessing [Liu and Murata

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:20

R. Rotta and A. Noack

Table III. Best Published Modularity Values for Four Algorithm Classes and the CJ0 ZS+ML50+LM
Heuristic in the Last Column
Graph
Karate
Dolphins
PolBooks
AFootball
Jazz
Celeg metab
Email
Erdos02
PGP main
CMat03 main
ND edu

Moving
(DA) .4188
(LPA) .5285
(LPA) .527
(LPA) .605
(DA) .4452
(LPA) .452
(LPA) .582
(LH) .6902
(LPA) .884
(LPA) .755
(BGLL) .935

Subdivision
(N) .419
(N) .4893
(GN) .509
(WS) .602
(SDJB) .445
(SDJB) .452
(SDJB) .580
(N) .5969
(SDJB) .867
(SDJB) .737

(YHY) .4198
(PL) .5171
(SC) .5269
(YHY) .605
(SC) .445
(SC) .450
(SC) .575
(PBD) .6817
(SC) .878
(YHY) .761
(SC) .939

Size
34
62
105
115
198
453
1,133
6,927
11k
28k
325k

Joining

Math Prog
(AK) .4197
(XTP) .5285
(AK) .5272
(AK) .6046
(AK) .445
(AK) .450
(AK) .579

CJ0+LM
.41978
.52760
.52560
.60155
.44467
.44603
.57837
.71597
.88403
.81603
.95102

Where possible, missing values were substituted with results from published implementations,
which are shown in italics. For references to the literature see Section 6.2.

2010]. The reported high modularity values are mostly due to selecting the best values
out of 100 randomized runs. As can be seen, randomization is a useful tool to ﬁnd good
clusterings more reliably, though at a high computational expense.

Mathematical programming approaches consistently ﬁnd better clusterings than
CJ0 ZS+ML50+LM, though by a small margin; however, they are computationally
much more demanding and do not scale to large graphs [Agarwal and Kempe 2008].

References. The graphs are: Karate, the famous network of karate club members
[Zachary 1977]; Dolphins, a social network of bottlenose dolphins [Lusseau et al. 2003];
PolBooks, a co-purchasing graph of books about U.S. politics [Krebs 2008]; AFootball,
a network of college football matches [Girvan and Newman 2002]; Jazz, a collaboration
network of 198 Jazz musicians [Gleiser and Danon 2003]; Celeg metab, a metabolic
network of the worm C.elegans [Duch and Arenas 2005]; Email, a network of e-mail
interchanges between university members [Guimer`a et al. 2003]; Erdos02, the famous
co-authorship network around Paul Erds [Grossman 2007]; PGP main, a PGP key-
signing network [Bogu ˜n´a et al. 2004]; CMat03 main, a collaboration network of scientists
in condensed matter physics [Newman 2001]; and ND edu, a Web page network from
the nd.edu domain [Albert et al. 1999].

The subdivision methods are the betweenness-based edge removal algorithm GN by
Newman and Girvan [2004]; Newman’s [2006b] recursive spectral bisection N; SDJB, a
spectral clustering combined with KL Reﬁnement [Sun et al. 2009]; and the spectral
clustering WS by White and Smyth [2005].

The CJ algorithms comprise the walktrap algorithm PL of Pons and Latapy [2006]; the
multistep heuristic SC of Schuetz and Caﬂisch [2008a]; and two algorithms that combine
joining with some preprocessing by VM: PBD [Pujol et al. 2006] and YHY [Ye et al. 2008].
The VM algorithms are BGLL, the hierarchical LM algorithm of Blondel et al. [2008];
GC, an extremal optimization method combined with recursive subdivision [Duch and
Arenas 2005]; LPA, a label propagation algorithm [Liu and Murata 2010]; and LH, an
iterated tabu search [L ¨u and Huang 2009].

The results for mathematical programming algorithms were collected from the re-
laxed linear and quadratic programming AK of Agarwal and Kempe [2008] and the
mixed integer programming XTP of Xu et al. [2007].

6.3. Published Implementations on Graphs with Rounded Weights
Implementations that support graphs with integer edge weights are available for
(cid:7)
the multistep joining algorithm of Schuetz and Caﬂisch [2008a] (with parameter
l = 0.25
f (V , V )/2, as recommended by Schuetz and Caﬂisch [2008b]), the algorithm
of Pons and Latapy [2006] based on short random walks (here of length 4, the default

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:21

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

7
5

.

0

5
5

.

0

3
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

8

7

6

5

4

3

2

1

with refinement
basic algorithm

with refinement
basic algorithm

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

Fig. 7. Mean modularity (left) and runtime ratio (right) from published implementations on graphs with
rounded weights. The reﬁnement bars refer to the standard reﬁnement of the respective implementations.
The modularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the
Louvain method (the fastest in this experiment).
value), and the hierarchical LM algorithm of Blondel et al. [2008], known as Louvain
method.

The graphs for this experiment were derived from the 62 benchmark graphs by
removing self-edges, extracting the largest connected component, and rounding the
edge weights to the next nonzero integer if necessary. In order to minimize rounding
errors, the weights were multiplied with (222 − 1)/ deg(V ) before rounding.

Figure 7 shows the obtained results. Coarsening by CJ0 ZS (with 50% reduction fac-
tor) and by LM MI (with 100% reduction factor) are signiﬁcantly more effective than
the other algorithms when combined with Multilevel Reﬁnement. Only the Louvain im-
plementation by Blondel et al. [2008] is slightly faster, but produces worse clusterings.
Coarsening by CJ0 MI (with 100% reduction factor) is included in the ﬁgure be-
cause it is equivalent to the widely used fast greedy joining of Clauset et al. [2004].
LM MI without reﬁnement is conceptually equivalent to the Louvain algorithm; accord-
ingly, both are similarly effective and efﬁcient. CJx MI corresponds to the algorithm of
Schuetz and Caﬂisch. Although the two implementations use a different formula for the
parameter l, both produce equally good clusterings. Thus, differences in the implemen-
tations or in the formulas for the parameter l do not affect the conclusions about CJx, in
particular its inferiority to the simpler and parameter-free CJ0 with the ZS prioritizer.
The absolute runtimes and their dependency on the graph size are shown in Figure 8.
Both the Louvain implementation and CJ0 ZS+ML50+LM scale well with graph size
and generally are very efﬁcient. The longest runtime of only 6.6 seconds was measured
on the graph EatRS, which has 23k vertices and 305k edges.

6.4. Published Implementations on Unweighted Graphs
Implementations for unweighted graphs are available for further clustering al-
gorithms: the fast greedy joining of Clauset et al. [2004], the spectral recursive
bisection of Newman [2006b], the spinglass simulated annealing of Reichardt and
Bornholdt [2006], and the fast joining algorithm of Wakita and Tsurumi [2007].

Because these implementations cannot process graphs with weighted edges, only 23
unweighted graphs of the benchmark collection were used. In some of these graphs,
negligible differences in edge weights and small amounts of self-edges were removed.
Figure 9 shows the results, compared to our implementation CJ0 ZS with optional
reﬁnement ML50+LM. Only Reichardt and Bornholdt’s implementation produces clus-
terings of similarly high modularity, but it is much slower, and again, only the Louvain
implementation is slightly faster, but produces worse clusterings. As can be seen in
Figure 10, CJ0 ZS+ML50+LM is consistently among the fastest implementations, in-
dependently of the graph size.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:22

l

e
a
c
s
−
g
o

l
 
,
]
s
[
 

e
m

i
t

n
u
r

2
0
+
e
1

1
0
+
e
1

0
0
+
e
1

1
0
−
e
1

2
0
−
e
1

● Pons

Schuetz+refinement
CJ0_ZS+ML50+LM
Louvain

6.6 seconds

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●●

●●●

●

●
●●●

R. Rotta and A. Noack

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●●

● ●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 8. Runtime by graph size on graphs with rounded weights. The dashed line marks the longest runtime
of the Multilevel algorithms.

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

8
5

.

0

4
5

.

0

0
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

4
1

0
1

8

6

4

2

with refinement
basic algorithm

with refinement
basic algorithm

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

Fig. 9. Mean modularity (left) and runtime ratio (right) from published implementations on unweighted
graphs. The reﬁnement bars refer to the standard reﬁnement of the respective implementations. The mod-
ularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the Louvain
method (the fastest in this experiment). Newman’s spectral clustering is on average 150 times slower than
the Louvain method and Reichardt’s spinglass algorithm is around 2,400 times slower.

7. SUMMARY
The three primary contributions are (i) new heuristics for modularity clustering with
improved effectiveness and efﬁciency, (ii) a coherent organization of existing and new
heuristics and their combinations, and (iii) a systematic experimental comparison of
the heuristics.

Concerning the algorithmic contributions, experiments have shown that the new
criterion ZS for choosing joined clusters slightly outperforms the best existing criteria,
and clearly outperforms the most widely used criterion MI. Moreover, the new (for
modularity clustering) Multilevel Reﬁnement has turned out to be signiﬁcantly more
effective than the conventional Single-Level Reﬁnement and no reﬁnement.

Concerning organization and uniﬁcation, several existing and new heuristics and a
vast number of combinations are comprised in a design space with ﬁve dimensions: the
coarsening algorithm with its prioritizer (including Vertex Moving as well as Single-
Step and Multistep Cluster Joining), the reduction factor for controlling the number of
coarsening levels, and the reﬁnement algorithm with its prioritizer (including Single-
Level and Multilevel Reﬁnement by Vertex Moving).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:23

● Reichardt
Newman
Clauset
Schuetz+refinement
CJ0_ZS+ML50+LM
Wakita−HN

●●

●

●●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

0
0
0
0
1

0
0
0
1

l

e
a
c
s
−
g
o

l
 
,

i

n
a
v
u
o
L
o

 

t
 

o

i
t

a
r
 

e
m

i
t

n
u
r

0
0
1

0
1

1

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 10. Runtime ratio by graph size on unweighted graphs. All ratios are relative to the Louvain method
(dashed line).

Table IV. Graph Sources

Source
AArenas
AClauset
ANoack
Cx-Nets
GraphDrawing
JFowler
MNewman
Pajek
UriAlon

Web Address
http://deim.urv.cat/∼aarenas/data/welcome.htm
http://www.santafe.edu/∼aaronc/hierarchy/
http://www-sst.informatik.tu-cottbus.de/∼an/GD/
http://cxnets.googlepages.com/
http://vlado.fmf.uni-lj.si/pub/networks/data/GD/GD.htm
http://jhfowler.ucsd.edu/judicial.htm
http://www-personal.umich.edu/∼mejn/netdata/
http://vlado.fmf.uni-lj.si/pub/networks/data/
http://www.weizmann.ac.il/mcb/UriAlon/

The experimental comparison of achieved modularities and required runtimes
yielded several signiﬁcant results. The two primary coarsening approaches—Cluster
Joining and Vertex Moving—turned out to perform fairly similar with their best param-
eter values and reﬁnement. Moreover, some widely used, complex, or computationally
expensive design alternatives (e.g., Multistep Cluster Joining, join prioritization by
MI, Single-Level Reﬁnement, and Global Vertex Moving)—were outperformed by new,
simpler, or more efﬁcient alternatives. Overall, Single-Step Cluster Joining with ZS
prioritizer combined with Multilevel Local by Local Vertex Moving Reﬁnement is one
of the most effective and efﬁcient algorithms for modularity clustering in the literature.

APPENDIX
Table V lists the graphs used for the experiments. The graphs postﬁxed with “ main”
just contain the largest connected component of the original graph. All graphs from
the subset “UW” were used without edge weights and self-edges for the experiments
in Section 6.4. For each graph the source collection is named in the last column. Web
addresses of these collections are listed in Table IV. For information about the original
authors, please visit the respective Web sites.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:24

R. Rotta and A. Noack

SouthernWomen
Karate
Football
Morse
Food
Dolphins
Terrorist
WorldImport1999
Grass web
Lesmis
World trade
A00 main
PolBooks
AdjNoun
AFootball
Baywet
Jazz
SmallW main
A01 main
CelegansNeural
USAir97
A03 main
Netscience main
WorldCities main
Celeg metab
USAir500
s838
Roget main
SmaGri main
A96
Email
PolBlogs main
NDyeast main
Java
Yeast main
SciMet main
ODLIS main
DutchElite main
Geom main
Kohonen main
Epa main
Eva main
PPI SCerevisiae main
USpowerGrid
Hep-th main
California main
Zewail main
Erdos02
Lederberg main
PairsP
PGP main
DaysAll
Foldoc
Astro-ph main
AS-22july06
EatRS
DIC28 main
Judicial main
Hep-th-new main
CMat03 main
USSC main
Wordnet3 main

Table V. Graph Collection

Subset

UW
UW

UW

UW
UW
UW

UW
UW

UW

UW

UW
UW

UW

UW

UW

UW
UW

UW

UW

UW

UW

UW

UW

Vertices

32
34
35
36
45
62
62
66
75
77
80
83
105
112
115
128
198
233
249
297
332
328
379
413
453
500
512
994
1,024
1,096
1,133
12,22
1,458
1,538
2,224
2,678
2,898
3,621
3,621
3,704
4,253
4,475
4,626
4,941
5,835
5,925
6,640
6,927
8,212
10,617
10,680
13,308
13,356
14,845
22,963
23,219
24,831
25,389
27,400
27,519
34,428
75,606

Edges
89
78
118
666
990
159
152
2,145
113
254
875
135
441
425
613
2,075
2,742
994
635
2,148
2,126
497
914
7,518
2,040
2,980
819
3,641
4,917
1,677
5,451
16,717
1,993
7,817
7,049
10,369
16,381
4,310
9,461
12,675
8,897
4,654
14,801
6,594
13,815
15,770
54,174
11,850
41,436
63,786
24,316
148,035
91,471
119,652
48,436
305,501
71,014
216,436
352,059
116,181
201,078
120,472

Edge Weight
89.0
78.0
295.0
25,448.0
11,426.0
159.0
304.0
4367,930.4
113.0
820.0
65761,594.0
135.0
441.0
425.0
616.0
3,459.4
5,484.0
1,988.0
642.0
8,817.0
2,126.0
497.0
489.5
16,892.0
4,596.0
453914,166.0
819.0
5,059.0
4,922.0
1,691.0
10,902.0
19,089.0
1,993.0
8,032.0
7,049.0
10,385.0
18,417.0
4,311.0
19,770.0
12,685.0
8,953.0
4,664.0
29,602.0
13,188.0
13,674.6
15,946.0
54,244.0
11,850.0
41,507.0
612,563.0
24,340.0
338,706.0
125,207.0
33,372.3
48,436.0
788,876.0
71,014.0
216,718.0
352,542.0
60,793.1
202,053.0
131,780.0

Type

Source

Pajek
social
MNewman
social
Pajek
economy
ANoack
similarity
ANoack
similarity
Pajek
social
AClauset
social
ANoack
economy
AClauset
biology
MNewman
social
Pajek
economy
software GraphDrawing
Pajek
similarity
MNewman
linguistics
MNewman
social
Pajek
biology
social
AArenas
Pajek
citation
Pajek
citation
MNewman
biology
ﬂight
Pajek
biology GraphDrawing
MNewman
co-author
Pajek
social
AArenas
biology
Cx-Nets
ﬂight
UriAlon
technology
Pajek
linguistics
citation
Pajek
software GraphDrawing
AArenas
Pajek
Pajek
software GraphDrawing
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Cx-Nets
Pajek
MNewman
Pajek
Pajek
Pajek
Pajek
Pajek
AArenas
Pajek
Pajek
MNewman
MNewman
Pajek
Pajek
JFowler
Pajek
MNewman
JFowler
Pajek

biology
citation
linguistics
economy
co-author
citation
web
economy
biology
technology
citation
web
citation
co-author
citation
similarity
social
similarity
linguistics
co-author
web
linguistics
linguistics
citation
co-author
co-author
citation
linguistics

social
citation
biology

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:25

ACKNOWLEDGMENTS

The authors thank G. Cs´ardi, T. Nepusz, P. Schuetz, A. Caﬂisch, P. Pons, M. Latapy, V.D. Blondel, J. Guil-
laume, R. Lambiotte, E. Lefebvre, K. Wakita, T. Tsurumi, A. Clauset, M.E.J. Newman, and C. Moore for
kindly providing their implementations. We are also grateful to U. Alon, A. Arenas, A. Clauset, V. Colizza,
J. Fowler, M.E.J. Newman, R. Pastor-Satorras, A. Vespignani, and especially the Pajek project [Batagelj and
Mrvar 2006] for collecting and publishing benchmark graphs.

REFERENCES
AARTS, E. AND LENSTRA, J. K., Eds. 2003. Local Search in Combinatorial Optimization. Princeton University

Press, Princeton, NJ.

AGARWAL, G. AND KEMPE, D. 2008. Modularity-maximizing graph communities via mathematical programming.

Eur. Phys. J. B 66, 3, 409–418.

ALBERT, R., JEONG, H., AND BARAB´ASI, A.-L. 1999. Diameter of the World-Wide Web. Nature 401, 6749, 130–131.
ARENAS, A., FERN´ANDEZ, A., AND G´OMEZ, S. 2008. Analysis of the structure of complex networks at different

resolution levels. New J. Phys. 10, 053039.

BADER, D. AND MADDURI, K. 2008. SNAP, Small-world Network Analysis and Partitioning: An open-source
parallel graph framework for the exploration of large-scale networks. In Proceedings of the IEEE In-
ternational Symposium on Parallel and Distributed Processing (IPDPS’08). IEEE, Los Alamitos, CA,
1–12.

BARBER, M. AND CLARK, J. 2009. Detecting network communities by propagating labels under constraints.

Phys. Rev. E 80, 2, 026129.

BATAGELJ, V. AND MRVAR, A. 2006. Pajek datasets. http://vlado.fmf.uni-lj.si/pub/networks/data/.
BLONDEL, V. D., GUILLAUME, J.-L., LAMBIOTTE, R., AND LEFEBVRE, E. 2008. Fast unfolding of communities in large

networks. J. Stat. Mech. Theory Exp. P10008.

BOETTCHER, S. AND PERCUS, A. G. 2001. Optimization with extremal dynamics. Phys. Rev. Lett. 86, 5211–5214.
BOGU˜N´A, M., PASTOR-SATORRAS, R., D´IAZ-GUILERA, A., AND ARENAS, A. 2004. Models of social networks based on

social distance attachment. Phys. Rev. E 70, 056122.

BRANDES, U., DELLING, D., GAERTLER, M., G¨ORKE, R., HOEFER, M., NIKOLOSKI, Z., AND WAGNER, D. 2008. On

modularity clustering. IEEE Trans. Knowl. Data Eng. 20, 2, 172–188.

BRANDES, U., GAERTLER, M., AND WAGNER, D. 2007. Engineering graph clustering: Models and experimental

evaluation. ACM J. Exp. Algorithmics 12, 1.1.

CLAUSET, A., NEWMAN, M. E. J., AND MOORE, C. 2004. Finding community structure in very large networks.

Phys. Rev. E 70, 066111.

CS´ARDI, G. AND NEPUSZ, T. 2006. The igraph software package for complex network research. Inter. Complex

Syst. 1695.

DANON, L., D´IAZ-GUILERA, A., AND ARENAS, A. 2006. Effect of size heterogeneity on community identiﬁcation in

complex networks. J. Stat. Mech. Theory Exp. P11010.

DANON, L., D´IAZ-GUILERA, A., DUCH, J., AND ARENAS, A. 2005. Comparing community structure identiﬁcation.

J. Stat. Mech. Theory Exp. P09008.

DAWES, B., NIEBLER, E., RIVERA, R., AND JAMES, D. 2009. BOOST C++ Libraries. http://www.boost.org.
DELLING, D., GAERTLER, M., G¨ORKE, R., AND WAGNER, D. 2008. Engineering comparators for graph clusterings. In
Proceedings of the 4th International Conference on Algorithmic Aspects in Information and Management
(AAIM’08). Springer-Verlag, Berlin, 131–142.

DICICCIO, T. J. AND EFRON, B. 1996. Bootstrap conﬁdence intervals. Stat. Sci. 11, 3, 189–228.
DJIDJEV, H. N. 2008. A scalable multilevel algorithm for graph clustering and community structure detection.
In Proceedings of the 4th International Workshop on Algorithms and Models for the Web-Graph (WAW’06).
Springer-Verlag, Berlin, 117–128.

DONETTI, L. AND MU˜NOZ, M. A. 2004. Detecting network communities: a new systematic and efﬁcient algorithm.

J. Stat. Mech. Theory Exp. P10012.

DONETTI, L. AND MU˜NOZ, M. A. 2005. Improved spectral algorithm for the detection of network communities.

Proc. AIP 779, 1, 104–107.

DUCH, J. AND ARENAS, A. 2005. Community detection in complex networks using extremal optimization. Phys.

Rev. E 72, 027104.

FORTUNATO, S. AND BARTH´ELEMY, M. 2007. Resolution limit in community detection. Proc. National Acad.

Sci. 104, 1, 36–41.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:19

Table II. Published Algorithms Related to the Design Space

Algorithm
Clauset et al. [2004] (CNM)
Danon et al. [2006]
Wakita and Tsurumi [2007]
Schuetz and Caﬂisch [2008a] (SC)
Pons and Latapy [2006] (PL)
Pujol et al. [2006] (PBD)
Ye et al. [2008] (YHY)
Liu and Murata [2010] (LPA)
Blondel et al. [2008] (BGLL)
L ¨u and Huang [2009] (LH)
Sun et al. [2009] (SDJB)
CJ0 ZS+ML50+LM
LM MI+ML100+LM

Coarsening
CJ0 MI
CJ0 GC
CJ0 WHN/WHE
CJx MI
CJ0 + random walk
prejoining, modiﬁed CJ0
mixed GM MI, CJ0 MI
mixed LM MI, CJx MI
LM MI
subdivision by tabu search SL
spectral bisection + KL MI SL
CJ0 ZS
LM MI

Multilevel Reﬁnement
-
-
-
SL
-
-
SL
SL
ML 100% no

no
no
no
LM MI
no
no
GM MI
LM MI

tabu search
KL MI
ML 50% LM MI
ML 100% LM MI

6.1. Overview of Algorithms
Algorithms for modularity clustering can be categorized into the following four types:
subdivision, cluster joining, vertex moving, and mathematical programming. Most
recent implementations actually use a combination of these approaches. Table II sum-
marizes how some of these roughly ﬁt into the design space.

Subdivision heuristics successively divide clusters. Some methods naturally produce
any number of clusters, for example by iteratively removing individual edges [Bader
and Madduri 2008; Newman and Girvan 2004]; other algorithms [Djidjev 2008; Duch
and Arenas 2005], in particular many eigenvector-based methods [Newman 2006b;
Richardson et al. 2009; Sun et al. 2009; White and Smyth 2005], essentially split the
graph into a ﬁxed number of clusters, but are extended to arbitrarily cluster counts
through recursive application.

CJ (or agglomeration) heuristics iteratively join clusters. Cluster pairs can be se-
lected based on random walks [Pons and Latapy 2006; Pujol et al. 2006], increase of
modularity [Clauset et al. 2004; Schuetz and Caﬂisch 2008a; Ye et al. 2008], or other
criteria [Danon et al. 2006; Donetti and Mu ˜noz 2005; Wakita and Tsurumi 2007].

VM heuristics move vertices between clusters, with KL style [Newman 2006b; Sun
et al. 2009] and locally greedy moving [Barber and Clark 2009; Blondel et al. 2008;
Liu and Murata 2010; Mei et al. 2009; Schuetz and Caﬂisch 2008a] being the most
prominent examples. Other approaches include tabu search [Arenas et al. 2008; L ¨u and
Huang 2009], extremal optimization [Duch and Arenas 2005], and simulated annealing
[Guimer`a and Amaral 2005; Massen and Doye 2005; Medus et al. 2005; Reichardt and
Bornholdt 2006].

Finally, mathematical programming approaches model modularity maximization as
a linear or quadratic programming problem, which can be solved with existing software
packages [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al. 2007]. For small
graphs, the integer linear programming methods allow the computation of optimal
modularity values.

6.2. Published Modularity Values
Table III compares the best modularity values from various publications with the
results of the heuristic CJ0 ZS+ML50+LM. For small graphs, all classes contain algo-
rithms that produce comparable or better clusterings. For larger graphs, the results of
CJ0 ZS+ML50+LM are better than any published value. This can be attributed to the
Multilevel Reﬁnement, which is not present in previous implementations.

On several small graphs, the label propagation algorithm LPA is able to ﬁnd better
clusterings than CJ0 ZS+ML50+LM. The algorithm basically produces a single coars-
ening level through LM MI and then applies CJx as postprocessing [Liu and Murata

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:20

R. Rotta and A. Noack

Table III. Best Published Modularity Values for Four Algorithm Classes and the CJ0 ZS+ML50+LM
Heuristic in the Last Column
Graph
Karate
Dolphins
PolBooks
AFootball
Jazz
Celeg metab
Email
Erdos02
PGP main
CMat03 main
ND edu

Moving
(DA) .4188
(LPA) .5285
(LPA) .527
(LPA) .605
(DA) .4452
(LPA) .452
(LPA) .582
(LH) .6902
(LPA) .884
(LPA) .755
(BGLL) .935

Subdivision
(N) .419
(N) .4893
(GN) .509
(WS) .602
(SDJB) .445
(SDJB) .452
(SDJB) .580
(N) .5969
(SDJB) .867
(SDJB) .737

(YHY) .4198
(PL) .5171
(SC) .5269
(YHY) .605
(SC) .445
(SC) .450
(SC) .575
(PBD) .6817
(SC) .878
(YHY) .761
(SC) .939

Size
34
62
105
115
198
453
1,133
6,927
11k
28k
325k

Joining

Math Prog
(AK) .4197
(XTP) .5285
(AK) .5272
(AK) .6046
(AK) .445
(AK) .450
(AK) .579

CJ0+LM
.41978
.52760
.52560
.60155
.44467
.44603
.57837
.71597
.88403
.81603
.95102

Where possible, missing values were substituted with results from published implementations,
which are shown in italics. For references to the literature see Section 6.2.

2010]. The reported high modularity values are mostly due to selecting the best values
out of 100 randomized runs. As can be seen, randomization is a useful tool to ﬁnd good
clusterings more reliably, though at a high computational expense.

Mathematical programming approaches consistently ﬁnd better clusterings than
CJ0 ZS+ML50+LM, though by a small margin; however, they are computationally
much more demanding and do not scale to large graphs [Agarwal and Kempe 2008].

References. The graphs are: Karate, the famous network of karate club members
[Zachary 1977]; Dolphins, a social network of bottlenose dolphins [Lusseau et al. 2003];
PolBooks, a co-purchasing graph of books about U.S. politics [Krebs 2008]; AFootball,
a network of college football matches [Girvan and Newman 2002]; Jazz, a collaboration
network of 198 Jazz musicians [Gleiser and Danon 2003]; Celeg metab, a metabolic
network of the worm C.elegans [Duch and Arenas 2005]; Email, a network of e-mail
interchanges between university members [Guimer`a et al. 2003]; Erdos02, the famous
co-authorship network around Paul Erds [Grossman 2007]; PGP main, a PGP key-
signing network [Bogu ˜n´a et al. 2004]; CMat03 main, a collaboration network of scientists
in condensed matter physics [Newman 2001]; and ND edu, a Web page network from
the nd.edu domain [Albert et al. 1999].

The subdivision methods are the betweenness-based edge removal algorithm GN by
Newman and Girvan [2004]; Newman’s [2006b] recursive spectral bisection N; SDJB, a
spectral clustering combined with KL Reﬁnement [Sun et al. 2009]; and the spectral
clustering WS by White and Smyth [2005].

The CJ algorithms comprise the walktrap algorithm PL of Pons and Latapy [2006]; the
multistep heuristic SC of Schuetz and Caﬂisch [2008a]; and two algorithms that combine
joining with some preprocessing by VM: PBD [Pujol et al. 2006] and YHY [Ye et al. 2008].
The VM algorithms are BGLL, the hierarchical LM algorithm of Blondel et al. [2008];
GC, an extremal optimization method combined with recursive subdivision [Duch and
Arenas 2005]; LPA, a label propagation algorithm [Liu and Murata 2010]; and LH, an
iterated tabu search [L ¨u and Huang 2009].

The results for mathematical programming algorithms were collected from the re-
laxed linear and quadratic programming AK of Agarwal and Kempe [2008] and the
mixed integer programming XTP of Xu et al. [2007].

6.3. Published Implementations on Graphs with Rounded Weights
Implementations that support graphs with integer edge weights are available for
(cid:7)
the multistep joining algorithm of Schuetz and Caﬂisch [2008a] (with parameter
l = 0.25
f (V , V )/2, as recommended by Schuetz and Caﬂisch [2008b]), the algorithm
of Pons and Latapy [2006] based on short random walks (here of length 4, the default

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:21

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

7
5

.

0

5
5

.

0

3
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

8

7

6

5

4

3

2

1

with refinement
basic algorithm

with refinement
basic algorithm

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

Fig. 7. Mean modularity (left) and runtime ratio (right) from published implementations on graphs with
rounded weights. The reﬁnement bars refer to the standard reﬁnement of the respective implementations.
The modularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the
Louvain method (the fastest in this experiment).
value), and the hierarchical LM algorithm of Blondel et al. [2008], known as Louvain
method.

The graphs for this experiment were derived from the 62 benchmark graphs by
removing self-edges, extracting the largest connected component, and rounding the
edge weights to the next nonzero integer if necessary. In order to minimize rounding
errors, the weights were multiplied with (222 − 1)/ deg(V ) before rounding.

Figure 7 shows the obtained results. Coarsening by CJ0 ZS (with 50% reduction fac-
tor) and by LM MI (with 100% reduction factor) are signiﬁcantly more effective than
the other algorithms when combined with Multilevel Reﬁnement. Only the Louvain im-
plementation by Blondel et al. [2008] is slightly faster, but produces worse clusterings.
Coarsening by CJ0 MI (with 100% reduction factor) is included in the ﬁgure be-
cause it is equivalent to the widely used fast greedy joining of Clauset et al. [2004].
LM MI without reﬁnement is conceptually equivalent to the Louvain algorithm; accord-
ingly, both are similarly effective and efﬁcient. CJx MI corresponds to the algorithm of
Schuetz and Caﬂisch. Although the two implementations use a different formula for the
parameter l, both produce equally good clusterings. Thus, differences in the implemen-
tations or in the formulas for the parameter l do not affect the conclusions about CJx, in
particular its inferiority to the simpler and parameter-free CJ0 with the ZS prioritizer.
The absolute runtimes and their dependency on the graph size are shown in Figure 8.
Both the Louvain implementation and CJ0 ZS+ML50+LM scale well with graph size
and generally are very efﬁcient. The longest runtime of only 6.6 seconds was measured
on the graph EatRS, which has 23k vertices and 305k edges.

6.4. Published Implementations on Unweighted Graphs
Implementations for unweighted graphs are available for further clustering al-
gorithms: the fast greedy joining of Clauset et al. [2004], the spectral recursive
bisection of Newman [2006b], the spinglass simulated annealing of Reichardt and
Bornholdt [2006], and the fast joining algorithm of Wakita and Tsurumi [2007].

Because these implementations cannot process graphs with weighted edges, only 23
unweighted graphs of the benchmark collection were used. In some of these graphs,
negligible differences in edge weights and small amounts of self-edges were removed.
Figure 9 shows the results, compared to our implementation CJ0 ZS with optional
reﬁnement ML50+LM. Only Reichardt and Bornholdt’s implementation produces clus-
terings of similarly high modularity, but it is much slower, and again, only the Louvain
implementation is slightly faster, but produces worse clusterings. As can be seen in
Figure 10, CJ0 ZS+ML50+LM is consistently among the fastest implementations, in-
dependently of the graph size.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:22

l

e
a
c
s
−
g
o

l
 
,
]
s
[
 

e
m

i
t

n
u
r

2
0
+
e
1

1
0
+
e
1

0
0
+
e
1

1
0
−
e
1

2
0
−
e
1

● Pons

Schuetz+refinement
CJ0_ZS+ML50+LM
Louvain

6.6 seconds

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●●

●●●

●

●
●●●

R. Rotta and A. Noack

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●●

● ●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 8. Runtime by graph size on graphs with rounded weights. The dashed line marks the longest runtime
of the Multilevel algorithms.

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

8
5

.

0

4
5

.

0

0
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

4
1

0
1

8

6

4

2

with refinement
basic algorithm

with refinement
basic algorithm

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

Fig. 9. Mean modularity (left) and runtime ratio (right) from published implementations on unweighted
graphs. The reﬁnement bars refer to the standard reﬁnement of the respective implementations. The mod-
ularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the Louvain
method (the fastest in this experiment). Newman’s spectral clustering is on average 150 times slower than
the Louvain method and Reichardt’s spinglass algorithm is around 2,400 times slower.

7. SUMMARY
The three primary contributions are (i) new heuristics for modularity clustering with
improved effectiveness and efﬁciency, (ii) a coherent organization of existing and new
heuristics and their combinations, and (iii) a systematic experimental comparison of
the heuristics.

Concerning the algorithmic contributions, experiments have shown that the new
criterion ZS for choosing joined clusters slightly outperforms the best existing criteria,
and clearly outperforms the most widely used criterion MI. Moreover, the new (for
modularity clustering) Multilevel Reﬁnement has turned out to be signiﬁcantly more
effective than the conventional Single-Level Reﬁnement and no reﬁnement.

Concerning organization and uniﬁcation, several existing and new heuristics and a
vast number of combinations are comprised in a design space with ﬁve dimensions: the
coarsening algorithm with its prioritizer (including Vertex Moving as well as Single-
Step and Multistep Cluster Joining), the reduction factor for controlling the number of
coarsening levels, and the reﬁnement algorithm with its prioritizer (including Single-
Level and Multilevel Reﬁnement by Vertex Moving).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:23

● Reichardt
Newman
Clauset
Schuetz+refinement
CJ0_ZS+ML50+LM
Wakita−HN

●●

●

●●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

0
0
0
0
1

0
0
0
1

l

e
a
c
s
−
g
o

l
 
,

i

n
a
v
u
o
L
o

 

t
 

o

i
t

a
r
 

e
m

i
t

n
u
r

0
0
1

0
1

1

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 10. Runtime ratio by graph size on unweighted graphs. All ratios are relative to the Louvain method
(dashed line).

Table IV. Graph Sources

Source
AArenas
AClauset
ANoack
Cx-Nets
GraphDrawing
JFowler
MNewman
Pajek
UriAlon

Web Address
http://deim.urv.cat/∼aarenas/data/welcome.htm
http://www.santafe.edu/∼aaronc/hierarchy/
http://www-sst.informatik.tu-cottbus.de/∼an/GD/
http://cxnets.googlepages.com/
http://vlado.fmf.uni-lj.si/pub/networks/data/GD/GD.htm
http://jhfowler.ucsd.edu/judicial.htm
http://www-personal.umich.edu/∼mejn/netdata/
http://vlado.fmf.uni-lj.si/pub/networks/data/
http://www.weizmann.ac.il/mcb/UriAlon/

The experimental comparison of achieved modularities and required runtimes
yielded several signiﬁcant results. The two primary coarsening approaches—Cluster
Joining and Vertex Moving—turned out to perform fairly similar with their best param-
eter values and reﬁnement. Moreover, some widely used, complex, or computationally
expensive design alternatives (e.g., Multistep Cluster Joining, join prioritization by
MI, Single-Level Reﬁnement, and Global Vertex Moving)—were outperformed by new,
simpler, or more efﬁcient alternatives. Overall, Single-Step Cluster Joining with ZS
prioritizer combined with Multilevel Local by Local Vertex Moving Reﬁnement is one
of the most effective and efﬁcient algorithms for modularity clustering in the literature.

APPENDIX
Table V lists the graphs used for the experiments. The graphs postﬁxed with “ main”
just contain the largest connected component of the original graph. All graphs from
the subset “UW” were used without edge weights and self-edges for the experiments
in Section 6.4. For each graph the source collection is named in the last column. Web
addresses of these collections are listed in Table IV. For information about the original
authors, please visit the respective Web sites.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:24

R. Rotta and A. Noack

SouthernWomen
Karate
Football
Morse
Food
Dolphins
Terrorist
WorldImport1999
Grass web
Lesmis
World trade
A00 main
PolBooks
AdjNoun
AFootball
Baywet
Jazz
SmallW main
A01 main
CelegansNeural
USAir97
A03 main
Netscience main
WorldCities main
Celeg metab
USAir500
s838
Roget main
SmaGri main
A96
Email
PolBlogs main
NDyeast main
Java
Yeast main
SciMet main
ODLIS main
DutchElite main
Geom main
Kohonen main
Epa main
Eva main
PPI SCerevisiae main
USpowerGrid
Hep-th main
California main
Zewail main
Erdos02
Lederberg main
PairsP
PGP main
DaysAll
Foldoc
Astro-ph main
AS-22july06
EatRS
DIC28 main
Judicial main
Hep-th-new main
CMat03 main
USSC main
Wordnet3 main

Table V. Graph Collection

Subset

UW
UW

UW

UW
UW
UW

UW
UW

UW

UW

UW
UW

UW

UW

UW

UW
UW

UW

UW

UW

UW

UW

UW

Vertices

32
34
35
36
45
62
62
66
75
77
80
83
105
112
115
128
198
233
249
297
332
328
379
413
453
500
512
994
1,024
1,096
1,133
12,22
1,458
1,538
2,224
2,678
2,898
3,621
3,621
3,704
4,253
4,475
4,626
4,941
5,835
5,925
6,640
6,927
8,212
10,617
10,680
13,308
13,356
14,845
22,963
23,219
24,831
25,389
27,400
27,519
34,428
75,606

Edges
89
78
118
666
990
159
152
2,145
113
254
875
135
441
425
613
2,075
2,742
994
635
2,148
2,126
497
914
7,518
2,040
2,980
819
3,641
4,917
1,677
5,451
16,717
1,993
7,817
7,049
10,369
16,381
4,310
9,461
12,675
8,897
4,654
14,801
6,594
13,815
15,770
54,174
11,850
41,436
63,786
24,316
148,035
91,471
119,652
48,436
305,501
71,014
216,436
352,059
116,181
201,078
120,472

Edge Weight
89.0
78.0
295.0
25,448.0
11,426.0
159.0
304.0
4367,930.4
113.0
820.0
65761,594.0
135.0
441.0
425.0
616.0
3,459.4
5,484.0
1,988.0
642.0
8,817.0
2,126.0
497.0
489.5
16,892.0
4,596.0
453914,166.0
819.0
5,059.0
4,922.0
1,691.0
10,902.0
19,089.0
1,993.0
8,032.0
7,049.0
10,385.0
18,417.0
4,311.0
19,770.0
12,685.0
8,953.0
4,664.0
29,602.0
13,188.0
13,674.6
15,946.0
54,244.0
11,850.0
41,507.0
612,563.0
24,340.0
338,706.0
125,207.0
33,372.3
48,436.0
788,876.0
71,014.0
216,718.0
352,542.0
60,793.1
202,053.0
131,780.0

Type

Source

Pajek
social
MNewman
social
Pajek
economy
ANoack
similarity
ANoack
similarity
Pajek
social
AClauset
social
ANoack
economy
AClauset
biology
MNewman
social
Pajek
economy
software GraphDrawing
Pajek
similarity
MNewman
linguistics
MNewman
social
Pajek
biology
social
AArenas
Pajek
citation
Pajek
citation
MNewman
biology
ﬂight
Pajek
biology GraphDrawing
MNewman
co-author
Pajek
social
AArenas
biology
Cx-Nets
ﬂight
UriAlon
technology
Pajek
linguistics
citation
Pajek
software GraphDrawing
AArenas
Pajek
Pajek
software GraphDrawing
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Cx-Nets
Pajek
MNewman
Pajek
Pajek
Pajek
Pajek
Pajek
AArenas
Pajek
Pajek
MNewman
MNewman
Pajek
Pajek
JFowler
Pajek
MNewman
JFowler
Pajek

biology
citation
linguistics
economy
co-author
citation
web
economy
biology
technology
citation
web
citation
co-author
citation
similarity
social
similarity
linguistics
co-author
web
linguistics
linguistics
citation
co-author
co-author
citation
linguistics

social
citation
biology

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:25

ACKNOWLEDGMENTS

The authors thank G. Cs´ardi, T. Nepusz, P. Schuetz, A. Caﬂisch, P. Pons, M. Latapy, V.D. Blondel, J. Guil-
laume, R. Lambiotte, E. Lefebvre, K. Wakita, T. Tsurumi, A. Clauset, M.E.J. Newman, and C. Moore for
kindly providing their implementations. We are also grateful to U. Alon, A. Arenas, A. Clauset, V. Colizza,
J. Fowler, M.E.J. Newman, R. Pastor-Satorras, A. Vespignani, and especially the Pajek project [Batagelj and
Mrvar 2006] for collecting and publishing benchmark graphs.

REFERENCES
AARTS, E. AND LENSTRA, J. K., Eds. 2003. Local Search in Combinatorial Optimization. Princeton University

Press, Princeton, NJ.

AGARWAL, G. AND KEMPE, D. 2008. Modularity-maximizing graph communities via mathematical programming.

Eur. Phys. J. B 66, 3, 409–418.

ALBERT, R., JEONG, H., AND BARAB´ASI, A.-L. 1999. Diameter of the World-Wide Web. Nature 401, 6749, 130–131.
ARENAS, A., FERN´ANDEZ, A., AND G´OMEZ, S. 2008. Analysis of the structure of complex networks at different

resolution levels. New J. Phys. 10, 053039.

BADER, D. AND MADDURI, K. 2008. SNAP, Small-world Network Analysis and Partitioning: An open-source
parallel graph framework for the exploration of large-scale networks. In Proceedings of the IEEE In-
ternational Symposium on Parallel and Distributed Processing (IPDPS’08). IEEE, Los Alamitos, CA,
1–12.

BARBER, M. AND CLARK, J. 2009. Detecting network communities by propagating labels under constraints.

Phys. Rev. E 80, 2, 026129.

BATAGELJ, V. AND MRVAR, A. 2006. Pajek datasets. http://vlado.fmf.uni-lj.si/pub/networks/data/.
BLONDEL, V. D., GUILLAUME, J.-L., LAMBIOTTE, R., AND LEFEBVRE, E. 2008. Fast unfolding of communities in large

networks. J. Stat. Mech. Theory Exp. P10008.

BOETTCHER, S. AND PERCUS, A. G. 2001. Optimization with extremal dynamics. Phys. Rev. Lett. 86, 5211–5214.
BOGU˜N´A, M., PASTOR-SATORRAS, R., D´IAZ-GUILERA, A., AND ARENAS, A. 2004. Models of social networks based on

social distance attachment. Phys. Rev. E 70, 056122.

BRANDES, U., DELLING, D., GAERTLER, M., G¨ORKE, R., HOEFER, M., NIKOLOSKI, Z., AND WAGNER, D. 2008. On

modularity clustering. IEEE Trans. Knowl. Data Eng. 20, 2, 172–188.

BRANDES, U., GAERTLER, M., AND WAGNER, D. 2007. Engineering graph clustering: Models and experimental

evaluation. ACM J. Exp. Algorithmics 12, 1.1.

CLAUSET, A., NEWMAN, M. E. J., AND MOORE, C. 2004. Finding community structure in very large networks.

Phys. Rev. E 70, 066111.

CS´ARDI, G. AND NEPUSZ, T. 2006. The igraph software package for complex network research. Inter. Complex

Syst. 1695.

DANON, L., D´IAZ-GUILERA, A., AND ARENAS, A. 2006. Effect of size heterogeneity on community identiﬁcation in

complex networks. J. Stat. Mech. Theory Exp. P11010.

DANON, L., D´IAZ-GUILERA, A., DUCH, J., AND ARENAS, A. 2005. Comparing community structure identiﬁcation.

J. Stat. Mech. Theory Exp. P09008.

DAWES, B., NIEBLER, E., RIVERA, R., AND JAMES, D. 2009. BOOST C++ Libraries. http://www.boost.org.
DELLING, D., GAERTLER, M., G¨ORKE, R., AND WAGNER, D. 2008. Engineering comparators for graph clusterings. In
Proceedings of the 4th International Conference on Algorithmic Aspects in Information and Management
(AAIM’08). Springer-Verlag, Berlin, 131–142.

DICICCIO, T. J. AND EFRON, B. 1996. Bootstrap conﬁdence intervals. Stat. Sci. 11, 3, 189–228.
DJIDJEV, H. N. 2008. A scalable multilevel algorithm for graph clustering and community structure detection.
In Proceedings of the 4th International Workshop on Algorithms and Models for the Web-Graph (WAW’06).
Springer-Verlag, Berlin, 117–128.

DONETTI, L. AND MU˜NOZ, M. A. 2004. Detecting network communities: a new systematic and efﬁcient algorithm.

J. Stat. Mech. Theory Exp. P10012.

DONETTI, L. AND MU˜NOZ, M. A. 2005. Improved spectral algorithm for the detection of network communities.

Proc. AIP 779, 1, 104–107.

DUCH, J. AND ARENAS, A. 2005. Community detection in complex networks using extremal optimization. Phys.

Rev. E 72, 027104.

FORTUNATO, S. AND BARTH´ELEMY, M. 2007. Resolution limit in community detection. Proc. National Acad.

Sci. 104, 1, 36–41.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:26

R. Rotta and A. Noack

GAERTLER, M. 2005. Clustering. In Network Analysis: Methodological Foundations, U. Brandes and T. Er-

lebach, Eds. Springer-Verlag, Berlin, 178–215.

GAERTLER, M., G¨ORKE, R., AND WAGNER, D. 2007. Signiﬁcance-driven graph clustering. In Proceedings of the 3rd
International Conference on Algorithmic Aspects in Information and Management (AAIM’07). Springer-
Verlag, Berlin, 11–26.

GIRVAN, M. AND NEWMAN, M. E. J. 2002. Community structure in social and biological networks. Proc. National

Acad. Sci. 99, 12, 7821–7826.

GLEISER, P. AND DANON, L. 2003. Community structure in jazz. Adv. Complex Syst. 6, 4, 565–573.
GROSSMAN, J. 2007. The Erds number project. http://www.oakland.edu/enp/.
GUIMER`A, R. AND AMARAL, L. A. N. 2005. Functional cartography of complex metabolic networks. Na-

ture 433, 7028, 895–900.

GUIMER`A, R., DANON, L., D´IAZ-GUILERA, A., GIRALT, F., AND ARENAS, A. 2003. Self-similar community structure

in a network of human interactions. Phys. Rev. E 68, 065103.

HENDRICKSON, B. AND LELAND, R. W. 1995. A multilevel algorithm for partitioning graphs. In Pro-
ceedings of the 1995 ACM/IEEE Conference on Supercomputing (Supercomputing’95). ACM, New
York, 28.

KANNAN, R., VEMPALA, S., AND VETTA, A. 2004. On clusterings: Good, bad and spectral. J. ACM 51, 3, 497–515.
KARRER, B. AND NEWMAN, M. E. J. 2009. Random acyclic networks. Phys. Rev. Lett. 102, 12, 128701.
KARYPIS, G. AND KUMAR, V. 1998. A fast and high quality multilevel scheme for partitioning irregular graphs.

SIAM J. Sci. Comput. 20, 1, 359–392.

KERNIGHAN, B. AND LIN, S. 1970. An efﬁcient heuristic procedure for partitioning graphs. Bell Sys. Techn.

J. 49, 2, 291–307.

KIRKPATRICK, S., GELATT, JR., C. D., AND VECCHI, M. P. 1983. Optimization by simulated annealing. Sci-

ence 220, 4598, 671–680.

KREBS, V. 2008. A network of books about recent US politics sold by the online bookseller amazon.com.

http://www.orgnet.com/.

LANCICHINETTI, A. AND FORTUNATO, S. 2009a. Benchmarks for testing community detection algorithms on

directed and weighted graphs with overlapping communities. Phys. Rev. E 80, 016118.

LANCICHINETTI, A. AND FORTUNATO, S. 2009b. Community detection algorithms: A comparative analysis. Phys.

Rev. E 80, 056117.

LIU, X. AND MURATA, T. 2010. Advanced modularity-specialized label propagation algorithm for detecting

communities in networks. Physica A 389, 7, 1493–1500.

L¨U, Z. AND HUANG, W. 2009. Iterated tabu search for identifying community structure in complex networks.

Phys. Rev. E 80, 026130.

LUSSEAU, D., SCHNEIDER, K., BOISSEAU, O. J., HAASE, P., SLOOTEN, E., AND DAWSON, S. M. 2003. The bottlenose
dolphin community of Doubtful Sound features a large proportion of long-lasting associations. Behav.
Ecol. Sociobiol. 54, 396–405.

MASSEN, C. P. AND DOYE, J. P. K. 2005. Identifying communities within energy landscapes. Phys. Rev. E 71,

046101.

MEDUS, A., ACU˜NA, G., AND DORSO, C. O. 2005. Detection of community structures in networks via global

optimization. Physica A 358, 2-4, 593–604.

MEI, J., HE, S., SHI, G., WANG, Z., AND LI, W. 2009. Revealing network communities through modularity

maximization by a contraction–dilation method. New J. Phys. 11, 043025.

NEWMAN, M. E. J. 2001. The structure of scientiﬁc collaboration networks. Proc. National Acad. Sci. 98, 2,

404–409.

NEWMAN, M. E. J. 2004a. Analysis of weighted networks. Phys. Rev. E 70, 056131.
NEWMAN, M. E. J. 2004b. Fast algorithm for detecting community structure in networks. Phys. Rev. E 69,

066133.

NEWMAN, M. E. J. 2006a. Finding community structure in networks using the eigenvectors of matrices. Phys.

Rev. E 74, 036104.

NEWMAN, M. E. J. 2006b. Modularity and community structure in networks. Proc. National Acad. Sci. 103, 23,

8577–8582.

NEWMAN, M. E. J. AND GIRVAN, M. 2004. Finding and evaluating community structure in networks. Phys. Rev.

E 69, 026113.

NOACK, A. 2007a. Energy models for graph clustering. J. Graph Algorithms Appl. 11, 2, 453–480.
NOACK, A. 2007b. Uniﬁed quality measures for clusterings, layouts, and orderings, and their application as

software design criteria. Ph.D. thesis, Brandenburg University of Technology.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

RANDOLF ROTTA and ANDREAS NOACK, Brandenburgische Technische Universit¨at Cottbus

Modularity is a widely used quality measure for graph clusterings. Its exact maximization is NP-hard and
prohibitively expensive for large graphs. Popular heuristics ﬁrst perform a coarsening phase, where local
search starting from singleton clusters is used to compute a preliminary clustering, and then optionally a
reﬁnement phase, where this clustering is improved by moving vertices between clusters. As a generalization,
multilevel heuristics coarsen in several stages, and reﬁne by moving entire clusters from each of these stages,
not only individual vertices.

This article organizes existing and new single-level and multilevel heuristics into a coherent design space,
and compares them experimentally with respect to their effectiveness (achieved modularity) and runtime.
For coarsening by iterated cluster joining, it turns out that the most widely used criterion for joining clusters
(modularity increase) is outperformed by other simple criteria, that a recent multistep algorithm [Schuetz
and Caﬂisch 2008] is no improvement over simple single-step coarsening for these criteria, and that the
recent multilevel coarsening by iterated vertex moving [Blondel et al. 2008] is somewhat faster but slightly
less effective (with reﬁnement). The new multilevel reﬁnement is signiﬁcantly more effective than the
conventional single-level reﬁnement or no reﬁnement, in reasonable runtime.

A comparison with published benchmark results and algorithm implementations shows that multilevel
local search heuristics, despite their relative simplicity, are competitive with the best algorithms in the
literature.
Categories and Subject Descriptors: I.5.3 [Pattern Recognition]: Clustering—Algorithms; E.1 [Data]: Data
Structures—Graphs and networks; G.4 [Mathematics of Computing]: Mathematical Software—Algorithm
design and analysis; Efﬁciency
General Terms: Algorithms, Design, Experimentation
Additional Key Words and Phrases: Graph clustering, local search, modularity, multilevel, reﬁnement
ACM Reference Format:
Rotta, R. and Noack, A. 2011. Multilevel local search algorithms for modularity clustering. ACM J. Exp.
Algor. 16, 2, Article 2.3 (June 2011), 27 pages.
DOI = 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

1. INTRODUCTION
A graph clustering partitions the vertex set of a graph into disjoint subsets called
clusters. Modularity was introduced by Newman and Girvan [2004] as a formalization
of the common requirement that the connections within graph clusters should be dense,
and the connections between different graph clusters should be sparse. It is by far not
the only quality measure for graph clusterings [Gaertler 2005; Schaeffer 2007; Porter

A previous version appeared as “Multilevel Algorithms for Modularity Clustering” at the 8th International
Symposium on Experimental Algorithms (SEA 2009).
Author’s address: R. Rotta, Lehrstuhl Theoretische Informatik, BTU Cottbus, Konrad-Wachsmann-Allee 1,
03046 Cottbus, Germany; email: rrotta@informatik.tu-cottbus.de.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that
copies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c(cid:2) 2011 ACM 1084-6654/2011/06-ART2.3 $10.00
DOI 10.1145/1963190.1970376 http://doi.acm.org/10.1145/1963190.1970376

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:2

R. Rotta and A. Noack

et al. 2009] but one of the most widely used measures, and it has successfully been
applied for detecting meaningful groups in a wide variety of real-world systems.

The problem of ﬁnding a clustering with maximum modularity for a given graph is
NP-hard [Brandes et al. 2008], and even recent exact algorithms scale only to graphs
with a few hundred vertices [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al.
2007]. In practice, modularity is almost exclusively optimized with heuristic algo-
rithms, and existing experimental results indicate that relatively simple heuristics
based on local search can be highly efﬁcient and effective (e.g., Blondel et al. [2008]
and Schuetz and Caﬂisch [2008a]).

Elementary local search strategies are the iterated joining of cluster pairs [Clauset
et al. 2004; Newman 2004b] and the iterated moving of vertices between clusters
[Barber and Clark 2009; Blondel et al. 2008]. Two such heuristics can be combined
as a coarsening phase, which computes a preliminary clustering starting from single-
ton clusters, and a reﬁnement phase, which further improves this clustering, typically
by vertex moving [Schuetz and Caﬂisch 2008a]. This approach can be generalized to
multilevel local search: During the coarsening phase, a series of successively coarser
clusterings called coarsening levels is recorded, and the reﬁnement moves entire clus-
ters of these coarsening levels, not just individual vertices. Such multilevel algorithms
proved to be very effective for minimum cut partitioning problems [Hendrickson and
Leland 1995; Preis and Diekmann 1997; Karypis and Kumar 1998; Walshaw and Cross
2000], but have not previously been adapted to modularity clustering.

In order to systematically describe and compare a large number of existing and new
local search heuristics for modularity clustering, we organized them into a design space
with the following dimensions: (i) coarsening algorithm, (ii) criterion for choosing the
joined clusters or moved vertices in coarsening, (iii) reﬁnement algorithm, (iv) criterion
for choosing the moved vertices or clusters in reﬁnement, and (v) number of coarsening
levels (with the conventional single level as special case). Sections 3 and 4 describe
design choices for each of these dimensions, including some new proposals. Overall, the
design space contains major existing heuristics as well as a vast number of promising
new variations and extensions.

Even for the existing heuristics, the comparability of the available evaluation results
is a major problem. The published modularity values and runtimes for the individual
heuristics have been obtained with different (and often small) graph collections and on
different hardware and software platforms. Moreover, the design space contains many
new heuristics, which have not yet been empirically evaluated at all. Therefore, we
have performed an extensive experimental comparison of the algorithms in the design
space, and present the results in Section 5.

To demonstrate that multilevel local search algorithms are among the most effective
and efﬁcient heuristics for modularity clustering, Section 6 provides a comparison
with published benchmark results and implementations of various algorithms from
the literature.

2. GRAPH CLUSTERINGS AND MODULARITY
2.1. Graphs and Clusterings
A graph (V , f ) consists of a ﬁnite set V of vertices and a function f : V × V → N
that assigns an edge weight to each vertex pair. To represent undirected graphs, this
function is symmetric: An undirected edge of weight w between vertices u (cid:3)= v is rep-
resented by f (u, v) = f (v, u) = w, a self-edge1 at vertex v by f (v, v) = 2w, and missing

1Undirected self-edges have to be counted in both directions.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:3

(cid:2)

u1∈U1

f (U1, U2) :=(cid:2)
edges have zero weight. The weights are naturally generalized to sets of vertices by
generalized to sets of vertices by deg(U ) :=(cid:2)
The degree deg(v) of a vertex v is the total weight f ({v}, V ) of its edges. It is naturally
u∈U deg(u). Note that deg(U ) ≥ f (U, U )
for all U ⊆ V , and in particular deg(V ) = f (V , V ).
A graph clustering C = {C1, . . . , Ck} partitions the vertex set V into disjoint nonempty
subsets Ci.

u2∈U2 f (u1, u2).

2.2. Modularity
Modularity is a widely used quality measure for graph clusterings. It was deﬁned by
Newman and Girvan [2004, 2004a] for undirected graphs as

(cid:4)

(cid:3)
C∈C

Q(C) :=

f (C, C)
f (V , V )

− deg(C)2
deg(V )2

(cid:5)

.

Intuitively, the ﬁrst term is the actual fraction of intracluster edge weight. In itself,
it is not a good measure of clustering quality because it takes the maximum value 1 for
the trivial clustering where one cluster contains all vertices. The second term speciﬁes
the expected fraction of intracluster edge weight in a null model where the end-vertices
of the deg(V )/2 edges are chosen at random, and the probability that an end-vertex of
an edge attaches to a particular vertex v is deg(v)
deg(V ) [Newman 2006a]. (In this null model,
the edge weight f (u, v) between a vertex pair (u, v) ∈ V 2 is binomially distributed with
the expected value deg(u) deg(v)
.) Among the many clustering quality measures that have
been proposed in the literature [Gaertler 2005; Schaeffer 2007], only few share this
important beneﬁt of modularity [Gaertler et al. 2007; Noack 2007a, 2007b]: the clear
interpretation of its values as deviation of the clustering from the expectation in an
explicit null model.

deg(V )

Joining two clusters C and D increases the modularity by

QC,D := 2 f (C, D)
f (V , V )

− 2 deg(C) deg(D)

deg(V )2

,

and moving a vertex v from its current cluster C to another cluster D increases the
modularity by

Qv,D := 2 f (v, D) − 2 f (v, C−v)

f (V , V )

− 2 deg(v) deg(D) − 2 deg(v) deg(C−v)

.

deg(V )2

From an algorithmic perspective, this means that the modularity of a clustering can
be quickly updated after each move or join; there is no need to recompute it from scratch.
Moreover, the search space can be restricted, because joining two nonadjacent clusters
( f (C, D) = 0) never increases the modularity, and moving a vertex to a nonadjacent
cluster ( f (v, D) = 0) never increases the modularity more than moving it to a new,
previously empty cluster (where deg(v) deg(D) takes the smallest possible value 0).

Modularity has a formal relation to (a speciﬁc notion of) intracluster density and
intercluster sparsity. Let the weighted density between two different clusters C and D
be deﬁned as
deg(C) deg(D) . In each maximum-modularity clustering of a given graph—in
fact, in each clustering whose modularity cannot be increased by joining or splitting
clusters—the weighted density between any two clusters is at most the weighted den-
sity f (V ,V )
deg(V )2 within the entire graph, and the weighted density between any two subclus-
ters obtained by splitting a cluster is at least the weighted density within the graph
[Reichardt and Bornholdt 2006; Noack 2009]. This follows from the fact that joining

f (C,D)

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:4

R. Rotta and A. Noack

two clusters C and D increases the modularity if and only if

f (C, D)

deg(C) deg(D)

>

f (V , V )
deg(V )2

.

The modularity measure has been criticized for its so-called resolution limit, that
is, the fact that two dense subgraphs connected by only one light-weight edge are
joined into a single cluster if the overall density within the graph is sufﬁciently small
[Fortunato and Barth´elemy 2007]. However, the resolution of optimal clusterings can
be arbitrarily increased or decreased by multiplying the second term of the modularity
measure with a positive constant greater or smaller than 1.

3. LOCAL SEARCH ALGORITHMS
Cluster Joining (CJ) and Vertex Moving (VM) are two classes of local search heuristics
that are widely used for improving clusterings in general, and for increasing modularity
in particular. CJ algorithms iteratively join two clusters; VM algorithms iteratively
move individual vertices to different clusters (including newly created clusters). The
cluster pair of each join, or the vertex and target cluster of each move, are chosen
according to a certain priority criterion, called Prioritizer, which is a parameter of
the algorithm. This section introduces two CJ algorithms, three VM algorithms, and
several Prioritizers.

Among the large variety of local search heuristics [Aarts and Lenstra 2003], the
presented techniques were selected because of their simplicity, popularity, and proven
suitability for modularity clustering or related problems. Notable exclusions are
simulated annealing Kirkpatrick et al. [1983] (used for modularity clustering in
Guimer`a and Amaral [2005], Massen and Doye [2005], Medus et al. [2005], and
Reichardt and Bornholdt [2006]) and extremal optimizations [Boettcher and Percus
2001] (used for modularity clustering in Duch and Arenas [2005]); for these heuristics,
a thorough experimental evaluation is particularly difﬁcult and expensive due to their
explicit randomness.

3.1. Single-Step Joining
Single-Step Joining (CJ0) iteratively joins the cluster pair with the largest priority
until this join would not increase the modularity.

ImplementationNotes. The join priority of each cluster pair can be computed in constant
time from the total edge weight between the two clusters (see Section 3.6). These total
weights change locally with each join and are thus stored in a dynamically coarsened
graph where each cluster is represented by a single vertex. In each join of two vertices
u and v, the edge list of the vertex with fewer edges (say u) is joined into the edge list
of the other vertex. Using the sorted double-linked edge lists proposed by Wakita and
Tsurumi [2007], this requires linear time in the list lengths. However, if some neighbor
vertices of u are not neighbors of v, then one end-vertex of the edges to these neighbors
changes from u to v, and the position of these edges in the neighbors’ edge lists must
be corrected to retain the sorting.

Let n be the initial number of clusters, m be the initial number of adjacent cluster
pairs, and d be the ﬁnal height of the join tree. Merging the edge lists of two clusters
has linear runtime in the list lengths, and each edge participates in at most d joins.
Thus, the worst-case runtime is O(dm) for the joins and, given that the length of each
edge list is at most n, O(dmn) for the position corrections. In practice, the number
of corrections and the list lengths are typically much smaller than the worst-case

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:5

ALGORITHM 1: Multistep Joining Algorithm
Input: graph, clustering, join prioritizer, join fraction
Output: clustering
l ← join fraction ·(cid:6)(cid:6){(C, D) : QC,D >0}(cid:6)(cid:6);
while ∃ cluster pair (C, D) : QC,D > 0 do
sort cluster pairs by join prioritizer;
mark all clusters as not joined;
for (cid:9)l(cid:10) most prioritized pairs (C, D) do

if C and D are not marked as joined then

join clusters C and D;
mark clusters C and D as joined;

end

end

end

bounds. (The implementation of Clauset et al. [2004] has better worst-case bounds, but
experimental results in Section 6 indicate that it is not more efﬁcient in practice.)

In order to quickly ﬁnd the prioritized cluster pair for the next join, a priority queue
(max-heap) is used, as proposed by Clauset et al. [2004]. The priority queue contains
the currently best partner of each cluster. After each join, the best partners of all
adjacent clusters are updated, as proposed by Wakita and Tsurumi [2007]. The total
number of these updates is bounded by dm, taking at most O(dmlog n) runtime.

3.2. Multistep Joining
To prevent extremely unbalanced cluster growth, Schuetz and Caﬂisch [2008a] intro-
duced Multistep Joining (CJx), which iteratively joins the l disjoint cluster pairs having
the highest priority, excluding pairs not increasing the modularity. A follow-up paper
[Schuetz and Caﬂisch 2008b] provided the empirical formula lopt := 0.251
Modularity Increase prioritizer on unweighted graphs.
However, the optimal value of the parameter l is highly dependent on the graph size
and not necessarily on the total edge weight in weighted graphs: For example, doubling
a graph by adding a second copy of itself doubles the number of disjoint pairs and thus
also doubles the optimal value of l, and doubling all edge weights would change lopt but
not the optimal clustering.

(cid:7)|E| for the

Therefore, we specify l as percentage of the number of modularity-increasing cluster
pairs, and call this percentage join fraction. It performs as good as the empirical formula
lopt (see Section 6) and is suitable for weighted graphs as well. Single-Step Joining (CJ0)
conceptually corresponds to the special case of l = 1, and for uniformity, we also denote
it as CJx with 0% join fraction.

ImplementationNotes. The same basic data structures as in CJ0 are used. A pseudocode
version is shown in Algorithm 1. To iterate over the l best cluster pairs in priority order,
the edges are sorted once before entering the inner loop. This requires O(mlog m) time
in the worst case. Alternative implementations optimized for very small join fractions
could use partial sorting with O(mlog l) (but a larger constant factor). Of these l cluster
pairs, only disjoint pairs are joined by marking the used clusters. The number of
iterations through the outer loop may be close to n if few disjoint pairs of adjacent
clusters exist, as in some power-law graphs but is typically much smaller.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:6

R. Rotta and A. Noack

3.3. Global Moving
Global Moving (GM) repeatedly performs the globally best vertex move until no fur-
ther modularity-increasing vertex move is possible. Here the best vertex move is a
modularity-increasing move with the highest priority over all vertices v and target
clusters D (including a new, previously empty cluster).

Implementation Notes. Vertices are moved in constant time using a vector mapping
vertices to their current cluster. To ﬁnd the best move, the move priority for each
vertex v and each cluster D needs to be determined. This move priority can be computed
in constant time from the total edge weight f (v, D) between v and D (see Section 3.6).
For each vertex, the algorithm collects these weights in one pass over its edges and
stores them in a temporary vector. Thus, ﬁnding the globally best move requires a
constant-time visit of all m edges. Assuming O(n) moves yields a runtime of O(nm).
3.4. Local Moving
The Local Moving (LM) algorithm repeatedly iterates through all vertices (in random-
ized order) and performs the best move for each vertex, until no modularity-increasing
move is found for any vertex. The best move of a vertex is the modularity-increasing
move with the highest priority over all target clusters (including a new, previously
empty cluster); if no modularity-increasing move exists, the vertex is not moved.

LM has been previously proposed by Schuetz and Caﬂisch [2008a], Ye et al. [2008],

and Blondel et al. [2008].
ImplementationNotes. The implementation is very similar to GM. The worst-case time
for one iteration over all n vertices is O(m), and very few iterations usually sufﬁce.
3.5. Adapted Kernighan-Lin Moving
Kernighan-Lin Moving (KL) extends GM with a basic capability to escape local maxima.
The algorithm was originally proposed by Kernighan and Lin [1970] for minimum cut
partitioning, and was adapted to modularity clustering by Newman [2006b] (though
with a limitation to two clusters). In its inner loop, the algorithm iteratively performs
the globally best vertex move, with the restriction that each vertex is moved only once,
but without the restriction to increase the modularity with each move. After all vertices
have been moved, the inner loop is restarted from the best found clustering. Preliminary
experiments indicated that it is much more efﬁcient and rarely less effective to abort the
inner loop when the best found clustering has not improved in the last k := 10 log2 |V|
vertex moves [Rotta 2008].

Implementation Notes. The implementation is largely straightforward (see Algo-
rithm 2). To improve efﬁciency, the current clustering is only copied to peak when
the modularity begins to decrease. The worst-case runtime is the same as for GM, as-
suming that a few outer iterations sufﬁce. In practice, the KL method runs somewhat
longer because it also performs modularity-decreasing moves.

3.6. Join Prioritizers
A join prioritizer assigns to each cluster pair (C, D) a real number called priority and
thereby determines the order in which the CJ algorithms select cluster pairs. Because
the joining algorithms use only the order of the priorities, two prioritizers can be
considered equivalent if one can be transformed into the other by adding a constant or
multiplying with a positive constant.

The Modularity Increase (MI) QC,D resulting from joining the clusters C and D is
an obvious and widely used prioritizer [Newman 2004b; Clauset et al. 2004; Schuetz
and Caﬂisch 2008a; Ye et al. 2008].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:7

ALGORITHM 2: Adapted Kernighan-Lin Moving Algorithm
Input: graph, clustering, move prioritizer
Output: clustering
repeat

peak ← clustering;
mark all vertices as unmoved;
while unmoved vertices exist do

(v, D) ← best move with v unmoved;
move v to cluster D, mark v as moved;
if Q(clustering) > Q(peak) then peak ← clustering;
if k moves since last peak then break;

end
clustering ← peak;

until no improved clustering found;

√

QC,D

The Weighted Density (WD) is deﬁned as

deg(C) deg(D) , that is, as the quotient of
the actual edge weight between C and D and the expected edge weight in the null
model of Section 2.2 (up to a constant factor). It is equivalent (in the earlier sense)
to
deg(C) deg(D) . As derived in Section 2.2, clusterings with maximal modularity have a
small intercluster density and a large intracluster density. Despite this close relation,
the WD has not previously been used as prioritizer in modularity clustering.

QC,D

f (C,D)

deg(V )

deg(C) deg(D)

The Z-Score (ZS),2 another new prioritizer, is deﬁned as

, and is thus a
natural compromise between MI and WD. A further motivation is its relation to the
(im)probability of the edge weight f (C, D) in the null model described in Section 2.2.
Under this null model, both the expected value and the variance of the edge weight
between C and D are approximately deg(C) deg(D)
for large enough deg(V ), and the ZS is
equivalent to the number of standard deviations that separate the actual edge weight
from the expected edge weight. In contrast, the standard deviation of MI grows with
deg(C) deg(D), and thus it is biased toward joining large clusters, while, conversely,
WD is biased toward joining small clusters.

The Graph Conductance (GC) measure3 [Kannan et al. 2004] was proposed as join
min(deg(C),deg(D)), based on the observation
prioritizer by Danon et al. [2006] in the form
that the MI QC,D tends to prioritize pairs of clusters with large degrees. It equals the
ZS if deg(C)= deg(D), and it is another compromise between MI and WD.

Wakita and Tsurumi [2007] found that joining by MI tends to join clusters of ex-
(cid:9)
tremely uneven sizes. To suppress unbalanced joining, they proposed the prioritizer
min
QC,D, where size(C) is either the number of vertices in C (prior-
itizer WHN) or the number of other clusters to which C is connected by an edge of
positive weight (prioritizer WHE).

, size(D)
size(C)

Other types of prioritizers are clearly possible. For example, vertex distances from
random walks or eigenvectors of certain matrices have been successfully applied in sev-
eral clustering algorithms [Pons and Latapy 2006; Donetti and Mu ˜noz 2004; Newman
2006a]. However, preliminary experiments suggest that these relatively complicated
and computationally expensive prioritizers may not be more effective than the simple
prioritizers in this section [Rotta 2008].

size(C)
size(D)

QC,D

(cid:8)

2This prioritizer was named Signiﬁcance (Sig) in earlier publications.
3This prioritizer was named Danon (DA) in earlier publications.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:8

R. Rotta and A. Noack

ALGORITHM 3: Multilevel Clustering Algorithm
Input: graph, coarsener, refiner, reduction factor
Output: clustering
level[1] ← graph;
repeat

// coarsening phase

clustering ← vertices of level[l];
clustering ← coarsener(level[l], clustering, reduction factor);
if cluster count reduced then

level[l+1] ← contract each cluster of clustering into a single vertex;

end

until cluster count not reduced;
// refinement phase
clustering ← vertices of level[lmax];
for l from lmax − 1 to 1 do

clustering ← project clustering from level[l+1] to level[l];
clustering ← refiner(level[l], clustering);

end

3.7. Move Prioritizers
A move prioritizer assigns a priority to each combination (v, D) of a vertex v ∈ V
and a target cluster D ∈ C. Again, the Modularity Increase (MI) Qv,D is the most
obvious prioritizer, and it is used by all previously published moving algorithms [Arenas
et al. 2008; Blondel et al. 2008; Liu and Murata 2010; L ¨u and Huang 2009; Mei et al.
2009; Newman 2006b; Schuetz and Caﬂisch 2008a; Sun et al. 2009]. Move prioritizers
corresponding to the other join prioritizers can be constructed canonically using the
priority of the join ({v}, D), ignoring the current cluster of v. (For the MI move prioritizer,
Qv,D is equivalent to Q{v},D if v is ﬁxed.)

If move priorities of the same vertex (for different target clusters) are compared, as
in LM, then ignoring the current cluster of the vertex does not distort the order of the
priorities, because the current cluster is the same for all compared moves. However, if
move priorities of different vertices are compared, ignoring the current cluster means
that small improvements of already well-assigned vertices can be favored over much
larger improvements of poorly assigned vertices. Therefore, only MI, which takes the
current cluster into account, will be used with GM and KL.
Moving a vertex v to a previously empty cluster D is a special case: Because deg(D) =
0, the values of the formulas for all move prioritizes except MI are undeﬁned. Joining {v}
with the empty cluster D does not change the clustering, thus Q{v},D = 0. Because all
move prioritizes except MI are, or can be expressed as, normalized forms of Q{v},D,
their natural value for an empty D is 0.

4. MULTILEVEL LOCAL SEARCH ALGORITHM
VM algorithms move only a single vertex in each step. They are thus unlikely to move
an entire group of densely interconnected vertices to another cluster, because this would
require a series of sharply modularity-decreasing vertex moves—a serious limitation,
particularly for large graphs. However, a successive coarsening, started from single-
vertex clusters, may well have joined this densely connected vertex group into a single
cluster at some stage of the coarsening. Then, a Vertex Mover could easily reassign it
by moving entire clusters of this intermediate clustering, instead of individual vertices.
This is the basic idea of multilevel search heuristics.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:9

Deﬁnition. The Multilevel Clustering algorithm proceeds in two phases (see Algo-
rithm 3). The coarsening phase produces a sequence of graphs called coarsening levels.
The ﬁrst coarsening level is the input graph. On each coarsening level, a clustering
is computed using a coarsener, which can be any CJ or VM algorithm from Section 3.
The coarsener starts with a clustering where each cluster is a single vertex (of the
respective coarsening level), and runs until (i) it terminates, or (ii) it has decreased the
number of clusters by a certain percentage since its start; this percentage is provided
as a parameter called reduction factor. The next coarsening level is generated by con-
tracting each cluster of the resulting clustering into a single vertex. The coarsening
phase ends when a ﬁxed point is reached (i.e., when the coarsener terminates without
changing the clustering).

The subsequent reﬁnement phase visits the coarsening levels in reverse order, that is,
from the coarsest graph to the original graph, and computes a clustering for each level.
On the coarsest level, the ﬁnal clustering is the trivial clustering with single-vertex
clusters. On each subsequent level, the initial clustering is obtained by projecting the
ﬁnal clustering of the previous level, that is, the initial cluster of each vertex is the
ﬁnal cluster of the respective vertex on the previous level. Then a reﬁner is applied
on this initial clustering to compute the ﬁnal clustering of the coarsening level. The
reﬁner can be any VM algorithm; CJ algorithms are unlikely to ﬁnd joinable clusters
in a nearly optimal clustering.

As abbreviated name for this Multilevel Reﬁnement (moving on all coarsening levels)
MLx will be used, where x is the reduction factor used during the coarsening phase.
The conventional Single-Level Reﬁnement (moving just vertices of the original graph)
will be abbreviated by SLx, where x is the reduction factor again.

Discussion. Generally, the number of coarsening levels increases with decreasing re-
duction factor. On the one hand, this means that the reﬁner has more opportunities to
improve the clustering. On the other hand, the more frequent contraction in coarsening
and the more thorough reﬁnement tend to increase the runtime.

For coarsening by CJ, the reduction factor does not affect the ﬁnal coarsening
level, because the initial clusters of each coarsening level have essentially the same
properties (degrees, edge weights) as the ﬁnal clusters of the previous level. Thus,
the ﬁnal clustering of the coarsening phase is independent of the reduction fac-
tor. (In practice, minor variations are possible due to different tie breaking of join
priorities.)

For coarsening by VM, a smaller reduction factor generally decreases the ﬁnal clus-
tering quality produced by the coarsening phase because the vertex movers on each
level are terminated before they have reached their modularity optimum, and some of
the resulting suboptimalities may not be correctable subsequently on coarser graphs.
On the other hand, the premature termination reduces the total runtime of the coarsen-
ers (but, as mentioned before, increases the total runtime of the graph contraction and
the reﬁnement phase).

With a reduction factor of 100%, coarsening by CJ produces exactly two coarsening
levels; the reﬁner works on only one level, namely the original graph. In contrast, coars-
ening by VM may still produce several coarsening levels. On each level, the coarsener
terminates when no further modularity-increasing vertex moves exist, but then each
cluster is contracted into a single vertex, which may enable new modularity-increasing
moves at the next coarsening level.

RelatedWork. The basic idea of multilevel search heuristics already proved to be very
effective for minimum cut partitioning problems. Popular implementations of such
heuristics are CHACO [Hendrickson and Leland 1995], PARTY [Preis and Diekmann
1997], METIS [Karypis and Kumar 1998], and JOSTLE [Walshaw and Cross 2000].

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:10

R. Rotta and A. Noack

Instead of maximizing the modularity quality measure, their task is to simultaneously
minimize the between-cluster edge weight and the cluster-weight imbalance for a ﬁxed,
conﬁgurable number of clusters.

In the coarsening phase, the minimum cut heuristics use join prioritizers related to
cut minimization and balancing the cluster weights. Because the requested number of
clusters is known, most implementations abort the coarsening phase earlier and use
other heuristics to ﬁnd a good initial clustering with the required size. In contrast,
prioritizers for the modularity should be related to the null model and ﬁnding a good
number of clusters is part of the modularity clustering problem. Here, the coarsening
phase already produces an initial clustering with a good size, and clusters are dynami-
cally added and removed in the reﬁnement phase. For minimum cut partitioning, very
efﬁcient implementations of KL exist and are used often. Unfortunately, the modular-
ity measure introduces more global dependencies, which makes global move selection
inefﬁcient.

Several recent algorithms for modularity clustering are related to the Multilevel
Clustering algorithm, but differ in crucial respects. Blondel et al. [2008] ﬁrst proposed
multilevel coarsening by LM, but their heuristic has no reﬁnement phase. Many pre-
vious heuristics combine a clustering algorithm with Single-Level Reﬁnement, that
is, reﬁnement only on the original graph [Massen and Doye 2005; Newman 2006b;
Richardson et al. 2009; Schuetz and Caﬂisch 2008a]. Algorithms by Sun et al. [2009]
and Ye et al. [2008] move vertices in several clusterings of different coarseness, but
only vertices of the original graph instead of coarse vertices (clusters). Djidjev’s [2008]
method is not itself a multilevel algorithm, but a divisive method built on an existing
multilevel algorithm for minimum cut partitioning.

Implementation Notes. With coarsening by CJ and reduction factor α, the number of
generated coarsening levels is at most log1/(1−α)(n). With coarsening by VM, the number
of levels will be slightly higher due to levels forced by local optima.

To generate each coarsening level, the vertices and edges of the previous level are
contracted using the clustering produced by the coarsener. Only the clustering produced
by the not the coarse graphs, coarsener, are used to decouple the Multilevel Clustering
from details of the coarsener. To contract the edges, for each cluster the outgoing edges
of its vertices are collected and sorted by the cluster of their target vertex. Then a
single iteration over the sorted edges sufﬁces to add a new coarse edge each time a
new target cluster occurs. The weights of the edges falling into the same coarse edge
are summed so that the total edge weight f (V , V ) and deg(V ) does not change. All
together, constructing the coarse graph takes at most O(mlog m) time per level.

The projection of clusterings from each coarsening level to its previous level is trivial:
For each vertex the cluster name of its coarse-vertex is copied using a vector that
maps each vertex to its coarse vertex; this vector is constructed as by-product of the
contraction. Thus, cluster projection is a linear-time operation.

5. EXPERIMENTS
The Multilevel Clustering algorithm introduced in the previous section has ﬁve pa-
rameters, as summarized in Table I: The coarsener (including the join fraction) with
its prioritizer, the coarsening and reﬁnement levels (including the reduction factor),
and the reﬁner with its prioritizer. The values of these parameters are encoded us-
ing a simple naming scheme, as for example in “CJ5 MI+ML50+LM MI”. This section
examines empirically the impact of the parameters on the effectiveness (achieved mod-
ularity) and efﬁciency (runtime) of Multilevel Clustering. After a description of the
experimental set-up, four sections present the results concerning

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:11

Table I. Parameters of the Multilevel Clustering Algorithm

Coarsening method (Section 3):

CJ0
CJx
LM
GM

Single-Step Cluster Joining (0% join fraction)
Multistep Cluster Joining (with x% join fraction)
Local Vertex Moving
Global Vertex Moving

Coarsening Prioritizer (Section 3.6):

ZS
GC
MI
WD
WHN, WHE Wakita’s node- and edge-based prioritizers

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

Levels (Section 4):

SLx
MLx

Single-Level Reﬁnement: Vertex Moving just on the original graph
Multilevel Reﬁnement: on all coarsening levels (with x% reduction factor)

Reﬁnement method (Section 3):
Local Vertex Moving
Global Vertex Moving
Adapted Kernighan-Lin Moving
Doing nothing

LM
GM
KL
no

Reﬁnement Prioritizer (Section 3.6):

ZS
GC
MI
WD

Z-Score prioritizer
Graph Conductance prioritizer
Modularity Increase prioritizer
Weighted Density prioritizer

—the parameters of coarsening by CJ,
—the parameters of coarsening by LM,
—the comparison of the coarsening algorithms (Joining, LM, and GM) with their best

parameters, and

—the parameters of reﬁnement (by LM and GM).

5.1. Set-up
The algorithms4 were implemented in C++. The software uses the Boost Library [Dawes
et al. 2009] Version 1.41 and the Boost Graph Library [Siek et al. 2002] for interface
deﬁnitions and graph adapter classes. All experiments were performed on a 2.8GHz
Intel Core 2 Duo processor (without using the second core) with 4GB main memory.
The operating system was MacOS 10.5.8 with the GNU Compilers version 4.2.1 from
Apple’s developer tools.

To compare the effectiveness of the algorithms, the mean modularity over a ﬁxed set
of benchmark graphs is measured; higher means indicate more effective algorithms.
Thus, only the differences between the means are interpreted, the absolute values are
not intended to be meaningful. The runtimes of the implementations are compared
using the geometric mean of runtime ratios relative to a reference implementation.
This mean ratio indicates how many times longer an implementation runs on average,
compared to the reference implementation. All runtime measurements exclude the
time required for reading the input graphs.

The sample of benchmark graphs is composed of 62 real-world graphs from various
public sources as listed in the Appendix.5 The available graphs were classiﬁed by their
application domain, and graphs of diverse size that represent fairly all major domains
were selected, with a preference for common benchmarks like Zachary’s [1977] karate
club network. The graphs range from a few to 75k vertices and 352k edges.

4Source code available at http://www.informatik.tu-cottbus.de/∼rrotta/.
5The complete graph collection can be requested from the authors by e-mail.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:12

R. Rotta and A. Noack

To indicate the signiﬁcance6 of the differences between the algorithms, we specify for
each measurement value (sample mean of the modularity or sample mean of runtime
ratio) a conﬁdence interval on the 95% level of the population mean. (To improve
the readability of the diagrams, some conﬁdence intervals are not shown if they are
similar to nearby intervals.) If, for example, the sample mean of the runtime ratio for
an algorithm lies outside the conﬁdence interval of the population mean for another
algorithm, the actual efﬁciency of the two algorithms differs with high probability.
To be precise, the conﬁdence interval listed for each modularity value does not refer
to the mean modularity achieved by the algorithm, but to the mean difference of its
modularity to the modularity of a speciﬁed reference algorithm; as mentioned earlier,
only the differences, not the absolute values, of the modularities are meant to be
interpreted. Because nothing is known about the distribution of the measurement
values, bootstrapping is used to compute the intervals [DiCiccio and Efron 1996]. Note
that absolute differences between modularity values are not always a reliable indicator
for the signiﬁcance of differences between algorithms. Close to the optimum, modularity
differences necessarily become smaller, but may still be highly signiﬁcant.

Besides real-world graphs, algorithms and quality measures can also be compared
on synthetic graphs with known clusters. The conformance of computed clusters to
known clusters can be quantiﬁed using a distance measure for clusterings, for example,
the normalized mutual information (see Delling et al. [2008] for a review). Girvan
and Newman [2002] proposed random graphs with ﬁxed probabilities for within- and
between-cluster edges, which were used in Barber and Clark [2009], Brandes et al.
[2007], Danon et al. [2005], Newman and Girvan [2004], and Pons and Latapy [2006].
Recently, interest in other models of clustered random graphs increased, especially
for other vertex degree distributions [Karrer and Newman 2009; Lancichinetti and
Fortunato 2009a, 2009b; Mei et al. 2009]. Sun et al. [2009] studied a bias of spectral
clustering methods on Erds-R´enyi random graphs, and Arenas et al. [2008] used
deterministically generated graphs with hierarchical clusters. While such experiments
help to evaluate combinations of an algorithm and a quality measure with respect to
their ability to reconstruct certain a priori clusterings, they do not contribute to the goal
of this section, namely to compare algorithms with respect to their ability to maximize
a given ﬁxed quality measure.

5.2. Coarsening by CJ
For all coarsening experiments, results without any reﬁnement and results with stan-
dard reﬁnement will be presented, where standard reﬁnement is LM with MI priori-
tizer. As discussed in Section 5.5, this choice of reﬁnement parameters does not bias
the results of the coarsening experiments because it is as effective as all other choices
(except KL), independently of the coarsening parameters.

Figure 1 compares the prioritizers and join fractions for coarsening by CJ without
reﬁnement, with reﬁnement at reduction factor 100%, and with Multilevel Reﬁnement
at reduction factor 50%.

Observations on the effectiveness of the prioritizers (left diagrams) include the fol-

lowing.
—The ZS and GC prioritizers are among the best for all join fractions without and with

reﬁnement.

—WD is slightly worse without reﬁnement, but competitive with reﬁnement at reduc-
tion factor 50%; apparently, reﬁnement beneﬁts from its bias toward balanced cluster
growth.

6The term “signiﬁcant” is used in its statistical sense: Probably not caused by mere chance.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:13

7
5

.

0

6
5
0

.

5
5
0

.

4
5
0

.

3
5

.

0

2
5
0

.

1
5

.

0

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
.
0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

●

●

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

20

40
60
join fraction [%]

80

100

0

.

4

0

.

3

5
2

.

0

.

2

5

.

1

0
1

.

●

0

● CJ_ZS+no
CJ_GC+no
CJ_WD+no
CJ_MI+no
CJ_WHN+no
CJ_WHE+no

●

●

●

●

●

●

20

40
60
join fraction [%]

80

100

(a) Without reﬁnement (+no).

● ● ●

●

●

●

●

● CJ_ZS+ML100+LM
CJ_GC+ML100+LM
CJ_WD+ML100+LM
CJ_MI+ML100+LM
CJ_WHN+ML100+LM
CJ_WHE+ML100+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0

20

40
60
join fraction [%]

80

100

●

●

●

0

●

20

●

●

●

40
60
join fraction [%]

80

100

(b) With Multilevel reﬁnement at 100% reduction factor (+ML100+LM MI).

8
5
.
0

7
5
.
0

6
5
.
0

5
5
.
0

4
5
0

.

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

● ● ●

●

●

●

●

● CJ_ZS+ML50+LM
CJ_GC+ML50+LM
CJ_WD+ML50+LM
CJ_MI+ML50+LM
CJ_WHN+ML50+LM
CJ_WHE+ML50+LM
CJ_ZS+no

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

0
.
4

0
.
3

5
.
2

0
.
2

5
.
1

0
1

.

●

●

●

●

●

●

●

0

20

40
60
join fraction [%]

40
60
join fraction [%]
(c) With Multilevel reﬁnement at 50% reduction factor (+ML50+LM MI).

100

80

20

0

80

100

Fig. 1. Coarsening by CJ with different prioritizers and join fractions. All conﬁdence intervals and runtime
ratios are relative to CJ0 ZS+no (ZS prioritizer at 0% join fraction without reﬁnement).
—MI is competitive without reﬁnement, but not with reﬁnement; apparently, the re-

ﬁnement suffers from its bias toward unbalanced cluster growth.

—Wakita’s prioritizers WHE and WHN are much less effective than the others.

Concerning the effectiveness of the join fractions, in particular the comparison of
Single-Step Joining (join fraction 0%) and Multistep Joining (join fractions greater

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:14

R. Rotta and A. Noack

than 0%), the ﬁgures allow the following observations:
—Only for the MI prioritizer with join fractions below 10% and without reﬁnement,
CJx is more effective than CJ0. Here it counteracts the bias of MI toward unbalanced
cluster growth.

—For the best prioritizers, in particular ZS, CJ0 is more effective than CJx. Here, the
artiﬁcial restriction of CJx regarding the joinable cluster pairs is not necessary to
balance cluster growth, but impedes some effective joins.
In respect of the efﬁciency (right diagrams), the most efﬁcient prioritizers are the ZS

and the WD, independently of the join fraction, and CJ0 is faster than CJx.

Overall, CJ0 with the ZS prioritizer (closely followed by WD and GC) produces
the best clusterings in the shortest time, particularly in the more effective case with
Multilevel Reﬁnement.

5.3. Coarsening by LM
Figure 2 compares prioritizers and reduction factors for coarsening by LM.

Concerning the reduction factors, 100% is most effective without reﬁnement, be-
cause the coarseners are not terminated before they reach their local optimum. With
reﬁnement, the effectiveness is similarly good for reduction factors above 80% and
slightly degrades with lower reduction factors. The efﬁciency is best for reduction fac-
tors around 50% to 80%. For smaller reduction factors, generating the large number
of coarsening levels is expensive, while for larger reduction factors, the coarseners run
signiﬁcantly longer.

Concerning the prioritizers, the impact on the achieved modularities and runtimes
is rather minor. Especially for the most effective reduction factors, the differences
between the prioritizers are not statistically signiﬁcant. They are naturally smaller
than in CJ because the prioritizers are only used to compare moves of the same vertex,
which reduces their difference, and because the vertices are moved in randomized
order, which reduces the risk of unbalanced cluster growth.

Overall, in the more effective case of reﬁnement with high reduction factors, Vertex
Moving is similarly effective with all prioritizers, and most efﬁcient (by a small margin)
with the MI prioritizer.

5.4. Coarsening by CJ vs. Coarsening by LM and GM
Figure 3 compares coarsening by CJ (with its best prioritizer ZS and best join frac-
tion 0%, i.e., CJ0), LM (with its best prioritizer MI), and GM (with its only applicable
prioritizer MI).

GM is by far the slowest coarsener. More surprisingly, it is also less effective than
LM. The reason is the unbalanced cluster growth caused by the MI prioritizer, which
is less pronounced in LM due to the randomized order of the moved vertices.

Without reﬁnement, LM with the best reduction factor (100%) is more effective
than CJ with any reduction factor. However with reﬁnement, CJ with reduction factor
below 50% is more effective than LM with any reduction factor.

Concerning the efﬁciency, the best LM (with reﬁnement at reduction factor 100%) is
around two times faster than the comparable effective CJ (with reﬁnement at reduction
factor 50%).

5.5. Reﬁnement by LM and GM
For these experiments, a good joining (CJ0 ZS) and a good moving heuristic (LM MI)
will be used as standard coarseners.

Figure 4 compares move prioritizers for reﬁnement by LM in combination with
coarsening by joining (CJ0 ZS) and coarsening by moving (LM MI). All prioritizers are
almost equally effective and efﬁcient; after a reasonably effective coarsening phase,

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:15

8
5

.

0

7
5

.

0

6
5

.

0

5
5

.

0

4
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

6
7
5

.

0

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

● LM_ZS+ML+no
LM_GC+ML+no
LM_WD+ML+no
LM_MI+ML+no

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Without reﬁnement (ML+no), modularity relative to LM MI+ML100+no.

●

●

●

●

●

●

●

● LM_ZS+ML+LM
LM_GC+ML+LM
LM_WD+ML+LM
LM_MI+ML+LM
LM_MI+ML+no

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

3

0

.

2

5

.

1

0

.

1

5

.

0

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) With standard reﬁnement (ML+LM MI), modularity relative to LM MI+ML100+LM.

Fig. 2. Coarsening by LM with different prioritizers and reduction factors. The runtime ratios are relative
to LM MI+ML100+no (MI prioritizer at 100% reduction factor without reﬁnement).

few opportunities for increasing modularity are left, so there is not much to prioritize.
Therefore, the MI prioritizer is used for the standard reﬁnement and in the following
experiments.

Concerning the reduction factor, Figure 4(a) shows that it is irrelevant for multilevel
coarsening by CJ without reﬁnement (CJ0 ZS+ML+no), but has a signiﬁcant impact
when adding reﬁnement (CJ0 ZS+ML+LM). Thus, with coarsening by CJ the reduction
factor is essentially a parameter of the reﬁnement. A reduction factor of 50% turns
out to be more effective than 80% and 100%, and is similarly efﬁcient. Reduction
factors below 50% do improve the modularity slightly, but increase the runtime. With
coarsening by LM (Figure 4(b)), the impact of the reduction factor on the effectiveness
is limited; apparently, its opposing effects on coarsening and reﬁnement (see Section 4)
almost cancel out. Here, reduction factors from 80% to 100% are effective and efﬁcient.
Figure 5 compares reﬁnement by LM, reﬁnement by GM, and reﬁnement by KL
for various reduction factors and the only common applicable prioritizer (Modular-
ity Increase). Reﬁnement by LM signiﬁcantly improves the clusterings with limited
additional runtime (around two times slower than without any reﬁnement). GM
is as effective as LM, but around ﬁve times slower. This conﬁrms the observation

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:16

R. Rotta and A. Noack

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5

.

0

5
6
5

.

0

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+no
LM_MI+ML+no
GM_MI+ML+no

●

●

●

●

●

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]
(a) Without reﬁnement (ML+no), modularity relative to CJ0 ZS+ML50+no.

●

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0

.

0
1

0

.

5

0

.

2

0

.

1

5

.

0

●

●

● CJ0_ZS+ML+LM
LM_MI+ML+LM
GM_MI+ML+LM
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

(b) With standard reﬁnement (ML+LM MI), modularity relative to CJ0 ZS+ML50+LM.

Fig. 3. Coarsening by Joining (CJ0) and Moving (LM, GM) with their best prioritizers. The runtime ratios
are relative to CJ0 ZS+ML50+no (ZS prioritizer at 50% reduction factor without reﬁnement).
from the prioritizer comparison, that the order of the vertex moves is irrelevant during
the reﬁnement phase. Only KL turns out to be more effective than LM. Apparently, its
ability to escape local optima is indeed effective. However, the huge runtime overhead
makes it unsuitable for most applications.

Figure 6 compares reﬁnement on all coarsening levels (Multilevel, ML) and reﬁne-
ment on just the original graph (Single-Level, SL).7 For coarsening by CJ0 ZS, the
Multilevel Reﬁnement is signiﬁcantly more effective than the conventional Single-
Level Reﬁnement for reductions factors smaller than 100%. For a reduction factor of
100%, both are identical. For coarsening by LM MI, Multilevel Reﬁnement is more
effective than Single-Level Reﬁnement for all reduction factors (and, of course, better
than no reﬁnement).

Overall, a signiﬁcant improvement of the coarsening results in reasonable time is
achieved by LM Reﬁnement with any prioritizer and a reduction factor around 50% for
coarsening by CJ or around 80% to 100% for coarsening by LM.

7For the largest graph (wordnet3), CJ0 ZS at reduction factor 50% produced just 12 coarsening levels, and
LM MI at reduction factor 100% produced 6 coarsening levels.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:17

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

5
8
5

.

0

0
8
5

.

0

5
7
5

.

0

0
7
5
0

.

5
6
5

.

0

●

●

●

●

l

e
a
c
s
−
g
o

l
 
,

o

●

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

●

.

0
2

5

.

1

0

.

1

5

.

0

● CJ0_ZS+ML+LM_ZS
CJ0_ZS+ML+LM_GC
CJ0_ZS+ML+LM_WD
CJ0_ZS+ML+LM_MI
CJ0_ZS+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(a) Based on coarsening by CJ0_ZS, modularity relative to CJ0_ZS+ML50+LM_MI. 

l

y
t
i
r
a
u
d
o
m
 
n
a
e
m

0
8
5
.
0

5
7
5
.
0

0
7
5
.
0

5
6
5
.
0

●

●

●

●

●

●

●

0
.
2

5
.
1

0
.
1

5
.
0

l

e
a
c
s
−
g
o
l
 
,
o
i
t
a
r
 
e
m

i
t
n
u
r
 
n
a
e
m
o
e
g

● LM_MI+ML+LM_ZS
LM_MI+ML+LM_GC
LM_MI+ML+LM_WD
LM_MI+ML+LM_MI
LM_MI+ML+no

●

●

●

0

20

40

60

80

100

0

20

40

60

80

100

reduction factor [%]

reduction factor [%]

(b) Based on coarsening by LM_MI, modularity relative to LM_MI+ML100+LM_MI.

Fig. 4. LM reﬁnement with different move prioritizers. The runtime ratios are relative to
CJ0 ZS+ML50+LM MI (Single-Step Joining with ZS at 50% reduction factor).

6
8
5

.

0

4
8
5

.

0

2
8
5

.

0

0
8
5
0

.

●

●

●

●

● CJ0_ZS+ML+KL_MI
CJ0_ZS+ML+GM_MI
CJ0_ZS+ML+LM_MI

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 
e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

0
2

0
1

5

2

1

●

●

●

●

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 5. Comparison of reﬁnement algorithms (KL,GM,LM) using the MI move prioritizer. For the coars-
ening phase, CJ0 with ZS prioritizer is used. All conﬁdence intervals and runtime ratios are relative
to CJ0 ZS+ML50+LM MI (LM Reﬁnement at 50% reduction factor). The mean modularity results of
CJ0 ZS+ML+no were left out to improve the readability (see Figure 4(a)).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:18

●

●

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

4
8
5

.

0

2
8
5

.

0

0
8
5

.

0

8
7
5

.

0

R. Rotta and A. Noack

●

●

● CJ0_ZS+ML+LM_MI
CJ0_ZS+SL+LM_MI
LM_MI+ML+LM_MI
LM_MI+SL+LM_MI

●

●

●

●

●

5

.

3

5

.

2

0

.

2

5

.

1

0

.

1

l

e
a
c
s
−
g
o

l
 
,

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
o
e
g

●

0

20

40

60

80

100

0

20

reduction factor [%]

40

60

reduction factor [%]

80

100

Fig. 6. Comparison of Multilevel (ML) and Single-Level (SL) Reﬁnement. All conﬁdence intervals and
runtime ratios are relative to LM MI+ML100+LM MI.

5.6. Conclusions
The best-performing algorithms are coarsening by CJ0 with ZS prioritizer and a
reduction factor of 50%, and coarsening by LM with any prioritizer and a re-
duction factor of 80% to 100%, both combined with reﬁnement by LM with any
prioritizer.

Concerning our new techniques, the ZS prioritizer is indeed the best overall prior-
itizer for CJ. It is slightly better than the GC prioritizer, and much better than the
widely used MI prioritizer. For coarsening by VM, the prioritizers have a marginal im-
pact. Multilevel Reﬁnement is indeed signiﬁcantly more effective than the conventional
Single-Level Reﬁnement and no reﬁnement for both, coarsening by CJ and coarsening
by local LM.

Interestingly, Single-Step Joining (CJ0) outperforms the more complex Multistep
Joining (CJx) and LM is a similarly effective reﬁner compared to GM and is only
slightly less effective than KL.

6. RELATED ALGORITHMS
An exhaustive review and comparison of the numerous existing algorithms for mod-
ularity clustering is beyond the scope of this article; the purpose of this section is to
provide evidence that the heuristic CJ0 ZS+ML50+LM—coarsening by CJ0 with the
ZS prioritizer combined with Multilevel Reﬁnement by LM with MI prioritizer and
reduction factor 50%—is competitive with the best methods in the literature.

After a brief overview of modularity clustering algorithms in the ﬁrst section, the
second section compares the results of CJ0 ZS+ML50+LM with modularity values from
various publications. This type of comparison is widely used in the literature, but its
conclusiveness is severely constrained by the limited accuracy (printed digits) of the
modularity values, the small number of graphs shared by most publications, and the
unavailability or incomparability of published runtimes.

In order to directly compare the new heuristics with existing algorithms, a range of
publicly available implementations was retrieved from authors’ Web sites and through
the igraph library of Cs´ardi and Nepusz [2006]. Because some of these implementations
can process only graphs with integer edge weights or unit weights, the comparison
is split into two sections: Experiments on rounded edge weights and on unweighted
graphs.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:19

Table II. Published Algorithms Related to the Design Space

Algorithm
Clauset et al. [2004] (CNM)
Danon et al. [2006]
Wakita and Tsurumi [2007]
Schuetz and Caﬂisch [2008a] (SC)
Pons and Latapy [2006] (PL)
Pujol et al. [2006] (PBD)
Ye et al. [2008] (YHY)
Liu and Murata [2010] (LPA)
Blondel et al. [2008] (BGLL)
L ¨u and Huang [2009] (LH)
Sun et al. [2009] (SDJB)
CJ0 ZS+ML50+LM
LM MI+ML100+LM

Coarsening
CJ0 MI
CJ0 GC
CJ0 WHN/WHE
CJx MI
CJ0 + random walk
prejoining, modiﬁed CJ0
mixed GM MI, CJ0 MI
mixed LM MI, CJx MI
LM MI
subdivision by tabu search SL
spectral bisection + KL MI SL
CJ0 ZS
LM MI

Multilevel Reﬁnement
-
-
-
SL
-
-
SL
SL
ML 100% no

no
no
no
LM MI
no
no
GM MI
LM MI

tabu search
KL MI
ML 50% LM MI
ML 100% LM MI

6.1. Overview of Algorithms
Algorithms for modularity clustering can be categorized into the following four types:
subdivision, cluster joining, vertex moving, and mathematical programming. Most
recent implementations actually use a combination of these approaches. Table II sum-
marizes how some of these roughly ﬁt into the design space.

Subdivision heuristics successively divide clusters. Some methods naturally produce
any number of clusters, for example by iteratively removing individual edges [Bader
and Madduri 2008; Newman and Girvan 2004]; other algorithms [Djidjev 2008; Duch
and Arenas 2005], in particular many eigenvector-based methods [Newman 2006b;
Richardson et al. 2009; Sun et al. 2009; White and Smyth 2005], essentially split the
graph into a ﬁxed number of clusters, but are extended to arbitrarily cluster counts
through recursive application.

CJ (or agglomeration) heuristics iteratively join clusters. Cluster pairs can be se-
lected based on random walks [Pons and Latapy 2006; Pujol et al. 2006], increase of
modularity [Clauset et al. 2004; Schuetz and Caﬂisch 2008a; Ye et al. 2008], or other
criteria [Danon et al. 2006; Donetti and Mu ˜noz 2005; Wakita and Tsurumi 2007].

VM heuristics move vertices between clusters, with KL style [Newman 2006b; Sun
et al. 2009] and locally greedy moving [Barber and Clark 2009; Blondel et al. 2008;
Liu and Murata 2010; Mei et al. 2009; Schuetz and Caﬂisch 2008a] being the most
prominent examples. Other approaches include tabu search [Arenas et al. 2008; L ¨u and
Huang 2009], extremal optimization [Duch and Arenas 2005], and simulated annealing
[Guimer`a and Amaral 2005; Massen and Doye 2005; Medus et al. 2005; Reichardt and
Bornholdt 2006].

Finally, mathematical programming approaches model modularity maximization as
a linear or quadratic programming problem, which can be solved with existing software
packages [Agarwal and Kempe 2008; Brandes et al. 2008; Xu et al. 2007]. For small
graphs, the integer linear programming methods allow the computation of optimal
modularity values.

6.2. Published Modularity Values
Table III compares the best modularity values from various publications with the
results of the heuristic CJ0 ZS+ML50+LM. For small graphs, all classes contain algo-
rithms that produce comparable or better clusterings. For larger graphs, the results of
CJ0 ZS+ML50+LM are better than any published value. This can be attributed to the
Multilevel Reﬁnement, which is not present in previous implementations.

On several small graphs, the label propagation algorithm LPA is able to ﬁnd better
clusterings than CJ0 ZS+ML50+LM. The algorithm basically produces a single coars-
ening level through LM MI and then applies CJx as postprocessing [Liu and Murata

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:20

R. Rotta and A. Noack

Table III. Best Published Modularity Values for Four Algorithm Classes and the CJ0 ZS+ML50+LM
Heuristic in the Last Column
Graph
Karate
Dolphins
PolBooks
AFootball
Jazz
Celeg metab
Email
Erdos02
PGP main
CMat03 main
ND edu

Moving
(DA) .4188
(LPA) .5285
(LPA) .527
(LPA) .605
(DA) .4452
(LPA) .452
(LPA) .582
(LH) .6902
(LPA) .884
(LPA) .755
(BGLL) .935

Subdivision
(N) .419
(N) .4893
(GN) .509
(WS) .602
(SDJB) .445
(SDJB) .452
(SDJB) .580
(N) .5969
(SDJB) .867
(SDJB) .737

(YHY) .4198
(PL) .5171
(SC) .5269
(YHY) .605
(SC) .445
(SC) .450
(SC) .575
(PBD) .6817
(SC) .878
(YHY) .761
(SC) .939

Size
34
62
105
115
198
453
1,133
6,927
11k
28k
325k

Joining

Math Prog
(AK) .4197
(XTP) .5285
(AK) .5272
(AK) .6046
(AK) .445
(AK) .450
(AK) .579

CJ0+LM
.41978
.52760
.52560
.60155
.44467
.44603
.57837
.71597
.88403
.81603
.95102

Where possible, missing values were substituted with results from published implementations,
which are shown in italics. For references to the literature see Section 6.2.

2010]. The reported high modularity values are mostly due to selecting the best values
out of 100 randomized runs. As can be seen, randomization is a useful tool to ﬁnd good
clusterings more reliably, though at a high computational expense.

Mathematical programming approaches consistently ﬁnd better clusterings than
CJ0 ZS+ML50+LM, though by a small margin; however, they are computationally
much more demanding and do not scale to large graphs [Agarwal and Kempe 2008].

References. The graphs are: Karate, the famous network of karate club members
[Zachary 1977]; Dolphins, a social network of bottlenose dolphins [Lusseau et al. 2003];
PolBooks, a co-purchasing graph of books about U.S. politics [Krebs 2008]; AFootball,
a network of college football matches [Girvan and Newman 2002]; Jazz, a collaboration
network of 198 Jazz musicians [Gleiser and Danon 2003]; Celeg metab, a metabolic
network of the worm C.elegans [Duch and Arenas 2005]; Email, a network of e-mail
interchanges between university members [Guimer`a et al. 2003]; Erdos02, the famous
co-authorship network around Paul Erds [Grossman 2007]; PGP main, a PGP key-
signing network [Bogu ˜n´a et al. 2004]; CMat03 main, a collaboration network of scientists
in condensed matter physics [Newman 2001]; and ND edu, a Web page network from
the nd.edu domain [Albert et al. 1999].

The subdivision methods are the betweenness-based edge removal algorithm GN by
Newman and Girvan [2004]; Newman’s [2006b] recursive spectral bisection N; SDJB, a
spectral clustering combined with KL Reﬁnement [Sun et al. 2009]; and the spectral
clustering WS by White and Smyth [2005].

The CJ algorithms comprise the walktrap algorithm PL of Pons and Latapy [2006]; the
multistep heuristic SC of Schuetz and Caﬂisch [2008a]; and two algorithms that combine
joining with some preprocessing by VM: PBD [Pujol et al. 2006] and YHY [Ye et al. 2008].
The VM algorithms are BGLL, the hierarchical LM algorithm of Blondel et al. [2008];
GC, an extremal optimization method combined with recursive subdivision [Duch and
Arenas 2005]; LPA, a label propagation algorithm [Liu and Murata 2010]; and LH, an
iterated tabu search [L ¨u and Huang 2009].

The results for mathematical programming algorithms were collected from the re-
laxed linear and quadratic programming AK of Agarwal and Kempe [2008] and the
mixed integer programming XTP of Xu et al. [2007].

6.3. Published Implementations on Graphs with Rounded Weights
Implementations that support graphs with integer edge weights are available for
(cid:7)
the multistep joining algorithm of Schuetz and Caﬂisch [2008a] (with parameter
l = 0.25
f (V , V )/2, as recommended by Schuetz and Caﬂisch [2008b]), the algorithm
of Pons and Latapy [2006] based on short random walks (here of length 4, the default

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:21

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

7
5

.

0

5
5

.

0

3
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

8

7

6

5

4

3

2

1

with refinement
basic algorithm

with refinement
basic algorithm

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

s
n
o
P

I

M
_
0
J
C

z
t

e
u
h
c
S

i

n
a
v
u
o
L

I

M
_
5
J
C

I

M
_
M
L

S
Z
_
0
J
C

Fig. 7. Mean modularity (left) and runtime ratio (right) from published implementations on graphs with
rounded weights. The reﬁnement bars refer to the standard reﬁnement of the respective implementations.
The modularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the
Louvain method (the fastest in this experiment).
value), and the hierarchical LM algorithm of Blondel et al. [2008], known as Louvain
method.

The graphs for this experiment were derived from the 62 benchmark graphs by
removing self-edges, extracting the largest connected component, and rounding the
edge weights to the next nonzero integer if necessary. In order to minimize rounding
errors, the weights were multiplied with (222 − 1)/ deg(V ) before rounding.

Figure 7 shows the obtained results. Coarsening by CJ0 ZS (with 50% reduction fac-
tor) and by LM MI (with 100% reduction factor) are signiﬁcantly more effective than
the other algorithms when combined with Multilevel Reﬁnement. Only the Louvain im-
plementation by Blondel et al. [2008] is slightly faster, but produces worse clusterings.
Coarsening by CJ0 MI (with 100% reduction factor) is included in the ﬁgure be-
cause it is equivalent to the widely used fast greedy joining of Clauset et al. [2004].
LM MI without reﬁnement is conceptually equivalent to the Louvain algorithm; accord-
ingly, both are similarly effective and efﬁcient. CJx MI corresponds to the algorithm of
Schuetz and Caﬂisch. Although the two implementations use a different formula for the
parameter l, both produce equally good clusterings. Thus, differences in the implemen-
tations or in the formulas for the parameter l do not affect the conclusions about CJx, in
particular its inferiority to the simpler and parameter-free CJ0 with the ZS prioritizer.
The absolute runtimes and their dependency on the graph size are shown in Figure 8.
Both the Louvain implementation and CJ0 ZS+ML50+LM scale well with graph size
and generally are very efﬁcient. The longest runtime of only 6.6 seconds was measured
on the graph EatRS, which has 23k vertices and 305k edges.

6.4. Published Implementations on Unweighted Graphs
Implementations for unweighted graphs are available for further clustering al-
gorithms: the fast greedy joining of Clauset et al. [2004], the spectral recursive
bisection of Newman [2006b], the spinglass simulated annealing of Reichardt and
Bornholdt [2006], and the fast joining algorithm of Wakita and Tsurumi [2007].

Because these implementations cannot process graphs with weighted edges, only 23
unweighted graphs of the benchmark collection were used. In some of these graphs,
negligible differences in edge weights and small amounts of self-edges were removed.
Figure 9 shows the results, compared to our implementation CJ0 ZS with optional
reﬁnement ML50+LM. Only Reichardt and Bornholdt’s implementation produces clus-
terings of similarly high modularity, but it is much slower, and again, only the Louvain
implementation is slightly faster, but produces worse clusterings. As can be seen in
Figure 10, CJ0 ZS+ML50+LM is consistently among the fastest implementations, in-
dependently of the graph size.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:22

l

e
a
c
s
−
g
o

l
 
,
]
s
[
 

e
m

i
t

n
u
r

2
0
+
e
1

1
0
+
e
1

0
0
+
e
1

1
0
−
e
1

2
0
−
e
1

● Pons

Schuetz+refinement
CJ0_ZS+ML50+LM
Louvain

6.6 seconds

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●●

●●●

●

●
●●●

R. Rotta and A. Noack

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●●

● ●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 8. Runtime by graph size on graphs with rounded weights. The dashed line marks the longest runtime
of the Multilevel algorithms.

l

y
t
i
r
a
u
d
o
m
n
a
e
m

 

8
5

.

0

4
5

.

0

0
5

.

0

o

i
t

a
r
 

e
m

i
t

n
u
r
 

n
a
e
m
m
o
e
g

4
1

0
1

8

6

4

2

with refinement
basic algorithm

with refinement
basic algorithm

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

n
a
m
w
e
N

E
H
−
a

t
i
k
a
W

s
n
o
P

N
H
−
a

t
i
k
a
W

t

e
s
u
a
C

l

i

n
a
v
u
o
L

z
t

e
u
h
c
S

t

d
r
a
h
c
e
R

i

S
Z
_
0
J
C

Fig. 9. Mean modularity (left) and runtime ratio (right) from published implementations on unweighted
graphs. The reﬁnement bars refer to the standard reﬁnement of the respective implementations. The mod-
ularity conﬁdence intervals are relative to CJ0 ZS+ML50+LM. Runtime ratios are relative to the Louvain
method (the fastest in this experiment). Newman’s spectral clustering is on average 150 times slower than
the Louvain method and Reichardt’s spinglass algorithm is around 2,400 times slower.

7. SUMMARY
The three primary contributions are (i) new heuristics for modularity clustering with
improved effectiveness and efﬁciency, (ii) a coherent organization of existing and new
heuristics and their combinations, and (iii) a systematic experimental comparison of
the heuristics.

Concerning the algorithmic contributions, experiments have shown that the new
criterion ZS for choosing joined clusters slightly outperforms the best existing criteria,
and clearly outperforms the most widely used criterion MI. Moreover, the new (for
modularity clustering) Multilevel Reﬁnement has turned out to be signiﬁcantly more
effective than the conventional Single-Level Reﬁnement and no reﬁnement.

Concerning organization and uniﬁcation, several existing and new heuristics and a
vast number of combinations are comprised in a design space with ﬁve dimensions: the
coarsening algorithm with its prioritizer (including Vertex Moving as well as Single-
Step and Multistep Cluster Joining), the reduction factor for controlling the number of
coarsening levels, and the reﬁnement algorithm with its prioritizer (including Single-
Level and Multilevel Reﬁnement by Vertex Moving).

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:23

● Reichardt
Newman
Clauset
Schuetz+refinement
CJ0_ZS+ML50+LM
Wakita−HN

●●

●

●●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

0
0
0
0
1

0
0
0
1

l

e
a
c
s
−
g
o

l
 
,

i

n
a
v
u
o
L
o

 

t
 

o

i
t

a
r
 

e
m

i
t

n
u
r

0
0
1

0
1

1

5e+01

1e+02

5e+02

1e+03

5e+03

1e+04

5e+04

1e+05

number of vertices, log−scale

Fig. 10. Runtime ratio by graph size on unweighted graphs. All ratios are relative to the Louvain method
(dashed line).

Table IV. Graph Sources

Source
AArenas
AClauset
ANoack
Cx-Nets
GraphDrawing
JFowler
MNewman
Pajek
UriAlon

Web Address
http://deim.urv.cat/∼aarenas/data/welcome.htm
http://www.santafe.edu/∼aaronc/hierarchy/
http://www-sst.informatik.tu-cottbus.de/∼an/GD/
http://cxnets.googlepages.com/
http://vlado.fmf.uni-lj.si/pub/networks/data/GD/GD.htm
http://jhfowler.ucsd.edu/judicial.htm
http://www-personal.umich.edu/∼mejn/netdata/
http://vlado.fmf.uni-lj.si/pub/networks/data/
http://www.weizmann.ac.il/mcb/UriAlon/

The experimental comparison of achieved modularities and required runtimes
yielded several signiﬁcant results. The two primary coarsening approaches—Cluster
Joining and Vertex Moving—turned out to perform fairly similar with their best param-
eter values and reﬁnement. Moreover, some widely used, complex, or computationally
expensive design alternatives (e.g., Multistep Cluster Joining, join prioritization by
MI, Single-Level Reﬁnement, and Global Vertex Moving)—were outperformed by new,
simpler, or more efﬁcient alternatives. Overall, Single-Step Cluster Joining with ZS
prioritizer combined with Multilevel Local by Local Vertex Moving Reﬁnement is one
of the most effective and efﬁcient algorithms for modularity clustering in the literature.

APPENDIX
Table V lists the graphs used for the experiments. The graphs postﬁxed with “ main”
just contain the largest connected component of the original graph. All graphs from
the subset “UW” were used without edge weights and self-edges for the experiments
in Section 6.4. For each graph the source collection is named in the last column. Web
addresses of these collections are listed in Table IV. For information about the original
authors, please visit the respective Web sites.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:24

R. Rotta and A. Noack

SouthernWomen
Karate
Football
Morse
Food
Dolphins
Terrorist
WorldImport1999
Grass web
Lesmis
World trade
A00 main
PolBooks
AdjNoun
AFootball
Baywet
Jazz
SmallW main
A01 main
CelegansNeural
USAir97
A03 main
Netscience main
WorldCities main
Celeg metab
USAir500
s838
Roget main
SmaGri main
A96
Email
PolBlogs main
NDyeast main
Java
Yeast main
SciMet main
ODLIS main
DutchElite main
Geom main
Kohonen main
Epa main
Eva main
PPI SCerevisiae main
USpowerGrid
Hep-th main
California main
Zewail main
Erdos02
Lederberg main
PairsP
PGP main
DaysAll
Foldoc
Astro-ph main
AS-22july06
EatRS
DIC28 main
Judicial main
Hep-th-new main
CMat03 main
USSC main
Wordnet3 main

Table V. Graph Collection

Subset

UW
UW

UW

UW
UW
UW

UW
UW

UW

UW

UW
UW

UW

UW

UW

UW
UW

UW

UW

UW

UW

UW

UW

Vertices

32
34
35
36
45
62
62
66
75
77
80
83
105
112
115
128
198
233
249
297
332
328
379
413
453
500
512
994
1,024
1,096
1,133
12,22
1,458
1,538
2,224
2,678
2,898
3,621
3,621
3,704
4,253
4,475
4,626
4,941
5,835
5,925
6,640
6,927
8,212
10,617
10,680
13,308
13,356
14,845
22,963
23,219
24,831
25,389
27,400
27,519
34,428
75,606

Edges
89
78
118
666
990
159
152
2,145
113
254
875
135
441
425
613
2,075
2,742
994
635
2,148
2,126
497
914
7,518
2,040
2,980
819
3,641
4,917
1,677
5,451
16,717
1,993
7,817
7,049
10,369
16,381
4,310
9,461
12,675
8,897
4,654
14,801
6,594
13,815
15,770
54,174
11,850
41,436
63,786
24,316
148,035
91,471
119,652
48,436
305,501
71,014
216,436
352,059
116,181
201,078
120,472

Edge Weight
89.0
78.0
295.0
25,448.0
11,426.0
159.0
304.0
4367,930.4
113.0
820.0
65761,594.0
135.0
441.0
425.0
616.0
3,459.4
5,484.0
1,988.0
642.0
8,817.0
2,126.0
497.0
489.5
16,892.0
4,596.0
453914,166.0
819.0
5,059.0
4,922.0
1,691.0
10,902.0
19,089.0
1,993.0
8,032.0
7,049.0
10,385.0
18,417.0
4,311.0
19,770.0
12,685.0
8,953.0
4,664.0
29,602.0
13,188.0
13,674.6
15,946.0
54,244.0
11,850.0
41,507.0
612,563.0
24,340.0
338,706.0
125,207.0
33,372.3
48,436.0
788,876.0
71,014.0
216,718.0
352,542.0
60,793.1
202,053.0
131,780.0

Type

Source

Pajek
social
MNewman
social
Pajek
economy
ANoack
similarity
ANoack
similarity
Pajek
social
AClauset
social
ANoack
economy
AClauset
biology
MNewman
social
Pajek
economy
software GraphDrawing
Pajek
similarity
MNewman
linguistics
MNewman
social
Pajek
biology
social
AArenas
Pajek
citation
Pajek
citation
MNewman
biology
ﬂight
Pajek
biology GraphDrawing
MNewman
co-author
Pajek
social
AArenas
biology
Cx-Nets
ﬂight
UriAlon
technology
Pajek
linguistics
citation
Pajek
software GraphDrawing
AArenas
Pajek
Pajek
software GraphDrawing
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Pajek
Cx-Nets
Pajek
MNewman
Pajek
Pajek
Pajek
Pajek
Pajek
AArenas
Pajek
Pajek
MNewman
MNewman
Pajek
Pajek
JFowler
Pajek
MNewman
JFowler
Pajek

biology
citation
linguistics
economy
co-author
citation
web
economy
biology
technology
citation
web
citation
co-author
citation
similarity
social
similarity
linguistics
co-author
web
linguistics
linguistics
citation
co-author
co-author
citation
linguistics

social
citation
biology

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:25

ACKNOWLEDGMENTS

The authors thank G. Cs´ardi, T. Nepusz, P. Schuetz, A. Caﬂisch, P. Pons, M. Latapy, V.D. Blondel, J. Guil-
laume, R. Lambiotte, E. Lefebvre, K. Wakita, T. Tsurumi, A. Clauset, M.E.J. Newman, and C. Moore for
kindly providing their implementations. We are also grateful to U. Alon, A. Arenas, A. Clauset, V. Colizza,
J. Fowler, M.E.J. Newman, R. Pastor-Satorras, A. Vespignani, and especially the Pajek project [Batagelj and
Mrvar 2006] for collecting and publishing benchmark graphs.

REFERENCES
AARTS, E. AND LENSTRA, J. K., Eds. 2003. Local Search in Combinatorial Optimization. Princeton University

Press, Princeton, NJ.

AGARWAL, G. AND KEMPE, D. 2008. Modularity-maximizing graph communities via mathematical programming.

Eur. Phys. J. B 66, 3, 409–418.

ALBERT, R., JEONG, H., AND BARAB´ASI, A.-L. 1999. Diameter of the World-Wide Web. Nature 401, 6749, 130–131.
ARENAS, A., FERN´ANDEZ, A., AND G´OMEZ, S. 2008. Analysis of the structure of complex networks at different

resolution levels. New J. Phys. 10, 053039.

BADER, D. AND MADDURI, K. 2008. SNAP, Small-world Network Analysis and Partitioning: An open-source
parallel graph framework for the exploration of large-scale networks. In Proceedings of the IEEE In-
ternational Symposium on Parallel and Distributed Processing (IPDPS’08). IEEE, Los Alamitos, CA,
1–12.

BARBER, M. AND CLARK, J. 2009. Detecting network communities by propagating labels under constraints.

Phys. Rev. E 80, 2, 026129.

BATAGELJ, V. AND MRVAR, A. 2006. Pajek datasets. http://vlado.fmf.uni-lj.si/pub/networks/data/.
BLONDEL, V. D., GUILLAUME, J.-L., LAMBIOTTE, R., AND LEFEBVRE, E. 2008. Fast unfolding of communities in large

networks. J. Stat. Mech. Theory Exp. P10008.

BOETTCHER, S. AND PERCUS, A. G. 2001. Optimization with extremal dynamics. Phys. Rev. Lett. 86, 5211–5214.
BOGU˜N´A, M., PASTOR-SATORRAS, R., D´IAZ-GUILERA, A., AND ARENAS, A. 2004. Models of social networks based on

social distance attachment. Phys. Rev. E 70, 056122.

BRANDES, U., DELLING, D., GAERTLER, M., G¨ORKE, R., HOEFER, M., NIKOLOSKI, Z., AND WAGNER, D. 2008. On

modularity clustering. IEEE Trans. Knowl. Data Eng. 20, 2, 172–188.

BRANDES, U., GAERTLER, M., AND WAGNER, D. 2007. Engineering graph clustering: Models and experimental

evaluation. ACM J. Exp. Algorithmics 12, 1.1.

CLAUSET, A., NEWMAN, M. E. J., AND MOORE, C. 2004. Finding community structure in very large networks.

Phys. Rev. E 70, 066111.

CS´ARDI, G. AND NEPUSZ, T. 2006. The igraph software package for complex network research. Inter. Complex

Syst. 1695.

DANON, L., D´IAZ-GUILERA, A., AND ARENAS, A. 2006. Effect of size heterogeneity on community identiﬁcation in

complex networks. J. Stat. Mech. Theory Exp. P11010.

DANON, L., D´IAZ-GUILERA, A., DUCH, J., AND ARENAS, A. 2005. Comparing community structure identiﬁcation.

J. Stat. Mech. Theory Exp. P09008.

DAWES, B., NIEBLER, E., RIVERA, R., AND JAMES, D. 2009. BOOST C++ Libraries. http://www.boost.org.
DELLING, D., GAERTLER, M., G¨ORKE, R., AND WAGNER, D. 2008. Engineering comparators for graph clusterings. In
Proceedings of the 4th International Conference on Algorithmic Aspects in Information and Management
(AAIM’08). Springer-Verlag, Berlin, 131–142.

DICICCIO, T. J. AND EFRON, B. 1996. Bootstrap conﬁdence intervals. Stat. Sci. 11, 3, 189–228.
DJIDJEV, H. N. 2008. A scalable multilevel algorithm for graph clustering and community structure detection.
In Proceedings of the 4th International Workshop on Algorithms and Models for the Web-Graph (WAW’06).
Springer-Verlag, Berlin, 117–128.

DONETTI, L. AND MU˜NOZ, M. A. 2004. Detecting network communities: a new systematic and efﬁcient algorithm.

J. Stat. Mech. Theory Exp. P10012.

DONETTI, L. AND MU˜NOZ, M. A. 2005. Improved spectral algorithm for the detection of network communities.

Proc. AIP 779, 1, 104–107.

DUCH, J. AND ARENAS, A. 2005. Community detection in complex networks using extremal optimization. Phys.

Rev. E 72, 027104.

FORTUNATO, S. AND BARTH´ELEMY, M. 2007. Resolution limit in community detection. Proc. National Acad.

Sci. 104, 1, 36–41.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

2.3:26

R. Rotta and A. Noack

GAERTLER, M. 2005. Clustering. In Network Analysis: Methodological Foundations, U. Brandes and T. Er-

lebach, Eds. Springer-Verlag, Berlin, 178–215.

GAERTLER, M., G¨ORKE, R., AND WAGNER, D. 2007. Signiﬁcance-driven graph clustering. In Proceedings of the 3rd
International Conference on Algorithmic Aspects in Information and Management (AAIM’07). Springer-
Verlag, Berlin, 11–26.

GIRVAN, M. AND NEWMAN, M. E. J. 2002. Community structure in social and biological networks. Proc. National

Acad. Sci. 99, 12, 7821–7826.

GLEISER, P. AND DANON, L. 2003. Community structure in jazz. Adv. Complex Syst. 6, 4, 565–573.
GROSSMAN, J. 2007. The Erds number project. http://www.oakland.edu/enp/.
GUIMER`A, R. AND AMARAL, L. A. N. 2005. Functional cartography of complex metabolic networks. Na-

ture 433, 7028, 895–900.

GUIMER`A, R., DANON, L., D´IAZ-GUILERA, A., GIRALT, F., AND ARENAS, A. 2003. Self-similar community structure

in a network of human interactions. Phys. Rev. E 68, 065103.

HENDRICKSON, B. AND LELAND, R. W. 1995. A multilevel algorithm for partitioning graphs. In Pro-
ceedings of the 1995 ACM/IEEE Conference on Supercomputing (Supercomputing’95). ACM, New
York, 28.

KANNAN, R., VEMPALA, S., AND VETTA, A. 2004. On clusterings: Good, bad and spectral. J. ACM 51, 3, 497–515.
KARRER, B. AND NEWMAN, M. E. J. 2009. Random acyclic networks. Phys. Rev. Lett. 102, 12, 128701.
KARYPIS, G. AND KUMAR, V. 1998. A fast and high quality multilevel scheme for partitioning irregular graphs.

SIAM J. Sci. Comput. 20, 1, 359–392.

KERNIGHAN, B. AND LIN, S. 1970. An efﬁcient heuristic procedure for partitioning graphs. Bell Sys. Techn.

J. 49, 2, 291–307.

KIRKPATRICK, S., GELATT, JR., C. D., AND VECCHI, M. P. 1983. Optimization by simulated annealing. Sci-

ence 220, 4598, 671–680.

KREBS, V. 2008. A network of books about recent US politics sold by the online bookseller amazon.com.

http://www.orgnet.com/.

LANCICHINETTI, A. AND FORTUNATO, S. 2009a. Benchmarks for testing community detection algorithms on

directed and weighted graphs with overlapping communities. Phys. Rev. E 80, 016118.

LANCICHINETTI, A. AND FORTUNATO, S. 2009b. Community detection algorithms: A comparative analysis. Phys.

Rev. E 80, 056117.

LIU, X. AND MURATA, T. 2010. Advanced modularity-specialized label propagation algorithm for detecting

communities in networks. Physica A 389, 7, 1493–1500.

L¨U, Z. AND HUANG, W. 2009. Iterated tabu search for identifying community structure in complex networks.

Phys. Rev. E 80, 026130.

LUSSEAU, D., SCHNEIDER, K., BOISSEAU, O. J., HAASE, P., SLOOTEN, E., AND DAWSON, S. M. 2003. The bottlenose
dolphin community of Doubtful Sound features a large proportion of long-lasting associations. Behav.
Ecol. Sociobiol. 54, 396–405.

MASSEN, C. P. AND DOYE, J. P. K. 2005. Identifying communities within energy landscapes. Phys. Rev. E 71,

046101.

MEDUS, A., ACU˜NA, G., AND DORSO, C. O. 2005. Detection of community structures in networks via global

optimization. Physica A 358, 2-4, 593–604.

MEI, J., HE, S., SHI, G., WANG, Z., AND LI, W. 2009. Revealing network communities through modularity

maximization by a contraction–dilation method. New J. Phys. 11, 043025.

NEWMAN, M. E. J. 2001. The structure of scientiﬁc collaboration networks. Proc. National Acad. Sci. 98, 2,

404–409.

NEWMAN, M. E. J. 2004a. Analysis of weighted networks. Phys. Rev. E 70, 056131.
NEWMAN, M. E. J. 2004b. Fast algorithm for detecting community structure in networks. Phys. Rev. E 69,

066133.

NEWMAN, M. E. J. 2006a. Finding community structure in networks using the eigenvectors of matrices. Phys.

Rev. E 74, 036104.

NEWMAN, M. E. J. 2006b. Modularity and community structure in networks. Proc. National Acad. Sci. 103, 23,

8577–8582.

NEWMAN, M. E. J. AND GIRVAN, M. 2004. Finding and evaluating community structure in networks. Phys. Rev.

E 69, 026113.

NOACK, A. 2007a. Energy models for graph clustering. J. Graph Algorithms Appl. 11, 2, 453–480.
NOACK, A. 2007b. Uniﬁed quality measures for clusterings, layouts, and orderings, and their application as

software design criteria. Ph.D. thesis, Brandenburg University of Technology.

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

Multilevel Local Search Algorithms for Modularity Clustering

2.3:27

NOACK, A. 2009. Modularity clustering is force-directed layout. Phys. Rev. E 79, 026102.
PONS, P. AND LATAPY, M. 2006. Computing communities in large networks using random walks. J. Graph

Algorithms Appl. 10, 2, 191–218.

PORTER, M. A., ONNELA, J.-P., AND MUCHA, P. J. 2009. Communities in networks. Not. Amer. Math. Soc. 56, 9,

1082–1097.

PREIS, R. AND DIEKMANN, R. 1997. PARTY—A software library for graph partitioning. B. Topping, Ed. Ad-
vances in Computational Mechanics with Parallel and Distributed Processing, Civil-Comp Press, United
Kingdom, 63–71.

PUJOL, J. M., B´EJAR, J., AND DELGADO, J. 2006. Clustering algorithm for determining community structure in

large networks. Phys. Rev. E 74, 016107.

REICHARDT, J. AND BORNHOLDT, S. 2006. Statistical mechanics of community detection. Phys. Rev. E 74, 016110.
RICHARDSON, T., MUCHA, P., AND PORTER, M. 2009. Spectral tripartitioning of networks. Phys. Rev. E 80, 036111.
ROTTA, R. 2008. A multi-level algorithm for modularity graph clustering. M.S. thesis, Brandenburg University

of Technology.

SCHAEFFER, S. E. 2007. Graph clustering. Comput. Sci. Rev. 1, 1, 27–64.
SCHUETZ, P. AND CAFLISCH, A. 2008a. Efﬁcient modularity optimization by multistep greedy algorithm and

vertex mover reﬁnement. Phys. Rev. E 77, 046112.

SCHUETZ, P. AND CAFLISCH, A. 2008b. Multistep greedy algorithm identiﬁes community structure in real-world

and computer-generated networks. Phys. Rev. E 78, 026112.

SIEK, J., LEE, L., AND LUMSDAINE, A. 2002. The Boost Graph Library: User Guide and Reference Manual.

Addison-Wesley, Upper Saddle River, NJ.

SUN, Y., DANILA, B., JOSI´C, K., AND BASSLER, K. 2009. Improved community structure detection using a modiﬁed

ﬁne-tuning strategy. EPL (Europhysics Letters) 86, 28004.

WAKITA, K. AND TSURUMI, T. 2007. Finding community structure in mega-scale social networks.

http://www2007.org/posters/poster950.pdf.

WALSHAW, C. AND CROSS, M. 2000. Mesh partitioning: A multilevel balancing and reﬁnement algorithm. SIAM

J. Sci. Comput. 22, 1, 63–80.

WHITE, S. AND SMYTH, P. 2005. A spectral clustering approach to ﬁnding communities in graphs. In Proceedings

of the 5th SIAM International Conference on Data Mining (SDM’05). SIAM, Philadelphia, 274–285.

XU, G., TSOKA, S., AND PAPAGEORGIOU, L. G. 2007. Finding community structures in complex networks using

mixed integer optimisation. Eur. Phys. J. B 60, 2, 231–239.

YE, Z., HU, S., AND YU, J. 2008. Adaptive clustering algorithm for community detection in complex networks.

Phys. Rev. E 78, 046115.

ZACHARY, W. W. 1977. An information ﬂow model for conﬂict and ﬁssion in small groups. J. Anthropol. Res. 33,

452–473.

Received November 2009; revised September 2010; accepted March 2011

ACM Journal of Experimental Algorithmics, Vol. 16, No. 2, Article 2.3, Publication date: June 2011.

