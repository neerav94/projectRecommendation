Experimenting with Algorithmic Composition Techniques

Jessica Kuo

jesskuo@stanford.edu

Horia Margarit

horia@stanford.edu

1

Introduction

Algorithmic composition, a form of artiﬁcial creativity, is not a new concept. Music is arguably the most mathematical
art form in existence and since there has been music, composers have tried to develop processes to supersede the
human creative process. Different structures were developed over time; notable examples include i) counterpoint in
the Baroque era, which dictated strict rules for organized music writing, and ii) the ubiquitous sonata rondo form in
the Classical era, which only supplied a high-level structure for an overall piece.
Pre-computer examples that could be considered algorithms include the famous Musikalisches Wrfelspiel (“musical
dice game”) implemented by W. A. Mozart, where the rolls of two six-sided dice randomly selected small sections
of music that were then patched together to create a musical piece. This game was capable of producing 1116 =
45, 949, 729, 863, 572, 161 different yet similar waltzes [1]. Music composition algorithms have evolved into three
main categories: i) aleatoric methods (e.g. Cage); ii) determinacy methods, where decisions over everything from
notes to dynamic markings were objectiﬁed to pre-composed series and matrices of values (e.g. Schoenberg, Webern,
and Berg); and iii) stochastic methods (e.g. Xenakis, Hiller). However, up to this point, the inputs still required manual
creative input from the composer.

1.1 Motivation

In the computer age, researchers have employed Markov models and artiﬁcial neural networks to further reduce the
level of manual input. The most notable software system was created by David Cope, and is called “Experiments in
Musical Intelligence” (or “EMI”) [2]. EMI uses a deconstruction method to ﬁrst analyze existing pieces of music and
separate it into parts and then second, to recombine it into a novel musical composition in the same style. This and
other similar approaches have been criticized to only superﬁcially manipulate or imitate the works of great composers
and often do not take into the more “human” aspects of music, such as spontaneous rhythms, harmonies, timbre, or
articulation. Further, most only apply to intra-genre mappings of musical sequences.
We intend to experiment with predicting the main key of a piece of music as well as the keys it may modulate to. In
music, modulation is the act of changing from one key to another within a piece — it adds interest by shifting the tonal
center. This may or may not be accompanied by a change in key signature (see Figure 1). The structures and rules of
modulation vary greatly between genres and eras. With the exclusion of twelve-tone and atonal music, knowing the
key a piece is written in is crucial for establishing tonality, which is the compositional foundation of all music. This
should assist us in overcoming some of the issues outlined above and help us further reduce the imitative processes
currently used in algorithmic compositions.

Figure 1: Excerpt from the third movement of Mozart’s Piano Sonata No. 11, K. 331 showing modulation

1

Experimenting with Algorithmic Composition Techniques

Jessica Kuo

jesskuo@stanford.edu

Horia Margarit

horia@stanford.edu

1

Introduction

Algorithmic composition, a form of artiﬁcial creativity, is not a new concept. Music is arguably the most mathematical
art form in existence and since there has been music, composers have tried to develop processes to supersede the
human creative process. Different structures were developed over time; notable examples include i) counterpoint in
the Baroque era, which dictated strict rules for organized music writing, and ii) the ubiquitous sonata rondo form in
the Classical era, which only supplied a high-level structure for an overall piece.
Pre-computer examples that could be considered algorithms include the famous Musikalisches Wrfelspiel (“musical
dice game”) implemented by W. A. Mozart, where the rolls of two six-sided dice randomly selected small sections
of music that were then patched together to create a musical piece. This game was capable of producing 1116 =
45, 949, 729, 863, 572, 161 different yet similar waltzes [1]. Music composition algorithms have evolved into three
main categories: i) aleatoric methods (e.g. Cage); ii) determinacy methods, where decisions over everything from
notes to dynamic markings were objectiﬁed to pre-composed series and matrices of values (e.g. Schoenberg, Webern,
and Berg); and iii) stochastic methods (e.g. Xenakis, Hiller). However, up to this point, the inputs still required manual
creative input from the composer.

1.1 Motivation

In the computer age, researchers have employed Markov models and artiﬁcial neural networks to further reduce the
level of manual input. The most notable software system was created by David Cope, and is called “Experiments in
Musical Intelligence” (or “EMI”) [2]. EMI uses a deconstruction method to ﬁrst analyze existing pieces of music and
separate it into parts and then second, to recombine it into a novel musical composition in the same style. This and
other similar approaches have been criticized to only superﬁcially manipulate or imitate the works of great composers
and often do not take into the more “human” aspects of music, such as spontaneous rhythms, harmonies, timbre, or
articulation. Further, most only apply to intra-genre mappings of musical sequences.
We intend to experiment with predicting the main key of a piece of music as well as the keys it may modulate to. In
music, modulation is the act of changing from one key to another within a piece — it adds interest by shifting the tonal
center. This may or may not be accompanied by a change in key signature (see Figure 1). The structures and rules of
modulation vary greatly between genres and eras. With the exclusion of twelve-tone and atonal music, knowing the
key a piece is written in is crucial for establishing tonality, which is the compositional foundation of all music. This
should assist us in overcoming some of the issues outlined above and help us further reduce the imitative processes
currently used in algorithmic compositions.

Figure 1: Excerpt from the third movement of Mozart’s Piano Sonata No. 11, K. 331 showing modulation

1

2 Features

2.1 Dataset

MIDI ﬁles carry event messages that specify features such as notation, pitch, and dynamics in numerical values. This
makes it easier for us to modify and manipulate music as data. Another advantage of MIDI is that it is compact and
can be stored in a few kilobytes.
We selected 636 samples of music from the Baroque and Classical eras in MIDI format and manually loaded them
into FL Studio, where we then checked them for accuracy and made edits wherever necessary. Each sample of music
was then separated into its respective voices (e.g. 4 for keyboard fugues from the Baroque era, 4 for string quartets, 2
for certain keyboard pieces from the Classical era where separating the data further beyond left hand / right hand may
impose sparsity, etc.). This was a time consuming process as the harmonic lines oftentimes came very close to or even
crossed over one another. We ended up with a dataset of 1,465 samples.

2.2 Features

<meta message key_jk=‘A-’>
<meta message time_signature numerator=2 denominator=4 clocks_per_click=96

notated_32nd_notes_per_beat=8 time=0>

<meta message set_tempo tempo=500000 time=0>
<meta message key_signature key=‘C’ time=0>
note_on channel=0 note=71 velocity=59 time=1144
note_off channel=0 note=71 velocity=0 time=92
note_on channel=0 note=69 velocity=59 time=4
note_off channel=0 note=69 velocity=0 time=92
note_on channel=0 note=68 velocity=59 time=4
note_off channel=0 note=68 velocity=0 time=92
note_on channel=0 note=69 velocity=59 time=4
note_off channel=0 note=69 velocity=0 time=92

The above is an excerpt of features once again taken from the third movement of Mozart’s Piano Sonata K. 331 after it
had been imported and converted in Python. Since the key signature by deﬁnition does not distinguish between major
and minor keys, we had to manually label the main key of each sample in our dataset for the purposes of error analysis.
The octave in which a certain musical note resides is irrelevant in evaluating the key and establishing tonality. There-
fore, to eliminate any extraneous noise that may result from notes that are spread out, we will normalize the note
data such that each musical note d is d ≡ n mod 12, where n ∈ {60, 61, . . . , 71}. This is done so that our model will
recognize each note, whether high or low, as one of the 12 notes from middle C to B. After that, we will take the n
values and encode them as compressed sparse column matrices for input into our model.
The technical deﬁnition of our feature is outlined in section 2.2.2 .

2.2.1 Key

There are 12 major keys and 12 minor keys that music can be written in. The tonic note and chord of a piece of music
gives the listener a subjective sense of arrival and completion.

2.2.2 Pitch

We represent the pitch of each musical note as a frequency data value d with respect to f, the frequency, where 440
Hz represents a concert A [3].

(cid:18) f

(cid:19)

440 Hz

d = 69 + 12 log2

2

Experimenting with Algorithmic Composition Techniques

Jessica Kuo

jesskuo@stanford.edu

Horia Margarit

horia@stanford.edu

1

Introduction

Algorithmic composition, a form of artiﬁcial creativity, is not a new concept. Music is arguably the most mathematical
art form in existence and since there has been music, composers have tried to develop processes to supersede the
human creative process. Different structures were developed over time; notable examples include i) counterpoint in
the Baroque era, which dictated strict rules for organized music writing, and ii) the ubiquitous sonata rondo form in
the Classical era, which only supplied a high-level structure for an overall piece.
Pre-computer examples that could be considered algorithms include the famous Musikalisches Wrfelspiel (“musical
dice game”) implemented by W. A. Mozart, where the rolls of two six-sided dice randomly selected small sections
of music that were then patched together to create a musical piece. This game was capable of producing 1116 =
45, 949, 729, 863, 572, 161 different yet similar waltzes [1]. Music composition algorithms have evolved into three
main categories: i) aleatoric methods (e.g. Cage); ii) determinacy methods, where decisions over everything from
notes to dynamic markings were objectiﬁed to pre-composed series and matrices of values (e.g. Schoenberg, Webern,
and Berg); and iii) stochastic methods (e.g. Xenakis, Hiller). However, up to this point, the inputs still required manual
creative input from the composer.

1.1 Motivation

In the computer age, researchers have employed Markov models and artiﬁcial neural networks to further reduce the
level of manual input. The most notable software system was created by David Cope, and is called “Experiments in
Musical Intelligence” (or “EMI”) [2]. EMI uses a deconstruction method to ﬁrst analyze existing pieces of music and
separate it into parts and then second, to recombine it into a novel musical composition in the same style. This and
other similar approaches have been criticized to only superﬁcially manipulate or imitate the works of great composers
and often do not take into the more “human” aspects of music, such as spontaneous rhythms, harmonies, timbre, or
articulation. Further, most only apply to intra-genre mappings of musical sequences.
We intend to experiment with predicting the main key of a piece of music as well as the keys it may modulate to. In
music, modulation is the act of changing from one key to another within a piece — it adds interest by shifting the tonal
center. This may or may not be accompanied by a change in key signature (see Figure 1). The structures and rules of
modulation vary greatly between genres and eras. With the exclusion of twelve-tone and atonal music, knowing the
key a piece is written in is crucial for establishing tonality, which is the compositional foundation of all music. This
should assist us in overcoming some of the issues outlined above and help us further reduce the imitative processes
currently used in algorithmic compositions.

Figure 1: Excerpt from the third movement of Mozart’s Piano Sonata No. 11, K. 331 showing modulation

1

2 Features

2.1 Dataset

MIDI ﬁles carry event messages that specify features such as notation, pitch, and dynamics in numerical values. This
makes it easier for us to modify and manipulate music as data. Another advantage of MIDI is that it is compact and
can be stored in a few kilobytes.
We selected 636 samples of music from the Baroque and Classical eras in MIDI format and manually loaded them
into FL Studio, where we then checked them for accuracy and made edits wherever necessary. Each sample of music
was then separated into its respective voices (e.g. 4 for keyboard fugues from the Baroque era, 4 for string quartets, 2
for certain keyboard pieces from the Classical era where separating the data further beyond left hand / right hand may
impose sparsity, etc.). This was a time consuming process as the harmonic lines oftentimes came very close to or even
crossed over one another. We ended up with a dataset of 1,465 samples.

2.2 Features

<meta message key_jk=‘A-’>
<meta message time_signature numerator=2 denominator=4 clocks_per_click=96

notated_32nd_notes_per_beat=8 time=0>

<meta message set_tempo tempo=500000 time=0>
<meta message key_signature key=‘C’ time=0>
note_on channel=0 note=71 velocity=59 time=1144
note_off channel=0 note=71 velocity=0 time=92
note_on channel=0 note=69 velocity=59 time=4
note_off channel=0 note=69 velocity=0 time=92
note_on channel=0 note=68 velocity=59 time=4
note_off channel=0 note=68 velocity=0 time=92
note_on channel=0 note=69 velocity=59 time=4
note_off channel=0 note=69 velocity=0 time=92

The above is an excerpt of features once again taken from the third movement of Mozart’s Piano Sonata K. 331 after it
had been imported and converted in Python. Since the key signature by deﬁnition does not distinguish between major
and minor keys, we had to manually label the main key of each sample in our dataset for the purposes of error analysis.
The octave in which a certain musical note resides is irrelevant in evaluating the key and establishing tonality. There-
fore, to eliminate any extraneous noise that may result from notes that are spread out, we will normalize the note
data such that each musical note d is d ≡ n mod 12, where n ∈ {60, 61, . . . , 71}. This is done so that our model will
recognize each note, whether high or low, as one of the 12 notes from middle C to B. After that, we will take the n
values and encode them as compressed sparse column matrices for input into our model.
The technical deﬁnition of our feature is outlined in section 2.2.2 .

2.2.1 Key

There are 12 major keys and 12 minor keys that music can be written in. The tonic note and chord of a piece of music
gives the listener a subjective sense of arrival and completion.

2.2.2 Pitch

We represent the pitch of each musical note as a frequency data value d with respect to f, the frequency, where 440
Hz represents a concert A [3].

(cid:18) f

(cid:19)

440 Hz

d = 69 + 12 log2

2

3 Methods

3.1 Model

We have constructed a generative model deﬁned by the process and parameters below. The random variable X denotes
the observed consecutive triplet of musical notes within any given sample with probability θ and where X (i) is some
instantiation of X, encoded as a 1-of-K columnar vector of dimension K with exactly one element of value 1 and all
other elements of value 0. Further details are included in section 3.2 .

K(cid:89)

∝ K(cid:89)

1

Beta(α)

θ αj−1

j

θ αj−1

j

j=1

j=1

θ = (θ1, . . . , θK) ∼ Dirichlet (α)

=

X (i)(cid:12)(cid:12) θ =

(cid:16)

x(i)
1 , . . . , x(i)

K

(cid:17) ∼ Categorical (θ ) =

K(cid:89)

j=1

x(i)
j

j

θ

K(cid:89)

3.2 Deriving the Conjugate Prior
Let X ∼ Categorical(θ) with x(i) being some instantiation of the random variable, encoded as a 1-of-K vector such
that it is columnar with dimension K, has exactly one element of the value 1 with all other elements of value 0. This
allows us to write:

log

K(cid:89)

 = exp

 K(cid:88)

j

x(i)
j

 = exp

 K(cid:88)

j

x(i)
j

p(x(i)| θ) =

j

x(i)
j = exp

This can be expressed in the general form of an exponential distribution: h(x(i)) exp(cid:0)ηT T (x(i)) − A(η)(cid:1) where

log θj

log θ

j=1

j=1

j=1

j=1

θ

θ

x(i)
j

h(x(i)) = 1, ηT = logT (θ), T (x(i)) = x(i), and A(η) = 0.
Now let θ ∼ Dirichlet(α) with any instantiation of the random vector θ being a K-dimensional vector of reals, such
that the jth element yields the probability of the categorical variable X taking on value j. The sum of an instantiation
of θ is therefore always 1.



Here, x = α and ηT = logT (θ).
Now let us compute the joint probability using what we have derived so far.

p(θ; α) =

Beta(α)

1

j

j

j=1

θ αj−1

∝ K(cid:89)
K(cid:89)
 = exp
log
K(cid:89)
 K(cid:88)
αj log θj − K(cid:88)

θ αj−1

j=1

j=1

j

j=1

j=1

log θj

= exp

= exp

p(x(1), . . . , x(m), θ ; α) ∝ exp(cid:0)ηT x(cid:1) m(cid:89)
(cid:32)
m(cid:88)

(cid:32)

(cid:32)

= exp

ηT

i=1

x +

= p

θ ; α +

θ αj−1

j

j=1

log θ αj−1

 K(cid:88)
 = exp(cid:0)ηT x(cid:1)
(cid:17)
(cid:16)
(cid:33)(cid:33)
m(cid:88)
(cid:33)

ηT T (x(i))

T (x(i))

i=1

exp

i=1

x(i)

3

 = exp

 K(cid:88)

(αj − 1) log θj



j=1

= exp(cid:0)ηT x(cid:1) exp
(cid:32)
m(cid:88)

θ ; α +

= p

m(cid:88)

(cid:16)

i=1

(cid:17)

ηT T (x(i))

(cid:33)

T (x(i))

i=1

Experimenting with Algorithmic Composition Techniques

Jessica Kuo

jesskuo@stanford.edu

Horia Margarit

horia@stanford.edu

1

Introduction

Algorithmic composition, a form of artiﬁcial creativity, is not a new concept. Music is arguably the most mathematical
art form in existence and since there has been music, composers have tried to develop processes to supersede the
human creative process. Different structures were developed over time; notable examples include i) counterpoint in
the Baroque era, which dictated strict rules for organized music writing, and ii) the ubiquitous sonata rondo form in
the Classical era, which only supplied a high-level structure for an overall piece.
Pre-computer examples that could be considered algorithms include the famous Musikalisches Wrfelspiel (“musical
dice game”) implemented by W. A. Mozart, where the rolls of two six-sided dice randomly selected small sections
of music that were then patched together to create a musical piece. This game was capable of producing 1116 =
45, 949, 729, 863, 572, 161 different yet similar waltzes [1]. Music composition algorithms have evolved into three
main categories: i) aleatoric methods (e.g. Cage); ii) determinacy methods, where decisions over everything from
notes to dynamic markings were objectiﬁed to pre-composed series and matrices of values (e.g. Schoenberg, Webern,
and Berg); and iii) stochastic methods (e.g. Xenakis, Hiller). However, up to this point, the inputs still required manual
creative input from the composer.

1.1 Motivation

In the computer age, researchers have employed Markov models and artiﬁcial neural networks to further reduce the
level of manual input. The most notable software system was created by David Cope, and is called “Experiments in
Musical Intelligence” (or “EMI”) [2]. EMI uses a deconstruction method to ﬁrst analyze existing pieces of music and
separate it into parts and then second, to recombine it into a novel musical composition in the same style. This and
other similar approaches have been criticized to only superﬁcially manipulate or imitate the works of great composers
and often do not take into the more “human” aspects of music, such as spontaneous rhythms, harmonies, timbre, or
articulation. Further, most only apply to intra-genre mappings of musical sequences.
We intend to experiment with predicting the main key of a piece of music as well as the keys it may modulate to. In
music, modulation is the act of changing from one key to another within a piece — it adds interest by shifting the tonal
center. This may or may not be accompanied by a change in key signature (see Figure 1). The structures and rules of
modulation vary greatly between genres and eras. With the exclusion of twelve-tone and atonal music, knowing the
key a piece is written in is crucial for establishing tonality, which is the compositional foundation of all music. This
should assist us in overcoming some of the issues outlined above and help us further reduce the imitative processes
currently used in algorithmic compositions.

Figure 1: Excerpt from the third movement of Mozart’s Piano Sonata No. 11, K. 331 showing modulation

1

2 Features

2.1 Dataset

MIDI ﬁles carry event messages that specify features such as notation, pitch, and dynamics in numerical values. This
makes it easier for us to modify and manipulate music as data. Another advantage of MIDI is that it is compact and
can be stored in a few kilobytes.
We selected 636 samples of music from the Baroque and Classical eras in MIDI format and manually loaded them
into FL Studio, where we then checked them for accuracy and made edits wherever necessary. Each sample of music
was then separated into its respective voices (e.g. 4 for keyboard fugues from the Baroque era, 4 for string quartets, 2
for certain keyboard pieces from the Classical era where separating the data further beyond left hand / right hand may
impose sparsity, etc.). This was a time consuming process as the harmonic lines oftentimes came very close to or even
crossed over one another. We ended up with a dataset of 1,465 samples.

2.2 Features

<meta message key_jk=‘A-’>
<meta message time_signature numerator=2 denominator=4 clocks_per_click=96

notated_32nd_notes_per_beat=8 time=0>

<meta message set_tempo tempo=500000 time=0>
<meta message key_signature key=‘C’ time=0>
note_on channel=0 note=71 velocity=59 time=1144
note_off channel=0 note=71 velocity=0 time=92
note_on channel=0 note=69 velocity=59 time=4
note_off channel=0 note=69 velocity=0 time=92
note_on channel=0 note=68 velocity=59 time=4
note_off channel=0 note=68 velocity=0 time=92
note_on channel=0 note=69 velocity=59 time=4
note_off channel=0 note=69 velocity=0 time=92

The above is an excerpt of features once again taken from the third movement of Mozart’s Piano Sonata K. 331 after it
had been imported and converted in Python. Since the key signature by deﬁnition does not distinguish between major
and minor keys, we had to manually label the main key of each sample in our dataset for the purposes of error analysis.
The octave in which a certain musical note resides is irrelevant in evaluating the key and establishing tonality. There-
fore, to eliminate any extraneous noise that may result from notes that are spread out, we will normalize the note
data such that each musical note d is d ≡ n mod 12, where n ∈ {60, 61, . . . , 71}. This is done so that our model will
recognize each note, whether high or low, as one of the 12 notes from middle C to B. After that, we will take the n
values and encode them as compressed sparse column matrices for input into our model.
The technical deﬁnition of our feature is outlined in section 2.2.2 .

2.2.1 Key

There are 12 major keys and 12 minor keys that music can be written in. The tonic note and chord of a piece of music
gives the listener a subjective sense of arrival and completion.

2.2.2 Pitch

We represent the pitch of each musical note as a frequency data value d with respect to f, the frequency, where 440
Hz represents a concert A [3].

(cid:18) f

(cid:19)

440 Hz

d = 69 + 12 log2

2

3 Methods

3.1 Model

We have constructed a generative model deﬁned by the process and parameters below. The random variable X denotes
the observed consecutive triplet of musical notes within any given sample with probability θ and where X (i) is some
instantiation of X, encoded as a 1-of-K columnar vector of dimension K with exactly one element of value 1 and all
other elements of value 0. Further details are included in section 3.2 .

K(cid:89)

∝ K(cid:89)

1

Beta(α)

θ αj−1

j

θ αj−1

j

j=1

j=1

θ = (θ1, . . . , θK) ∼ Dirichlet (α)

=

X (i)(cid:12)(cid:12) θ =

(cid:16)

x(i)
1 , . . . , x(i)

K

(cid:17) ∼ Categorical (θ ) =

K(cid:89)

j=1

x(i)
j

j

θ

K(cid:89)

3.2 Deriving the Conjugate Prior
Let X ∼ Categorical(θ) with x(i) being some instantiation of the random variable, encoded as a 1-of-K vector such
that it is columnar with dimension K, has exactly one element of the value 1 with all other elements of value 0. This
allows us to write:

log

K(cid:89)

 = exp

 K(cid:88)

j

x(i)
j

 = exp

 K(cid:88)

j

x(i)
j

p(x(i)| θ) =

j

x(i)
j = exp

This can be expressed in the general form of an exponential distribution: h(x(i)) exp(cid:0)ηT T (x(i)) − A(η)(cid:1) where

log θj

log θ

j=1

j=1

j=1

j=1

θ

θ

x(i)
j

h(x(i)) = 1, ηT = logT (θ), T (x(i)) = x(i), and A(η) = 0.
Now let θ ∼ Dirichlet(α) with any instantiation of the random vector θ being a K-dimensional vector of reals, such
that the jth element yields the probability of the categorical variable X taking on value j. The sum of an instantiation
of θ is therefore always 1.



Here, x = α and ηT = logT (θ).
Now let us compute the joint probability using what we have derived so far.

p(θ; α) =

Beta(α)

1

j

j

j=1

θ αj−1

∝ K(cid:89)
K(cid:89)
 = exp
log
K(cid:89)
 K(cid:88)
αj log θj − K(cid:88)

θ αj−1

j=1

j=1

j

j=1

j=1

log θj

= exp

= exp

p(x(1), . . . , x(m), θ ; α) ∝ exp(cid:0)ηT x(cid:1) m(cid:89)
(cid:32)
m(cid:88)

(cid:32)

(cid:32)

= exp

ηT

i=1

x +

= p

θ ; α +

θ αj−1

j

j=1

log θ αj−1

 K(cid:88)
 = exp(cid:0)ηT x(cid:1)
(cid:17)
(cid:16)
(cid:33)(cid:33)
m(cid:88)
(cid:33)

ηT T (x(i))

T (x(i))

i=1

exp

i=1

x(i)

3

 = exp

 K(cid:88)

(αj − 1) log θj



j=1

= exp(cid:0)ηT x(cid:1) exp
(cid:32)
m(cid:88)

θ ; α +

= p

m(cid:88)

(cid:16)

i=1

(cid:17)

ηT T (x(i))

(cid:33)

T (x(i))

i=1

But observe that by Bayes’ Rule, the posterior p(θ | . . . , x(i), . . . ; α) is proportional to the joint probability we just
calculated. They are in the same family and therefore, the prior and posterior are conjugate distributions, which makes
the prior a conjugate prior. Note that “updating the prior” speciﬁcally amounts to the model updating its belief about
the distribution of θ on every pass through of the data.

3.3 Parameter Estimates

We use the EM Algorithm on this model to compute the parameters of the latent variables.

3.3.1 E-Step

3.3.2 M-Step

4 Conclusion

q(t)(θ(cid:12)(cid:12) x(i)) := p(cid:0)θ(cid:12)(cid:12) x(i); α(t−1)(cid:1)
m(cid:88)

θ(cid:12)(cid:12) x(i)(cid:17) K(cid:88)

(cid:16)

(cid:16)

x(i)
j

q

i=1

j=1

θ(t) := arg max

θ

log θj + αj log θj

(cid:17)

One of the biggest hurdles in our modelling came into play when we realized that our original dataset lacked the crucial
features for the learning algorithm we originally set out to build. The other big hurdle occurred when we attempted to
model the parameter of the categorical distribution over X as being dependent on the central key of the musical piece.
The Dirichlet conjugate prior over the parameter of the Categorical does not account for the hidden state — the central
key — when it updates the frequency counts for the α parameter. This entails that our model and EM algorithm will
converge on the maximum incomplete log likelihood that satisﬁes any central key. Intuitively, this maximum is less
than the maximum that would have been obtained if it were conditioned on the central key.
Our current remedy to this problem is to use the fact that we arduously labelled the central keys of 636 musical pieces,
which enables us to group the labelled musical pieces by central key. Then we could plate the graphical model (see
section 3.1 ) such that we run one instance of the model on each group of musical pieces. The end result, and the goal
of this entire endeavor, is to use EM and Bayesian inference to learn the optimal probability distribution of musical
notes, given a particular central key of the musical score. To conclude, this endeavor provides us with the ability to
run the model on data which is unlabeled and to select the central key under which the observed data is most likely.

5 Appendix

5.1 Deriving EM

(cid:16)

x(i)(cid:17)

p

m(cid:89)

i=1

log

= log

=

i=1

= log

m(cid:88)
m(cid:88)
≥ m(cid:88)

i=1

=

i=1

4

(cid:16)

θ

i=1

(cid:90)
m(cid:89)
(cid:32) m(cid:89)
(cid:90)
(cid:32)(cid:90)
(cid:32)

log

i=1

θ

θ

p

x(i), θ ; α

q(θ | x(i))

q(θ | x(i))

log

Eq(θ | x(i))

(cid:34)

Eq(θ | x(i))

log

(cid:17)
q(cid:0)θ | x(i)(cid:1) (cid:33)
p(cid:0)x(i), θ ; α(cid:1)
q(cid:0)θ | x(i)(cid:1) (cid:33)
p(cid:0)x(i), θ ; α(cid:1)
q(cid:0)θ | x(i)(cid:1) (cid:35)(cid:33)
(cid:34)
p(cid:0)x(i), θ ; α(cid:1)
q(cid:0)θ | x(i)(cid:1) (cid:35)
p(cid:0)x(i), θ ; α(cid:1)

Experimenting with Algorithmic Composition Techniques

Jessica Kuo

jesskuo@stanford.edu

Horia Margarit

horia@stanford.edu

1

Introduction

Algorithmic composition, a form of artiﬁcial creativity, is not a new concept. Music is arguably the most mathematical
art form in existence and since there has been music, composers have tried to develop processes to supersede the
human creative process. Different structures were developed over time; notable examples include i) counterpoint in
the Baroque era, which dictated strict rules for organized music writing, and ii) the ubiquitous sonata rondo form in
the Classical era, which only supplied a high-level structure for an overall piece.
Pre-computer examples that could be considered algorithms include the famous Musikalisches Wrfelspiel (“musical
dice game”) implemented by W. A. Mozart, where the rolls of two six-sided dice randomly selected small sections
of music that were then patched together to create a musical piece. This game was capable of producing 1116 =
45, 949, 729, 863, 572, 161 different yet similar waltzes [1]. Music composition algorithms have evolved into three
main categories: i) aleatoric methods (e.g. Cage); ii) determinacy methods, where decisions over everything from
notes to dynamic markings were objectiﬁed to pre-composed series and matrices of values (e.g. Schoenberg, Webern,
and Berg); and iii) stochastic methods (e.g. Xenakis, Hiller). However, up to this point, the inputs still required manual
creative input from the composer.

1.1 Motivation

In the computer age, researchers have employed Markov models and artiﬁcial neural networks to further reduce the
level of manual input. The most notable software system was created by David Cope, and is called “Experiments in
Musical Intelligence” (or “EMI”) [2]. EMI uses a deconstruction method to ﬁrst analyze existing pieces of music and
separate it into parts and then second, to recombine it into a novel musical composition in the same style. This and
other similar approaches have been criticized to only superﬁcially manipulate or imitate the works of great composers
and often do not take into the more “human” aspects of music, such as spontaneous rhythms, harmonies, timbre, or
articulation. Further, most only apply to intra-genre mappings of musical sequences.
We intend to experiment with predicting the main key of a piece of music as well as the keys it may modulate to. In
music, modulation is the act of changing from one key to another within a piece — it adds interest by shifting the tonal
center. This may or may not be accompanied by a change in key signature (see Figure 1). The structures and rules of
modulation vary greatly between genres and eras. With the exclusion of twelve-tone and atonal music, knowing the
key a piece is written in is crucial for establishing tonality, which is the compositional foundation of all music. This
should assist us in overcoming some of the issues outlined above and help us further reduce the imitative processes
currently used in algorithmic compositions.

Figure 1: Excerpt from the third movement of Mozart’s Piano Sonata No. 11, K. 331 showing modulation

1

2 Features

2.1 Dataset

MIDI ﬁles carry event messages that specify features such as notation, pitch, and dynamics in numerical values. This
makes it easier for us to modify and manipulate music as data. Another advantage of MIDI is that it is compact and
can be stored in a few kilobytes.
We selected 636 samples of music from the Baroque and Classical eras in MIDI format and manually loaded them
into FL Studio, where we then checked them for accuracy and made edits wherever necessary. Each sample of music
was then separated into its respective voices (e.g. 4 for keyboard fugues from the Baroque era, 4 for string quartets, 2
for certain keyboard pieces from the Classical era where separating the data further beyond left hand / right hand may
impose sparsity, etc.). This was a time consuming process as the harmonic lines oftentimes came very close to or even
crossed over one another. We ended up with a dataset of 1,465 samples.

2.2 Features

<meta message key_jk=‘A-’>
<meta message time_signature numerator=2 denominator=4 clocks_per_click=96

notated_32nd_notes_per_beat=8 time=0>

<meta message set_tempo tempo=500000 time=0>
<meta message key_signature key=‘C’ time=0>
note_on channel=0 note=71 velocity=59 time=1144
note_off channel=0 note=71 velocity=0 time=92
note_on channel=0 note=69 velocity=59 time=4
note_off channel=0 note=69 velocity=0 time=92
note_on channel=0 note=68 velocity=59 time=4
note_off channel=0 note=68 velocity=0 time=92
note_on channel=0 note=69 velocity=59 time=4
note_off channel=0 note=69 velocity=0 time=92

The above is an excerpt of features once again taken from the third movement of Mozart’s Piano Sonata K. 331 after it
had been imported and converted in Python. Since the key signature by deﬁnition does not distinguish between major
and minor keys, we had to manually label the main key of each sample in our dataset for the purposes of error analysis.
The octave in which a certain musical note resides is irrelevant in evaluating the key and establishing tonality. There-
fore, to eliminate any extraneous noise that may result from notes that are spread out, we will normalize the note
data such that each musical note d is d ≡ n mod 12, where n ∈ {60, 61, . . . , 71}. This is done so that our model will
recognize each note, whether high or low, as one of the 12 notes from middle C to B. After that, we will take the n
values and encode them as compressed sparse column matrices for input into our model.
The technical deﬁnition of our feature is outlined in section 2.2.2 .

2.2.1 Key

There are 12 major keys and 12 minor keys that music can be written in. The tonic note and chord of a piece of music
gives the listener a subjective sense of arrival and completion.

2.2.2 Pitch

We represent the pitch of each musical note as a frequency data value d with respect to f, the frequency, where 440
Hz represents a concert A [3].

(cid:18) f

(cid:19)

440 Hz

d = 69 + 12 log2

2

3 Methods

3.1 Model

We have constructed a generative model deﬁned by the process and parameters below. The random variable X denotes
the observed consecutive triplet of musical notes within any given sample with probability θ and where X (i) is some
instantiation of X, encoded as a 1-of-K columnar vector of dimension K with exactly one element of value 1 and all
other elements of value 0. Further details are included in section 3.2 .

K(cid:89)

∝ K(cid:89)

1

Beta(α)

θ αj−1

j

θ αj−1

j

j=1

j=1

θ = (θ1, . . . , θK) ∼ Dirichlet (α)

=

X (i)(cid:12)(cid:12) θ =

(cid:16)

x(i)
1 , . . . , x(i)

K

(cid:17) ∼ Categorical (θ ) =

K(cid:89)

j=1

x(i)
j

j

θ

K(cid:89)

3.2 Deriving the Conjugate Prior
Let X ∼ Categorical(θ) with x(i) being some instantiation of the random variable, encoded as a 1-of-K vector such
that it is columnar with dimension K, has exactly one element of the value 1 with all other elements of value 0. This
allows us to write:

log

K(cid:89)

 = exp

 K(cid:88)

j

x(i)
j

 = exp

 K(cid:88)

j

x(i)
j

p(x(i)| θ) =

j

x(i)
j = exp

This can be expressed in the general form of an exponential distribution: h(x(i)) exp(cid:0)ηT T (x(i)) − A(η)(cid:1) where

log θj

log θ

j=1

j=1

j=1

j=1

θ

θ

x(i)
j

h(x(i)) = 1, ηT = logT (θ), T (x(i)) = x(i), and A(η) = 0.
Now let θ ∼ Dirichlet(α) with any instantiation of the random vector θ being a K-dimensional vector of reals, such
that the jth element yields the probability of the categorical variable X taking on value j. The sum of an instantiation
of θ is therefore always 1.



Here, x = α and ηT = logT (θ).
Now let us compute the joint probability using what we have derived so far.

p(θ; α) =

Beta(α)

1

j

j

j=1

θ αj−1

∝ K(cid:89)
K(cid:89)
 = exp
log
K(cid:89)
 K(cid:88)
αj log θj − K(cid:88)

θ αj−1

j=1

j=1

j

j=1

j=1

log θj

= exp

= exp

p(x(1), . . . , x(m), θ ; α) ∝ exp(cid:0)ηT x(cid:1) m(cid:89)
(cid:32)
m(cid:88)

(cid:32)

(cid:32)

= exp

ηT

i=1

x +

= p

θ ; α +

θ αj−1

j

j=1

log θ αj−1

 K(cid:88)
 = exp(cid:0)ηT x(cid:1)
(cid:17)
(cid:16)
(cid:33)(cid:33)
m(cid:88)
(cid:33)

ηT T (x(i))

T (x(i))

i=1

exp

i=1

x(i)

3

 = exp

 K(cid:88)

(αj − 1) log θj



j=1

= exp(cid:0)ηT x(cid:1) exp
(cid:32)
m(cid:88)

θ ; α +

= p

m(cid:88)

(cid:16)

i=1

(cid:17)

ηT T (x(i))

(cid:33)

T (x(i))

i=1

But observe that by Bayes’ Rule, the posterior p(θ | . . . , x(i), . . . ; α) is proportional to the joint probability we just
calculated. They are in the same family and therefore, the prior and posterior are conjugate distributions, which makes
the prior a conjugate prior. Note that “updating the prior” speciﬁcally amounts to the model updating its belief about
the distribution of θ on every pass through of the data.

3.3 Parameter Estimates

We use the EM Algorithm on this model to compute the parameters of the latent variables.

3.3.1 E-Step

3.3.2 M-Step

4 Conclusion

q(t)(θ(cid:12)(cid:12) x(i)) := p(cid:0)θ(cid:12)(cid:12) x(i); α(t−1)(cid:1)
m(cid:88)

θ(cid:12)(cid:12) x(i)(cid:17) K(cid:88)

(cid:16)

(cid:16)

x(i)
j

q

i=1

j=1

θ(t) := arg max

θ

log θj + αj log θj

(cid:17)

One of the biggest hurdles in our modelling came into play when we realized that our original dataset lacked the crucial
features for the learning algorithm we originally set out to build. The other big hurdle occurred when we attempted to
model the parameter of the categorical distribution over X as being dependent on the central key of the musical piece.
The Dirichlet conjugate prior over the parameter of the Categorical does not account for the hidden state — the central
key — when it updates the frequency counts for the α parameter. This entails that our model and EM algorithm will
converge on the maximum incomplete log likelihood that satisﬁes any central key. Intuitively, this maximum is less
than the maximum that would have been obtained if it were conditioned on the central key.
Our current remedy to this problem is to use the fact that we arduously labelled the central keys of 636 musical pieces,
which enables us to group the labelled musical pieces by central key. Then we could plate the graphical model (see
section 3.1 ) such that we run one instance of the model on each group of musical pieces. The end result, and the goal
of this entire endeavor, is to use EM and Bayesian inference to learn the optimal probability distribution of musical
notes, given a particular central key of the musical score. To conclude, this endeavor provides us with the ability to
run the model on data which is unlabeled and to select the central key under which the observed data is most likely.

5 Appendix

5.1 Deriving EM

(cid:16)

x(i)(cid:17)

p

m(cid:89)

i=1

log

= log

=

i=1

= log

m(cid:88)
m(cid:88)
≥ m(cid:88)

i=1

=

i=1

4

(cid:16)

θ

i=1

(cid:90)
m(cid:89)
(cid:32) m(cid:89)
(cid:90)
(cid:32)(cid:90)
(cid:32)

log

i=1

θ

θ

p

x(i), θ ; α

q(θ | x(i))

q(θ | x(i))

log

Eq(θ | x(i))

(cid:34)

Eq(θ | x(i))

log

(cid:17)
q(cid:0)θ | x(i)(cid:1) (cid:33)
p(cid:0)x(i), θ ; α(cid:1)
q(cid:0)θ | x(i)(cid:1) (cid:33)
p(cid:0)x(i), θ ; α(cid:1)
q(cid:0)θ | x(i)(cid:1) (cid:35)(cid:33)
(cid:34)
p(cid:0)x(i), θ ; α(cid:1)
q(cid:0)θ | x(i)(cid:1) (cid:35)
p(cid:0)x(i), θ ; α(cid:1)

The inequality above results directly from Jensen’s Inequality. Setting q(cid:0)θ | x(i)(cid:1) := p(cid:0)θ | x(i) ; α(cid:1) results in equality

[4]. We therefore have a lower bound on the incomplete log likelihood that is given by:

(cid:90)

m(cid:88)

i=1

θ

(cid:16)

θ | x(i)(cid:17)

q

log

(cid:90)
(cid:90)

m(cid:88)
m(cid:88)

i=1

i=1

θ

θ

q

(cid:17)

=

=

i=1

m(cid:89)
p(cid:0)x(i)(cid:1)
 K(cid:88)

x(i)
j

j=1

p(cid:0)x(i), θ ; α(cid:1)
q(cid:0)θ | x(i)(cid:1) ≤ log
exp
(cid:16)

θ(cid:12)(cid:12) x(i)(cid:17)
(cid:16)
θ(cid:12)(cid:12) x(i)(cid:17) K(cid:88)
(cid:16)
(cid:16)

x(i)
j

log

j=1

q

log θj + αj log θj

 K(cid:88)

j=1

 exp
(cid:17)



log θj

αj log θj

By recognizing that the denominator of the term inside the log is not a function of any of the parameters, as it is held
constant during the M-Step, the overall expression to be maximized during the M-Step therefore reduces to:

(cid:90)

m(cid:88)

i=1

θ

(cid:16)

θ(cid:12)(cid:12) x(i)(cid:17)

q

(cid:16)

p(x(i) | θ) p(θ; α)

log

Since we will be taking the derivative with respect to θ, the integral cancels out and we end up having to maximize:

m(cid:88)

(cid:16)

θ(cid:12)(cid:12) x(i)(cid:17) K(cid:88)

q

(cid:17)

x(i)
j

log θj + αj log θj

References

i=1

j=1

[1] Zbikowski, Lawrence M. (2002). Conceptualizing Music: Cognitive Structure, Theory, and Analysis, pp. 142-143. New York:
Oxford University Press.
[2] Cope, David (2006). Computer Models of Musical Creativity. Cambridge, MA: MIT Press.
[3] Midi Manufacturers Association, Midi Tuning Speciﬁcation. Retrieved from http://www.midi.org/techspecs/
midituning.php.
[4] Ng, Andrew (2015). CS229 Lecture Notes: The EM Algorithm. Retrieved from http://cs229.stanford.edu/notes/
cs229-notes8.pdf.
[5] Marxer, Ricard & Purwins, Hendrik (2010). Unsupervised Incremental Learning and Prediction of Music Signals. Sydney,
Australia: ISMA.
[6] Kosta, K., Marchini, M. & Purwins, H. (2012). Unsupervised Chord-Sequence Generation from an Audio Sample. Porto,
Portugal: ISMIR.
[7] Marchini, Marco & Purwins, H. (2010). Unsupervised Generation of Percussion Sound Sequences from a Sound Example. In
Sound and Music Computing Conference (Vol. 220).

5

