Twitter Classiﬁcation into the Amazon Browse Node Hierarchy

Matthew Long, Jiao Yu, Anshul Kundani

Stanford University

December 12, 2014

Abstract

In this project, we attempt to classify tweets
into root categories of the Amazon browse node
hierarchy using a set of tweets with browse node
ID labels, a much larger set of tweets without la-
bels, and a set of Amazon reviews. Examining
twitter data presents unique challenges in that the
samples are short (under 140 characters) and of-
ten contain misspellings or abbreviations that are
trivial for a human to decipher but difﬁcult for a
computer to parse. A variety of query and docu-
ment expansion techniques are implemented in an
effort to improve information retrieval to modest
success.

1 Introduction

Internet users post information regarding a topic on a
number of different websites, but companies and organi-
zations typically only train their classiﬁcation algorithms
using only the information posted on their own platform.
Obviously, data from competitors is often difﬁcult to ac-
quire, but in cases where it is freely available, cross-
platform analysis can only beneﬁt a model as data from
other sources can be used only if it improves performance.
In order for this data to be valuable, it has to be correctly
classiﬁed by what it refers to.

The goal of this project is to to ﬁnd a likely product
category within the root categories of the Amazon browse
node hierarchy for a given tweet. Twitter data consisted
of a training dataset with 58,000 tweets labeled with Ama-
zon browse node IDs, and a much larger set of 15,000,000
unlabeled tweets that can be used for augmentation. The
Amazon data consisted of 1,900,000 reviews for products
labeled by their browse node ID. All of the datasets orig-
inally were in JSON format and contained metadata as
well as text content for each review or tweet. To obtain
root nodes for tweets, a browse node ID tree was created
so that a simple parent traversal could identify a root cat-
egory. The Amazon product hierarchy is more of a di-

1

rected graph in that it children categories can have multi-
ple parents. In these cases, the parent is chosen randomly.
28 root categories were identiﬁed from the browse nodes
within the labeled dataset, but the distribution was heav-
ily skewed, with 47,000 tweets in the books root category,
and 10 or fewer in 5 categories. Furthermore, over half
of the tweets were re-tweets, which have the same text
content as the original tweet, providing no additional in-
formation to a classiﬁer while inﬂating accuracy mislead-
ingly. Once re-tweets and tweets from categories with
fewer than 5 tweets were removed, the labeled corpus con-
tained 23,910 tweets from 24 root categories.

Amazon Category
Books
Home & Kitchen
Clothing, Shoes & Jewelry
Movies & TV
Electronics

Tweets
19531
1332
647
643
403

Common Keywords
’free’, ’ebook’, ’kindle’
’clock’, ’lp’, ’wall’
’woman’, ’dress’, ’skirt’
’dvd’, ’video’, ’instant’
’apple’, ’hd’, ’sony’

Table 1: Top 5 categories by number of tweets

2 Method
2.1 Preprocessing
As the data consists of text strings, a bag-of-words model
was used to represent the tweets. To reduce feature
size and trim unhelpful data, all the tweets were con-
verted to lower case and stripped of all punctuation ex-
cept hashtags. Additionally, URLs and stop words from
both a list within the Natural Language Toolkit and a
list we developed speciﬁcally for Twitter were removed
and words were stemmed with the WordNet Lemma-
tizer (1)(2). With 5-fold cross validation, corresponding
to an 80/20 training/testing split, the unprocessed tweets
had 48,000 unique words, which got truncated to 22,213
words after pre-processing. Text was then transformed
to a sparse matrix representation of TF-IDF features in
order to be acceptable for downstream estimators. This

Twitter Classiﬁcation into the Amazon Browse Node Hierarchy

Matthew Long, Jiao Yu, Anshul Kundani

Stanford University

December 12, 2014

Abstract

In this project, we attempt to classify tweets
into root categories of the Amazon browse node
hierarchy using a set of tweets with browse node
ID labels, a much larger set of tweets without la-
bels, and a set of Amazon reviews. Examining
twitter data presents unique challenges in that the
samples are short (under 140 characters) and of-
ten contain misspellings or abbreviations that are
trivial for a human to decipher but difﬁcult for a
computer to parse. A variety of query and docu-
ment expansion techniques are implemented in an
effort to improve information retrieval to modest
success.

1 Introduction

Internet users post information regarding a topic on a
number of different websites, but companies and organi-
zations typically only train their classiﬁcation algorithms
using only the information posted on their own platform.
Obviously, data from competitors is often difﬁcult to ac-
quire, but in cases where it is freely available, cross-
platform analysis can only beneﬁt a model as data from
other sources can be used only if it improves performance.
In order for this data to be valuable, it has to be correctly
classiﬁed by what it refers to.

The goal of this project is to to ﬁnd a likely product
category within the root categories of the Amazon browse
node hierarchy for a given tweet. Twitter data consisted
of a training dataset with 58,000 tweets labeled with Ama-
zon browse node IDs, and a much larger set of 15,000,000
unlabeled tweets that can be used for augmentation. The
Amazon data consisted of 1,900,000 reviews for products
labeled by their browse node ID. All of the datasets orig-
inally were in JSON format and contained metadata as
well as text content for each review or tweet. To obtain
root nodes for tweets, a browse node ID tree was created
so that a simple parent traversal could identify a root cat-
egory. The Amazon product hierarchy is more of a di-

1

rected graph in that it children categories can have multi-
ple parents. In these cases, the parent is chosen randomly.
28 root categories were identiﬁed from the browse nodes
within the labeled dataset, but the distribution was heav-
ily skewed, with 47,000 tweets in the books root category,
and 10 or fewer in 5 categories. Furthermore, over half
of the tweets were re-tweets, which have the same text
content as the original tweet, providing no additional in-
formation to a classiﬁer while inﬂating accuracy mislead-
ingly. Once re-tweets and tweets from categories with
fewer than 5 tweets were removed, the labeled corpus con-
tained 23,910 tweets from 24 root categories.

Amazon Category
Books
Home & Kitchen
Clothing, Shoes & Jewelry
Movies & TV
Electronics

Tweets
19531
1332
647
643
403

Common Keywords
’free’, ’ebook’, ’kindle’
’clock’, ’lp’, ’wall’
’woman’, ’dress’, ’skirt’
’dvd’, ’video’, ’instant’
’apple’, ’hd’, ’sony’

Table 1: Top 5 categories by number of tweets

2 Method
2.1 Preprocessing
As the data consists of text strings, a bag-of-words model
was used to represent the tweets. To reduce feature
size and trim unhelpful data, all the tweets were con-
verted to lower case and stripped of all punctuation ex-
cept hashtags. Additionally, URLs and stop words from
both a list within the Natural Language Toolkit and a
list we developed speciﬁcally for Twitter were removed
and words were stemmed with the WordNet Lemma-
tizer (1)(2). With 5-fold cross validation, corresponding
to an 80/20 training/testing split, the unprocessed tweets
had 48,000 unique words, which got truncated to 22,213
words after pre-processing. Text was then transformed
to a sparse matrix representation of TF-IDF features in
order to be acceptable for downstream estimators. This

weighting scheme was chosen because it weights against
words that show up frequently across all documents and
thus implicitly reﬂects the importance of a word in a doc-
ument (3) (4). TF-IDF refers to the term-frequency multi-
plied by the inverse document frequency and is calculated
as,

f
fi j

tfij =

∑
i

idfi = log

N
dfi

2.2 Baseline Models
To evaluate the impact of our tests, we compared different
learning algorithms performance when trained on the pre-
processed dataset with all features. To ensure that there
were both training and testing examples for each category
a stratiﬁed 5-fold cross-validation was used to split up the
dataset into training and testing sets. The metrics asso-
ciated with each classiﬁer indicate the unweighted mean
of the metrics for each category. We choose to evaluate
model quality in this fashion because of the imbalanced
nature of the labeled dataset. The vectorization of the cor-
pus and the training of the models were done using the
Scikit-Learn package (5).

Classiﬁer
Multinomial NB
Logistic Regression
Linear SVM
Linear SVM (w/ class weights)

Precision
9.2%
46.1%
48.0%
51.7%

F1 Score

Recall
23.6% 0.10
73.7% 0.54
75.6% 0.56
72.3% 0.58

Table 2: Baseline Classiﬁer Average Test Scores

Class weights provide a way of correcting for imbal-
ance by weighting for or against certain classes but would
be difﬁcult to tune for each technique we will explore (6).
For this reason, an unweighted linear SVM will be used
as the baseline against which to measure the effectiveness
of our approach, although class weights will be used for
ﬁnal model. The evaluation metric for these comparisons
will be the F1-score, as it combines precision and recall
into a single number.

F1 = 2· precision· recall

precision + recall

Figure 1: Anova F-values for unigram features and uni-
gram+bigram features

Figure 2: F1-scores for unigram features and uni-
gram+bigram features

It is clear from Figure 2, that precision and recall in
the test set stabilize after using around 20% of the fea-
tures in both the unigram and unigram+bigram cases. As
the F1-score for both of these cases were roughly similar,
and the absolute number of features for a given percent-
age is much lower for only unigram features, we decided
to use 25% of the unigram features for our models.

2.3 Feature Selection
Features were ranked according to their Anova F-values
and models were evaluated when trained on the top n per-
cent of features (7). We trained models for unigram fea-
tures and unigram and bigram features.

2.4 Expansion
As tweets are shorter than typical documents, expanding
them seems reasonable as it improves the vocabulary of
the model (8). In order to improve classiﬁcation accuracy,
we considered query expansion, in which terms are added

2

Twitter Classiﬁcation into the Amazon Browse Node Hierarchy

Matthew Long, Jiao Yu, Anshul Kundani

Stanford University

December 12, 2014

Abstract

In this project, we attempt to classify tweets
into root categories of the Amazon browse node
hierarchy using a set of tweets with browse node
ID labels, a much larger set of tweets without la-
bels, and a set of Amazon reviews. Examining
twitter data presents unique challenges in that the
samples are short (under 140 characters) and of-
ten contain misspellings or abbreviations that are
trivial for a human to decipher but difﬁcult for a
computer to parse. A variety of query and docu-
ment expansion techniques are implemented in an
effort to improve information retrieval to modest
success.

1 Introduction

Internet users post information regarding a topic on a
number of different websites, but companies and organi-
zations typically only train their classiﬁcation algorithms
using only the information posted on their own platform.
Obviously, data from competitors is often difﬁcult to ac-
quire, but in cases where it is freely available, cross-
platform analysis can only beneﬁt a model as data from
other sources can be used only if it improves performance.
In order for this data to be valuable, it has to be correctly
classiﬁed by what it refers to.

The goal of this project is to to ﬁnd a likely product
category within the root categories of the Amazon browse
node hierarchy for a given tweet. Twitter data consisted
of a training dataset with 58,000 tweets labeled with Ama-
zon browse node IDs, and a much larger set of 15,000,000
unlabeled tweets that can be used for augmentation. The
Amazon data consisted of 1,900,000 reviews for products
labeled by their browse node ID. All of the datasets orig-
inally were in JSON format and contained metadata as
well as text content for each review or tweet. To obtain
root nodes for tweets, a browse node ID tree was created
so that a simple parent traversal could identify a root cat-
egory. The Amazon product hierarchy is more of a di-

1

rected graph in that it children categories can have multi-
ple parents. In these cases, the parent is chosen randomly.
28 root categories were identiﬁed from the browse nodes
within the labeled dataset, but the distribution was heav-
ily skewed, with 47,000 tweets in the books root category,
and 10 or fewer in 5 categories. Furthermore, over half
of the tweets were re-tweets, which have the same text
content as the original tweet, providing no additional in-
formation to a classiﬁer while inﬂating accuracy mislead-
ingly. Once re-tweets and tweets from categories with
fewer than 5 tweets were removed, the labeled corpus con-
tained 23,910 tweets from 24 root categories.

Amazon Category
Books
Home & Kitchen
Clothing, Shoes & Jewelry
Movies & TV
Electronics

Tweets
19531
1332
647
643
403

Common Keywords
’free’, ’ebook’, ’kindle’
’clock’, ’lp’, ’wall’
’woman’, ’dress’, ’skirt’
’dvd’, ’video’, ’instant’
’apple’, ’hd’, ’sony’

Table 1: Top 5 categories by number of tweets

2 Method
2.1 Preprocessing
As the data consists of text strings, a bag-of-words model
was used to represent the tweets. To reduce feature
size and trim unhelpful data, all the tweets were con-
verted to lower case and stripped of all punctuation ex-
cept hashtags. Additionally, URLs and stop words from
both a list within the Natural Language Toolkit and a
list we developed speciﬁcally for Twitter were removed
and words were stemmed with the WordNet Lemma-
tizer (1)(2). With 5-fold cross validation, corresponding
to an 80/20 training/testing split, the unprocessed tweets
had 48,000 unique words, which got truncated to 22,213
words after pre-processing. Text was then transformed
to a sparse matrix representation of TF-IDF features in
order to be acceptable for downstream estimators. This

weighting scheme was chosen because it weights against
words that show up frequently across all documents and
thus implicitly reﬂects the importance of a word in a doc-
ument (3) (4). TF-IDF refers to the term-frequency multi-
plied by the inverse document frequency and is calculated
as,

f
fi j

tfij =

∑
i

idfi = log

N
dfi

2.2 Baseline Models
To evaluate the impact of our tests, we compared different
learning algorithms performance when trained on the pre-
processed dataset with all features. To ensure that there
were both training and testing examples for each category
a stratiﬁed 5-fold cross-validation was used to split up the
dataset into training and testing sets. The metrics asso-
ciated with each classiﬁer indicate the unweighted mean
of the metrics for each category. We choose to evaluate
model quality in this fashion because of the imbalanced
nature of the labeled dataset. The vectorization of the cor-
pus and the training of the models were done using the
Scikit-Learn package (5).

Classiﬁer
Multinomial NB
Logistic Regression
Linear SVM
Linear SVM (w/ class weights)

Precision
9.2%
46.1%
48.0%
51.7%

F1 Score

Recall
23.6% 0.10
73.7% 0.54
75.6% 0.56
72.3% 0.58

Table 2: Baseline Classiﬁer Average Test Scores

Class weights provide a way of correcting for imbal-
ance by weighting for or against certain classes but would
be difﬁcult to tune for each technique we will explore (6).
For this reason, an unweighted linear SVM will be used
as the baseline against which to measure the effectiveness
of our approach, although class weights will be used for
ﬁnal model. The evaluation metric for these comparisons
will be the F1-score, as it combines precision and recall
into a single number.

F1 = 2· precision· recall

precision + recall

Figure 1: Anova F-values for unigram features and uni-
gram+bigram features

Figure 2: F1-scores for unigram features and uni-
gram+bigram features

It is clear from Figure 2, that precision and recall in
the test set stabilize after using around 20% of the fea-
tures in both the unigram and unigram+bigram cases. As
the F1-score for both of these cases were roughly similar,
and the absolute number of features for a given percent-
age is much lower for only unigram features, we decided
to use 25% of the unigram features for our models.

2.3 Feature Selection
Features were ranked according to their Anova F-values
and models were evaluated when trained on the top n per-
cent of features (7). We trained models for unigram fea-
tures and unigram and bigram features.

2.4 Expansion
As tweets are shorter than typical documents, expanding
them seems reasonable as it improves the vocabulary of
the model (8). In order to improve classiﬁcation accuracy,
we considered query expansion, in which terms are added

2

to testing tweets, and document expansion, in which terms
are added to training tweets. Both topics are areas of re-
search in Information Retrieval (IR), although query ex-
pansion is the more promising, and thus more studied
ﬁeld (9).

Amazon node expansion also slightly worsened perfor-
mance, but not as drastically as training set Twitter node
expansion. Figure 3 details the results expansion for vari-
ous expansion lengths.

2.4.1 Document

Tweets from the training set were expanded based upon
hashtags contained within them and the root category they
belonged to. To perform hashtag expansion a thesaurus
was built up of the most frequent words in tweets con-
taining a given hashtag using the large unlabeled Twitter
dataset. n randomly selected words from the top 2n words
from each hashtag were then added to each tweet contain-
ing that hashtag. No words from the stop lists would be
added, nor would the hashtag word. For root category ex-
pansion, one thesaurus was built using for each category
using the words in the training set portion of the labeled
tweets and another was built for the reviews in the Ama-
zon set. When building the thesaurus for root category
expansion using Twitter, the top words for each category
were chosen with a TF-IDF weighting scheme, however,
because both the corpus the thesaurus was built upon was
much smaller allowing the process to be computationally
feasible.

2.4.2 Query

As the hashtag thesaurus was built from an external
dataset, hashtag expansion could be used on tweets from
the testing set portion of the labeled tweets as well. An
identical process to document hashtag expansion was
used.

Tweet
”wepnewlyrelease
new
read bulletin board #ﬁction
#thriller”
”aburke59 dead sister jessica
huntington deser free sample
#mystery”

Suggested Expansion Words
’review’, ’book’, ’ﬁction’, ’lit-
erature’, ’child’

’get’, ’read’, ’star’, ’murder’

Table 3: Hashtag Expansion Examples

3 Results
3.1 Expansion
Tweet expansion saw mixed results in category classiﬁca-
tion. Hashtag expansion on both the training and testing
set marginally improved performance, while hashtag ex-
pansion on each set exclusively worsened performance.

Figure 3: F1-scores for various expansion techniques

3.2 Overall
In the ﬁnal model, we used both hashtag document and
query expansion and also added class weights to the lin-
ear SVM classiﬁer. The class weighting scheme that was
added was primarily directed at reducing the effects of
the imbalance toward the books category so a weight of
0.1 was applied to that category, while other categories
weighted by 1 (6). Additionally, the C parameter of the
SVM estimator was tuned using the GridSearch function
of Scikit-Learn and a value of 5 was selected. Table 4
shows the results of our ﬁnal model.

Figure 4: Scores for various class weights against books

3

Twitter Classiﬁcation into the Amazon Browse Node Hierarchy

Matthew Long, Jiao Yu, Anshul Kundani

Stanford University

December 12, 2014

Abstract

In this project, we attempt to classify tweets
into root categories of the Amazon browse node
hierarchy using a set of tweets with browse node
ID labels, a much larger set of tweets without la-
bels, and a set of Amazon reviews. Examining
twitter data presents unique challenges in that the
samples are short (under 140 characters) and of-
ten contain misspellings or abbreviations that are
trivial for a human to decipher but difﬁcult for a
computer to parse. A variety of query and docu-
ment expansion techniques are implemented in an
effort to improve information retrieval to modest
success.

1 Introduction

Internet users post information regarding a topic on a
number of different websites, but companies and organi-
zations typically only train their classiﬁcation algorithms
using only the information posted on their own platform.
Obviously, data from competitors is often difﬁcult to ac-
quire, but in cases where it is freely available, cross-
platform analysis can only beneﬁt a model as data from
other sources can be used only if it improves performance.
In order for this data to be valuable, it has to be correctly
classiﬁed by what it refers to.

The goal of this project is to to ﬁnd a likely product
category within the root categories of the Amazon browse
node hierarchy for a given tweet. Twitter data consisted
of a training dataset with 58,000 tweets labeled with Ama-
zon browse node IDs, and a much larger set of 15,000,000
unlabeled tweets that can be used for augmentation. The
Amazon data consisted of 1,900,000 reviews for products
labeled by their browse node ID. All of the datasets orig-
inally were in JSON format and contained metadata as
well as text content for each review or tweet. To obtain
root nodes for tweets, a browse node ID tree was created
so that a simple parent traversal could identify a root cat-
egory. The Amazon product hierarchy is more of a di-

1

rected graph in that it children categories can have multi-
ple parents. In these cases, the parent is chosen randomly.
28 root categories were identiﬁed from the browse nodes
within the labeled dataset, but the distribution was heav-
ily skewed, with 47,000 tweets in the books root category,
and 10 or fewer in 5 categories. Furthermore, over half
of the tweets were re-tweets, which have the same text
content as the original tweet, providing no additional in-
formation to a classiﬁer while inﬂating accuracy mislead-
ingly. Once re-tweets and tweets from categories with
fewer than 5 tweets were removed, the labeled corpus con-
tained 23,910 tweets from 24 root categories.

Amazon Category
Books
Home & Kitchen
Clothing, Shoes & Jewelry
Movies & TV
Electronics

Tweets
19531
1332
647
643
403

Common Keywords
’free’, ’ebook’, ’kindle’
’clock’, ’lp’, ’wall’
’woman’, ’dress’, ’skirt’
’dvd’, ’video’, ’instant’
’apple’, ’hd’, ’sony’

Table 1: Top 5 categories by number of tweets

2 Method
2.1 Preprocessing
As the data consists of text strings, a bag-of-words model
was used to represent the tweets. To reduce feature
size and trim unhelpful data, all the tweets were con-
verted to lower case and stripped of all punctuation ex-
cept hashtags. Additionally, URLs and stop words from
both a list within the Natural Language Toolkit and a
list we developed speciﬁcally for Twitter were removed
and words were stemmed with the WordNet Lemma-
tizer (1)(2). With 5-fold cross validation, corresponding
to an 80/20 training/testing split, the unprocessed tweets
had 48,000 unique words, which got truncated to 22,213
words after pre-processing. Text was then transformed
to a sparse matrix representation of TF-IDF features in
order to be acceptable for downstream estimators. This

weighting scheme was chosen because it weights against
words that show up frequently across all documents and
thus implicitly reﬂects the importance of a word in a doc-
ument (3) (4). TF-IDF refers to the term-frequency multi-
plied by the inverse document frequency and is calculated
as,

f
fi j

tfij =

∑
i

idfi = log

N
dfi

2.2 Baseline Models
To evaluate the impact of our tests, we compared different
learning algorithms performance when trained on the pre-
processed dataset with all features. To ensure that there
were both training and testing examples for each category
a stratiﬁed 5-fold cross-validation was used to split up the
dataset into training and testing sets. The metrics asso-
ciated with each classiﬁer indicate the unweighted mean
of the metrics for each category. We choose to evaluate
model quality in this fashion because of the imbalanced
nature of the labeled dataset. The vectorization of the cor-
pus and the training of the models were done using the
Scikit-Learn package (5).

Classiﬁer
Multinomial NB
Logistic Regression
Linear SVM
Linear SVM (w/ class weights)

Precision
9.2%
46.1%
48.0%
51.7%

F1 Score

Recall
23.6% 0.10
73.7% 0.54
75.6% 0.56
72.3% 0.58

Table 2: Baseline Classiﬁer Average Test Scores

Class weights provide a way of correcting for imbal-
ance by weighting for or against certain classes but would
be difﬁcult to tune for each technique we will explore (6).
For this reason, an unweighted linear SVM will be used
as the baseline against which to measure the effectiveness
of our approach, although class weights will be used for
ﬁnal model. The evaluation metric for these comparisons
will be the F1-score, as it combines precision and recall
into a single number.

F1 = 2· precision· recall

precision + recall

Figure 1: Anova F-values for unigram features and uni-
gram+bigram features

Figure 2: F1-scores for unigram features and uni-
gram+bigram features

It is clear from Figure 2, that precision and recall in
the test set stabilize after using around 20% of the fea-
tures in both the unigram and unigram+bigram cases. As
the F1-score for both of these cases were roughly similar,
and the absolute number of features for a given percent-
age is much lower for only unigram features, we decided
to use 25% of the unigram features for our models.

2.3 Feature Selection
Features were ranked according to their Anova F-values
and models were evaluated when trained on the top n per-
cent of features (7). We trained models for unigram fea-
tures and unigram and bigram features.

2.4 Expansion
As tweets are shorter than typical documents, expanding
them seems reasonable as it improves the vocabulary of
the model (8). In order to improve classiﬁcation accuracy,
we considered query expansion, in which terms are added

2

to testing tweets, and document expansion, in which terms
are added to training tweets. Both topics are areas of re-
search in Information Retrieval (IR), although query ex-
pansion is the more promising, and thus more studied
ﬁeld (9).

Amazon node expansion also slightly worsened perfor-
mance, but not as drastically as training set Twitter node
expansion. Figure 3 details the results expansion for vari-
ous expansion lengths.

2.4.1 Document

Tweets from the training set were expanded based upon
hashtags contained within them and the root category they
belonged to. To perform hashtag expansion a thesaurus
was built up of the most frequent words in tweets con-
taining a given hashtag using the large unlabeled Twitter
dataset. n randomly selected words from the top 2n words
from each hashtag were then added to each tweet contain-
ing that hashtag. No words from the stop lists would be
added, nor would the hashtag word. For root category ex-
pansion, one thesaurus was built using for each category
using the words in the training set portion of the labeled
tweets and another was built for the reviews in the Ama-
zon set. When building the thesaurus for root category
expansion using Twitter, the top words for each category
were chosen with a TF-IDF weighting scheme, however,
because both the corpus the thesaurus was built upon was
much smaller allowing the process to be computationally
feasible.

2.4.2 Query

As the hashtag thesaurus was built from an external
dataset, hashtag expansion could be used on tweets from
the testing set portion of the labeled tweets as well. An
identical process to document hashtag expansion was
used.

Tweet
”wepnewlyrelease
new
read bulletin board #ﬁction
#thriller”
”aburke59 dead sister jessica
huntington deser free sample
#mystery”

Suggested Expansion Words
’review’, ’book’, ’ﬁction’, ’lit-
erature’, ’child’

’get’, ’read’, ’star’, ’murder’

Table 3: Hashtag Expansion Examples

3 Results
3.1 Expansion
Tweet expansion saw mixed results in category classiﬁca-
tion. Hashtag expansion on both the training and testing
set marginally improved performance, while hashtag ex-
pansion on each set exclusively worsened performance.

Figure 3: F1-scores for various expansion techniques

3.2 Overall
In the ﬁnal model, we used both hashtag document and
query expansion and also added class weights to the lin-
ear SVM classiﬁer. The class weighting scheme that was
added was primarily directed at reducing the effects of
the imbalance toward the books category so a weight of
0.1 was applied to that category, while other categories
weighted by 1 (6). Additionally, the C parameter of the
SVM estimator was tuned using the GridSearch function
of Scikit-Learn and a value of 5 was selected. Table 4
shows the results of our ﬁnal model.

Figure 4: Scores for various class weights against books

3

Precision
0.89
0.78

Recall
1.00
0.85

F1-Score
0.94
0.81

Support
8
46

so the effects of query expansion was only received by a
fraction of the test set.

Amazon Category
Baby Products
Health & Personal
Care
Digital Music
Beauty
Sports & Outdoors
Arts,
Crafts &
Sewing
Video Games
Home & Kitchen
Kindle Store
Tools & Home Im-
provement
Collectibles & Fine
Art
CDs & Vinyl
Patio, Lawn & Gar-
den
Clothing, Shoes &
Jewelry
Cell Phones & Ac-
cessories
Books
Pet Supplies
Automotive
Musical
ments
Movies & TV
Ofﬁce Products
Toys & Games
Electronics
Grocery & Gourmet
Food
Category Average
Absolute Average

Instru-

0.82
0.71
0.69
1.00

0.89
0.84
1.00
0.75

0.87

0.83
0.00

0.89

1.00

0.96
1.00
1.00
1.00

0.74
1.00
0.86
0.82
0.00

0.81
0.94

0.64
0.50
0.62
0.20

0.53
0.89
0.67
0.50

0.81

0.35
0.00

0.76

0.14

0.98
0.11
0.60
0.70

0.69
0.56
0.24
0.75
0.00

0.54
0.94

0.72
0.59
0.65
0.33

0.67
0.87
0.80
0.60

0.84

0.49
0.00

0.82

0.25

0.98
0.20
0.75
0.82

0.71
0.71
0.38
0.78
0.00

0.61
0.94

22
10
56
5

32
334
3
18

16

55
7

162

7

4883
9
5
10

161
9
25
101
2

5896
5896

Figure 5: Category Size compared with F1-Scores

5 Future Work

The next step to take would be to build up a thesaurus on
individual words from both Amazon and unlabeled Twit-
ter data in order to expand testing and training tweets on a
per word basis. Building these thesauruses will be space
intensive because for each word the frequency of all the
other words it has appeared with in a tweet or review has
to be stored. This step holds promise as it could be used
for both query and document expansion and could be used
upon all tweets. With a full word thesaurus, selective ex-
pansion could also be explored, where only certain cate-
gories are expanded. There are existing thesauruses that
can be downloaded such as WordNet, but the frequent use
of abbreviations and slang on Twitter makes building a
thesaurus from a corpus of tweets potentially more bene-
ﬁcial citewn. Another step that would provide immediate
beneﬁts is building a larger corpus for under-represented
categories. Manually labeling a few dozen tweets for each
category could quickly improve categories with under 20
tweets in them.

6 Acknowledgements

We would like to thank Aditya Jami and Professor
Ashutosh Saxena for providing us with the datasets and
for guiding us throughout the project. We would also like
to thank Professor Ng and the teaching assistant staff for
giving us the skills necessary to work on this project.

Table 4: Model Results for 75/25 training/testing split

4 Discussion

The model achieved an average F1-score across all cate-
gories of 0.61 with average precision of 81% and average
recall of 54%. Categories with more tweets tended to be
classiﬁed more accurately than tweets with few samples
to draw upon. This makes intuitive sense as the vocab-
ulary of the samples in the small categories is limited so
there are high odds that the testing samples do not contain
the same words as in the training samples. This is repre-
sentative of the fact that the bound on generalization error
decreases as the sample size increases, so naturally larger
categories are capable of better testing accuracy. Figure 5
demonstrates this rough trend. Query expansion is typi-
cally regarded to be more effective than document expan-
sion and the only thing we expanded in the test set were
hashtags (9). Many tweets do not contain any hashtags,

4

Twitter Classiﬁcation into the Amazon Browse Node Hierarchy

Matthew Long, Jiao Yu, Anshul Kundani

Stanford University

December 12, 2014

Abstract

In this project, we attempt to classify tweets
into root categories of the Amazon browse node
hierarchy using a set of tweets with browse node
ID labels, a much larger set of tweets without la-
bels, and a set of Amazon reviews. Examining
twitter data presents unique challenges in that the
samples are short (under 140 characters) and of-
ten contain misspellings or abbreviations that are
trivial for a human to decipher but difﬁcult for a
computer to parse. A variety of query and docu-
ment expansion techniques are implemented in an
effort to improve information retrieval to modest
success.

1 Introduction

Internet users post information regarding a topic on a
number of different websites, but companies and organi-
zations typically only train their classiﬁcation algorithms
using only the information posted on their own platform.
Obviously, data from competitors is often difﬁcult to ac-
quire, but in cases where it is freely available, cross-
platform analysis can only beneﬁt a model as data from
other sources can be used only if it improves performance.
In order for this data to be valuable, it has to be correctly
classiﬁed by what it refers to.

The goal of this project is to to ﬁnd a likely product
category within the root categories of the Amazon browse
node hierarchy for a given tweet. Twitter data consisted
of a training dataset with 58,000 tweets labeled with Ama-
zon browse node IDs, and a much larger set of 15,000,000
unlabeled tweets that can be used for augmentation. The
Amazon data consisted of 1,900,000 reviews for products
labeled by their browse node ID. All of the datasets orig-
inally were in JSON format and contained metadata as
well as text content for each review or tweet. To obtain
root nodes for tweets, a browse node ID tree was created
so that a simple parent traversal could identify a root cat-
egory. The Amazon product hierarchy is more of a di-

1

rected graph in that it children categories can have multi-
ple parents. In these cases, the parent is chosen randomly.
28 root categories were identiﬁed from the browse nodes
within the labeled dataset, but the distribution was heav-
ily skewed, with 47,000 tweets in the books root category,
and 10 or fewer in 5 categories. Furthermore, over half
of the tweets were re-tweets, which have the same text
content as the original tweet, providing no additional in-
formation to a classiﬁer while inﬂating accuracy mislead-
ingly. Once re-tweets and tweets from categories with
fewer than 5 tweets were removed, the labeled corpus con-
tained 23,910 tweets from 24 root categories.

Amazon Category
Books
Home & Kitchen
Clothing, Shoes & Jewelry
Movies & TV
Electronics

Tweets
19531
1332
647
643
403

Common Keywords
’free’, ’ebook’, ’kindle’
’clock’, ’lp’, ’wall’
’woman’, ’dress’, ’skirt’
’dvd’, ’video’, ’instant’
’apple’, ’hd’, ’sony’

Table 1: Top 5 categories by number of tweets

2 Method
2.1 Preprocessing
As the data consists of text strings, a bag-of-words model
was used to represent the tweets. To reduce feature
size and trim unhelpful data, all the tweets were con-
verted to lower case and stripped of all punctuation ex-
cept hashtags. Additionally, URLs and stop words from
both a list within the Natural Language Toolkit and a
list we developed speciﬁcally for Twitter were removed
and words were stemmed with the WordNet Lemma-
tizer (1)(2). With 5-fold cross validation, corresponding
to an 80/20 training/testing split, the unprocessed tweets
had 48,000 unique words, which got truncated to 22,213
words after pre-processing. Text was then transformed
to a sparse matrix representation of TF-IDF features in
order to be acceptable for downstream estimators. This

weighting scheme was chosen because it weights against
words that show up frequently across all documents and
thus implicitly reﬂects the importance of a word in a doc-
ument (3) (4). TF-IDF refers to the term-frequency multi-
plied by the inverse document frequency and is calculated
as,

f
fi j

tfij =

∑
i

idfi = log

N
dfi

2.2 Baseline Models
To evaluate the impact of our tests, we compared different
learning algorithms performance when trained on the pre-
processed dataset with all features. To ensure that there
were both training and testing examples for each category
a stratiﬁed 5-fold cross-validation was used to split up the
dataset into training and testing sets. The metrics asso-
ciated with each classiﬁer indicate the unweighted mean
of the metrics for each category. We choose to evaluate
model quality in this fashion because of the imbalanced
nature of the labeled dataset. The vectorization of the cor-
pus and the training of the models were done using the
Scikit-Learn package (5).

Classiﬁer
Multinomial NB
Logistic Regression
Linear SVM
Linear SVM (w/ class weights)

Precision
9.2%
46.1%
48.0%
51.7%

F1 Score

Recall
23.6% 0.10
73.7% 0.54
75.6% 0.56
72.3% 0.58

Table 2: Baseline Classiﬁer Average Test Scores

Class weights provide a way of correcting for imbal-
ance by weighting for or against certain classes but would
be difﬁcult to tune for each technique we will explore (6).
For this reason, an unweighted linear SVM will be used
as the baseline against which to measure the effectiveness
of our approach, although class weights will be used for
ﬁnal model. The evaluation metric for these comparisons
will be the F1-score, as it combines precision and recall
into a single number.

F1 = 2· precision· recall

precision + recall

Figure 1: Anova F-values for unigram features and uni-
gram+bigram features

Figure 2: F1-scores for unigram features and uni-
gram+bigram features

It is clear from Figure 2, that precision and recall in
the test set stabilize after using around 20% of the fea-
tures in both the unigram and unigram+bigram cases. As
the F1-score for both of these cases were roughly similar,
and the absolute number of features for a given percent-
age is much lower for only unigram features, we decided
to use 25% of the unigram features for our models.

2.3 Feature Selection
Features were ranked according to their Anova F-values
and models were evaluated when trained on the top n per-
cent of features (7). We trained models for unigram fea-
tures and unigram and bigram features.

2.4 Expansion
As tweets are shorter than typical documents, expanding
them seems reasonable as it improves the vocabulary of
the model (8). In order to improve classiﬁcation accuracy,
we considered query expansion, in which terms are added

2

to testing tweets, and document expansion, in which terms
are added to training tweets. Both topics are areas of re-
search in Information Retrieval (IR), although query ex-
pansion is the more promising, and thus more studied
ﬁeld (9).

Amazon node expansion also slightly worsened perfor-
mance, but not as drastically as training set Twitter node
expansion. Figure 3 details the results expansion for vari-
ous expansion lengths.

2.4.1 Document

Tweets from the training set were expanded based upon
hashtags contained within them and the root category they
belonged to. To perform hashtag expansion a thesaurus
was built up of the most frequent words in tweets con-
taining a given hashtag using the large unlabeled Twitter
dataset. n randomly selected words from the top 2n words
from each hashtag were then added to each tweet contain-
ing that hashtag. No words from the stop lists would be
added, nor would the hashtag word. For root category ex-
pansion, one thesaurus was built using for each category
using the words in the training set portion of the labeled
tweets and another was built for the reviews in the Ama-
zon set. When building the thesaurus for root category
expansion using Twitter, the top words for each category
were chosen with a TF-IDF weighting scheme, however,
because both the corpus the thesaurus was built upon was
much smaller allowing the process to be computationally
feasible.

2.4.2 Query

As the hashtag thesaurus was built from an external
dataset, hashtag expansion could be used on tweets from
the testing set portion of the labeled tweets as well. An
identical process to document hashtag expansion was
used.

Tweet
”wepnewlyrelease
new
read bulletin board #ﬁction
#thriller”
”aburke59 dead sister jessica
huntington deser free sample
#mystery”

Suggested Expansion Words
’review’, ’book’, ’ﬁction’, ’lit-
erature’, ’child’

’get’, ’read’, ’star’, ’murder’

Table 3: Hashtag Expansion Examples

3 Results
3.1 Expansion
Tweet expansion saw mixed results in category classiﬁca-
tion. Hashtag expansion on both the training and testing
set marginally improved performance, while hashtag ex-
pansion on each set exclusively worsened performance.

Figure 3: F1-scores for various expansion techniques

3.2 Overall
In the ﬁnal model, we used both hashtag document and
query expansion and also added class weights to the lin-
ear SVM classiﬁer. The class weighting scheme that was
added was primarily directed at reducing the effects of
the imbalance toward the books category so a weight of
0.1 was applied to that category, while other categories
weighted by 1 (6). Additionally, the C parameter of the
SVM estimator was tuned using the GridSearch function
of Scikit-Learn and a value of 5 was selected. Table 4
shows the results of our ﬁnal model.

Figure 4: Scores for various class weights against books

3

Precision
0.89
0.78

Recall
1.00
0.85

F1-Score
0.94
0.81

Support
8
46

so the effects of query expansion was only received by a
fraction of the test set.

Amazon Category
Baby Products
Health & Personal
Care
Digital Music
Beauty
Sports & Outdoors
Arts,
Crafts &
Sewing
Video Games
Home & Kitchen
Kindle Store
Tools & Home Im-
provement
Collectibles & Fine
Art
CDs & Vinyl
Patio, Lawn & Gar-
den
Clothing, Shoes &
Jewelry
Cell Phones & Ac-
cessories
Books
Pet Supplies
Automotive
Musical
ments
Movies & TV
Ofﬁce Products
Toys & Games
Electronics
Grocery & Gourmet
Food
Category Average
Absolute Average

Instru-

0.82
0.71
0.69
1.00

0.89
0.84
1.00
0.75

0.87

0.83
0.00

0.89

1.00

0.96
1.00
1.00
1.00

0.74
1.00
0.86
0.82
0.00

0.81
0.94

0.64
0.50
0.62
0.20

0.53
0.89
0.67
0.50

0.81

0.35
0.00

0.76

0.14

0.98
0.11
0.60
0.70

0.69
0.56
0.24
0.75
0.00

0.54
0.94

0.72
0.59
0.65
0.33

0.67
0.87
0.80
0.60

0.84

0.49
0.00

0.82

0.25

0.98
0.20
0.75
0.82

0.71
0.71
0.38
0.78
0.00

0.61
0.94

22
10
56
5

32
334
3
18

16

55
7

162

7

4883
9
5
10

161
9
25
101
2

5896
5896

Figure 5: Category Size compared with F1-Scores

5 Future Work

The next step to take would be to build up a thesaurus on
individual words from both Amazon and unlabeled Twit-
ter data in order to expand testing and training tweets on a
per word basis. Building these thesauruses will be space
intensive because for each word the frequency of all the
other words it has appeared with in a tweet or review has
to be stored. This step holds promise as it could be used
for both query and document expansion and could be used
upon all tweets. With a full word thesaurus, selective ex-
pansion could also be explored, where only certain cate-
gories are expanded. There are existing thesauruses that
can be downloaded such as WordNet, but the frequent use
of abbreviations and slang on Twitter makes building a
thesaurus from a corpus of tweets potentially more bene-
ﬁcial citewn. Another step that would provide immediate
beneﬁts is building a larger corpus for under-represented
categories. Manually labeling a few dozen tweets for each
category could quickly improve categories with under 20
tweets in them.

6 Acknowledgements

We would like to thank Aditya Jami and Professor
Ashutosh Saxena for providing us with the datasets and
for guiding us throughout the project. We would also like
to thank Professor Ng and the teaching assistant staff for
giving us the skills necessary to work on this project.

Table 4: Model Results for 75/25 training/testing split

4 Discussion

The model achieved an average F1-score across all cate-
gories of 0.61 with average precision of 81% and average
recall of 54%. Categories with more tweets tended to be
classiﬁed more accurately than tweets with few samples
to draw upon. This makes intuitive sense as the vocab-
ulary of the samples in the small categories is limited so
there are high odds that the testing samples do not contain
the same words as in the training samples. This is repre-
sentative of the fact that the bound on generalization error
decreases as the sample size increases, so naturally larger
categories are capable of better testing accuracy. Figure 5
demonstrates this rough trend. Query expansion is typi-
cally regarded to be more effective than document expan-
sion and the only thing we expanded in the test set were
hashtags (9). Many tweets do not contain any hashtags,

4

References
[1] S. Bird, E. Loper, and E Klein. Natural Language Processing with Python. OReilly Media Inc., 2009.

[2] Princeton University. About wordnet. http://wordnet.princeton.edu, 2010.

[3] C.D. Manning, P. Raghavan, and H. Schtze. Introduction to Information Retrieval. Cambridge University Press.,

2008.

[4] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information Processing and

Management, 24:513523, 1988.

[5] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.

[6] V. Ganganwar. An overview of classiﬁcation algorithms for imbalanced datasets. International Journal of Emerg-

ing Technology and Advanced Engineering, 2, 2012.

[7] Y.W. Chen and C.J. Lin. Feature extraction, foundations and applications. Springer, 2006.

[8] E. Agirre, X. Arregi, and A. Otegi. Document expansion based on wordnet for robust ir. Association for Compu-

tational Linguistics, 2010.

[9] B. Billerbeck and J. Zobel. Document expansion versus query expansion for ad-hoc retrieval. Proceedings of the

Tenth Australasian Document Computing Symposium, 2005.

5

