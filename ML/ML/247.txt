User Review Sentiment Classiﬁcation and Aggregation

Steven Garcia

garcias@stanford.edu

Ping Yin

pingyin@stanford.edu

ABSTRACT
User reviews provide a wealth of information but are often
overwhelming in volume. In this work we propose a novel
approach to extract positive and negative sentiments from
user review data leveraging only the overall review scores
that are part of the data itself. We then investigate clus-
tering techniques to identify key positive and negative senti-
ment aspects to provide a user friendly summary for users.

1.

INTRODUCTION

User reviews on product websites such as Yelp or Ama-
zon provide a wealth of information about the subject of the
review. In many cases when selecting a service (i.e. restau-
rant) or when choosing a product (i.e. book, dishwasher
etc.) the amount of information available in user reviews
can be of greater volume and more trustworthy than the
oﬃcial product descriptions provided by the vendor.

The number of reviews for a single item can at times be
overwhelming. For instance, the Amazon Kindle keyboard
6” e-reader has over 42,000 user reviews as of this writing [1].
The review reader can gain an understanding of overall satis-
faction by looking at summary statistics such as the average
rating or score, but the details about what make each prod-
uct great or not are hidden inside the body of the reviews
themselves. With such a large amount of data to process, it
would be beneﬁcial to automate the process and generate a
summary of the reviews.

In this project we propose an approach to review summa-
rization that leverages trained models for classiﬁcation and
clustering to derive sentiment for the key aspects of a re-
viewed item. Given a set of item reviews, our system will
output sentences from the reviews that are representative of
key aspects for which users had strong positive and negative
opinions about.

This paper is structured as follows. In Section 2 we discuss
related works in the ares of sentiment detection and analysis.
In Section 3 we provide an overview of the Amazon and Yelp
datasets used in our experiments. In Section 4 and Section 5
we outline our novel approach to review summarization and
present results. Finally, in Section 6 we conclude our ﬁnd-
ings and outline future directions of exploration.

2. BACKGROUND

Liu and Zhang [7] outline ﬁve tasks that are core elements
of sentiment analysis: entity extraction and grouping; aspect
extraction and grouping; opinion holder and time extrac-
tion; aspect sentiment classiﬁcation; and opinion quintuple
generation. In this work we will focus on the tasks of aspect
extraction and grouping, and aspect sentiment classiﬁcation.
Pang et al. [10] successfully use bag-of-words features to
predict positive and negative labels for movie reviews. They
ﬁnd little diﬀerence between Naive Bayes and SVM classi-
ﬁers. Based on this work our experiments begin with bag-
of-words features and Naive Bayes classiﬁers.

Das et al. [5] classify user sentiment from user message
boards using a range of algorithms and show that model
selection can have a signiﬁcant impact on accuracy. Sim-
ilarly, we experiment with several models for our classiﬁer
and choose our best model using precision and recall metrics.
Cataldi et al. [4] propose a system that leverages NLP to
identify feature level sentiment. There is signiﬁcant cost in
parsing sentence structure and typically such works is lim-
ited to small data sets. Although we utilize large data sets
in this work, we explore the the impact of POS tagging in
a later component of our system where the cost to generate
this data is reduced.

3. DATASET

In this work we experiment with two datasets. The Yelp
Challenge dataset containing 1.6M reviews spanning 61K
businesses and 366K users [11]. We also utilize the Ama-
zon product data set which provides nearly two decades of
reviews for a variety of product segments [8]. From the Ama-
zon dataset we experiment with the Video Games, Tool and
Home Improvement, and Musical Instruments subsets. For
all datasets, each record contains a review text and an over-
all review score. This work is limited to consuming those
two ﬁelds but could be extended to include the data pro-
vided other ﬁelds.

For each review we breakdown the reviews into sentence
snippets and then into tokens to use as features. Addition-
ally some experiments utilize stemming and part of speech
tagging. All text manipulation, stemming and tagging was
performed using the Natural Language Toolkit [2]. Figure 1
shows the distribution of sentences per review and the break-
down of review scores per dataset.

4. METHODS

Our goal is to identify sentences that are representative of
positive and negative key aspects of the review subjects. To
this end we must establish a mechanism to identify positive
and negative sentiment within the body of each review.

A key challenge to classifying review sentences is that we
do not have labels for each individual sentence within the
data set. Given the large number of reviews in our data sets
we instead label each sentence with the overall review score.
The overall review score therefore acts as a surrogate score
for each sentence in that review. Although noisy, we can
justify this decision for two reasons. First, due to the over-
whelmingly large amount of data we expect that the learn-
ing algorithm will successfully learn to classify sentences to
a reasonable degree. As shown in Figure 1, all but one of our
data sets has at least 100,000 reviews at every rating. Sec-
ond, our extraction of representative aspect sentences does
not require every sentence to be correctly classiﬁed. Indeed,
even with a large number of unclassiﬁed sentences, we can
still produce a meaningful summary for the user.
In Sec-
tion 5 we measure the precision of this approach.

User Review Sentiment Classiﬁcation and Aggregation

Steven Garcia

garcias@stanford.edu

Ping Yin

pingyin@stanford.edu

ABSTRACT
User reviews provide a wealth of information but are often
overwhelming in volume. In this work we propose a novel
approach to extract positive and negative sentiments from
user review data leveraging only the overall review scores
that are part of the data itself. We then investigate clus-
tering techniques to identify key positive and negative senti-
ment aspects to provide a user friendly summary for users.

1.

INTRODUCTION

User reviews on product websites such as Yelp or Ama-
zon provide a wealth of information about the subject of the
review. In many cases when selecting a service (i.e. restau-
rant) or when choosing a product (i.e. book, dishwasher
etc.) the amount of information available in user reviews
can be of greater volume and more trustworthy than the
oﬃcial product descriptions provided by the vendor.

The number of reviews for a single item can at times be
overwhelming. For instance, the Amazon Kindle keyboard
6” e-reader has over 42,000 user reviews as of this writing [1].
The review reader can gain an understanding of overall satis-
faction by looking at summary statistics such as the average
rating or score, but the details about what make each prod-
uct great or not are hidden inside the body of the reviews
themselves. With such a large amount of data to process, it
would be beneﬁcial to automate the process and generate a
summary of the reviews.

In this project we propose an approach to review summa-
rization that leverages trained models for classiﬁcation and
clustering to derive sentiment for the key aspects of a re-
viewed item. Given a set of item reviews, our system will
output sentences from the reviews that are representative of
key aspects for which users had strong positive and negative
opinions about.

This paper is structured as follows. In Section 2 we discuss
related works in the ares of sentiment detection and analysis.
In Section 3 we provide an overview of the Amazon and Yelp
datasets used in our experiments. In Section 4 and Section 5
we outline our novel approach to review summarization and
present results. Finally, in Section 6 we conclude our ﬁnd-
ings and outline future directions of exploration.

2. BACKGROUND

Liu and Zhang [7] outline ﬁve tasks that are core elements
of sentiment analysis: entity extraction and grouping; aspect
extraction and grouping; opinion holder and time extrac-
tion; aspect sentiment classiﬁcation; and opinion quintuple
generation. In this work we will focus on the tasks of aspect
extraction and grouping, and aspect sentiment classiﬁcation.
Pang et al. [10] successfully use bag-of-words features to
predict positive and negative labels for movie reviews. They
ﬁnd little diﬀerence between Naive Bayes and SVM classi-
ﬁers. Based on this work our experiments begin with bag-
of-words features and Naive Bayes classiﬁers.

Das et al. [5] classify user sentiment from user message
boards using a range of algorithms and show that model
selection can have a signiﬁcant impact on accuracy. Sim-
ilarly, we experiment with several models for our classiﬁer
and choose our best model using precision and recall metrics.
Cataldi et al. [4] propose a system that leverages NLP to
identify feature level sentiment. There is signiﬁcant cost in
parsing sentence structure and typically such works is lim-
ited to small data sets. Although we utilize large data sets
in this work, we explore the the impact of POS tagging in
a later component of our system where the cost to generate
this data is reduced.

3. DATASET

In this work we experiment with two datasets. The Yelp
Challenge dataset containing 1.6M reviews spanning 61K
businesses and 366K users [11]. We also utilize the Ama-
zon product data set which provides nearly two decades of
reviews for a variety of product segments [8]. From the Ama-
zon dataset we experiment with the Video Games, Tool and
Home Improvement, and Musical Instruments subsets. For
all datasets, each record contains a review text and an over-
all review score. This work is limited to consuming those
two ﬁelds but could be extended to include the data pro-
vided other ﬁelds.

For each review we breakdown the reviews into sentence
snippets and then into tokens to use as features. Addition-
ally some experiments utilize stemming and part of speech
tagging. All text manipulation, stemming and tagging was
performed using the Natural Language Toolkit [2]. Figure 1
shows the distribution of sentences per review and the break-
down of review scores per dataset.

4. METHODS

Our goal is to identify sentences that are representative of
positive and negative key aspects of the review subjects. To
this end we must establish a mechanism to identify positive
and negative sentiment within the body of each review.

A key challenge to classifying review sentences is that we
do not have labels for each individual sentence within the
data set. Given the large number of reviews in our data sets
we instead label each sentence with the overall review score.
The overall review score therefore acts as a surrogate score
for each sentence in that review. Although noisy, we can
justify this decision for two reasons. First, due to the over-
whelmingly large amount of data we expect that the learn-
ing algorithm will successfully learn to classify sentences to
a reasonable degree. As shown in Figure 1, all but one of our
data sets has at least 100,000 reviews at every rating. Sec-
ond, our extraction of representative aspect sentences does
not require every sentence to be correctly classiﬁed. Indeed,
even with a large number of unclassiﬁed sentences, we can
still produce a meaningful summary for the user.
In Sec-
tion 5 we measure the precision of this approach.

Figure 1: Dataset review and length breakdown

Having established a classiﬁcation of positive and negative
sentences within a review, we can then apply a clustering
algorithm to ﬁnd groupings which are representative of key
elements.

An overview of our system is depicted in Figure 2. The
Sentence Parser splits reviews into sentence fragments. The
Sentence Classifer then assigns a positive, negative, or unsure
label to each sentence. The Clustering component groups
together common sentences of each labeled class. Finally,
Aspect Selection ﬁlters the clusters and outputs a represen-
tative sentence for each selected grouping.
4.1 Classiﬁcation

To classify the sentences as positive or negative we con-
sider three candidate algorithms. Naive Bayes classiﬁers are
well explored in the literature for use on text classiﬁcation
problems. They are a natural ﬁt for text classiﬁcation as
they are a generative class of models that estimate the like-
lihood of observing the data. Speciﬁcally, the classiﬁer uses
Bayes rule to determine the likelihood of a class given the
data with P (y|x) = P (x|y)P (y)
((cid:81)n
, and using an independence
(cid:80)K
k ((cid:81)n
P (x)
i=1 P (xi|y=j))P (y=j)
assumption this reduces to
i=1 P (xi|y=k))P (y=k)

set of features n, aiming to predict the likelihood of observ-
ing class j in the set of classes K.

for a

McCallum et al. [9] compare Baysian models and ﬁnd that
the Multinomial Naive Bayes classiﬁer is almost uniformly
better than the Bernoulli classiﬁer. However in this work we
deal with very relatively short documents (individual sen-
tences). The Multinomial model speciﬁcally accounts for
feature frequency in determining the likelihood of a docu-
ment. This makes sense for longer documents, but for short
sentences (like the ones we aim to classify) it could be ar-
gued that the frequency of the features is less important
than the actual occurrence of those features. As such we
consider both the Bernoulli Naive Bayes classiﬁer and the

Figure 2: Overall system architecture

Multinomial Naive Bayes classiﬁer.

One limitation of the Naive Bayes classiﬁers is that they
make an independence assumption. A tree classiﬁer can im-
plicitly capture dependency between features. To explore
this we also experiment with a gradient boosted tree classi-
ﬁer [3]. This classiﬁer is built as the aggregation of a col-
lection of trees. To classify an instance, the set of trees are
evaluated and the average score of all trees is taken as the
classiﬁcation result.

To train the model, on each training iteration a new tree
is constructed to maximize correct classiﬁcation of the train-
ing set. To build each tree, nodes are added by selecting the
feature that best partitions the data at the current node into
the target classes. The process is repeated for the output
of each node up to a maximum depth which is speciﬁed as
a parameter. One optimization to reduce training time is
to randomly sample features for each node partition (as op-
posed to trying every possible feature). A related optimiza-
tion is to sample from the training data for each training
iteration (bag-fraction). Both of these optimization are con-
trolled through parameters. To avoid over ﬁtting you can
specify a minimum number of samples in a leaf node. Other
parameters used by the model include the learning rate, and
the maximum number of trees to train.

An interesting property of this algorithm is that as each
tree is added, the learning algorithm will evaluate the ac-
curacy of the model and apply a weight to each training
sample such that the next tree is biased towards correctly
classifying those training samples that were previously clas-
siﬁed incorrectly. In this way, each new tree is more targeted
towards those subsets of the training data that are hard to
classify.

As noted in Section 4, our labels are approximated for
each input sentence using the overall review score. To reduce
ambiguity for our model we train on only very negative (one
star) and very positive (ﬁve star) reviews. In Section 5 we
show that this results in a classiﬁer that is suitable for the
task.

Our features are tokens (or terms) extracted from the re-
view sentences. For each token we count the number of
occurrences of that token in the training sample. We choose
to stem our tokens to allow the model to better leverage the
features. Stemming is applied using the NLTK library. To
reduce the feature space we stop terms that occur in less
than 1% of our training data with the expectation that such
rare terms will not impact the ﬁnal clustering result.

The ﬁnal step for deploying our classiﬁer is to ensure high
accuracy. As not every sentence can be classiﬁed success-
fully as positive or negative, we apply two thresholds to our

User Review Sentiment Classiﬁcation and Aggregation

Steven Garcia

garcias@stanford.edu

Ping Yin

pingyin@stanford.edu

ABSTRACT
User reviews provide a wealth of information but are often
overwhelming in volume. In this work we propose a novel
approach to extract positive and negative sentiments from
user review data leveraging only the overall review scores
that are part of the data itself. We then investigate clus-
tering techniques to identify key positive and negative senti-
ment aspects to provide a user friendly summary for users.

1.

INTRODUCTION

User reviews on product websites such as Yelp or Ama-
zon provide a wealth of information about the subject of the
review. In many cases when selecting a service (i.e. restau-
rant) or when choosing a product (i.e. book, dishwasher
etc.) the amount of information available in user reviews
can be of greater volume and more trustworthy than the
oﬃcial product descriptions provided by the vendor.

The number of reviews for a single item can at times be
overwhelming. For instance, the Amazon Kindle keyboard
6” e-reader has over 42,000 user reviews as of this writing [1].
The review reader can gain an understanding of overall satis-
faction by looking at summary statistics such as the average
rating or score, but the details about what make each prod-
uct great or not are hidden inside the body of the reviews
themselves. With such a large amount of data to process, it
would be beneﬁcial to automate the process and generate a
summary of the reviews.

In this project we propose an approach to review summa-
rization that leverages trained models for classiﬁcation and
clustering to derive sentiment for the key aspects of a re-
viewed item. Given a set of item reviews, our system will
output sentences from the reviews that are representative of
key aspects for which users had strong positive and negative
opinions about.

This paper is structured as follows. In Section 2 we discuss
related works in the ares of sentiment detection and analysis.
In Section 3 we provide an overview of the Amazon and Yelp
datasets used in our experiments. In Section 4 and Section 5
we outline our novel approach to review summarization and
present results. Finally, in Section 6 we conclude our ﬁnd-
ings and outline future directions of exploration.

2. BACKGROUND

Liu and Zhang [7] outline ﬁve tasks that are core elements
of sentiment analysis: entity extraction and grouping; aspect
extraction and grouping; opinion holder and time extrac-
tion; aspect sentiment classiﬁcation; and opinion quintuple
generation. In this work we will focus on the tasks of aspect
extraction and grouping, and aspect sentiment classiﬁcation.
Pang et al. [10] successfully use bag-of-words features to
predict positive and negative labels for movie reviews. They
ﬁnd little diﬀerence between Naive Bayes and SVM classi-
ﬁers. Based on this work our experiments begin with bag-
of-words features and Naive Bayes classiﬁers.

Das et al. [5] classify user sentiment from user message
boards using a range of algorithms and show that model
selection can have a signiﬁcant impact on accuracy. Sim-
ilarly, we experiment with several models for our classiﬁer
and choose our best model using precision and recall metrics.
Cataldi et al. [4] propose a system that leverages NLP to
identify feature level sentiment. There is signiﬁcant cost in
parsing sentence structure and typically such works is lim-
ited to small data sets. Although we utilize large data sets
in this work, we explore the the impact of POS tagging in
a later component of our system where the cost to generate
this data is reduced.

3. DATASET

In this work we experiment with two datasets. The Yelp
Challenge dataset containing 1.6M reviews spanning 61K
businesses and 366K users [11]. We also utilize the Ama-
zon product data set which provides nearly two decades of
reviews for a variety of product segments [8]. From the Ama-
zon dataset we experiment with the Video Games, Tool and
Home Improvement, and Musical Instruments subsets. For
all datasets, each record contains a review text and an over-
all review score. This work is limited to consuming those
two ﬁelds but could be extended to include the data pro-
vided other ﬁelds.

For each review we breakdown the reviews into sentence
snippets and then into tokens to use as features. Addition-
ally some experiments utilize stemming and part of speech
tagging. All text manipulation, stemming and tagging was
performed using the Natural Language Toolkit [2]. Figure 1
shows the distribution of sentences per review and the break-
down of review scores per dataset.

4. METHODS

Our goal is to identify sentences that are representative of
positive and negative key aspects of the review subjects. To
this end we must establish a mechanism to identify positive
and negative sentiment within the body of each review.

A key challenge to classifying review sentences is that we
do not have labels for each individual sentence within the
data set. Given the large number of reviews in our data sets
we instead label each sentence with the overall review score.
The overall review score therefore acts as a surrogate score
for each sentence in that review. Although noisy, we can
justify this decision for two reasons. First, due to the over-
whelmingly large amount of data we expect that the learn-
ing algorithm will successfully learn to classify sentences to
a reasonable degree. As shown in Figure 1, all but one of our
data sets has at least 100,000 reviews at every rating. Sec-
ond, our extraction of representative aspect sentences does
not require every sentence to be correctly classiﬁed. Indeed,
even with a large number of unclassiﬁed sentences, we can
still produce a meaningful summary for the user.
In Sec-
tion 5 we measure the precision of this approach.

Figure 1: Dataset review and length breakdown

Having established a classiﬁcation of positive and negative
sentences within a review, we can then apply a clustering
algorithm to ﬁnd groupings which are representative of key
elements.

An overview of our system is depicted in Figure 2. The
Sentence Parser splits reviews into sentence fragments. The
Sentence Classifer then assigns a positive, negative, or unsure
label to each sentence. The Clustering component groups
together common sentences of each labeled class. Finally,
Aspect Selection ﬁlters the clusters and outputs a represen-
tative sentence for each selected grouping.
4.1 Classiﬁcation

To classify the sentences as positive or negative we con-
sider three candidate algorithms. Naive Bayes classiﬁers are
well explored in the literature for use on text classiﬁcation
problems. They are a natural ﬁt for text classiﬁcation as
they are a generative class of models that estimate the like-
lihood of observing the data. Speciﬁcally, the classiﬁer uses
Bayes rule to determine the likelihood of a class given the
data with P (y|x) = P (x|y)P (y)
((cid:81)n
, and using an independence
(cid:80)K
k ((cid:81)n
P (x)
i=1 P (xi|y=j))P (y=j)
assumption this reduces to
i=1 P (xi|y=k))P (y=k)

set of features n, aiming to predict the likelihood of observ-
ing class j in the set of classes K.

for a

McCallum et al. [9] compare Baysian models and ﬁnd that
the Multinomial Naive Bayes classiﬁer is almost uniformly
better than the Bernoulli classiﬁer. However in this work we
deal with very relatively short documents (individual sen-
tences). The Multinomial model speciﬁcally accounts for
feature frequency in determining the likelihood of a docu-
ment. This makes sense for longer documents, but for short
sentences (like the ones we aim to classify) it could be ar-
gued that the frequency of the features is less important
than the actual occurrence of those features. As such we
consider both the Bernoulli Naive Bayes classiﬁer and the

Figure 2: Overall system architecture

Multinomial Naive Bayes classiﬁer.

One limitation of the Naive Bayes classiﬁers is that they
make an independence assumption. A tree classiﬁer can im-
plicitly capture dependency between features. To explore
this we also experiment with a gradient boosted tree classi-
ﬁer [3]. This classiﬁer is built as the aggregation of a col-
lection of trees. To classify an instance, the set of trees are
evaluated and the average score of all trees is taken as the
classiﬁcation result.

To train the model, on each training iteration a new tree
is constructed to maximize correct classiﬁcation of the train-
ing set. To build each tree, nodes are added by selecting the
feature that best partitions the data at the current node into
the target classes. The process is repeated for the output
of each node up to a maximum depth which is speciﬁed as
a parameter. One optimization to reduce training time is
to randomly sample features for each node partition (as op-
posed to trying every possible feature). A related optimiza-
tion is to sample from the training data for each training
iteration (bag-fraction). Both of these optimization are con-
trolled through parameters. To avoid over ﬁtting you can
specify a minimum number of samples in a leaf node. Other
parameters used by the model include the learning rate, and
the maximum number of trees to train.

An interesting property of this algorithm is that as each
tree is added, the learning algorithm will evaluate the ac-
curacy of the model and apply a weight to each training
sample such that the next tree is biased towards correctly
classifying those training samples that were previously clas-
siﬁed incorrectly. In this way, each new tree is more targeted
towards those subsets of the training data that are hard to
classify.

As noted in Section 4, our labels are approximated for
each input sentence using the overall review score. To reduce
ambiguity for our model we train on only very negative (one
star) and very positive (ﬁve star) reviews. In Section 5 we
show that this results in a classiﬁer that is suitable for the
task.

Our features are tokens (or terms) extracted from the re-
view sentences. For each token we count the number of
occurrences of that token in the training sample. We choose
to stem our tokens to allow the model to better leverage the
features. Stemming is applied using the NLTK library. To
reduce the feature space we stop terms that occur in less
than 1% of our training data with the expectation that such
rare terms will not impact the ﬁnal clustering result.

The ﬁnal step for deploying our classiﬁer is to ensure high
accuracy. As not every sentence can be classiﬁed success-
fully as positive or negative, we apply two thresholds to our

classiﬁer. A high threshold is used to limit which instances
are classiﬁed as positive, and a low threshold is used to limit
the negative classiﬁcations. Anything in between is consid-
ered unsure. Using this approach we can control for any
target level of precision on the classiﬁcation. We select a
target of 90% precision in this work, although this can be
tuned as desired with a trade-oﬀ on recall.
4.2 Clustering and Aspect Selection

With a trained classiﬁer, we now consider the task of clus-
tering and aspect selection. We use a K-Means [6] classiﬁer
to create clusters of similar sentences among the positive and
negatively classiﬁed results. K-means works by establishing
k centroids in an n-dimensional space, and then for every
data point assigning that point to the nearest k-centroid.
Once complete the centroids are updated to be the actual
center of all items assigned to its cluster. The algorithm
then iterates the above steps until the centroids have con-
verged. Distance is typically the Euclidian distance as given
||x − µi||2, where S is the set of data points

by(cid:80)k

(cid:80)

i=0

to cluster.

x∈Si

One challenge is that the number of clusters within a posi-
tive or negative sentence set is unknown before applying the
clustering algorithm. To address this issue we propose a cri-
teria for valid clusters and iterate the clustering algorithm
over a range of k values. Once complete we select the value
of k that resulted in the largest number of valid clusters. We
deﬁne a valid cluster to be one where the most frequent 5
features/terms can be found in more than 90% of the clus-
ter, and where the cluster consists of 10 or more diﬀerent
review sentences. The ﬁrst criteria is a heuristic which en-
sures that the cluster contains a subset of terms that will
be identiﬁable for the aspect. The second criteria ensures
that our cluster is representative of multiple reviews. The
parameters are conﬁgurable and explored in Section 5.

Finally, having clustered a set of classiﬁed sentences, we
select a representative example from each valid cluster, by
selecting the sentence that is closest to the centroid using
Euclidiean distance.

5. EXPERIMENTS

For all experiments below we sample data as speciﬁed in
the experiment description. The sampled data is then split
into training and test sets at a ratio of 70% train and 30%
test. Results presented are for test data unless otherwise
speciﬁed.

Our ﬁrst experiment compares Naive Bayes classiﬁers with
the gradient boosted tree classiﬁer for the Yelp data set.
Not every sentence will be strictly positive or negative (in-
deed some may be both). As such we aim to classify only
the strongly positive and negative sentences for our ﬁnal re-
sults. To achieve high precision we apply two thresholds,
one on the upper end for positive sentiment, and one on the
lower end for negative sentiment. Figure 3 shows the recal-
l/precision curves on the test data as we vary thresholds for
both positive and negative classiﬁers. Models were trained
using 200,000 positive and negative training samples. The
results show that the gradient boosted tree provides supe-
rior accuracy on almost all points of the curve for classifying
sentences back to the original label. Based on this result we
select the gradient boosted tree classiﬁer for further experi-
ments.

As discussed in Section 4.1, the gradient boosted tree
model requires various parameters. To get the best perfor-
mance out of the model we run a sweep across the parame-

Figure 3: Recall/precision curves of positive and
negative classiﬁers for candidate algorithms

ter space selecting 100 possible parameter combinations and
choose the best performing model. Figure 4 shows the how
tuning the parameters can improve the performance of the
model by showing the reduction in L2 error. Each time we
trained our model, we ran the parameter sweep. We found
our best model (including the improvements discussed in the
remainder of Section 5) using 1,000 trees, a learning rate of
0.17, a bag fraction parameter of 0.63, feature sampling of
0.46, 200 minimum labels per leaf, and a maximum tree size
of 264. Finally we select our thresholds to obtain 90% pre-
cision for both the positive and negative classiﬁers.

One variable that can impact the performance of a model
is the number of training samples to use. To explore this
we trained our model using a range of training set sizes.
Figure 5 shows the impact on recall for the positive and
negative classiﬁers when targeting 90% precision. Note that
after splitting the Yelp data set reviews into sentences we

Figure 4: L2 error on test set for gradient boosted
tree over model parameter space for Yelp data set

User Review Sentiment Classiﬁcation and Aggregation

Steven Garcia

garcias@stanford.edu

Ping Yin

pingyin@stanford.edu

ABSTRACT
User reviews provide a wealth of information but are often
overwhelming in volume. In this work we propose a novel
approach to extract positive and negative sentiments from
user review data leveraging only the overall review scores
that are part of the data itself. We then investigate clus-
tering techniques to identify key positive and negative senti-
ment aspects to provide a user friendly summary for users.

1.

INTRODUCTION

User reviews on product websites such as Yelp or Ama-
zon provide a wealth of information about the subject of the
review. In many cases when selecting a service (i.e. restau-
rant) or when choosing a product (i.e. book, dishwasher
etc.) the amount of information available in user reviews
can be of greater volume and more trustworthy than the
oﬃcial product descriptions provided by the vendor.

The number of reviews for a single item can at times be
overwhelming. For instance, the Amazon Kindle keyboard
6” e-reader has over 42,000 user reviews as of this writing [1].
The review reader can gain an understanding of overall satis-
faction by looking at summary statistics such as the average
rating or score, but the details about what make each prod-
uct great or not are hidden inside the body of the reviews
themselves. With such a large amount of data to process, it
would be beneﬁcial to automate the process and generate a
summary of the reviews.

In this project we propose an approach to review summa-
rization that leverages trained models for classiﬁcation and
clustering to derive sentiment for the key aspects of a re-
viewed item. Given a set of item reviews, our system will
output sentences from the reviews that are representative of
key aspects for which users had strong positive and negative
opinions about.

This paper is structured as follows. In Section 2 we discuss
related works in the ares of sentiment detection and analysis.
In Section 3 we provide an overview of the Amazon and Yelp
datasets used in our experiments. In Section 4 and Section 5
we outline our novel approach to review summarization and
present results. Finally, in Section 6 we conclude our ﬁnd-
ings and outline future directions of exploration.

2. BACKGROUND

Liu and Zhang [7] outline ﬁve tasks that are core elements
of sentiment analysis: entity extraction and grouping; aspect
extraction and grouping; opinion holder and time extrac-
tion; aspect sentiment classiﬁcation; and opinion quintuple
generation. In this work we will focus on the tasks of aspect
extraction and grouping, and aspect sentiment classiﬁcation.
Pang et al. [10] successfully use bag-of-words features to
predict positive and negative labels for movie reviews. They
ﬁnd little diﬀerence between Naive Bayes and SVM classi-
ﬁers. Based on this work our experiments begin with bag-
of-words features and Naive Bayes classiﬁers.

Das et al. [5] classify user sentiment from user message
boards using a range of algorithms and show that model
selection can have a signiﬁcant impact on accuracy. Sim-
ilarly, we experiment with several models for our classiﬁer
and choose our best model using precision and recall metrics.
Cataldi et al. [4] propose a system that leverages NLP to
identify feature level sentiment. There is signiﬁcant cost in
parsing sentence structure and typically such works is lim-
ited to small data sets. Although we utilize large data sets
in this work, we explore the the impact of POS tagging in
a later component of our system where the cost to generate
this data is reduced.

3. DATASET

In this work we experiment with two datasets. The Yelp
Challenge dataset containing 1.6M reviews spanning 61K
businesses and 366K users [11]. We also utilize the Ama-
zon product data set which provides nearly two decades of
reviews for a variety of product segments [8]. From the Ama-
zon dataset we experiment with the Video Games, Tool and
Home Improvement, and Musical Instruments subsets. For
all datasets, each record contains a review text and an over-
all review score. This work is limited to consuming those
two ﬁelds but could be extended to include the data pro-
vided other ﬁelds.

For each review we breakdown the reviews into sentence
snippets and then into tokens to use as features. Addition-
ally some experiments utilize stemming and part of speech
tagging. All text manipulation, stemming and tagging was
performed using the Natural Language Toolkit [2]. Figure 1
shows the distribution of sentences per review and the break-
down of review scores per dataset.

4. METHODS

Our goal is to identify sentences that are representative of
positive and negative key aspects of the review subjects. To
this end we must establish a mechanism to identify positive
and negative sentiment within the body of each review.

A key challenge to classifying review sentences is that we
do not have labels for each individual sentence within the
data set. Given the large number of reviews in our data sets
we instead label each sentence with the overall review score.
The overall review score therefore acts as a surrogate score
for each sentence in that review. Although noisy, we can
justify this decision for two reasons. First, due to the over-
whelmingly large amount of data we expect that the learn-
ing algorithm will successfully learn to classify sentences to
a reasonable degree. As shown in Figure 1, all but one of our
data sets has at least 100,000 reviews at every rating. Sec-
ond, our extraction of representative aspect sentences does
not require every sentence to be correctly classiﬁed. Indeed,
even with a large number of unclassiﬁed sentences, we can
still produce a meaningful summary for the user.
In Sec-
tion 5 we measure the precision of this approach.

Figure 1: Dataset review and length breakdown

Having established a classiﬁcation of positive and negative
sentences within a review, we can then apply a clustering
algorithm to ﬁnd groupings which are representative of key
elements.

An overview of our system is depicted in Figure 2. The
Sentence Parser splits reviews into sentence fragments. The
Sentence Classifer then assigns a positive, negative, or unsure
label to each sentence. The Clustering component groups
together common sentences of each labeled class. Finally,
Aspect Selection ﬁlters the clusters and outputs a represen-
tative sentence for each selected grouping.
4.1 Classiﬁcation

To classify the sentences as positive or negative we con-
sider three candidate algorithms. Naive Bayes classiﬁers are
well explored in the literature for use on text classiﬁcation
problems. They are a natural ﬁt for text classiﬁcation as
they are a generative class of models that estimate the like-
lihood of observing the data. Speciﬁcally, the classiﬁer uses
Bayes rule to determine the likelihood of a class given the
data with P (y|x) = P (x|y)P (y)
((cid:81)n
, and using an independence
(cid:80)K
k ((cid:81)n
P (x)
i=1 P (xi|y=j))P (y=j)
assumption this reduces to
i=1 P (xi|y=k))P (y=k)

set of features n, aiming to predict the likelihood of observ-
ing class j in the set of classes K.

for a

McCallum et al. [9] compare Baysian models and ﬁnd that
the Multinomial Naive Bayes classiﬁer is almost uniformly
better than the Bernoulli classiﬁer. However in this work we
deal with very relatively short documents (individual sen-
tences). The Multinomial model speciﬁcally accounts for
feature frequency in determining the likelihood of a docu-
ment. This makes sense for longer documents, but for short
sentences (like the ones we aim to classify) it could be ar-
gued that the frequency of the features is less important
than the actual occurrence of those features. As such we
consider both the Bernoulli Naive Bayes classiﬁer and the

Figure 2: Overall system architecture

Multinomial Naive Bayes classiﬁer.

One limitation of the Naive Bayes classiﬁers is that they
make an independence assumption. A tree classiﬁer can im-
plicitly capture dependency between features. To explore
this we also experiment with a gradient boosted tree classi-
ﬁer [3]. This classiﬁer is built as the aggregation of a col-
lection of trees. To classify an instance, the set of trees are
evaluated and the average score of all trees is taken as the
classiﬁcation result.

To train the model, on each training iteration a new tree
is constructed to maximize correct classiﬁcation of the train-
ing set. To build each tree, nodes are added by selecting the
feature that best partitions the data at the current node into
the target classes. The process is repeated for the output
of each node up to a maximum depth which is speciﬁed as
a parameter. One optimization to reduce training time is
to randomly sample features for each node partition (as op-
posed to trying every possible feature). A related optimiza-
tion is to sample from the training data for each training
iteration (bag-fraction). Both of these optimization are con-
trolled through parameters. To avoid over ﬁtting you can
specify a minimum number of samples in a leaf node. Other
parameters used by the model include the learning rate, and
the maximum number of trees to train.

An interesting property of this algorithm is that as each
tree is added, the learning algorithm will evaluate the ac-
curacy of the model and apply a weight to each training
sample such that the next tree is biased towards correctly
classifying those training samples that were previously clas-
siﬁed incorrectly. In this way, each new tree is more targeted
towards those subsets of the training data that are hard to
classify.

As noted in Section 4, our labels are approximated for
each input sentence using the overall review score. To reduce
ambiguity for our model we train on only very negative (one
star) and very positive (ﬁve star) reviews. In Section 5 we
show that this results in a classiﬁer that is suitable for the
task.

Our features are tokens (or terms) extracted from the re-
view sentences. For each token we count the number of
occurrences of that token in the training sample. We choose
to stem our tokens to allow the model to better leverage the
features. Stemming is applied using the NLTK library. To
reduce the feature space we stop terms that occur in less
than 1% of our training data with the expectation that such
rare terms will not impact the ﬁnal clustering result.

The ﬁnal step for deploying our classiﬁer is to ensure high
accuracy. As not every sentence can be classiﬁed success-
fully as positive or negative, we apply two thresholds to our

classiﬁer. A high threshold is used to limit which instances
are classiﬁed as positive, and a low threshold is used to limit
the negative classiﬁcations. Anything in between is consid-
ered unsure. Using this approach we can control for any
target level of precision on the classiﬁcation. We select a
target of 90% precision in this work, although this can be
tuned as desired with a trade-oﬀ on recall.
4.2 Clustering and Aspect Selection

With a trained classiﬁer, we now consider the task of clus-
tering and aspect selection. We use a K-Means [6] classiﬁer
to create clusters of similar sentences among the positive and
negatively classiﬁed results. K-means works by establishing
k centroids in an n-dimensional space, and then for every
data point assigning that point to the nearest k-centroid.
Once complete the centroids are updated to be the actual
center of all items assigned to its cluster. The algorithm
then iterates the above steps until the centroids have con-
verged. Distance is typically the Euclidian distance as given
||x − µi||2, where S is the set of data points

by(cid:80)k

(cid:80)

i=0

to cluster.

x∈Si

One challenge is that the number of clusters within a posi-
tive or negative sentence set is unknown before applying the
clustering algorithm. To address this issue we propose a cri-
teria for valid clusters and iterate the clustering algorithm
over a range of k values. Once complete we select the value
of k that resulted in the largest number of valid clusters. We
deﬁne a valid cluster to be one where the most frequent 5
features/terms can be found in more than 90% of the clus-
ter, and where the cluster consists of 10 or more diﬀerent
review sentences. The ﬁrst criteria is a heuristic which en-
sures that the cluster contains a subset of terms that will
be identiﬁable for the aspect. The second criteria ensures
that our cluster is representative of multiple reviews. The
parameters are conﬁgurable and explored in Section 5.

Finally, having clustered a set of classiﬁed sentences, we
select a representative example from each valid cluster, by
selecting the sentence that is closest to the centroid using
Euclidiean distance.

5. EXPERIMENTS

For all experiments below we sample data as speciﬁed in
the experiment description. The sampled data is then split
into training and test sets at a ratio of 70% train and 30%
test. Results presented are for test data unless otherwise
speciﬁed.

Our ﬁrst experiment compares Naive Bayes classiﬁers with
the gradient boosted tree classiﬁer for the Yelp data set.
Not every sentence will be strictly positive or negative (in-
deed some may be both). As such we aim to classify only
the strongly positive and negative sentences for our ﬁnal re-
sults. To achieve high precision we apply two thresholds,
one on the upper end for positive sentiment, and one on the
lower end for negative sentiment. Figure 3 shows the recal-
l/precision curves on the test data as we vary thresholds for
both positive and negative classiﬁers. Models were trained
using 200,000 positive and negative training samples. The
results show that the gradient boosted tree provides supe-
rior accuracy on almost all points of the curve for classifying
sentences back to the original label. Based on this result we
select the gradient boosted tree classiﬁer for further experi-
ments.

As discussed in Section 4.1, the gradient boosted tree
model requires various parameters. To get the best perfor-
mance out of the model we run a sweep across the parame-

Figure 3: Recall/precision curves of positive and
negative classiﬁers for candidate algorithms

ter space selecting 100 possible parameter combinations and
choose the best performing model. Figure 4 shows the how
tuning the parameters can improve the performance of the
model by showing the reduction in L2 error. Each time we
trained our model, we ran the parameter sweep. We found
our best model (including the improvements discussed in the
remainder of Section 5) using 1,000 trees, a learning rate of
0.17, a bag fraction parameter of 0.63, feature sampling of
0.46, 200 minimum labels per leaf, and a maximum tree size
of 264. Finally we select our thresholds to obtain 90% pre-
cision for both the positive and negative classiﬁers.

One variable that can impact the performance of a model
is the number of training samples to use. To explore this
we trained our model using a range of training set sizes.
Figure 5 shows the impact on recall for the positive and
negative classiﬁers when targeting 90% precision. Note that
after splitting the Yelp data set reviews into sentences we

Figure 4: L2 error on test set for gradient boosted
tree over model parameter space for Yelp data set

clear trend of predicting a larger fraction of positive cases for
reviews that are labeled positive (4 and 5 stars) when com-
pared to classifying negative cases for the negatively labeled
reviews (1 and 2 stars).

A critical assumption in this work is that the overall re-
view label can act as a surrogate for the sentence label. To
verify that this assumption we randomly sampled 400 clas-
siﬁed sentences from 2 entities in the Yelp data and man-
ually assigned labels of positive/negative/neither to those
sentences. Table 1 shows that the positive predictor achieves
our expected precision of 90%, classifying only 19 neither
labeled examples as positive. The negative classiﬁer is less
accurate than expected with only 60% true negatives in the
negative prediction set and 14% positives classiﬁed as neg-
ative. We discuss potential improvements for the negative
classiﬁer in Section 6.

With classiﬁed sets of positive and negative sentences for
each review subject we now apply clustering and select rep-
resentative sentences from clusters. We run k-means cluster-
ing with a range of values for k from 10 to 100 and observe
three general patterns in the results: ﬁrst, a cluster may
be too narrow with a handful of sentences; second a cluster
may be too broad, this often seemed to be the case for a

Table 1: Accuracy of predictions

Prediction

Judgment
Positive
Neither
Negative

Positive
181
19
0

Negative
28
63
119

Table 2: Cluster ﬁltering accuracy
Useful
0
0
0
0
0
0
0
0
0
0

Narrow
7
14
21
29
31
44
52
61
71
85

Broad
1
1
1
1
1
1
1
1
1
1

K
10
20
30
40
50
60
70
80
90
100

Table 3: Cluster reﬁnement with noun tokens

Noun features

Useful Topic
Y
Y
Y

All features

Topic
attentive staﬀ
crab cake
chocolate strawber-
ries
ﬁlet mignon
Y
food dressings
Y
reservation
Y
romantic setting
Y
rose ﬂowers
Y
salad cart
Y
seafood
Y
sommeli
Y
table side food
Y
fantastic
N
four queen (location) N
N
incredible
N
red
N
share
treat
N

beef wellington
broad menu
chocolate stawber-
ries
classy
sommelier
sorbet
steak
table side salad
bottle
course
experience
house
old
restaurant
thing

Useful
Y
Y
Y

Y
Y
Y
Y
Y
N
N
N
N
N
N
N

Figure 5: Eﬀect of training set size on recall at 90%
precision

Figure 6: Distribution of predictions across labels
for all data sets after training model. Yelp (Y),
Amazon: Tools and Home (AT), Amazon: Video
Games (AV), and Amazon: Musical Instruments
(AM)

have on the order of 14 million sentences overall and just
over 2 million 1-star sentences. The ﬁgure shows that once
we are unable to select a balanced amount of positive and
negative training samples, our classiﬁer begins to bias to-
wards a positive prediction. It is worth noting that before
we reach the point of imbalance we see that recall contin-
ues to increase as we add more training data. This suggests
that obtaining more data could help, however the rate of
improvement is diminishing so it is unclear how much more
recall we could obtain for the cost. We chose to train our
ﬁnal model using 2 million positive and negative samples.

Our model is trained using sentence words as features. A
problem with this approach is that terms like rose and roses
are considered completely independent features although they
represent the same aspect. By applying stemming to our
feature set we can combine these features and allow the
model to better leverage the signal provided by these fea-
tures. At our selected precision level of 90%, recall for the
Yelp positive classiﬁer increases from 46% to 51%, and for
the negative classiﬁer it increases from 40% to 45%. Before
stemming our model uses 2,195 features, and after stemming
it uses 684 features.

Having trained and tested our model on the one and ﬁve
star data, we now assess the performance of the model for
all candidate sentences. Figure 6 shows the distribution of
predicted labels when compared to the overall review label
for our data sets. As expected a large fraction of the sen-
tences are classiﬁed as unsure. For all data sets there is a

User Review Sentiment Classiﬁcation and Aggregation

Steven Garcia

garcias@stanford.edu

Ping Yin

pingyin@stanford.edu

ABSTRACT
User reviews provide a wealth of information but are often
overwhelming in volume. In this work we propose a novel
approach to extract positive and negative sentiments from
user review data leveraging only the overall review scores
that are part of the data itself. We then investigate clus-
tering techniques to identify key positive and negative senti-
ment aspects to provide a user friendly summary for users.

1.

INTRODUCTION

User reviews on product websites such as Yelp or Ama-
zon provide a wealth of information about the subject of the
review. In many cases when selecting a service (i.e. restau-
rant) or when choosing a product (i.e. book, dishwasher
etc.) the amount of information available in user reviews
can be of greater volume and more trustworthy than the
oﬃcial product descriptions provided by the vendor.

The number of reviews for a single item can at times be
overwhelming. For instance, the Amazon Kindle keyboard
6” e-reader has over 42,000 user reviews as of this writing [1].
The review reader can gain an understanding of overall satis-
faction by looking at summary statistics such as the average
rating or score, but the details about what make each prod-
uct great or not are hidden inside the body of the reviews
themselves. With such a large amount of data to process, it
would be beneﬁcial to automate the process and generate a
summary of the reviews.

In this project we propose an approach to review summa-
rization that leverages trained models for classiﬁcation and
clustering to derive sentiment for the key aspects of a re-
viewed item. Given a set of item reviews, our system will
output sentences from the reviews that are representative of
key aspects for which users had strong positive and negative
opinions about.

This paper is structured as follows. In Section 2 we discuss
related works in the ares of sentiment detection and analysis.
In Section 3 we provide an overview of the Amazon and Yelp
datasets used in our experiments. In Section 4 and Section 5
we outline our novel approach to review summarization and
present results. Finally, in Section 6 we conclude our ﬁnd-
ings and outline future directions of exploration.

2. BACKGROUND

Liu and Zhang [7] outline ﬁve tasks that are core elements
of sentiment analysis: entity extraction and grouping; aspect
extraction and grouping; opinion holder and time extrac-
tion; aspect sentiment classiﬁcation; and opinion quintuple
generation. In this work we will focus on the tasks of aspect
extraction and grouping, and aspect sentiment classiﬁcation.
Pang et al. [10] successfully use bag-of-words features to
predict positive and negative labels for movie reviews. They
ﬁnd little diﬀerence between Naive Bayes and SVM classi-
ﬁers. Based on this work our experiments begin with bag-
of-words features and Naive Bayes classiﬁers.

Das et al. [5] classify user sentiment from user message
boards using a range of algorithms and show that model
selection can have a signiﬁcant impact on accuracy. Sim-
ilarly, we experiment with several models for our classiﬁer
and choose our best model using precision and recall metrics.
Cataldi et al. [4] propose a system that leverages NLP to
identify feature level sentiment. There is signiﬁcant cost in
parsing sentence structure and typically such works is lim-
ited to small data sets. Although we utilize large data sets
in this work, we explore the the impact of POS tagging in
a later component of our system where the cost to generate
this data is reduced.

3. DATASET

In this work we experiment with two datasets. The Yelp
Challenge dataset containing 1.6M reviews spanning 61K
businesses and 366K users [11]. We also utilize the Ama-
zon product data set which provides nearly two decades of
reviews for a variety of product segments [8]. From the Ama-
zon dataset we experiment with the Video Games, Tool and
Home Improvement, and Musical Instruments subsets. For
all datasets, each record contains a review text and an over-
all review score. This work is limited to consuming those
two ﬁelds but could be extended to include the data pro-
vided other ﬁelds.

For each review we breakdown the reviews into sentence
snippets and then into tokens to use as features. Addition-
ally some experiments utilize stemming and part of speech
tagging. All text manipulation, stemming and tagging was
performed using the Natural Language Toolkit [2]. Figure 1
shows the distribution of sentences per review and the break-
down of review scores per dataset.

4. METHODS

Our goal is to identify sentences that are representative of
positive and negative key aspects of the review subjects. To
this end we must establish a mechanism to identify positive
and negative sentiment within the body of each review.

A key challenge to classifying review sentences is that we
do not have labels for each individual sentence within the
data set. Given the large number of reviews in our data sets
we instead label each sentence with the overall review score.
The overall review score therefore acts as a surrogate score
for each sentence in that review. Although noisy, we can
justify this decision for two reasons. First, due to the over-
whelmingly large amount of data we expect that the learn-
ing algorithm will successfully learn to classify sentences to
a reasonable degree. As shown in Figure 1, all but one of our
data sets has at least 100,000 reviews at every rating. Sec-
ond, our extraction of representative aspect sentences does
not require every sentence to be correctly classiﬁed. Indeed,
even with a large number of unclassiﬁed sentences, we can
still produce a meaningful summary for the user.
In Sec-
tion 5 we measure the precision of this approach.

Figure 1: Dataset review and length breakdown

Having established a classiﬁcation of positive and negative
sentences within a review, we can then apply a clustering
algorithm to ﬁnd groupings which are representative of key
elements.

An overview of our system is depicted in Figure 2. The
Sentence Parser splits reviews into sentence fragments. The
Sentence Classifer then assigns a positive, negative, or unsure
label to each sentence. The Clustering component groups
together common sentences of each labeled class. Finally,
Aspect Selection ﬁlters the clusters and outputs a represen-
tative sentence for each selected grouping.
4.1 Classiﬁcation

To classify the sentences as positive or negative we con-
sider three candidate algorithms. Naive Bayes classiﬁers are
well explored in the literature for use on text classiﬁcation
problems. They are a natural ﬁt for text classiﬁcation as
they are a generative class of models that estimate the like-
lihood of observing the data. Speciﬁcally, the classiﬁer uses
Bayes rule to determine the likelihood of a class given the
data with P (y|x) = P (x|y)P (y)
((cid:81)n
, and using an independence
(cid:80)K
k ((cid:81)n
P (x)
i=1 P (xi|y=j))P (y=j)
assumption this reduces to
i=1 P (xi|y=k))P (y=k)

set of features n, aiming to predict the likelihood of observ-
ing class j in the set of classes K.

for a

McCallum et al. [9] compare Baysian models and ﬁnd that
the Multinomial Naive Bayes classiﬁer is almost uniformly
better than the Bernoulli classiﬁer. However in this work we
deal with very relatively short documents (individual sen-
tences). The Multinomial model speciﬁcally accounts for
feature frequency in determining the likelihood of a docu-
ment. This makes sense for longer documents, but for short
sentences (like the ones we aim to classify) it could be ar-
gued that the frequency of the features is less important
than the actual occurrence of those features. As such we
consider both the Bernoulli Naive Bayes classiﬁer and the

Figure 2: Overall system architecture

Multinomial Naive Bayes classiﬁer.

One limitation of the Naive Bayes classiﬁers is that they
make an independence assumption. A tree classiﬁer can im-
plicitly capture dependency between features. To explore
this we also experiment with a gradient boosted tree classi-
ﬁer [3]. This classiﬁer is built as the aggregation of a col-
lection of trees. To classify an instance, the set of trees are
evaluated and the average score of all trees is taken as the
classiﬁcation result.

To train the model, on each training iteration a new tree
is constructed to maximize correct classiﬁcation of the train-
ing set. To build each tree, nodes are added by selecting the
feature that best partitions the data at the current node into
the target classes. The process is repeated for the output
of each node up to a maximum depth which is speciﬁed as
a parameter. One optimization to reduce training time is
to randomly sample features for each node partition (as op-
posed to trying every possible feature). A related optimiza-
tion is to sample from the training data for each training
iteration (bag-fraction). Both of these optimization are con-
trolled through parameters. To avoid over ﬁtting you can
specify a minimum number of samples in a leaf node. Other
parameters used by the model include the learning rate, and
the maximum number of trees to train.

An interesting property of this algorithm is that as each
tree is added, the learning algorithm will evaluate the ac-
curacy of the model and apply a weight to each training
sample such that the next tree is biased towards correctly
classifying those training samples that were previously clas-
siﬁed incorrectly. In this way, each new tree is more targeted
towards those subsets of the training data that are hard to
classify.

As noted in Section 4, our labels are approximated for
each input sentence using the overall review score. To reduce
ambiguity for our model we train on only very negative (one
star) and very positive (ﬁve star) reviews. In Section 5 we
show that this results in a classiﬁer that is suitable for the
task.

Our features are tokens (or terms) extracted from the re-
view sentences. For each token we count the number of
occurrences of that token in the training sample. We choose
to stem our tokens to allow the model to better leverage the
features. Stemming is applied using the NLTK library. To
reduce the feature space we stop terms that occur in less
than 1% of our training data with the expectation that such
rare terms will not impact the ﬁnal clustering result.

The ﬁnal step for deploying our classiﬁer is to ensure high
accuracy. As not every sentence can be classiﬁed success-
fully as positive or negative, we apply two thresholds to our

classiﬁer. A high threshold is used to limit which instances
are classiﬁed as positive, and a low threshold is used to limit
the negative classiﬁcations. Anything in between is consid-
ered unsure. Using this approach we can control for any
target level of precision on the classiﬁcation. We select a
target of 90% precision in this work, although this can be
tuned as desired with a trade-oﬀ on recall.
4.2 Clustering and Aspect Selection

With a trained classiﬁer, we now consider the task of clus-
tering and aspect selection. We use a K-Means [6] classiﬁer
to create clusters of similar sentences among the positive and
negatively classiﬁed results. K-means works by establishing
k centroids in an n-dimensional space, and then for every
data point assigning that point to the nearest k-centroid.
Once complete the centroids are updated to be the actual
center of all items assigned to its cluster. The algorithm
then iterates the above steps until the centroids have con-
verged. Distance is typically the Euclidian distance as given
||x − µi||2, where S is the set of data points

by(cid:80)k

(cid:80)

i=0

to cluster.

x∈Si

One challenge is that the number of clusters within a posi-
tive or negative sentence set is unknown before applying the
clustering algorithm. To address this issue we propose a cri-
teria for valid clusters and iterate the clustering algorithm
over a range of k values. Once complete we select the value
of k that resulted in the largest number of valid clusters. We
deﬁne a valid cluster to be one where the most frequent 5
features/terms can be found in more than 90% of the clus-
ter, and where the cluster consists of 10 or more diﬀerent
review sentences. The ﬁrst criteria is a heuristic which en-
sures that the cluster contains a subset of terms that will
be identiﬁable for the aspect. The second criteria ensures
that our cluster is representative of multiple reviews. The
parameters are conﬁgurable and explored in Section 5.

Finally, having clustered a set of classiﬁed sentences, we
select a representative example from each valid cluster, by
selecting the sentence that is closest to the centroid using
Euclidiean distance.

5. EXPERIMENTS

For all experiments below we sample data as speciﬁed in
the experiment description. The sampled data is then split
into training and test sets at a ratio of 70% train and 30%
test. Results presented are for test data unless otherwise
speciﬁed.

Our ﬁrst experiment compares Naive Bayes classiﬁers with
the gradient boosted tree classiﬁer for the Yelp data set.
Not every sentence will be strictly positive or negative (in-
deed some may be both). As such we aim to classify only
the strongly positive and negative sentences for our ﬁnal re-
sults. To achieve high precision we apply two thresholds,
one on the upper end for positive sentiment, and one on the
lower end for negative sentiment. Figure 3 shows the recal-
l/precision curves on the test data as we vary thresholds for
both positive and negative classiﬁers. Models were trained
using 200,000 positive and negative training samples. The
results show that the gradient boosted tree provides supe-
rior accuracy on almost all points of the curve for classifying
sentences back to the original label. Based on this result we
select the gradient boosted tree classiﬁer for further experi-
ments.

As discussed in Section 4.1, the gradient boosted tree
model requires various parameters. To get the best perfor-
mance out of the model we run a sweep across the parame-

Figure 3: Recall/precision curves of positive and
negative classiﬁers for candidate algorithms

ter space selecting 100 possible parameter combinations and
choose the best performing model. Figure 4 shows the how
tuning the parameters can improve the performance of the
model by showing the reduction in L2 error. Each time we
trained our model, we ran the parameter sweep. We found
our best model (including the improvements discussed in the
remainder of Section 5) using 1,000 trees, a learning rate of
0.17, a bag fraction parameter of 0.63, feature sampling of
0.46, 200 minimum labels per leaf, and a maximum tree size
of 264. Finally we select our thresholds to obtain 90% pre-
cision for both the positive and negative classiﬁers.

One variable that can impact the performance of a model
is the number of training samples to use. To explore this
we trained our model using a range of training set sizes.
Figure 5 shows the impact on recall for the positive and
negative classiﬁers when targeting 90% precision. Note that
after splitting the Yelp data set reviews into sentences we

Figure 4: L2 error on test set for gradient boosted
tree over model parameter space for Yelp data set

clear trend of predicting a larger fraction of positive cases for
reviews that are labeled positive (4 and 5 stars) when com-
pared to classifying negative cases for the negatively labeled
reviews (1 and 2 stars).

A critical assumption in this work is that the overall re-
view label can act as a surrogate for the sentence label. To
verify that this assumption we randomly sampled 400 clas-
siﬁed sentences from 2 entities in the Yelp data and man-
ually assigned labels of positive/negative/neither to those
sentences. Table 1 shows that the positive predictor achieves
our expected precision of 90%, classifying only 19 neither
labeled examples as positive. The negative classiﬁer is less
accurate than expected with only 60% true negatives in the
negative prediction set and 14% positives classiﬁed as neg-
ative. We discuss potential improvements for the negative
classiﬁer in Section 6.

With classiﬁed sets of positive and negative sentences for
each review subject we now apply clustering and select rep-
resentative sentences from clusters. We run k-means cluster-
ing with a range of values for k from 10 to 100 and observe
three general patterns in the results: ﬁrst, a cluster may
be too narrow with a handful of sentences; second a cluster
may be too broad, this often seemed to be the case for a

Table 1: Accuracy of predictions

Prediction

Judgment
Positive
Neither
Negative

Positive
181
19
0

Negative
28
63
119

Table 2: Cluster ﬁltering accuracy
Useful
0
0
0
0
0
0
0
0
0
0

Narrow
7
14
21
29
31
44
52
61
71
85

Broad
1
1
1
1
1
1
1
1
1
1

K
10
20
30
40
50
60
70
80
90
100

Table 3: Cluster reﬁnement with noun tokens

Noun features

Useful Topic
Y
Y
Y

All features

Topic
attentive staﬀ
crab cake
chocolate strawber-
ries
ﬁlet mignon
Y
food dressings
Y
reservation
Y
romantic setting
Y
rose ﬂowers
Y
salad cart
Y
seafood
Y
sommeli
Y
table side food
Y
fantastic
N
four queen (location) N
N
incredible
N
red
N
share
treat
N

beef wellington
broad menu
chocolate stawber-
ries
classy
sommelier
sorbet
steak
table side salad
bottle
course
experience
house
old
restaurant
thing

Useful
Y
Y
Y

Y
Y
Y
Y
Y
N
N
N
N
N
N
N

Figure 5: Eﬀect of training set size on recall at 90%
precision

Figure 6: Distribution of predictions across labels
for all data sets after training model. Yelp (Y),
Amazon: Tools and Home (AT), Amazon: Video
Games (AV), and Amazon: Musical Instruments
(AM)

have on the order of 14 million sentences overall and just
over 2 million 1-star sentences. The ﬁgure shows that once
we are unable to select a balanced amount of positive and
negative training samples, our classiﬁer begins to bias to-
wards a positive prediction. It is worth noting that before
we reach the point of imbalance we see that recall contin-
ues to increase as we add more training data. This suggests
that obtaining more data could help, however the rate of
improvement is diminishing so it is unclear how much more
recall we could obtain for the cost. We chose to train our
ﬁnal model using 2 million positive and negative samples.

Our model is trained using sentence words as features. A
problem with this approach is that terms like rose and roses
are considered completely independent features although they
represent the same aspect. By applying stemming to our
feature set we can combine these features and allow the
model to better leverage the signal provided by these fea-
tures. At our selected precision level of 90%, recall for the
Yelp positive classiﬁer increases from 46% to 51%, and for
the negative classiﬁer it increases from 40% to 45%. Before
stemming our model uses 2,195 features, and after stemming
it uses 684 features.

Having trained and tested our model on the one and ﬁve
star data, we now assess the performance of the model for
all candidate sentences. Figure 6 shows the distribution of
predicted labels when compared to the overall review label
for our data sets. As expected a large fraction of the sen-
tences are classiﬁed as unsure. For all data sets there is a

Table 4: Examples from ﬁnal results

Data set

Item/Business

Yelp

Hugo’s Cellar

Amazon: Tools and Home G7 Power Minden

LED Recessed Bulb

Amazon: Video Games

God of War: Ascen-
sion

Amazon: Instruments

Brokeback Mountain +
−

single cluster which acted as a catch all for the sentences
that the algorithm was unable to classify more speciﬁcally;
ﬁnally there were clusters for groups that tended to repre-
sent a small group of features with a moderate number of
sentences. In an eﬀort to select sentences that are of value
to the user we propose a ﬁltering criteria for selecting clus-
ters which removes a cluster that consists of less than 10
review sentences. This restriction allows us to select only
aspects that are representative of a wide number of review-
ers. Secondly we require that a large portion of the cluster
(we select 90%) be represented by at most 5 features/terms.
That is, a subset of 5 features must be present in 95% of the
sentences belonging to the cluster. This allows us to ensure
that a cluster has a small focus/aspect as represented by the
feature set.

To examine our ﬁltering criteria we selected a sample re-
view set, applied our clustering algorithm and ﬁltering cri-
teria. We then labeled the ﬁltered clusters for each value of
k. Table 2 shows for each value of k the number for removed
clusters in each identiﬁed class. The results show that no
clusters are removed that would be useful for the user.

With conﬁdence that our cluster ﬁltering criteria can re-
move clusters of little value to the user we now examine
the clusters that successfully pass through the ﬁlter. After
clustering and ﬁltering we observed some clusters are still
generic in nature, for example a cluster formed primarily
around the feature incredible can contain many aspects. To
address this issue we applied POS tagging and attempted to
cluster only features that were also classiﬁed as nouns. Ta-
ble 3 shows how restricting clusters to noun features changes
the types of clusters formed. For each cluster we provide a
manual label summary to describe it, and a label indicating
if it would provide useful information to the user. However
there are still clusters that are centered on concepts of little
value to the user such as house which can relate to house
wine or on the house and so on. Further, applying this re-
striction led to a loss of aspects that were useful such as
attentive staﬀ and reservations. We proceed without POS
tagging and note that we need to further reﬁne the clustering
selection in future work.

+/−
+
+
−

+

+
−

+

+
−

+

Aspect Representation

Wonderful Caesar salad prepared table side to our speciﬁcations.
They also have an extensive wine list, and a knowledgeable som-
melier.
Tucked underneath the Four Queens Hotel in downtown Las
Vegas, this place...

Got these to replace cﬂ bulbs in my great room and kitchen and
all I can say is hot damn I love these things.
No more dimmer problems and saves me quite a bit of money on
my electric bill.
I’m going to just eat the rest of them as the burn out, and replace
them with a known brand.

I particularly found the Blades of Chaos system to be the best it
has ever been in the series.
The puzzles in God of War are great, sometimes exceedingly chal-
lenging (especially in the second game)...
Ascension just feels like a basic bare bones God of War without all
the memorable moments...

Both Michelle Williams and Anne Hathaway give commendable
performances as....
Simply put, this is the best movie I’ve seen in a while.
I thought this was supposed to be a love story??

Finally to select results for the user we choose the sen-
tence that closest to the center of the cluster. We present
a sample of the ﬁnal results for our system over the various
data sets in Table 4. We can see that the negative clusters
lack precision in some cases due to the lack of accuracy at
the classiﬁcation phase (as observed in the Yelp and Video
game examples). Similarly there are cases where the posi-
tive cluster has an aspect which is of questionable value (for
example the simply cluster for the instrument data). How-
ever, overall the system is able to identify clusters pivoted
on aspects that users should ﬁnd of interest.

6. CONCLUSIONS AND FUTURE WORK
We have developed a system to process a large number
of reviews and report sentences from the reviews that are
representative of the best and worst aspects. We found that
with only review level labels, we could adequately train a
classiﬁer to predict positive sentences. For classiﬁcation we
recommend a gradient boosted tree over Naive Bayes clas-
siﬁers. We developed a heuristic to ﬁlter the number of
clusters to present to the user for each classiﬁed review set.
Finally we show that that our system correctly identiﬁes key
aspects with end-to-end examples across multiple data sets.
Our approach of utilizing the review score for sentence
level labels worked well for positive sentences, but was less
eﬀective for negative sentences. Labeled data sets at the
sentence level may provide richer data with which to train
our model. Use of POS tagging was unable to improve the
performance of our clustering algorithm, however the lit-
erature suggests this is a candidate avenue of exploration
for the classiﬁer. Our clustering at times formed clusters
around common concepts. Advanced NLP techniques may
be of value to help improve the clustering component of our
system. Finally, our experiments made some decisions with
respect to parameterization, feature space and training data
that although pragmatic are open to scrutiny. We intend to
explore these decisions along with an exploration of other
mechanisms for selecting the optimal number of clusters as
described in the literature.

User Review Sentiment Classiﬁcation and Aggregation

Steven Garcia

garcias@stanford.edu

Ping Yin

pingyin@stanford.edu

ABSTRACT
User reviews provide a wealth of information but are often
overwhelming in volume. In this work we propose a novel
approach to extract positive and negative sentiments from
user review data leveraging only the overall review scores
that are part of the data itself. We then investigate clus-
tering techniques to identify key positive and negative senti-
ment aspects to provide a user friendly summary for users.

1.

INTRODUCTION

User reviews on product websites such as Yelp or Ama-
zon provide a wealth of information about the subject of the
review. In many cases when selecting a service (i.e. restau-
rant) or when choosing a product (i.e. book, dishwasher
etc.) the amount of information available in user reviews
can be of greater volume and more trustworthy than the
oﬃcial product descriptions provided by the vendor.

The number of reviews for a single item can at times be
overwhelming. For instance, the Amazon Kindle keyboard
6” e-reader has over 42,000 user reviews as of this writing [1].
The review reader can gain an understanding of overall satis-
faction by looking at summary statistics such as the average
rating or score, but the details about what make each prod-
uct great or not are hidden inside the body of the reviews
themselves. With such a large amount of data to process, it
would be beneﬁcial to automate the process and generate a
summary of the reviews.

In this project we propose an approach to review summa-
rization that leverages trained models for classiﬁcation and
clustering to derive sentiment for the key aspects of a re-
viewed item. Given a set of item reviews, our system will
output sentences from the reviews that are representative of
key aspects for which users had strong positive and negative
opinions about.

This paper is structured as follows. In Section 2 we discuss
related works in the ares of sentiment detection and analysis.
In Section 3 we provide an overview of the Amazon and Yelp
datasets used in our experiments. In Section 4 and Section 5
we outline our novel approach to review summarization and
present results. Finally, in Section 6 we conclude our ﬁnd-
ings and outline future directions of exploration.

2. BACKGROUND

Liu and Zhang [7] outline ﬁve tasks that are core elements
of sentiment analysis: entity extraction and grouping; aspect
extraction and grouping; opinion holder and time extrac-
tion; aspect sentiment classiﬁcation; and opinion quintuple
generation. In this work we will focus on the tasks of aspect
extraction and grouping, and aspect sentiment classiﬁcation.
Pang et al. [10] successfully use bag-of-words features to
predict positive and negative labels for movie reviews. They
ﬁnd little diﬀerence between Naive Bayes and SVM classi-
ﬁers. Based on this work our experiments begin with bag-
of-words features and Naive Bayes classiﬁers.

Das et al. [5] classify user sentiment from user message
boards using a range of algorithms and show that model
selection can have a signiﬁcant impact on accuracy. Sim-
ilarly, we experiment with several models for our classiﬁer
and choose our best model using precision and recall metrics.
Cataldi et al. [4] propose a system that leverages NLP to
identify feature level sentiment. There is signiﬁcant cost in
parsing sentence structure and typically such works is lim-
ited to small data sets. Although we utilize large data sets
in this work, we explore the the impact of POS tagging in
a later component of our system where the cost to generate
this data is reduced.

3. DATASET

In this work we experiment with two datasets. The Yelp
Challenge dataset containing 1.6M reviews spanning 61K
businesses and 366K users [11]. We also utilize the Ama-
zon product data set which provides nearly two decades of
reviews for a variety of product segments [8]. From the Ama-
zon dataset we experiment with the Video Games, Tool and
Home Improvement, and Musical Instruments subsets. For
all datasets, each record contains a review text and an over-
all review score. This work is limited to consuming those
two ﬁelds but could be extended to include the data pro-
vided other ﬁelds.

For each review we breakdown the reviews into sentence
snippets and then into tokens to use as features. Addition-
ally some experiments utilize stemming and part of speech
tagging. All text manipulation, stemming and tagging was
performed using the Natural Language Toolkit [2]. Figure 1
shows the distribution of sentences per review and the break-
down of review scores per dataset.

4. METHODS

Our goal is to identify sentences that are representative of
positive and negative key aspects of the review subjects. To
this end we must establish a mechanism to identify positive
and negative sentiment within the body of each review.

A key challenge to classifying review sentences is that we
do not have labels for each individual sentence within the
data set. Given the large number of reviews in our data sets
we instead label each sentence with the overall review score.
The overall review score therefore acts as a surrogate score
for each sentence in that review. Although noisy, we can
justify this decision for two reasons. First, due to the over-
whelmingly large amount of data we expect that the learn-
ing algorithm will successfully learn to classify sentences to
a reasonable degree. As shown in Figure 1, all but one of our
data sets has at least 100,000 reviews at every rating. Sec-
ond, our extraction of representative aspect sentences does
not require every sentence to be correctly classiﬁed. Indeed,
even with a large number of unclassiﬁed sentences, we can
still produce a meaningful summary for the user.
In Sec-
tion 5 we measure the precision of this approach.

Figure 1: Dataset review and length breakdown

Having established a classiﬁcation of positive and negative
sentences within a review, we can then apply a clustering
algorithm to ﬁnd groupings which are representative of key
elements.

An overview of our system is depicted in Figure 2. The
Sentence Parser splits reviews into sentence fragments. The
Sentence Classifer then assigns a positive, negative, or unsure
label to each sentence. The Clustering component groups
together common sentences of each labeled class. Finally,
Aspect Selection ﬁlters the clusters and outputs a represen-
tative sentence for each selected grouping.
4.1 Classiﬁcation

To classify the sentences as positive or negative we con-
sider three candidate algorithms. Naive Bayes classiﬁers are
well explored in the literature for use on text classiﬁcation
problems. They are a natural ﬁt for text classiﬁcation as
they are a generative class of models that estimate the like-
lihood of observing the data. Speciﬁcally, the classiﬁer uses
Bayes rule to determine the likelihood of a class given the
data with P (y|x) = P (x|y)P (y)
((cid:81)n
, and using an independence
(cid:80)K
k ((cid:81)n
P (x)
i=1 P (xi|y=j))P (y=j)
assumption this reduces to
i=1 P (xi|y=k))P (y=k)

set of features n, aiming to predict the likelihood of observ-
ing class j in the set of classes K.

for a

McCallum et al. [9] compare Baysian models and ﬁnd that
the Multinomial Naive Bayes classiﬁer is almost uniformly
better than the Bernoulli classiﬁer. However in this work we
deal with very relatively short documents (individual sen-
tences). The Multinomial model speciﬁcally accounts for
feature frequency in determining the likelihood of a docu-
ment. This makes sense for longer documents, but for short
sentences (like the ones we aim to classify) it could be ar-
gued that the frequency of the features is less important
than the actual occurrence of those features. As such we
consider both the Bernoulli Naive Bayes classiﬁer and the

Figure 2: Overall system architecture

Multinomial Naive Bayes classiﬁer.

One limitation of the Naive Bayes classiﬁers is that they
make an independence assumption. A tree classiﬁer can im-
plicitly capture dependency between features. To explore
this we also experiment with a gradient boosted tree classi-
ﬁer [3]. This classiﬁer is built as the aggregation of a col-
lection of trees. To classify an instance, the set of trees are
evaluated and the average score of all trees is taken as the
classiﬁcation result.

To train the model, on each training iteration a new tree
is constructed to maximize correct classiﬁcation of the train-
ing set. To build each tree, nodes are added by selecting the
feature that best partitions the data at the current node into
the target classes. The process is repeated for the output
of each node up to a maximum depth which is speciﬁed as
a parameter. One optimization to reduce training time is
to randomly sample features for each node partition (as op-
posed to trying every possible feature). A related optimiza-
tion is to sample from the training data for each training
iteration (bag-fraction). Both of these optimization are con-
trolled through parameters. To avoid over ﬁtting you can
specify a minimum number of samples in a leaf node. Other
parameters used by the model include the learning rate, and
the maximum number of trees to train.

An interesting property of this algorithm is that as each
tree is added, the learning algorithm will evaluate the ac-
curacy of the model and apply a weight to each training
sample such that the next tree is biased towards correctly
classifying those training samples that were previously clas-
siﬁed incorrectly. In this way, each new tree is more targeted
towards those subsets of the training data that are hard to
classify.

As noted in Section 4, our labels are approximated for
each input sentence using the overall review score. To reduce
ambiguity for our model we train on only very negative (one
star) and very positive (ﬁve star) reviews. In Section 5 we
show that this results in a classiﬁer that is suitable for the
task.

Our features are tokens (or terms) extracted from the re-
view sentences. For each token we count the number of
occurrences of that token in the training sample. We choose
to stem our tokens to allow the model to better leverage the
features. Stemming is applied using the NLTK library. To
reduce the feature space we stop terms that occur in less
than 1% of our training data with the expectation that such
rare terms will not impact the ﬁnal clustering result.

The ﬁnal step for deploying our classiﬁer is to ensure high
accuracy. As not every sentence can be classiﬁed success-
fully as positive or negative, we apply two thresholds to our

classiﬁer. A high threshold is used to limit which instances
are classiﬁed as positive, and a low threshold is used to limit
the negative classiﬁcations. Anything in between is consid-
ered unsure. Using this approach we can control for any
target level of precision on the classiﬁcation. We select a
target of 90% precision in this work, although this can be
tuned as desired with a trade-oﬀ on recall.
4.2 Clustering and Aspect Selection

With a trained classiﬁer, we now consider the task of clus-
tering and aspect selection. We use a K-Means [6] classiﬁer
to create clusters of similar sentences among the positive and
negatively classiﬁed results. K-means works by establishing
k centroids in an n-dimensional space, and then for every
data point assigning that point to the nearest k-centroid.
Once complete the centroids are updated to be the actual
center of all items assigned to its cluster. The algorithm
then iterates the above steps until the centroids have con-
verged. Distance is typically the Euclidian distance as given
||x − µi||2, where S is the set of data points

by(cid:80)k

(cid:80)

i=0

to cluster.

x∈Si

One challenge is that the number of clusters within a posi-
tive or negative sentence set is unknown before applying the
clustering algorithm. To address this issue we propose a cri-
teria for valid clusters and iterate the clustering algorithm
over a range of k values. Once complete we select the value
of k that resulted in the largest number of valid clusters. We
deﬁne a valid cluster to be one where the most frequent 5
features/terms can be found in more than 90% of the clus-
ter, and where the cluster consists of 10 or more diﬀerent
review sentences. The ﬁrst criteria is a heuristic which en-
sures that the cluster contains a subset of terms that will
be identiﬁable for the aspect. The second criteria ensures
that our cluster is representative of multiple reviews. The
parameters are conﬁgurable and explored in Section 5.

Finally, having clustered a set of classiﬁed sentences, we
select a representative example from each valid cluster, by
selecting the sentence that is closest to the centroid using
Euclidiean distance.

5. EXPERIMENTS

For all experiments below we sample data as speciﬁed in
the experiment description. The sampled data is then split
into training and test sets at a ratio of 70% train and 30%
test. Results presented are for test data unless otherwise
speciﬁed.

Our ﬁrst experiment compares Naive Bayes classiﬁers with
the gradient boosted tree classiﬁer for the Yelp data set.
Not every sentence will be strictly positive or negative (in-
deed some may be both). As such we aim to classify only
the strongly positive and negative sentences for our ﬁnal re-
sults. To achieve high precision we apply two thresholds,
one on the upper end for positive sentiment, and one on the
lower end for negative sentiment. Figure 3 shows the recal-
l/precision curves on the test data as we vary thresholds for
both positive and negative classiﬁers. Models were trained
using 200,000 positive and negative training samples. The
results show that the gradient boosted tree provides supe-
rior accuracy on almost all points of the curve for classifying
sentences back to the original label. Based on this result we
select the gradient boosted tree classiﬁer for further experi-
ments.

As discussed in Section 4.1, the gradient boosted tree
model requires various parameters. To get the best perfor-
mance out of the model we run a sweep across the parame-

Figure 3: Recall/precision curves of positive and
negative classiﬁers for candidate algorithms

ter space selecting 100 possible parameter combinations and
choose the best performing model. Figure 4 shows the how
tuning the parameters can improve the performance of the
model by showing the reduction in L2 error. Each time we
trained our model, we ran the parameter sweep. We found
our best model (including the improvements discussed in the
remainder of Section 5) using 1,000 trees, a learning rate of
0.17, a bag fraction parameter of 0.63, feature sampling of
0.46, 200 minimum labels per leaf, and a maximum tree size
of 264. Finally we select our thresholds to obtain 90% pre-
cision for both the positive and negative classiﬁers.

One variable that can impact the performance of a model
is the number of training samples to use. To explore this
we trained our model using a range of training set sizes.
Figure 5 shows the impact on recall for the positive and
negative classiﬁers when targeting 90% precision. Note that
after splitting the Yelp data set reviews into sentences we

Figure 4: L2 error on test set for gradient boosted
tree over model parameter space for Yelp data set

clear trend of predicting a larger fraction of positive cases for
reviews that are labeled positive (4 and 5 stars) when com-
pared to classifying negative cases for the negatively labeled
reviews (1 and 2 stars).

A critical assumption in this work is that the overall re-
view label can act as a surrogate for the sentence label. To
verify that this assumption we randomly sampled 400 clas-
siﬁed sentences from 2 entities in the Yelp data and man-
ually assigned labels of positive/negative/neither to those
sentences. Table 1 shows that the positive predictor achieves
our expected precision of 90%, classifying only 19 neither
labeled examples as positive. The negative classiﬁer is less
accurate than expected with only 60% true negatives in the
negative prediction set and 14% positives classiﬁed as neg-
ative. We discuss potential improvements for the negative
classiﬁer in Section 6.

With classiﬁed sets of positive and negative sentences for
each review subject we now apply clustering and select rep-
resentative sentences from clusters. We run k-means cluster-
ing with a range of values for k from 10 to 100 and observe
three general patterns in the results: ﬁrst, a cluster may
be too narrow with a handful of sentences; second a cluster
may be too broad, this often seemed to be the case for a

Table 1: Accuracy of predictions

Prediction

Judgment
Positive
Neither
Negative

Positive
181
19
0

Negative
28
63
119

Table 2: Cluster ﬁltering accuracy
Useful
0
0
0
0
0
0
0
0
0
0

Narrow
7
14
21
29
31
44
52
61
71
85

Broad
1
1
1
1
1
1
1
1
1
1

K
10
20
30
40
50
60
70
80
90
100

Table 3: Cluster reﬁnement with noun tokens

Noun features

Useful Topic
Y
Y
Y

All features

Topic
attentive staﬀ
crab cake
chocolate strawber-
ries
ﬁlet mignon
Y
food dressings
Y
reservation
Y
romantic setting
Y
rose ﬂowers
Y
salad cart
Y
seafood
Y
sommeli
Y
table side food
Y
fantastic
N
four queen (location) N
N
incredible
N
red
N
share
treat
N

beef wellington
broad menu
chocolate stawber-
ries
classy
sommelier
sorbet
steak
table side salad
bottle
course
experience
house
old
restaurant
thing

Useful
Y
Y
Y

Y
Y
Y
Y
Y
N
N
N
N
N
N
N

Figure 5: Eﬀect of training set size on recall at 90%
precision

Figure 6: Distribution of predictions across labels
for all data sets after training model. Yelp (Y),
Amazon: Tools and Home (AT), Amazon: Video
Games (AV), and Amazon: Musical Instruments
(AM)

have on the order of 14 million sentences overall and just
over 2 million 1-star sentences. The ﬁgure shows that once
we are unable to select a balanced amount of positive and
negative training samples, our classiﬁer begins to bias to-
wards a positive prediction. It is worth noting that before
we reach the point of imbalance we see that recall contin-
ues to increase as we add more training data. This suggests
that obtaining more data could help, however the rate of
improvement is diminishing so it is unclear how much more
recall we could obtain for the cost. We chose to train our
ﬁnal model using 2 million positive and negative samples.

Our model is trained using sentence words as features. A
problem with this approach is that terms like rose and roses
are considered completely independent features although they
represent the same aspect. By applying stemming to our
feature set we can combine these features and allow the
model to better leverage the signal provided by these fea-
tures. At our selected precision level of 90%, recall for the
Yelp positive classiﬁer increases from 46% to 51%, and for
the negative classiﬁer it increases from 40% to 45%. Before
stemming our model uses 2,195 features, and after stemming
it uses 684 features.

Having trained and tested our model on the one and ﬁve
star data, we now assess the performance of the model for
all candidate sentences. Figure 6 shows the distribution of
predicted labels when compared to the overall review label
for our data sets. As expected a large fraction of the sen-
tences are classiﬁed as unsure. For all data sets there is a

Table 4: Examples from ﬁnal results

Data set

Item/Business

Yelp

Hugo’s Cellar

Amazon: Tools and Home G7 Power Minden

LED Recessed Bulb

Amazon: Video Games

God of War: Ascen-
sion

Amazon: Instruments

Brokeback Mountain +
−

single cluster which acted as a catch all for the sentences
that the algorithm was unable to classify more speciﬁcally;
ﬁnally there were clusters for groups that tended to repre-
sent a small group of features with a moderate number of
sentences. In an eﬀort to select sentences that are of value
to the user we propose a ﬁltering criteria for selecting clus-
ters which removes a cluster that consists of less than 10
review sentences. This restriction allows us to select only
aspects that are representative of a wide number of review-
ers. Secondly we require that a large portion of the cluster
(we select 90%) be represented by at most 5 features/terms.
That is, a subset of 5 features must be present in 95% of the
sentences belonging to the cluster. This allows us to ensure
that a cluster has a small focus/aspect as represented by the
feature set.

To examine our ﬁltering criteria we selected a sample re-
view set, applied our clustering algorithm and ﬁltering cri-
teria. We then labeled the ﬁltered clusters for each value of
k. Table 2 shows for each value of k the number for removed
clusters in each identiﬁed class. The results show that no
clusters are removed that would be useful for the user.

With conﬁdence that our cluster ﬁltering criteria can re-
move clusters of little value to the user we now examine
the clusters that successfully pass through the ﬁlter. After
clustering and ﬁltering we observed some clusters are still
generic in nature, for example a cluster formed primarily
around the feature incredible can contain many aspects. To
address this issue we applied POS tagging and attempted to
cluster only features that were also classiﬁed as nouns. Ta-
ble 3 shows how restricting clusters to noun features changes
the types of clusters formed. For each cluster we provide a
manual label summary to describe it, and a label indicating
if it would provide useful information to the user. However
there are still clusters that are centered on concepts of little
value to the user such as house which can relate to house
wine or on the house and so on. Further, applying this re-
striction led to a loss of aspects that were useful such as
attentive staﬀ and reservations. We proceed without POS
tagging and note that we need to further reﬁne the clustering
selection in future work.

+/−
+
+
−

+

+
−

+

+
−

+

Aspect Representation

Wonderful Caesar salad prepared table side to our speciﬁcations.
They also have an extensive wine list, and a knowledgeable som-
melier.
Tucked underneath the Four Queens Hotel in downtown Las
Vegas, this place...

Got these to replace cﬂ bulbs in my great room and kitchen and
all I can say is hot damn I love these things.
No more dimmer problems and saves me quite a bit of money on
my electric bill.
I’m going to just eat the rest of them as the burn out, and replace
them with a known brand.

I particularly found the Blades of Chaos system to be the best it
has ever been in the series.
The puzzles in God of War are great, sometimes exceedingly chal-
lenging (especially in the second game)...
Ascension just feels like a basic bare bones God of War without all
the memorable moments...

Both Michelle Williams and Anne Hathaway give commendable
performances as....
Simply put, this is the best movie I’ve seen in a while.
I thought this was supposed to be a love story??

Finally to select results for the user we choose the sen-
tence that closest to the center of the cluster. We present
a sample of the ﬁnal results for our system over the various
data sets in Table 4. We can see that the negative clusters
lack precision in some cases due to the lack of accuracy at
the classiﬁcation phase (as observed in the Yelp and Video
game examples). Similarly there are cases where the posi-
tive cluster has an aspect which is of questionable value (for
example the simply cluster for the instrument data). How-
ever, overall the system is able to identify clusters pivoted
on aspects that users should ﬁnd of interest.

6. CONCLUSIONS AND FUTURE WORK
We have developed a system to process a large number
of reviews and report sentences from the reviews that are
representative of the best and worst aspects. We found that
with only review level labels, we could adequately train a
classiﬁer to predict positive sentences. For classiﬁcation we
recommend a gradient boosted tree over Naive Bayes clas-
siﬁers. We developed a heuristic to ﬁlter the number of
clusters to present to the user for each classiﬁed review set.
Finally we show that that our system correctly identiﬁes key
aspects with end-to-end examples across multiple data sets.
Our approach of utilizing the review score for sentence
level labels worked well for positive sentences, but was less
eﬀective for negative sentences. Labeled data sets at the
sentence level may provide richer data with which to train
our model. Use of POS tagging was unable to improve the
performance of our clustering algorithm, however the lit-
erature suggests this is a candidate avenue of exploration
for the classiﬁer. Our clustering at times formed clusters
around common concepts. Advanced NLP techniques may
be of value to help improve the clustering component of our
system. Finally, our experiments made some decisions with
respect to parameterization, feature space and training data
that although pragmatic are open to scrutiny. We intend to
explore these decisions along with an exploration of other
mechanisms for selecting the optimal number of clusters as
described in the literature.

7. REFERENCES
[1] Amazon. Kindle keyboard, wi-ﬁ, 6” e ink display.

http://www.amazon.com/
Kindle-Wireless-Reader-Wifi-Graphite/
product-reviews/B002Y27P3M/. Accessed November
11, 2015.

[2] S. Bird. Nltk: the natural language toolkit. In

Proceedings of the COLING/ACL on Interactive
presentation sessions, pages 69–72. Association for
Computational Linguistics, 2006.

[3] C. J. Burges. From ranknet to lambdarank to
lambdamart: An overview. Technical Report
MSR-TR-2010-82, June 2010.

[4] M. Cataldi, A. Ballatore, I. Tiddi, and M.-A. Aufaure.

Good location, terrible food: detecting feature
sentiment in user-generated reviews. Social Network
Analysis and Mining, 3(4):1149–1163, 2013.

[5] S. R. Das and M. Y. Chen. Yahoo! for amazon:

Sentiment extraction from small talk on the web.
Management Science, 53(9):1375–1388, 2007.

[6] J. A. Hartigan and M. A. Wong. Algorithm as 136: A
k-means clustering algorithm. Applied statistics, pages
100–108, 1979.

[7] B. Liu and L. Zhang. A survey of opinion mining and

sentiment analysis. In C. C. Aggarwal and C. Zhai,
editors, Mining Text Data, pages 415–463. Springer
US, 2012.

[8] J. J. McAuley, C. Targett, Q. Shi, and A. van den

Hengel. Image-based recommendations on styles and
substitutes. In R. A. Baeza-Yates, M. Lalmas,
A. Moﬀat, and B. A. Ribeiro-Neto, editors, SIGIR,
pages 43–52. ACM, 2015.

[9] A. McCallum, K. Nigam, et al. A comparison of event
models for naive bayes text classiﬁcation. In AAAI-98
workshop on learning for text categorization, volume
752, pages 41–48, 1998.

[10] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?:

sentiment classiﬁcation using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language
processing-Volume 10, pages 79–86. Association for
Computational Linguistics, 2002.

[11] Yelp. Yelp challenge dataset.

https://www.yelp.com/dataset_challenge/dataset.
Accessed October 16, 2015.

