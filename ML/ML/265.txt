	 Â 1	 Â 

 

Classification of Cardiac Arrhythmias Patients 

Azar	 Â Fazel,	 Â Fatema	 Â Algharbi,	 Â Batool	 Â Haider	 Â 
Azar Fazel, Fatema Algharbi, Batool Haider 
CS229 Final Project Report, Fall 2014 

CS229	 Â Final	 Â Project	 Â Report,	 Â Fall	 Â 2014	 Â 

Abstract: 

Cardiac Arrhythmias are any of a group of conditions in which the electrical activity 
of the heart is irregular or is faster or slower than normal. It is the leading cause of death 
for  both  men  and  women  in  the  world  [1][2].  In  this  project,  we  aim  to  classify  heart 
arrhythmias  patients  among  16  different  classes  based  on  ECG  (Electrocardiography) 
data.  After  applying  rigorous  data  pre-processing  and  feature  selection  techniques,  we 
used 5 different machine learning algorithms; SVM, Logistic Regression, KNN, Random 
Forest and Decision Trees. Our best accuracy was 73%, obtained via SVM. We also used 
some of these methods to come up with the most important attributes that determined the 
class  of  arrhythmia.  This  work  can  be  of  immense  importance  to  researchers  who  are 
exploring  various  techniques  to  capture  the  key  pre-informers  of  a  potential  cardiac 
disease, well before it is too late. 

	 Â 

 

Introduction 
Heart  diseases  kill  more 
than  385,000  people 
annually.  In  the  United  States,  someone  has  a  heart 
attack every 34 seconds [4]. In this paper, we present 
our  methodology  and  the  outcomes  of  developing  a 
machine learning system that is capable of classifying 
a  patient 
into  16  different  cardiac  arrhythmic 
categories. This work has great potential to serve the 
medicine industry. With the advancement in medical 
technology, database will only continue to grow. The 
evolution of smart body chips capable of sending real 
time  patientsâ€™ 
information  are  rigorously  being 
researched  upon.  Algorithms  such  as  these  can  be 
have  ground  breaking  impact  regarding  helping 
researchers target the keys features that cause cardiac 
arrhythmia  and  assist  them  in  classifying  patients  in 
right categories, so as to be able to take measures in 
the right direction. 
 
Data 
    The  data  has  been  taken  from  a  well-maintained 
ECG 
database 
(https://archive.ics.uci.edu/ml/datasets/Arrhythmia). It 
contains  279  attributes 
related 
variables)  and  452  instances.  The  variable  â€˜Classâ€™  is 
our target variable.  Class 01 refers to 'normal' ECG, 
classes  02 
to  different  classes  of 
Arrhythmia  and  class  16  refers  to  the  rest  of 
unclassified classes. Figure (1) shows the distribution 
of different classes in our database. As can be seen, 
almost half of the instances are classified to class 1, 

(Electrocardiography) 

(ECG/  Patient 

to  15  refer 

 

3

 

 

4

 

 

0
5

 

5
2

 

3
1

 

5
1

 

5
1

 

4
4

  9
2

 

5
4
2

2
  2
5

 
s
e
c
n
a
t
s
n
I
 
f
o
 
r
e
b
m
u
N

while there are few instances in other classes. Thus, in 
the database, we do not have much evidence for some 
of the classes like class 02 or 03. 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
 
 
Analysis & Data Pre-processing 
    Following are the main steps took to process the 
data. 
 
1) Normalization 
    Since many algorithms require normalized dataset, 
we normalized all the feature values except â€œgenderâ€ 

using  Z-score  (ğ‘§=!!! Â ! )  normalization  and  scaled 

Fig. 1. Distribution of instances in various classes 

1  2  3  4  5  6  7  8  9  10  11  12  13 

them between 0 and 1. We then proceeded on towards 
analyzing the data. 

 Classes 

  

2) Feature Selection:  
    We first ran various algorithms on the data set with 
all  the  features.  The  performance  of  most  was  not 

	 Â 1	 Â 

 

Classification of Cardiac Arrhythmias Patients 

Azar	 Â Fazel,	 Â Fatema	 Â Algharbi,	 Â Batool	 Â Haider	 Â 
Azar Fazel, Fatema Algharbi, Batool Haider 
CS229 Final Project Report, Fall 2014 

CS229	 Â Final	 Â Project	 Â Report,	 Â Fall	 Â 2014	 Â 

Abstract: 

Cardiac Arrhythmias are any of a group of conditions in which the electrical activity 
of the heart is irregular or is faster or slower than normal. It is the leading cause of death 
for  both  men  and  women  in  the  world  [1][2].  In  this  project,  we  aim  to  classify  heart 
arrhythmias  patients  among  16  different  classes  based  on  ECG  (Electrocardiography) 
data.  After  applying  rigorous  data  pre-processing  and  feature  selection  techniques,  we 
used 5 different machine learning algorithms; SVM, Logistic Regression, KNN, Random 
Forest and Decision Trees. Our best accuracy was 73%, obtained via SVM. We also used 
some of these methods to come up with the most important attributes that determined the 
class  of  arrhythmia.  This  work  can  be  of  immense  importance  to  researchers  who  are 
exploring  various  techniques  to  capture  the  key  pre-informers  of  a  potential  cardiac 
disease, well before it is too late. 

	 Â 

 

Introduction 
Heart  diseases  kill  more 
than  385,000  people 
annually.  In  the  United  States,  someone  has  a  heart 
attack every 34 seconds [4]. In this paper, we present 
our  methodology  and  the  outcomes  of  developing  a 
machine learning system that is capable of classifying 
a  patient 
into  16  different  cardiac  arrhythmic 
categories. This work has great potential to serve the 
medicine industry. With the advancement in medical 
technology, database will only continue to grow. The 
evolution of smart body chips capable of sending real 
time  patientsâ€™ 
information  are  rigorously  being 
researched  upon.  Algorithms  such  as  these  can  be 
have  ground  breaking  impact  regarding  helping 
researchers target the keys features that cause cardiac 
arrhythmia  and  assist  them  in  classifying  patients  in 
right categories, so as to be able to take measures in 
the right direction. 
 
Data 
    The  data  has  been  taken  from  a  well-maintained 
ECG 
database 
(https://archive.ics.uci.edu/ml/datasets/Arrhythmia). It 
contains  279  attributes 
related 
variables)  and  452  instances.  The  variable  â€˜Classâ€™  is 
our target variable.  Class 01 refers to 'normal' ECG, 
classes  02 
to  different  classes  of 
Arrhythmia  and  class  16  refers  to  the  rest  of 
unclassified classes. Figure (1) shows the distribution 
of different classes in our database. As can be seen, 
almost half of the instances are classified to class 1, 

(Electrocardiography) 

(ECG/  Patient 

to  15  refer 

 

3

 

 

4

 

 

0
5

 

5
2

 

3
1

 

5
1

 

5
1

 

4
4

  9
2

 

5
4
2

2
  2
5

 
s
e
c
n
a
t
s
n
I
 
f
o
 
r
e
b
m
u
N

while there are few instances in other classes. Thus, in 
the database, we do not have much evidence for some 
of the classes like class 02 or 03. 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
 
 
Analysis & Data Pre-processing 
    Following are the main steps took to process the 
data. 
 
1) Normalization 
    Since many algorithms require normalized dataset, 
we normalized all the feature values except â€œgenderâ€ 

using  Z-score  (ğ‘§=!!! Â ! )  normalization  and  scaled 

Fig. 1. Distribution of instances in various classes 

1  2  3  4  5  6  7  8  9  10  11  12  13 

them between 0 and 1. We then proceeded on towards 
analyzing the data. 

 Classes 

  

2) Feature Selection:  
    We first ran various algorithms on the data set with 
all  the  features.  The  performance  of  most  was  not 

	 Â 2	 Â 
satisfactory.  With  the  instances  to  feature  ratio  of 
only 1.6, we decided to improve the performance of 
algorithms by reducing the number of features.  
 
i) Invariant Features 
    Firstly,  we  removed  some  the  categorical  features 
that were 95% of time indicating either all 0â€™s or all 
1â€™s. These seemed not to help a lot in decision making 
as  they  were  pointing  to  the  same  category  most  of 
the time. As expected, their removal, did not disturb 
the  accuracy  but  reduced  the  feature  space  and 
simplified the data base.	 Â 
	 Â 
ii) Mutual information:  
    Next,  we  found  tuples  of  features  that  were 
correlated  at  least  with  0.95  correlations.  This  is  to 
say the set of features giving approximately the same 
information,  and  so  we  included  only  one  feature 
from each tuple. This reduced the number of features 
to 160. 
 
iii) Feature Importance Score via Decision trees:  
    We took two approaches: the first was to cluster the 
classes  into  â€œClass  1â€  and  â€œNot  Class  1â€.  We  then 
found  the  features  that  best  described  the  responses 
for  these  two  large  classes.  In  the  second  approach, 
we used all the classes in their original labeling in the 
dataset and found out the most important features for 
the  response  via  decision  tree.  Figure  2.1  shows  the 
results of decision tree. As seen in this figure, based 
on the features importance score, only 11 features out 
of 160 features were considered as the most important 
features. 

 
 
 
 
 
 
 
 
 
 
 
 
 

Fig. 2.1 Decision Tree for Cardiac Arrhythmias patients 

 
 
Figure 2.2 shows the most important 30 variables for 
the response using the decision trees. We noticed that 

1

4

7

8

9

5

6

2

3

10

14

15

16

V

5

0.5

I
 
e
l
b
a
i
r
a
V

 
e
c
n
a
t
r
o
p
m

40 
35 
30 
25 
20 
15 
10 
5 
0 

using decision trees 

Fig. 2.2 Variable Importance for Cardiac Arrhythmias patients 

the heart rate is the most important variable followed 
by V6_T_wave, V5_T_wave and V3_Q_wave. 
 
 
 
 
 
 
 
 
 
 
 
 
 
We  then  applied  Principle  Components  Analysis 
(PCA) to these features to reduce the dimensionality 
of the dataset while retaining most of the variation in 
the  data  set.  Figure  3.  shows  the  results  of  this 
analysis.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Models: 
    We  applied  several  models  and  algorithms  to  our 
dataset,  accounting  for  their  merits  and  demerits 
based on literature review. These are as follows: 
 
1) KNN (K-Nearest Neighbors) 
    We used KNN because it is simple to implement & 
very straight forward. Here, an object is classified by 
a majority vote of its neighbors, with the object being 
assigned 
the  class  most  common  among 
its k nearest  neighbors  [3].  This  could  be  done  by 
measuring  â€˜distancesâ€™  between  the  object  and  its 
neighbors.  The 
a 
representation of simple Euclidian distance, where â€˜aâ€™ 
and â€˜bâ€™ are the respective positions of the object and 
one of its neighbors. 

Fig. 3. Using PCA to visualize the features selected by Decision 

following 

V3_Q2_wave
V3_R2_wave

formula 

shows 

PC1 (23.6% explained var.)

_

w

T

_

V6_T_wave
DII_T_wave
aveDI_T_wave

0.0
Tree 

AVR_T_wave

_ w a
_ R N O T
T
O
N
R
_
n s
1
V
efle ctio
d

V3_Q_wave

V 1 _ Q R S A
A
e
v
_
2
_
2
V

w

S

Q

v

a

e

R

to 

)
.
r
a
v
 

i

l

d
e
n
a
p
x
e
%
4

 

.

8
1
(
 
2
C
P

e
v
a
w
_
Q
_
F
V
A

V 1
_intr _

âˆ’0.5

âˆ’0.5

0.0

H
e

a

r

t

_

r

a

t

e

0.5

1.0

1

V

	 Â 1	 Â 

 

Classification of Cardiac Arrhythmias Patients 

Azar	 Â Fazel,	 Â Fatema	 Â Algharbi,	 Â Batool	 Â Haider	 Â 
Azar Fazel, Fatema Algharbi, Batool Haider 
CS229 Final Project Report, Fall 2014 

CS229	 Â Final	 Â Project	 Â Report,	 Â Fall	 Â 2014	 Â 

Abstract: 

Cardiac Arrhythmias are any of a group of conditions in which the electrical activity 
of the heart is irregular or is faster or slower than normal. It is the leading cause of death 
for  both  men  and  women  in  the  world  [1][2].  In  this  project,  we  aim  to  classify  heart 
arrhythmias  patients  among  16  different  classes  based  on  ECG  (Electrocardiography) 
data.  After  applying  rigorous  data  pre-processing  and  feature  selection  techniques,  we 
used 5 different machine learning algorithms; SVM, Logistic Regression, KNN, Random 
Forest and Decision Trees. Our best accuracy was 73%, obtained via SVM. We also used 
some of these methods to come up with the most important attributes that determined the 
class  of  arrhythmia.  This  work  can  be  of  immense  importance  to  researchers  who  are 
exploring  various  techniques  to  capture  the  key  pre-informers  of  a  potential  cardiac 
disease, well before it is too late. 

	 Â 

 

Introduction 
Heart  diseases  kill  more 
than  385,000  people 
annually.  In  the  United  States,  someone  has  a  heart 
attack every 34 seconds [4]. In this paper, we present 
our  methodology  and  the  outcomes  of  developing  a 
machine learning system that is capable of classifying 
a  patient 
into  16  different  cardiac  arrhythmic 
categories. This work has great potential to serve the 
medicine industry. With the advancement in medical 
technology, database will only continue to grow. The 
evolution of smart body chips capable of sending real 
time  patientsâ€™ 
information  are  rigorously  being 
researched  upon.  Algorithms  such  as  these  can  be 
have  ground  breaking  impact  regarding  helping 
researchers target the keys features that cause cardiac 
arrhythmia  and  assist  them  in  classifying  patients  in 
right categories, so as to be able to take measures in 
the right direction. 
 
Data 
    The  data  has  been  taken  from  a  well-maintained 
ECG 
database 
(https://archive.ics.uci.edu/ml/datasets/Arrhythmia). It 
contains  279  attributes 
related 
variables)  and  452  instances.  The  variable  â€˜Classâ€™  is 
our target variable.  Class 01 refers to 'normal' ECG, 
classes  02 
to  different  classes  of 
Arrhythmia  and  class  16  refers  to  the  rest  of 
unclassified classes. Figure (1) shows the distribution 
of different classes in our database. As can be seen, 
almost half of the instances are classified to class 1, 

(Electrocardiography) 

(ECG/  Patient 

to  15  refer 

 

3

 

 

4

 

 

0
5

 

5
2

 

3
1

 

5
1

 

5
1

 

4
4

  9
2

 

5
4
2

2
  2
5

 
s
e
c
n
a
t
s
n
I
 
f
o
 
r
e
b
m
u
N

while there are few instances in other classes. Thus, in 
the database, we do not have much evidence for some 
of the classes like class 02 or 03. 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
 
 
Analysis & Data Pre-processing 
    Following are the main steps took to process the 
data. 
 
1) Normalization 
    Since many algorithms require normalized dataset, 
we normalized all the feature values except â€œgenderâ€ 

using  Z-score  (ğ‘§=!!! Â ! )  normalization  and  scaled 

Fig. 1. Distribution of instances in various classes 

1  2  3  4  5  6  7  8  9  10  11  12  13 

them between 0 and 1. We then proceeded on towards 
analyzing the data. 

 Classes 

  

2) Feature Selection:  
    We first ran various algorithms on the data set with 
all  the  features.  The  performance  of  most  was  not 

	 Â 2	 Â 
satisfactory.  With  the  instances  to  feature  ratio  of 
only 1.6, we decided to improve the performance of 
algorithms by reducing the number of features.  
 
i) Invariant Features 
    Firstly,  we  removed  some  the  categorical  features 
that were 95% of time indicating either all 0â€™s or all 
1â€™s. These seemed not to help a lot in decision making 
as  they  were  pointing  to  the  same  category  most  of 
the time. As expected, their removal, did not disturb 
the  accuracy  but  reduced  the  feature  space  and 
simplified the data base.	 Â 
	 Â 
ii) Mutual information:  
    Next,  we  found  tuples  of  features  that  were 
correlated  at  least  with  0.95  correlations.  This  is  to 
say the set of features giving approximately the same 
information,  and  so  we  included  only  one  feature 
from each tuple. This reduced the number of features 
to 160. 
 
iii) Feature Importance Score via Decision trees:  
    We took two approaches: the first was to cluster the 
classes  into  â€œClass  1â€  and  â€œNot  Class  1â€.  We  then 
found  the  features  that  best  described  the  responses 
for  these  two  large  classes.  In  the  second  approach, 
we used all the classes in their original labeling in the 
dataset and found out the most important features for 
the  response  via  decision  tree.  Figure  2.1  shows  the 
results of decision tree. As seen in this figure, based 
on the features importance score, only 11 features out 
of 160 features were considered as the most important 
features. 

 
 
 
 
 
 
 
 
 
 
 
 
 

Fig. 2.1 Decision Tree for Cardiac Arrhythmias patients 

 
 
Figure 2.2 shows the most important 30 variables for 
the response using the decision trees. We noticed that 

1

4

7

8

9

5

6

2

3

10

14

15

16

V

5

0.5

I
 
e
l
b
a
i
r
a
V

 
e
c
n
a
t
r
o
p
m

40 
35 
30 
25 
20 
15 
10 
5 
0 

using decision trees 

Fig. 2.2 Variable Importance for Cardiac Arrhythmias patients 

the heart rate is the most important variable followed 
by V6_T_wave, V5_T_wave and V3_Q_wave. 
 
 
 
 
 
 
 
 
 
 
 
 
 
We  then  applied  Principle  Components  Analysis 
(PCA) to these features to reduce the dimensionality 
of the dataset while retaining most of the variation in 
the  data  set.  Figure  3.  shows  the  results  of  this 
analysis.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Models: 
    We  applied  several  models  and  algorithms  to  our 
dataset,  accounting  for  their  merits  and  demerits 
based on literature review. These are as follows: 
 
1) KNN (K-Nearest Neighbors) 
    We used KNN because it is simple to implement & 
very straight forward. Here, an object is classified by 
a majority vote of its neighbors, with the object being 
assigned 
the  class  most  common  among 
its k nearest  neighbors  [3].  This  could  be  done  by 
measuring  â€˜distancesâ€™  between  the  object  and  its 
neighbors.  The 
a 
representation of simple Euclidian distance, where â€˜aâ€™ 
and â€˜bâ€™ are the respective positions of the object and 
one of its neighbors. 

Fig. 3. Using PCA to visualize the features selected by Decision 

following 

V3_Q2_wave
V3_R2_wave

formula 

shows 

PC1 (23.6% explained var.)

_

w

T

_

V6_T_wave
DII_T_wave
aveDI_T_wave

0.0
Tree 

AVR_T_wave

_ w a
_ R N O T
T
O
N
R
_
n s
1
V
efle ctio
d

V3_Q_wave

V 1 _ Q R S A
A
e
v
_
2
_
2
V

w

S

Q

v

a

e

R

to 

)
.
r
a
v
 

i

l

d
e
n
a
p
x
e
%
4

 

.

8
1
(
 
2
C
P

e
v
a
w
_
Q
_
F
V
A

V 1
_intr _

âˆ’0.5

âˆ’0.5

0.0

H
e

a

r

t

_

r

a

t

e

0.5

1.0

1

V

	 Â 

	 Â 

	 Â 3	 Â 

 
 

D(a,b) =

n

âˆ‘

(bi âˆ’ ai )2

i

	 Â 
KNN  is  very  sensitive  to  irrelevant  or  redundant 
features  because  all  features  contribute 
the 
similarity  and  thus  to  the  classification.  This  was 
ameliorated  by  careful  feature  selection  described 
previously. 

to 

2) Decision Trees 
    We used Decision trees as they implicitly perform 
feature selection & can tackle nonlinear relationships 
between  parameters.  Each  leaf  of  the  tree  is  labeled 
with  a  class  or  probability  distribution  over  the 
classes. 
A tree can be "learned" by splitting the source set into 
subsets  based  on  attributes  and  the  recursion is 
completed when splitting no longer adds value to the 
predictions.  The  information  gained  is  based  on  the 
decrease in â€œentropyâ€ after dataset is split. Following 
equation shows the formula for entropy, where â€˜pâ€™ is 
the  probability  of  certain  class  occurring,  given  a 
specific feature 
p P
(

	 Â 
3) Random Forest  
    We  tried  Random  forests  as  they are  an ensemble 
learning method 
constructing 
multitude  of  decision 
their 
performance is often better than decision trees alone 
and can tackle issues like â€˜pruningâ€™ (often an issue in 
decision  tree)  automatically.    Since  random  forest 
works quite well with several features, we first tried it 
on  the  full  set  of  features.  Table  1  shows  its 
performance class wise. 
 

that  operate  by 

trees.  Therefore 

p P
)log2 (

)log2 (

E s
( )

p N
(

p N

=âˆ’

)
âˆ’

)

Class  N Cases  N Instances 
misclassified 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
14 
15 
16 

245 
44 
15 
15 
13 
25 
3 
2 
9 
50 
4 
5 
22 

192 
24 
1 
1 
6 
12 
3 
2 
1 
29 
1 
2 
21 

Pct. Error 
78.37% 
54.55% 
6.67% 
6.67% 
46.15% 
48.00% 
100.00% 
100.00% 
11.11% 
58.00% 
25.00% 
40.00% 
95.45% 

Table 1. Random Forest Performance on full feature set 

	 Â 

r
o
r
r
e

 

n
o

i
t

a
c

i
f
i

s
s
a
c
 

l

g
a
b
-
f

o
-
t

u
O

0.56

0.55

0.54

0.53

0.52

0.51

0.5

0.49

0

100

200

300

 
 

 
 
 
 
 
 
 
 

 
 

 
Since  the  error  obtained  was  quite  high  (approx. 
50%),  we  then  tried  it  on  the  reduced  features  set 
outlined previously. The error graph (figure 4) below 
shows that the performance did not improve much. 

 

400

600
Number of grown trees

500

700

800

900

1000

Figure 4. Random Forest Performance on reduced 

 
 
 
 
4) SVM (Support Vector Machines) 

feature set 

	 Â 

We used SVM with a cross validation of k = 10 
to allow us average the error, among the 10 accuracy 
we found, the highest that was reached is 0.8 and the 
mean was approximately 0.73.  
We  tried  both  the  polynomial  and  the  linear  kernels 
for  the  SVM  and  found  out  that  the  linear  kernel 
outperformed the polynomial kernel. The linear SVM 
with CV = 10 gave the best accuracy among all the 
other models we used. The features that were selected 
for the SVM where determined by using the decision 
trees as explained in the feature selection section.   
 
4) Logistic Regression  

Since  the  logistic  regression  is  used  for  binary 
classification  of  datasets  with  categorical  dependent 
features,  in  order  to  apply  logistic  regression  to  our 
multi-class dataset, we firstly classified our instances 
into  two  major  classes,  class  1  (which  contained  all 
the instances with â€œclass 01â€ label) and class NOT-1 
(which  contained  the  instances  for  all  the  other 
classes). We classified our data in this way, because 
about half of our instances were labeled as class 01. 
      Just  like  SVM,  we  used  cross  validation  with 
k=10  folds  to  validate  our  model.  Although  we  got 
accuracy  of  about  0.92  for  the  training  set,  the 
accuracy for test set was about 0.62.  
 
 

	 Â 1	 Â 

 

Classification of Cardiac Arrhythmias Patients 

Azar	 Â Fazel,	 Â Fatema	 Â Algharbi,	 Â Batool	 Â Haider	 Â 
Azar Fazel, Fatema Algharbi, Batool Haider 
CS229 Final Project Report, Fall 2014 

CS229	 Â Final	 Â Project	 Â Report,	 Â Fall	 Â 2014	 Â 

Abstract: 

Cardiac Arrhythmias are any of a group of conditions in which the electrical activity 
of the heart is irregular or is faster or slower than normal. It is the leading cause of death 
for  both  men  and  women  in  the  world  [1][2].  In  this  project,  we  aim  to  classify  heart 
arrhythmias  patients  among  16  different  classes  based  on  ECG  (Electrocardiography) 
data.  After  applying  rigorous  data  pre-processing  and  feature  selection  techniques,  we 
used 5 different machine learning algorithms; SVM, Logistic Regression, KNN, Random 
Forest and Decision Trees. Our best accuracy was 73%, obtained via SVM. We also used 
some of these methods to come up with the most important attributes that determined the 
class  of  arrhythmia.  This  work  can  be  of  immense  importance  to  researchers  who  are 
exploring  various  techniques  to  capture  the  key  pre-informers  of  a  potential  cardiac 
disease, well before it is too late. 

	 Â 

 

Introduction 
Heart  diseases  kill  more 
than  385,000  people 
annually.  In  the  United  States,  someone  has  a  heart 
attack every 34 seconds [4]. In this paper, we present 
our  methodology  and  the  outcomes  of  developing  a 
machine learning system that is capable of classifying 
a  patient 
into  16  different  cardiac  arrhythmic 
categories. This work has great potential to serve the 
medicine industry. With the advancement in medical 
technology, database will only continue to grow. The 
evolution of smart body chips capable of sending real 
time  patientsâ€™ 
information  are  rigorously  being 
researched  upon.  Algorithms  such  as  these  can  be 
have  ground  breaking  impact  regarding  helping 
researchers target the keys features that cause cardiac 
arrhythmia  and  assist  them  in  classifying  patients  in 
right categories, so as to be able to take measures in 
the right direction. 
 
Data 
    The  data  has  been  taken  from  a  well-maintained 
ECG 
database 
(https://archive.ics.uci.edu/ml/datasets/Arrhythmia). It 
contains  279  attributes 
related 
variables)  and  452  instances.  The  variable  â€˜Classâ€™  is 
our target variable.  Class 01 refers to 'normal' ECG, 
classes  02 
to  different  classes  of 
Arrhythmia  and  class  16  refers  to  the  rest  of 
unclassified classes. Figure (1) shows the distribution 
of different classes in our database. As can be seen, 
almost half of the instances are classified to class 1, 

(Electrocardiography) 

(ECG/  Patient 

to  15  refer 

 

3

 

 

4

 

 

0
5

 

5
2

 

3
1

 

5
1

 

5
1

 

4
4

  9
2

 

5
4
2

2
  2
5

 
s
e
c
n
a
t
s
n
I
 
f
o
 
r
e
b
m
u
N

while there are few instances in other classes. Thus, in 
the database, we do not have much evidence for some 
of the classes like class 02 or 03. 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
 
 
Analysis & Data Pre-processing 
    Following are the main steps took to process the 
data. 
 
1) Normalization 
    Since many algorithms require normalized dataset, 
we normalized all the feature values except â€œgenderâ€ 

using  Z-score  (ğ‘§=!!! Â ! )  normalization  and  scaled 

Fig. 1. Distribution of instances in various classes 

1  2  3  4  5  6  7  8  9  10  11  12  13 

them between 0 and 1. We then proceeded on towards 
analyzing the data. 

 Classes 

  

2) Feature Selection:  
    We first ran various algorithms on the data set with 
all  the  features.  The  performance  of  most  was  not 

	 Â 2	 Â 
satisfactory.  With  the  instances  to  feature  ratio  of 
only 1.6, we decided to improve the performance of 
algorithms by reducing the number of features.  
 
i) Invariant Features 
    Firstly,  we  removed  some  the  categorical  features 
that were 95% of time indicating either all 0â€™s or all 
1â€™s. These seemed not to help a lot in decision making 
as  they  were  pointing  to  the  same  category  most  of 
the time. As expected, their removal, did not disturb 
the  accuracy  but  reduced  the  feature  space  and 
simplified the data base.	 Â 
	 Â 
ii) Mutual information:  
    Next,  we  found  tuples  of  features  that  were 
correlated  at  least  with  0.95  correlations.  This  is  to 
say the set of features giving approximately the same 
information,  and  so  we  included  only  one  feature 
from each tuple. This reduced the number of features 
to 160. 
 
iii) Feature Importance Score via Decision trees:  
    We took two approaches: the first was to cluster the 
classes  into  â€œClass  1â€  and  â€œNot  Class  1â€.  We  then 
found  the  features  that  best  described  the  responses 
for  these  two  large  classes.  In  the  second  approach, 
we used all the classes in their original labeling in the 
dataset and found out the most important features for 
the  response  via  decision  tree.  Figure  2.1  shows  the 
results of decision tree. As seen in this figure, based 
on the features importance score, only 11 features out 
of 160 features were considered as the most important 
features. 

 
 
 
 
 
 
 
 
 
 
 
 
 

Fig. 2.1 Decision Tree for Cardiac Arrhythmias patients 

 
 
Figure 2.2 shows the most important 30 variables for 
the response using the decision trees. We noticed that 

1

4

7

8

9

5

6

2

3

10

14

15

16

V

5

0.5

I
 
e
l
b
a
i
r
a
V

 
e
c
n
a
t
r
o
p
m

40 
35 
30 
25 
20 
15 
10 
5 
0 

using decision trees 

Fig. 2.2 Variable Importance for Cardiac Arrhythmias patients 

the heart rate is the most important variable followed 
by V6_T_wave, V5_T_wave and V3_Q_wave. 
 
 
 
 
 
 
 
 
 
 
 
 
 
We  then  applied  Principle  Components  Analysis 
(PCA) to these features to reduce the dimensionality 
of the dataset while retaining most of the variation in 
the  data  set.  Figure  3.  shows  the  results  of  this 
analysis.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Models: 
    We  applied  several  models  and  algorithms  to  our 
dataset,  accounting  for  their  merits  and  demerits 
based on literature review. These are as follows: 
 
1) KNN (K-Nearest Neighbors) 
    We used KNN because it is simple to implement & 
very straight forward. Here, an object is classified by 
a majority vote of its neighbors, with the object being 
assigned 
the  class  most  common  among 
its k nearest  neighbors  [3].  This  could  be  done  by 
measuring  â€˜distancesâ€™  between  the  object  and  its 
neighbors.  The 
a 
representation of simple Euclidian distance, where â€˜aâ€™ 
and â€˜bâ€™ are the respective positions of the object and 
one of its neighbors. 

Fig. 3. Using PCA to visualize the features selected by Decision 

following 

V3_Q2_wave
V3_R2_wave

formula 

shows 

PC1 (23.6% explained var.)

_

w

T

_

V6_T_wave
DII_T_wave
aveDI_T_wave

0.0
Tree 

AVR_T_wave

_ w a
_ R N O T
T
O
N
R
_
n s
1
V
efle ctio
d

V3_Q_wave

V 1 _ Q R S A
A
e
v
_
2
_
2
V

w

S

Q

v

a

e

R

to 

)
.
r
a
v
 

i

l

d
e
n
a
p
x
e
%
4

 

.

8
1
(
 
2
C
P

e
v
a
w
_
Q
_
F
V
A

V 1
_intr _

âˆ’0.5

âˆ’0.5

0.0

H
e

a

r

t

_

r

a

t

e

0.5

1.0

1

V

	 Â 

	 Â 

	 Â 3	 Â 

 
 

D(a,b) =

n

âˆ‘

(bi âˆ’ ai )2

i

	 Â 
KNN  is  very  sensitive  to  irrelevant  or  redundant 
features  because  all  features  contribute 
the 
similarity  and  thus  to  the  classification.  This  was 
ameliorated  by  careful  feature  selection  described 
previously. 

to 

2) Decision Trees 
    We used Decision trees as they implicitly perform 
feature selection & can tackle nonlinear relationships 
between  parameters.  Each  leaf  of  the  tree  is  labeled 
with  a  class  or  probability  distribution  over  the 
classes. 
A tree can be "learned" by splitting the source set into 
subsets  based  on  attributes  and  the  recursion is 
completed when splitting no longer adds value to the 
predictions.  The  information  gained  is  based  on  the 
decrease in â€œentropyâ€ after dataset is split. Following 
equation shows the formula for entropy, where â€˜pâ€™ is 
the  probability  of  certain  class  occurring,  given  a 
specific feature 
p P
(

	 Â 
3) Random Forest  
    We  tried  Random  forests  as  they are  an ensemble 
learning method 
constructing 
multitude  of  decision 
their 
performance is often better than decision trees alone 
and can tackle issues like â€˜pruningâ€™ (often an issue in 
decision  tree)  automatically.    Since  random  forest 
works quite well with several features, we first tried it 
on  the  full  set  of  features.  Table  1  shows  its 
performance class wise. 
 

that  operate  by 

trees.  Therefore 

p P
)log2 (

)log2 (

E s
( )

p N
(

p N

=âˆ’

)
âˆ’

)

Class  N Cases  N Instances 
misclassified 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
14 
15 
16 

245 
44 
15 
15 
13 
25 
3 
2 
9 
50 
4 
5 
22 

192 
24 
1 
1 
6 
12 
3 
2 
1 
29 
1 
2 
21 

Pct. Error 
78.37% 
54.55% 
6.67% 
6.67% 
46.15% 
48.00% 
100.00% 
100.00% 
11.11% 
58.00% 
25.00% 
40.00% 
95.45% 

Table 1. Random Forest Performance on full feature set 

	 Â 

r
o
r
r
e

 

n
o

i
t

a
c

i
f
i

s
s
a
c
 

l

g
a
b
-
f

o
-
t

u
O

0.56

0.55

0.54

0.53

0.52

0.51

0.5

0.49

0

100

200

300

 
 

 
 
 
 
 
 
 
 

 
 

 
Since  the  error  obtained  was  quite  high  (approx. 
50%),  we  then  tried  it  on  the  reduced  features  set 
outlined previously. The error graph (figure 4) below 
shows that the performance did not improve much. 

 

400

600
Number of grown trees

500

700

800

900

1000

Figure 4. Random Forest Performance on reduced 

 
 
 
 
4) SVM (Support Vector Machines) 

feature set 

	 Â 

We used SVM with a cross validation of k = 10 
to allow us average the error, among the 10 accuracy 
we found, the highest that was reached is 0.8 and the 
mean was approximately 0.73.  
We  tried  both  the  polynomial  and  the  linear  kernels 
for  the  SVM  and  found  out  that  the  linear  kernel 
outperformed the polynomial kernel. The linear SVM 
with CV = 10 gave the best accuracy among all the 
other models we used. The features that were selected 
for the SVM where determined by using the decision 
trees as explained in the feature selection section.   
 
4) Logistic Regression  

Since  the  logistic  regression  is  used  for  binary 
classification  of  datasets  with  categorical  dependent 
features,  in  order  to  apply  logistic  regression  to  our 
multi-class dataset, we firstly classified our instances 
into  two  major  classes,  class  1  (which  contained  all 
the instances with â€œclass 01â€ label) and class NOT-1 
(which  contained  the  instances  for  all  the  other 
classes). We classified our data in this way, because 
about half of our instances were labeled as class 01. 
      Just  like  SVM,  we  used  cross  validation  with 
k=10  folds  to  validate  our  model.  Although  we  got 
accuracy  of  about  0.92  for  the  training  set,  the 
accuracy for test set was about 0.62.  
 
 

	 Â 4	 Â 
Results 

 

Table  2  summarizes  the  results  obtained  from 

each of the outlined methods. 

As can be seen SVM with linear kernel gave the best 
performance.    Trees  on  the  other  hand  did  not 
perform so well. One of the possible reasons may be 
the presence of 16 classes. Our discussion with some 
of  the  other  CS229  teams  with  similar  project 
revealed  that  decision  trees  performance  improved 
drastically  with  reduction  in  the  total  number  of 
classes. Other methods appear to be less sensitive to 
this. 
Additionally,  we  used  Cross  Validation  to  obtain 
error estimate on the test set. This helped us to be sure 
that the error was the mean of all the various test sets 
that  could  be  obtained  from  the  given  data  and  so 
results are less sensitive to the choice of test/training 
set.  We  used  10  k-folds  for  the  purpose.    Figure  5 
shows the comparison between errors that we got for 
different models. 
 

	 Â 	 Â 

	 Â 

validation of 10 folds  

Table 2. Summary of various modelsâ€™ performance using cross 

 
 
 
Figure  5  shows  the  accuracy  for  the  SVM,  KNN, 
Decision  Trees,  and  Logistic  Regression  using 
various folds. It is clear that the SVM outperformed 
all of the other models in our case. 
 
 
 
 
 
 
 
 
 
 
 
 

Figure 5. Comparison of the accuracy of various models 

using cross validation	 Â 

	 Â 
	 Â 
	 Â 
	 Â 

	 Â 
	 Â 
	 Â 

 
Future Work 
 Since about 50% of the data were clustered in class 
1, it is believed that with more data on the patients of 
the  other  classes,  it  is  possible  to  learn  more  to  get 
more accurate classification. Some of the classes had 
only  2  to  3  instances  in  the  data;  which  makes  it 
difficult  to  learn  about  these  classes  and  hence  their 
misclassificationâ€™s  probability  is  high  when  using 
various  algorithms.  It  is  clear  that  class  1  has  the 
dominant  effect  on 
the  predicting  models  so 
collecting  more  instances  of  patients  in  the  other 
classes is a goal for better predictions in future. 
 
Apart from this and based on our findings throughout 
this project, here is what we would like to propose for 
future work in this area- 
 

1.  It  will  be  interesting  to  group  the  features 
based on their physical similarity (like also the 
ECGâ€™s P wave variables together and all the Q 
wave  variables  together)  and  re-check  the 
performance of these algorithms.    

2.  Though  we  used  some  rigorous  feature 
selection  techniques,  one  more  method  that 
can  be  tried  is  â€œForwardâ€  or  â€œBackwardâ€ 
search  techniques,  where  the  features  are 
dropped or added to check their impact on the 
algorithmsâ€™ accuracy. A great thing to do here 
would be to combine all the feature selection 
techniques described in this paper and average 
out  the  score  assigned  to  each  feature  in  the 
data  set.  This  will  give  researchers  a  very 
good 
idea  of  which  features  are  most 
important  distinguisher  of  various  Cardiac 
Arrhythmias. These ranks could be discussed 
with  experts  in  the  field  of  Cardiology  to 
check  how  well  did 
the  data  driven 
assignments match expert opinion. 
 

3.  It will be worthwhile to reduce the number of 
classes  based  on  literature  review  (group 
similar  classes  together)  to  check  if  the 
performance  of  the  model  improves.  One 
could break this algorithm into two parts. First 
part can be used to give the user results based 
on reduced number of classes (say 5) and then 
on  all  16  classes.  This  way,  even  if  the 
accuracy  with  all  the  16  classes  is  not 
extremely high, at least the user will know in 

	 Â 1	 Â 

 

Classification of Cardiac Arrhythmias Patients 

Azar	 Â Fazel,	 Â Fatema	 Â Algharbi,	 Â Batool	 Â Haider	 Â 
Azar Fazel, Fatema Algharbi, Batool Haider 
CS229 Final Project Report, Fall 2014 

CS229	 Â Final	 Â Project	 Â Report,	 Â Fall	 Â 2014	 Â 

Abstract: 

Cardiac Arrhythmias are any of a group of conditions in which the electrical activity 
of the heart is irregular or is faster or slower than normal. It is the leading cause of death 
for  both  men  and  women  in  the  world  [1][2].  In  this  project,  we  aim  to  classify  heart 
arrhythmias  patients  among  16  different  classes  based  on  ECG  (Electrocardiography) 
data.  After  applying  rigorous  data  pre-processing  and  feature  selection  techniques,  we 
used 5 different machine learning algorithms; SVM, Logistic Regression, KNN, Random 
Forest and Decision Trees. Our best accuracy was 73%, obtained via SVM. We also used 
some of these methods to come up with the most important attributes that determined the 
class  of  arrhythmia.  This  work  can  be  of  immense  importance  to  researchers  who  are 
exploring  various  techniques  to  capture  the  key  pre-informers  of  a  potential  cardiac 
disease, well before it is too late. 

	 Â 

 

Introduction 
Heart  diseases  kill  more 
than  385,000  people 
annually.  In  the  United  States,  someone  has  a  heart 
attack every 34 seconds [4]. In this paper, we present 
our  methodology  and  the  outcomes  of  developing  a 
machine learning system that is capable of classifying 
a  patient 
into  16  different  cardiac  arrhythmic 
categories. This work has great potential to serve the 
medicine industry. With the advancement in medical 
technology, database will only continue to grow. The 
evolution of smart body chips capable of sending real 
time  patientsâ€™ 
information  are  rigorously  being 
researched  upon.  Algorithms  such  as  these  can  be 
have  ground  breaking  impact  regarding  helping 
researchers target the keys features that cause cardiac 
arrhythmia  and  assist  them  in  classifying  patients  in 
right categories, so as to be able to take measures in 
the right direction. 
 
Data 
    The  data  has  been  taken  from  a  well-maintained 
ECG 
database 
(https://archive.ics.uci.edu/ml/datasets/Arrhythmia). It 
contains  279  attributes 
related 
variables)  and  452  instances.  The  variable  â€˜Classâ€™  is 
our target variable.  Class 01 refers to 'normal' ECG, 
classes  02 
to  different  classes  of 
Arrhythmia  and  class  16  refers  to  the  rest  of 
unclassified classes. Figure (1) shows the distribution 
of different classes in our database. As can be seen, 
almost half of the instances are classified to class 1, 

(Electrocardiography) 

(ECG/  Patient 

to  15  refer 

 

3

 

 

4

 

 

0
5

 

5
2

 

3
1

 

5
1

 

5
1

 

4
4

  9
2

 

5
4
2

2
  2
5

 
s
e
c
n
a
t
s
n
I
 
f
o
 
r
e
b
m
u
N

while there are few instances in other classes. Thus, in 
the database, we do not have much evidence for some 
of the classes like class 02 or 03. 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
 
 
Analysis & Data Pre-processing 
    Following are the main steps took to process the 
data. 
 
1) Normalization 
    Since many algorithms require normalized dataset, 
we normalized all the feature values except â€œgenderâ€ 

using  Z-score  (ğ‘§=!!! Â ! )  normalization  and  scaled 

Fig. 1. Distribution of instances in various classes 

1  2  3  4  5  6  7  8  9  10  11  12  13 

them between 0 and 1. We then proceeded on towards 
analyzing the data. 

 Classes 

  

2) Feature Selection:  
    We first ran various algorithms on the data set with 
all  the  features.  The  performance  of  most  was  not 

	 Â 2	 Â 
satisfactory.  With  the  instances  to  feature  ratio  of 
only 1.6, we decided to improve the performance of 
algorithms by reducing the number of features.  
 
i) Invariant Features 
    Firstly,  we  removed  some  the  categorical  features 
that were 95% of time indicating either all 0â€™s or all 
1â€™s. These seemed not to help a lot in decision making 
as  they  were  pointing  to  the  same  category  most  of 
the time. As expected, their removal, did not disturb 
the  accuracy  but  reduced  the  feature  space  and 
simplified the data base.	 Â 
	 Â 
ii) Mutual information:  
    Next,  we  found  tuples  of  features  that  were 
correlated  at  least  with  0.95  correlations.  This  is  to 
say the set of features giving approximately the same 
information,  and  so  we  included  only  one  feature 
from each tuple. This reduced the number of features 
to 160. 
 
iii) Feature Importance Score via Decision trees:  
    We took two approaches: the first was to cluster the 
classes  into  â€œClass  1â€  and  â€œNot  Class  1â€.  We  then 
found  the  features  that  best  described  the  responses 
for  these  two  large  classes.  In  the  second  approach, 
we used all the classes in their original labeling in the 
dataset and found out the most important features for 
the  response  via  decision  tree.  Figure  2.1  shows  the 
results of decision tree. As seen in this figure, based 
on the features importance score, only 11 features out 
of 160 features were considered as the most important 
features. 

 
 
 
 
 
 
 
 
 
 
 
 
 

Fig. 2.1 Decision Tree for Cardiac Arrhythmias patients 

 
 
Figure 2.2 shows the most important 30 variables for 
the response using the decision trees. We noticed that 

1

4

7

8

9

5

6

2

3

10

14

15

16

V

5

0.5

I
 
e
l
b
a
i
r
a
V

 
e
c
n
a
t
r
o
p
m

40 
35 
30 
25 
20 
15 
10 
5 
0 

using decision trees 

Fig. 2.2 Variable Importance for Cardiac Arrhythmias patients 

the heart rate is the most important variable followed 
by V6_T_wave, V5_T_wave and V3_Q_wave. 
 
 
 
 
 
 
 
 
 
 
 
 
 
We  then  applied  Principle  Components  Analysis 
(PCA) to these features to reduce the dimensionality 
of the dataset while retaining most of the variation in 
the  data  set.  Figure  3.  shows  the  results  of  this 
analysis.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Models: 
    We  applied  several  models  and  algorithms  to  our 
dataset,  accounting  for  their  merits  and  demerits 
based on literature review. These are as follows: 
 
1) KNN (K-Nearest Neighbors) 
    We used KNN because it is simple to implement & 
very straight forward. Here, an object is classified by 
a majority vote of its neighbors, with the object being 
assigned 
the  class  most  common  among 
its k nearest  neighbors  [3].  This  could  be  done  by 
measuring  â€˜distancesâ€™  between  the  object  and  its 
neighbors.  The 
a 
representation of simple Euclidian distance, where â€˜aâ€™ 
and â€˜bâ€™ are the respective positions of the object and 
one of its neighbors. 

Fig. 3. Using PCA to visualize the features selected by Decision 

following 

V3_Q2_wave
V3_R2_wave

formula 

shows 

PC1 (23.6% explained var.)

_

w

T

_

V6_T_wave
DII_T_wave
aveDI_T_wave

0.0
Tree 

AVR_T_wave

_ w a
_ R N O T
T
O
N
R
_
n s
1
V
efle ctio
d

V3_Q_wave

V 1 _ Q R S A
A
e
v
_
2
_
2
V

w

S

Q

v

a

e

R

to 

)
.
r
a
v
 

i

l

d
e
n
a
p
x
e
%
4

 

.

8
1
(
 
2
C
P

e
v
a
w
_
Q
_
F
V
A

V 1
_intr _

âˆ’0.5

âˆ’0.5

0.0

H
e

a

r

t

_

r

a

t

e

0.5

1.0

1

V

	 Â 

	 Â 

	 Â 3	 Â 

 
 

D(a,b) =

n

âˆ‘

(bi âˆ’ ai )2

i

	 Â 
KNN  is  very  sensitive  to  irrelevant  or  redundant 
features  because  all  features  contribute 
the 
similarity  and  thus  to  the  classification.  This  was 
ameliorated  by  careful  feature  selection  described 
previously. 

to 

2) Decision Trees 
    We used Decision trees as they implicitly perform 
feature selection & can tackle nonlinear relationships 
between  parameters.  Each  leaf  of  the  tree  is  labeled 
with  a  class  or  probability  distribution  over  the 
classes. 
A tree can be "learned" by splitting the source set into 
subsets  based  on  attributes  and  the  recursion is 
completed when splitting no longer adds value to the 
predictions.  The  information  gained  is  based  on  the 
decrease in â€œentropyâ€ after dataset is split. Following 
equation shows the formula for entropy, where â€˜pâ€™ is 
the  probability  of  certain  class  occurring,  given  a 
specific feature 
p P
(

	 Â 
3) Random Forest  
    We  tried  Random  forests  as  they are  an ensemble 
learning method 
constructing 
multitude  of  decision 
their 
performance is often better than decision trees alone 
and can tackle issues like â€˜pruningâ€™ (often an issue in 
decision  tree)  automatically.    Since  random  forest 
works quite well with several features, we first tried it 
on  the  full  set  of  features.  Table  1  shows  its 
performance class wise. 
 

that  operate  by 

trees.  Therefore 

p P
)log2 (

)log2 (

E s
( )

p N
(

p N

=âˆ’

)
âˆ’

)

Class  N Cases  N Instances 
misclassified 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
14 
15 
16 

245 
44 
15 
15 
13 
25 
3 
2 
9 
50 
4 
5 
22 

192 
24 
1 
1 
6 
12 
3 
2 
1 
29 
1 
2 
21 

Pct. Error 
78.37% 
54.55% 
6.67% 
6.67% 
46.15% 
48.00% 
100.00% 
100.00% 
11.11% 
58.00% 
25.00% 
40.00% 
95.45% 

Table 1. Random Forest Performance on full feature set 

	 Â 

r
o
r
r
e

 

n
o

i
t

a
c

i
f
i

s
s
a
c
 

l

g
a
b
-
f

o
-
t

u
O

0.56

0.55

0.54

0.53

0.52

0.51

0.5

0.49

0

100

200

300

 
 

 
 
 
 
 
 
 
 

 
 

 
Since  the  error  obtained  was  quite  high  (approx. 
50%),  we  then  tried  it  on  the  reduced  features  set 
outlined previously. The error graph (figure 4) below 
shows that the performance did not improve much. 

 

400

600
Number of grown trees

500

700

800

900

1000

Figure 4. Random Forest Performance on reduced 

 
 
 
 
4) SVM (Support Vector Machines) 

feature set 

	 Â 

We used SVM with a cross validation of k = 10 
to allow us average the error, among the 10 accuracy 
we found, the highest that was reached is 0.8 and the 
mean was approximately 0.73.  
We  tried  both  the  polynomial  and  the  linear  kernels 
for  the  SVM  and  found  out  that  the  linear  kernel 
outperformed the polynomial kernel. The linear SVM 
with CV = 10 gave the best accuracy among all the 
other models we used. The features that were selected 
for the SVM where determined by using the decision 
trees as explained in the feature selection section.   
 
4) Logistic Regression  

Since  the  logistic  regression  is  used  for  binary 
classification  of  datasets  with  categorical  dependent 
features,  in  order  to  apply  logistic  regression  to  our 
multi-class dataset, we firstly classified our instances 
into  two  major  classes,  class  1  (which  contained  all 
the instances with â€œclass 01â€ label) and class NOT-1 
(which  contained  the  instances  for  all  the  other 
classes). We classified our data in this way, because 
about half of our instances were labeled as class 01. 
      Just  like  SVM,  we  used  cross  validation  with 
k=10  folds  to  validate  our  model.  Although  we  got 
accuracy  of  about  0.92  for  the  training  set,  the 
accuracy for test set was about 0.62.  
 
 

	 Â 4	 Â 
Results 

 

Table  2  summarizes  the  results  obtained  from 

each of the outlined methods. 

As can be seen SVM with linear kernel gave the best 
performance.    Trees  on  the  other  hand  did  not 
perform so well. One of the possible reasons may be 
the presence of 16 classes. Our discussion with some 
of  the  other  CS229  teams  with  similar  project 
revealed  that  decision  trees  performance  improved 
drastically  with  reduction  in  the  total  number  of 
classes. Other methods appear to be less sensitive to 
this. 
Additionally,  we  used  Cross  Validation  to  obtain 
error estimate on the test set. This helped us to be sure 
that the error was the mean of all the various test sets 
that  could  be  obtained  from  the  given  data  and  so 
results are less sensitive to the choice of test/training 
set.  We  used  10  k-folds  for  the  purpose.    Figure  5 
shows the comparison between errors that we got for 
different models. 
 

	 Â 	 Â 

	 Â 

validation of 10 folds  

Table 2. Summary of various modelsâ€™ performance using cross 

 
 
 
Figure  5  shows  the  accuracy  for  the  SVM,  KNN, 
Decision  Trees,  and  Logistic  Regression  using 
various folds. It is clear that the SVM outperformed 
all of the other models in our case. 
 
 
 
 
 
 
 
 
 
 
 
 

Figure 5. Comparison of the accuracy of various models 

using cross validation	 Â 

	 Â 
	 Â 
	 Â 
	 Â 

	 Â 
	 Â 
	 Â 

 
Future Work 
 Since about 50% of the data were clustered in class 
1, it is believed that with more data on the patients of 
the  other  classes,  it  is  possible  to  learn  more  to  get 
more accurate classification. Some of the classes had 
only  2  to  3  instances  in  the  data;  which  makes  it 
difficult  to  learn  about  these  classes  and  hence  their 
misclassificationâ€™s  probability  is  high  when  using 
various  algorithms.  It  is  clear  that  class  1  has  the 
dominant  effect  on 
the  predicting  models  so 
collecting  more  instances  of  patients  in  the  other 
classes is a goal for better predictions in future. 
 
Apart from this and based on our findings throughout 
this project, here is what we would like to propose for 
future work in this area- 
 

1.  It  will  be  interesting  to  group  the  features 
based on their physical similarity (like also the 
ECGâ€™s P wave variables together and all the Q 
wave  variables  together)  and  re-check  the 
performance of these algorithms.    

2.  Though  we  used  some  rigorous  feature 
selection  techniques,  one  more  method  that 
can  be  tried  is  â€œForwardâ€  or  â€œBackwardâ€ 
search  techniques,  where  the  features  are 
dropped or added to check their impact on the 
algorithmsâ€™ accuracy. A great thing to do here 
would be to combine all the feature selection 
techniques described in this paper and average 
out  the  score  assigned  to  each  feature  in  the 
data  set.  This  will  give  researchers  a  very 
good 
idea  of  which  features  are  most 
important  distinguisher  of  various  Cardiac 
Arrhythmias. These ranks could be discussed 
with  experts  in  the  field  of  Cardiology  to 
check  how  well  did 
the  data  driven 
assignments match expert opinion. 
 

3.  It will be worthwhile to reduce the number of 
classes  based  on  literature  review  (group 
similar  classes  together)  to  check  if  the 
performance  of  the  model  improves.  One 
could break this algorithm into two parts. First 
part can be used to give the user results based 
on reduced number of classes (say 5) and then 
on  all  16  classes.  This  way,  even  if  the 
accuracy  with  all  the  16  classes  is  not 
extremely high, at least the user will know in 

 
 
References 
 
[1].http://en.wikipedia.org/wiki/Cardiac_dysrhythmia 
[2].http://www.cdc.gov/dhdsp/data_statistics/fact_sheets/d
ocs/fs_heart_disease.pdf 
[3].Cunningham  2007.  k-Nearest  Neighbor  Classifiers. 
Technical  Report  UCD-CSI-2007-4.  University  College 
Dublin 
[4]  Roger  VL  et  al.  Heart  disease  and  stroke  statisticsâ€”
2012  update:  a 
the  American  Heart 
Association http://www.cdc.gov/Other/disclaimer.html 
 
	 Â 

report 

from 

	 Â 5	 Â 

general which of the 5 broad categories he/she 
fall in.	 Â   

Conclusions 
     Using linear SVM, we built a predicting model to 
classify  the  Cardiac  Arrhythmia  Patients.  The  most 
important  features  were  selected  via  decision  trees. 
We  were  successful  in  reducing  the  variables  from 
279  to  15  variables  and  using  cross  validation  our 
models  accuracy  is  approximately  73%.  The  data  is 
skewed as about 50% of the patients are classified as 
class 1; hence, for prediction accuracy improvement it 
is  essential  to  gather  more  data  on  patients  in  other 
classes. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

