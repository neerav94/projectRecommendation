1  Abstract	 Â 

2 

Introduction	 Â 

Predict	 Â seizures	 Â in	 Â intracranial	 Â EEG	 Â recordings	 Â 
Linyu	 Â He	 Â (linyu90@stanford.edu)	 Â and	 Â Lingbin	 Â Li	 Â (lingbin@stanford.edu)	 Â 

	 Â 
This	 Â project	 Â aims	 Â to	 Â predict	 Â seizures	 Â in	 Â intracranial	 Â electroencephalography	 Â (iEEG)	 Â recordings	 Â using	 Â four	 Â algorithms.	 Â The	 Â data	 Â are	 Â a	 Â series	 Â of	 Â 10-Â­â€
minute	 Â iEEG	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Our	 Â 
goal	 Â is	 Â to	 Â distinguish	 Â between	 Â the	 Â two	 Â states.	 Â The	 Â major	 Â challenge	 Â is	 Â that	 Â the	 Â data	 Â are	 Â highly	 Â imbalanced,	 Â i.e.,	 Â the	 Â number	 Â of	 Â positive	 Â examples	 Â is	 Â less	 Â 
than	 Â 10%	 Â of	 Â that	 Â of	 Â negative	 Â examples.	 Â Our	 Â work	 Â is	 Â to	 Â make	 Â modifications	 Â to	 Â each	 Â of	 Â the	 Â four	 Â models	 Â and	 Â analyze	 Â the	 Â corresponding	 Â performance	 Â 
gain.	 Â 	 Â 
Spontaneous	 Â seizures	 Â are	 Â the	 Â typical	 Â symptom	 Â of	 Â epilepsy,	 Â which	 Â is	 Â a	 Â common	 Â but	 Â refractory	 Â neurological	 Â disorder	 Â that	 Â afflicts	 Â nearly	 Â 1%	 Â of	 Â the	 Â 
worldâ€™s	 Â population.	 Â Anticonvulsant	 Â medications	 Â are	 Â administered	 Â to	 Â many	 Â patients	 Â at	 Â high	 Â doses	 Â to	 Â prevent	 Â seizures,	 Â but	 Â their	 Â effectiveness	 Â is	 Â 
limited	 Â and	 Â patients	 Â often	 Â suffer	 Â their	 Â side	 Â effects.	 Â Even	 Â for	 Â patients	 Â whose	 Â epilepsy-Â­â€causing	 Â brain	 Â tissue	 Â is	 Â removed	 Â via	 Â surgery,	 Â spontaneous	 Â 
seizures	 Â still	 Â persist.	 Â Due	 Â to	 Â the	 Â seemingly	 Â unpredictable	 Â occurrence	 Â of	 Â seizures,	 Â patients	 Â with	 Â epilepsy	 Â experience	 Â constant	 Â anxiety	 Â [1].	 Â 
This	 Â project	 Â aims	 Â to	 Â make	 Â it	 Â possible	 Â that	 Â devices	 Â designed	 Â to	 Â monitor	 Â patientsâ€™	 Â brain	 Â activity	 Â can	 Â warn	 Â them	 Â of	 Â impeding	 Â seizures	 Â so	 Â that	 Â patients	 Â 
are	 Â able	 Â to	 Â take	 Â appropriate	 Â precautions.	 Â It	 Â is	 Â also	 Â helpful	 Â to	 Â reduce	 Â overall	 Â side	 Â effects	 Â caused	 Â by	 Â anticonvulsant	 Â medications	 Â taken	 Â by	 Â these	 Â 
patients.	 Â By	 Â providing	 Â them	 Â with	 Â devices	 Â with	 Â the	 Â ability	 Â to	 Â predict	 Â an	 Â impending	 Â seizure,	 Â anticonvulsant	 Â medications	 Â could	 Â be	 Â administered	 Â only	 Â 
when	 Â necessary,	 Â thus	 Â lowering	 Â the	 Â doses	 Â given	 Â to	 Â patients.	 Â 
Multiple	 Â researches	 Â support	 Â the	 Â notion	 Â that	 Â the	 Â occurrence	 Â of	 Â seizures	 Â is	 Â not	 Â random.	 Â According	 Â to	 Â evidence	 Â shown	 Â by	 Â related	 Â researches,	 Â for	 Â 
patients	 Â with	 Â epilepsy,	 Â the	 Â temporal	 Â dynamics	 Â of	 Â brain	 Â activity	 Â can	 Â be	 Â classified	 Â into	 Â 4	 Â states:	 Â interictal	 Â (between	 Â seizures),	 Â preictal	 Â (prior	 Â to	 Â 
seizures),	 Â ictal	 Â (seizure)	 Â and	 Â postictal	 Â (after	 Â seizures).	 Â The	 Â brain	 Â activity	 Â of	 Â each	 Â state	 Â can	 Â be	 Â recorded	 Â by	 Â iEEG	 Â [1].	 Â Our	 Â goal	 Â is	 Â to	 Â employ	 Â machine	 Â 
learning	 Â techniques	 Â to	 Â learn	 Â from	 Â iEEG	 Â data	 Â the	 Â characteristics	 Â of	 Â preictal	 Â states	 Â and	 Â then	 Â distinguish	 Â these	 Â states	 Â from	 Â the	 Â interictal	 Â states.	 Â After	 Â 
one	 Â preictal	 Â state	 Â is	 Â identified,	 Â a	 Â warning	 Â should	 Â be	 Â sent	 Â to	 Â the	 Â patient	 Â to	 Â prepare	 Â him	 Â or	 Â her	 Â for	 Â an	 Â impending	 Â seizure.	 Â 
Kaggle	 Â provides	 Â iEEG	 Â data	 Â collected	 Â from	 Â canine	 Â subjects.	 Â The	 Â data	 Â of	 Â each	 Â subject	 Â is	 Â organized	 Â into	 Â 10-Â­â€minute	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â 
data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Each	 Â clip	 Â contains	 Â 16	 Â channels	 Â of	 Â iEEG	 Â data	 Â where	 Â 
each	 Â channel	 Â corresponds	 Â to	 Â one	 Â electrode	 Â implanted	 Â in	 Â the	 Â subjectâ€™s	 Â brain.	 Â For	 Â each	 Â training	 Â example	 Â ğ‘¥(!),ğ‘¦(!),	 Â ğ‘¦(!)	 Â is	 Â the	 Â label	 Â and	 Â ğ‘¥(!)	 Â is	 Â a	 Â clip	 Â in	 Â 
which	 Â each	 Â row	 Â corresponds	 Â to	 Â one	 Â channel	 Â and	 Â each	 Â column	 Â corresponds	 Â to	 Â iEEG	 Â readings	 Â at	 Â one	 Â sampling	 Â time	 Â point.	 Â 
Since	 Â seizures	 Â in	 Â most	 Â patients	 Â are	 Â associated	 Â with	 Â a	 Â stereotypic	 Â EEG	 Â discharge	 Â with	 Â characteristic	 Â spectral	 Â pattern,	 Â we	 Â employed	 Â the	 Â following	 Â 
feature	 Â extraction	 Â procedure	 Â [2]:	 Â 
Apply	 Â fast	 Â Fourier	 Â transform	 Â to	 Â each	 Â channel	 Â in	 Â a	 Â clip	 Â and	 Â divide	 Â the	 Â resulting	 Â power	 Â spectrum	 Â into	 Â 6	 Â bands:	 Â delta	 Â (0.1	 Â â€“	 Â 4	 Â Hz),	 Â theta	 Â (4	 Â â€“	 Â 8	 Â Hz),	 Â 
alpha	 Â (8	 Â â€“	 Â 12	 Â Hz),	 Â beta	 Â (12	 Â â€“	 Â 30	 Â Hz),	 Â low-Â­â€gamma	 Â (30	 Â â€“	 Â 70	 Â Hz),	 Â and	 Â high-Â­â€gamma	 Â (70	 Â â€“	 Â 180	 Â Hz).	 Â In	 Â each	 Â band,	 Â sum	 Â the	 Â power	 Â over	 Â all	 Â band	 Â 
frequencies	 Â to	 Â produce	 Â a	 Â power-Â­â€in-Â­â€band	 Â (PIB)	 Â feature.	 Â Therefore,	 Â 6	 Â features	 Â are	 Â obtained	 Â in	 Â each	 Â channel	 Â and	 Â 96	 Â features	 Â are	 Â obtained	 Â in	 Â one	 Â clip.	 Â 
The	 Â procedure	 Â above	 Â is	 Â also	 Â illustrated	 Â in	 Â Fig	 Â 1,	 Â where	 Â ğ‘(ğ‘“)	 Â is	 Â the	 Â power	 Â spectrum.	 Â 
Channel	 Â 1	 Â 
Channel	 Â 2	 Â 
â€¦	 Â â€¦	 Â â€¦	 Â Channel	 Â 16	 Â 
There	 Â are	 Â 3939	 Â examples	 Â in	 Â total,	 Â in	 Â which	 Â 3674	 Â are	 Â negative	 Â and	 Â 265	 Â are	 Â positive.	 Â For	 Â each	 Â time	 Â of	 Â cross	 Â validation,	 Â we	 Â randomly	 Â pick	 Â 70%	 Â of	 Â the	 Â 
negative	 Â examples	 Â and	 Â 70%	 Â of	 Â the	 Â positive	 Â examples	 Â for	 Â training	 Â and	 Â use	 Â the	 Â rest	 Â for	 Â testing.	 Â This	 Â process	 Â is	 Â repeated	 Â for	 Â 100	 Â times	 Â to	 Â calculate	 Â the	 Â 
average	 Â evaluation.	 Â 
	 Â 
1	 Â 

delta	 Â (0.1	 Â â€“	 Â 4Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!.!
theta	 Â (4	 Â â€“	 Â 8Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!
alpha	 Â (8	 Â â€“	 Â 12Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!
beta	 Â (12	 Â â€“	 Â 30Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!"
low-Â­â€gamma	 Â (30	 Â â€“	 Â 70Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"!!!"
high-Â­â€gamma	 Â (70	 Â â€“	 Â 180Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"#!!!"

Fig.	 Â 1	 Â Feature	 Â Extraction	 Â using	 Â FFT	 Â 

3  Data	 Â and	 Â Feature	 Â Extraction	 Â 

FFT	 Â 

4  Cross	 Â Validation	 Â 

1  Abstract	 Â 

2 

Introduction	 Â 

Predict	 Â seizures	 Â in	 Â intracranial	 Â EEG	 Â recordings	 Â 
Linyu	 Â He	 Â (linyu90@stanford.edu)	 Â and	 Â Lingbin	 Â Li	 Â (lingbin@stanford.edu)	 Â 

	 Â 
This	 Â project	 Â aims	 Â to	 Â predict	 Â seizures	 Â in	 Â intracranial	 Â electroencephalography	 Â (iEEG)	 Â recordings	 Â using	 Â four	 Â algorithms.	 Â The	 Â data	 Â are	 Â a	 Â series	 Â of	 Â 10-Â­â€
minute	 Â iEEG	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Our	 Â 
goal	 Â is	 Â to	 Â distinguish	 Â between	 Â the	 Â two	 Â states.	 Â The	 Â major	 Â challenge	 Â is	 Â that	 Â the	 Â data	 Â are	 Â highly	 Â imbalanced,	 Â i.e.,	 Â the	 Â number	 Â of	 Â positive	 Â examples	 Â is	 Â less	 Â 
than	 Â 10%	 Â of	 Â that	 Â of	 Â negative	 Â examples.	 Â Our	 Â work	 Â is	 Â to	 Â make	 Â modifications	 Â to	 Â each	 Â of	 Â the	 Â four	 Â models	 Â and	 Â analyze	 Â the	 Â corresponding	 Â performance	 Â 
gain.	 Â 	 Â 
Spontaneous	 Â seizures	 Â are	 Â the	 Â typical	 Â symptom	 Â of	 Â epilepsy,	 Â which	 Â is	 Â a	 Â common	 Â but	 Â refractory	 Â neurological	 Â disorder	 Â that	 Â afflicts	 Â nearly	 Â 1%	 Â of	 Â the	 Â 
worldâ€™s	 Â population.	 Â Anticonvulsant	 Â medications	 Â are	 Â administered	 Â to	 Â many	 Â patients	 Â at	 Â high	 Â doses	 Â to	 Â prevent	 Â seizures,	 Â but	 Â their	 Â effectiveness	 Â is	 Â 
limited	 Â and	 Â patients	 Â often	 Â suffer	 Â their	 Â side	 Â effects.	 Â Even	 Â for	 Â patients	 Â whose	 Â epilepsy-Â­â€causing	 Â brain	 Â tissue	 Â is	 Â removed	 Â via	 Â surgery,	 Â spontaneous	 Â 
seizures	 Â still	 Â persist.	 Â Due	 Â to	 Â the	 Â seemingly	 Â unpredictable	 Â occurrence	 Â of	 Â seizures,	 Â patients	 Â with	 Â epilepsy	 Â experience	 Â constant	 Â anxiety	 Â [1].	 Â 
This	 Â project	 Â aims	 Â to	 Â make	 Â it	 Â possible	 Â that	 Â devices	 Â designed	 Â to	 Â monitor	 Â patientsâ€™	 Â brain	 Â activity	 Â can	 Â warn	 Â them	 Â of	 Â impeding	 Â seizures	 Â so	 Â that	 Â patients	 Â 
are	 Â able	 Â to	 Â take	 Â appropriate	 Â precautions.	 Â It	 Â is	 Â also	 Â helpful	 Â to	 Â reduce	 Â overall	 Â side	 Â effects	 Â caused	 Â by	 Â anticonvulsant	 Â medications	 Â taken	 Â by	 Â these	 Â 
patients.	 Â By	 Â providing	 Â them	 Â with	 Â devices	 Â with	 Â the	 Â ability	 Â to	 Â predict	 Â an	 Â impending	 Â seizure,	 Â anticonvulsant	 Â medications	 Â could	 Â be	 Â administered	 Â only	 Â 
when	 Â necessary,	 Â thus	 Â lowering	 Â the	 Â doses	 Â given	 Â to	 Â patients.	 Â 
Multiple	 Â researches	 Â support	 Â the	 Â notion	 Â that	 Â the	 Â occurrence	 Â of	 Â seizures	 Â is	 Â not	 Â random.	 Â According	 Â to	 Â evidence	 Â shown	 Â by	 Â related	 Â researches,	 Â for	 Â 
patients	 Â with	 Â epilepsy,	 Â the	 Â temporal	 Â dynamics	 Â of	 Â brain	 Â activity	 Â can	 Â be	 Â classified	 Â into	 Â 4	 Â states:	 Â interictal	 Â (between	 Â seizures),	 Â preictal	 Â (prior	 Â to	 Â 
seizures),	 Â ictal	 Â (seizure)	 Â and	 Â postictal	 Â (after	 Â seizures).	 Â The	 Â brain	 Â activity	 Â of	 Â each	 Â state	 Â can	 Â be	 Â recorded	 Â by	 Â iEEG	 Â [1].	 Â Our	 Â goal	 Â is	 Â to	 Â employ	 Â machine	 Â 
learning	 Â techniques	 Â to	 Â learn	 Â from	 Â iEEG	 Â data	 Â the	 Â characteristics	 Â of	 Â preictal	 Â states	 Â and	 Â then	 Â distinguish	 Â these	 Â states	 Â from	 Â the	 Â interictal	 Â states.	 Â After	 Â 
one	 Â preictal	 Â state	 Â is	 Â identified,	 Â a	 Â warning	 Â should	 Â be	 Â sent	 Â to	 Â the	 Â patient	 Â to	 Â prepare	 Â him	 Â or	 Â her	 Â for	 Â an	 Â impending	 Â seizure.	 Â 
Kaggle	 Â provides	 Â iEEG	 Â data	 Â collected	 Â from	 Â canine	 Â subjects.	 Â The	 Â data	 Â of	 Â each	 Â subject	 Â is	 Â organized	 Â into	 Â 10-Â­â€minute	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â 
data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Each	 Â clip	 Â contains	 Â 16	 Â channels	 Â of	 Â iEEG	 Â data	 Â where	 Â 
each	 Â channel	 Â corresponds	 Â to	 Â one	 Â electrode	 Â implanted	 Â in	 Â the	 Â subjectâ€™s	 Â brain.	 Â For	 Â each	 Â training	 Â example	 Â ğ‘¥(!),ğ‘¦(!),	 Â ğ‘¦(!)	 Â is	 Â the	 Â label	 Â and	 Â ğ‘¥(!)	 Â is	 Â a	 Â clip	 Â in	 Â 
which	 Â each	 Â row	 Â corresponds	 Â to	 Â one	 Â channel	 Â and	 Â each	 Â column	 Â corresponds	 Â to	 Â iEEG	 Â readings	 Â at	 Â one	 Â sampling	 Â time	 Â point.	 Â 
Since	 Â seizures	 Â in	 Â most	 Â patients	 Â are	 Â associated	 Â with	 Â a	 Â stereotypic	 Â EEG	 Â discharge	 Â with	 Â characteristic	 Â spectral	 Â pattern,	 Â we	 Â employed	 Â the	 Â following	 Â 
feature	 Â extraction	 Â procedure	 Â [2]:	 Â 
Apply	 Â fast	 Â Fourier	 Â transform	 Â to	 Â each	 Â channel	 Â in	 Â a	 Â clip	 Â and	 Â divide	 Â the	 Â resulting	 Â power	 Â spectrum	 Â into	 Â 6	 Â bands:	 Â delta	 Â (0.1	 Â â€“	 Â 4	 Â Hz),	 Â theta	 Â (4	 Â â€“	 Â 8	 Â Hz),	 Â 
alpha	 Â (8	 Â â€“	 Â 12	 Â Hz),	 Â beta	 Â (12	 Â â€“	 Â 30	 Â Hz),	 Â low-Â­â€gamma	 Â (30	 Â â€“	 Â 70	 Â Hz),	 Â and	 Â high-Â­â€gamma	 Â (70	 Â â€“	 Â 180	 Â Hz).	 Â In	 Â each	 Â band,	 Â sum	 Â the	 Â power	 Â over	 Â all	 Â band	 Â 
frequencies	 Â to	 Â produce	 Â a	 Â power-Â­â€in-Â­â€band	 Â (PIB)	 Â feature.	 Â Therefore,	 Â 6	 Â features	 Â are	 Â obtained	 Â in	 Â each	 Â channel	 Â and	 Â 96	 Â features	 Â are	 Â obtained	 Â in	 Â one	 Â clip.	 Â 
The	 Â procedure	 Â above	 Â is	 Â also	 Â illustrated	 Â in	 Â Fig	 Â 1,	 Â where	 Â ğ‘(ğ‘“)	 Â is	 Â the	 Â power	 Â spectrum.	 Â 
Channel	 Â 1	 Â 
Channel	 Â 2	 Â 
â€¦	 Â â€¦	 Â â€¦	 Â Channel	 Â 16	 Â 
There	 Â are	 Â 3939	 Â examples	 Â in	 Â total,	 Â in	 Â which	 Â 3674	 Â are	 Â negative	 Â and	 Â 265	 Â are	 Â positive.	 Â For	 Â each	 Â time	 Â of	 Â cross	 Â validation,	 Â we	 Â randomly	 Â pick	 Â 70%	 Â of	 Â the	 Â 
negative	 Â examples	 Â and	 Â 70%	 Â of	 Â the	 Â positive	 Â examples	 Â for	 Â training	 Â and	 Â use	 Â the	 Â rest	 Â for	 Â testing.	 Â This	 Â process	 Â is	 Â repeated	 Â for	 Â 100	 Â times	 Â to	 Â calculate	 Â the	 Â 
average	 Â evaluation.	 Â 
	 Â 
1	 Â 

delta	 Â (0.1	 Â â€“	 Â 4Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!.!
theta	 Â (4	 Â â€“	 Â 8Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!
alpha	 Â (8	 Â â€“	 Â 12Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!
beta	 Â (12	 Â â€“	 Â 30Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!"
low-Â­â€gamma	 Â (30	 Â â€“	 Â 70Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"!!!"
high-Â­â€gamma	 Â (70	 Â â€“	 Â 180Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"#!!!"

Fig.	 Â 1	 Â Feature	 Â Extraction	 Â using	 Â FFT	 Â 

3  Data	 Â and	 Â Feature	 Â Extraction	 Â 

FFT	 Â 

4  Cross	 Â Validation	 Â 

Name	 Â 

Definition	 Â 

5 

Besides	 Â the	 Â training	 Â error	 Â and	 Â the	 Â testing	 Â error,	 Â the	 Â following	 Â values	 Â are	 Â adopted	 Â to	 Â evaluate	 Â the	 Â performance	 Â of	 Â each	 Â model	 Â since	 Â the	 Â data	 Â are	 Â 
highly	 Â imbalanced.	 Â 
	 Â 

Table	 Â 1.	 Â Values	 Â chosen	 Â to	 Â evaluate	 Â the	 Â performance	 Â 

Learning	 Â Algorithms	 Â 

Accuracy	 Â (ACC)	 Â 
Positive	 Â Predictive	 Â Rate(PPV)/Precision	 Â 
True	 Â Positive	 Â Rate	 Â (TPR)/Recall	 Â 
False	 Â Negative	 Â Rate	 Â (FNR)/Miss	 Â Rate	 Â 
False	 Â Positive	 Â Rate	 Â (FPR)/Fall-Â­â€out	 Â 

ğ‘‡ğ‘ƒ+ğ‘‡ğ‘
ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘	 Â 
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ	 Â 
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘	 Â 
ğ¹ğ‘ğ‘‡ğ‘ƒ+ğ¹ğ‘	 Â 
ğ¹ğ‘ƒ
ğ¹ğ‘ƒ+ğ‘‡ğ‘	 Â 
	 Â In	 Â Table	 Â 1,	 Â ğ‘‡ğ‘ƒ,	 Â ğ‘‡ğ‘,	 Â ğ¹ğ‘ƒ	 Â and	 Â ğ¹ğ‘	 Â are	 Â the	 Â number	 Â of	 Â true	 Â positives,	 Â true	 Â negatives,	 Â false	 Â positives	 Â and	 Â false	 Â negatives,	 Â respectively.	 Â Finally,	 Â the	 Â receiver	 Â 
operating	 Â characteristic	 Â (ROC)	 Â curves	 Â and	 Â precision-Â­â€recall	 Â curves	 Â will	 Â be	 Â plotted	 Â based	 Â on	 Â values	 Â in	 Â the	 Â table	 Â above.	 Â 
In	 Â out	 Â attempt	 Â to	 Â seek	 Â a	 Â solution,	 Â three	 Â models	 Â covered	 Â in	 Â CS	 Â 229	 Â were	 Â first	 Â adopted,	 Â which	 Â are	 Â logistic	 Â regression,	 Â naÃ¯ve	 Â Bayes	 Â classifiers	 Â and	 Â 
support	 Â vector	 Â machines	 Â (SVMs).	 Â Later,	 Â we	 Â employed	 Â a	 Â model	 Â inherited	 Â from	 Â communication	 Â systems,	 Â which	 Â makes	 Â a	 Â prediction	 Â based	 Â on	 Â 
correlation	 Â coefficients	 Â between	 Â the	 Â test	 Â example	 Â and	 Â all	 Â training	 Â examples.	 Â Modifications	 Â are	 Â made	 Â to	 Â each	 Â model	 Â to	 Â improve	 Â their	 Â performance	 Â on	 Â 
an	 Â imbalanced	 Â data	 Â set.	 Â In	 Â the	 Â following	 Â discussion,	 Â we	 Â use	 Â ğ‘¥(!),ğ‘¦(!)	 Â to	 Â denote	 Â each	 Â training	 Â example	 Â where	 Â ğ‘¥(!)âˆˆğ‘…!"	 Â is	 Â the	 Â set	 Â of	 Â features	 Â and	 Â 
ğ‘¦(!)âˆˆ 0,1	 Â is	 Â a	 Â label	 Â (0	 Â corresponds	 Â to	 Â a	 Â negative	 Â label	 Â and	 Â 1	 Â corresponds	 Â to	 Â a	 Â positive	 Â label.	 Â In	 Â SVMs,	 Â ğ‘¦(!)âˆˆ âˆ’1,1	 Â where	 Â -Â­â€1	 Â corresponds	 Â to	 Â a	 Â 
negative	 Â label	 Â and	 Â 1	 Â corresponds	 Â to	 Â a	 Â positive	 Â label).	 Â We	 Â use	 Â ğ‘‹	 Â to	 Â denote	 Â a	 Â query	 Â point	 Â (test	 Â example)	 Â and	 Â ğ‘Œ	 Â to	 Â denote	 Â the	 Â label	 Â predicted	 Â by	 Â a	 Â 
model.	 Â 
!!!!!!!	 Â where	 Â ğœƒ	 Â is	 Â the	 Â parameter.	 Â The	 Â probability	 Â of	 Â ğ‘Œ=1	 Â conditioned	 Â on	 Â ğ‘‹	 Â and	 Â parameterized	 Â by	 Â 
In	 Â logistic	 Â regression,	 Â the	 Â hypothesis	 Â is	 Â â„!ğ‘¥ =
!
ğœƒ	 Â is	 Â ğ‘ƒğ‘Œ=1|ğ‘‹,ğœƒ =â„!ğ‘‹.	 Â 
Usually,	 Â the	 Â prediction	 Â that	 Â ğ‘Œ=1	 Â is	 Â made	 Â if	 Â â„!ğ‘‹ â‰¥0.5.	 Â Since	 Â the	 Â data	 Â set	 Â is	 Â highly	 Â imbalanced,	 Â i.e.,	 Â the	 Â number	 Â of	 Â negative	 Â examples	 Â is	 Â much	 Â 
larger	 Â than	 Â that	 Â of	 Â positive	 Â ones,	 Â we	 Â consider	 Â it	 Â more	 Â important	 Â to	 Â correctly	 Â classify	 Â more	 Â positive	 Â test	 Â examples	 Â than	 Â to	 Â have	 Â a	 Â few	 Â false	 Â positives.	 Â 
Therefore,	 Â the	 Â decision	 Â threshold	 Â of	 Â â„!ğ‘‹	 Â can	 Â be	 Â less	 Â than	 Â 0.5,	 Â which	 Â makes	 Â it	 Â more	 Â likely	 Â to	 Â classify	 Â a	 Â test	 Â example	 Â as	 Â positive.	 Â We	 Â set	 Â the	 Â decision	 Â 
threshold	 Â to	 Â be	 Â ğœ‚	 Â where	 Â ğœ‚âˆˆ Â [0,0.5]	 Â and	 Â plot	 Â the	 Â cross	 Â validation	 Â results	 Â when	 Â choosing	 Â different	 Â values	 Â of	 Â ğœ‚,	 Â which	 Â is	 Â shown	 Â in	 Â Fig.	 Â 2.	 Â 

5.1 

Logistic	 Â Regression	 Â 

Fig.	 Â 2	 Â The	 Â relation	 Â between	 Â the	 Â decision	 Â threshold	 Â ğœ‚	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â logistic	 Â regression	 Â model	 Â 
	 Â It	 Â can	 Â be	 Â seen	 Â in	 Â Fig.	 Â 2	 Â that	 Â when	 Â ğœ‚=0.5,	 Â both	 Â accuracy	 Â and	 Â precision	 Â are	 Â high,	 Â but	 Â the	 Â recall	 Â is	 Â not	 Â satisfactory.	 Â When	 Â ğœ‚=0,	 Â we	 Â achieve	 Â the	 Â 
maximum	 Â recall	 Â but	 Â the	 Â accuracy	 Â and	 Â precision	 Â are	 Â very	 Â low.	 Â So	 Â a	 Â trade-Â­â€off	 Â has	 Â to	 Â be	 Â made	 Â between	 Â recall	 Â and	 Â precision/accuracy	 Â by	 Â choosing	 Â an	 Â 
appropriate	 Â value	 Â of	 Â ğœ‚.	 Â For	 Â example,	 Â if	 Â ğœ‚=0.04,	 Â both	 Â accuracy	 Â and	 Â recall	 Â are	 Â high	 Â and	 Â close	 Â to	 Â each	 Â other,	 Â meaning	 Â the	 Â accuracy	 Â for	 Â classifying	 Â all	 Â 
test	 Â examples	 Â and	 Â the	 Â one	 Â for	 Â classifying	 Â positive	 Â test	 Â examples	 Â are	 Â close.	 Â The	 Â low	 Â precision	 Â when	 Â ğœ‚=0.04	 Â is	 Â caused	 Â by	 Â the	 Â increased	 Â number	 Â of	 Â 
false	 Â positives,	 Â which	 Â is	 Â acceptable	 Â to	 Â some	 Â extent	 Â since	 Â false	 Â positives	 Â are	 Â less	 Â important	 Â than	 Â true	 Â positives	 Â in	 Â seizure	 Â prediction.	 Â 
	 Â 

2	 Â 

1  Abstract	 Â 

2 

Introduction	 Â 

Predict	 Â seizures	 Â in	 Â intracranial	 Â EEG	 Â recordings	 Â 
Linyu	 Â He	 Â (linyu90@stanford.edu)	 Â and	 Â Lingbin	 Â Li	 Â (lingbin@stanford.edu)	 Â 

	 Â 
This	 Â project	 Â aims	 Â to	 Â predict	 Â seizures	 Â in	 Â intracranial	 Â electroencephalography	 Â (iEEG)	 Â recordings	 Â using	 Â four	 Â algorithms.	 Â The	 Â data	 Â are	 Â a	 Â series	 Â of	 Â 10-Â­â€
minute	 Â iEEG	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Our	 Â 
goal	 Â is	 Â to	 Â distinguish	 Â between	 Â the	 Â two	 Â states.	 Â The	 Â major	 Â challenge	 Â is	 Â that	 Â the	 Â data	 Â are	 Â highly	 Â imbalanced,	 Â i.e.,	 Â the	 Â number	 Â of	 Â positive	 Â examples	 Â is	 Â less	 Â 
than	 Â 10%	 Â of	 Â that	 Â of	 Â negative	 Â examples.	 Â Our	 Â work	 Â is	 Â to	 Â make	 Â modifications	 Â to	 Â each	 Â of	 Â the	 Â four	 Â models	 Â and	 Â analyze	 Â the	 Â corresponding	 Â performance	 Â 
gain.	 Â 	 Â 
Spontaneous	 Â seizures	 Â are	 Â the	 Â typical	 Â symptom	 Â of	 Â epilepsy,	 Â which	 Â is	 Â a	 Â common	 Â but	 Â refractory	 Â neurological	 Â disorder	 Â that	 Â afflicts	 Â nearly	 Â 1%	 Â of	 Â the	 Â 
worldâ€™s	 Â population.	 Â Anticonvulsant	 Â medications	 Â are	 Â administered	 Â to	 Â many	 Â patients	 Â at	 Â high	 Â doses	 Â to	 Â prevent	 Â seizures,	 Â but	 Â their	 Â effectiveness	 Â is	 Â 
limited	 Â and	 Â patients	 Â often	 Â suffer	 Â their	 Â side	 Â effects.	 Â Even	 Â for	 Â patients	 Â whose	 Â epilepsy-Â­â€causing	 Â brain	 Â tissue	 Â is	 Â removed	 Â via	 Â surgery,	 Â spontaneous	 Â 
seizures	 Â still	 Â persist.	 Â Due	 Â to	 Â the	 Â seemingly	 Â unpredictable	 Â occurrence	 Â of	 Â seizures,	 Â patients	 Â with	 Â epilepsy	 Â experience	 Â constant	 Â anxiety	 Â [1].	 Â 
This	 Â project	 Â aims	 Â to	 Â make	 Â it	 Â possible	 Â that	 Â devices	 Â designed	 Â to	 Â monitor	 Â patientsâ€™	 Â brain	 Â activity	 Â can	 Â warn	 Â them	 Â of	 Â impeding	 Â seizures	 Â so	 Â that	 Â patients	 Â 
are	 Â able	 Â to	 Â take	 Â appropriate	 Â precautions.	 Â It	 Â is	 Â also	 Â helpful	 Â to	 Â reduce	 Â overall	 Â side	 Â effects	 Â caused	 Â by	 Â anticonvulsant	 Â medications	 Â taken	 Â by	 Â these	 Â 
patients.	 Â By	 Â providing	 Â them	 Â with	 Â devices	 Â with	 Â the	 Â ability	 Â to	 Â predict	 Â an	 Â impending	 Â seizure,	 Â anticonvulsant	 Â medications	 Â could	 Â be	 Â administered	 Â only	 Â 
when	 Â necessary,	 Â thus	 Â lowering	 Â the	 Â doses	 Â given	 Â to	 Â patients.	 Â 
Multiple	 Â researches	 Â support	 Â the	 Â notion	 Â that	 Â the	 Â occurrence	 Â of	 Â seizures	 Â is	 Â not	 Â random.	 Â According	 Â to	 Â evidence	 Â shown	 Â by	 Â related	 Â researches,	 Â for	 Â 
patients	 Â with	 Â epilepsy,	 Â the	 Â temporal	 Â dynamics	 Â of	 Â brain	 Â activity	 Â can	 Â be	 Â classified	 Â into	 Â 4	 Â states:	 Â interictal	 Â (between	 Â seizures),	 Â preictal	 Â (prior	 Â to	 Â 
seizures),	 Â ictal	 Â (seizure)	 Â and	 Â postictal	 Â (after	 Â seizures).	 Â The	 Â brain	 Â activity	 Â of	 Â each	 Â state	 Â can	 Â be	 Â recorded	 Â by	 Â iEEG	 Â [1].	 Â Our	 Â goal	 Â is	 Â to	 Â employ	 Â machine	 Â 
learning	 Â techniques	 Â to	 Â learn	 Â from	 Â iEEG	 Â data	 Â the	 Â characteristics	 Â of	 Â preictal	 Â states	 Â and	 Â then	 Â distinguish	 Â these	 Â states	 Â from	 Â the	 Â interictal	 Â states.	 Â After	 Â 
one	 Â preictal	 Â state	 Â is	 Â identified,	 Â a	 Â warning	 Â should	 Â be	 Â sent	 Â to	 Â the	 Â patient	 Â to	 Â prepare	 Â him	 Â or	 Â her	 Â for	 Â an	 Â impending	 Â seizure.	 Â 
Kaggle	 Â provides	 Â iEEG	 Â data	 Â collected	 Â from	 Â canine	 Â subjects.	 Â The	 Â data	 Â of	 Â each	 Â subject	 Â is	 Â organized	 Â into	 Â 10-Â­â€minute	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â 
data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Each	 Â clip	 Â contains	 Â 16	 Â channels	 Â of	 Â iEEG	 Â data	 Â where	 Â 
each	 Â channel	 Â corresponds	 Â to	 Â one	 Â electrode	 Â implanted	 Â in	 Â the	 Â subjectâ€™s	 Â brain.	 Â For	 Â each	 Â training	 Â example	 Â ğ‘¥(!),ğ‘¦(!),	 Â ğ‘¦(!)	 Â is	 Â the	 Â label	 Â and	 Â ğ‘¥(!)	 Â is	 Â a	 Â clip	 Â in	 Â 
which	 Â each	 Â row	 Â corresponds	 Â to	 Â one	 Â channel	 Â and	 Â each	 Â column	 Â corresponds	 Â to	 Â iEEG	 Â readings	 Â at	 Â one	 Â sampling	 Â time	 Â point.	 Â 
Since	 Â seizures	 Â in	 Â most	 Â patients	 Â are	 Â associated	 Â with	 Â a	 Â stereotypic	 Â EEG	 Â discharge	 Â with	 Â characteristic	 Â spectral	 Â pattern,	 Â we	 Â employed	 Â the	 Â following	 Â 
feature	 Â extraction	 Â procedure	 Â [2]:	 Â 
Apply	 Â fast	 Â Fourier	 Â transform	 Â to	 Â each	 Â channel	 Â in	 Â a	 Â clip	 Â and	 Â divide	 Â the	 Â resulting	 Â power	 Â spectrum	 Â into	 Â 6	 Â bands:	 Â delta	 Â (0.1	 Â â€“	 Â 4	 Â Hz),	 Â theta	 Â (4	 Â â€“	 Â 8	 Â Hz),	 Â 
alpha	 Â (8	 Â â€“	 Â 12	 Â Hz),	 Â beta	 Â (12	 Â â€“	 Â 30	 Â Hz),	 Â low-Â­â€gamma	 Â (30	 Â â€“	 Â 70	 Â Hz),	 Â and	 Â high-Â­â€gamma	 Â (70	 Â â€“	 Â 180	 Â Hz).	 Â In	 Â each	 Â band,	 Â sum	 Â the	 Â power	 Â over	 Â all	 Â band	 Â 
frequencies	 Â to	 Â produce	 Â a	 Â power-Â­â€in-Â­â€band	 Â (PIB)	 Â feature.	 Â Therefore,	 Â 6	 Â features	 Â are	 Â obtained	 Â in	 Â each	 Â channel	 Â and	 Â 96	 Â features	 Â are	 Â obtained	 Â in	 Â one	 Â clip.	 Â 
The	 Â procedure	 Â above	 Â is	 Â also	 Â illustrated	 Â in	 Â Fig	 Â 1,	 Â where	 Â ğ‘(ğ‘“)	 Â is	 Â the	 Â power	 Â spectrum.	 Â 
Channel	 Â 1	 Â 
Channel	 Â 2	 Â 
â€¦	 Â â€¦	 Â â€¦	 Â Channel	 Â 16	 Â 
There	 Â are	 Â 3939	 Â examples	 Â in	 Â total,	 Â in	 Â which	 Â 3674	 Â are	 Â negative	 Â and	 Â 265	 Â are	 Â positive.	 Â For	 Â each	 Â time	 Â of	 Â cross	 Â validation,	 Â we	 Â randomly	 Â pick	 Â 70%	 Â of	 Â the	 Â 
negative	 Â examples	 Â and	 Â 70%	 Â of	 Â the	 Â positive	 Â examples	 Â for	 Â training	 Â and	 Â use	 Â the	 Â rest	 Â for	 Â testing.	 Â This	 Â process	 Â is	 Â repeated	 Â for	 Â 100	 Â times	 Â to	 Â calculate	 Â the	 Â 
average	 Â evaluation.	 Â 
	 Â 
1	 Â 

delta	 Â (0.1	 Â â€“	 Â 4Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!.!
theta	 Â (4	 Â â€“	 Â 8Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!
alpha	 Â (8	 Â â€“	 Â 12Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!
beta	 Â (12	 Â â€“	 Â 30Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!"
low-Â­â€gamma	 Â (30	 Â â€“	 Â 70Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"!!!"
high-Â­â€gamma	 Â (70	 Â â€“	 Â 180Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"#!!!"

Fig.	 Â 1	 Â Feature	 Â Extraction	 Â using	 Â FFT	 Â 

3  Data	 Â and	 Â Feature	 Â Extraction	 Â 

FFT	 Â 

4  Cross	 Â Validation	 Â 

Name	 Â 

Definition	 Â 

5 

Besides	 Â the	 Â training	 Â error	 Â and	 Â the	 Â testing	 Â error,	 Â the	 Â following	 Â values	 Â are	 Â adopted	 Â to	 Â evaluate	 Â the	 Â performance	 Â of	 Â each	 Â model	 Â since	 Â the	 Â data	 Â are	 Â 
highly	 Â imbalanced.	 Â 
	 Â 

Table	 Â 1.	 Â Values	 Â chosen	 Â to	 Â evaluate	 Â the	 Â performance	 Â 

Learning	 Â Algorithms	 Â 

Accuracy	 Â (ACC)	 Â 
Positive	 Â Predictive	 Â Rate(PPV)/Precision	 Â 
True	 Â Positive	 Â Rate	 Â (TPR)/Recall	 Â 
False	 Â Negative	 Â Rate	 Â (FNR)/Miss	 Â Rate	 Â 
False	 Â Positive	 Â Rate	 Â (FPR)/Fall-Â­â€out	 Â 

ğ‘‡ğ‘ƒ+ğ‘‡ğ‘
ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘	 Â 
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ	 Â 
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘	 Â 
ğ¹ğ‘ğ‘‡ğ‘ƒ+ğ¹ğ‘	 Â 
ğ¹ğ‘ƒ
ğ¹ğ‘ƒ+ğ‘‡ğ‘	 Â 
	 Â In	 Â Table	 Â 1,	 Â ğ‘‡ğ‘ƒ,	 Â ğ‘‡ğ‘,	 Â ğ¹ğ‘ƒ	 Â and	 Â ğ¹ğ‘	 Â are	 Â the	 Â number	 Â of	 Â true	 Â positives,	 Â true	 Â negatives,	 Â false	 Â positives	 Â and	 Â false	 Â negatives,	 Â respectively.	 Â Finally,	 Â the	 Â receiver	 Â 
operating	 Â characteristic	 Â (ROC)	 Â curves	 Â and	 Â precision-Â­â€recall	 Â curves	 Â will	 Â be	 Â plotted	 Â based	 Â on	 Â values	 Â in	 Â the	 Â table	 Â above.	 Â 
In	 Â out	 Â attempt	 Â to	 Â seek	 Â a	 Â solution,	 Â three	 Â models	 Â covered	 Â in	 Â CS	 Â 229	 Â were	 Â first	 Â adopted,	 Â which	 Â are	 Â logistic	 Â regression,	 Â naÃ¯ve	 Â Bayes	 Â classifiers	 Â and	 Â 
support	 Â vector	 Â machines	 Â (SVMs).	 Â Later,	 Â we	 Â employed	 Â a	 Â model	 Â inherited	 Â from	 Â communication	 Â systems,	 Â which	 Â makes	 Â a	 Â prediction	 Â based	 Â on	 Â 
correlation	 Â coefficients	 Â between	 Â the	 Â test	 Â example	 Â and	 Â all	 Â training	 Â examples.	 Â Modifications	 Â are	 Â made	 Â to	 Â each	 Â model	 Â to	 Â improve	 Â their	 Â performance	 Â on	 Â 
an	 Â imbalanced	 Â data	 Â set.	 Â In	 Â the	 Â following	 Â discussion,	 Â we	 Â use	 Â ğ‘¥(!),ğ‘¦(!)	 Â to	 Â denote	 Â each	 Â training	 Â example	 Â where	 Â ğ‘¥(!)âˆˆğ‘…!"	 Â is	 Â the	 Â set	 Â of	 Â features	 Â and	 Â 
ğ‘¦(!)âˆˆ 0,1	 Â is	 Â a	 Â label	 Â (0	 Â corresponds	 Â to	 Â a	 Â negative	 Â label	 Â and	 Â 1	 Â corresponds	 Â to	 Â a	 Â positive	 Â label.	 Â In	 Â SVMs,	 Â ğ‘¦(!)âˆˆ âˆ’1,1	 Â where	 Â -Â­â€1	 Â corresponds	 Â to	 Â a	 Â 
negative	 Â label	 Â and	 Â 1	 Â corresponds	 Â to	 Â a	 Â positive	 Â label).	 Â We	 Â use	 Â ğ‘‹	 Â to	 Â denote	 Â a	 Â query	 Â point	 Â (test	 Â example)	 Â and	 Â ğ‘Œ	 Â to	 Â denote	 Â the	 Â label	 Â predicted	 Â by	 Â a	 Â 
model.	 Â 
!!!!!!!	 Â where	 Â ğœƒ	 Â is	 Â the	 Â parameter.	 Â The	 Â probability	 Â of	 Â ğ‘Œ=1	 Â conditioned	 Â on	 Â ğ‘‹	 Â and	 Â parameterized	 Â by	 Â 
In	 Â logistic	 Â regression,	 Â the	 Â hypothesis	 Â is	 Â â„!ğ‘¥ =
!
ğœƒ	 Â is	 Â ğ‘ƒğ‘Œ=1|ğ‘‹,ğœƒ =â„!ğ‘‹.	 Â 
Usually,	 Â the	 Â prediction	 Â that	 Â ğ‘Œ=1	 Â is	 Â made	 Â if	 Â â„!ğ‘‹ â‰¥0.5.	 Â Since	 Â the	 Â data	 Â set	 Â is	 Â highly	 Â imbalanced,	 Â i.e.,	 Â the	 Â number	 Â of	 Â negative	 Â examples	 Â is	 Â much	 Â 
larger	 Â than	 Â that	 Â of	 Â positive	 Â ones,	 Â we	 Â consider	 Â it	 Â more	 Â important	 Â to	 Â correctly	 Â classify	 Â more	 Â positive	 Â test	 Â examples	 Â than	 Â to	 Â have	 Â a	 Â few	 Â false	 Â positives.	 Â 
Therefore,	 Â the	 Â decision	 Â threshold	 Â of	 Â â„!ğ‘‹	 Â can	 Â be	 Â less	 Â than	 Â 0.5,	 Â which	 Â makes	 Â it	 Â more	 Â likely	 Â to	 Â classify	 Â a	 Â test	 Â example	 Â as	 Â positive.	 Â We	 Â set	 Â the	 Â decision	 Â 
threshold	 Â to	 Â be	 Â ğœ‚	 Â where	 Â ğœ‚âˆˆ Â [0,0.5]	 Â and	 Â plot	 Â the	 Â cross	 Â validation	 Â results	 Â when	 Â choosing	 Â different	 Â values	 Â of	 Â ğœ‚,	 Â which	 Â is	 Â shown	 Â in	 Â Fig.	 Â 2.	 Â 

5.1 

Logistic	 Â Regression	 Â 

Fig.	 Â 2	 Â The	 Â relation	 Â between	 Â the	 Â decision	 Â threshold	 Â ğœ‚	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â logistic	 Â regression	 Â model	 Â 
	 Â It	 Â can	 Â be	 Â seen	 Â in	 Â Fig.	 Â 2	 Â that	 Â when	 Â ğœ‚=0.5,	 Â both	 Â accuracy	 Â and	 Â precision	 Â are	 Â high,	 Â but	 Â the	 Â recall	 Â is	 Â not	 Â satisfactory.	 Â When	 Â ğœ‚=0,	 Â we	 Â achieve	 Â the	 Â 
maximum	 Â recall	 Â but	 Â the	 Â accuracy	 Â and	 Â precision	 Â are	 Â very	 Â low.	 Â So	 Â a	 Â trade-Â­â€off	 Â has	 Â to	 Â be	 Â made	 Â between	 Â recall	 Â and	 Â precision/accuracy	 Â by	 Â choosing	 Â an	 Â 
appropriate	 Â value	 Â of	 Â ğœ‚.	 Â For	 Â example,	 Â if	 Â ğœ‚=0.04,	 Â both	 Â accuracy	 Â and	 Â recall	 Â are	 Â high	 Â and	 Â close	 Â to	 Â each	 Â other,	 Â meaning	 Â the	 Â accuracy	 Â for	 Â classifying	 Â all	 Â 
test	 Â examples	 Â and	 Â the	 Â one	 Â for	 Â classifying	 Â positive	 Â test	 Â examples	 Â are	 Â close.	 Â The	 Â low	 Â precision	 Â when	 Â ğœ‚=0.04	 Â is	 Â caused	 Â by	 Â the	 Â increased	 Â number	 Â of	 Â 
false	 Â positives,	 Â which	 Â is	 Â acceptable	 Â to	 Â some	 Â extent	 Â since	 Â false	 Â positives	 Â are	 Â less	 Â important	 Â than	 Â true	 Â positives	 Â in	 Â seizure	 Â prediction.	 Â 
	 Â 

2	 Â 

5.2 

NaÃ¯ve	 Â Bayes	 Â 

The	 Â multinomial	 Â distribution	 Â is	 Â used	 Â to	 Â model	 Â the	 Â features	 Â of	 Â each	 Â iEEG	 Â clip.	 Â Since	 Â the	 Â value	 Â of	 Â each	 Â feature	 Â is	 Â continuous,	 Â we	 Â first	 Â discretize	 Â the	 Â 
values	 Â into	 Â ğ‘	 Â groups	 Â where	 Â ğ‘=!!"#! ,	 Â ğ‘‰!"#	 Â is	 Â the	 Â maximum	 Â value	 Â of	 Â the	 Â features	 Â of	 Â all	 Â clips	 Â and	 Â ğº	 Â is	 Â the	 Â group	 Â size.	 Â Similar	 Â to	 Â the	 Â modification	 Â made	 Â 
to	 Â logistic	 Â regression,	 Â a	 Â test	 Â example	 Â is	 Â labeled	 Â â€œ1â€	 Â if	 Â ğ‘„ğ‘ƒğ‘Œ=1|ğ‘‹ â‰¥ğ‘ƒğ‘Œ=0|ğ‘‹	 Â where	 Â ğ‘„	 Â is	 Â a	 Â positive	 Â constant	 Â specified	 Â to	 Â overcome	 Â the	 Â 
imbalanced	 Â data	 Â when	 Â making	 Â predictions.	 Â Fig.	 Â 3	 Â indicates	 Â that	 Â the	 Â naÃ¯ve	 Â Bayesian	 Â model	 Â cannot	 Â make	 Â satisfactory	 Â predictions	 Â no	 Â matter	 Â what	 Â the	 Â 
value	 Â of	 Â ğ‘„	 Â is.	 Â 

Fig.	 Â 3	 Â The	 Â relation	 Â between	 Â ğ‘„	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â naÃ¯ve	 Â Bayesian	 Â model	 Â 	 Â 

5.3 

Support	 Â Vector	 Â Machine	 Â (SVM)	 Â 

In	 Â the	 Â ğ‘™!-Â­â€regularized	 Â SVM,	 Â the	 Â hypothesis	 Â is	 Â â„!,!ğ‘¥ =ğ‘¤!ğ‘¥+ğ‘	 Â where	 Â parameters	 Â ğ‘¤	 Â and	 Â ğ‘	 Â are	 Â obtained	 Â by	 Â solving	 Â the	 Â primal	 Â optimization	 Â problem	 Â 
whose	 Â objective	 Â is	 Â min!,!!! ğ‘¤ !+ğ¶
	 Â is	 Â the	 Â cost	 Â term.	 Â Since	 Â it	 Â is	 Â more	 Â important	 Â to	 Â correctly	 Â classify	 Â more	 Â positive	 Â test	 Â 
examples	 Â than	 Â to	 Â have	 Â a	 Â few	 Â false	 Â positives,	 Â the	 Â 2-Â­â€cost-Â­â€sensitive	 Â SVM	 Â (2C-Â­â€SVM)	 Â [3]	 Â is	 Â adopted	 Â in	 Â which	 Â two	 Â different	 Â costs	 Â are	 Â assigned	 Â to	 Â negative	 Â 
and	 Â positive	 Â examples,	 Â respectively.	 Â In	 Â 2C-Â­â€SVM,	 Â the	 Â objective	 Â of	 Â the	 Â primal	 Â optimization	 Â problem	 Â is	 Â min!,!!! ğ‘¤ !+ğ¶!
	 Â where	 Â 
ğ¶=ğ¶!+ğ¶!	 Â is	 Â a	 Â trade	 Â off	 Â between	 Â the	 Â classification	 Â margin	 Â and	 Â misclassified	 Â or	 Â non-Â­â€separable	 Â examples	 Â and	 Â the	 Â cost	 Â factor	 Â ğ‘…=!!!!	 Â is	 Â the	 Â ratio	 Â of	 Â 
costs	 Â between	 Â positive	 Â and	 Â negative	 Â examples.	 Â We	 Â employ	 Â the	 Â LIBSVM	 Â library	 Â [4]	 Â as	 Â our	 Â 2C-Â­â€SVM	 Â implementation.	 Â 

ğœ‰!
!âˆˆ!! +ğ¶!

,	 Â where	 Â ğ¶

ğœ‰!!!!!

ğœ‰!!!!!

ğœ‰!

!âˆˆ!!

Fig.	 Â 4	 Â The	 Â relation	 Â between	 Â ğ¶,	 Â ğ‘…	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â 2C-Â­â€SVM	 Â model	 Â 

	 Â Different	 Â values	 Â of	 Â ğ¶>0	 Â and	 Â ğ‘…â‰¥1	 Â are	 Â chosen	 Â and	 Â the	 Â corresponding	 Â results	 Â of	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â are	 Â shown	 Â in	 Â Fig.	 Â 4.	 Â Satisfactory	 Â 
accuracy	 Â and	 Â recall	 Â can	 Â be	 Â achieved	 Â when	 Â (ğ¶,ğ‘…)	 Â is	 Â chosen	 Â as	 Â 10!!,10!,	 Â 10!!,10!	 Â or	 Â 10!!,10!,	 Â etc.	 Â 
We	 Â derived	 Â the	 Â correlation	 Â decision	 Â model	 Â from	 Â a	 Â few	 Â concepts	 Â in	 Â communication	 Â systems.	 Â The	 Â previous	 Â three	 Â models	 Â discard	 Â the	 Â training	 Â set	 Â after	 Â 
a	 Â hypothesis	 Â is	 Â built.	 Â However,	 Â in	 Â correlation	 Â decision,	 Â we	 Â do	 Â not	 Â use	 Â training	 Â examples	 Â to	 Â build	 Â a	 Â hypothesis	 Â and	 Â we	 Â keep	 Â the	 Â entire	 Â training	 Â set.	 Â 

Correlation	 Â Decision	 Â 

5.4 

	 Â 

	 Â 

3	 Â 

1  Abstract	 Â 

2 

Introduction	 Â 

Predict	 Â seizures	 Â in	 Â intracranial	 Â EEG	 Â recordings	 Â 
Linyu	 Â He	 Â (linyu90@stanford.edu)	 Â and	 Â Lingbin	 Â Li	 Â (lingbin@stanford.edu)	 Â 

	 Â 
This	 Â project	 Â aims	 Â to	 Â predict	 Â seizures	 Â in	 Â intracranial	 Â electroencephalography	 Â (iEEG)	 Â recordings	 Â using	 Â four	 Â algorithms.	 Â The	 Â data	 Â are	 Â a	 Â series	 Â of	 Â 10-Â­â€
minute	 Â iEEG	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Our	 Â 
goal	 Â is	 Â to	 Â distinguish	 Â between	 Â the	 Â two	 Â states.	 Â The	 Â major	 Â challenge	 Â is	 Â that	 Â the	 Â data	 Â are	 Â highly	 Â imbalanced,	 Â i.e.,	 Â the	 Â number	 Â of	 Â positive	 Â examples	 Â is	 Â less	 Â 
than	 Â 10%	 Â of	 Â that	 Â of	 Â negative	 Â examples.	 Â Our	 Â work	 Â is	 Â to	 Â make	 Â modifications	 Â to	 Â each	 Â of	 Â the	 Â four	 Â models	 Â and	 Â analyze	 Â the	 Â corresponding	 Â performance	 Â 
gain.	 Â 	 Â 
Spontaneous	 Â seizures	 Â are	 Â the	 Â typical	 Â symptom	 Â of	 Â epilepsy,	 Â which	 Â is	 Â a	 Â common	 Â but	 Â refractory	 Â neurological	 Â disorder	 Â that	 Â afflicts	 Â nearly	 Â 1%	 Â of	 Â the	 Â 
worldâ€™s	 Â population.	 Â Anticonvulsant	 Â medications	 Â are	 Â administered	 Â to	 Â many	 Â patients	 Â at	 Â high	 Â doses	 Â to	 Â prevent	 Â seizures,	 Â but	 Â their	 Â effectiveness	 Â is	 Â 
limited	 Â and	 Â patients	 Â often	 Â suffer	 Â their	 Â side	 Â effects.	 Â Even	 Â for	 Â patients	 Â whose	 Â epilepsy-Â­â€causing	 Â brain	 Â tissue	 Â is	 Â removed	 Â via	 Â surgery,	 Â spontaneous	 Â 
seizures	 Â still	 Â persist.	 Â Due	 Â to	 Â the	 Â seemingly	 Â unpredictable	 Â occurrence	 Â of	 Â seizures,	 Â patients	 Â with	 Â epilepsy	 Â experience	 Â constant	 Â anxiety	 Â [1].	 Â 
This	 Â project	 Â aims	 Â to	 Â make	 Â it	 Â possible	 Â that	 Â devices	 Â designed	 Â to	 Â monitor	 Â patientsâ€™	 Â brain	 Â activity	 Â can	 Â warn	 Â them	 Â of	 Â impeding	 Â seizures	 Â so	 Â that	 Â patients	 Â 
are	 Â able	 Â to	 Â take	 Â appropriate	 Â precautions.	 Â It	 Â is	 Â also	 Â helpful	 Â to	 Â reduce	 Â overall	 Â side	 Â effects	 Â caused	 Â by	 Â anticonvulsant	 Â medications	 Â taken	 Â by	 Â these	 Â 
patients.	 Â By	 Â providing	 Â them	 Â with	 Â devices	 Â with	 Â the	 Â ability	 Â to	 Â predict	 Â an	 Â impending	 Â seizure,	 Â anticonvulsant	 Â medications	 Â could	 Â be	 Â administered	 Â only	 Â 
when	 Â necessary,	 Â thus	 Â lowering	 Â the	 Â doses	 Â given	 Â to	 Â patients.	 Â 
Multiple	 Â researches	 Â support	 Â the	 Â notion	 Â that	 Â the	 Â occurrence	 Â of	 Â seizures	 Â is	 Â not	 Â random.	 Â According	 Â to	 Â evidence	 Â shown	 Â by	 Â related	 Â researches,	 Â for	 Â 
patients	 Â with	 Â epilepsy,	 Â the	 Â temporal	 Â dynamics	 Â of	 Â brain	 Â activity	 Â can	 Â be	 Â classified	 Â into	 Â 4	 Â states:	 Â interictal	 Â (between	 Â seizures),	 Â preictal	 Â (prior	 Â to	 Â 
seizures),	 Â ictal	 Â (seizure)	 Â and	 Â postictal	 Â (after	 Â seizures).	 Â The	 Â brain	 Â activity	 Â of	 Â each	 Â state	 Â can	 Â be	 Â recorded	 Â by	 Â iEEG	 Â [1].	 Â Our	 Â goal	 Â is	 Â to	 Â employ	 Â machine	 Â 
learning	 Â techniques	 Â to	 Â learn	 Â from	 Â iEEG	 Â data	 Â the	 Â characteristics	 Â of	 Â preictal	 Â states	 Â and	 Â then	 Â distinguish	 Â these	 Â states	 Â from	 Â the	 Â interictal	 Â states.	 Â After	 Â 
one	 Â preictal	 Â state	 Â is	 Â identified,	 Â a	 Â warning	 Â should	 Â be	 Â sent	 Â to	 Â the	 Â patient	 Â to	 Â prepare	 Â him	 Â or	 Â her	 Â for	 Â an	 Â impending	 Â seizure.	 Â 
Kaggle	 Â provides	 Â iEEG	 Â data	 Â collected	 Â from	 Â canine	 Â subjects.	 Â The	 Â data	 Â of	 Â each	 Â subject	 Â is	 Â organized	 Â into	 Â 10-Â­â€minute	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â 
data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Each	 Â clip	 Â contains	 Â 16	 Â channels	 Â of	 Â iEEG	 Â data	 Â where	 Â 
each	 Â channel	 Â corresponds	 Â to	 Â one	 Â electrode	 Â implanted	 Â in	 Â the	 Â subjectâ€™s	 Â brain.	 Â For	 Â each	 Â training	 Â example	 Â ğ‘¥(!),ğ‘¦(!),	 Â ğ‘¦(!)	 Â is	 Â the	 Â label	 Â and	 Â ğ‘¥(!)	 Â is	 Â a	 Â clip	 Â in	 Â 
which	 Â each	 Â row	 Â corresponds	 Â to	 Â one	 Â channel	 Â and	 Â each	 Â column	 Â corresponds	 Â to	 Â iEEG	 Â readings	 Â at	 Â one	 Â sampling	 Â time	 Â point.	 Â 
Since	 Â seizures	 Â in	 Â most	 Â patients	 Â are	 Â associated	 Â with	 Â a	 Â stereotypic	 Â EEG	 Â discharge	 Â with	 Â characteristic	 Â spectral	 Â pattern,	 Â we	 Â employed	 Â the	 Â following	 Â 
feature	 Â extraction	 Â procedure	 Â [2]:	 Â 
Apply	 Â fast	 Â Fourier	 Â transform	 Â to	 Â each	 Â channel	 Â in	 Â a	 Â clip	 Â and	 Â divide	 Â the	 Â resulting	 Â power	 Â spectrum	 Â into	 Â 6	 Â bands:	 Â delta	 Â (0.1	 Â â€“	 Â 4	 Â Hz),	 Â theta	 Â (4	 Â â€“	 Â 8	 Â Hz),	 Â 
alpha	 Â (8	 Â â€“	 Â 12	 Â Hz),	 Â beta	 Â (12	 Â â€“	 Â 30	 Â Hz),	 Â low-Â­â€gamma	 Â (30	 Â â€“	 Â 70	 Â Hz),	 Â and	 Â high-Â­â€gamma	 Â (70	 Â â€“	 Â 180	 Â Hz).	 Â In	 Â each	 Â band,	 Â sum	 Â the	 Â power	 Â over	 Â all	 Â band	 Â 
frequencies	 Â to	 Â produce	 Â a	 Â power-Â­â€in-Â­â€band	 Â (PIB)	 Â feature.	 Â Therefore,	 Â 6	 Â features	 Â are	 Â obtained	 Â in	 Â each	 Â channel	 Â and	 Â 96	 Â features	 Â are	 Â obtained	 Â in	 Â one	 Â clip.	 Â 
The	 Â procedure	 Â above	 Â is	 Â also	 Â illustrated	 Â in	 Â Fig	 Â 1,	 Â where	 Â ğ‘(ğ‘“)	 Â is	 Â the	 Â power	 Â spectrum.	 Â 
Channel	 Â 1	 Â 
Channel	 Â 2	 Â 
â€¦	 Â â€¦	 Â â€¦	 Â Channel	 Â 16	 Â 
There	 Â are	 Â 3939	 Â examples	 Â in	 Â total,	 Â in	 Â which	 Â 3674	 Â are	 Â negative	 Â and	 Â 265	 Â are	 Â positive.	 Â For	 Â each	 Â time	 Â of	 Â cross	 Â validation,	 Â we	 Â randomly	 Â pick	 Â 70%	 Â of	 Â the	 Â 
negative	 Â examples	 Â and	 Â 70%	 Â of	 Â the	 Â positive	 Â examples	 Â for	 Â training	 Â and	 Â use	 Â the	 Â rest	 Â for	 Â testing.	 Â This	 Â process	 Â is	 Â repeated	 Â for	 Â 100	 Â times	 Â to	 Â calculate	 Â the	 Â 
average	 Â evaluation.	 Â 
	 Â 
1	 Â 

delta	 Â (0.1	 Â â€“	 Â 4Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!.!
theta	 Â (4	 Â â€“	 Â 8Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!
alpha	 Â (8	 Â â€“	 Â 12Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!
beta	 Â (12	 Â â€“	 Â 30Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!"
low-Â­â€gamma	 Â (30	 Â â€“	 Â 70Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"!!!"
high-Â­â€gamma	 Â (70	 Â â€“	 Â 180Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"#!!!"

Fig.	 Â 1	 Â Feature	 Â Extraction	 Â using	 Â FFT	 Â 

3  Data	 Â and	 Â Feature	 Â Extraction	 Â 

FFT	 Â 

4  Cross	 Â Validation	 Â 

Name	 Â 

Definition	 Â 

5 

Besides	 Â the	 Â training	 Â error	 Â and	 Â the	 Â testing	 Â error,	 Â the	 Â following	 Â values	 Â are	 Â adopted	 Â to	 Â evaluate	 Â the	 Â performance	 Â of	 Â each	 Â model	 Â since	 Â the	 Â data	 Â are	 Â 
highly	 Â imbalanced.	 Â 
	 Â 

Table	 Â 1.	 Â Values	 Â chosen	 Â to	 Â evaluate	 Â the	 Â performance	 Â 

Learning	 Â Algorithms	 Â 

Accuracy	 Â (ACC)	 Â 
Positive	 Â Predictive	 Â Rate(PPV)/Precision	 Â 
True	 Â Positive	 Â Rate	 Â (TPR)/Recall	 Â 
False	 Â Negative	 Â Rate	 Â (FNR)/Miss	 Â Rate	 Â 
False	 Â Positive	 Â Rate	 Â (FPR)/Fall-Â­â€out	 Â 

ğ‘‡ğ‘ƒ+ğ‘‡ğ‘
ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘	 Â 
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ	 Â 
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘	 Â 
ğ¹ğ‘ğ‘‡ğ‘ƒ+ğ¹ğ‘	 Â 
ğ¹ğ‘ƒ
ğ¹ğ‘ƒ+ğ‘‡ğ‘	 Â 
	 Â In	 Â Table	 Â 1,	 Â ğ‘‡ğ‘ƒ,	 Â ğ‘‡ğ‘,	 Â ğ¹ğ‘ƒ	 Â and	 Â ğ¹ğ‘	 Â are	 Â the	 Â number	 Â of	 Â true	 Â positives,	 Â true	 Â negatives,	 Â false	 Â positives	 Â and	 Â false	 Â negatives,	 Â respectively.	 Â Finally,	 Â the	 Â receiver	 Â 
operating	 Â characteristic	 Â (ROC)	 Â curves	 Â and	 Â precision-Â­â€recall	 Â curves	 Â will	 Â be	 Â plotted	 Â based	 Â on	 Â values	 Â in	 Â the	 Â table	 Â above.	 Â 
In	 Â out	 Â attempt	 Â to	 Â seek	 Â a	 Â solution,	 Â three	 Â models	 Â covered	 Â in	 Â CS	 Â 229	 Â were	 Â first	 Â adopted,	 Â which	 Â are	 Â logistic	 Â regression,	 Â naÃ¯ve	 Â Bayes	 Â classifiers	 Â and	 Â 
support	 Â vector	 Â machines	 Â (SVMs).	 Â Later,	 Â we	 Â employed	 Â a	 Â model	 Â inherited	 Â from	 Â communication	 Â systems,	 Â which	 Â makes	 Â a	 Â prediction	 Â based	 Â on	 Â 
correlation	 Â coefficients	 Â between	 Â the	 Â test	 Â example	 Â and	 Â all	 Â training	 Â examples.	 Â Modifications	 Â are	 Â made	 Â to	 Â each	 Â model	 Â to	 Â improve	 Â their	 Â performance	 Â on	 Â 
an	 Â imbalanced	 Â data	 Â set.	 Â In	 Â the	 Â following	 Â discussion,	 Â we	 Â use	 Â ğ‘¥(!),ğ‘¦(!)	 Â to	 Â denote	 Â each	 Â training	 Â example	 Â where	 Â ğ‘¥(!)âˆˆğ‘…!"	 Â is	 Â the	 Â set	 Â of	 Â features	 Â and	 Â 
ğ‘¦(!)âˆˆ 0,1	 Â is	 Â a	 Â label	 Â (0	 Â corresponds	 Â to	 Â a	 Â negative	 Â label	 Â and	 Â 1	 Â corresponds	 Â to	 Â a	 Â positive	 Â label.	 Â In	 Â SVMs,	 Â ğ‘¦(!)âˆˆ âˆ’1,1	 Â where	 Â -Â­â€1	 Â corresponds	 Â to	 Â a	 Â 
negative	 Â label	 Â and	 Â 1	 Â corresponds	 Â to	 Â a	 Â positive	 Â label).	 Â We	 Â use	 Â ğ‘‹	 Â to	 Â denote	 Â a	 Â query	 Â point	 Â (test	 Â example)	 Â and	 Â ğ‘Œ	 Â to	 Â denote	 Â the	 Â label	 Â predicted	 Â by	 Â a	 Â 
model.	 Â 
!!!!!!!	 Â where	 Â ğœƒ	 Â is	 Â the	 Â parameter.	 Â The	 Â probability	 Â of	 Â ğ‘Œ=1	 Â conditioned	 Â on	 Â ğ‘‹	 Â and	 Â parameterized	 Â by	 Â 
In	 Â logistic	 Â regression,	 Â the	 Â hypothesis	 Â is	 Â â„!ğ‘¥ =
!
ğœƒ	 Â is	 Â ğ‘ƒğ‘Œ=1|ğ‘‹,ğœƒ =â„!ğ‘‹.	 Â 
Usually,	 Â the	 Â prediction	 Â that	 Â ğ‘Œ=1	 Â is	 Â made	 Â if	 Â â„!ğ‘‹ â‰¥0.5.	 Â Since	 Â the	 Â data	 Â set	 Â is	 Â highly	 Â imbalanced,	 Â i.e.,	 Â the	 Â number	 Â of	 Â negative	 Â examples	 Â is	 Â much	 Â 
larger	 Â than	 Â that	 Â of	 Â positive	 Â ones,	 Â we	 Â consider	 Â it	 Â more	 Â important	 Â to	 Â correctly	 Â classify	 Â more	 Â positive	 Â test	 Â examples	 Â than	 Â to	 Â have	 Â a	 Â few	 Â false	 Â positives.	 Â 
Therefore,	 Â the	 Â decision	 Â threshold	 Â of	 Â â„!ğ‘‹	 Â can	 Â be	 Â less	 Â than	 Â 0.5,	 Â which	 Â makes	 Â it	 Â more	 Â likely	 Â to	 Â classify	 Â a	 Â test	 Â example	 Â as	 Â positive.	 Â We	 Â set	 Â the	 Â decision	 Â 
threshold	 Â to	 Â be	 Â ğœ‚	 Â where	 Â ğœ‚âˆˆ Â [0,0.5]	 Â and	 Â plot	 Â the	 Â cross	 Â validation	 Â results	 Â when	 Â choosing	 Â different	 Â values	 Â of	 Â ğœ‚,	 Â which	 Â is	 Â shown	 Â in	 Â Fig.	 Â 2.	 Â 

5.1 

Logistic	 Â Regression	 Â 

Fig.	 Â 2	 Â The	 Â relation	 Â between	 Â the	 Â decision	 Â threshold	 Â ğœ‚	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â logistic	 Â regression	 Â model	 Â 
	 Â It	 Â can	 Â be	 Â seen	 Â in	 Â Fig.	 Â 2	 Â that	 Â when	 Â ğœ‚=0.5,	 Â both	 Â accuracy	 Â and	 Â precision	 Â are	 Â high,	 Â but	 Â the	 Â recall	 Â is	 Â not	 Â satisfactory.	 Â When	 Â ğœ‚=0,	 Â we	 Â achieve	 Â the	 Â 
maximum	 Â recall	 Â but	 Â the	 Â accuracy	 Â and	 Â precision	 Â are	 Â very	 Â low.	 Â So	 Â a	 Â trade-Â­â€off	 Â has	 Â to	 Â be	 Â made	 Â between	 Â recall	 Â and	 Â precision/accuracy	 Â by	 Â choosing	 Â an	 Â 
appropriate	 Â value	 Â of	 Â ğœ‚.	 Â For	 Â example,	 Â if	 Â ğœ‚=0.04,	 Â both	 Â accuracy	 Â and	 Â recall	 Â are	 Â high	 Â and	 Â close	 Â to	 Â each	 Â other,	 Â meaning	 Â the	 Â accuracy	 Â for	 Â classifying	 Â all	 Â 
test	 Â examples	 Â and	 Â the	 Â one	 Â for	 Â classifying	 Â positive	 Â test	 Â examples	 Â are	 Â close.	 Â The	 Â low	 Â precision	 Â when	 Â ğœ‚=0.04	 Â is	 Â caused	 Â by	 Â the	 Â increased	 Â number	 Â of	 Â 
false	 Â positives,	 Â which	 Â is	 Â acceptable	 Â to	 Â some	 Â extent	 Â since	 Â false	 Â positives	 Â are	 Â less	 Â important	 Â than	 Â true	 Â positives	 Â in	 Â seizure	 Â prediction.	 Â 
	 Â 

2	 Â 

5.2 

NaÃ¯ve	 Â Bayes	 Â 

The	 Â multinomial	 Â distribution	 Â is	 Â used	 Â to	 Â model	 Â the	 Â features	 Â of	 Â each	 Â iEEG	 Â clip.	 Â Since	 Â the	 Â value	 Â of	 Â each	 Â feature	 Â is	 Â continuous,	 Â we	 Â first	 Â discretize	 Â the	 Â 
values	 Â into	 Â ğ‘	 Â groups	 Â where	 Â ğ‘=!!"#! ,	 Â ğ‘‰!"#	 Â is	 Â the	 Â maximum	 Â value	 Â of	 Â the	 Â features	 Â of	 Â all	 Â clips	 Â and	 Â ğº	 Â is	 Â the	 Â group	 Â size.	 Â Similar	 Â to	 Â the	 Â modification	 Â made	 Â 
to	 Â logistic	 Â regression,	 Â a	 Â test	 Â example	 Â is	 Â labeled	 Â â€œ1â€	 Â if	 Â ğ‘„ğ‘ƒğ‘Œ=1|ğ‘‹ â‰¥ğ‘ƒğ‘Œ=0|ğ‘‹	 Â where	 Â ğ‘„	 Â is	 Â a	 Â positive	 Â constant	 Â specified	 Â to	 Â overcome	 Â the	 Â 
imbalanced	 Â data	 Â when	 Â making	 Â predictions.	 Â Fig.	 Â 3	 Â indicates	 Â that	 Â the	 Â naÃ¯ve	 Â Bayesian	 Â model	 Â cannot	 Â make	 Â satisfactory	 Â predictions	 Â no	 Â matter	 Â what	 Â the	 Â 
value	 Â of	 Â ğ‘„	 Â is.	 Â 

Fig.	 Â 3	 Â The	 Â relation	 Â between	 Â ğ‘„	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â naÃ¯ve	 Â Bayesian	 Â model	 Â 	 Â 

5.3 

Support	 Â Vector	 Â Machine	 Â (SVM)	 Â 

In	 Â the	 Â ğ‘™!-Â­â€regularized	 Â SVM,	 Â the	 Â hypothesis	 Â is	 Â â„!,!ğ‘¥ =ğ‘¤!ğ‘¥+ğ‘	 Â where	 Â parameters	 Â ğ‘¤	 Â and	 Â ğ‘	 Â are	 Â obtained	 Â by	 Â solving	 Â the	 Â primal	 Â optimization	 Â problem	 Â 
whose	 Â objective	 Â is	 Â min!,!!! ğ‘¤ !+ğ¶
	 Â is	 Â the	 Â cost	 Â term.	 Â Since	 Â it	 Â is	 Â more	 Â important	 Â to	 Â correctly	 Â classify	 Â more	 Â positive	 Â test	 Â 
examples	 Â than	 Â to	 Â have	 Â a	 Â few	 Â false	 Â positives,	 Â the	 Â 2-Â­â€cost-Â­â€sensitive	 Â SVM	 Â (2C-Â­â€SVM)	 Â [3]	 Â is	 Â adopted	 Â in	 Â which	 Â two	 Â different	 Â costs	 Â are	 Â assigned	 Â to	 Â negative	 Â 
and	 Â positive	 Â examples,	 Â respectively.	 Â In	 Â 2C-Â­â€SVM,	 Â the	 Â objective	 Â of	 Â the	 Â primal	 Â optimization	 Â problem	 Â is	 Â min!,!!! ğ‘¤ !+ğ¶!
	 Â where	 Â 
ğ¶=ğ¶!+ğ¶!	 Â is	 Â a	 Â trade	 Â off	 Â between	 Â the	 Â classification	 Â margin	 Â and	 Â misclassified	 Â or	 Â non-Â­â€separable	 Â examples	 Â and	 Â the	 Â cost	 Â factor	 Â ğ‘…=!!!!	 Â is	 Â the	 Â ratio	 Â of	 Â 
costs	 Â between	 Â positive	 Â and	 Â negative	 Â examples.	 Â We	 Â employ	 Â the	 Â LIBSVM	 Â library	 Â [4]	 Â as	 Â our	 Â 2C-Â­â€SVM	 Â implementation.	 Â 

ğœ‰!
!âˆˆ!! +ğ¶!

,	 Â where	 Â ğ¶

ğœ‰!!!!!

ğœ‰!!!!!

ğœ‰!

!âˆˆ!!

Fig.	 Â 4	 Â The	 Â relation	 Â between	 Â ğ¶,	 Â ğ‘…	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â 2C-Â­â€SVM	 Â model	 Â 

	 Â Different	 Â values	 Â of	 Â ğ¶>0	 Â and	 Â ğ‘…â‰¥1	 Â are	 Â chosen	 Â and	 Â the	 Â corresponding	 Â results	 Â of	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â are	 Â shown	 Â in	 Â Fig.	 Â 4.	 Â Satisfactory	 Â 
accuracy	 Â and	 Â recall	 Â can	 Â be	 Â achieved	 Â when	 Â (ğ¶,ğ‘…)	 Â is	 Â chosen	 Â as	 Â 10!!,10!,	 Â 10!!,10!	 Â or	 Â 10!!,10!,	 Â etc.	 Â 
We	 Â derived	 Â the	 Â correlation	 Â decision	 Â model	 Â from	 Â a	 Â few	 Â concepts	 Â in	 Â communication	 Â systems.	 Â The	 Â previous	 Â three	 Â models	 Â discard	 Â the	 Â training	 Â set	 Â after	 Â 
a	 Â hypothesis	 Â is	 Â built.	 Â However,	 Â in	 Â correlation	 Â decision,	 Â we	 Â do	 Â not	 Â use	 Â training	 Â examples	 Â to	 Â build	 Â a	 Â hypothesis	 Â and	 Â we	 Â keep	 Â the	 Â entire	 Â training	 Â set.	 Â 

Correlation	 Â Decision	 Â 

5.4 

	 Â 

	 Â 

3	 Â 

Consider	 Â  a	 Â  given	 Â  query	 Â  point	 Â ğ‘‹.	 Â  Calculate	 Â 
its	 Â  correlation	 Â  with	 Â  each	 Â  training	 Â  example	 Â  and	 Â  assign	 Â  a	 Â  score	 Â  to	 Â  each	 Â  of	 Â  them:	 Â 
ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘– =ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘‹,ğ‘‹(!),âˆ€ğ‘–=1,2â€¦ Â ğ‘š.	 Â Find	 Â the	 Â training	 Â example	 Â with	 Â the	 Â maximum	 Â score:	 Â ğ‘–âˆ—=ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ Â ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ Â (ğ‘–).	 Â Since	 Â ğ‘‹(!âˆ—)	 Â is	 Â the	 Â training	 Â 
example	 Â that	 Â the	 Â query	 Â point	 Â is	 Â most	 Â similar	 Â to,	 Â we	 Â can	 Â make	 Â a	 Â prediction	 Â that	 Â ğ‘Œ=ğ‘Œ(!âˆ—).	 Â 
In	 Â order	 Â to	 Â classify	 Â more	 Â positive	 Â examples	 Â correctly,	 Â in	 Â other	 Â words,	 Â to	 Â output	 Â more	 Â positives,	 Â we	 Â increase	 Â the	 Â scores	 Â for	 Â positive	 Â training	 Â 
examples	 Â by	 Â a	 Â factor	 Â ğ›¾â‰¥0,	 Â namely,	 Â ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’âˆ¶= Â ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’Ã—(1+ğ›¾)	 Â for	 Â all	 Â positive	 Â training	 Â examples.	 Â 	 Â 

ACC	 Â 

Recall	 Â 

5.5 

Others	 Â 

0.5Î·=
0.04
Î·=

Decision	 Â 	 Â 
Parameters	 Â 

6  Results	 Â and	 Â Discussion	 Â 

Fig.	 Â 5	 Â The	 Â relation	 Â between	 Â ğ›¾	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â correlation	 Â decision	 Â model	 Â 	 Â 
We	 Â also	 Â tried	 Â extracting	 Â features	 Â using	 Â PCA	 Â and	 Â ICA,	 Â using	 Â different	 Â kernels	 Â (such	 Â as	 Â the	 Â Gaussian	 Â kernel	 Â and	 Â the	 Â sigmoid	 Â kernel)	 Â in	 Â SVMs,	 Â and	 Â 
capturing	 Â non-Â­â€linear	 Â behaviors	 Â of	 Â features	 Â (such	 Â as	 Â logğ‘¥,	 Â  ğ‘¥	 Â and	 Â ğ‘¥!).	 Â But	 Â we	 Â didnâ€™t	 Â achieve	 Â improvement	 Â in	 Â performance.	 Â 
Table	 Â 2	 Â shows	 Â the	 Â results	 Â of	 Â the	 Â models	 Â discussed	 Â above,	 Â where	 Â for	 Â each	 Â model,	 Â the	 Â results	 Â before	 Â and	 Â after	 Â our	 Â modification	 Â to	 Â these	 Â models	 Â are	 Â 
compared.	 Â For	 Â each	 Â modified	 Â model	 Â except	 Â naÃ¯ve	 Â Bayes,	 Â the	 Â decision	 Â parameters	 Â that	 Â achieve	 Â an	 Â acceptable	 Â performance	 Â are	 Â shown.	 Â Among	 Â all	 Â 
chosen	 Â models,	 Â logistic	 Â regression	 Â with	 Â a	 Â threshold	 Â of	 Â 0.04	 Â works	 Â best,	 Â in	 Â whose	 Â results	 Â both	 Â accuracy	 Â and	 Â recall	 Â are	 Â close	 Â to	 Â 90%.	 Â 
	 Â 
67%	 Â 
34%	 Â 
NAN	 Â 
8%	 Â 
55%	 Â 
24%	 Â 
66%	 Â 
28%	 Â 

Logistic	 Â Regression	 Â 
NaÃ¯ve	 Â Bayes	 Â 
2C-Â­â€SVM	 Â 
Correlation	 Â Decision	 Â 
	 Â Fig.	 Â 6	 Â shows	 Â the	 Â ROC	 Â curve	 Â and	 Â the	 Â precision-Â­â€recall	 Â curve	 Â of	 Â each	 Â model.	 Â The	 Â area	 Â under	 Â curve	 Â (AUC)	 Â is	 Â calculated	 Â by	 Â using	 Â the	 Â trapezoidal	 Â areas	 Â 
created	 Â between	 Â each	 Â point	 Â [5].	 Â It	 Â can	 Â be	 Â seen	 Â that	 Â logistic	 Â regression	 Â outperforms	 Â others	 Â since	 Â its	 Â AUCs	 Â for	 Â ROC	 Â and	 Â precision-Â­â€recall	 Â curves	 Â are	 Â the	 Â 
highest.	 Â 	 Â Correlation	 Â decision	 Â model	 Â also	 Â provides	 Â high	 Â AUCs	 Â for	 Â both	 Â kinds	 Â of	 Â curves,	 Â which	 Â indicates	 Â its	 Â performance	 Â is	 Â close	 Â to	 Â that	 Â of	 Â logistic	 Â 
regression.	 Â 	 Â 
Seizure	 Â prediction	 Â is	 Â usually	 Â performed	 Â in	 Â real	 Â time.	 Â Table	 Â 3	 Â gives	 Â the	 Â comparison	 Â of	 Â average	 Â runtime	 Â for	 Â different	 Â models.	 Â The	 Â test	 Â is	 Â performed	 Â on	 Â 
the	 Â same	 Â PC	 Â with	 Â a	 Â 2.2GHz	 Â CPU.	 Â It	 Â can	 Â be	 Â seen	 Â that	 Â logistic	 Â regression	 Â consumes	 Â the	 Â least	 Â time.	 Â Since	 Â it	 Â also	 Â has	 Â the	 Â highest	 Â prediction	 Â performance,	 Â 
it	 Â is	 Â the	 Â most	 Â cost-Â­â€efficient	 Â algorithm	 Â in	 Â this	 Â situation.	 Â Although	 Â the	 Â performance	 Â of	 Â the	 Â correlation	 Â decision	 Â model	 Â is	 Â as	 Â good	 Â as	 Â that	 Â of	 Â logistic	 Â 
	 Â 
4	 Â 

	 Â 
1Q = 	 Â 
Linear	 Â 
Kernel;	 Â 
1C = 	 Â 
1R = 	 Â 
Linear	 Â 
Kernel;	 Â 
	 Â 
	 Â 
0Î³ = 	 Â 

96%	 Â 
88%	 Â 
93%	 Â 
53%	 Â 
94%	 Â 
80%	 Â 
95%	 Â 
85%	 Â 

71%	 Â 
88%	 Â 
0	 Â 
54%	 Â 
67%	 Â 
85%	 Â 
62%	 Â 
85%	 Â 

Precision	 Â 

FPR	 Â 

FNR	 Â 

Error	 Â 

Training	 Â 	 Â 

Test	 Â 	 Â 
Error	 Â 

Table	 Â 2.	 Â Performance	 Â results	 Â of	 Â different	 Â models	 Â 
0.0262	 Â 
0.0046	 Â 
0.0567	 Â 
0.1257	 Â 
0	 Â 
0.0685	 Â 
0.1993	 Â 
0.4697	 Â 
0.0018	 Â 
0.0412	 Â 
0.2034	 Â 
0.0387	 Â 
0.0244	 Â 
-Â­â€	 Â 
0.1521	 Â 
-Â­â€	 Â 

0.0443	 Â 
0.1255	 Â 
0.0691	 Â 
0.4688	 Â 
0.0610	 Â 
0.1994	 Â 
0.0489	 Â 
0.1502	 Â 

0.2871	 Â 
0.1232	 Â 
1	 Â 
0.4565	 Â 
0.3273	 Â 
0.1453	 Â 
0.3780	 Â 
0.1341	 Â 

7010Q =

C
=
R =

âˆ’

410
210

Î³ =

0.0125

Model	 Â 

	 Â 
	 Â 

	 Â 

1  Abstract	 Â 

2 

Introduction	 Â 

Predict	 Â seizures	 Â in	 Â intracranial	 Â EEG	 Â recordings	 Â 
Linyu	 Â He	 Â (linyu90@stanford.edu)	 Â and	 Â Lingbin	 Â Li	 Â (lingbin@stanford.edu)	 Â 

	 Â 
This	 Â project	 Â aims	 Â to	 Â predict	 Â seizures	 Â in	 Â intracranial	 Â electroencephalography	 Â (iEEG)	 Â recordings	 Â using	 Â four	 Â algorithms.	 Â The	 Â data	 Â are	 Â a	 Â series	 Â of	 Â 10-Â­â€
minute	 Â iEEG	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Our	 Â 
goal	 Â is	 Â to	 Â distinguish	 Â between	 Â the	 Â two	 Â states.	 Â The	 Â major	 Â challenge	 Â is	 Â that	 Â the	 Â data	 Â are	 Â highly	 Â imbalanced,	 Â i.e.,	 Â the	 Â number	 Â of	 Â positive	 Â examples	 Â is	 Â less	 Â 
than	 Â 10%	 Â of	 Â that	 Â of	 Â negative	 Â examples.	 Â Our	 Â work	 Â is	 Â to	 Â make	 Â modifications	 Â to	 Â each	 Â of	 Â the	 Â four	 Â models	 Â and	 Â analyze	 Â the	 Â corresponding	 Â performance	 Â 
gain.	 Â 	 Â 
Spontaneous	 Â seizures	 Â are	 Â the	 Â typical	 Â symptom	 Â of	 Â epilepsy,	 Â which	 Â is	 Â a	 Â common	 Â but	 Â refractory	 Â neurological	 Â disorder	 Â that	 Â afflicts	 Â nearly	 Â 1%	 Â of	 Â the	 Â 
worldâ€™s	 Â population.	 Â Anticonvulsant	 Â medications	 Â are	 Â administered	 Â to	 Â many	 Â patients	 Â at	 Â high	 Â doses	 Â to	 Â prevent	 Â seizures,	 Â but	 Â their	 Â effectiveness	 Â is	 Â 
limited	 Â and	 Â patients	 Â often	 Â suffer	 Â their	 Â side	 Â effects.	 Â Even	 Â for	 Â patients	 Â whose	 Â epilepsy-Â­â€causing	 Â brain	 Â tissue	 Â is	 Â removed	 Â via	 Â surgery,	 Â spontaneous	 Â 
seizures	 Â still	 Â persist.	 Â Due	 Â to	 Â the	 Â seemingly	 Â unpredictable	 Â occurrence	 Â of	 Â seizures,	 Â patients	 Â with	 Â epilepsy	 Â experience	 Â constant	 Â anxiety	 Â [1].	 Â 
This	 Â project	 Â aims	 Â to	 Â make	 Â it	 Â possible	 Â that	 Â devices	 Â designed	 Â to	 Â monitor	 Â patientsâ€™	 Â brain	 Â activity	 Â can	 Â warn	 Â them	 Â of	 Â impeding	 Â seizures	 Â so	 Â that	 Â patients	 Â 
are	 Â able	 Â to	 Â take	 Â appropriate	 Â precautions.	 Â It	 Â is	 Â also	 Â helpful	 Â to	 Â reduce	 Â overall	 Â side	 Â effects	 Â caused	 Â by	 Â anticonvulsant	 Â medications	 Â taken	 Â by	 Â these	 Â 
patients.	 Â By	 Â providing	 Â them	 Â with	 Â devices	 Â with	 Â the	 Â ability	 Â to	 Â predict	 Â an	 Â impending	 Â seizure,	 Â anticonvulsant	 Â medications	 Â could	 Â be	 Â administered	 Â only	 Â 
when	 Â necessary,	 Â thus	 Â lowering	 Â the	 Â doses	 Â given	 Â to	 Â patients.	 Â 
Multiple	 Â researches	 Â support	 Â the	 Â notion	 Â that	 Â the	 Â occurrence	 Â of	 Â seizures	 Â is	 Â not	 Â random.	 Â According	 Â to	 Â evidence	 Â shown	 Â by	 Â related	 Â researches,	 Â for	 Â 
patients	 Â with	 Â epilepsy,	 Â the	 Â temporal	 Â dynamics	 Â of	 Â brain	 Â activity	 Â can	 Â be	 Â classified	 Â into	 Â 4	 Â states:	 Â interictal	 Â (between	 Â seizures),	 Â preictal	 Â (prior	 Â to	 Â 
seizures),	 Â ictal	 Â (seizure)	 Â and	 Â postictal	 Â (after	 Â seizures).	 Â The	 Â brain	 Â activity	 Â of	 Â each	 Â state	 Â can	 Â be	 Â recorded	 Â by	 Â iEEG	 Â [1].	 Â Our	 Â goal	 Â is	 Â to	 Â employ	 Â machine	 Â 
learning	 Â techniques	 Â to	 Â learn	 Â from	 Â iEEG	 Â data	 Â the	 Â characteristics	 Â of	 Â preictal	 Â states	 Â and	 Â then	 Â distinguish	 Â these	 Â states	 Â from	 Â the	 Â interictal	 Â states.	 Â After	 Â 
one	 Â preictal	 Â state	 Â is	 Â identified,	 Â a	 Â warning	 Â should	 Â be	 Â sent	 Â to	 Â the	 Â patient	 Â to	 Â prepare	 Â him	 Â or	 Â her	 Â for	 Â an	 Â impending	 Â seizure.	 Â 
Kaggle	 Â provides	 Â iEEG	 Â data	 Â collected	 Â from	 Â canine	 Â subjects.	 Â The	 Â data	 Â of	 Â each	 Â subject	 Â is	 Â organized	 Â into	 Â 10-Â­â€minute	 Â clips	 Â labeled	 Â â€œpreictal	 Â (positive)â€	 Â for	 Â 
data	 Â recorded	 Â prior	 Â to	 Â seizures	 Â or	 Â â€œinterictal	 Â (negative)â€	 Â for	 Â data	 Â recorded	 Â between	 Â seizures.	 Â Each	 Â clip	 Â contains	 Â 16	 Â channels	 Â of	 Â iEEG	 Â data	 Â where	 Â 
each	 Â channel	 Â corresponds	 Â to	 Â one	 Â electrode	 Â implanted	 Â in	 Â the	 Â subjectâ€™s	 Â brain.	 Â For	 Â each	 Â training	 Â example	 Â ğ‘¥(!),ğ‘¦(!),	 Â ğ‘¦(!)	 Â is	 Â the	 Â label	 Â and	 Â ğ‘¥(!)	 Â is	 Â a	 Â clip	 Â in	 Â 
which	 Â each	 Â row	 Â corresponds	 Â to	 Â one	 Â channel	 Â and	 Â each	 Â column	 Â corresponds	 Â to	 Â iEEG	 Â readings	 Â at	 Â one	 Â sampling	 Â time	 Â point.	 Â 
Since	 Â seizures	 Â in	 Â most	 Â patients	 Â are	 Â associated	 Â with	 Â a	 Â stereotypic	 Â EEG	 Â discharge	 Â with	 Â characteristic	 Â spectral	 Â pattern,	 Â we	 Â employed	 Â the	 Â following	 Â 
feature	 Â extraction	 Â procedure	 Â [2]:	 Â 
Apply	 Â fast	 Â Fourier	 Â transform	 Â to	 Â each	 Â channel	 Â in	 Â a	 Â clip	 Â and	 Â divide	 Â the	 Â resulting	 Â power	 Â spectrum	 Â into	 Â 6	 Â bands:	 Â delta	 Â (0.1	 Â â€“	 Â 4	 Â Hz),	 Â theta	 Â (4	 Â â€“	 Â 8	 Â Hz),	 Â 
alpha	 Â (8	 Â â€“	 Â 12	 Â Hz),	 Â beta	 Â (12	 Â â€“	 Â 30	 Â Hz),	 Â low-Â­â€gamma	 Â (30	 Â â€“	 Â 70	 Â Hz),	 Â and	 Â high-Â­â€gamma	 Â (70	 Â â€“	 Â 180	 Â Hz).	 Â In	 Â each	 Â band,	 Â sum	 Â the	 Â power	 Â over	 Â all	 Â band	 Â 
frequencies	 Â to	 Â produce	 Â a	 Â power-Â­â€in-Â­â€band	 Â (PIB)	 Â feature.	 Â Therefore,	 Â 6	 Â features	 Â are	 Â obtained	 Â in	 Â each	 Â channel	 Â and	 Â 96	 Â features	 Â are	 Â obtained	 Â in	 Â one	 Â clip.	 Â 
The	 Â procedure	 Â above	 Â is	 Â also	 Â illustrated	 Â in	 Â Fig	 Â 1,	 Â where	 Â ğ‘(ğ‘“)	 Â is	 Â the	 Â power	 Â spectrum.	 Â 
Channel	 Â 1	 Â 
Channel	 Â 2	 Â 
â€¦	 Â â€¦	 Â â€¦	 Â Channel	 Â 16	 Â 
There	 Â are	 Â 3939	 Â examples	 Â in	 Â total,	 Â in	 Â which	 Â 3674	 Â are	 Â negative	 Â and	 Â 265	 Â are	 Â positive.	 Â For	 Â each	 Â time	 Â of	 Â cross	 Â validation,	 Â we	 Â randomly	 Â pick	 Â 70%	 Â of	 Â the	 Â 
negative	 Â examples	 Â and	 Â 70%	 Â of	 Â the	 Â positive	 Â examples	 Â for	 Â training	 Â and	 Â use	 Â the	 Â rest	 Â for	 Â testing.	 Â This	 Â process	 Â is	 Â repeated	 Â for	 Â 100	 Â times	 Â to	 Â calculate	 Â the	 Â 
average	 Â evaluation.	 Â 
	 Â 
1	 Â 

delta	 Â (0.1	 Â â€“	 Â 4Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!.!
theta	 Â (4	 Â â€“	 Â 8Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!!!!
alpha	 Â (8	 Â â€“	 Â 12Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!
beta	 Â (12	 Â â€“	 Â 30Hz):	 Â âˆ‘
	 Â 
ğ‘(ğ‘“)
!"!!!"
low-Â­â€gamma	 Â (30	 Â â€“	 Â 70Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"!!!"
high-Â­â€gamma	 Â (70	 Â â€“	 Â 180Hz):	 Â âˆ‘
ğ‘(ğ‘“)
	 Â 
!"#!!!"

Fig.	 Â 1	 Â Feature	 Â Extraction	 Â using	 Â FFT	 Â 

3  Data	 Â and	 Â Feature	 Â Extraction	 Â 

FFT	 Â 

4  Cross	 Â Validation	 Â 

Name	 Â 

Definition	 Â 

5 

Besides	 Â the	 Â training	 Â error	 Â and	 Â the	 Â testing	 Â error,	 Â the	 Â following	 Â values	 Â are	 Â adopted	 Â to	 Â evaluate	 Â the	 Â performance	 Â of	 Â each	 Â model	 Â since	 Â the	 Â data	 Â are	 Â 
highly	 Â imbalanced.	 Â 
	 Â 

Table	 Â 1.	 Â Values	 Â chosen	 Â to	 Â evaluate	 Â the	 Â performance	 Â 

Learning	 Â Algorithms	 Â 

Accuracy	 Â (ACC)	 Â 
Positive	 Â Predictive	 Â Rate(PPV)/Precision	 Â 
True	 Â Positive	 Â Rate	 Â (TPR)/Recall	 Â 
False	 Â Negative	 Â Rate	 Â (FNR)/Miss	 Â Rate	 Â 
False	 Â Positive	 Â Rate	 Â (FPR)/Fall-Â­â€out	 Â 

ğ‘‡ğ‘ƒ+ğ‘‡ğ‘
ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘	 Â 
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ	 Â 
ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘	 Â 
ğ¹ğ‘ğ‘‡ğ‘ƒ+ğ¹ğ‘	 Â 
ğ¹ğ‘ƒ
ğ¹ğ‘ƒ+ğ‘‡ğ‘	 Â 
	 Â In	 Â Table	 Â 1,	 Â ğ‘‡ğ‘ƒ,	 Â ğ‘‡ğ‘,	 Â ğ¹ğ‘ƒ	 Â and	 Â ğ¹ğ‘	 Â are	 Â the	 Â number	 Â of	 Â true	 Â positives,	 Â true	 Â negatives,	 Â false	 Â positives	 Â and	 Â false	 Â negatives,	 Â respectively.	 Â Finally,	 Â the	 Â receiver	 Â 
operating	 Â characteristic	 Â (ROC)	 Â curves	 Â and	 Â precision-Â­â€recall	 Â curves	 Â will	 Â be	 Â plotted	 Â based	 Â on	 Â values	 Â in	 Â the	 Â table	 Â above.	 Â 
In	 Â out	 Â attempt	 Â to	 Â seek	 Â a	 Â solution,	 Â three	 Â models	 Â covered	 Â in	 Â CS	 Â 229	 Â were	 Â first	 Â adopted,	 Â which	 Â are	 Â logistic	 Â regression,	 Â naÃ¯ve	 Â Bayes	 Â classifiers	 Â and	 Â 
support	 Â vector	 Â machines	 Â (SVMs).	 Â Later,	 Â we	 Â employed	 Â a	 Â model	 Â inherited	 Â from	 Â communication	 Â systems,	 Â which	 Â makes	 Â a	 Â prediction	 Â based	 Â on	 Â 
correlation	 Â coefficients	 Â between	 Â the	 Â test	 Â example	 Â and	 Â all	 Â training	 Â examples.	 Â Modifications	 Â are	 Â made	 Â to	 Â each	 Â model	 Â to	 Â improve	 Â their	 Â performance	 Â on	 Â 
an	 Â imbalanced	 Â data	 Â set.	 Â In	 Â the	 Â following	 Â discussion,	 Â we	 Â use	 Â ğ‘¥(!),ğ‘¦(!)	 Â to	 Â denote	 Â each	 Â training	 Â example	 Â where	 Â ğ‘¥(!)âˆˆğ‘…!"	 Â is	 Â the	 Â set	 Â of	 Â features	 Â and	 Â 
ğ‘¦(!)âˆˆ 0,1	 Â is	 Â a	 Â label	 Â (0	 Â corresponds	 Â to	 Â a	 Â negative	 Â label	 Â and	 Â 1	 Â corresponds	 Â to	 Â a	 Â positive	 Â label.	 Â In	 Â SVMs,	 Â ğ‘¦(!)âˆˆ âˆ’1,1	 Â where	 Â -Â­â€1	 Â corresponds	 Â to	 Â a	 Â 
negative	 Â label	 Â and	 Â 1	 Â corresponds	 Â to	 Â a	 Â positive	 Â label).	 Â We	 Â use	 Â ğ‘‹	 Â to	 Â denote	 Â a	 Â query	 Â point	 Â (test	 Â example)	 Â and	 Â ğ‘Œ	 Â to	 Â denote	 Â the	 Â label	 Â predicted	 Â by	 Â a	 Â 
model.	 Â 
!!!!!!!	 Â where	 Â ğœƒ	 Â is	 Â the	 Â parameter.	 Â The	 Â probability	 Â of	 Â ğ‘Œ=1	 Â conditioned	 Â on	 Â ğ‘‹	 Â and	 Â parameterized	 Â by	 Â 
In	 Â logistic	 Â regression,	 Â the	 Â hypothesis	 Â is	 Â â„!ğ‘¥ =
!
ğœƒ	 Â is	 Â ğ‘ƒğ‘Œ=1|ğ‘‹,ğœƒ =â„!ğ‘‹.	 Â 
Usually,	 Â the	 Â prediction	 Â that	 Â ğ‘Œ=1	 Â is	 Â made	 Â if	 Â â„!ğ‘‹ â‰¥0.5.	 Â Since	 Â the	 Â data	 Â set	 Â is	 Â highly	 Â imbalanced,	 Â i.e.,	 Â the	 Â number	 Â of	 Â negative	 Â examples	 Â is	 Â much	 Â 
larger	 Â than	 Â that	 Â of	 Â positive	 Â ones,	 Â we	 Â consider	 Â it	 Â more	 Â important	 Â to	 Â correctly	 Â classify	 Â more	 Â positive	 Â test	 Â examples	 Â than	 Â to	 Â have	 Â a	 Â few	 Â false	 Â positives.	 Â 
Therefore,	 Â the	 Â decision	 Â threshold	 Â of	 Â â„!ğ‘‹	 Â can	 Â be	 Â less	 Â than	 Â 0.5,	 Â which	 Â makes	 Â it	 Â more	 Â likely	 Â to	 Â classify	 Â a	 Â test	 Â example	 Â as	 Â positive.	 Â We	 Â set	 Â the	 Â decision	 Â 
threshold	 Â to	 Â be	 Â ğœ‚	 Â where	 Â ğœ‚âˆˆ Â [0,0.5]	 Â and	 Â plot	 Â the	 Â cross	 Â validation	 Â results	 Â when	 Â choosing	 Â different	 Â values	 Â of	 Â ğœ‚,	 Â which	 Â is	 Â shown	 Â in	 Â Fig.	 Â 2.	 Â 

5.1 

Logistic	 Â Regression	 Â 

Fig.	 Â 2	 Â The	 Â relation	 Â between	 Â the	 Â decision	 Â threshold	 Â ğœ‚	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â logistic	 Â regression	 Â model	 Â 
	 Â It	 Â can	 Â be	 Â seen	 Â in	 Â Fig.	 Â 2	 Â that	 Â when	 Â ğœ‚=0.5,	 Â both	 Â accuracy	 Â and	 Â precision	 Â are	 Â high,	 Â but	 Â the	 Â recall	 Â is	 Â not	 Â satisfactory.	 Â When	 Â ğœ‚=0,	 Â we	 Â achieve	 Â the	 Â 
maximum	 Â recall	 Â but	 Â the	 Â accuracy	 Â and	 Â precision	 Â are	 Â very	 Â low.	 Â So	 Â a	 Â trade-Â­â€off	 Â has	 Â to	 Â be	 Â made	 Â between	 Â recall	 Â and	 Â precision/accuracy	 Â by	 Â choosing	 Â an	 Â 
appropriate	 Â value	 Â of	 Â ğœ‚.	 Â For	 Â example,	 Â if	 Â ğœ‚=0.04,	 Â both	 Â accuracy	 Â and	 Â recall	 Â are	 Â high	 Â and	 Â close	 Â to	 Â each	 Â other,	 Â meaning	 Â the	 Â accuracy	 Â for	 Â classifying	 Â all	 Â 
test	 Â examples	 Â and	 Â the	 Â one	 Â for	 Â classifying	 Â positive	 Â test	 Â examples	 Â are	 Â close.	 Â The	 Â low	 Â precision	 Â when	 Â ğœ‚=0.04	 Â is	 Â caused	 Â by	 Â the	 Â increased	 Â number	 Â of	 Â 
false	 Â positives,	 Â which	 Â is	 Â acceptable	 Â to	 Â some	 Â extent	 Â since	 Â false	 Â positives	 Â are	 Â less	 Â important	 Â than	 Â true	 Â positives	 Â in	 Â seizure	 Â prediction.	 Â 
	 Â 

2	 Â 

5.2 

NaÃ¯ve	 Â Bayes	 Â 

The	 Â multinomial	 Â distribution	 Â is	 Â used	 Â to	 Â model	 Â the	 Â features	 Â of	 Â each	 Â iEEG	 Â clip.	 Â Since	 Â the	 Â value	 Â of	 Â each	 Â feature	 Â is	 Â continuous,	 Â we	 Â first	 Â discretize	 Â the	 Â 
values	 Â into	 Â ğ‘	 Â groups	 Â where	 Â ğ‘=!!"#! ,	 Â ğ‘‰!"#	 Â is	 Â the	 Â maximum	 Â value	 Â of	 Â the	 Â features	 Â of	 Â all	 Â clips	 Â and	 Â ğº	 Â is	 Â the	 Â group	 Â size.	 Â Similar	 Â to	 Â the	 Â modification	 Â made	 Â 
to	 Â logistic	 Â regression,	 Â a	 Â test	 Â example	 Â is	 Â labeled	 Â â€œ1â€	 Â if	 Â ğ‘„ğ‘ƒğ‘Œ=1|ğ‘‹ â‰¥ğ‘ƒğ‘Œ=0|ğ‘‹	 Â where	 Â ğ‘„	 Â is	 Â a	 Â positive	 Â constant	 Â specified	 Â to	 Â overcome	 Â the	 Â 
imbalanced	 Â data	 Â when	 Â making	 Â predictions.	 Â Fig.	 Â 3	 Â indicates	 Â that	 Â the	 Â naÃ¯ve	 Â Bayesian	 Â model	 Â cannot	 Â make	 Â satisfactory	 Â predictions	 Â no	 Â matter	 Â what	 Â the	 Â 
value	 Â of	 Â ğ‘„	 Â is.	 Â 

Fig.	 Â 3	 Â The	 Â relation	 Â between	 Â ğ‘„	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â naÃ¯ve	 Â Bayesian	 Â model	 Â 	 Â 

5.3 

Support	 Â Vector	 Â Machine	 Â (SVM)	 Â 

In	 Â the	 Â ğ‘™!-Â­â€regularized	 Â SVM,	 Â the	 Â hypothesis	 Â is	 Â â„!,!ğ‘¥ =ğ‘¤!ğ‘¥+ğ‘	 Â where	 Â parameters	 Â ğ‘¤	 Â and	 Â ğ‘	 Â are	 Â obtained	 Â by	 Â solving	 Â the	 Â primal	 Â optimization	 Â problem	 Â 
whose	 Â objective	 Â is	 Â min!,!!! ğ‘¤ !+ğ¶
	 Â is	 Â the	 Â cost	 Â term.	 Â Since	 Â it	 Â is	 Â more	 Â important	 Â to	 Â correctly	 Â classify	 Â more	 Â positive	 Â test	 Â 
examples	 Â than	 Â to	 Â have	 Â a	 Â few	 Â false	 Â positives,	 Â the	 Â 2-Â­â€cost-Â­â€sensitive	 Â SVM	 Â (2C-Â­â€SVM)	 Â [3]	 Â is	 Â adopted	 Â in	 Â which	 Â two	 Â different	 Â costs	 Â are	 Â assigned	 Â to	 Â negative	 Â 
and	 Â positive	 Â examples,	 Â respectively.	 Â In	 Â 2C-Â­â€SVM,	 Â the	 Â objective	 Â of	 Â the	 Â primal	 Â optimization	 Â problem	 Â is	 Â min!,!!! ğ‘¤ !+ğ¶!
	 Â where	 Â 
ğ¶=ğ¶!+ğ¶!	 Â is	 Â a	 Â trade	 Â off	 Â between	 Â the	 Â classification	 Â margin	 Â and	 Â misclassified	 Â or	 Â non-Â­â€separable	 Â examples	 Â and	 Â the	 Â cost	 Â factor	 Â ğ‘…=!!!!	 Â is	 Â the	 Â ratio	 Â of	 Â 
costs	 Â between	 Â positive	 Â and	 Â negative	 Â examples.	 Â We	 Â employ	 Â the	 Â LIBSVM	 Â library	 Â [4]	 Â as	 Â our	 Â 2C-Â­â€SVM	 Â implementation.	 Â 

ğœ‰!
!âˆˆ!! +ğ¶!

,	 Â where	 Â ğ¶

ğœ‰!!!!!

ğœ‰!!!!!

ğœ‰!

!âˆˆ!!

Fig.	 Â 4	 Â The	 Â relation	 Â between	 Â ğ¶,	 Â ğ‘…	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â 2C-Â­â€SVM	 Â model	 Â 

	 Â Different	 Â values	 Â of	 Â ğ¶>0	 Â and	 Â ğ‘…â‰¥1	 Â are	 Â chosen	 Â and	 Â the	 Â corresponding	 Â results	 Â of	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â are	 Â shown	 Â in	 Â Fig.	 Â 4.	 Â Satisfactory	 Â 
accuracy	 Â and	 Â recall	 Â can	 Â be	 Â achieved	 Â when	 Â (ğ¶,ğ‘…)	 Â is	 Â chosen	 Â as	 Â 10!!,10!,	 Â 10!!,10!	 Â or	 Â 10!!,10!,	 Â etc.	 Â 
We	 Â derived	 Â the	 Â correlation	 Â decision	 Â model	 Â from	 Â a	 Â few	 Â concepts	 Â in	 Â communication	 Â systems.	 Â The	 Â previous	 Â three	 Â models	 Â discard	 Â the	 Â training	 Â set	 Â after	 Â 
a	 Â hypothesis	 Â is	 Â built.	 Â However,	 Â in	 Â correlation	 Â decision,	 Â we	 Â do	 Â not	 Â use	 Â training	 Â examples	 Â to	 Â build	 Â a	 Â hypothesis	 Â and	 Â we	 Â keep	 Â the	 Â entire	 Â training	 Â set.	 Â 

Correlation	 Â Decision	 Â 

5.4 

	 Â 

	 Â 

3	 Â 

Consider	 Â  a	 Â  given	 Â  query	 Â  point	 Â ğ‘‹.	 Â  Calculate	 Â 
its	 Â  correlation	 Â  with	 Â  each	 Â  training	 Â  example	 Â  and	 Â  assign	 Â  a	 Â  score	 Â  to	 Â  each	 Â  of	 Â  them:	 Â 
ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘– =ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘‹,ğ‘‹(!),âˆ€ğ‘–=1,2â€¦ Â ğ‘š.	 Â Find	 Â the	 Â training	 Â example	 Â with	 Â the	 Â maximum	 Â score:	 Â ğ‘–âˆ—=ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ Â ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ Â (ğ‘–).	 Â Since	 Â ğ‘‹(!âˆ—)	 Â is	 Â the	 Â training	 Â 
example	 Â that	 Â the	 Â query	 Â point	 Â is	 Â most	 Â similar	 Â to,	 Â we	 Â can	 Â make	 Â a	 Â prediction	 Â that	 Â ğ‘Œ=ğ‘Œ(!âˆ—).	 Â 
In	 Â order	 Â to	 Â classify	 Â more	 Â positive	 Â examples	 Â correctly,	 Â in	 Â other	 Â words,	 Â to	 Â output	 Â more	 Â positives,	 Â we	 Â increase	 Â the	 Â scores	 Â for	 Â positive	 Â training	 Â 
examples	 Â by	 Â a	 Â factor	 Â ğ›¾â‰¥0,	 Â namely,	 Â ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’âˆ¶= Â ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’Ã—(1+ğ›¾)	 Â for	 Â all	 Â positive	 Â training	 Â examples.	 Â 	 Â 

ACC	 Â 

Recall	 Â 

5.5 

Others	 Â 

0.5Î·=
0.04
Î·=

Decision	 Â 	 Â 
Parameters	 Â 

6  Results	 Â and	 Â Discussion	 Â 

Fig.	 Â 5	 Â The	 Â relation	 Â between	 Â ğ›¾	 Â and	 Â accuracy,	 Â recall	 Â and	 Â precision	 Â of	 Â the	 Â correlation	 Â decision	 Â model	 Â 	 Â 
We	 Â also	 Â tried	 Â extracting	 Â features	 Â using	 Â PCA	 Â and	 Â ICA,	 Â using	 Â different	 Â kernels	 Â (such	 Â as	 Â the	 Â Gaussian	 Â kernel	 Â and	 Â the	 Â sigmoid	 Â kernel)	 Â in	 Â SVMs,	 Â and	 Â 
capturing	 Â non-Â­â€linear	 Â behaviors	 Â of	 Â features	 Â (such	 Â as	 Â logğ‘¥,	 Â  ğ‘¥	 Â and	 Â ğ‘¥!).	 Â But	 Â we	 Â didnâ€™t	 Â achieve	 Â improvement	 Â in	 Â performance.	 Â 
Table	 Â 2	 Â shows	 Â the	 Â results	 Â of	 Â the	 Â models	 Â discussed	 Â above,	 Â where	 Â for	 Â each	 Â model,	 Â the	 Â results	 Â before	 Â and	 Â after	 Â our	 Â modification	 Â to	 Â these	 Â models	 Â are	 Â 
compared.	 Â For	 Â each	 Â modified	 Â model	 Â except	 Â naÃ¯ve	 Â Bayes,	 Â the	 Â decision	 Â parameters	 Â that	 Â achieve	 Â an	 Â acceptable	 Â performance	 Â are	 Â shown.	 Â Among	 Â all	 Â 
chosen	 Â models,	 Â logistic	 Â regression	 Â with	 Â a	 Â threshold	 Â of	 Â 0.04	 Â works	 Â best,	 Â in	 Â whose	 Â results	 Â both	 Â accuracy	 Â and	 Â recall	 Â are	 Â close	 Â to	 Â 90%.	 Â 
	 Â 
67%	 Â 
34%	 Â 
NAN	 Â 
8%	 Â 
55%	 Â 
24%	 Â 
66%	 Â 
28%	 Â 

Logistic	 Â Regression	 Â 
NaÃ¯ve	 Â Bayes	 Â 
2C-Â­â€SVM	 Â 
Correlation	 Â Decision	 Â 
	 Â Fig.	 Â 6	 Â shows	 Â the	 Â ROC	 Â curve	 Â and	 Â the	 Â precision-Â­â€recall	 Â curve	 Â of	 Â each	 Â model.	 Â The	 Â area	 Â under	 Â curve	 Â (AUC)	 Â is	 Â calculated	 Â by	 Â using	 Â the	 Â trapezoidal	 Â areas	 Â 
created	 Â between	 Â each	 Â point	 Â [5].	 Â It	 Â can	 Â be	 Â seen	 Â that	 Â logistic	 Â regression	 Â outperforms	 Â others	 Â since	 Â its	 Â AUCs	 Â for	 Â ROC	 Â and	 Â precision-Â­â€recall	 Â curves	 Â are	 Â the	 Â 
highest.	 Â 	 Â Correlation	 Â decision	 Â model	 Â also	 Â provides	 Â high	 Â AUCs	 Â for	 Â both	 Â kinds	 Â of	 Â curves,	 Â which	 Â indicates	 Â its	 Â performance	 Â is	 Â close	 Â to	 Â that	 Â of	 Â logistic	 Â 
regression.	 Â 	 Â 
Seizure	 Â prediction	 Â is	 Â usually	 Â performed	 Â in	 Â real	 Â time.	 Â Table	 Â 3	 Â gives	 Â the	 Â comparison	 Â of	 Â average	 Â runtime	 Â for	 Â different	 Â models.	 Â The	 Â test	 Â is	 Â performed	 Â on	 Â 
the	 Â same	 Â PC	 Â with	 Â a	 Â 2.2GHz	 Â CPU.	 Â It	 Â can	 Â be	 Â seen	 Â that	 Â logistic	 Â regression	 Â consumes	 Â the	 Â least	 Â time.	 Â Since	 Â it	 Â also	 Â has	 Â the	 Â highest	 Â prediction	 Â performance,	 Â 
it	 Â is	 Â the	 Â most	 Â cost-Â­â€efficient	 Â algorithm	 Â in	 Â this	 Â situation.	 Â Although	 Â the	 Â performance	 Â of	 Â the	 Â correlation	 Â decision	 Â model	 Â is	 Â as	 Â good	 Â as	 Â that	 Â of	 Â logistic	 Â 
	 Â 
4	 Â 

	 Â 
1Q = 	 Â 
Linear	 Â 
Kernel;	 Â 
1C = 	 Â 
1R = 	 Â 
Linear	 Â 
Kernel;	 Â 
	 Â 
	 Â 
0Î³ = 	 Â 

96%	 Â 
88%	 Â 
93%	 Â 
53%	 Â 
94%	 Â 
80%	 Â 
95%	 Â 
85%	 Â 

71%	 Â 
88%	 Â 
0	 Â 
54%	 Â 
67%	 Â 
85%	 Â 
62%	 Â 
85%	 Â 

Precision	 Â 

FPR	 Â 

FNR	 Â 

Error	 Â 

Training	 Â 	 Â 

Test	 Â 	 Â 
Error	 Â 

Table	 Â 2.	 Â Performance	 Â results	 Â of	 Â different	 Â models	 Â 
0.0262	 Â 
0.0046	 Â 
0.0567	 Â 
0.1257	 Â 
0	 Â 
0.0685	 Â 
0.1993	 Â 
0.4697	 Â 
0.0018	 Â 
0.0412	 Â 
0.2034	 Â 
0.0387	 Â 
0.0244	 Â 
-Â­â€	 Â 
0.1521	 Â 
-Â­â€	 Â 

0.0443	 Â 
0.1255	 Â 
0.0691	 Â 
0.4688	 Â 
0.0610	 Â 
0.1994	 Â 
0.0489	 Â 
0.1502	 Â 

0.2871	 Â 
0.1232	 Â 
1	 Â 
0.4565	 Â 
0.3273	 Â 
0.1453	 Â 
0.3780	 Â 
0.1341	 Â 

7010Q =

C
=
R =

âˆ’

410
210

Î³ =

0.0125

Model	 Â 

	 Â 
	 Â 

	 Â 

regression,	 Â it	 Â runs	 Â much	 Â more	 Â slowly,	 Â since	 Â it	 Â has	 Â to	 Â keep	 Â track	 Â of	 Â all	 Â training	 Â examples	 Â during	 Â the	 Â prediction	 Â process.	 Â Therefore,	 Â we	 Â consider	 Â 
logistic	 Â regression	 Â with	 Â threshold	 Â modification	 Â as	 Â the	 Â best	 Â model	 Â in	 Â this	 Â project.	 Â 

Model	 Â 

Î·=

0.04

C

410

âˆ’

=

R =

210

Runtime	 Â (in	 Â seconds)	 Â 

NaÃ¯ve	 Â Bayes	 Â 
(
)	 Â 
0.488759	 Â 

7010Q =

7  Conclusions	 Â and	 Â Future	 Â Work	 Â 

Fig.	 Â 6	 Â 	 Â The	 Â ROC	 Â curve	 Â and	 Â precision-Â­â€recall	 Â curve	 Â of	 Â each	 Â model	 Â 
2C-Â­â€SVM	 Â 
Logistic	 Â Regression	 Â 
(
)	 Â 
,
0.267448	 Â 
0.432028	 Â 

	 Â 
	 Â 
Table	 Â 3.	 Â The	 Â average	 Â runtime	 Â of	 Â different	 Â models	 Â (#	 Â of	 Â training	 Â examples	 Â :	 Â 2758,	 Â #	 Â of	 Â test	 Â examples	 Â :	 Â 1182)	 Â 
Correlation	 Â Decision	 Â 
(
)	 Â 
12.8690	 Â 
	 Â The	 Â major	 Â challenge	 Â of	 Â this	 Â project	 Â is	 Â the	 Â imbalanced	 Â data.	 Â What	 Â weâ€™ve	 Â done	 Â so	 Â far	 Â is	 Â to	 Â sacrifice	 Â false	 Â positive	 Â rate	 Â to	 Â achieve	 Â a	 Â low	 Â false	 Â negative	 Â 
rate	 Â because	 Â a	 Â false	 Â negative	 Â is	 Â far	 Â more	 Â dangerous	 Â than	 Â a	 Â false	 Â positive	 Â in	 Â seizure	 Â prediction.	 Â The	 Â current	 Â results	 Â are	 Â within	 Â our	 Â expectations	 Â but	 Â 
they	 Â are	 Â not	 Â good	 Â enough	 Â since	 Â we	 Â believe	 Â the	 Â information	 Â extracted	 Â from	 Â the	 Â limited	 Â number	 Â of	 Â positive	 Â examples	 Â is	 Â not	 Â enough	 Â to	 Â perfectly	 Â 
distinguish	 Â between	 Â the	 Â two	 Â classes.	 Â 
Three	 Â supervised	 Â learning	 Â models	 Â covered	 Â in	 Â CS229	 Â and	 Â one	 Â model	 Â inherited	 Â from	 Â communication	 Â systems	 Â are	 Â employed	 Â in	 Â this	 Â project	 Â to	 Â predict	 Â 
the	 Â occurrence	 Â of	 Â seizures.	 Â Modifications	 Â are	 Â made	 Â to	 Â these	 Â models	 Â to	 Â deal	 Â with	 Â the	 Â highly	 Â imbalanced	 Â data.	 Â Among	 Â the	 Â four	 Â models,	 Â logistic	 Â 
regression	 Â outperforms	 Â others,	 Â which	 Â obtains	 Â the	 Â highest	 Â AUCs	 Â for	 Â ROC	 Â and	 Â precision-Â­â€recall	 Â curves.	 Â Meanwhile,	 Â when	 Â choosing	 Â the	 Â decision	 Â 
parameter	 Â properly,	 Â both	 Â accuracy	 Â and	 Â recall	 Â of	 Â logistic	 Â regression	 Â are	 Â close	 Â to	 Â 90%.	 Â 
An	 Â important	 Â method	 Â to	 Â deal	 Â with	 Â imbalanced	 Â bits	 Â â€œ0â€	 Â and	 Â â€œ1â€	 Â in	 Â wireless	 Â communication	 Â is	 Â to	 Â code	 Â â€œ0â€	 Â into	 Â â€œ01â€	 Â and	 Â â€œ1â€	 Â into	 Â â€œ10â€	 Â so	 Â that	 Â the	 Â 
numbers	 Â of	 Â both	 Â classes	 Â become	 Â balanced.	 Â Weâ€™ve	 Â been	 Â trying	 Â to	 Â apply	 Â similar	 Â ideas	 Â to	 Â the	 Â project,	 Â but	 Â have	 Â yet	 Â got	 Â a	 Â satisfying	 Â result.	 Â So	 Â the	 Â 
exploration	 Â will	 Â be	 Â continued	 Â to	 Â seek	 Â better	 Â solutions.	 Â 
Also,	 Â instead	 Â of	 Â using	 Â a	 Â single	 Â model	 Â to	 Â build	 Â a	 Â classifier,	 Â attempts	 Â can	 Â be	 Â made	 Â to	 Â combine	 Â the	 Â predictions	 Â of	 Â different	 Â models	 Â and	 Â develop	 Â 
strategies	 Â to	 Â make	 Â a	 Â final	 Â decision.	 Â Models	 Â involved	 Â in	 Â the	 Â combination	 Â may	 Â differ	 Â in	 Â their	 Â feature	 Â extraction	 Â process	 Â since	 Â it	 Â is	 Â possible	 Â to	 Â develop	 Â 
for	 Â each	 Â model	 Â the	 Â features	 Â that	 Â best	 Â fit	 Â the	 Â model.	 Â 
[1]	 Â Kaggle	 Â Inc.	 Â (2014)	 Â Kaggle:	 Â The	 Â Home	 Â of	 Â Data	 Â Science.	 Â [Online].	 Â http://www.kaggle.com/c/seizure-Â­â€prediction	 Â 
[2]	 Â J.	 Â Jeffry	 Â Howbert	 Â et	 Â al.,	 Â "Forecasting	 Â seizures	 Â in	 Â dogs	 Â with	 Â naturally	 Â occurring	 Â epilepsy,"	 Â PloS	 Â one,	 Â vol.	 Â 9,	 Â no.	 Â 1,	 Â p.	 Â e81920,	 Â 2014.	 Â 
[3]	 Â Yun	 Â Park,	 Â Lan	 Â Luo,	 Â Keshab	 Â K.	 Â Parhi,	 Â and	 Â Theoden	 Â Netoff,	 Â "Seizure	 Â prediction	 Â with	 Â spectral	 Â power	 Â of	 Â EEG	 Â using	 Â cost-Â­â€sensitive	 Â support	 Â vector	 Â 
machines.,"	 Â Epilepsia	 Â 52,	 Â no.	 Â 10,	 Â pp.	 Â 1761-Â­â€1770,	 Â 2011.	 Â 
[4]	 Â Chih-Â­â€Chung	 Â 
[Online].	 Â 
Vector	 Â  Machines.	 Â 
Support	 Â 
(2014)	 Â 
Lin.	 Â 
Chih-Â­â€Jen	 Â 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/	 Â 
[5]	 Â Jesse	 Â Davis	 Â and	 Â Mark	 Â Goadrich,	 Â "The	 Â relationship	 Â between	 Â Precision-Â­â€Recall	 Â and	 Â ROC	 Â curves,"	 Â in	 Â Proceedings	 Â of	 Â the	 Â 23rd	 Â international	 Â conference	 Â 
on	 Â Machine	 Â learning,	 Â 2006,	 Â pp.	 Â 233-Â­â€240.	 Â 
	 Â 
	 Â 

8  References	 Â 

LIBSVM	 Â 

Library	 Â 

Chang	 Â 

and	 Â 

-Â­â€-Â­â€	 Â 

A	 Â 

for	 Â 

(

)	 Â 

Î³ =

0.0125

5	 Â 

