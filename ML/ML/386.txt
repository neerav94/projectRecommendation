Vector-based Sentiment Analysis of Movie Reviews

Ian Roberts and Lisa Yan

{iroberts, yanlisa}@stanford.edu

ABSTRACT
Sentiment analysis is an important step towards comprehen-
sion in natural language processing. Movie reviews are a
convenient source of highly polarized sentences for use in
sentiment analysis.

Achieving a high level of accuracy in the sign of non-
neutral sentiment is a challenge. When the problem is ex-
panded to choosing one of 5 sentiment levels the problem
becomes signiﬁcantly harder. Among the models that we
tested, softmax regression with a bag of phrases feature set
provided the best 5-bin error rate, while SVM with the same
bag of phrases features and a simple word sentiment sum
model provided the best error rate on sign prediction.

1.

INTRODUCTION

We investigate sentence sentiment using the Pang and Lee
dataset as annotated by Socher, et al. [1]. Sentiment analysis
research focuses on understanding the positive or negative
tone of a sentence based on sentence syntax, structure, and
content. Previous research used a tree-based model to label
sentence sentiment on a scale of 5 points. Our project takes a
diﬀerent approach of abstracting the sentence as a vector and
apply vector classiﬁcation schemes. We explore two compo-
nents: ﬁrst, we would like to analyze the use of diﬀerent sen-
tence representations, such as bag of words, word sentiment
location, negation, etc., and abstract them into a set of fea-
tures. Second, we would like to classify sentence sentiment
using this set of features and compare the eﬀectiveness of
diﬀerent models. While sentiment polarity was analyzed in
a previous year’s project, we would like to explore 5 degrees
of sentiment labeling (Figure 1).

We chose to investigate sentiment of movie reviews which
could be compared to numeric movie ratings. We looked
at a variety of models and feature types to attempt to cap-
ture the context important for accurate sentiment decoding
of the phrases of a sentence. While a tree-based feature set
by Socher et al. [1] exists, we also wanted to explore how
linear feature vectors would fare for sentence sentiment clas-
siﬁcation .

2. DATASET

We used the datasets from Stanford’s Sentiment Analy-

5 (most positive)
4 (positive)
3 (neutral)
2 (negative)
1 (most negative)

It was a breathtaking movie.
It was a good movie.
It was a movie.
It was a bad movie.
It was a terrible movie.

Figure 1: Rating sentence sentiment on a 5-point scale.

Sentences

Elements

Training Set

8544

Test Set
2210

Dictionary

Phrases
239232

Unique Words

22348

Table 1: Stanford Sentiment Analysis dataset.

sis website which split the sentences into a training set and
test set and provided a dictionary of sub-phrases and unique
words (Table 1). All phrases (and words) have a sentiment
label as well, which was determined in the original dataset
via Amazon Mechanical Turk. Sentiment is labeled on a 5
point scale of 1 (most negative) to 5 (most positive), with 3
being neutral; a distribution of the sentence sentiment labels
is in Figure 2.

The dataset also includes a sentiment labeled partitioning
of each sentence on a parse tree. Recursive node recombina-
tion can therefore be trained at all levels rather than just with
the root sentiment.

3. APPROACH

3.1 Features

In Figure 3 we examine the distribution of individual word
sentiments within sentences with each level of overall senti-
ment. The distributions are nearly identical with highly pos-
itive words showing up in highly negative sentences just as
often as in highly positive ones. This provides a clear in-
dication that phrases and syntax are key to sentence senti-
ment analysis. While the sentiments of individual words and
phrases were labeled in our dataset the lack of correlation
between word sentiment and sentence sentiment led us to
ignore this labeling for some of our features.

We now overview our vector-based features, where we
represent the ith sentence in our dataset as a vector x(i) over

1

Vector-based Sentiment Analysis of Movie Reviews

Ian Roberts and Lisa Yan

{iroberts, yanlisa}@stanford.edu

ABSTRACT
Sentiment analysis is an important step towards comprehen-
sion in natural language processing. Movie reviews are a
convenient source of highly polarized sentences for use in
sentiment analysis.

Achieving a high level of accuracy in the sign of non-
neutral sentiment is a challenge. When the problem is ex-
panded to choosing one of 5 sentiment levels the problem
becomes signiﬁcantly harder. Among the models that we
tested, softmax regression with a bag of phrases feature set
provided the best 5-bin error rate, while SVM with the same
bag of phrases features and a simple word sentiment sum
model provided the best error rate on sign prediction.

1.

INTRODUCTION

We investigate sentence sentiment using the Pang and Lee
dataset as annotated by Socher, et al. [1]. Sentiment analysis
research focuses on understanding the positive or negative
tone of a sentence based on sentence syntax, structure, and
content. Previous research used a tree-based model to label
sentence sentiment on a scale of 5 points. Our project takes a
diﬀerent approach of abstracting the sentence as a vector and
apply vector classiﬁcation schemes. We explore two compo-
nents: ﬁrst, we would like to analyze the use of diﬀerent sen-
tence representations, such as bag of words, word sentiment
location, negation, etc., and abstract them into a set of fea-
tures. Second, we would like to classify sentence sentiment
using this set of features and compare the eﬀectiveness of
diﬀerent models. While sentiment polarity was analyzed in
a previous year’s project, we would like to explore 5 degrees
of sentiment labeling (Figure 1).

We chose to investigate sentiment of movie reviews which
could be compared to numeric movie ratings. We looked
at a variety of models and feature types to attempt to cap-
ture the context important for accurate sentiment decoding
of the phrases of a sentence. While a tree-based feature set
by Socher et al. [1] exists, we also wanted to explore how
linear feature vectors would fare for sentence sentiment clas-
siﬁcation .

2. DATASET

We used the datasets from Stanford’s Sentiment Analy-

5 (most positive)
4 (positive)
3 (neutral)
2 (negative)
1 (most negative)

It was a breathtaking movie.
It was a good movie.
It was a movie.
It was a bad movie.
It was a terrible movie.

Figure 1: Rating sentence sentiment on a 5-point scale.

Sentences

Elements

Training Set

8544

Test Set
2210

Dictionary

Phrases
239232

Unique Words

22348

Table 1: Stanford Sentiment Analysis dataset.

sis website which split the sentences into a training set and
test set and provided a dictionary of sub-phrases and unique
words (Table 1). All phrases (and words) have a sentiment
label as well, which was determined in the original dataset
via Amazon Mechanical Turk. Sentiment is labeled on a 5
point scale of 1 (most negative) to 5 (most positive), with 3
being neutral; a distribution of the sentence sentiment labels
is in Figure 2.

The dataset also includes a sentiment labeled partitioning
of each sentence on a parse tree. Recursive node recombina-
tion can therefore be trained at all levels rather than just with
the root sentiment.

3. APPROACH

3.1 Features

In Figure 3 we examine the distribution of individual word
sentiments within sentences with each level of overall senti-
ment. The distributions are nearly identical with highly pos-
itive words showing up in highly negative sentences just as
often as in highly positive ones. This provides a clear in-
dication that phrases and syntax are key to sentence senti-
ment analysis. While the sentiments of individual words and
phrases were labeled in our dataset the lack of correlation
between word sentiment and sentence sentiment led us to
ignore this labeling for some of our features.

We now overview our vector-based features, where we
represent the ith sentence in our dataset as a vector x(i) over

1

Feature Name
Bag of Words
Bag of Phrases

Word Sentiment Counts (WSC)

Domain of x(i)

{0, 1}|VW|
{0, 1}|V|
Z5

Word Features (WF)

Z3ni

(i)

x j
1{ jth word in ithsentence}
1{ jth phrase in ithsentence}
# words with sentiment j in ithsentence
word index in dictionary V of jth word in ithsentence
tf-idf of jth word in ithsentence
sentiment of jth word in ithsentence

(a) Summary table of vector-based features.
Bag of
Words

Bag of WSC WF Dictionary Negation
Phrases

indices

Tree

Model Name
Sentiment Sum
Naive Bayes

K-Nearest Neighbors
Softmax Regression

SVM

Tree prediction

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(b) Summary table of models.

Table 2: Summary of features and models.

Figure 2: Distribution of sentence sentiment labels across
dataset.

some domain, where the length of the sentence is ni words,
and x(i)
is the jth element in the feature vector x(i). Our dic-
j
tionary of words and phrases are VW and V, respectively. A
summary of these features is in Table 2a.

Bag of Words, Bag of Phrases In the bag of words rep-
resentation a vector of length equal to the dictionary size
is populated with indicators for the presence of each word
within a sentence. This approach takes into account neither
individual word sentiment nor positioning of words, but it is
very simple to implement. The expansion to a bag of phrases
model adds sentence fragments to the model. The sentence
fragments incorporate context and phrasing that were ignored
by the bag of words model. The bag of phrases model is
limited by the level of overlap between phrases used in the
training and test sets.

Word Sentiment Counts (WSC) This model represents a
sentence as a 5 element vector where each element indicates
the number of sentence words with the corresponding sen-
timent label. This model assumes that individual word sen-
timents are correlated with the sentiment of the overall sen-

Figure 3: Histogram of sentiment labels for each sentiment label.

tence. The distribution of word sentiments remaining nearly
constant with varying sentence sentiment as observed in Fig-
ure 3 indicates that this is likely to be a poor assumption.

Words with Features (WF) This feature vector aggre-
gates three diﬀerent vector sentence representations. The
ﬁrst is the dictionary index of each word in the sentence.
With the dictionary of words this allows complete recon-
struction of the sentence. This means that sentence structure
and context are incorporated, although a linear predictor is
unlikely to be able to predict more than a small portion of the
structure in this representation. The second component is the
term frequency-inverse document frequency for each word
in the sentence. This metric provides an indication of the
level of signiﬁcance of the word within the corpus. The last
component is the sentiment label of each constituent word
of the sentence.

3.2 Models

The most basic estimation of sentence sentiment comes
from a vector sum of the sentiments of the words in the sen-
tence. Shifting the result such that a vector of neutral words
remains neutral the majority vote of non-neutral terms deter-
mines the estimated sentence sentiment. The conclusion of

2

Vector-based Sentiment Analysis of Movie Reviews

Ian Roberts and Lisa Yan

{iroberts, yanlisa}@stanford.edu

ABSTRACT
Sentiment analysis is an important step towards comprehen-
sion in natural language processing. Movie reviews are a
convenient source of highly polarized sentences for use in
sentiment analysis.

Achieving a high level of accuracy in the sign of non-
neutral sentiment is a challenge. When the problem is ex-
panded to choosing one of 5 sentiment levels the problem
becomes signiﬁcantly harder. Among the models that we
tested, softmax regression with a bag of phrases feature set
provided the best 5-bin error rate, while SVM with the same
bag of phrases features and a simple word sentiment sum
model provided the best error rate on sign prediction.

1.

INTRODUCTION

We investigate sentence sentiment using the Pang and Lee
dataset as annotated by Socher, et al. [1]. Sentiment analysis
research focuses on understanding the positive or negative
tone of a sentence based on sentence syntax, structure, and
content. Previous research used a tree-based model to label
sentence sentiment on a scale of 5 points. Our project takes a
diﬀerent approach of abstracting the sentence as a vector and
apply vector classiﬁcation schemes. We explore two compo-
nents: ﬁrst, we would like to analyze the use of diﬀerent sen-
tence representations, such as bag of words, word sentiment
location, negation, etc., and abstract them into a set of fea-
tures. Second, we would like to classify sentence sentiment
using this set of features and compare the eﬀectiveness of
diﬀerent models. While sentiment polarity was analyzed in
a previous year’s project, we would like to explore 5 degrees
of sentiment labeling (Figure 1).

We chose to investigate sentiment of movie reviews which
could be compared to numeric movie ratings. We looked
at a variety of models and feature types to attempt to cap-
ture the context important for accurate sentiment decoding
of the phrases of a sentence. While a tree-based feature set
by Socher et al. [1] exists, we also wanted to explore how
linear feature vectors would fare for sentence sentiment clas-
siﬁcation .

2. DATASET

We used the datasets from Stanford’s Sentiment Analy-

5 (most positive)
4 (positive)
3 (neutral)
2 (negative)
1 (most negative)

It was a breathtaking movie.
It was a good movie.
It was a movie.
It was a bad movie.
It was a terrible movie.

Figure 1: Rating sentence sentiment on a 5-point scale.

Sentences

Elements

Training Set

8544

Test Set
2210

Dictionary

Phrases
239232

Unique Words

22348

Table 1: Stanford Sentiment Analysis dataset.

sis website which split the sentences into a training set and
test set and provided a dictionary of sub-phrases and unique
words (Table 1). All phrases (and words) have a sentiment
label as well, which was determined in the original dataset
via Amazon Mechanical Turk. Sentiment is labeled on a 5
point scale of 1 (most negative) to 5 (most positive), with 3
being neutral; a distribution of the sentence sentiment labels
is in Figure 2.

The dataset also includes a sentiment labeled partitioning
of each sentence on a parse tree. Recursive node recombina-
tion can therefore be trained at all levels rather than just with
the root sentiment.

3. APPROACH

3.1 Features

In Figure 3 we examine the distribution of individual word
sentiments within sentences with each level of overall senti-
ment. The distributions are nearly identical with highly pos-
itive words showing up in highly negative sentences just as
often as in highly positive ones. This provides a clear in-
dication that phrases and syntax are key to sentence senti-
ment analysis. While the sentiments of individual words and
phrases were labeled in our dataset the lack of correlation
between word sentiment and sentence sentiment led us to
ignore this labeling for some of our features.

We now overview our vector-based features, where we
represent the ith sentence in our dataset as a vector x(i) over

1

Feature Name
Bag of Words
Bag of Phrases

Word Sentiment Counts (WSC)

Domain of x(i)

{0, 1}|VW|
{0, 1}|V|
Z5

Word Features (WF)

Z3ni

(i)

x j
1{ jth word in ithsentence}
1{ jth phrase in ithsentence}
# words with sentiment j in ithsentence
word index in dictionary V of jth word in ithsentence
tf-idf of jth word in ithsentence
sentiment of jth word in ithsentence

(a) Summary table of vector-based features.
Bag of
Words

Bag of WSC WF Dictionary Negation
Phrases

indices

Tree

Model Name
Sentiment Sum
Naive Bayes

K-Nearest Neighbors
Softmax Regression

SVM

Tree prediction

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(b) Summary table of models.

Table 2: Summary of features and models.

Figure 2: Distribution of sentence sentiment labels across
dataset.

some domain, where the length of the sentence is ni words,
and x(i)
is the jth element in the feature vector x(i). Our dic-
j
tionary of words and phrases are VW and V, respectively. A
summary of these features is in Table 2a.

Bag of Words, Bag of Phrases In the bag of words rep-
resentation a vector of length equal to the dictionary size
is populated with indicators for the presence of each word
within a sentence. This approach takes into account neither
individual word sentiment nor positioning of words, but it is
very simple to implement. The expansion to a bag of phrases
model adds sentence fragments to the model. The sentence
fragments incorporate context and phrasing that were ignored
by the bag of words model. The bag of phrases model is
limited by the level of overlap between phrases used in the
training and test sets.

Word Sentiment Counts (WSC) This model represents a
sentence as a 5 element vector where each element indicates
the number of sentence words with the corresponding sen-
timent label. This model assumes that individual word sen-
timents are correlated with the sentiment of the overall sen-

Figure 3: Histogram of sentiment labels for each sentiment label.

tence. The distribution of word sentiments remaining nearly
constant with varying sentence sentiment as observed in Fig-
ure 3 indicates that this is likely to be a poor assumption.

Words with Features (WF) This feature vector aggre-
gates three diﬀerent vector sentence representations. The
ﬁrst is the dictionary index of each word in the sentence.
With the dictionary of words this allows complete recon-
struction of the sentence. This means that sentence structure
and context are incorporated, although a linear predictor is
unlikely to be able to predict more than a small portion of the
structure in this representation. The second component is the
term frequency-inverse document frequency for each word
in the sentence. This metric provides an indication of the
level of signiﬁcance of the word within the corpus. The last
component is the sentiment label of each constituent word
of the sentence.

3.2 Models

The most basic estimation of sentence sentiment comes
from a vector sum of the sentiments of the words in the sen-
tence. Shifting the result such that a vector of neutral words
remains neutral the majority vote of non-neutral terms deter-
mines the estimated sentence sentiment. The conclusion of

2

Figure 3 is that this should be a poor estimate of the sentence
sentiment as sentences of all sentiments contain non-neutral
words with nearly constant distributions. Positive words are
generally not used to buoy the sentiment of sentences with
overall negative sentiment. Instead they are generally altered
with control words or used as a contrast to emphasize the
negative sentiment of the sentence. Movie reviews in par-
ticular make use of ﬂamboyant and sarcastic phrasing to ex-
press negative sentiments. The most straightforward set of
control words are inversion of which "not" and the contrac-
tion suﬃx "n’t" are typical examples. Incorporating negation
in a model building sentence sentiments from word senti-
ments should improve performance. Negation in a sentence
generally alters a sub-section of a sentence rather than ap-
plying to all words within the sentence. Localized negation
is not compatible with linear models and must either be used
with simple models such as vector sum of sentiments or be
introduced as a derived modiﬁcation to the feature vector be-
fore modeling.

Most negation has the position of the negating word as a
well deﬁned initial boundary for the extent of the negation.
Determining the ﬁnal extent of the negation is much a diﬃ-
cult problem [2]. We have used the approximation that nega-
tion runs until the end of the sentence but also allow multiple
negations to accumulate and continue to ﬂip the sentiment
interpretation.

We used Naive Bayes, K-Nearest Neighbors, and Soft-
max regression to test both our dictionary feature set and
our word sentiment feature set. For K-Nearest Neighbors,
we set k to be 20% of the training samples; for each test vec-
tor, the model selects the k-nearest training vectors, selected
by hamming distance, and outputs the mode (most frequent)
of the training sentiments. We ran softmax regression to con-
verge within a change of 0.05, which was 3820 iterations (al-
pha = 0.1) on the full phrase dictionary bag of phrases and
705 iterations (alpha = 0.01) on the word sentiment vector.
We implemented these models in Matlab.

In the below equations, m is the size of our training set,
and g(x(i)) = s describes a hypothesis function g that gives
the ith sentence a sentiment label s, as a function of the fea-
ture vector x(i).

Sentiment Sum We used a vector sum of word sentiments

as our baseline model, using the WSC model:

5(cid:88)

g(x(i)) =

(i) − 3) + 3

( jx j

j=1

Negation tracking detects a negating word and inverts the
sentiment of all subsequent words.

Naïve Bayes (NB) We used Bag of Words, Bag of Phrases,
and a modiﬁed version of our feature vectors which ignored
the frequency products and word sentiments and instead fo-
cused only on dictionary indices of the sentence. Our Naïve
Bayes model used a Bernoulli event model for each of the 5

3

sentiment labels and the most probable label was selected.
p(g(x(i)) = s|x(i)) =

(i)|g(x(i)) = s)(cid:1)p(g(x(i)) = s)
(i)|g(x(i)) = ˆs)(cid:1)g(x(i)) = ˆs)

(cid:0)(cid:81)m
(cid:0)(cid:81)m
(cid:80)

j=1 p(x j

j=1 p(x j

ˆs

K-Nearest Neighbors (KNN) We used Euclidean distance
to ﬁnd the k closest training samples and returned the most
common sentence sentiment label, and tested/trained on all
features.



hθ(x(i)) =

2

T x(i)



j=1 eθ j



eθT
eθT
eθT
eθT
eθT

 =

D(x, x(i)) = ||x − x(i)||2
1(cid:80)5

p(y(i) = 1|x(i); θ)
p(y(i) = 2|x(i); θ)
p(y(i) = 3|x(i); θ)
p(y(i) = 4|x(i); θ)
p(y(i) = 5|x(i); θ)

1 x(i)
2 x(i)
3 x(i)
4 x(i)
5 x(i)
g(x(i)) = modex(p)(yp) : x(p) ∈ {k nearest samples}
5(cid:88)
5(cid:88)
m(cid:88)
m(cid:88)
(cid:2)x(i)(cid:0)1{y(i) = j} − p(y(i) = j|x(i); θ)(cid:3) − λθ j

1{y(i) = j} log p(y(i) = j|x(i); θ) − λ
2

p=1

i=1

j=1

||θp||2

2

Softmax regression with l1-regularization We tested and
trained softmax regression on all features.

The gradient ascent update equation is as follows:

max

θ

i=1

feature vectors.

SVM We used liblinear 1.95 to train SVM models for our

Tree prediction Elements of node recombination matrix
are deﬁned by the average sentiments of nodes in the training
set with child labels corresponding to the indices of the ma-
trix element. Negation was included in this model by adding
a negation label to the set of sentiment labels. Negation is
primarily right combining and not all trees separate phrases
such that negations occur as the left child of a node so the
(cid:80)
negation persists in instances where the negation occurs on
a right child.
(cid:80)m
N∈Nodes(x(k)) s(N)1{s(Nle f t) = i}1{s(Nright) = j}

N 1{s(Nle f t) = i}1{s(Nright) = j}

(cid:80)m

Mi, j =

(cid:80)

k=1

k=1
4. RESULTS

The simple baseline model of vector sum of sentiments
provided better results than many of our more expressive
models. This is particularly true for polarity error as shown
in Table 3. Test error and train error are determined by cor-
rect labeling from the 5 sentiment labels. Polarity error is
determined by the polarity of the predicted sentiment being
correct for non-neutral sentences. The expected gain in per-
formance of the vector sum of sentiments method from nega-
tion tracking did not show up. There are 357 sentences that
include negations in our test set of 2210 sentences so the im-
provement from negation tracking should be small but not

Vector-based Sentiment Analysis of Movie Reviews

Ian Roberts and Lisa Yan

{iroberts, yanlisa}@stanford.edu

ABSTRACT
Sentiment analysis is an important step towards comprehen-
sion in natural language processing. Movie reviews are a
convenient source of highly polarized sentences for use in
sentiment analysis.

Achieving a high level of accuracy in the sign of non-
neutral sentiment is a challenge. When the problem is ex-
panded to choosing one of 5 sentiment levels the problem
becomes signiﬁcantly harder. Among the models that we
tested, softmax regression with a bag of phrases feature set
provided the best 5-bin error rate, while SVM with the same
bag of phrases features and a simple word sentiment sum
model provided the best error rate on sign prediction.

1.

INTRODUCTION

We investigate sentence sentiment using the Pang and Lee
dataset as annotated by Socher, et al. [1]. Sentiment analysis
research focuses on understanding the positive or negative
tone of a sentence based on sentence syntax, structure, and
content. Previous research used a tree-based model to label
sentence sentiment on a scale of 5 points. Our project takes a
diﬀerent approach of abstracting the sentence as a vector and
apply vector classiﬁcation schemes. We explore two compo-
nents: ﬁrst, we would like to analyze the use of diﬀerent sen-
tence representations, such as bag of words, word sentiment
location, negation, etc., and abstract them into a set of fea-
tures. Second, we would like to classify sentence sentiment
using this set of features and compare the eﬀectiveness of
diﬀerent models. While sentiment polarity was analyzed in
a previous year’s project, we would like to explore 5 degrees
of sentiment labeling (Figure 1).

We chose to investigate sentiment of movie reviews which
could be compared to numeric movie ratings. We looked
at a variety of models and feature types to attempt to cap-
ture the context important for accurate sentiment decoding
of the phrases of a sentence. While a tree-based feature set
by Socher et al. [1] exists, we also wanted to explore how
linear feature vectors would fare for sentence sentiment clas-
siﬁcation .

2. DATASET

We used the datasets from Stanford’s Sentiment Analy-

5 (most positive)
4 (positive)
3 (neutral)
2 (negative)
1 (most negative)

It was a breathtaking movie.
It was a good movie.
It was a movie.
It was a bad movie.
It was a terrible movie.

Figure 1: Rating sentence sentiment on a 5-point scale.

Sentences

Elements

Training Set

8544

Test Set
2210

Dictionary

Phrases
239232

Unique Words

22348

Table 1: Stanford Sentiment Analysis dataset.

sis website which split the sentences into a training set and
test set and provided a dictionary of sub-phrases and unique
words (Table 1). All phrases (and words) have a sentiment
label as well, which was determined in the original dataset
via Amazon Mechanical Turk. Sentiment is labeled on a 5
point scale of 1 (most negative) to 5 (most positive), with 3
being neutral; a distribution of the sentence sentiment labels
is in Figure 2.

The dataset also includes a sentiment labeled partitioning
of each sentence on a parse tree. Recursive node recombina-
tion can therefore be trained at all levels rather than just with
the root sentiment.

3. APPROACH

3.1 Features

In Figure 3 we examine the distribution of individual word
sentiments within sentences with each level of overall senti-
ment. The distributions are nearly identical with highly pos-
itive words showing up in highly negative sentences just as
often as in highly positive ones. This provides a clear in-
dication that phrases and syntax are key to sentence senti-
ment analysis. While the sentiments of individual words and
phrases were labeled in our dataset the lack of correlation
between word sentiment and sentence sentiment led us to
ignore this labeling for some of our features.

We now overview our vector-based features, where we
represent the ith sentence in our dataset as a vector x(i) over

1

Feature Name
Bag of Words
Bag of Phrases

Word Sentiment Counts (WSC)

Domain of x(i)

{0, 1}|VW|
{0, 1}|V|
Z5

Word Features (WF)

Z3ni

(i)

x j
1{ jth word in ithsentence}
1{ jth phrase in ithsentence}
# words with sentiment j in ithsentence
word index in dictionary V of jth word in ithsentence
tf-idf of jth word in ithsentence
sentiment of jth word in ithsentence

(a) Summary table of vector-based features.
Bag of
Words

Bag of WSC WF Dictionary Negation
Phrases

indices

Tree

Model Name
Sentiment Sum
Naive Bayes

K-Nearest Neighbors
Softmax Regression

SVM

Tree prediction

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(b) Summary table of models.

Table 2: Summary of features and models.

Figure 2: Distribution of sentence sentiment labels across
dataset.

some domain, where the length of the sentence is ni words,
and x(i)
is the jth element in the feature vector x(i). Our dic-
j
tionary of words and phrases are VW and V, respectively. A
summary of these features is in Table 2a.

Bag of Words, Bag of Phrases In the bag of words rep-
resentation a vector of length equal to the dictionary size
is populated with indicators for the presence of each word
within a sentence. This approach takes into account neither
individual word sentiment nor positioning of words, but it is
very simple to implement. The expansion to a bag of phrases
model adds sentence fragments to the model. The sentence
fragments incorporate context and phrasing that were ignored
by the bag of words model. The bag of phrases model is
limited by the level of overlap between phrases used in the
training and test sets.

Word Sentiment Counts (WSC) This model represents a
sentence as a 5 element vector where each element indicates
the number of sentence words with the corresponding sen-
timent label. This model assumes that individual word sen-
timents are correlated with the sentiment of the overall sen-

Figure 3: Histogram of sentiment labels for each sentiment label.

tence. The distribution of word sentiments remaining nearly
constant with varying sentence sentiment as observed in Fig-
ure 3 indicates that this is likely to be a poor assumption.

Words with Features (WF) This feature vector aggre-
gates three diﬀerent vector sentence representations. The
ﬁrst is the dictionary index of each word in the sentence.
With the dictionary of words this allows complete recon-
struction of the sentence. This means that sentence structure
and context are incorporated, although a linear predictor is
unlikely to be able to predict more than a small portion of the
structure in this representation. The second component is the
term frequency-inverse document frequency for each word
in the sentence. This metric provides an indication of the
level of signiﬁcance of the word within the corpus. The last
component is the sentiment label of each constituent word
of the sentence.

3.2 Models

The most basic estimation of sentence sentiment comes
from a vector sum of the sentiments of the words in the sen-
tence. Shifting the result such that a vector of neutral words
remains neutral the majority vote of non-neutral terms deter-
mines the estimated sentence sentiment. The conclusion of

2

Figure 3 is that this should be a poor estimate of the sentence
sentiment as sentences of all sentiments contain non-neutral
words with nearly constant distributions. Positive words are
generally not used to buoy the sentiment of sentences with
overall negative sentiment. Instead they are generally altered
with control words or used as a contrast to emphasize the
negative sentiment of the sentence. Movie reviews in par-
ticular make use of ﬂamboyant and sarcastic phrasing to ex-
press negative sentiments. The most straightforward set of
control words are inversion of which "not" and the contrac-
tion suﬃx "n’t" are typical examples. Incorporating negation
in a model building sentence sentiments from word senti-
ments should improve performance. Negation in a sentence
generally alters a sub-section of a sentence rather than ap-
plying to all words within the sentence. Localized negation
is not compatible with linear models and must either be used
with simple models such as vector sum of sentiments or be
introduced as a derived modiﬁcation to the feature vector be-
fore modeling.

Most negation has the position of the negating word as a
well deﬁned initial boundary for the extent of the negation.
Determining the ﬁnal extent of the negation is much a diﬃ-
cult problem [2]. We have used the approximation that nega-
tion runs until the end of the sentence but also allow multiple
negations to accumulate and continue to ﬂip the sentiment
interpretation.

We used Naive Bayes, K-Nearest Neighbors, and Soft-
max regression to test both our dictionary feature set and
our word sentiment feature set. For K-Nearest Neighbors,
we set k to be 20% of the training samples; for each test vec-
tor, the model selects the k-nearest training vectors, selected
by hamming distance, and outputs the mode (most frequent)
of the training sentiments. We ran softmax regression to con-
verge within a change of 0.05, which was 3820 iterations (al-
pha = 0.1) on the full phrase dictionary bag of phrases and
705 iterations (alpha = 0.01) on the word sentiment vector.
We implemented these models in Matlab.

In the below equations, m is the size of our training set,
and g(x(i)) = s describes a hypothesis function g that gives
the ith sentence a sentiment label s, as a function of the fea-
ture vector x(i).

Sentiment Sum We used a vector sum of word sentiments

as our baseline model, using the WSC model:

5(cid:88)

g(x(i)) =

(i) − 3) + 3

( jx j

j=1

Negation tracking detects a negating word and inverts the
sentiment of all subsequent words.

Naïve Bayes (NB) We used Bag of Words, Bag of Phrases,
and a modiﬁed version of our feature vectors which ignored
the frequency products and word sentiments and instead fo-
cused only on dictionary indices of the sentence. Our Naïve
Bayes model used a Bernoulli event model for each of the 5

3

sentiment labels and the most probable label was selected.
p(g(x(i)) = s|x(i)) =

(i)|g(x(i)) = s)(cid:1)p(g(x(i)) = s)
(i)|g(x(i)) = ˆs)(cid:1)g(x(i)) = ˆs)

(cid:0)(cid:81)m
(cid:0)(cid:81)m
(cid:80)

j=1 p(x j

j=1 p(x j

ˆs

K-Nearest Neighbors (KNN) We used Euclidean distance
to ﬁnd the k closest training samples and returned the most
common sentence sentiment label, and tested/trained on all
features.



hθ(x(i)) =

2

T x(i)



j=1 eθ j



eθT
eθT
eθT
eθT
eθT

 =

D(x, x(i)) = ||x − x(i)||2
1(cid:80)5

p(y(i) = 1|x(i); θ)
p(y(i) = 2|x(i); θ)
p(y(i) = 3|x(i); θ)
p(y(i) = 4|x(i); θ)
p(y(i) = 5|x(i); θ)

1 x(i)
2 x(i)
3 x(i)
4 x(i)
5 x(i)
g(x(i)) = modex(p)(yp) : x(p) ∈ {k nearest samples}
5(cid:88)
5(cid:88)
m(cid:88)
m(cid:88)
(cid:2)x(i)(cid:0)1{y(i) = j} − p(y(i) = j|x(i); θ)(cid:3) − λθ j

1{y(i) = j} log p(y(i) = j|x(i); θ) − λ
2

p=1

i=1

j=1

||θp||2

2

Softmax regression with l1-regularization We tested and
trained softmax regression on all features.

The gradient ascent update equation is as follows:

max

θ

i=1

feature vectors.

SVM We used liblinear 1.95 to train SVM models for our

Tree prediction Elements of node recombination matrix
are deﬁned by the average sentiments of nodes in the training
set with child labels corresponding to the indices of the ma-
trix element. Negation was included in this model by adding
a negation label to the set of sentiment labels. Negation is
primarily right combining and not all trees separate phrases
such that negations occur as the left child of a node so the
(cid:80)
negation persists in instances where the negation occurs on
a right child.
(cid:80)m
N∈Nodes(x(k)) s(N)1{s(Nle f t) = i}1{s(Nright) = j}

N 1{s(Nle f t) = i}1{s(Nright) = j}

(cid:80)m

Mi, j =

(cid:80)

k=1

k=1
4. RESULTS

The simple baseline model of vector sum of sentiments
provided better results than many of our more expressive
models. This is particularly true for polarity error as shown
in Table 3. Test error and train error are determined by cor-
rect labeling from the 5 sentiment labels. Polarity error is
determined by the polarity of the predicted sentiment being
correct for non-neutral sentences. The expected gain in per-
formance of the vector sum of sentiments method from nega-
tion tracking did not show up. There are 357 sentences that
include negations in our test set of 2210 sentences so the im-
provement from negation tracking should be small but not

Model

Features

WSC

Negation

Bag of Words
Bag of Phrases

WSC

Dictionary indices

Bag of Words
Bag of Phrases

Sentiment Sum

Naïve Bayes

KNN

Softmax

SVM

Tree prediction

Bag of Words
Bag of Phrases

Bag of Words
Bag of Phrases

WSC
WF

WSC
WF

WSC
WF
Tree

Test
Error
0.67
0.67
0.77
0.75
0.72
0.62
0.77
0.77
0.77
0.71
0.73
0.59
0.65
0.63
0.72
0.60
0.62
0.62
0.65

Train
Error
0.67
0.67
0.26
0.13
0.75
0.46
0.73
0.73
0.73
0.74
0.22
0.10
0.64
0.64
0.20
0.00
0.63
0.62
0.64

Polarity
Error
0.25
0.26
0.42
0.26
0.46
0.37
0.82
0.82
0.82
0.82
0.52
0.39
0.41
0.39
0.41
0.23
0.26
0.27
0.41

Table 3: Table of results.

insigniﬁcant. There are several possible explanations for the
lack of improvement. The ﬁrst is that our choice to negate
until the end of the sentence is a poor estimate of the proper
negation extent. The other good estimate of the extent of
negation is to negate until the next punctuation mark, but
the beneﬁt of this form of negation tracking is also reported
to be insigniﬁcant [2]. Another explanation for the poor per-
formance is that negation in sentences from movie reviews is
used in hyperbolic phrases and combined with other rhetor-
ical devices such that the eﬀect of negation tracking alone
is insigniﬁcant. The conclusion from either of these expla-
nations is that simple negation tracking does not improve
sentiment labeling.

The Naïve Bayes method provides relatively poor perfor-
mance for most of our feature vectors that is only slightly
better than a random guess for 5-bin sentiment accuracy.
Evaluating the model on the training set provides dramati-
cally improved results, potentially indicating an overﬁtting
problem. Reducing the training set of phrases to only a set
of words reduces the discrepancy between the training and
test error rates, but also results in overall worse performance.
The doubling of training set error for bag of words compared
to bag of phrases makes it clear that it is longer phrases that
are key to the prediction strength of this naive Bayes model,
but training the longer features requires a training set that
shares these longer features with the test set. With a much
larger dataset this method may be able to overcome its lack
of structural awareness, but for this dataset it does not seem
practical beyond use as a baseline for comparing other meth-
ods. Naïve Bayes with a feature vector of dictionary indices
provides noticably better 5 level accuracy than vector sum of
sentiments model but provides a signiﬁcantly higher polarity
error.

Softmax regression provides good performance with all

4

Figure 4: Test and training error vs. training set size for softmax
regression on Bag of Phrases.

feature types other than bag of words and the bag of phrases
features provide the best 5 level test error of all our models.
The training set error for bag of words and bag of phrases is
dramatically lower than the test error rate. The features in
these two scenarios are derived from the words and phrases
present in the training set and many of these are unique in the
dataset. This causes signiﬁcant overﬁtting to the training set
but only the portion of these overﬁt features that also appear
in the test set are realized as overﬁtting errors.

SVM provides similar 5 level error rates for the diﬀerent
feature types as softmax regression. The bag of phrases fea-
ture set once again provides the best performance although it
is slightly inferior to softmax. The advantage to SVM comes
from the signiﬁcantly lower polarity error. The overﬁtting of
the bag of phrases feature set is most apparent in SVM where
a 0% training set error rate is attained.

The relatively high polarity error for the best Naïve Bayes
and softmax regression models indicate that these are rela-
tively poor models for general application. When they do
make a mistake on the 5 level sentiment for a non-neutral
word they make a large number of big mistakes as reﬂected
by their roughly 40% polarity error rates.

KNN provides very poor results for all of the features
tested. The polarity error rate is higher than the 5 label er-
ror rate as the polarity error rate is evaluated on non-neutral
sentences only. The high error rate of KNN is likely due to
the Euclidean distance metric chosen; for high dimension bi-
nary vectors such as produced by the bag of words and bag
of phrases models, the distance between sentence vectors is
large and close to randomly distributed.

The bag of phrases feature set provides the best perfor-
mance for the SVM and softmax regression models but it is
not consistent as the error rate with both KNN and Naïve
Bayes is near 80%. The reduced feature set of bag of words
provides uniformly inferior performance to bag of phrases.
For SVM and softmax regression this diﬀerence due to the
context and phrasing that is incorporated by the bag of phrases
model provides a dramatic improvement in test error rate of
at least 15%. WF is much less variable with the training

Vector-based Sentiment Analysis of Movie Reviews

Ian Roberts and Lisa Yan

{iroberts, yanlisa}@stanford.edu

ABSTRACT
Sentiment analysis is an important step towards comprehen-
sion in natural language processing. Movie reviews are a
convenient source of highly polarized sentences for use in
sentiment analysis.

Achieving a high level of accuracy in the sign of non-
neutral sentiment is a challenge. When the problem is ex-
panded to choosing one of 5 sentiment levels the problem
becomes signiﬁcantly harder. Among the models that we
tested, softmax regression with a bag of phrases feature set
provided the best 5-bin error rate, while SVM with the same
bag of phrases features and a simple word sentiment sum
model provided the best error rate on sign prediction.

1.

INTRODUCTION

We investigate sentence sentiment using the Pang and Lee
dataset as annotated by Socher, et al. [1]. Sentiment analysis
research focuses on understanding the positive or negative
tone of a sentence based on sentence syntax, structure, and
content. Previous research used a tree-based model to label
sentence sentiment on a scale of 5 points. Our project takes a
diﬀerent approach of abstracting the sentence as a vector and
apply vector classiﬁcation schemes. We explore two compo-
nents: ﬁrst, we would like to analyze the use of diﬀerent sen-
tence representations, such as bag of words, word sentiment
location, negation, etc., and abstract them into a set of fea-
tures. Second, we would like to classify sentence sentiment
using this set of features and compare the eﬀectiveness of
diﬀerent models. While sentiment polarity was analyzed in
a previous year’s project, we would like to explore 5 degrees
of sentiment labeling (Figure 1).

We chose to investigate sentiment of movie reviews which
could be compared to numeric movie ratings. We looked
at a variety of models and feature types to attempt to cap-
ture the context important for accurate sentiment decoding
of the phrases of a sentence. While a tree-based feature set
by Socher et al. [1] exists, we also wanted to explore how
linear feature vectors would fare for sentence sentiment clas-
siﬁcation .

2. DATASET

We used the datasets from Stanford’s Sentiment Analy-

5 (most positive)
4 (positive)
3 (neutral)
2 (negative)
1 (most negative)

It was a breathtaking movie.
It was a good movie.
It was a movie.
It was a bad movie.
It was a terrible movie.

Figure 1: Rating sentence sentiment on a 5-point scale.

Sentences

Elements

Training Set

8544

Test Set
2210

Dictionary

Phrases
239232

Unique Words

22348

Table 1: Stanford Sentiment Analysis dataset.

sis website which split the sentences into a training set and
test set and provided a dictionary of sub-phrases and unique
words (Table 1). All phrases (and words) have a sentiment
label as well, which was determined in the original dataset
via Amazon Mechanical Turk. Sentiment is labeled on a 5
point scale of 1 (most negative) to 5 (most positive), with 3
being neutral; a distribution of the sentence sentiment labels
is in Figure 2.

The dataset also includes a sentiment labeled partitioning
of each sentence on a parse tree. Recursive node recombina-
tion can therefore be trained at all levels rather than just with
the root sentiment.

3. APPROACH

3.1 Features

In Figure 3 we examine the distribution of individual word
sentiments within sentences with each level of overall senti-
ment. The distributions are nearly identical with highly pos-
itive words showing up in highly negative sentences just as
often as in highly positive ones. This provides a clear in-
dication that phrases and syntax are key to sentence senti-
ment analysis. While the sentiments of individual words and
phrases were labeled in our dataset the lack of correlation
between word sentiment and sentence sentiment led us to
ignore this labeling for some of our features.

We now overview our vector-based features, where we
represent the ith sentence in our dataset as a vector x(i) over

1

Feature Name
Bag of Words
Bag of Phrases

Word Sentiment Counts (WSC)

Domain of x(i)

{0, 1}|VW|
{0, 1}|V|
Z5

Word Features (WF)

Z3ni

(i)

x j
1{ jth word in ithsentence}
1{ jth phrase in ithsentence}
# words with sentiment j in ithsentence
word index in dictionary V of jth word in ithsentence
tf-idf of jth word in ithsentence
sentiment of jth word in ithsentence

(a) Summary table of vector-based features.
Bag of
Words

Bag of WSC WF Dictionary Negation
Phrases

indices

Tree

Model Name
Sentiment Sum
Naive Bayes

K-Nearest Neighbors
Softmax Regression

SVM

Tree prediction

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(b) Summary table of models.

Table 2: Summary of features and models.

Figure 2: Distribution of sentence sentiment labels across
dataset.

some domain, where the length of the sentence is ni words,
and x(i)
is the jth element in the feature vector x(i). Our dic-
j
tionary of words and phrases are VW and V, respectively. A
summary of these features is in Table 2a.

Bag of Words, Bag of Phrases In the bag of words rep-
resentation a vector of length equal to the dictionary size
is populated with indicators for the presence of each word
within a sentence. This approach takes into account neither
individual word sentiment nor positioning of words, but it is
very simple to implement. The expansion to a bag of phrases
model adds sentence fragments to the model. The sentence
fragments incorporate context and phrasing that were ignored
by the bag of words model. The bag of phrases model is
limited by the level of overlap between phrases used in the
training and test sets.

Word Sentiment Counts (WSC) This model represents a
sentence as a 5 element vector where each element indicates
the number of sentence words with the corresponding sen-
timent label. This model assumes that individual word sen-
timents are correlated with the sentiment of the overall sen-

Figure 3: Histogram of sentiment labels for each sentiment label.

tence. The distribution of word sentiments remaining nearly
constant with varying sentence sentiment as observed in Fig-
ure 3 indicates that this is likely to be a poor assumption.

Words with Features (WF) This feature vector aggre-
gates three diﬀerent vector sentence representations. The
ﬁrst is the dictionary index of each word in the sentence.
With the dictionary of words this allows complete recon-
struction of the sentence. This means that sentence structure
and context are incorporated, although a linear predictor is
unlikely to be able to predict more than a small portion of the
structure in this representation. The second component is the
term frequency-inverse document frequency for each word
in the sentence. This metric provides an indication of the
level of signiﬁcance of the word within the corpus. The last
component is the sentiment label of each constituent word
of the sentence.

3.2 Models

The most basic estimation of sentence sentiment comes
from a vector sum of the sentiments of the words in the sen-
tence. Shifting the result such that a vector of neutral words
remains neutral the majority vote of non-neutral terms deter-
mines the estimated sentence sentiment. The conclusion of

2

Figure 3 is that this should be a poor estimate of the sentence
sentiment as sentences of all sentiments contain non-neutral
words with nearly constant distributions. Positive words are
generally not used to buoy the sentiment of sentences with
overall negative sentiment. Instead they are generally altered
with control words or used as a contrast to emphasize the
negative sentiment of the sentence. Movie reviews in par-
ticular make use of ﬂamboyant and sarcastic phrasing to ex-
press negative sentiments. The most straightforward set of
control words are inversion of which "not" and the contrac-
tion suﬃx "n’t" are typical examples. Incorporating negation
in a model building sentence sentiments from word senti-
ments should improve performance. Negation in a sentence
generally alters a sub-section of a sentence rather than ap-
plying to all words within the sentence. Localized negation
is not compatible with linear models and must either be used
with simple models such as vector sum of sentiments or be
introduced as a derived modiﬁcation to the feature vector be-
fore modeling.

Most negation has the position of the negating word as a
well deﬁned initial boundary for the extent of the negation.
Determining the ﬁnal extent of the negation is much a diﬃ-
cult problem [2]. We have used the approximation that nega-
tion runs until the end of the sentence but also allow multiple
negations to accumulate and continue to ﬂip the sentiment
interpretation.

We used Naive Bayes, K-Nearest Neighbors, and Soft-
max regression to test both our dictionary feature set and
our word sentiment feature set. For K-Nearest Neighbors,
we set k to be 20% of the training samples; for each test vec-
tor, the model selects the k-nearest training vectors, selected
by hamming distance, and outputs the mode (most frequent)
of the training sentiments. We ran softmax regression to con-
verge within a change of 0.05, which was 3820 iterations (al-
pha = 0.1) on the full phrase dictionary bag of phrases and
705 iterations (alpha = 0.01) on the word sentiment vector.
We implemented these models in Matlab.

In the below equations, m is the size of our training set,
and g(x(i)) = s describes a hypothesis function g that gives
the ith sentence a sentiment label s, as a function of the fea-
ture vector x(i).

Sentiment Sum We used a vector sum of word sentiments

as our baseline model, using the WSC model:

5(cid:88)

g(x(i)) =

(i) − 3) + 3

( jx j

j=1

Negation tracking detects a negating word and inverts the
sentiment of all subsequent words.

Naïve Bayes (NB) We used Bag of Words, Bag of Phrases,
and a modiﬁed version of our feature vectors which ignored
the frequency products and word sentiments and instead fo-
cused only on dictionary indices of the sentence. Our Naïve
Bayes model used a Bernoulli event model for each of the 5

3

sentiment labels and the most probable label was selected.
p(g(x(i)) = s|x(i)) =

(i)|g(x(i)) = s)(cid:1)p(g(x(i)) = s)
(i)|g(x(i)) = ˆs)(cid:1)g(x(i)) = ˆs)

(cid:0)(cid:81)m
(cid:0)(cid:81)m
(cid:80)

j=1 p(x j

j=1 p(x j

ˆs

K-Nearest Neighbors (KNN) We used Euclidean distance
to ﬁnd the k closest training samples and returned the most
common sentence sentiment label, and tested/trained on all
features.



hθ(x(i)) =

2

T x(i)



j=1 eθ j



eθT
eθT
eθT
eθT
eθT

 =

D(x, x(i)) = ||x − x(i)||2
1(cid:80)5

p(y(i) = 1|x(i); θ)
p(y(i) = 2|x(i); θ)
p(y(i) = 3|x(i); θ)
p(y(i) = 4|x(i); θ)
p(y(i) = 5|x(i); θ)

1 x(i)
2 x(i)
3 x(i)
4 x(i)
5 x(i)
g(x(i)) = modex(p)(yp) : x(p) ∈ {k nearest samples}
5(cid:88)
5(cid:88)
m(cid:88)
m(cid:88)
(cid:2)x(i)(cid:0)1{y(i) = j} − p(y(i) = j|x(i); θ)(cid:3) − λθ j

1{y(i) = j} log p(y(i) = j|x(i); θ) − λ
2

p=1

i=1

j=1

||θp||2

2

Softmax regression with l1-regularization We tested and
trained softmax regression on all features.

The gradient ascent update equation is as follows:

max

θ

i=1

feature vectors.

SVM We used liblinear 1.95 to train SVM models for our

Tree prediction Elements of node recombination matrix
are deﬁned by the average sentiments of nodes in the training
set with child labels corresponding to the indices of the ma-
trix element. Negation was included in this model by adding
a negation label to the set of sentiment labels. Negation is
primarily right combining and not all trees separate phrases
such that negations occur as the left child of a node so the
(cid:80)
negation persists in instances where the negation occurs on
a right child.
(cid:80)m
N∈Nodes(x(k)) s(N)1{s(Nle f t) = i}1{s(Nright) = j}

N 1{s(Nle f t) = i}1{s(Nright) = j}

(cid:80)m

Mi, j =

(cid:80)

k=1

k=1
4. RESULTS

The simple baseline model of vector sum of sentiments
provided better results than many of our more expressive
models. This is particularly true for polarity error as shown
in Table 3. Test error and train error are determined by cor-
rect labeling from the 5 sentiment labels. Polarity error is
determined by the polarity of the predicted sentiment being
correct for non-neutral sentences. The expected gain in per-
formance of the vector sum of sentiments method from nega-
tion tracking did not show up. There are 357 sentences that
include negations in our test set of 2210 sentences so the im-
provement from negation tracking should be small but not

Model

Features

WSC

Negation

Bag of Words
Bag of Phrases

WSC

Dictionary indices

Bag of Words
Bag of Phrases

Sentiment Sum

Naïve Bayes

KNN

Softmax

SVM

Tree prediction

Bag of Words
Bag of Phrases

Bag of Words
Bag of Phrases

WSC
WF

WSC
WF

WSC
WF
Tree

Test
Error
0.67
0.67
0.77
0.75
0.72
0.62
0.77
0.77
0.77
0.71
0.73
0.59
0.65
0.63
0.72
0.60
0.62
0.62
0.65

Train
Error
0.67
0.67
0.26
0.13
0.75
0.46
0.73
0.73
0.73
0.74
0.22
0.10
0.64
0.64
0.20
0.00
0.63
0.62
0.64

Polarity
Error
0.25
0.26
0.42
0.26
0.46
0.37
0.82
0.82
0.82
0.82
0.52
0.39
0.41
0.39
0.41
0.23
0.26
0.27
0.41

Table 3: Table of results.

insigniﬁcant. There are several possible explanations for the
lack of improvement. The ﬁrst is that our choice to negate
until the end of the sentence is a poor estimate of the proper
negation extent. The other good estimate of the extent of
negation is to negate until the next punctuation mark, but
the beneﬁt of this form of negation tracking is also reported
to be insigniﬁcant [2]. Another explanation for the poor per-
formance is that negation in sentences from movie reviews is
used in hyperbolic phrases and combined with other rhetor-
ical devices such that the eﬀect of negation tracking alone
is insigniﬁcant. The conclusion from either of these expla-
nations is that simple negation tracking does not improve
sentiment labeling.

The Naïve Bayes method provides relatively poor perfor-
mance for most of our feature vectors that is only slightly
better than a random guess for 5-bin sentiment accuracy.
Evaluating the model on the training set provides dramati-
cally improved results, potentially indicating an overﬁtting
problem. Reducing the training set of phrases to only a set
of words reduces the discrepancy between the training and
test error rates, but also results in overall worse performance.
The doubling of training set error for bag of words compared
to bag of phrases makes it clear that it is longer phrases that
are key to the prediction strength of this naive Bayes model,
but training the longer features requires a training set that
shares these longer features with the test set. With a much
larger dataset this method may be able to overcome its lack
of structural awareness, but for this dataset it does not seem
practical beyond use as a baseline for comparing other meth-
ods. Naïve Bayes with a feature vector of dictionary indices
provides noticably better 5 level accuracy than vector sum of
sentiments model but provides a signiﬁcantly higher polarity
error.

Softmax regression provides good performance with all

4

Figure 4: Test and training error vs. training set size for softmax
regression on Bag of Phrases.

feature types other than bag of words and the bag of phrases
features provide the best 5 level test error of all our models.
The training set error for bag of words and bag of phrases is
dramatically lower than the test error rate. The features in
these two scenarios are derived from the words and phrases
present in the training set and many of these are unique in the
dataset. This causes signiﬁcant overﬁtting to the training set
but only the portion of these overﬁt features that also appear
in the test set are realized as overﬁtting errors.

SVM provides similar 5 level error rates for the diﬀerent
feature types as softmax regression. The bag of phrases fea-
ture set once again provides the best performance although it
is slightly inferior to softmax. The advantage to SVM comes
from the signiﬁcantly lower polarity error. The overﬁtting of
the bag of phrases feature set is most apparent in SVM where
a 0% training set error rate is attained.

The relatively high polarity error for the best Naïve Bayes
and softmax regression models indicate that these are rela-
tively poor models for general application. When they do
make a mistake on the 5 level sentiment for a non-neutral
word they make a large number of big mistakes as reﬂected
by their roughly 40% polarity error rates.

KNN provides very poor results for all of the features
tested. The polarity error rate is higher than the 5 label er-
ror rate as the polarity error rate is evaluated on non-neutral
sentences only. The high error rate of KNN is likely due to
the Euclidean distance metric chosen; for high dimension bi-
nary vectors such as produced by the bag of words and bag
of phrases models, the distance between sentence vectors is
large and close to randomly distributed.

The bag of phrases feature set provides the best perfor-
mance for the SVM and softmax regression models but it is
not consistent as the error rate with both KNN and Naïve
Bayes is near 80%. The reduced feature set of bag of words
provides uniformly inferior performance to bag of phrases.
For SVM and softmax regression this diﬀerence due to the
context and phrasing that is incorporated by the bag of phrases
model provides a dramatic improvement in test error rate of
at least 15%. WF is much less variable with the training

6. FUTURE

If we were to expound on this project, we would try im-

plementing the following list.

• Develop additional features for use in tree reconstruc-

tion

• Explore deep learning for tree reconstruction of sen-

tences as pioneered by Socher et al.

• Apply classiﬁcation algorithms to phrases
• Add negation tracking to linear models
• Find a better distance metric for KNN
• Build up from a model of word compositions and build
towards sentences rather than decomposing sentences

References
[1] Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning,
C. D., Ng, A. Y., and Potts, C. Recursive deep models for se-
mantic compositionality. Empirical Methods in Natural Lan-
guage Processing (2013).

[2] Wiegand, M., Balahur, A., Roth, B., Klakow, D., and Mon-
toyo, A. A survey on the role of negation in sentiment analysis.
In Proceedings of the Workshop on Negation and Speculation
in Natural Language Processing (July 2010).

model used as it never has great performance but it also never
has the worst performance.

We attempted to use Principal Component Analysis (PCA)
in order to select the most relevant features from our mod-
els that exhibited overﬁtting. We did not get any beneﬁt from
this which is likely due to the unusual features being unlikely
to appear in the test set.

The learning curve for softmax regression show in Fig-
ure 4 shows a large separation between test and training er-
ror. The bag of phrases feature set depends heavily on those
phrases that are represented in the training set and the over-
lap of those phrases with the test set. With a dramatically
larger training set the training and test errors would converge
to some intermediate value. No matter how large the training
set is, however, such a model will be unable to properly de-
compose a test sentence with new and original phrasing. A
model that groups similar words and then trains their phras-
ing patterns has a much lower error rate limit and requires
much less training data.

The best literature result provides a 54.3% error rate for
sentences for 5 sentiment labels [1]. Our best result of 59%
is close but also notably below this level.

Our initial intention was to compare sentiment analysis
models that used a vector sentence representation to recur-
sive tree recombination models. We were able to conclude
that vector sentence representations are noticeably less ex-
pressive in terms of sentence structure and word context.
We thus started looking at applying models to tree recon-
struction. The general reconstruction problem is expressed
by training an W×W matrix where W is the number of words
in the training set dictionary. W is 22348 in our dataset
which leads to a matrix that is much too large to be well esti-
mated with our set of training sentences. The neural network
based approach introduced by Socher et al. appears to be a
very promising way to reduce the recombination problem to
a manageable form [1]. We took an alternate approach by
reducing the feature set of the recombination matrix. The
reduction of the matrix to a set of 5 sentiment labels plus
a negation label makes the model very similar to the vector
sum of sentiments model and the resulting performance is
also similar.

5. CONCLUSION

We found that a bag of phrases feature set with an SVM
model provided the best results among vector based sentence
models. While negation tracking did not provide any bene-
ﬁts in our experiments, negation is fundamental to the in-
terpreted sentiment of many of our test sentences. A better
method of handling negation that potentially also accounts
for contrastive sentence structures will certainly improve per-
formance. The additional ﬂexibility of tree based represen-
tations appears to be the best future path for sentiment anal-
ysis.

5

