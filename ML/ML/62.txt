Dynamic Eye Gaze Tracking for Foveated Rendering and Retinal Blur

Kushagr Gupta, Suleman Kazi, Terry Kong

Abstract— This project deals with tracking eye gaze location
and predicting future eye movement sequences using classical
machine learning techniques and Hidden Markov Models,
respectively, for users wearing head mounted displays (HMD).
Eye gaze location can enhance the virtual reality experience
inside an HMD. In this report, we discuss how we collected
a dataset for eye gaze tracking and the features we extracted.
Then, we evaluated three learning approaches to estimate eye
gaze and evaluate the performance using regularized linear re-
gression, support vector regression (SVR), and neural networks.
We also brieﬂy discuss our approach to the prediction problem.

probabilities of where the user is likely to look at) the user
views to estimate eye gaze instead and is able to achieve an
accuracy of <3.0◦. State of the art eye-tracking hardware [7]
is advertised as being able to achieve a typical accuracy of
accuracy of 0.5◦. In our work, instead of relying on signal
or image processing algorithms we use machine learning
techniques and instead of relying on one, we also compare
different learning algorithms and quantify their results as
described in the sections below.

I. INTRODUCTION

III. DATASET AND FEATURES

The ability to track and predict eye gaze accurately can
have a huge impact in Virtual Reality (VR) headsets since
it gives content developers the ﬂexibility to enable foveated
rendering or simulate retinal blur to enhance content and
improve interactivity using eye-gaze information. A recent
HMD project with integrated eye tracking [6] raised well in
excess of its funding goal via kickstarter. Foveated rendering
is a technique in which the image resolution is not uniform
across the image. Retinal blur refers to blurred perception of
objects outside the center of gaze (in the peripheral vision).
In order to apply these techniques the eye gaze location
must be accurately known. To provide enough time for
rendering and to account for latency the eye gaze location
in the next few frames must also be predicted. This requires
high accuracy so that the scene is rendered and blurred at
the right locations. We use machine learning techniques to
output an estimated eye gaze location in terms of screen x-y
coordinates.

This is a joint project for CS229 and CS221. The work
shown in this report explains the eye gaze tracking portion
of the project which was done for CS229. The approach to
predict the eye gaze location constitutes the CS221 portion
of the project and is very brieﬂy summarized at the end to
connect the two projects. For a much more in depth analysis
of the prediction approach, we refer the reader to our report
for CS221.

II. RELATED WORK

A large number of the modern eye tracking algorithms we
came across [1]–[4] use signal/image processing in conjunc-
tion with geometry to accomplish eye gaze tracking. In fact
[5] which is a survey of models for eye gaze tracking does
not list any machine learning methods. [8] uses Artiﬁcial
Neural Networks for eye gaze tracking and is able to achieve
an accuracy of 1.5◦. [9] also uses a probabilistic incremental
learning algorithm to estimate eye gaze but instead of looking
at
images of the user’s eyes to do so it relies on pre-
computed saliency maps of the videos (showing the prior

A. Dataset Acquisition

The dataset consists of images of the user’s eye looking at
points displayed on a screen with a resolution of 1920x1080
pixels and a size of 16.5x30.5 cm. These points serve as
the ground truth data. The user sits 30cm away from the
screen, wearing a pair of glasses with a webcam mounted
on them 5cm from the eye and off-axis as to not obstruct
the screen. By using this method we did not have to worry
about inadequate illumination inside a VR headset whilst
also getting images similar to what a camera inside a VR
headset would produce. We also looked up online datasets
but they did not ﬁt our desired input data requirements. For
example the dataset in [16] gives the eye gaze location (x,y)
values on the screen but doesnt give images of the eye. The
dataset in [17] gives images of eye but with camera right in
front of the eye thus obstructing content on screen. Our data
collection setup is shown in Fig. 1.

Fig. 1. Setup For Data Collection

1) As a one time procedure user is asked to look straight
at the camera and a reference image is captured. A box
is manually drawn around the position of the user’s eye
and the image is cropped to this box and saved.

2) Proceeding to collect a dataset user must take another
reference image. The master reference obtained in step
1 is cross correlated with this reference image to ﬁnd
the location of the user’s eye and a box of the same size
as the master reference is drawn around the eye auto-
matically. This step removes slight variations of user
head position between different dataset acquisitions.

Dynamic Eye Gaze Tracking for Foveated Rendering and Retinal Blur

Kushagr Gupta, Suleman Kazi, Terry Kong

Abstract— This project deals with tracking eye gaze location
and predicting future eye movement sequences using classical
machine learning techniques and Hidden Markov Models,
respectively, for users wearing head mounted displays (HMD).
Eye gaze location can enhance the virtual reality experience
inside an HMD. In this report, we discuss how we collected
a dataset for eye gaze tracking and the features we extracted.
Then, we evaluated three learning approaches to estimate eye
gaze and evaluate the performance using regularized linear re-
gression, support vector regression (SVR), and neural networks.
We also brieﬂy discuss our approach to the prediction problem.

probabilities of where the user is likely to look at) the user
views to estimate eye gaze instead and is able to achieve an
accuracy of <3.0◦. State of the art eye-tracking hardware [7]
is advertised as being able to achieve a typical accuracy of
accuracy of 0.5◦. In our work, instead of relying on signal
or image processing algorithms we use machine learning
techniques and instead of relying on one, we also compare
different learning algorithms and quantify their results as
described in the sections below.

I. INTRODUCTION

III. DATASET AND FEATURES

The ability to track and predict eye gaze accurately can
have a huge impact in Virtual Reality (VR) headsets since
it gives content developers the ﬂexibility to enable foveated
rendering or simulate retinal blur to enhance content and
improve interactivity using eye-gaze information. A recent
HMD project with integrated eye tracking [6] raised well in
excess of its funding goal via kickstarter. Foveated rendering
is a technique in which the image resolution is not uniform
across the image. Retinal blur refers to blurred perception of
objects outside the center of gaze (in the peripheral vision).
In order to apply these techniques the eye gaze location
must be accurately known. To provide enough time for
rendering and to account for latency the eye gaze location
in the next few frames must also be predicted. This requires
high accuracy so that the scene is rendered and blurred at
the right locations. We use machine learning techniques to
output an estimated eye gaze location in terms of screen x-y
coordinates.

This is a joint project for CS229 and CS221. The work
shown in this report explains the eye gaze tracking portion
of the project which was done for CS229. The approach to
predict the eye gaze location constitutes the CS221 portion
of the project and is very brieﬂy summarized at the end to
connect the two projects. For a much more in depth analysis
of the prediction approach, we refer the reader to our report
for CS221.

II. RELATED WORK

A large number of the modern eye tracking algorithms we
came across [1]–[4] use signal/image processing in conjunc-
tion with geometry to accomplish eye gaze tracking. In fact
[5] which is a survey of models for eye gaze tracking does
not list any machine learning methods. [8] uses Artiﬁcial
Neural Networks for eye gaze tracking and is able to achieve
an accuracy of 1.5◦. [9] also uses a probabilistic incremental
learning algorithm to estimate eye gaze but instead of looking
at
images of the user’s eyes to do so it relies on pre-
computed saliency maps of the videos (showing the prior

A. Dataset Acquisition

The dataset consists of images of the user’s eye looking at
points displayed on a screen with a resolution of 1920x1080
pixels and a size of 16.5x30.5 cm. These points serve as
the ground truth data. The user sits 30cm away from the
screen, wearing a pair of glasses with a webcam mounted
on them 5cm from the eye and off-axis as to not obstruct
the screen. By using this method we did not have to worry
about inadequate illumination inside a VR headset whilst
also getting images similar to what a camera inside a VR
headset would produce. We also looked up online datasets
but they did not ﬁt our desired input data requirements. For
example the dataset in [16] gives the eye gaze location (x,y)
values on the screen but doesnt give images of the eye. The
dataset in [17] gives images of eye but with camera right in
front of the eye thus obstructing content on screen. Our data
collection setup is shown in Fig. 1.

Fig. 1. Setup For Data Collection

1) As a one time procedure user is asked to look straight
at the camera and a reference image is captured. A box
is manually drawn around the position of the user’s eye
and the image is cropped to this box and saved.

2) Proceeding to collect a dataset user must take another
reference image. The master reference obtained in step
1 is cross correlated with this reference image to ﬁnd
the location of the user’s eye and a box of the same size
as the master reference is drawn around the eye auto-
matically. This step removes slight variations of user
head position between different dataset acquisitions.

3) The user is then shown a random point on the screen
and each key press captures an image and displays
a new point on the screen. Each captured image is
cropped based on the bounding box calculated in step
2 and saved along with the location of the point shown
on screen.

B. Feature Selection

Initially all the images were converted to grayscale and
vectorized and used as features but it resulted in too many
features compared to the training set size and thus each image
was down sampled and vectorized which serve as features for
the algorithm. Different down sampling factors for resizing
the images were considered and it was observed that the
performance gain after a certain feature length was marginal
and the training time became too prohibitive. We thus chose a
downsampling factor that performed reasonably well giving
a ﬁnal feature length of 300 as compared to the original
image size of 300000 pixels as shown in Fig. 2.

We introduced two additional sets of features namely the
inner product with the eigen-eyes and the inner product with
the ﬁsher-eyes [19] which help as they increase variance
using Principal Component Analysis. They are explained as
follows.

1) Eigen Eyes : Given the vectorized training images the
mean of the entire set µ is removed and output is
concatenated into a matrix, say S, of dimension number
of pixels by number of training images. Eigen eyes are
essentially the eigen-vectors of the scatter matrix S∗S(cid:48).
This gives us the directions in which there is higher
variance based on the eigen values with an illustration
in 2.

2) Fisher Eyes : In eigen eyes we consider the entire
dataset and discriminate based on it, we can improve
this using ﬁsher analysis. The aim here is to maximize
between class scatter while minimizing within class
scatter. For the purpose of eye gaze tracking, we
considered a ﬁnite set of classes based on the true
locations of target point where the eyes are looking
at and each image (mean removed) is binned into one
of these classes. The generalized eigen vectors of the
problem are called ﬁsher eyes with an illustration in 2.
While the initial computation of all the eigen-eyes and
ﬁsher-eyes may take some time, they will only need
to be computed once at the beginning using all the
training data.

Fig. 2. Raw Image, Downsampled Image, Eigen Eye, Fisher Eye

IV. METHODS: TRACKING APPROACH

To evaluate our tracking approach, we compared three
different
learning methods: regularized linear regression,
SVR, and neural networks. In all three cases we treat the

x and y dimensions of the eye gaze location independently.
While we could consider the dimensions together, there is
no added value. As the results will show, using least squares
where the labels are multidimensional (x and y) actually
performs worse which justiﬁes our assumption.
A. LEAST SQUARES WITH REGULARIZATION

Least squares, or linear regression, was used as a baseline
due to its simplicity and straightforward implementation.
Least squares assumes that the data can be modeled as

p = Xw

where p is the eye gaze location (x or y), X is the design
matrix and w is the weight vector. Since least squares can
lead to over-ﬁtting, we also considered regularized least
squares, which has an extra term augmented to the least
squares objective. With µ = 0 it reduces to linear regression
without regularization.

JLSR(w) = (cid:107)p − Xw(cid:107)2

2 + µ(cid:107)w(cid:107)2

2

The hyper-parameter µ controls how much over-ﬁtting we
will tolerate. This modiﬁed objective function is equivalent
to assuming a Gaussian prior on the weights with zero mean
and non-zero variance.

Taking gradients yields the closed form solution:1

wLSR = (X T X + µI)−1X T p

where I is the n by n identity matrix and n is the dimension
of a feature vector.

Both of these results assume that X is full rank and skinny.
While this isn’t always the case, we can always perform SVD
or QR factorization to remove the redundant columns in the
X. Another approach is to pass the optimization problem
into a convex solver, which will likely perform Newton’s
method which doesn’t require any priors on the data.
B. SUPPORT VECTOR REGRESSION

Another approach is to use SVR. The problem inherits
part of its name from the popular support vector machine
problem. Like most regression problems,
the hypothesis
function is of the form h(x) = wT x + b where w is the
weight vector and b is the constant offset.

To use this model, one must select a tolerance, , and
a hyper-parameter for the slack variables, C, and then the
weights are learned to minimize the objective. The physical
meaning of  is the amount of deviation that is tolerated
when evaluating with the hypothesis function.

This problem can be solved more efﬁciently in the dual
domain and is explained more thoroughly in [14]. The
corresponding dual problem is

i )(αj − α∗

i ) +(cid:80)m

j ) < xi, xj >
i=1 y(i)(αi − α∗
i )
i ∈ [0, C]

i ) = 0 and αi, α∗

(cid:40)− 1
(cid:80)m
−(cid:80)m
i,j=1(αi − α∗
m(cid:88)
i=1(αi − α∗

(αi − α∗

2

maximize

subject to

i=1

(1)

1This also assumes that the design matrix is skinny and full rank.

Dynamic Eye Gaze Tracking for Foveated Rendering and Retinal Blur

Kushagr Gupta, Suleman Kazi, Terry Kong

Abstract— This project deals with tracking eye gaze location
and predicting future eye movement sequences using classical
machine learning techniques and Hidden Markov Models,
respectively, for users wearing head mounted displays (HMD).
Eye gaze location can enhance the virtual reality experience
inside an HMD. In this report, we discuss how we collected
a dataset for eye gaze tracking and the features we extracted.
Then, we evaluated three learning approaches to estimate eye
gaze and evaluate the performance using regularized linear re-
gression, support vector regression (SVR), and neural networks.
We also brieﬂy discuss our approach to the prediction problem.

probabilities of where the user is likely to look at) the user
views to estimate eye gaze instead and is able to achieve an
accuracy of <3.0◦. State of the art eye-tracking hardware [7]
is advertised as being able to achieve a typical accuracy of
accuracy of 0.5◦. In our work, instead of relying on signal
or image processing algorithms we use machine learning
techniques and instead of relying on one, we also compare
different learning algorithms and quantify their results as
described in the sections below.

I. INTRODUCTION

III. DATASET AND FEATURES

The ability to track and predict eye gaze accurately can
have a huge impact in Virtual Reality (VR) headsets since
it gives content developers the ﬂexibility to enable foveated
rendering or simulate retinal blur to enhance content and
improve interactivity using eye-gaze information. A recent
HMD project with integrated eye tracking [6] raised well in
excess of its funding goal via kickstarter. Foveated rendering
is a technique in which the image resolution is not uniform
across the image. Retinal blur refers to blurred perception of
objects outside the center of gaze (in the peripheral vision).
In order to apply these techniques the eye gaze location
must be accurately known. To provide enough time for
rendering and to account for latency the eye gaze location
in the next few frames must also be predicted. This requires
high accuracy so that the scene is rendered and blurred at
the right locations. We use machine learning techniques to
output an estimated eye gaze location in terms of screen x-y
coordinates.

This is a joint project for CS229 and CS221. The work
shown in this report explains the eye gaze tracking portion
of the project which was done for CS229. The approach to
predict the eye gaze location constitutes the CS221 portion
of the project and is very brieﬂy summarized at the end to
connect the two projects. For a much more in depth analysis
of the prediction approach, we refer the reader to our report
for CS221.

II. RELATED WORK

A large number of the modern eye tracking algorithms we
came across [1]–[4] use signal/image processing in conjunc-
tion with geometry to accomplish eye gaze tracking. In fact
[5] which is a survey of models for eye gaze tracking does
not list any machine learning methods. [8] uses Artiﬁcial
Neural Networks for eye gaze tracking and is able to achieve
an accuracy of 1.5◦. [9] also uses a probabilistic incremental
learning algorithm to estimate eye gaze but instead of looking
at
images of the user’s eyes to do so it relies on pre-
computed saliency maps of the videos (showing the prior

A. Dataset Acquisition

The dataset consists of images of the user’s eye looking at
points displayed on a screen with a resolution of 1920x1080
pixels and a size of 16.5x30.5 cm. These points serve as
the ground truth data. The user sits 30cm away from the
screen, wearing a pair of glasses with a webcam mounted
on them 5cm from the eye and off-axis as to not obstruct
the screen. By using this method we did not have to worry
about inadequate illumination inside a VR headset whilst
also getting images similar to what a camera inside a VR
headset would produce. We also looked up online datasets
but they did not ﬁt our desired input data requirements. For
example the dataset in [16] gives the eye gaze location (x,y)
values on the screen but doesnt give images of the eye. The
dataset in [17] gives images of eye but with camera right in
front of the eye thus obstructing content on screen. Our data
collection setup is shown in Fig. 1.

Fig. 1. Setup For Data Collection

1) As a one time procedure user is asked to look straight
at the camera and a reference image is captured. A box
is manually drawn around the position of the user’s eye
and the image is cropped to this box and saved.

2) Proceeding to collect a dataset user must take another
reference image. The master reference obtained in step
1 is cross correlated with this reference image to ﬁnd
the location of the user’s eye and a box of the same size
as the master reference is drawn around the eye auto-
matically. This step removes slight variations of user
head position between different dataset acquisitions.

3) The user is then shown a random point on the screen
and each key press captures an image and displays
a new point on the screen. Each captured image is
cropped based on the bounding box calculated in step
2 and saved along with the location of the point shown
on screen.

B. Feature Selection

Initially all the images were converted to grayscale and
vectorized and used as features but it resulted in too many
features compared to the training set size and thus each image
was down sampled and vectorized which serve as features for
the algorithm. Different down sampling factors for resizing
the images were considered and it was observed that the
performance gain after a certain feature length was marginal
and the training time became too prohibitive. We thus chose a
downsampling factor that performed reasonably well giving
a ﬁnal feature length of 300 as compared to the original
image size of 300000 pixels as shown in Fig. 2.

We introduced two additional sets of features namely the
inner product with the eigen-eyes and the inner product with
the ﬁsher-eyes [19] which help as they increase variance
using Principal Component Analysis. They are explained as
follows.

1) Eigen Eyes : Given the vectorized training images the
mean of the entire set µ is removed and output is
concatenated into a matrix, say S, of dimension number
of pixels by number of training images. Eigen eyes are
essentially the eigen-vectors of the scatter matrix S∗S(cid:48).
This gives us the directions in which there is higher
variance based on the eigen values with an illustration
in 2.

2) Fisher Eyes : In eigen eyes we consider the entire
dataset and discriminate based on it, we can improve
this using ﬁsher analysis. The aim here is to maximize
between class scatter while minimizing within class
scatter. For the purpose of eye gaze tracking, we
considered a ﬁnite set of classes based on the true
locations of target point where the eyes are looking
at and each image (mean removed) is binned into one
of these classes. The generalized eigen vectors of the
problem are called ﬁsher eyes with an illustration in 2.
While the initial computation of all the eigen-eyes and
ﬁsher-eyes may take some time, they will only need
to be computed once at the beginning using all the
training data.

Fig. 2. Raw Image, Downsampled Image, Eigen Eye, Fisher Eye

IV. METHODS: TRACKING APPROACH

To evaluate our tracking approach, we compared three
different
learning methods: regularized linear regression,
SVR, and neural networks. In all three cases we treat the

x and y dimensions of the eye gaze location independently.
While we could consider the dimensions together, there is
no added value. As the results will show, using least squares
where the labels are multidimensional (x and y) actually
performs worse which justiﬁes our assumption.
A. LEAST SQUARES WITH REGULARIZATION

Least squares, or linear regression, was used as a baseline
due to its simplicity and straightforward implementation.
Least squares assumes that the data can be modeled as

p = Xw

where p is the eye gaze location (x or y), X is the design
matrix and w is the weight vector. Since least squares can
lead to over-ﬁtting, we also considered regularized least
squares, which has an extra term augmented to the least
squares objective. With µ = 0 it reduces to linear regression
without regularization.

JLSR(w) = (cid:107)p − Xw(cid:107)2

2 + µ(cid:107)w(cid:107)2

2

The hyper-parameter µ controls how much over-ﬁtting we
will tolerate. This modiﬁed objective function is equivalent
to assuming a Gaussian prior on the weights with zero mean
and non-zero variance.

Taking gradients yields the closed form solution:1

wLSR = (X T X + µI)−1X T p

where I is the n by n identity matrix and n is the dimension
of a feature vector.

Both of these results assume that X is full rank and skinny.
While this isn’t always the case, we can always perform SVD
or QR factorization to remove the redundant columns in the
X. Another approach is to pass the optimization problem
into a convex solver, which will likely perform Newton’s
method which doesn’t require any priors on the data.
B. SUPPORT VECTOR REGRESSION

Another approach is to use SVR. The problem inherits
part of its name from the popular support vector machine
problem. Like most regression problems,
the hypothesis
function is of the form h(x) = wT x + b where w is the
weight vector and b is the constant offset.

To use this model, one must select a tolerance, , and
a hyper-parameter for the slack variables, C, and then the
weights are learned to minimize the objective. The physical
meaning of  is the amount of deviation that is tolerated
when evaluating with the hypothesis function.

This problem can be solved more efﬁciently in the dual
domain and is explained more thoroughly in [14]. The
corresponding dual problem is

i )(αj − α∗

i ) +(cid:80)m

j ) < xi, xj >
i=1 y(i)(αi − α∗
i )
i ∈ [0, C]

i ) = 0 and αi, α∗

(cid:40)− 1
(cid:80)m
−(cid:80)m
i,j=1(αi − α∗
m(cid:88)
i=1(αi − α∗

(αi − α∗

2

maximize

subject to

i=1

(1)

1This also assumes that the design matrix is skinny and full rank.

where x(i) is a training feature vector, y(i), m is the number
of training examples, αi and α∗
i are the dual variables. One
nuance to the dual problem is that there is no longer a
mention of the constant term b. Thus, it needs to be inferred
from the solution of the dual problem. For the purpose of
this report, we have set2

bmax + bmin

b =

2

m(cid:88)

where bmax = max{− + y(i)− < w, x(i)|ai < C or a∗
0}, bmin = min{− + y(i)− < w, x(i)|ai > 0 or a∗
and < ·,· > represents a kernel.
Using the solution of the dual problem (αi and α∗

i >
i < C},

i ), we

can construct the weight vectors as follows:

w =

(αi − α∗

i ) < x(i), x(i) >

i=1

Like SVM, SVR can also be kernelized.

C. NEURAL NETWORKS

Neural Networks consist neurons which take in inputs
(weighted by a weight vector which may be different for
each neuron) and process them using a transfer function
to give an output. A neural network consists of a number
of interconnected neurons. We use a feed-forward neural
network in which the information ﬂow is unidirectional. If
we have a neuron j with a sigmoid transfer function, a weight
vector wj = [w1j....wnj]T , bias term b and inputs ok, from
the previous neurons then the output for neuron j will be:

Where α is the learning rate. We used MATLAB’s neural
network framework to design, train, visualize, and simulate
the network. We have a two-layer feed forward neural
network. The hidden layer neurons have a sigmoid transfer
function and the output layer neurons have a linear transfer
function. A diagram of the ﬁnal neural network selected is
shown in Fig. 3

Fig. 3. Neural Network

V. RESULTS AND ANALYSIS

All our algorithms ﬁnd average MSE in terms of the x and
y locations of the points shown on screen. This is generalized
to yield values in terms of viewing angle error. Horizontal
and Vertical ﬁeld of view is calculated using the screen size
and distance of user from the screen which came out to be
52◦ and 30◦ respectively. Using the discretized number of
bins for both directions we convert the L2 error in terms of
viewing angle error as explained for one axis in [].

(cid:18)

(cid:19)

θx = tan−1

tan(HorF OV ◦)

AvgL2Dist
#HorBins

(2)

1

1 + exp(−z)

The dimension of the feature vector for all of the testing
and training data used for all cross validation is 330. The
makeup of the 330 features is:

oj = φ(z) = φ(netj) =

Where:

z =

n(cid:88)

wkjok

k=1

The network is trained using a back-propagation algorithm
which uses gradient descent to optimize the weights. The
squared error in this case is:
1
2

(t − y)2

E =

Where t is the desired output for an input vector and y is
the actual output.From [18] we see that if we take the partial
derivatives using the chain rule we get:

∂E
∂wij

= δjoi



Where:

δj =

∂E
∂oj

(oj − tj)φ(netj)(1 − φ(netj))
(cid:80)
if j is an output neuron
l∈L δlwjlφ(netj)(1 − φ(netj))
if j is an inner neuron
Then the weight update equation is simply:

∂oj
∂netj

∆wij = −α

∂E
∂wij

2This is inspired from [14]

• 300 features make up the vectorization of the 15x20
the eye from the original

downsampled image of
480x640

• 20 features are the coefﬁcients of the top 20 eigen eyes
• 10 features are the coefﬁcients of the top 10 ﬁsher eyes

A. HYPER-PARAMETER SELECTION

To select the hyper-parameters for each model of each
approach, we perform hold-out cross validation where we
train on a random subset of 70% of the data and test on
30% on the data. We then select the hyper-parameters that
correspond to the model that has the smallest test error.

We use hold-out cross validation instead of k-fold cross
validation because the time to train the SVR model and the
neural network on MATLAB was prohibitively long.

1) REGULARIZED LEAST SQUARES: For regularized
least squares, there is one hyper-parameter µ, which controls
how much the model cares about over-ﬁtting. The trade-off
curve of hyper-parameters to test error is show in Fig. 4 The
minimum on this curve is achieved with the model where
µ = 0.003232. Note that the model where µ = 0 corresponds
to the least squares model, without regularization. The fact
that the minimum occurs when µ is non-zero means that
the regularization drove some of the weights down and that
improved the model. This means that there are a number of

Dynamic Eye Gaze Tracking for Foveated Rendering and Retinal Blur

Kushagr Gupta, Suleman Kazi, Terry Kong

Abstract— This project deals with tracking eye gaze location
and predicting future eye movement sequences using classical
machine learning techniques and Hidden Markov Models,
respectively, for users wearing head mounted displays (HMD).
Eye gaze location can enhance the virtual reality experience
inside an HMD. In this report, we discuss how we collected
a dataset for eye gaze tracking and the features we extracted.
Then, we evaluated three learning approaches to estimate eye
gaze and evaluate the performance using regularized linear re-
gression, support vector regression (SVR), and neural networks.
We also brieﬂy discuss our approach to the prediction problem.

probabilities of where the user is likely to look at) the user
views to estimate eye gaze instead and is able to achieve an
accuracy of <3.0◦. State of the art eye-tracking hardware [7]
is advertised as being able to achieve a typical accuracy of
accuracy of 0.5◦. In our work, instead of relying on signal
or image processing algorithms we use machine learning
techniques and instead of relying on one, we also compare
different learning algorithms and quantify their results as
described in the sections below.

I. INTRODUCTION

III. DATASET AND FEATURES

The ability to track and predict eye gaze accurately can
have a huge impact in Virtual Reality (VR) headsets since
it gives content developers the ﬂexibility to enable foveated
rendering or simulate retinal blur to enhance content and
improve interactivity using eye-gaze information. A recent
HMD project with integrated eye tracking [6] raised well in
excess of its funding goal via kickstarter. Foveated rendering
is a technique in which the image resolution is not uniform
across the image. Retinal blur refers to blurred perception of
objects outside the center of gaze (in the peripheral vision).
In order to apply these techniques the eye gaze location
must be accurately known. To provide enough time for
rendering and to account for latency the eye gaze location
in the next few frames must also be predicted. This requires
high accuracy so that the scene is rendered and blurred at
the right locations. We use machine learning techniques to
output an estimated eye gaze location in terms of screen x-y
coordinates.

This is a joint project for CS229 and CS221. The work
shown in this report explains the eye gaze tracking portion
of the project which was done for CS229. The approach to
predict the eye gaze location constitutes the CS221 portion
of the project and is very brieﬂy summarized at the end to
connect the two projects. For a much more in depth analysis
of the prediction approach, we refer the reader to our report
for CS221.

II. RELATED WORK

A large number of the modern eye tracking algorithms we
came across [1]–[4] use signal/image processing in conjunc-
tion with geometry to accomplish eye gaze tracking. In fact
[5] which is a survey of models for eye gaze tracking does
not list any machine learning methods. [8] uses Artiﬁcial
Neural Networks for eye gaze tracking and is able to achieve
an accuracy of 1.5◦. [9] also uses a probabilistic incremental
learning algorithm to estimate eye gaze but instead of looking
at
images of the user’s eyes to do so it relies on pre-
computed saliency maps of the videos (showing the prior

A. Dataset Acquisition

The dataset consists of images of the user’s eye looking at
points displayed on a screen with a resolution of 1920x1080
pixels and a size of 16.5x30.5 cm. These points serve as
the ground truth data. The user sits 30cm away from the
screen, wearing a pair of glasses with a webcam mounted
on them 5cm from the eye and off-axis as to not obstruct
the screen. By using this method we did not have to worry
about inadequate illumination inside a VR headset whilst
also getting images similar to what a camera inside a VR
headset would produce. We also looked up online datasets
but they did not ﬁt our desired input data requirements. For
example the dataset in [16] gives the eye gaze location (x,y)
values on the screen but doesnt give images of the eye. The
dataset in [17] gives images of eye but with camera right in
front of the eye thus obstructing content on screen. Our data
collection setup is shown in Fig. 1.

Fig. 1. Setup For Data Collection

1) As a one time procedure user is asked to look straight
at the camera and a reference image is captured. A box
is manually drawn around the position of the user’s eye
and the image is cropped to this box and saved.

2) Proceeding to collect a dataset user must take another
reference image. The master reference obtained in step
1 is cross correlated with this reference image to ﬁnd
the location of the user’s eye and a box of the same size
as the master reference is drawn around the eye auto-
matically. This step removes slight variations of user
head position between different dataset acquisitions.

3) The user is then shown a random point on the screen
and each key press captures an image and displays
a new point on the screen. Each captured image is
cropped based on the bounding box calculated in step
2 and saved along with the location of the point shown
on screen.

B. Feature Selection

Initially all the images were converted to grayscale and
vectorized and used as features but it resulted in too many
features compared to the training set size and thus each image
was down sampled and vectorized which serve as features for
the algorithm. Different down sampling factors for resizing
the images were considered and it was observed that the
performance gain after a certain feature length was marginal
and the training time became too prohibitive. We thus chose a
downsampling factor that performed reasonably well giving
a ﬁnal feature length of 300 as compared to the original
image size of 300000 pixels as shown in Fig. 2.

We introduced two additional sets of features namely the
inner product with the eigen-eyes and the inner product with
the ﬁsher-eyes [19] which help as they increase variance
using Principal Component Analysis. They are explained as
follows.

1) Eigen Eyes : Given the vectorized training images the
mean of the entire set µ is removed and output is
concatenated into a matrix, say S, of dimension number
of pixels by number of training images. Eigen eyes are
essentially the eigen-vectors of the scatter matrix S∗S(cid:48).
This gives us the directions in which there is higher
variance based on the eigen values with an illustration
in 2.

2) Fisher Eyes : In eigen eyes we consider the entire
dataset and discriminate based on it, we can improve
this using ﬁsher analysis. The aim here is to maximize
between class scatter while minimizing within class
scatter. For the purpose of eye gaze tracking, we
considered a ﬁnite set of classes based on the true
locations of target point where the eyes are looking
at and each image (mean removed) is binned into one
of these classes. The generalized eigen vectors of the
problem are called ﬁsher eyes with an illustration in 2.
While the initial computation of all the eigen-eyes and
ﬁsher-eyes may take some time, they will only need
to be computed once at the beginning using all the
training data.

Fig. 2. Raw Image, Downsampled Image, Eigen Eye, Fisher Eye

IV. METHODS: TRACKING APPROACH

To evaluate our tracking approach, we compared three
different
learning methods: regularized linear regression,
SVR, and neural networks. In all three cases we treat the

x and y dimensions of the eye gaze location independently.
While we could consider the dimensions together, there is
no added value. As the results will show, using least squares
where the labels are multidimensional (x and y) actually
performs worse which justiﬁes our assumption.
A. LEAST SQUARES WITH REGULARIZATION

Least squares, or linear regression, was used as a baseline
due to its simplicity and straightforward implementation.
Least squares assumes that the data can be modeled as

p = Xw

where p is the eye gaze location (x or y), X is the design
matrix and w is the weight vector. Since least squares can
lead to over-ﬁtting, we also considered regularized least
squares, which has an extra term augmented to the least
squares objective. With µ = 0 it reduces to linear regression
without regularization.

JLSR(w) = (cid:107)p − Xw(cid:107)2

2 + µ(cid:107)w(cid:107)2

2

The hyper-parameter µ controls how much over-ﬁtting we
will tolerate. This modiﬁed objective function is equivalent
to assuming a Gaussian prior on the weights with zero mean
and non-zero variance.

Taking gradients yields the closed form solution:1

wLSR = (X T X + µI)−1X T p

where I is the n by n identity matrix and n is the dimension
of a feature vector.

Both of these results assume that X is full rank and skinny.
While this isn’t always the case, we can always perform SVD
or QR factorization to remove the redundant columns in the
X. Another approach is to pass the optimization problem
into a convex solver, which will likely perform Newton’s
method which doesn’t require any priors on the data.
B. SUPPORT VECTOR REGRESSION

Another approach is to use SVR. The problem inherits
part of its name from the popular support vector machine
problem. Like most regression problems,
the hypothesis
function is of the form h(x) = wT x + b where w is the
weight vector and b is the constant offset.

To use this model, one must select a tolerance, , and
a hyper-parameter for the slack variables, C, and then the
weights are learned to minimize the objective. The physical
meaning of  is the amount of deviation that is tolerated
when evaluating with the hypothesis function.

This problem can be solved more efﬁciently in the dual
domain and is explained more thoroughly in [14]. The
corresponding dual problem is

i )(αj − α∗

i ) +(cid:80)m

j ) < xi, xj >
i=1 y(i)(αi − α∗
i )
i ∈ [0, C]

i ) = 0 and αi, α∗

(cid:40)− 1
(cid:80)m
−(cid:80)m
i,j=1(αi − α∗
m(cid:88)
i=1(αi − α∗

(αi − α∗

2

maximize

subject to

i=1

(1)

1This also assumes that the design matrix is skinny and full rank.

where x(i) is a training feature vector, y(i), m is the number
of training examples, αi and α∗
i are the dual variables. One
nuance to the dual problem is that there is no longer a
mention of the constant term b. Thus, it needs to be inferred
from the solution of the dual problem. For the purpose of
this report, we have set2

bmax + bmin

b =

2

m(cid:88)

where bmax = max{− + y(i)− < w, x(i)|ai < C or a∗
0}, bmin = min{− + y(i)− < w, x(i)|ai > 0 or a∗
and < ·,· > represents a kernel.
Using the solution of the dual problem (αi and α∗

i >
i < C},

i ), we

can construct the weight vectors as follows:

w =

(αi − α∗

i ) < x(i), x(i) >

i=1

Like SVM, SVR can also be kernelized.

C. NEURAL NETWORKS

Neural Networks consist neurons which take in inputs
(weighted by a weight vector which may be different for
each neuron) and process them using a transfer function
to give an output. A neural network consists of a number
of interconnected neurons. We use a feed-forward neural
network in which the information ﬂow is unidirectional. If
we have a neuron j with a sigmoid transfer function, a weight
vector wj = [w1j....wnj]T , bias term b and inputs ok, from
the previous neurons then the output for neuron j will be:

Where α is the learning rate. We used MATLAB’s neural
network framework to design, train, visualize, and simulate
the network. We have a two-layer feed forward neural
network. The hidden layer neurons have a sigmoid transfer
function and the output layer neurons have a linear transfer
function. A diagram of the ﬁnal neural network selected is
shown in Fig. 3

Fig. 3. Neural Network

V. RESULTS AND ANALYSIS

All our algorithms ﬁnd average MSE in terms of the x and
y locations of the points shown on screen. This is generalized
to yield values in terms of viewing angle error. Horizontal
and Vertical ﬁeld of view is calculated using the screen size
and distance of user from the screen which came out to be
52◦ and 30◦ respectively. Using the discretized number of
bins for both directions we convert the L2 error in terms of
viewing angle error as explained for one axis in [].

(cid:18)

(cid:19)

θx = tan−1

tan(HorF OV ◦)

AvgL2Dist
#HorBins

(2)

1

1 + exp(−z)

The dimension of the feature vector for all of the testing
and training data used for all cross validation is 330. The
makeup of the 330 features is:

oj = φ(z) = φ(netj) =

Where:

z =

n(cid:88)

wkjok

k=1

The network is trained using a back-propagation algorithm
which uses gradient descent to optimize the weights. The
squared error in this case is:
1
2

(t − y)2

E =

Where t is the desired output for an input vector and y is
the actual output.From [18] we see that if we take the partial
derivatives using the chain rule we get:

∂E
∂wij

= δjoi



Where:

δj =

∂E
∂oj

(oj − tj)φ(netj)(1 − φ(netj))
(cid:80)
if j is an output neuron
l∈L δlwjlφ(netj)(1 − φ(netj))
if j is an inner neuron
Then the weight update equation is simply:

∂oj
∂netj

∆wij = −α

∂E
∂wij

2This is inspired from [14]

• 300 features make up the vectorization of the 15x20
the eye from the original

downsampled image of
480x640

• 20 features are the coefﬁcients of the top 20 eigen eyes
• 10 features are the coefﬁcients of the top 10 ﬁsher eyes

A. HYPER-PARAMETER SELECTION

To select the hyper-parameters for each model of each
approach, we perform hold-out cross validation where we
train on a random subset of 70% of the data and test on
30% on the data. We then select the hyper-parameters that
correspond to the model that has the smallest test error.

We use hold-out cross validation instead of k-fold cross
validation because the time to train the SVR model and the
neural network on MATLAB was prohibitively long.

1) REGULARIZED LEAST SQUARES: For regularized
least squares, there is one hyper-parameter µ, which controls
how much the model cares about over-ﬁtting. The trade-off
curve of hyper-parameters to test error is show in Fig. 4 The
minimum on this curve is achieved with the model where
µ = 0.003232. Note that the model where µ = 0 corresponds
to the least squares model, without regularization. The fact
that the minimum occurs when µ is non-zero means that
the regularization drove some of the weights down and that
improved the model. This means that there are a number of

we have less than 10 neurons both training and testing error
increase which means the network is under ﬁtting.

Fig. 4. Trade-off curve for least squares of testing and training error vs.
µ. The minimum test error is marked with a red indicator.

redundant features, which was expected. This also means that
our choice of downsampling the image to a 15x20 resolution
still produced redundant features meaning that we have not
downsampled the image too much.

2) SUPPORT VECTOR REGRESSION: There are two
hyper-parameters to select for SVR are C and . Because we
must optimize over two variable models, we present multiple
trade-off curves superimposed in Fig. 5.

Fig. 6. Trade-off curve for neural nets, showing number of neuron vs. test
and train error

B. SAMPLE OUTPUT

Fig. 7. Visualization using neural nets for training, showing actual points
and estimated points

Fig. 8. Visualization using Regularized Least Squares for training, showing
actual points and estimated points

Fig. 7 and 8 show the 30% of the outputs of the testing
set to avoid a cluttered illustration. The blue points are the
ground truth points shown to the users and the red points
are the ones estimated by our algorithm (in MATLAB plot
units). The green line between points is the distance between
the actual and estimated points.

One thing we observed was that error is typically larger
near the edge, regardless of which algorithm is used. We
suspect that this is because the subject used to create the
dataset needed to strain his eyes to reach the points in the
far corners. As a result, the subject could have possibly not
rotated his eyes all the way to view the indicator.

C. ABLATIVE ANALYSIS

Fig. 5. Trade-off curve for SVR (linear kernel) of testing and training error
vs. . Different values of C are presented as different lines. The minimum
test error is marked with a black indicator.4

The minimum over all of the curves is achieved with
the model where  = 0.53 and C = 10000. This result is
surprising since the largest C value that was tested was used
in the best model. This illustrates how important the slack
variable is in SVR.

We did not present the results for other kernels, e.g.,
polynomial kernels, since the results seemed worse. Rather,
we instead focused on using neural networks which seemed
to yield a better result.

3) NEURAL NETWORK: With neural networks, our vari-
able parameter was the number of neurons in the hidden
layer. We tested out different values for the number of
neurons and plotted the test and train error vs. number of
neurons as shown in Fig. 6. As we see, the training error is
minimized at when we have 10 neurons in the hidden layer,
beyond this number although the training error decreases the
test error increase which means the network is over-ﬁtting. If

4The training error is not shown in the SVR trade-off plot in order to

magnify the difference between the hyper-parameter settings.

We also carried out the analysis of varying the number of
features as well as including the eigen eyes and ﬁsher eyes

00.0020.0040.0060.0080.010.0120.0140.0160.0180.02µ3.43.53.63.73.83.944.14.2Average L2 DistErrorRegularized Linear Regression TradeofftestingErrortrainingErrorBest Model Choice0123456789104681012141618εAverage L2 DistErrorSVR model Tradeoff  C = 10C = 40C = 70C = 200C = 800C = 10000Dynamic Eye Gaze Tracking for Foveated Rendering and Retinal Blur

Kushagr Gupta, Suleman Kazi, Terry Kong

Abstract— This project deals with tracking eye gaze location
and predicting future eye movement sequences using classical
machine learning techniques and Hidden Markov Models,
respectively, for users wearing head mounted displays (HMD).
Eye gaze location can enhance the virtual reality experience
inside an HMD. In this report, we discuss how we collected
a dataset for eye gaze tracking and the features we extracted.
Then, we evaluated three learning approaches to estimate eye
gaze and evaluate the performance using regularized linear re-
gression, support vector regression (SVR), and neural networks.
We also brieﬂy discuss our approach to the prediction problem.

probabilities of where the user is likely to look at) the user
views to estimate eye gaze instead and is able to achieve an
accuracy of <3.0◦. State of the art eye-tracking hardware [7]
is advertised as being able to achieve a typical accuracy of
accuracy of 0.5◦. In our work, instead of relying on signal
or image processing algorithms we use machine learning
techniques and instead of relying on one, we also compare
different learning algorithms and quantify their results as
described in the sections below.

I. INTRODUCTION

III. DATASET AND FEATURES

The ability to track and predict eye gaze accurately can
have a huge impact in Virtual Reality (VR) headsets since
it gives content developers the ﬂexibility to enable foveated
rendering or simulate retinal blur to enhance content and
improve interactivity using eye-gaze information. A recent
HMD project with integrated eye tracking [6] raised well in
excess of its funding goal via kickstarter. Foveated rendering
is a technique in which the image resolution is not uniform
across the image. Retinal blur refers to blurred perception of
objects outside the center of gaze (in the peripheral vision).
In order to apply these techniques the eye gaze location
must be accurately known. To provide enough time for
rendering and to account for latency the eye gaze location
in the next few frames must also be predicted. This requires
high accuracy so that the scene is rendered and blurred at
the right locations. We use machine learning techniques to
output an estimated eye gaze location in terms of screen x-y
coordinates.

This is a joint project for CS229 and CS221. The work
shown in this report explains the eye gaze tracking portion
of the project which was done for CS229. The approach to
predict the eye gaze location constitutes the CS221 portion
of the project and is very brieﬂy summarized at the end to
connect the two projects. For a much more in depth analysis
of the prediction approach, we refer the reader to our report
for CS221.

II. RELATED WORK

A large number of the modern eye tracking algorithms we
came across [1]–[4] use signal/image processing in conjunc-
tion with geometry to accomplish eye gaze tracking. In fact
[5] which is a survey of models for eye gaze tracking does
not list any machine learning methods. [8] uses Artiﬁcial
Neural Networks for eye gaze tracking and is able to achieve
an accuracy of 1.5◦. [9] also uses a probabilistic incremental
learning algorithm to estimate eye gaze but instead of looking
at
images of the user’s eyes to do so it relies on pre-
computed saliency maps of the videos (showing the prior

A. Dataset Acquisition

The dataset consists of images of the user’s eye looking at
points displayed on a screen with a resolution of 1920x1080
pixels and a size of 16.5x30.5 cm. These points serve as
the ground truth data. The user sits 30cm away from the
screen, wearing a pair of glasses with a webcam mounted
on them 5cm from the eye and off-axis as to not obstruct
the screen. By using this method we did not have to worry
about inadequate illumination inside a VR headset whilst
also getting images similar to what a camera inside a VR
headset would produce. We also looked up online datasets
but they did not ﬁt our desired input data requirements. For
example the dataset in [16] gives the eye gaze location (x,y)
values on the screen but doesnt give images of the eye. The
dataset in [17] gives images of eye but with camera right in
front of the eye thus obstructing content on screen. Our data
collection setup is shown in Fig. 1.

Fig. 1. Setup For Data Collection

1) As a one time procedure user is asked to look straight
at the camera and a reference image is captured. A box
is manually drawn around the position of the user’s eye
and the image is cropped to this box and saved.

2) Proceeding to collect a dataset user must take another
reference image. The master reference obtained in step
1 is cross correlated with this reference image to ﬁnd
the location of the user’s eye and a box of the same size
as the master reference is drawn around the eye auto-
matically. This step removes slight variations of user
head position between different dataset acquisitions.

3) The user is then shown a random point on the screen
and each key press captures an image and displays
a new point on the screen. Each captured image is
cropped based on the bounding box calculated in step
2 and saved along with the location of the point shown
on screen.

B. Feature Selection

Initially all the images were converted to grayscale and
vectorized and used as features but it resulted in too many
features compared to the training set size and thus each image
was down sampled and vectorized which serve as features for
the algorithm. Different down sampling factors for resizing
the images were considered and it was observed that the
performance gain after a certain feature length was marginal
and the training time became too prohibitive. We thus chose a
downsampling factor that performed reasonably well giving
a ﬁnal feature length of 300 as compared to the original
image size of 300000 pixels as shown in Fig. 2.

We introduced two additional sets of features namely the
inner product with the eigen-eyes and the inner product with
the ﬁsher-eyes [19] which help as they increase variance
using Principal Component Analysis. They are explained as
follows.

1) Eigen Eyes : Given the vectorized training images the
mean of the entire set µ is removed and output is
concatenated into a matrix, say S, of dimension number
of pixels by number of training images. Eigen eyes are
essentially the eigen-vectors of the scatter matrix S∗S(cid:48).
This gives us the directions in which there is higher
variance based on the eigen values with an illustration
in 2.

2) Fisher Eyes : In eigen eyes we consider the entire
dataset and discriminate based on it, we can improve
this using ﬁsher analysis. The aim here is to maximize
between class scatter while minimizing within class
scatter. For the purpose of eye gaze tracking, we
considered a ﬁnite set of classes based on the true
locations of target point where the eyes are looking
at and each image (mean removed) is binned into one
of these classes. The generalized eigen vectors of the
problem are called ﬁsher eyes with an illustration in 2.
While the initial computation of all the eigen-eyes and
ﬁsher-eyes may take some time, they will only need
to be computed once at the beginning using all the
training data.

Fig. 2. Raw Image, Downsampled Image, Eigen Eye, Fisher Eye

IV. METHODS: TRACKING APPROACH

To evaluate our tracking approach, we compared three
different
learning methods: regularized linear regression,
SVR, and neural networks. In all three cases we treat the

x and y dimensions of the eye gaze location independently.
While we could consider the dimensions together, there is
no added value. As the results will show, using least squares
where the labels are multidimensional (x and y) actually
performs worse which justiﬁes our assumption.
A. LEAST SQUARES WITH REGULARIZATION

Least squares, or linear regression, was used as a baseline
due to its simplicity and straightforward implementation.
Least squares assumes that the data can be modeled as

p = Xw

where p is the eye gaze location (x or y), X is the design
matrix and w is the weight vector. Since least squares can
lead to over-ﬁtting, we also considered regularized least
squares, which has an extra term augmented to the least
squares objective. With µ = 0 it reduces to linear regression
without regularization.

JLSR(w) = (cid:107)p − Xw(cid:107)2

2 + µ(cid:107)w(cid:107)2

2

The hyper-parameter µ controls how much over-ﬁtting we
will tolerate. This modiﬁed objective function is equivalent
to assuming a Gaussian prior on the weights with zero mean
and non-zero variance.

Taking gradients yields the closed form solution:1

wLSR = (X T X + µI)−1X T p

where I is the n by n identity matrix and n is the dimension
of a feature vector.

Both of these results assume that X is full rank and skinny.
While this isn’t always the case, we can always perform SVD
or QR factorization to remove the redundant columns in the
X. Another approach is to pass the optimization problem
into a convex solver, which will likely perform Newton’s
method which doesn’t require any priors on the data.
B. SUPPORT VECTOR REGRESSION

Another approach is to use SVR. The problem inherits
part of its name from the popular support vector machine
problem. Like most regression problems,
the hypothesis
function is of the form h(x) = wT x + b where w is the
weight vector and b is the constant offset.

To use this model, one must select a tolerance, , and
a hyper-parameter for the slack variables, C, and then the
weights are learned to minimize the objective. The physical
meaning of  is the amount of deviation that is tolerated
when evaluating with the hypothesis function.

This problem can be solved more efﬁciently in the dual
domain and is explained more thoroughly in [14]. The
corresponding dual problem is

i )(αj − α∗

i ) +(cid:80)m

j ) < xi, xj >
i=1 y(i)(αi − α∗
i )
i ∈ [0, C]

i ) = 0 and αi, α∗

(cid:40)− 1
(cid:80)m
−(cid:80)m
i,j=1(αi − α∗
m(cid:88)
i=1(αi − α∗

(αi − α∗

2

maximize

subject to

i=1

(1)

1This also assumes that the design matrix is skinny and full rank.

where x(i) is a training feature vector, y(i), m is the number
of training examples, αi and α∗
i are the dual variables. One
nuance to the dual problem is that there is no longer a
mention of the constant term b. Thus, it needs to be inferred
from the solution of the dual problem. For the purpose of
this report, we have set2

bmax + bmin

b =

2

m(cid:88)

where bmax = max{− + y(i)− < w, x(i)|ai < C or a∗
0}, bmin = min{− + y(i)− < w, x(i)|ai > 0 or a∗
and < ·,· > represents a kernel.
Using the solution of the dual problem (αi and α∗

i >
i < C},

i ), we

can construct the weight vectors as follows:

w =

(αi − α∗

i ) < x(i), x(i) >

i=1

Like SVM, SVR can also be kernelized.

C. NEURAL NETWORKS

Neural Networks consist neurons which take in inputs
(weighted by a weight vector which may be different for
each neuron) and process them using a transfer function
to give an output. A neural network consists of a number
of interconnected neurons. We use a feed-forward neural
network in which the information ﬂow is unidirectional. If
we have a neuron j with a sigmoid transfer function, a weight
vector wj = [w1j....wnj]T , bias term b and inputs ok, from
the previous neurons then the output for neuron j will be:

Where α is the learning rate. We used MATLAB’s neural
network framework to design, train, visualize, and simulate
the network. We have a two-layer feed forward neural
network. The hidden layer neurons have a sigmoid transfer
function and the output layer neurons have a linear transfer
function. A diagram of the ﬁnal neural network selected is
shown in Fig. 3

Fig. 3. Neural Network

V. RESULTS AND ANALYSIS

All our algorithms ﬁnd average MSE in terms of the x and
y locations of the points shown on screen. This is generalized
to yield values in terms of viewing angle error. Horizontal
and Vertical ﬁeld of view is calculated using the screen size
and distance of user from the screen which came out to be
52◦ and 30◦ respectively. Using the discretized number of
bins for both directions we convert the L2 error in terms of
viewing angle error as explained for one axis in [].

(cid:18)

(cid:19)

θx = tan−1

tan(HorF OV ◦)

AvgL2Dist
#HorBins

(2)

1

1 + exp(−z)

The dimension of the feature vector for all of the testing
and training data used for all cross validation is 330. The
makeup of the 330 features is:

oj = φ(z) = φ(netj) =

Where:

z =

n(cid:88)

wkjok

k=1

The network is trained using a back-propagation algorithm
which uses gradient descent to optimize the weights. The
squared error in this case is:
1
2

(t − y)2

E =

Where t is the desired output for an input vector and y is
the actual output.From [18] we see that if we take the partial
derivatives using the chain rule we get:

∂E
∂wij

= δjoi



Where:

δj =

∂E
∂oj

(oj − tj)φ(netj)(1 − φ(netj))
(cid:80)
if j is an output neuron
l∈L δlwjlφ(netj)(1 − φ(netj))
if j is an inner neuron
Then the weight update equation is simply:

∂oj
∂netj

∆wij = −α

∂E
∂wij

2This is inspired from [14]

• 300 features make up the vectorization of the 15x20
the eye from the original

downsampled image of
480x640

• 20 features are the coefﬁcients of the top 20 eigen eyes
• 10 features are the coefﬁcients of the top 10 ﬁsher eyes

A. HYPER-PARAMETER SELECTION

To select the hyper-parameters for each model of each
approach, we perform hold-out cross validation where we
train on a random subset of 70% of the data and test on
30% on the data. We then select the hyper-parameters that
correspond to the model that has the smallest test error.

We use hold-out cross validation instead of k-fold cross
validation because the time to train the SVR model and the
neural network on MATLAB was prohibitively long.

1) REGULARIZED LEAST SQUARES: For regularized
least squares, there is one hyper-parameter µ, which controls
how much the model cares about over-ﬁtting. The trade-off
curve of hyper-parameters to test error is show in Fig. 4 The
minimum on this curve is achieved with the model where
µ = 0.003232. Note that the model where µ = 0 corresponds
to the least squares model, without regularization. The fact
that the minimum occurs when µ is non-zero means that
the regularization drove some of the weights down and that
improved the model. This means that there are a number of

we have less than 10 neurons both training and testing error
increase which means the network is under ﬁtting.

Fig. 4. Trade-off curve for least squares of testing and training error vs.
µ. The minimum test error is marked with a red indicator.

redundant features, which was expected. This also means that
our choice of downsampling the image to a 15x20 resolution
still produced redundant features meaning that we have not
downsampled the image too much.

2) SUPPORT VECTOR REGRESSION: There are two
hyper-parameters to select for SVR are C and . Because we
must optimize over two variable models, we present multiple
trade-off curves superimposed in Fig. 5.

Fig. 6. Trade-off curve for neural nets, showing number of neuron vs. test
and train error

B. SAMPLE OUTPUT

Fig. 7. Visualization using neural nets for training, showing actual points
and estimated points

Fig. 8. Visualization using Regularized Least Squares for training, showing
actual points and estimated points

Fig. 7 and 8 show the 30% of the outputs of the testing
set to avoid a cluttered illustration. The blue points are the
ground truth points shown to the users and the red points
are the ones estimated by our algorithm (in MATLAB plot
units). The green line between points is the distance between
the actual and estimated points.

One thing we observed was that error is typically larger
near the edge, regardless of which algorithm is used. We
suspect that this is because the subject used to create the
dataset needed to strain his eyes to reach the points in the
far corners. As a result, the subject could have possibly not
rotated his eyes all the way to view the indicator.

C. ABLATIVE ANALYSIS

Fig. 5. Trade-off curve for SVR (linear kernel) of testing and training error
vs. . Different values of C are presented as different lines. The minimum
test error is marked with a black indicator.4

The minimum over all of the curves is achieved with
the model where  = 0.53 and C = 10000. This result is
surprising since the largest C value that was tested was used
in the best model. This illustrates how important the slack
variable is in SVR.

We did not present the results for other kernels, e.g.,
polynomial kernels, since the results seemed worse. Rather,
we instead focused on using neural networks which seemed
to yield a better result.

3) NEURAL NETWORK: With neural networks, our vari-
able parameter was the number of neurons in the hidden
layer. We tested out different values for the number of
neurons and plotted the test and train error vs. number of
neurons as shown in Fig. 6. As we see, the training error is
minimized at when we have 10 neurons in the hidden layer,
beyond this number although the training error decreases the
test error increase which means the network is over-ﬁtting. If

4The training error is not shown in the SVR trade-off plot in order to

magnify the difference between the hyper-parameter settings.

We also carried out the analysis of varying the number of
features as well as including the eigen eyes and ﬁsher eyes

00.0020.0040.0060.0080.010.0120.0140.0160.0180.02µ3.43.53.63.73.83.944.14.2Average L2 DistErrorRegularized Linear Regression TradeofftestingErrortrainingErrorBest Model Choice0123456789104681012141618εAverage L2 DistErrorSVR model Tradeoff  C = 10C = 40C = 70C = 200C = 800C = 10000for one of the tests. The images were downsampled at four
scales which gave different feature vector lengths and then
training and testing was carried out on the baseline algorithm
i.e. regularized least squares. According to Table I,
the
performance improvement beyond feature vector length of
300 i.e. after doubling feature vector length was marginal but
with adding 30 PCA components there was improvement and
hence 330 features were ﬁnally used for different algorithms.

TABLE I

TESTING ERROR (IN DEGREES) VARYING FEATURE LENGTH FOR REG.

LINEAR REGRESSION

# feat
Error

99
1.857◦

no PCA
1.470◦

300

+30 PCA
1.452◦

588
1.362◦

972
1.333◦

D. ALGORITHM COMPARISON

Using the optimal model for each algorithm, we have
tested all three algorithms on the 30% validation data. The
results are tabulated in Table II. As can be seen from the
table the training and testing error are of the same order and
comparable which implies that we are not over-ﬁtting the
training data.

TESTING ERROR (IN DEGREES) OF ALL APPROACHES

TABLE II

Algorithm
Linear Reg

Regularized Linear Reg

SVR

Neural Net

Train Error

1.250◦
1.276◦
1.452◦
0.600◦

Test Error
1.477◦
1.444◦
1.570◦
1.071◦

One conclusion we can make from these results is that
neural networks work best compared to linear regression and
SVR (using a linear kernel). The purpose of neural networks
is usually to implicitly solve for relationships between the
features, which makes it a more sophisticated algorithm
compared to least squares, which simply minimizes the
square loss objective. We suspect the reason SVR performs
worse than least squares is that the data is not ”linearly
separable” in the regression sense.

VI. FUTURE WORK AND EXTENSIONS

A. PREDICTION APPROACH

This is a part of our CS 221 project wherein using past eye
gaze locations we predict the future eye gaze location. Our
models of prediction depend on the framework of Hidden
Markov Models. Much like [10], our approach is also to
model the true motion of the eye as a hidden random variable.
Where our models differ is that our observed variables
are (noisy) measurements from the eye tracker and the
hidden variables are the true (noiseless) measurements. An
illustration of a discrete HMM as a Bayesian network is
shown in Fig. 9.

6Also called ”evidence” in some literature.

H1

H2

. . .

Ht

Ht+1

E1

E2

Et

Fig. 9. An HMM with hidden variables Hi which each emit one discrete
observed variable6Ei. Here the variable t is an integer that indexes time.

As Fig. 9 suggests, each variable Hi and Ei are distributed

as follows:

Hi ∼ P (Hi|Hi−1)
Ei ∼ P (Ei|Hi)

(3)

(4)

When a Bayesian network has the form in Fig. 9 and
has variables distributed as above, it is called an HMM.
For the rest of this report we will refer to P (Hi|Hi−1)
as the transition probability and P (Ei|Hi) as the emission
probability.

An HMM is completely speciﬁed by giving the hidden
and observed variables as well as the transition and emission
probabilities. First we will cover the positional HMM, which
uses eye gaze location as variables and then we will cover
the angular HMM which uses the angle of the eye gaze’s
velocity as variables.

1) POSITIONAL HIDDEN MARKOV MODEL: The ﬁrst
Hidden Markov Model
implemented uses true eye gaze
locations, i.e., (x, y) positions, in the scene as the hidden
states and the sensor output as the noisy observations, i.e.
,(˜x, ˜y) positions, of these hidden states.

2) ANGULAR HIDDEN MARKOV MODEL: The second
Hidden Markov Model used for prediction uses true dis-
cretized directions (or angles) of the eye gaze’s velocity as
hidden variables, Hi, and the estimation of the eye gaze’s
velocity from the eye tracker as the evidence variables.

VII. CONCLUSIONS

The purpose of this paper was to evaluate different learn-
ing approaches to the task of eye gaze tracking. Our results
have shown that a simple two layer feed-forward neural
network outperforms linear regression, and a seemingly more
complex approach: SVR. As we expected from using an
image of the eye as a feature, there is are many relationships
that can be made between pixels, which is the kind of data
that works well in neural networks.

For future work, we would like to combine the CS221
approach for eye gaze prediction with the neural network
approach to build a fast real-time application. We would also
like to explore what performance gains we can expect by
increasing the neural net layer, or by using convolutional
neural networks.

REFERENCES

[1] Li, Dongheng, David Winﬁeld, and Derrick J. Parkhurst. ”Starburst:
A hybrid algorithm for video-based eye tracking combining feature-
based and model-based approaches.” Computer Vision and Pattern
Recognition-Workshops, 2005. CVPR Workshops. IEEE Computer
Society Conference on. IEEE, 2005.

Dynamic Eye Gaze Tracking for Foveated Rendering and Retinal Blur

Kushagr Gupta, Suleman Kazi, Terry Kong

Abstract— This project deals with tracking eye gaze location
and predicting future eye movement sequences using classical
machine learning techniques and Hidden Markov Models,
respectively, for users wearing head mounted displays (HMD).
Eye gaze location can enhance the virtual reality experience
inside an HMD. In this report, we discuss how we collected
a dataset for eye gaze tracking and the features we extracted.
Then, we evaluated three learning approaches to estimate eye
gaze and evaluate the performance using regularized linear re-
gression, support vector regression (SVR), and neural networks.
We also brieﬂy discuss our approach to the prediction problem.

probabilities of where the user is likely to look at) the user
views to estimate eye gaze instead and is able to achieve an
accuracy of <3.0◦. State of the art eye-tracking hardware [7]
is advertised as being able to achieve a typical accuracy of
accuracy of 0.5◦. In our work, instead of relying on signal
or image processing algorithms we use machine learning
techniques and instead of relying on one, we also compare
different learning algorithms and quantify their results as
described in the sections below.

I. INTRODUCTION

III. DATASET AND FEATURES

The ability to track and predict eye gaze accurately can
have a huge impact in Virtual Reality (VR) headsets since
it gives content developers the ﬂexibility to enable foveated
rendering or simulate retinal blur to enhance content and
improve interactivity using eye-gaze information. A recent
HMD project with integrated eye tracking [6] raised well in
excess of its funding goal via kickstarter. Foveated rendering
is a technique in which the image resolution is not uniform
across the image. Retinal blur refers to blurred perception of
objects outside the center of gaze (in the peripheral vision).
In order to apply these techniques the eye gaze location
must be accurately known. To provide enough time for
rendering and to account for latency the eye gaze location
in the next few frames must also be predicted. This requires
high accuracy so that the scene is rendered and blurred at
the right locations. We use machine learning techniques to
output an estimated eye gaze location in terms of screen x-y
coordinates.

This is a joint project for CS229 and CS221. The work
shown in this report explains the eye gaze tracking portion
of the project which was done for CS229. The approach to
predict the eye gaze location constitutes the CS221 portion
of the project and is very brieﬂy summarized at the end to
connect the two projects. For a much more in depth analysis
of the prediction approach, we refer the reader to our report
for CS221.

II. RELATED WORK

A large number of the modern eye tracking algorithms we
came across [1]–[4] use signal/image processing in conjunc-
tion with geometry to accomplish eye gaze tracking. In fact
[5] which is a survey of models for eye gaze tracking does
not list any machine learning methods. [8] uses Artiﬁcial
Neural Networks for eye gaze tracking and is able to achieve
an accuracy of 1.5◦. [9] also uses a probabilistic incremental
learning algorithm to estimate eye gaze but instead of looking
at
images of the user’s eyes to do so it relies on pre-
computed saliency maps of the videos (showing the prior

A. Dataset Acquisition

The dataset consists of images of the user’s eye looking at
points displayed on a screen with a resolution of 1920x1080
pixels and a size of 16.5x30.5 cm. These points serve as
the ground truth data. The user sits 30cm away from the
screen, wearing a pair of glasses with a webcam mounted
on them 5cm from the eye and off-axis as to not obstruct
the screen. By using this method we did not have to worry
about inadequate illumination inside a VR headset whilst
also getting images similar to what a camera inside a VR
headset would produce. We also looked up online datasets
but they did not ﬁt our desired input data requirements. For
example the dataset in [16] gives the eye gaze location (x,y)
values on the screen but doesnt give images of the eye. The
dataset in [17] gives images of eye but with camera right in
front of the eye thus obstructing content on screen. Our data
collection setup is shown in Fig. 1.

Fig. 1. Setup For Data Collection

1) As a one time procedure user is asked to look straight
at the camera and a reference image is captured. A box
is manually drawn around the position of the user’s eye
and the image is cropped to this box and saved.

2) Proceeding to collect a dataset user must take another
reference image. The master reference obtained in step
1 is cross correlated with this reference image to ﬁnd
the location of the user’s eye and a box of the same size
as the master reference is drawn around the eye auto-
matically. This step removes slight variations of user
head position between different dataset acquisitions.

3) The user is then shown a random point on the screen
and each key press captures an image and displays
a new point on the screen. Each captured image is
cropped based on the bounding box calculated in step
2 and saved along with the location of the point shown
on screen.

B. Feature Selection

Initially all the images were converted to grayscale and
vectorized and used as features but it resulted in too many
features compared to the training set size and thus each image
was down sampled and vectorized which serve as features for
the algorithm. Different down sampling factors for resizing
the images were considered and it was observed that the
performance gain after a certain feature length was marginal
and the training time became too prohibitive. We thus chose a
downsampling factor that performed reasonably well giving
a ﬁnal feature length of 300 as compared to the original
image size of 300000 pixels as shown in Fig. 2.

We introduced two additional sets of features namely the
inner product with the eigen-eyes and the inner product with
the ﬁsher-eyes [19] which help as they increase variance
using Principal Component Analysis. They are explained as
follows.

1) Eigen Eyes : Given the vectorized training images the
mean of the entire set µ is removed and output is
concatenated into a matrix, say S, of dimension number
of pixels by number of training images. Eigen eyes are
essentially the eigen-vectors of the scatter matrix S∗S(cid:48).
This gives us the directions in which there is higher
variance based on the eigen values with an illustration
in 2.

2) Fisher Eyes : In eigen eyes we consider the entire
dataset and discriminate based on it, we can improve
this using ﬁsher analysis. The aim here is to maximize
between class scatter while minimizing within class
scatter. For the purpose of eye gaze tracking, we
considered a ﬁnite set of classes based on the true
locations of target point where the eyes are looking
at and each image (mean removed) is binned into one
of these classes. The generalized eigen vectors of the
problem are called ﬁsher eyes with an illustration in 2.
While the initial computation of all the eigen-eyes and
ﬁsher-eyes may take some time, they will only need
to be computed once at the beginning using all the
training data.

Fig. 2. Raw Image, Downsampled Image, Eigen Eye, Fisher Eye

IV. METHODS: TRACKING APPROACH

To evaluate our tracking approach, we compared three
different
learning methods: regularized linear regression,
SVR, and neural networks. In all three cases we treat the

x and y dimensions of the eye gaze location independently.
While we could consider the dimensions together, there is
no added value. As the results will show, using least squares
where the labels are multidimensional (x and y) actually
performs worse which justiﬁes our assumption.
A. LEAST SQUARES WITH REGULARIZATION

Least squares, or linear regression, was used as a baseline
due to its simplicity and straightforward implementation.
Least squares assumes that the data can be modeled as

p = Xw

where p is the eye gaze location (x or y), X is the design
matrix and w is the weight vector. Since least squares can
lead to over-ﬁtting, we also considered regularized least
squares, which has an extra term augmented to the least
squares objective. With µ = 0 it reduces to linear regression
without regularization.

JLSR(w) = (cid:107)p − Xw(cid:107)2

2 + µ(cid:107)w(cid:107)2

2

The hyper-parameter µ controls how much over-ﬁtting we
will tolerate. This modiﬁed objective function is equivalent
to assuming a Gaussian prior on the weights with zero mean
and non-zero variance.

Taking gradients yields the closed form solution:1

wLSR = (X T X + µI)−1X T p

where I is the n by n identity matrix and n is the dimension
of a feature vector.

Both of these results assume that X is full rank and skinny.
While this isn’t always the case, we can always perform SVD
or QR factorization to remove the redundant columns in the
X. Another approach is to pass the optimization problem
into a convex solver, which will likely perform Newton’s
method which doesn’t require any priors on the data.
B. SUPPORT VECTOR REGRESSION

Another approach is to use SVR. The problem inherits
part of its name from the popular support vector machine
problem. Like most regression problems,
the hypothesis
function is of the form h(x) = wT x + b where w is the
weight vector and b is the constant offset.

To use this model, one must select a tolerance, , and
a hyper-parameter for the slack variables, C, and then the
weights are learned to minimize the objective. The physical
meaning of  is the amount of deviation that is tolerated
when evaluating with the hypothesis function.

This problem can be solved more efﬁciently in the dual
domain and is explained more thoroughly in [14]. The
corresponding dual problem is

i )(αj − α∗

i ) +(cid:80)m

j ) < xi, xj >
i=1 y(i)(αi − α∗
i )
i ∈ [0, C]

i ) = 0 and αi, α∗

(cid:40)− 1
(cid:80)m
−(cid:80)m
i,j=1(αi − α∗
m(cid:88)
i=1(αi − α∗

(αi − α∗

2

maximize

subject to

i=1

(1)

1This also assumes that the design matrix is skinny and full rank.

where x(i) is a training feature vector, y(i), m is the number
of training examples, αi and α∗
i are the dual variables. One
nuance to the dual problem is that there is no longer a
mention of the constant term b. Thus, it needs to be inferred
from the solution of the dual problem. For the purpose of
this report, we have set2

bmax + bmin

b =

2

m(cid:88)

where bmax = max{− + y(i)− < w, x(i)|ai < C or a∗
0}, bmin = min{− + y(i)− < w, x(i)|ai > 0 or a∗
and < ·,· > represents a kernel.
Using the solution of the dual problem (αi and α∗

i >
i < C},

i ), we

can construct the weight vectors as follows:

w =

(αi − α∗

i ) < x(i), x(i) >

i=1

Like SVM, SVR can also be kernelized.

C. NEURAL NETWORKS

Neural Networks consist neurons which take in inputs
(weighted by a weight vector which may be different for
each neuron) and process them using a transfer function
to give an output. A neural network consists of a number
of interconnected neurons. We use a feed-forward neural
network in which the information ﬂow is unidirectional. If
we have a neuron j with a sigmoid transfer function, a weight
vector wj = [w1j....wnj]T , bias term b and inputs ok, from
the previous neurons then the output for neuron j will be:

Where α is the learning rate. We used MATLAB’s neural
network framework to design, train, visualize, and simulate
the network. We have a two-layer feed forward neural
network. The hidden layer neurons have a sigmoid transfer
function and the output layer neurons have a linear transfer
function. A diagram of the ﬁnal neural network selected is
shown in Fig. 3

Fig. 3. Neural Network

V. RESULTS AND ANALYSIS

All our algorithms ﬁnd average MSE in terms of the x and
y locations of the points shown on screen. This is generalized
to yield values in terms of viewing angle error. Horizontal
and Vertical ﬁeld of view is calculated using the screen size
and distance of user from the screen which came out to be
52◦ and 30◦ respectively. Using the discretized number of
bins for both directions we convert the L2 error in terms of
viewing angle error as explained for one axis in [].

(cid:18)

(cid:19)

θx = tan−1

tan(HorF OV ◦)

AvgL2Dist
#HorBins

(2)

1

1 + exp(−z)

The dimension of the feature vector for all of the testing
and training data used for all cross validation is 330. The
makeup of the 330 features is:

oj = φ(z) = φ(netj) =

Where:

z =

n(cid:88)

wkjok

k=1

The network is trained using a back-propagation algorithm
which uses gradient descent to optimize the weights. The
squared error in this case is:
1
2

(t − y)2

E =

Where t is the desired output for an input vector and y is
the actual output.From [18] we see that if we take the partial
derivatives using the chain rule we get:

∂E
∂wij

= δjoi



Where:

δj =

∂E
∂oj

(oj − tj)φ(netj)(1 − φ(netj))
(cid:80)
if j is an output neuron
l∈L δlwjlφ(netj)(1 − φ(netj))
if j is an inner neuron
Then the weight update equation is simply:

∂oj
∂netj

∆wij = −α

∂E
∂wij

2This is inspired from [14]

• 300 features make up the vectorization of the 15x20
the eye from the original

downsampled image of
480x640

• 20 features are the coefﬁcients of the top 20 eigen eyes
• 10 features are the coefﬁcients of the top 10 ﬁsher eyes

A. HYPER-PARAMETER SELECTION

To select the hyper-parameters for each model of each
approach, we perform hold-out cross validation where we
train on a random subset of 70% of the data and test on
30% on the data. We then select the hyper-parameters that
correspond to the model that has the smallest test error.

We use hold-out cross validation instead of k-fold cross
validation because the time to train the SVR model and the
neural network on MATLAB was prohibitively long.

1) REGULARIZED LEAST SQUARES: For regularized
least squares, there is one hyper-parameter µ, which controls
how much the model cares about over-ﬁtting. The trade-off
curve of hyper-parameters to test error is show in Fig. 4 The
minimum on this curve is achieved with the model where
µ = 0.003232. Note that the model where µ = 0 corresponds
to the least squares model, without regularization. The fact
that the minimum occurs when µ is non-zero means that
the regularization drove some of the weights down and that
improved the model. This means that there are a number of

we have less than 10 neurons both training and testing error
increase which means the network is under ﬁtting.

Fig. 4. Trade-off curve for least squares of testing and training error vs.
µ. The minimum test error is marked with a red indicator.

redundant features, which was expected. This also means that
our choice of downsampling the image to a 15x20 resolution
still produced redundant features meaning that we have not
downsampled the image too much.

2) SUPPORT VECTOR REGRESSION: There are two
hyper-parameters to select for SVR are C and . Because we
must optimize over two variable models, we present multiple
trade-off curves superimposed in Fig. 5.

Fig. 6. Trade-off curve for neural nets, showing number of neuron vs. test
and train error

B. SAMPLE OUTPUT

Fig. 7. Visualization using neural nets for training, showing actual points
and estimated points

Fig. 8. Visualization using Regularized Least Squares for training, showing
actual points and estimated points

Fig. 7 and 8 show the 30% of the outputs of the testing
set to avoid a cluttered illustration. The blue points are the
ground truth points shown to the users and the red points
are the ones estimated by our algorithm (in MATLAB plot
units). The green line between points is the distance between
the actual and estimated points.

One thing we observed was that error is typically larger
near the edge, regardless of which algorithm is used. We
suspect that this is because the subject used to create the
dataset needed to strain his eyes to reach the points in the
far corners. As a result, the subject could have possibly not
rotated his eyes all the way to view the indicator.

C. ABLATIVE ANALYSIS

Fig. 5. Trade-off curve for SVR (linear kernel) of testing and training error
vs. . Different values of C are presented as different lines. The minimum
test error is marked with a black indicator.4

The minimum over all of the curves is achieved with
the model where  = 0.53 and C = 10000. This result is
surprising since the largest C value that was tested was used
in the best model. This illustrates how important the slack
variable is in SVR.

We did not present the results for other kernels, e.g.,
polynomial kernels, since the results seemed worse. Rather,
we instead focused on using neural networks which seemed
to yield a better result.

3) NEURAL NETWORK: With neural networks, our vari-
able parameter was the number of neurons in the hidden
layer. We tested out different values for the number of
neurons and plotted the test and train error vs. number of
neurons as shown in Fig. 6. As we see, the training error is
minimized at when we have 10 neurons in the hidden layer,
beyond this number although the training error decreases the
test error increase which means the network is over-ﬁtting. If

4The training error is not shown in the SVR trade-off plot in order to

magnify the difference between the hyper-parameter settings.

We also carried out the analysis of varying the number of
features as well as including the eigen eyes and ﬁsher eyes

00.0020.0040.0060.0080.010.0120.0140.0160.0180.02µ3.43.53.63.73.83.944.14.2Average L2 DistErrorRegularized Linear Regression TradeofftestingErrortrainingErrorBest Model Choice0123456789104681012141618εAverage L2 DistErrorSVR model Tradeoff  C = 10C = 40C = 70C = 200C = 800C = 10000for one of the tests. The images were downsampled at four
scales which gave different feature vector lengths and then
training and testing was carried out on the baseline algorithm
i.e. regularized least squares. According to Table I,
the
performance improvement beyond feature vector length of
300 i.e. after doubling feature vector length was marginal but
with adding 30 PCA components there was improvement and
hence 330 features were ﬁnally used for different algorithms.

TABLE I

TESTING ERROR (IN DEGREES) VARYING FEATURE LENGTH FOR REG.

LINEAR REGRESSION

# feat
Error

99
1.857◦

no PCA
1.470◦

300

+30 PCA
1.452◦

588
1.362◦

972
1.333◦

D. ALGORITHM COMPARISON

Using the optimal model for each algorithm, we have
tested all three algorithms on the 30% validation data. The
results are tabulated in Table II. As can be seen from the
table the training and testing error are of the same order and
comparable which implies that we are not over-ﬁtting the
training data.

TESTING ERROR (IN DEGREES) OF ALL APPROACHES

TABLE II

Algorithm
Linear Reg

Regularized Linear Reg

SVR

Neural Net

Train Error

1.250◦
1.276◦
1.452◦
0.600◦

Test Error
1.477◦
1.444◦
1.570◦
1.071◦

One conclusion we can make from these results is that
neural networks work best compared to linear regression and
SVR (using a linear kernel). The purpose of neural networks
is usually to implicitly solve for relationships between the
features, which makes it a more sophisticated algorithm
compared to least squares, which simply minimizes the
square loss objective. We suspect the reason SVR performs
worse than least squares is that the data is not ”linearly
separable” in the regression sense.

VI. FUTURE WORK AND EXTENSIONS

A. PREDICTION APPROACH

This is a part of our CS 221 project wherein using past eye
gaze locations we predict the future eye gaze location. Our
models of prediction depend on the framework of Hidden
Markov Models. Much like [10], our approach is also to
model the true motion of the eye as a hidden random variable.
Where our models differ is that our observed variables
are (noisy) measurements from the eye tracker and the
hidden variables are the true (noiseless) measurements. An
illustration of a discrete HMM as a Bayesian network is
shown in Fig. 9.

6Also called ”evidence” in some literature.

H1

H2

. . .

Ht

Ht+1

E1

E2

Et

Fig. 9. An HMM with hidden variables Hi which each emit one discrete
observed variable6Ei. Here the variable t is an integer that indexes time.

As Fig. 9 suggests, each variable Hi and Ei are distributed

as follows:

Hi ∼ P (Hi|Hi−1)
Ei ∼ P (Ei|Hi)

(3)

(4)

When a Bayesian network has the form in Fig. 9 and
has variables distributed as above, it is called an HMM.
For the rest of this report we will refer to P (Hi|Hi−1)
as the transition probability and P (Ei|Hi) as the emission
probability.

An HMM is completely speciﬁed by giving the hidden
and observed variables as well as the transition and emission
probabilities. First we will cover the positional HMM, which
uses eye gaze location as variables and then we will cover
the angular HMM which uses the angle of the eye gaze’s
velocity as variables.

1) POSITIONAL HIDDEN MARKOV MODEL: The ﬁrst
Hidden Markov Model
implemented uses true eye gaze
locations, i.e., (x, y) positions, in the scene as the hidden
states and the sensor output as the noisy observations, i.e.
,(˜x, ˜y) positions, of these hidden states.

2) ANGULAR HIDDEN MARKOV MODEL: The second
Hidden Markov Model used for prediction uses true dis-
cretized directions (or angles) of the eye gaze’s velocity as
hidden variables, Hi, and the estimation of the eye gaze’s
velocity from the eye tracker as the evidence variables.

VII. CONCLUSIONS

The purpose of this paper was to evaluate different learn-
ing approaches to the task of eye gaze tracking. Our results
have shown that a simple two layer feed-forward neural
network outperforms linear regression, and a seemingly more
complex approach: SVR. As we expected from using an
image of the eye as a feature, there is are many relationships
that can be made between pixels, which is the kind of data
that works well in neural networks.

For future work, we would like to combine the CS221
approach for eye gaze prediction with the neural network
approach to build a fast real-time application. We would also
like to explore what performance gains we can expect by
increasing the neural net layer, or by using convolutional
neural networks.

REFERENCES

[1] Li, Dongheng, David Winﬁeld, and Derrick J. Parkhurst. ”Starburst:
A hybrid algorithm for video-based eye tracking combining feature-
based and model-based approaches.” Computer Vision and Pattern
Recognition-Workshops, 2005. CVPR Workshops. IEEE Computer
Society Conference on. IEEE, 2005.

[14] A. Smola and B. Sch,lkopf, ”A tutorial on support vector regression”

Statistics and Computing, 2001

[15] Yunlong Feng; Cheung, G.; Wai-tian Tan; Le Callet, P.; Yusheng Ji,
”Low-Cost Eye Gaze Prediction System for Interactive Networked
Video Streaming,” in Multimedia, IE EE Transactions on , vol.15,
no.8, pp.1865-1879, Dec. 2013

[16] H. Hadizadeh, M. J. Enriquez, and I. V. Baji, ”Eye-tracking database
for a set of standard video sequences,” IEEE Trans. Image Processing,
vol. 21, no. 2, pp. 898-903, Feb. 2012.

[17] B.A. Smith, Q. Yin, S.K. Feiner and S.K. Nayar,”Gaze Locking:
Passive Eye Contact Detection for HumanObject Interaction,”ACM
Symposium on User Interface Software and Technology (UIST),pp.
271-280, Oct. 2013.
[18] ”Backpropagation.”
dia. Wikimedia
10
¡http://en.wikipedia.org/wiki/Backpropagation¿

Free
December

Encyclope-
2015.

Wikipedia:

Foundation,

Inc.

The

[19] https://www.web.stanford.edu/class/ee368/Handouts/Lectures/2015 Autumn/10-

EigenImages 16x9.pdf

[2] Merad, Djamel, Stphanie Mailles-Viard Metz, and Serge Miguet.
”Eye and gaze tracking algorithm for collaborative learning system.”
ICINCO-RA 2006. 2006.

[3] Ji, Qiang, and Xiaojie Yang. ”Real-time eye, gaze, and face pose
tracking for monitoring driver vigilance.” Real-Time Imaging 8.5
(2002): 357-377.

[4] Kim, Kyung-Nam, and R. S. Ramakrishna. ”Vision-based eye-gaze
tracking for human computer interface.” Systems, Man, and Cyber-
netics, 1999. IEEE SMC’99 Conference Proceedings. 1999 IEEE
International Conference on. Vol. 2. IEEE, 1999.

[5] Hansen, Dan Witzner, and Qiang Ji. ”In the eye of the beholder: A
survey of models for eyes and gaze.” Pattern Analysis and Machine
Intelligence, IEEE Transactions on 32.3 (2010): 478-500.

[6] https://www.kickstarter.com/projects/fove/fove-the-worlds-ﬁrst-eye-

tracking-virtual-reality/description

[7] SensoMotoric Instruments GmbH, (September 2015) SMI Eye Track-

ing Glasses 2 Wireless. Available at: http://goo.gl/NZjvwO

[8] Baluja, Shumeet, and Dean Pomerleau. Non-intrusive gaze tracking
using artiﬁcial neural networks. No. CMU-CS-94-102. CARNEGIE-
MELLON UNIV PITTSBURGH PA DEPT OF COMPUTER SCI-
ENCE, 1994.

[9] Chen, Jixu, and Qiang Ji. ”A Probabilistic Approach to Online
Eye Gaze Tracking Without Explicit Personal Calibration.” Image
Processing, IEEE Transactions on 24.3 (2015): 1076-1086.

[10] Yunlong Feng; Gene Cheung; Wai-tian Tan; Yusheng Ji, ”Hidden
Markov Model for eye gaze prediction in networked video streaming,”
in Multimedia and Expo (ICME), 2011 IEEE International Conference
on , vol., no., pp.1-6, 11-15 July 2011

[11] H. Hadizadeh, M. J. Enriquez, and I. V. Baji, ”Eye-tracking database
for a set of standard video sequences,” IEEE Trans. Image Processing,
vol. 21, no. 2, pp. 898-903, Feb. 2012.

[12] O. V. Komogortsev and J. Khan ”Kalman ﬁltering in the design of eye-
gaze-guided computer interfaces” Proc. 12th Int. Conf. Hum.-Comput.
Interact. (HCI), pp. 1-10, 2007

[13] John Findlay and Robin Walker (2012) Human saccadic eye move-

ments. Scholarpedia, 7(7):5095.

