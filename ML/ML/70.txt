Handwritten Digit Recognition via Unsupervised

Learning

Xinyi Jiang, Huy Pham, Qingyang Xu

December 11, 2015

Abstract

We present the results of several unsupervised algo-
rithms tested on the MNIST database as well as tech-
niques we used to improve the classiﬁcation accuracy.
We ﬁnd that spiking neural network outperforms k-
means clustering and reaches the same level as the
supervised SVM. We then discuss several inherent
issues of unsupervised methods for the handwritten
digit classﬁcation problem and propose several meth-
ods to further improve the accuracy.

I. Introduction

Handwritten digit recognition is a benchmark test for
computer vision algorithms and has a wide range of
applications from bank check processing to postcode
recognition. Currently the supervised learning meth-
ods yield much higher classiﬁcation accuracy than
most unsupervised ones.

Despite its high accuracy,

supervised learning
methods tend to be very expensive to develop since
they require large training sets to be manually la-
beled. On the other hand, unsupervised approaches
have the potential of being much more eﬃcient to
train and can be generalized to more complicated clas-
siﬁcation problems such as handwritten alphabet or
Chinese characters with low additional cost.

In this project, we aim to implement unsupervised
algorithms and apply novel techniques to improve the
accuracy. We use the MNIST database[1], which con-
sists of 60,000 images in training set and 10,000 in test
set. Each image is a 28 × 28 array of pixels in gray
scale ranging from 0 to 255 and labeled with the digit
it represents.

Our ﬁrst approach is the k-means clustering algo-

rithm, divided into three stages.

We ﬁrst preprocess the data in order to extract
the small subset of most relevant features from the
high dimensional input so the program runs more ef-
fciently. We use feature selection techniques, such as

principal component analysis and sparse autoencoder,
as well as better heuristics such as input discretization
and image centralization.

Then we improve the initialization process of k-
means, either by the probabilistic k-means++ initial-
ization or by choosing the centers which minimize cer-
tain cost function we construct.

Finally, we run the clustering process with certain
modiﬁcations such as k-medians and k-medoids algo-
rithms and gauge their eﬀects.

With the improved feature set and initialization,
our k-means algorithm achieves 60% classiﬁcation ac-
curacy on average, 10% higher than the original.

Spiking neural network is the best performing un-
supervised method for handwritten digit recognition
up-to-date. We reproduced the 89% accuracy of Diehl
and Cook (2015) with the Brian simulator of SNN.

For reference of accuracy, we also run supervised
algorithms such as SVM and neural network. SVM
yields 90% high accuracy, and it also reveals some in-
herent issues of handwritten digit recognition which
might explain the relatively low accuracy of unsuper-
vised approaches. Neural network has a surprisingly
low accuracy around 20%, and we ﬁnd that the ﬁtting
parameters tend to oscillate without converging.

II. Preprocessing Data

2.1 General Methods
We ﬁrst reshape each image from a 28 × 28 array of
pixels into a vector of the same size. The default
metric we used to cluster and classify the images is
the Euclidean distance between them. Since k-means
only assigns the inputs to clusters to minimize the to-
tal distance from each training example to its corre-
sponding cluster centroid, there is no way to directly
calculate the accuracy. Instead, in any given cluster
S, we choose the digit d which appears the most fre-
quently in S and calculate the ratio of all d present

1

Handwritten Digit Recognition via Unsupervised

Learning

Xinyi Jiang, Huy Pham, Qingyang Xu

December 11, 2015

Abstract

We present the results of several unsupervised algo-
rithms tested on the MNIST database as well as tech-
niques we used to improve the classiﬁcation accuracy.
We ﬁnd that spiking neural network outperforms k-
means clustering and reaches the same level as the
supervised SVM. We then discuss several inherent
issues of unsupervised methods for the handwritten
digit classﬁcation problem and propose several meth-
ods to further improve the accuracy.

I. Introduction

Handwritten digit recognition is a benchmark test for
computer vision algorithms and has a wide range of
applications from bank check processing to postcode
recognition. Currently the supervised learning meth-
ods yield much higher classiﬁcation accuracy than
most unsupervised ones.

Despite its high accuracy,

supervised learning
methods tend to be very expensive to develop since
they require large training sets to be manually la-
beled. On the other hand, unsupervised approaches
have the potential of being much more eﬃcient to
train and can be generalized to more complicated clas-
siﬁcation problems such as handwritten alphabet or
Chinese characters with low additional cost.

In this project, we aim to implement unsupervised
algorithms and apply novel techniques to improve the
accuracy. We use the MNIST database[1], which con-
sists of 60,000 images in training set and 10,000 in test
set. Each image is a 28 × 28 array of pixels in gray
scale ranging from 0 to 255 and labeled with the digit
it represents.

Our ﬁrst approach is the k-means clustering algo-

rithm, divided into three stages.

We ﬁrst preprocess the data in order to extract
the small subset of most relevant features from the
high dimensional input so the program runs more ef-
fciently. We use feature selection techniques, such as

principal component analysis and sparse autoencoder,
as well as better heuristics such as input discretization
and image centralization.

Then we improve the initialization process of k-
means, either by the probabilistic k-means++ initial-
ization or by choosing the centers which minimize cer-
tain cost function we construct.

Finally, we run the clustering process with certain
modiﬁcations such as k-medians and k-medoids algo-
rithms and gauge their eﬀects.

With the improved feature set and initialization,
our k-means algorithm achieves 60% classiﬁcation ac-
curacy on average, 10% higher than the original.

Spiking neural network is the best performing un-
supervised method for handwritten digit recognition
up-to-date. We reproduced the 89% accuracy of Diehl
and Cook (2015) with the Brian simulator of SNN.

For reference of accuracy, we also run supervised
algorithms such as SVM and neural network. SVM
yields 90% high accuracy, and it also reveals some in-
herent issues of handwritten digit recognition which
might explain the relatively low accuracy of unsuper-
vised approaches. Neural network has a surprisingly
low accuracy around 20%, and we ﬁnd that the ﬁtting
parameters tend to oscillate without converging.

II. Preprocessing Data

2.1 General Methods
We ﬁrst reshape each image from a 28 × 28 array of
pixels into a vector of the same size. The default
metric we used to cluster and classify the images is
the Euclidean distance between them. Since k-means
only assigns the inputs to clusters to minimize the to-
tal distance from each training example to its corre-
sponding cluster centroid, there is no way to directly
calculate the accuracy. Instead, in any given cluster
S, we choose the digit d which appears the most fre-
quently in S and calculate the ratio of all d present

1

most relevant structure of each digit. Each neuron in
the input and output layers corresponds to a pixel in
the training images, and we test to ﬁnd the optimal
number of hidden neurons that leads to lowest train-
ing errors.

We use the sigmoid function as the activation func-
tion for the autoencoder. Hence we need to normal-
ize the input coordinates to between 0 and 1 so the
the autoencoder can eﬀectively reconstruct the input.
Once the training is complete, we step through the
training set again and feed the outputs of the hidden
neuron on each training example as the new input to
the k-means clustering algorithm.

We ﬁnd that the optimal hidden dimension for
training with 2000 examples is 600, with accuracy as
high as PCA. However, the accuracy has a large ﬂuc-
tuation with the hidden dimension (Fig. 2). Also, we
ﬁnd that sparse autoencoder cannot reduce dimen-
sion as eﬀectively as PCA since the accuracy drops
sharply when the hidden dimension gets below 100.

in S and take this as the classiﬁcation accuracy of
S. The total accuracy is obtained by averaging over
accuracies of the ten clusters.

2.2 Feature Selection

The high input dimensions, if weighted equally, not
only causes the program to run extremely slowly but
also obscures the essential underlying structure of
each digit, which is the most crucial in the classiﬁ-
cation process and exactly what our program tries to
learn. Hence we ﬁrst need to reduce the dimension of
the input and replace it with a much smaller subset
of most relevant features.

Towards this end, we implement two algorithms:
principal component analysis and sparse autoencoder.
To compare their accuracies, we train both models
under the same initial condition of 2000 inputs and
with the same number of reduced dimensions.

2.2.1 Principal Component Analysis

Principal component analysis selects, via the co-
variance matrix, the k−dimensional linear subspace
spanned by the 28 × 28 dimensional inputs with the
largest correlation to each other. In practice, for 2000
inputs, the accuracy does not vary much when we
lower the input dimension to 100, and we ﬁnd that the
optimal number of reduced dimensions is 80 (Fig.1).

Figure 2: Accuracy ﬂuctuates with varying hidden
dimensions for sparse autoencoder

2.3 Heuristics

2.3.1 Input Discretization

The intensity of the pixels, which ranges from 0 to
255, may be redundant and create extra noise in clas-
siﬁcation since the structure of the digits should be
largely independent of the intensity. Hence, we need
to empirically determine the optimal cutoﬀ in pixel
intensity, discretize the pixel values to 0 or 1 and use
these binary values as the new input for k-means.

Figure 1: Reducing dimension with PCA increases
the classiﬁcation accuracy

2.2.2 Sparse Autoencoder

The sparse autoencoder is a variation of neural net-
work that tries to reconstruct the input through a
small number of hidden neurons, which encode the

The optimal cutoﬀ value we ﬁnd is 130 for 2000
training examples. With this threshold, the eﬀect of
input discretization varies among diﬀerently initial-

2

Handwritten Digit Recognition via Unsupervised

Learning

Xinyi Jiang, Huy Pham, Qingyang Xu

December 11, 2015

Abstract

We present the results of several unsupervised algo-
rithms tested on the MNIST database as well as tech-
niques we used to improve the classiﬁcation accuracy.
We ﬁnd that spiking neural network outperforms k-
means clustering and reaches the same level as the
supervised SVM. We then discuss several inherent
issues of unsupervised methods for the handwritten
digit classﬁcation problem and propose several meth-
ods to further improve the accuracy.

I. Introduction

Handwritten digit recognition is a benchmark test for
computer vision algorithms and has a wide range of
applications from bank check processing to postcode
recognition. Currently the supervised learning meth-
ods yield much higher classiﬁcation accuracy than
most unsupervised ones.

Despite its high accuracy,

supervised learning
methods tend to be very expensive to develop since
they require large training sets to be manually la-
beled. On the other hand, unsupervised approaches
have the potential of being much more eﬃcient to
train and can be generalized to more complicated clas-
siﬁcation problems such as handwritten alphabet or
Chinese characters with low additional cost.

In this project, we aim to implement unsupervised
algorithms and apply novel techniques to improve the
accuracy. We use the MNIST database[1], which con-
sists of 60,000 images in training set and 10,000 in test
set. Each image is a 28 × 28 array of pixels in gray
scale ranging from 0 to 255 and labeled with the digit
it represents.

Our ﬁrst approach is the k-means clustering algo-

rithm, divided into three stages.

We ﬁrst preprocess the data in order to extract
the small subset of most relevant features from the
high dimensional input so the program runs more ef-
fciently. We use feature selection techniques, such as

principal component analysis and sparse autoencoder,
as well as better heuristics such as input discretization
and image centralization.

Then we improve the initialization process of k-
means, either by the probabilistic k-means++ initial-
ization or by choosing the centers which minimize cer-
tain cost function we construct.

Finally, we run the clustering process with certain
modiﬁcations such as k-medians and k-medoids algo-
rithms and gauge their eﬀects.

With the improved feature set and initialization,
our k-means algorithm achieves 60% classiﬁcation ac-
curacy on average, 10% higher than the original.

Spiking neural network is the best performing un-
supervised method for handwritten digit recognition
up-to-date. We reproduced the 89% accuracy of Diehl
and Cook (2015) with the Brian simulator of SNN.

For reference of accuracy, we also run supervised
algorithms such as SVM and neural network. SVM
yields 90% high accuracy, and it also reveals some in-
herent issues of handwritten digit recognition which
might explain the relatively low accuracy of unsuper-
vised approaches. Neural network has a surprisingly
low accuracy around 20%, and we ﬁnd that the ﬁtting
parameters tend to oscillate without converging.

II. Preprocessing Data

2.1 General Methods
We ﬁrst reshape each image from a 28 × 28 array of
pixels into a vector of the same size. The default
metric we used to cluster and classify the images is
the Euclidean distance between them. Since k-means
only assigns the inputs to clusters to minimize the to-
tal distance from each training example to its corre-
sponding cluster centroid, there is no way to directly
calculate the accuracy. Instead, in any given cluster
S, we choose the digit d which appears the most fre-
quently in S and calculate the ratio of all d present

1

most relevant structure of each digit. Each neuron in
the input and output layers corresponds to a pixel in
the training images, and we test to ﬁnd the optimal
number of hidden neurons that leads to lowest train-
ing errors.

We use the sigmoid function as the activation func-
tion for the autoencoder. Hence we need to normal-
ize the input coordinates to between 0 and 1 so the
the autoencoder can eﬀectively reconstruct the input.
Once the training is complete, we step through the
training set again and feed the outputs of the hidden
neuron on each training example as the new input to
the k-means clustering algorithm.

We ﬁnd that the optimal hidden dimension for
training with 2000 examples is 600, with accuracy as
high as PCA. However, the accuracy has a large ﬂuc-
tuation with the hidden dimension (Fig. 2). Also, we
ﬁnd that sparse autoencoder cannot reduce dimen-
sion as eﬀectively as PCA since the accuracy drops
sharply when the hidden dimension gets below 100.

in S and take this as the classiﬁcation accuracy of
S. The total accuracy is obtained by averaging over
accuracies of the ten clusters.

2.2 Feature Selection

The high input dimensions, if weighted equally, not
only causes the program to run extremely slowly but
also obscures the essential underlying structure of
each digit, which is the most crucial in the classiﬁ-
cation process and exactly what our program tries to
learn. Hence we ﬁrst need to reduce the dimension of
the input and replace it with a much smaller subset
of most relevant features.

Towards this end, we implement two algorithms:
principal component analysis and sparse autoencoder.
To compare their accuracies, we train both models
under the same initial condition of 2000 inputs and
with the same number of reduced dimensions.

2.2.1 Principal Component Analysis

Principal component analysis selects, via the co-
variance matrix, the k−dimensional linear subspace
spanned by the 28 × 28 dimensional inputs with the
largest correlation to each other. In practice, for 2000
inputs, the accuracy does not vary much when we
lower the input dimension to 100, and we ﬁnd that the
optimal number of reduced dimensions is 80 (Fig.1).

Figure 2: Accuracy ﬂuctuates with varying hidden
dimensions for sparse autoencoder

2.3 Heuristics

2.3.1 Input Discretization

The intensity of the pixels, which ranges from 0 to
255, may be redundant and create extra noise in clas-
siﬁcation since the structure of the digits should be
largely independent of the intensity. Hence, we need
to empirically determine the optimal cutoﬀ in pixel
intensity, discretize the pixel values to 0 or 1 and use
these binary values as the new input for k-means.

Figure 1: Reducing dimension with PCA increases
the classiﬁcation accuracy

2.2.2 Sparse Autoencoder

The sparse autoencoder is a variation of neural net-
work that tries to reconstruct the input through a
small number of hidden neurons, which encode the

The optimal cutoﬀ value we ﬁnd is 130 for 2000
training examples. With this threshold, the eﬀect of
input discretization varies among diﬀerently initial-

2

ized clustering algorithms (Fig. 3 c.f. Section 3.1).
The accuracy of the probablisitc k-mean++ slightly
improves and the eﬀect is consistent with larger train-
ing sets. However, for deterministically initialized
k-means with PCA, the accuracy is lower with dis-
cretization since we have eliminated too much infor-
mation from the input so that the new covariance
matrix is less eﬀective in revealing the essential cor-
relation between the input components.

III. K-means
Clustering Algorithm

3.1 Improved Initialization

The k-means clustering algorithm is sensitive to the
intialization of the centroids which, if randomly cho-
sen, often converges to local optimum that does not
necessarily yield the best classiﬁcation.

To improve centroid initialization, we implement
the k-means++ heuristic[7], which ﬁrst randomly
chooses a training sample as the center, computes the
distance D(x) of every other inputs to its closest cen-
ter, and then randomly chooses another center from
the other inputs with probability weighted by D(x)2.
It repeats this process to generate all ten centroids.

We also implement a deterministic heuristic which

computes the cost function for each image I

n(cid:88)

i=1

(cid:80)n

d(I, Ii)
k=1 d(Ii, Ik)

C(I) =

Figure 3: The eﬀect of input discretization varies with
determinisitic (left two) and probablistic (right) ini-
tialization.

2.3.2 Image Centralization

The digit is independent of whether it appears slightly
to the left or right of the image, we use the transla-
tional invariance to ﬁx the center of all images at the
same point and reduce uncorrelated variance.

For each image I, we compute its center by the

where d(I, Ii) represents the Euclidean distance be-
tween our image and the ith training example. We
then take the ten images with lowest cost function as
the initial centroids.

Both initialization methods show overall improve-
ment on accuracy (Fig. 4), with k-means++ slightly
outperforming minimization of the cost function. For
comparison, we also try to initialize the centroids with
the ﬁrst ten images of the training set.

weighted average of the pixel values

(cid:34)(cid:80)
(cid:80)
0≤m,n<28 Imn × m

0≤m,n<28 Imn

(

(cid:35)

(cid:34)(cid:80)
(cid:80)
0≤m,n<28 Imn × n
0≤m,n<28 Imn

(cid:35)

)

,

where [x] denotes the integer closest to x. We round
to the closest integer instead of rounding down since
the latter can cause the oﬀ-by-1 shift which is detri-
mental to the classiﬁcation.

We ﬁnd that the images in the MNIST dataset are
already very well-centered. Computing the weighted
average with respect to the original pixel values and
the discretized binary values both give the same cen-
ter (14, 14) for every image in the data set.

Figure 4: K-means++ overall outperforms cost func-
tion minimization with 2000 trianing samples

3

Handwritten Digit Recognition via Unsupervised

Learning

Xinyi Jiang, Huy Pham, Qingyang Xu

December 11, 2015

Abstract

We present the results of several unsupervised algo-
rithms tested on the MNIST database as well as tech-
niques we used to improve the classiﬁcation accuracy.
We ﬁnd that spiking neural network outperforms k-
means clustering and reaches the same level as the
supervised SVM. We then discuss several inherent
issues of unsupervised methods for the handwritten
digit classﬁcation problem and propose several meth-
ods to further improve the accuracy.

I. Introduction

Handwritten digit recognition is a benchmark test for
computer vision algorithms and has a wide range of
applications from bank check processing to postcode
recognition. Currently the supervised learning meth-
ods yield much higher classiﬁcation accuracy than
most unsupervised ones.

Despite its high accuracy,

supervised learning
methods tend to be very expensive to develop since
they require large training sets to be manually la-
beled. On the other hand, unsupervised approaches
have the potential of being much more eﬃcient to
train and can be generalized to more complicated clas-
siﬁcation problems such as handwritten alphabet or
Chinese characters with low additional cost.

In this project, we aim to implement unsupervised
algorithms and apply novel techniques to improve the
accuracy. We use the MNIST database[1], which con-
sists of 60,000 images in training set and 10,000 in test
set. Each image is a 28 × 28 array of pixels in gray
scale ranging from 0 to 255 and labeled with the digit
it represents.

Our ﬁrst approach is the k-means clustering algo-

rithm, divided into three stages.

We ﬁrst preprocess the data in order to extract
the small subset of most relevant features from the
high dimensional input so the program runs more ef-
fciently. We use feature selection techniques, such as

principal component analysis and sparse autoencoder,
as well as better heuristics such as input discretization
and image centralization.

Then we improve the initialization process of k-
means, either by the probabilistic k-means++ initial-
ization or by choosing the centers which minimize cer-
tain cost function we construct.

Finally, we run the clustering process with certain
modiﬁcations such as k-medians and k-medoids algo-
rithms and gauge their eﬀects.

With the improved feature set and initialization,
our k-means algorithm achieves 60% classiﬁcation ac-
curacy on average, 10% higher than the original.

Spiking neural network is the best performing un-
supervised method for handwritten digit recognition
up-to-date. We reproduced the 89% accuracy of Diehl
and Cook (2015) with the Brian simulator of SNN.

For reference of accuracy, we also run supervised
algorithms such as SVM and neural network. SVM
yields 90% high accuracy, and it also reveals some in-
herent issues of handwritten digit recognition which
might explain the relatively low accuracy of unsuper-
vised approaches. Neural network has a surprisingly
low accuracy around 20%, and we ﬁnd that the ﬁtting
parameters tend to oscillate without converging.

II. Preprocessing Data

2.1 General Methods
We ﬁrst reshape each image from a 28 × 28 array of
pixels into a vector of the same size. The default
metric we used to cluster and classify the images is
the Euclidean distance between them. Since k-means
only assigns the inputs to clusters to minimize the to-
tal distance from each training example to its corre-
sponding cluster centroid, there is no way to directly
calculate the accuracy. Instead, in any given cluster
S, we choose the digit d which appears the most fre-
quently in S and calculate the ratio of all d present

1

most relevant structure of each digit. Each neuron in
the input and output layers corresponds to a pixel in
the training images, and we test to ﬁnd the optimal
number of hidden neurons that leads to lowest train-
ing errors.

We use the sigmoid function as the activation func-
tion for the autoencoder. Hence we need to normal-
ize the input coordinates to between 0 and 1 so the
the autoencoder can eﬀectively reconstruct the input.
Once the training is complete, we step through the
training set again and feed the outputs of the hidden
neuron on each training example as the new input to
the k-means clustering algorithm.

We ﬁnd that the optimal hidden dimension for
training with 2000 examples is 600, with accuracy as
high as PCA. However, the accuracy has a large ﬂuc-
tuation with the hidden dimension (Fig. 2). Also, we
ﬁnd that sparse autoencoder cannot reduce dimen-
sion as eﬀectively as PCA since the accuracy drops
sharply when the hidden dimension gets below 100.

in S and take this as the classiﬁcation accuracy of
S. The total accuracy is obtained by averaging over
accuracies of the ten clusters.

2.2 Feature Selection

The high input dimensions, if weighted equally, not
only causes the program to run extremely slowly but
also obscures the essential underlying structure of
each digit, which is the most crucial in the classiﬁ-
cation process and exactly what our program tries to
learn. Hence we ﬁrst need to reduce the dimension of
the input and replace it with a much smaller subset
of most relevant features.

Towards this end, we implement two algorithms:
principal component analysis and sparse autoencoder.
To compare their accuracies, we train both models
under the same initial condition of 2000 inputs and
with the same number of reduced dimensions.

2.2.1 Principal Component Analysis

Principal component analysis selects, via the co-
variance matrix, the k−dimensional linear subspace
spanned by the 28 × 28 dimensional inputs with the
largest correlation to each other. In practice, for 2000
inputs, the accuracy does not vary much when we
lower the input dimension to 100, and we ﬁnd that the
optimal number of reduced dimensions is 80 (Fig.1).

Figure 2: Accuracy ﬂuctuates with varying hidden
dimensions for sparse autoencoder

2.3 Heuristics

2.3.1 Input Discretization

The intensity of the pixels, which ranges from 0 to
255, may be redundant and create extra noise in clas-
siﬁcation since the structure of the digits should be
largely independent of the intensity. Hence, we need
to empirically determine the optimal cutoﬀ in pixel
intensity, discretize the pixel values to 0 or 1 and use
these binary values as the new input for k-means.

Figure 1: Reducing dimension with PCA increases
the classiﬁcation accuracy

2.2.2 Sparse Autoencoder

The sparse autoencoder is a variation of neural net-
work that tries to reconstruct the input through a
small number of hidden neurons, which encode the

The optimal cutoﬀ value we ﬁnd is 130 for 2000
training examples. With this threshold, the eﬀect of
input discretization varies among diﬀerently initial-

2

ized clustering algorithms (Fig. 3 c.f. Section 3.1).
The accuracy of the probablisitc k-mean++ slightly
improves and the eﬀect is consistent with larger train-
ing sets. However, for deterministically initialized
k-means with PCA, the accuracy is lower with dis-
cretization since we have eliminated too much infor-
mation from the input so that the new covariance
matrix is less eﬀective in revealing the essential cor-
relation between the input components.

III. K-means
Clustering Algorithm

3.1 Improved Initialization

The k-means clustering algorithm is sensitive to the
intialization of the centroids which, if randomly cho-
sen, often converges to local optimum that does not
necessarily yield the best classiﬁcation.

To improve centroid initialization, we implement
the k-means++ heuristic[7], which ﬁrst randomly
chooses a training sample as the center, computes the
distance D(x) of every other inputs to its closest cen-
ter, and then randomly chooses another center from
the other inputs with probability weighted by D(x)2.
It repeats this process to generate all ten centroids.

We also implement a deterministic heuristic which

computes the cost function for each image I

n(cid:88)

i=1

(cid:80)n

d(I, Ii)
k=1 d(Ii, Ik)

C(I) =

Figure 3: The eﬀect of input discretization varies with
determinisitic (left two) and probablistic (right) ini-
tialization.

2.3.2 Image Centralization

The digit is independent of whether it appears slightly
to the left or right of the image, we use the transla-
tional invariance to ﬁx the center of all images at the
same point and reduce uncorrelated variance.

For each image I, we compute its center by the

where d(I, Ii) represents the Euclidean distance be-
tween our image and the ith training example. We
then take the ten images with lowest cost function as
the initial centroids.

Both initialization methods show overall improve-
ment on accuracy (Fig. 4), with k-means++ slightly
outperforming minimization of the cost function. For
comparison, we also try to initialize the centroids with
the ﬁrst ten images of the training set.

weighted average of the pixel values

(cid:34)(cid:80)
(cid:80)
0≤m,n<28 Imn × m

0≤m,n<28 Imn

(

(cid:35)

(cid:34)(cid:80)
(cid:80)
0≤m,n<28 Imn × n
0≤m,n<28 Imn

(cid:35)

)

,

where [x] denotes the integer closest to x. We round
to the closest integer instead of rounding down since
the latter can cause the oﬀ-by-1 shift which is detri-
mental to the classiﬁcation.

We ﬁnd that the images in the MNIST dataset are
already very well-centered. Computing the weighted
average with respect to the original pixel values and
the discretized binary values both give the same cen-
ter (14, 14) for every image in the data set.

Figure 4: K-means++ overall outperforms cost func-
tion minimization with 2000 trianing samples

3

3.2 Execution

In order to get a good estimate of average-case ac-
cuarcy and minimize the eﬀect of anomalies in ran-
dom initialization, we need to run the k-means clus-
tering for multiple trials on the same training set to
gauge the overall eﬀect.

Each round of clustering terminates when the cost
function, deﬁned as the sum of Euclidean distances
squared of all
images to their assigned centroids,
converges within 10−2. After each convergence, we
choose at random a training example from each clus-
ter as the new centroids and repeat the clustering to
see if it further minimizes the cost function.

The program terminates when choosing new cen-
troids either has not improved the cost function for
kf ix consecutive trials or when it has improved for
kimp trials in total. For the results below (Fig. 5), we
set kimp = kf ix = 3 and report the averaged accuracy.

and communicate with each other by sending spikes
through synapses. We train the model by updating
the weight associated with each synapse depending
on how frequently spikes travel on it. So far SNN has
the best performance on handwritten digit recogni-
tion and, aided by the Brian simulation package and
their code available online, we are able to reproduce
the 89% accuracy by Diehl and Cook[3].

One issue with the current implementation of SNN
is that after training, we need to label each neuron
with the digit it represents by showing the SNN im-
ages of all digits and see how each neuron responds.
In this sense, SNN still requires labels of the images
and therefore is not completely unsupervised.

V. Supervised
Learning Algorithms

5.1 Support Vector Machine

We use the liblinear library[8] to run SVM and ﬁnd
that it yields high accuracy which increases with
training set size. But certain digits such as 3 and 8
have consistently higher error rate, which means that
their structures are inherently harder to classify.

Figure 5: Accuracy vs. training set size for regular
k-means and k-means++

Rerunning the clustering in the above process in
general yields lower values of the cost function. How-
ever, in practice this does not guarantee a higher clas-
siﬁcation accuracy. Therefore, even with the reduced
input from feature selection, the default metric of Eu-
clidean distance still has limited eﬀect in capturing
the structural similarity between digits.

IV. Spiking Neural
Network

The spiking neural network (SNN) is a biologically
inspired model
in which neurons evolve in time

Figure 6: Error rate of each digit showing certain
digits consistently worse than others

5.2 Neural Network

Our neural network has input and output layers of
the same dimension as the training images. We use
sigmoid function as activation, train the model by
varying the number of hidden neurons and ﬁnd that it
has overall low accuracy around 20%. Close investiga-
tion shows that the ﬁtting parameters often oscillate
without converging during each backward propaga-
tion update. In a few cases when they do converge,

4

Handwritten Digit Recognition via Unsupervised

Learning

Xinyi Jiang, Huy Pham, Qingyang Xu

December 11, 2015

Abstract

We present the results of several unsupervised algo-
rithms tested on the MNIST database as well as tech-
niques we used to improve the classiﬁcation accuracy.
We ﬁnd that spiking neural network outperforms k-
means clustering and reaches the same level as the
supervised SVM. We then discuss several inherent
issues of unsupervised methods for the handwritten
digit classﬁcation problem and propose several meth-
ods to further improve the accuracy.

I. Introduction

Handwritten digit recognition is a benchmark test for
computer vision algorithms and has a wide range of
applications from bank check processing to postcode
recognition. Currently the supervised learning meth-
ods yield much higher classiﬁcation accuracy than
most unsupervised ones.

Despite its high accuracy,

supervised learning
methods tend to be very expensive to develop since
they require large training sets to be manually la-
beled. On the other hand, unsupervised approaches
have the potential of being much more eﬃcient to
train and can be generalized to more complicated clas-
siﬁcation problems such as handwritten alphabet or
Chinese characters with low additional cost.

In this project, we aim to implement unsupervised
algorithms and apply novel techniques to improve the
accuracy. We use the MNIST database[1], which con-
sists of 60,000 images in training set and 10,000 in test
set. Each image is a 28 × 28 array of pixels in gray
scale ranging from 0 to 255 and labeled with the digit
it represents.

Our ﬁrst approach is the k-means clustering algo-

rithm, divided into three stages.

We ﬁrst preprocess the data in order to extract
the small subset of most relevant features from the
high dimensional input so the program runs more ef-
fciently. We use feature selection techniques, such as

principal component analysis and sparse autoencoder,
as well as better heuristics such as input discretization
and image centralization.

Then we improve the initialization process of k-
means, either by the probabilistic k-means++ initial-
ization or by choosing the centers which minimize cer-
tain cost function we construct.

Finally, we run the clustering process with certain
modiﬁcations such as k-medians and k-medoids algo-
rithms and gauge their eﬀects.

With the improved feature set and initialization,
our k-means algorithm achieves 60% classiﬁcation ac-
curacy on average, 10% higher than the original.

Spiking neural network is the best performing un-
supervised method for handwritten digit recognition
up-to-date. We reproduced the 89% accuracy of Diehl
and Cook (2015) with the Brian simulator of SNN.

For reference of accuracy, we also run supervised
algorithms such as SVM and neural network. SVM
yields 90% high accuracy, and it also reveals some in-
herent issues of handwritten digit recognition which
might explain the relatively low accuracy of unsuper-
vised approaches. Neural network has a surprisingly
low accuracy around 20%, and we ﬁnd that the ﬁtting
parameters tend to oscillate without converging.

II. Preprocessing Data

2.1 General Methods
We ﬁrst reshape each image from a 28 × 28 array of
pixels into a vector of the same size. The default
metric we used to cluster and classify the images is
the Euclidean distance between them. Since k-means
only assigns the inputs to clusters to minimize the to-
tal distance from each training example to its corre-
sponding cluster centroid, there is no way to directly
calculate the accuracy. Instead, in any given cluster
S, we choose the digit d which appears the most fre-
quently in S and calculate the ratio of all d present

1

most relevant structure of each digit. Each neuron in
the input and output layers corresponds to a pixel in
the training images, and we test to ﬁnd the optimal
number of hidden neurons that leads to lowest train-
ing errors.

We use the sigmoid function as the activation func-
tion for the autoencoder. Hence we need to normal-
ize the input coordinates to between 0 and 1 so the
the autoencoder can eﬀectively reconstruct the input.
Once the training is complete, we step through the
training set again and feed the outputs of the hidden
neuron on each training example as the new input to
the k-means clustering algorithm.

We ﬁnd that the optimal hidden dimension for
training with 2000 examples is 600, with accuracy as
high as PCA. However, the accuracy has a large ﬂuc-
tuation with the hidden dimension (Fig. 2). Also, we
ﬁnd that sparse autoencoder cannot reduce dimen-
sion as eﬀectively as PCA since the accuracy drops
sharply when the hidden dimension gets below 100.

in S and take this as the classiﬁcation accuracy of
S. The total accuracy is obtained by averaging over
accuracies of the ten clusters.

2.2 Feature Selection

The high input dimensions, if weighted equally, not
only causes the program to run extremely slowly but
also obscures the essential underlying structure of
each digit, which is the most crucial in the classiﬁ-
cation process and exactly what our program tries to
learn. Hence we ﬁrst need to reduce the dimension of
the input and replace it with a much smaller subset
of most relevant features.

Towards this end, we implement two algorithms:
principal component analysis and sparse autoencoder.
To compare their accuracies, we train both models
under the same initial condition of 2000 inputs and
with the same number of reduced dimensions.

2.2.1 Principal Component Analysis

Principal component analysis selects, via the co-
variance matrix, the k−dimensional linear subspace
spanned by the 28 × 28 dimensional inputs with the
largest correlation to each other. In practice, for 2000
inputs, the accuracy does not vary much when we
lower the input dimension to 100, and we ﬁnd that the
optimal number of reduced dimensions is 80 (Fig.1).

Figure 2: Accuracy ﬂuctuates with varying hidden
dimensions for sparse autoencoder

2.3 Heuristics

2.3.1 Input Discretization

The intensity of the pixels, which ranges from 0 to
255, may be redundant and create extra noise in clas-
siﬁcation since the structure of the digits should be
largely independent of the intensity. Hence, we need
to empirically determine the optimal cutoﬀ in pixel
intensity, discretize the pixel values to 0 or 1 and use
these binary values as the new input for k-means.

Figure 1: Reducing dimension with PCA increases
the classiﬁcation accuracy

2.2.2 Sparse Autoencoder

The sparse autoencoder is a variation of neural net-
work that tries to reconstruct the input through a
small number of hidden neurons, which encode the

The optimal cutoﬀ value we ﬁnd is 130 for 2000
training examples. With this threshold, the eﬀect of
input discretization varies among diﬀerently initial-

2

ized clustering algorithms (Fig. 3 c.f. Section 3.1).
The accuracy of the probablisitc k-mean++ slightly
improves and the eﬀect is consistent with larger train-
ing sets. However, for deterministically initialized
k-means with PCA, the accuracy is lower with dis-
cretization since we have eliminated too much infor-
mation from the input so that the new covariance
matrix is less eﬀective in revealing the essential cor-
relation between the input components.

III. K-means
Clustering Algorithm

3.1 Improved Initialization

The k-means clustering algorithm is sensitive to the
intialization of the centroids which, if randomly cho-
sen, often converges to local optimum that does not
necessarily yield the best classiﬁcation.

To improve centroid initialization, we implement
the k-means++ heuristic[7], which ﬁrst randomly
chooses a training sample as the center, computes the
distance D(x) of every other inputs to its closest cen-
ter, and then randomly chooses another center from
the other inputs with probability weighted by D(x)2.
It repeats this process to generate all ten centroids.

We also implement a deterministic heuristic which

computes the cost function for each image I

n(cid:88)

i=1

(cid:80)n

d(I, Ii)
k=1 d(Ii, Ik)

C(I) =

Figure 3: The eﬀect of input discretization varies with
determinisitic (left two) and probablistic (right) ini-
tialization.

2.3.2 Image Centralization

The digit is independent of whether it appears slightly
to the left or right of the image, we use the transla-
tional invariance to ﬁx the center of all images at the
same point and reduce uncorrelated variance.

For each image I, we compute its center by the

where d(I, Ii) represents the Euclidean distance be-
tween our image and the ith training example. We
then take the ten images with lowest cost function as
the initial centroids.

Both initialization methods show overall improve-
ment on accuracy (Fig. 4), with k-means++ slightly
outperforming minimization of the cost function. For
comparison, we also try to initialize the centroids with
the ﬁrst ten images of the training set.

weighted average of the pixel values

(cid:34)(cid:80)
(cid:80)
0≤m,n<28 Imn × m

0≤m,n<28 Imn

(

(cid:35)

(cid:34)(cid:80)
(cid:80)
0≤m,n<28 Imn × n
0≤m,n<28 Imn

(cid:35)

)

,

where [x] denotes the integer closest to x. We round
to the closest integer instead of rounding down since
the latter can cause the oﬀ-by-1 shift which is detri-
mental to the classiﬁcation.

We ﬁnd that the images in the MNIST dataset are
already very well-centered. Computing the weighted
average with respect to the original pixel values and
the discretized binary values both give the same cen-
ter (14, 14) for every image in the data set.

Figure 4: K-means++ overall outperforms cost func-
tion minimization with 2000 trianing samples

3

3.2 Execution

In order to get a good estimate of average-case ac-
cuarcy and minimize the eﬀect of anomalies in ran-
dom initialization, we need to run the k-means clus-
tering for multiple trials on the same training set to
gauge the overall eﬀect.

Each round of clustering terminates when the cost
function, deﬁned as the sum of Euclidean distances
squared of all
images to their assigned centroids,
converges within 10−2. After each convergence, we
choose at random a training example from each clus-
ter as the new centroids and repeat the clustering to
see if it further minimizes the cost function.

The program terminates when choosing new cen-
troids either has not improved the cost function for
kf ix consecutive trials or when it has improved for
kimp trials in total. For the results below (Fig. 5), we
set kimp = kf ix = 3 and report the averaged accuracy.

and communicate with each other by sending spikes
through synapses. We train the model by updating
the weight associated with each synapse depending
on how frequently spikes travel on it. So far SNN has
the best performance on handwritten digit recogni-
tion and, aided by the Brian simulation package and
their code available online, we are able to reproduce
the 89% accuracy by Diehl and Cook[3].

One issue with the current implementation of SNN
is that after training, we need to label each neuron
with the digit it represents by showing the SNN im-
ages of all digits and see how each neuron responds.
In this sense, SNN still requires labels of the images
and therefore is not completely unsupervised.

V. Supervised
Learning Algorithms

5.1 Support Vector Machine

We use the liblinear library[8] to run SVM and ﬁnd
that it yields high accuracy which increases with
training set size. But certain digits such as 3 and 8
have consistently higher error rate, which means that
their structures are inherently harder to classify.

Figure 5: Accuracy vs. training set size for regular
k-means and k-means++

Rerunning the clustering in the above process in
general yields lower values of the cost function. How-
ever, in practice this does not guarantee a higher clas-
siﬁcation accuracy. Therefore, even with the reduced
input from feature selection, the default metric of Eu-
clidean distance still has limited eﬀect in capturing
the structural similarity between digits.

IV. Spiking Neural
Network

The spiking neural network (SNN) is a biologically
inspired model
in which neurons evolve in time

Figure 6: Error rate of each digit showing certain
digits consistently worse than others

5.2 Neural Network

Our neural network has input and output layers of
the same dimension as the training images. We use
sigmoid function as activation, train the model by
varying the number of hidden neurons and ﬁnd that it
has overall low accuracy around 20%. Close investiga-
tion shows that the ﬁtting parameters often oscillate
without converging during each backward propaga-
tion update. In a few cases when they do converge,

4

however, these parameters yield very high error. The
problem persists when we instead use softmax func-
tion to compute activation.

VI. Discussion and
Future Work

The overall performance of each algorithm we imple-
ment is summarized in Fig. 7 below:

tially quantify the structural similarity between im-
ages which may correctly suggest whether they rep-
resent the same digit. The current Euclidean metric
does not take into account the position of the pixels,
and we tried another metric which scales the pixel val-
ues according to the distance from the center, but it
yields even lower accuracy. Still, we should try to con-
struct better cost functions whose output eﬀectively
quantiﬁes similarity.

Finally, as mentioned in Part IV, the current im-
plementation of SNN is not strictly unsupervised. So
it still has the disadvantage of requiring large, manu-
ally labeled traing sets. Based on the work by Diehl
and Cook, we would like to correctly classify the ex-
citatory neurons in SNN without labels. This would
make SNN much less expensive and much easier to
generalize into other situations.

VII. Reference

Figure 7: Overall accuracy of each algorithm

1. MNIST database

of

handwritten

digits:

http://yann.lecun.com/exdb/mnist/

2. Andrew Ng. Lecture notes on Neural Network

and Sparse Autoencoder for CS294A.

3. Peter Diehl, Matthew Cook.

Unsupervised
Learning of Digit Recognition Using Spike-
Timing-Dependent Plasticity.
IEEE TRANS-
ACTIONS IN NEURAL NETWORKS AND
LEARNING SYSTEMS.

4. Hae-Sang Park, Chi-Hyuck Jun. A simple and
fast algorithm for K-medoids clustering. Expert
Systems with Applications 36 (2009).

5. Andrew Ng. CS229 lecture notes.

6. Stanford Unsupervised Feature Learning and

Deep Learning Tutorial.

7. David Arthur and Sergei Vassilvitskii.

k-
means++: The Advantages of Careful Seeding.
SODA ’07 Proceedings of the eighteenth annual
ACM-SIAM symposium on Discrete algorithms
Pages 1027-1035.

8. Chih-Chung Chang and Chih-Jen Lin. LIBSVM
: a library for support vector machines. ACM
Transactions on Intelligent Systems and Tech-
nology, 2:27:1–27:27, 2011. Software available at
http://www.csie.ntu.edu.tw/˜cjlin/libsvm.

We improve k-means clustering with PCA and
sparse autoencoder to reduce the dimension of the
input, as well as k-mean++ and cost function to op-
timize initialization. Overall, the improved versions
of k-means achieve 60% accuracy when we train with
2000 inputs. K-means++ and PCA run fairly quickly
while sparse autoencoder takes longer to train.

One inherent issue with handwritten digit recogni-
tion is that when we look at the distribution of digits
inside each cluster after running k-means, we usually
ﬁnd that digits 0, 1, 2, 6 tend to be much better clas-
siﬁed into their respective clusters than 3, 5, and 8.
The error rate of each digit in SVM (Fig. 6) conﬁrms
that the structures of 3, 5, and 8 are inherently harder
to classify even in supervised setting.

We believe the underlying cause of this problem
is that the various optimizations we implement to k-
means mainly improve the clustering eﬃciency. How-
ever, as discussed in Section 3.2, higher clustering ef-
ﬁciency does not guarantee higher classiﬁcation ac-
curacy if the algorithm cannot correctly tell one digit
from another. Therefore, future work should focus on
improving the eﬀectiveness of our algorithm to recog-
nize the structural distinction of each digit

One possible solution, inspired by the generative
learning models, is to ﬁrst construct prior models to
encode the most essential structure of each digit and
then perform clustering with these prior models as ei-
ther centroids or reference to how we select centroids.
Another remedy to the same problem is to improve
the cost function which is minimized by each itera-
tion of clusteirng. The cost function should essen-

5

