CLASSIFICATION OF HIGGS JETS AS DECAY PRODUCTS OF A RANDALL-SUNDRUM

GRAVITON AT THE ATLAS EXPERIMENT

AVIV CUKIERMAN, ZIHAO JIANG

1. Introduction

In 2012, physicists at the Large Hadron Collider announced the discovery of the Higgs boson, which earned the moniker
“God Particle” in the popular media due to its far-reaching theoretical implications. The Standard Model, which is by
far the most successful model we have describing matter and its interactions, predicts that the Higgs boson exists and
gives mass to quarks. The Higgs boson was the ﬁnal particle predicted by the Standard Model that had not yet been
discovered and so the discovery was a major triumph for the Standard Model. However, there are problems with the
Standard Model. For instance, there is a question of why the mass of the Higgs itself is so small, when naive theoretical
extensions of the Standard Model predict it should be much higher.

Among all the more sophisticated models which account for this problem, one model that yields detectable quantities
is the Randall-Sundrum Model, which predicts there is a new particle, a Graviton, which can decay to two Higgs bosons .
A search for such a particle requires distinguishing the Higgs bosons from background processes, mostly quark and gluon
showering from quantum chromodynamic (QCD) eﬀects. In this project, we use machine learning techniques to distinguish
the signal of a Randall-Sundrum Graviton (RSG) decay to two Higgs from the QCD background, using kinematic variables
that would be observed in the ATLAS detector at CERN. In particular, we look at the case where each of the Higgs
bosons decay to a pair of bottom-antibottom (bb) quarks.

2. Data & Features

The study uses Monte Carlo simulated data with full reconstruction of ATLAS detector response. For the signal, we
assume the mass of the RSM Graviton to be 1000 GeV/c2, consisting of 24779 events. For the background, we use ATLAS
tuned QCD simulations consisting of 25944 unweighted events.

In a signal event we have two Higgs bosons, each of which decay to a bb pair of quarks. These quarks are are observed as
collimated energy deposits in the detector, which are called jets. We know from physics that in fact the quark-antiquark
pair will be detected with small angular separation in the detector, and so the quark jets form the sub-structure of a
larger Higgs jet. From these energy deposits we can reconstruct the space-time four momenta of the two quarks and the
Higgs bosons (→ 3 ∗ 4 = 12 features). In addition, we have b-tagging variables constructed from other observables that
describe the probability that the quarks are bottoms (→ 2 features). So in total we have (12+2=14) features for each
potential Higgs jet.

3. Models

3.1. Naive Bayes.

is p(F1, ..., Fn|X) =(cid:81)n

2πσ2
i

2σ2
i

The Naive Bayes model holds the assumption that the probability of a set of features {Fn} conditioned on a variable X
i p(Fi|X) In our case, X = 0, 1 denoting whether the a jet is or is not a Higgs boson. The features
are the continuous kinematic variables of the jets and the sub-jets. We model the conditional probabilities as a Gaussian
p(Fi|X) =
, where the parameters µ and σ are calculated directly from the mean and RMS of the
kinematic variables.

exp− (x−µi)2

1√

Heuristically, the features that are most relevant to this classiﬁcation problem are the invariant mass and transverse
momenta (pT) of the jets. Higgs jets have invariant mass around 125 GeV, while the QCD jets have smaller mass because
the showering of gluon and quarks usually produces light mesons. The mass and energy of sub-jets (decay products) are
predictive. The Higgs tend to decay to heavy-ﬂavor quarks like Bottom and Charm, while the sub-jets in QCD come from
radiation of light particles.

For the Naive Bayes algorithm, we consider the parameters in the following order: mass of the jet, pT of the jet, mass
of the two sub-jets, pT of the two sub-jets, and b-tagging score of the two sub-jets. We add these variables to our feature
set one by one, and each time we re-run the learning algorithm and testing for 100 iterations. For each iteration, we use
70% of the signal and background to train the Gaussian parameters and test the prediction on the remaining 30% of the
data set. We take the training error and testing error as the mean of these 100 iterations and RMS as the uncertainties.
The results of this algorithm can be seen in Figure 1. The tagger training and testing errors quickly get down to percent
level as we increase the size of the feature set. For the signal, the errors are getting signiﬁcantly smaller as we consider

1

CLASSIFICATION OF HIGGS JETS AS DECAY PRODUCTS OF A RANDALL-SUNDRUM

GRAVITON AT THE ATLAS EXPERIMENT

AVIV CUKIERMAN, ZIHAO JIANG

1. Introduction

In 2012, physicists at the Large Hadron Collider announced the discovery of the Higgs boson, which earned the moniker
“God Particle” in the popular media due to its far-reaching theoretical implications. The Standard Model, which is by
far the most successful model we have describing matter and its interactions, predicts that the Higgs boson exists and
gives mass to quarks. The Higgs boson was the ﬁnal particle predicted by the Standard Model that had not yet been
discovered and so the discovery was a major triumph for the Standard Model. However, there are problems with the
Standard Model. For instance, there is a question of why the mass of the Higgs itself is so small, when naive theoretical
extensions of the Standard Model predict it should be much higher.

Among all the more sophisticated models which account for this problem, one model that yields detectable quantities
is the Randall-Sundrum Model, which predicts there is a new particle, a Graviton, which can decay to two Higgs bosons .
A search for such a particle requires distinguishing the Higgs bosons from background processes, mostly quark and gluon
showering from quantum chromodynamic (QCD) eﬀects. In this project, we use machine learning techniques to distinguish
the signal of a Randall-Sundrum Graviton (RSG) decay to two Higgs from the QCD background, using kinematic variables
that would be observed in the ATLAS detector at CERN. In particular, we look at the case where each of the Higgs
bosons decay to a pair of bottom-antibottom (bb) quarks.

2. Data & Features

The study uses Monte Carlo simulated data with full reconstruction of ATLAS detector response. For the signal, we
assume the mass of the RSM Graviton to be 1000 GeV/c2, consisting of 24779 events. For the background, we use ATLAS
tuned QCD simulations consisting of 25944 unweighted events.

In a signal event we have two Higgs bosons, each of which decay to a bb pair of quarks. These quarks are are observed as
collimated energy deposits in the detector, which are called jets. We know from physics that in fact the quark-antiquark
pair will be detected with small angular separation in the detector, and so the quark jets form the sub-structure of a
larger Higgs jet. From these energy deposits we can reconstruct the space-time four momenta of the two quarks and the
Higgs bosons (→ 3 ∗ 4 = 12 features). In addition, we have b-tagging variables constructed from other observables that
describe the probability that the quarks are bottoms (→ 2 features). So in total we have (12+2=14) features for each
potential Higgs jet.

3. Models

3.1. Naive Bayes.

is p(F1, ..., Fn|X) =(cid:81)n

2πσ2
i

2σ2
i

The Naive Bayes model holds the assumption that the probability of a set of features {Fn} conditioned on a variable X
i p(Fi|X) In our case, X = 0, 1 denoting whether the a jet is or is not a Higgs boson. The features
are the continuous kinematic variables of the jets and the sub-jets. We model the conditional probabilities as a Gaussian
p(Fi|X) =
, where the parameters µ and σ are calculated directly from the mean and RMS of the
kinematic variables.

exp− (x−µi)2

1√

Heuristically, the features that are most relevant to this classiﬁcation problem are the invariant mass and transverse
momenta (pT) of the jets. Higgs jets have invariant mass around 125 GeV, while the QCD jets have smaller mass because
the showering of gluon and quarks usually produces light mesons. The mass and energy of sub-jets (decay products) are
predictive. The Higgs tend to decay to heavy-ﬂavor quarks like Bottom and Charm, while the sub-jets in QCD come from
radiation of light particles.

For the Naive Bayes algorithm, we consider the parameters in the following order: mass of the jet, pT of the jet, mass
of the two sub-jets, pT of the two sub-jets, and b-tagging score of the two sub-jets. We add these variables to our feature
set one by one, and each time we re-run the learning algorithm and testing for 100 iterations. For each iteration, we use
70% of the signal and background to train the Gaussian parameters and test the prediction on the remaining 30% of the
data set. We take the training error and testing error as the mean of these 100 iterations and RMS as the uncertainties.
The results of this algorithm can be seen in Figure 1. The tagger training and testing errors quickly get down to percent
level as we increase the size of the feature set. For the signal, the errors are getting signiﬁcantly smaller as we consider

1

2

AVIV CUKIERMAN, ZIHAO JIANG

more features. However including the sub-jets information doesn’t improve the performance of tagging background. We
also note that that after including the ﬁrst two features (mass and pT of the Higgs jet), almost all of the discriminative
power is achieved.

Figure 1. Naive Bayes Driven Higgs Tagger. With 8 features, the signal error was 5% and the back-
ground error was 8%.

3.2. Logistic Regression.

In logistic regression we have a hypothesis of the form

hθ(x) = g(θT x) =

1

1 + exp(−θT x)

Where x is a (D + 1)-dimensional vector with x0 = 1 and x1,...,D the features, and θ ∈ RD+1 are the parameters of the
model. The prediction for y ∈ {0, 1} is 1{hθ(x) > 0.5}.

We split the data into 70% training set and 30% testing set. We optimize the parameters of the regression via batch

gradient ascent on the log-likelihood function on the training set. I.e. we run the following algorithm:
Repeat for N iterations:

Where in the algorithm we set α = 1, and x(1), ..., x(m) is the training set.

We then calculate the error in the testing set (and training set) by calculating

θj+1 := θj + α(cid:80)m
(cid:80)mTest/Train
(cid:80)mTest/Train

i=1

Test/Train
Signal,N =

Test/Train
Background,N =

i=1

i=1(y(i) − hθ(x(i)))x(i)

j

1{y(i) = 1}1{hθ(x(i)) < 0.5}}

mTest/Train

Signal

1{y(i) = 0}1{hθ(x(i)) > 0.5}}

mTest/Train
Background

For each N .

Test/Train
N

= Test/Train

Signal,N + Test/Train

Background,N

Sometimes Train

M > Train

N

for M > N as the algorithm moves out of local minima. So we look at

M∗ = argmini∈{1,...,N}(Train

i

)

Since in practice we can choose whatever θj minimizes the training error over the course of the algorithm, which isn’t
necessarily θN .

Running the algorithm with all the features, we get convergence of Train

M∗,min at around N = 1000 iterations, which
In order to compare across algorithms, we are concerned with the minimum total error when

can be seen in Fig. 2.
Signal ∼ 0.05. So we deﬁne similarly

S = {i : |Train
The results after N = 5000 iterations are shown in Table 1.

M∗

5% = argmini∈S(Train

i

)

Signal,i − 0.05| < 0.01}

M = M∗
M = M∗

5%

Train
Signal,M Train
0.0291
0.0501

Background,M Train
0.1426
0.1467

0.1135
0.0966

M

Test
Signal,M Test
0.0258
0.0471

0.1129
0.0940

Background,M

Test
M
0.1387
0.1411

CLASSIFICATION OF HIGGS JETS AS DECAY PRODUCTS OF A RANDALL-SUNDRUM

GRAVITON AT THE ATLAS EXPERIMENT

AVIV CUKIERMAN, ZIHAO JIANG

1. Introduction

In 2012, physicists at the Large Hadron Collider announced the discovery of the Higgs boson, which earned the moniker
“God Particle” in the popular media due to its far-reaching theoretical implications. The Standard Model, which is by
far the most successful model we have describing matter and its interactions, predicts that the Higgs boson exists and
gives mass to quarks. The Higgs boson was the ﬁnal particle predicted by the Standard Model that had not yet been
discovered and so the discovery was a major triumph for the Standard Model. However, there are problems with the
Standard Model. For instance, there is a question of why the mass of the Higgs itself is so small, when naive theoretical
extensions of the Standard Model predict it should be much higher.

Among all the more sophisticated models which account for this problem, one model that yields detectable quantities
is the Randall-Sundrum Model, which predicts there is a new particle, a Graviton, which can decay to two Higgs bosons .
A search for such a particle requires distinguishing the Higgs bosons from background processes, mostly quark and gluon
showering from quantum chromodynamic (QCD) eﬀects. In this project, we use machine learning techniques to distinguish
the signal of a Randall-Sundrum Graviton (RSG) decay to two Higgs from the QCD background, using kinematic variables
that would be observed in the ATLAS detector at CERN. In particular, we look at the case where each of the Higgs
bosons decay to a pair of bottom-antibottom (bb) quarks.

2. Data & Features

The study uses Monte Carlo simulated data with full reconstruction of ATLAS detector response. For the signal, we
assume the mass of the RSM Graviton to be 1000 GeV/c2, consisting of 24779 events. For the background, we use ATLAS
tuned QCD simulations consisting of 25944 unweighted events.

In a signal event we have two Higgs bosons, each of which decay to a bb pair of quarks. These quarks are are observed as
collimated energy deposits in the detector, which are called jets. We know from physics that in fact the quark-antiquark
pair will be detected with small angular separation in the detector, and so the quark jets form the sub-structure of a
larger Higgs jet. From these energy deposits we can reconstruct the space-time four momenta of the two quarks and the
Higgs bosons (→ 3 ∗ 4 = 12 features). In addition, we have b-tagging variables constructed from other observables that
describe the probability that the quarks are bottoms (→ 2 features). So in total we have (12+2=14) features for each
potential Higgs jet.

3. Models

3.1. Naive Bayes.

is p(F1, ..., Fn|X) =(cid:81)n

2πσ2
i

2σ2
i

The Naive Bayes model holds the assumption that the probability of a set of features {Fn} conditioned on a variable X
i p(Fi|X) In our case, X = 0, 1 denoting whether the a jet is or is not a Higgs boson. The features
are the continuous kinematic variables of the jets and the sub-jets. We model the conditional probabilities as a Gaussian
p(Fi|X) =
, where the parameters µ and σ are calculated directly from the mean and RMS of the
kinematic variables.

exp− (x−µi)2

1√

Heuristically, the features that are most relevant to this classiﬁcation problem are the invariant mass and transverse
momenta (pT) of the jets. Higgs jets have invariant mass around 125 GeV, while the QCD jets have smaller mass because
the showering of gluon and quarks usually produces light mesons. The mass and energy of sub-jets (decay products) are
predictive. The Higgs tend to decay to heavy-ﬂavor quarks like Bottom and Charm, while the sub-jets in QCD come from
radiation of light particles.

For the Naive Bayes algorithm, we consider the parameters in the following order: mass of the jet, pT of the jet, mass
of the two sub-jets, pT of the two sub-jets, and b-tagging score of the two sub-jets. We add these variables to our feature
set one by one, and each time we re-run the learning algorithm and testing for 100 iterations. For each iteration, we use
70% of the signal and background to train the Gaussian parameters and test the prediction on the remaining 30% of the
data set. We take the training error and testing error as the mean of these 100 iterations and RMS as the uncertainties.
The results of this algorithm can be seen in Figure 1. The tagger training and testing errors quickly get down to percent
level as we increase the size of the feature set. For the signal, the errors are getting signiﬁcantly smaller as we consider

1

2

AVIV CUKIERMAN, ZIHAO JIANG

more features. However including the sub-jets information doesn’t improve the performance of tagging background. We
also note that that after including the ﬁrst two features (mass and pT of the Higgs jet), almost all of the discriminative
power is achieved.

Figure 1. Naive Bayes Driven Higgs Tagger. With 8 features, the signal error was 5% and the back-
ground error was 8%.

3.2. Logistic Regression.

In logistic regression we have a hypothesis of the form

hθ(x) = g(θT x) =

1

1 + exp(−θT x)

Where x is a (D + 1)-dimensional vector with x0 = 1 and x1,...,D the features, and θ ∈ RD+1 are the parameters of the
model. The prediction for y ∈ {0, 1} is 1{hθ(x) > 0.5}.

We split the data into 70% training set and 30% testing set. We optimize the parameters of the regression via batch

gradient ascent on the log-likelihood function on the training set. I.e. we run the following algorithm:
Repeat for N iterations:

Where in the algorithm we set α = 1, and x(1), ..., x(m) is the training set.

We then calculate the error in the testing set (and training set) by calculating

θj+1 := θj + α(cid:80)m
(cid:80)mTest/Train
(cid:80)mTest/Train

i=1

Test/Train
Signal,N =

Test/Train
Background,N =

i=1

i=1(y(i) − hθ(x(i)))x(i)

j

1{y(i) = 1}1{hθ(x(i)) < 0.5}}

mTest/Train

Signal

1{y(i) = 0}1{hθ(x(i)) > 0.5}}

mTest/Train
Background

For each N .

Test/Train
N

= Test/Train

Signal,N + Test/Train

Background,N

Sometimes Train

M > Train

N

for M > N as the algorithm moves out of local minima. So we look at

M∗ = argmini∈{1,...,N}(Train

i

)

Since in practice we can choose whatever θj minimizes the training error over the course of the algorithm, which isn’t
necessarily θN .

Running the algorithm with all the features, we get convergence of Train

M∗,min at around N = 1000 iterations, which
In order to compare across algorithms, we are concerned with the minimum total error when

can be seen in Fig. 2.
Signal ∼ 0.05. So we deﬁne similarly

S = {i : |Train
The results after N = 5000 iterations are shown in Table 1.

M∗

5% = argmini∈S(Train

i

)

Signal,i − 0.05| < 0.01}

M = M∗
M = M∗

5%

Train
Signal,M Train
0.0291
0.0501

Background,M Train
0.1426
0.1467

0.1135
0.0966

M

Test
Signal,M Test
0.0258
0.0471

0.1129
0.0940

Background,M

Test
M
0.1387
0.1411

CLASSIFICATION OF HIGGS JETS AS DECAY PRODUCTS OF A RANDALL-SUNDRUM GRAVITON AT THE ATLAS EXPERIMENT 3

Figure 2. Train/Test
converged to within a few percent level of the N → ∞ limit.

N,min

as a function of N using all the features. By about N = 1000, the algorithm has

Table 1. Results of logistic regression algorithm.

3.3. k-clustering. We examine a signiﬁcantly modiﬁed k-means clustering algorithm designed by us. First, we run an
ordinary unsupervised clustering algorithm on the training data:
1. Choose k centroids µ1, ..., µk randomly from the data
2. For every i, set c(i) = argminj||x(i) − µj||2
i=1 1{c(i) = j} ∗ x(i)
4. Repeat the last two steps until convergence

3. For every j, set µj =(cid:80)m
5. Repeat steps 1-4 a few times to globally minimize the cost function J =(cid:80)m

i=1 ||x(i) − µc(i)||2.

Then we supervise the unsupervised algorithm. For each cluster j we look at what fraction of the data classiﬁed in that
cluster is signal:

(cid:80)m
(cid:80)m
i=1 1{y(i) = 1}1{c(i) = j}
i=1 1{c(i) = j}

fj =

We then set a signal threshold s and background threshold b to label signal (fj > s) and background (fj < b) centroids,
respectively. Then using only the centroids that passed the signal or background threshold cuts (i.e. the number of clusters
used in the testing phase kTest ≤ k), we cluster and label the testing data to get signal and background identiﬁcation
eﬃciencies:

eTest/Train
Signal

(s) =

1

mTest/Train

Signal

y(i)1{fc(i) > s}

mTest/Train

Signal(cid:88)
Background(cid:88)

i=1

mTest/Train

eTest/Train
Background(b) =

And the error  = 1 − e.

1

mTest/Train
Background

i=1

(1 − y(i))1{fc(i) < b}

By varying s and b, we get a ROC curve, which is shown in Figure 3.

3.4. Multivariate Analysis. There are correlations between diﬀerent kinematics variables in the dataset we consider.
For instance, both the Higgs jets and QCD background jets will split to two sub-jets of the same amount of energy.
Therefore, the pT of the sub-jets are roughly the same. To account for such correlations we implement more advanced
multivariate analysis tools BDT (Boosted Decision Tree) and k-NN (kth nearest neighborhood) which are widely used in
ATLAS Collaboration.

3.4.1. Boosted Decision Tree. The Boosted Decision Tree (BDT) is an ensemble of weak classiﬁers called decision trees.
The algorithm involves two steps: (1) building a forest of decision trees, and (2) adaptive boosting. A decision tree is
a multi-layer binary classiﬁer. The training starts with the root node and splits to two subsets, each of which searches
for the best variable and its corresponding cut based on calculation of the Gini Index, deﬁned as p(1 − p), where p is
the purity of the sample. If in a sample, signal and background are mixed equally then p = 0.5 and the Gini Index is
maximized, while it falls to zero if the sample is entirely signal or background. In this search step, we partition each
variable into 20 equal segments and decide the cut based on the maximum change between the parent node Gini Index

CLASSIFICATION OF HIGGS JETS AS DECAY PRODUCTS OF A RANDALL-SUNDRUM

GRAVITON AT THE ATLAS EXPERIMENT

AVIV CUKIERMAN, ZIHAO JIANG

1. Introduction

In 2012, physicists at the Large Hadron Collider announced the discovery of the Higgs boson, which earned the moniker
“God Particle” in the popular media due to its far-reaching theoretical implications. The Standard Model, which is by
far the most successful model we have describing matter and its interactions, predicts that the Higgs boson exists and
gives mass to quarks. The Higgs boson was the ﬁnal particle predicted by the Standard Model that had not yet been
discovered and so the discovery was a major triumph for the Standard Model. However, there are problems with the
Standard Model. For instance, there is a question of why the mass of the Higgs itself is so small, when naive theoretical
extensions of the Standard Model predict it should be much higher.

Among all the more sophisticated models which account for this problem, one model that yields detectable quantities
is the Randall-Sundrum Model, which predicts there is a new particle, a Graviton, which can decay to two Higgs bosons .
A search for such a particle requires distinguishing the Higgs bosons from background processes, mostly quark and gluon
showering from quantum chromodynamic (QCD) eﬀects. In this project, we use machine learning techniques to distinguish
the signal of a Randall-Sundrum Graviton (RSG) decay to two Higgs from the QCD background, using kinematic variables
that would be observed in the ATLAS detector at CERN. In particular, we look at the case where each of the Higgs
bosons decay to a pair of bottom-antibottom (bb) quarks.

2. Data & Features

The study uses Monte Carlo simulated data with full reconstruction of ATLAS detector response. For the signal, we
assume the mass of the RSM Graviton to be 1000 GeV/c2, consisting of 24779 events. For the background, we use ATLAS
tuned QCD simulations consisting of 25944 unweighted events.

In a signal event we have two Higgs bosons, each of which decay to a bb pair of quarks. These quarks are are observed as
collimated energy deposits in the detector, which are called jets. We know from physics that in fact the quark-antiquark
pair will be detected with small angular separation in the detector, and so the quark jets form the sub-structure of a
larger Higgs jet. From these energy deposits we can reconstruct the space-time four momenta of the two quarks and the
Higgs bosons (→ 3 ∗ 4 = 12 features). In addition, we have b-tagging variables constructed from other observables that
describe the probability that the quarks are bottoms (→ 2 features). So in total we have (12+2=14) features for each
potential Higgs jet.

3. Models

3.1. Naive Bayes.

is p(F1, ..., Fn|X) =(cid:81)n

2πσ2
i

2σ2
i

The Naive Bayes model holds the assumption that the probability of a set of features {Fn} conditioned on a variable X
i p(Fi|X) In our case, X = 0, 1 denoting whether the a jet is or is not a Higgs boson. The features
are the continuous kinematic variables of the jets and the sub-jets. We model the conditional probabilities as a Gaussian
p(Fi|X) =
, where the parameters µ and σ are calculated directly from the mean and RMS of the
kinematic variables.

exp− (x−µi)2

1√

Heuristically, the features that are most relevant to this classiﬁcation problem are the invariant mass and transverse
momenta (pT) of the jets. Higgs jets have invariant mass around 125 GeV, while the QCD jets have smaller mass because
the showering of gluon and quarks usually produces light mesons. The mass and energy of sub-jets (decay products) are
predictive. The Higgs tend to decay to heavy-ﬂavor quarks like Bottom and Charm, while the sub-jets in QCD come from
radiation of light particles.

For the Naive Bayes algorithm, we consider the parameters in the following order: mass of the jet, pT of the jet, mass
of the two sub-jets, pT of the two sub-jets, and b-tagging score of the two sub-jets. We add these variables to our feature
set one by one, and each time we re-run the learning algorithm and testing for 100 iterations. For each iteration, we use
70% of the signal and background to train the Gaussian parameters and test the prediction on the remaining 30% of the
data set. We take the training error and testing error as the mean of these 100 iterations and RMS as the uncertainties.
The results of this algorithm can be seen in Figure 1. The tagger training and testing errors quickly get down to percent
level as we increase the size of the feature set. For the signal, the errors are getting signiﬁcantly smaller as we consider

1

2

AVIV CUKIERMAN, ZIHAO JIANG

more features. However including the sub-jets information doesn’t improve the performance of tagging background. We
also note that that after including the ﬁrst two features (mass and pT of the Higgs jet), almost all of the discriminative
power is achieved.

Figure 1. Naive Bayes Driven Higgs Tagger. With 8 features, the signal error was 5% and the back-
ground error was 8%.

3.2. Logistic Regression.

In logistic regression we have a hypothesis of the form

hθ(x) = g(θT x) =

1

1 + exp(−θT x)

Where x is a (D + 1)-dimensional vector with x0 = 1 and x1,...,D the features, and θ ∈ RD+1 are the parameters of the
model. The prediction for y ∈ {0, 1} is 1{hθ(x) > 0.5}.

We split the data into 70% training set and 30% testing set. We optimize the parameters of the regression via batch

gradient ascent on the log-likelihood function on the training set. I.e. we run the following algorithm:
Repeat for N iterations:

Where in the algorithm we set α = 1, and x(1), ..., x(m) is the training set.

We then calculate the error in the testing set (and training set) by calculating

θj+1 := θj + α(cid:80)m
(cid:80)mTest/Train
(cid:80)mTest/Train

i=1

Test/Train
Signal,N =

Test/Train
Background,N =

i=1

i=1(y(i) − hθ(x(i)))x(i)

j

1{y(i) = 1}1{hθ(x(i)) < 0.5}}

mTest/Train

Signal

1{y(i) = 0}1{hθ(x(i)) > 0.5}}

mTest/Train
Background

For each N .

Test/Train
N

= Test/Train

Signal,N + Test/Train

Background,N

Sometimes Train

M > Train

N

for M > N as the algorithm moves out of local minima. So we look at

M∗ = argmini∈{1,...,N}(Train

i

)

Since in practice we can choose whatever θj minimizes the training error over the course of the algorithm, which isn’t
necessarily θN .

Running the algorithm with all the features, we get convergence of Train

M∗,min at around N = 1000 iterations, which
In order to compare across algorithms, we are concerned with the minimum total error when

can be seen in Fig. 2.
Signal ∼ 0.05. So we deﬁne similarly

S = {i : |Train
The results after N = 5000 iterations are shown in Table 1.

M∗

5% = argmini∈S(Train

i

)

Signal,i − 0.05| < 0.01}

M = M∗
M = M∗

5%

Train
Signal,M Train
0.0291
0.0501

Background,M Train
0.1426
0.1467

0.1135
0.0966

M

Test
Signal,M Test
0.0258
0.0471

0.1129
0.0940

Background,M

Test
M
0.1387
0.1411

CLASSIFICATION OF HIGGS JETS AS DECAY PRODUCTS OF A RANDALL-SUNDRUM GRAVITON AT THE ATLAS EXPERIMENT 3

Figure 2. Train/Test
converged to within a few percent level of the N → ∞ limit.

N,min

as a function of N using all the features. By about N = 1000, the algorithm has

Table 1. Results of logistic regression algorithm.

3.3. k-clustering. We examine a signiﬁcantly modiﬁed k-means clustering algorithm designed by us. First, we run an
ordinary unsupervised clustering algorithm on the training data:
1. Choose k centroids µ1, ..., µk randomly from the data
2. For every i, set c(i) = argminj||x(i) − µj||2
i=1 1{c(i) = j} ∗ x(i)
4. Repeat the last two steps until convergence

3. For every j, set µj =(cid:80)m
5. Repeat steps 1-4 a few times to globally minimize the cost function J =(cid:80)m

i=1 ||x(i) − µc(i)||2.

Then we supervise the unsupervised algorithm. For each cluster j we look at what fraction of the data classiﬁed in that
cluster is signal:

(cid:80)m
(cid:80)m
i=1 1{y(i) = 1}1{c(i) = j}
i=1 1{c(i) = j}

fj =

We then set a signal threshold s and background threshold b to label signal (fj > s) and background (fj < b) centroids,
respectively. Then using only the centroids that passed the signal or background threshold cuts (i.e. the number of clusters
used in the testing phase kTest ≤ k), we cluster and label the testing data to get signal and background identiﬁcation
eﬃciencies:

eTest/Train
Signal

(s) =

1

mTest/Train

Signal

y(i)1{fc(i) > s}

mTest/Train

Signal(cid:88)
Background(cid:88)

i=1

mTest/Train

eTest/Train
Background(b) =

And the error  = 1 − e.

1

mTest/Train
Background

i=1

(1 − y(i))1{fc(i) < b}

By varying s and b, we get a ROC curve, which is shown in Figure 3.

3.4. Multivariate Analysis. There are correlations between diﬀerent kinematics variables in the dataset we consider.
For instance, both the Higgs jets and QCD background jets will split to two sub-jets of the same amount of energy.
Therefore, the pT of the sub-jets are roughly the same. To account for such correlations we implement more advanced
multivariate analysis tools BDT (Boosted Decision Tree) and k-NN (kth nearest neighborhood) which are widely used in
ATLAS Collaboration.

3.4.1. Boosted Decision Tree. The Boosted Decision Tree (BDT) is an ensemble of weak classiﬁers called decision trees.
The algorithm involves two steps: (1) building a forest of decision trees, and (2) adaptive boosting. A decision tree is
a multi-layer binary classiﬁer. The training starts with the root node and splits to two subsets, each of which searches
for the best variable and its corresponding cut based on calculation of the Gini Index, deﬁned as p(1 − p), where p is
the purity of the sample. If in a sample, signal and background are mixed equally then p = 0.5 and the Gini Index is
maximized, while it falls to zero if the sample is entirely signal or background. In this search step, we partition each
variable into 20 equal segments and decide the cut based on the maximum change between the parent node Gini Index

4

AVIV CUKIERMAN, ZIHAO JIANG

Figure 3. The ROC curves for the the modiﬁed k-clustering algorithm. The model improved with
increasing numbers of clusters, but low data statistics prevented making more clusters.

obtained, f (x) =(cid:80)T =200

and the sum of the daughter nodes Gini Indices. The tree keeps splitting until the resulting split subsets have fewer than
200 data points. Once we obtain 200 such weak classiﬁers hi, we pass them to the boosting step.

(cid:80)
The goal of the boosting is to construct a strong classiﬁer as a linear combination of the 200 weak classiﬁers we
αtht(x). For b ∈ {−1, 1}, we come up with a prediction model Hb(x) = 1{f (x) > b}. Given m
i 1{Hb(xi) (cid:54)= yi} is minimized. The minimization scheme we use
training samples, the α’s are chosen such that b = 1
m
is adaptive boosting, which estimates and minimizes an upper bound of . By varying b, we get a ROC curve, which can
be seen in Figure 4.

t=1

The BDT algorithm is also useful in determining which variables are most useful for separating signal and background.
We calculated the discriminating power of each feature as the number of splits occurred for that variable weighted by the
gain-squared and number of events in it. The gain is calculated as the change in information entropy H from a previous
stage to a new stage with more information. As seen in Table 2, we ﬁnd that the Higgs jet transverse momentum and
mass have the most discriminating power.

Importance Variable
Variable
0.19
jet pT
jet mass
0.15
sub-leading sub-jet mass 0.14
sub-leading sub-jet pT
0.13

leading sub-jet b-tagging score
sub-leading sub-jet b-tagging score
leading sub-jet mass
leading sub-jet pT

Importance
0.13
0.12
0.09
0.07

Table 2. Importance calculated by BDT algorithm. Most indicative variables are the jet pT and jet mass.

3.4.2. k-NN. The philosophy of k-NN is neighborhood majority vote. The label of a point is decided by the labels of the
majority of its neighborhood points. To formalize the problem, we need a metric to decide for two points to be close
or neighbors. For this problem, we use the conventional Euclidean metric. Given a test point p and a set of features
{f1, f2, ..., fn}, we calculate the metric R =
, where ai and bij are the ith feature value for p and jth
training point. Then we select the closest k training points (in our case, we use k = 20) and assign a probability of p
being signal Ps = ks
k , where ks is the number of signal points of the k-nearest points. The weight factor wi in the metric
is the width of feature i for all the data. By varying the cut on the signal probability, we come up with a ROC curve,
which can be seen in Figure 4.

|ai − bij|(cid:17) 1

(cid:16)(cid:80)n

2

1
w2
i

i

The results are detailed in the Models section. But we wish to compare the models used. Table 3 shows the minimum

test background error for each model with test signal error ∼ 5%.

4. Results

Model

Naive Bayes

Logistic Regression
Modiﬁed k-clustering

BDT
k-NN

Table 3. Results of the models used here for test signal error ∼ 5%.

Background Error

0.08
0.0966
0.1129
0.0265
0.0473

CLASSIFICATION OF HIGGS JETS AS DECAY PRODUCTS OF A RANDALL-SUNDRUM

GRAVITON AT THE ATLAS EXPERIMENT

AVIV CUKIERMAN, ZIHAO JIANG

1. Introduction

In 2012, physicists at the Large Hadron Collider announced the discovery of the Higgs boson, which earned the moniker
“God Particle” in the popular media due to its far-reaching theoretical implications. The Standard Model, which is by
far the most successful model we have describing matter and its interactions, predicts that the Higgs boson exists and
gives mass to quarks. The Higgs boson was the ﬁnal particle predicted by the Standard Model that had not yet been
discovered and so the discovery was a major triumph for the Standard Model. However, there are problems with the
Standard Model. For instance, there is a question of why the mass of the Higgs itself is so small, when naive theoretical
extensions of the Standard Model predict it should be much higher.

Among all the more sophisticated models which account for this problem, one model that yields detectable quantities
is the Randall-Sundrum Model, which predicts there is a new particle, a Graviton, which can decay to two Higgs bosons .
A search for such a particle requires distinguishing the Higgs bosons from background processes, mostly quark and gluon
showering from quantum chromodynamic (QCD) eﬀects. In this project, we use machine learning techniques to distinguish
the signal of a Randall-Sundrum Graviton (RSG) decay to two Higgs from the QCD background, using kinematic variables
that would be observed in the ATLAS detector at CERN. In particular, we look at the case where each of the Higgs
bosons decay to a pair of bottom-antibottom (bb) quarks.

2. Data & Features

The study uses Monte Carlo simulated data with full reconstruction of ATLAS detector response. For the signal, we
assume the mass of the RSM Graviton to be 1000 GeV/c2, consisting of 24779 events. For the background, we use ATLAS
tuned QCD simulations consisting of 25944 unweighted events.

In a signal event we have two Higgs bosons, each of which decay to a bb pair of quarks. These quarks are are observed as
collimated energy deposits in the detector, which are called jets. We know from physics that in fact the quark-antiquark
pair will be detected with small angular separation in the detector, and so the quark jets form the sub-structure of a
larger Higgs jet. From these energy deposits we can reconstruct the space-time four momenta of the two quarks and the
Higgs bosons (→ 3 ∗ 4 = 12 features). In addition, we have b-tagging variables constructed from other observables that
describe the probability that the quarks are bottoms (→ 2 features). So in total we have (12+2=14) features for each
potential Higgs jet.

3. Models

3.1. Naive Bayes.

is p(F1, ..., Fn|X) =(cid:81)n

2πσ2
i

2σ2
i

The Naive Bayes model holds the assumption that the probability of a set of features {Fn} conditioned on a variable X
i p(Fi|X) In our case, X = 0, 1 denoting whether the a jet is or is not a Higgs boson. The features
are the continuous kinematic variables of the jets and the sub-jets. We model the conditional probabilities as a Gaussian
p(Fi|X) =
, where the parameters µ and σ are calculated directly from the mean and RMS of the
kinematic variables.

exp− (x−µi)2

1√

Heuristically, the features that are most relevant to this classiﬁcation problem are the invariant mass and transverse
momenta (pT) of the jets. Higgs jets have invariant mass around 125 GeV, while the QCD jets have smaller mass because
the showering of gluon and quarks usually produces light mesons. The mass and energy of sub-jets (decay products) are
predictive. The Higgs tend to decay to heavy-ﬂavor quarks like Bottom and Charm, while the sub-jets in QCD come from
radiation of light particles.

For the Naive Bayes algorithm, we consider the parameters in the following order: mass of the jet, pT of the jet, mass
of the two sub-jets, pT of the two sub-jets, and b-tagging score of the two sub-jets. We add these variables to our feature
set one by one, and each time we re-run the learning algorithm and testing for 100 iterations. For each iteration, we use
70% of the signal and background to train the Gaussian parameters and test the prediction on the remaining 30% of the
data set. We take the training error and testing error as the mean of these 100 iterations and RMS as the uncertainties.
The results of this algorithm can be seen in Figure 1. The tagger training and testing errors quickly get down to percent
level as we increase the size of the feature set. For the signal, the errors are getting signiﬁcantly smaller as we consider

1

2

AVIV CUKIERMAN, ZIHAO JIANG

more features. However including the sub-jets information doesn’t improve the performance of tagging background. We
also note that that after including the ﬁrst two features (mass and pT of the Higgs jet), almost all of the discriminative
power is achieved.

Figure 1. Naive Bayes Driven Higgs Tagger. With 8 features, the signal error was 5% and the back-
ground error was 8%.

3.2. Logistic Regression.

In logistic regression we have a hypothesis of the form

hθ(x) = g(θT x) =

1

1 + exp(−θT x)

Where x is a (D + 1)-dimensional vector with x0 = 1 and x1,...,D the features, and θ ∈ RD+1 are the parameters of the
model. The prediction for y ∈ {0, 1} is 1{hθ(x) > 0.5}.

We split the data into 70% training set and 30% testing set. We optimize the parameters of the regression via batch

gradient ascent on the log-likelihood function on the training set. I.e. we run the following algorithm:
Repeat for N iterations:

Where in the algorithm we set α = 1, and x(1), ..., x(m) is the training set.

We then calculate the error in the testing set (and training set) by calculating

θj+1 := θj + α(cid:80)m
(cid:80)mTest/Train
(cid:80)mTest/Train

i=1

Test/Train
Signal,N =

Test/Train
Background,N =

i=1

i=1(y(i) − hθ(x(i)))x(i)

j

1{y(i) = 1}1{hθ(x(i)) < 0.5}}

mTest/Train

Signal

1{y(i) = 0}1{hθ(x(i)) > 0.5}}

mTest/Train
Background

For each N .

Test/Train
N

= Test/Train

Signal,N + Test/Train

Background,N

Sometimes Train

M > Train

N

for M > N as the algorithm moves out of local minima. So we look at

M∗ = argmini∈{1,...,N}(Train

i

)

Since in practice we can choose whatever θj minimizes the training error over the course of the algorithm, which isn’t
necessarily θN .

Running the algorithm with all the features, we get convergence of Train

M∗,min at around N = 1000 iterations, which
In order to compare across algorithms, we are concerned with the minimum total error when

can be seen in Fig. 2.
Signal ∼ 0.05. So we deﬁne similarly

S = {i : |Train
The results after N = 5000 iterations are shown in Table 1.

M∗

5% = argmini∈S(Train

i

)

Signal,i − 0.05| < 0.01}

M = M∗
M = M∗

5%

Train
Signal,M Train
0.0291
0.0501

Background,M Train
0.1426
0.1467

0.1135
0.0966

M

Test
Signal,M Test
0.0258
0.0471

0.1129
0.0940

Background,M

Test
M
0.1387
0.1411

CLASSIFICATION OF HIGGS JETS AS DECAY PRODUCTS OF A RANDALL-SUNDRUM GRAVITON AT THE ATLAS EXPERIMENT 3

Figure 2. Train/Test
converged to within a few percent level of the N → ∞ limit.

N,min

as a function of N using all the features. By about N = 1000, the algorithm has

Table 1. Results of logistic regression algorithm.

3.3. k-clustering. We examine a signiﬁcantly modiﬁed k-means clustering algorithm designed by us. First, we run an
ordinary unsupervised clustering algorithm on the training data:
1. Choose k centroids µ1, ..., µk randomly from the data
2. For every i, set c(i) = argminj||x(i) − µj||2
i=1 1{c(i) = j} ∗ x(i)
4. Repeat the last two steps until convergence

3. For every j, set µj =(cid:80)m
5. Repeat steps 1-4 a few times to globally minimize the cost function J =(cid:80)m

i=1 ||x(i) − µc(i)||2.

Then we supervise the unsupervised algorithm. For each cluster j we look at what fraction of the data classiﬁed in that
cluster is signal:

(cid:80)m
(cid:80)m
i=1 1{y(i) = 1}1{c(i) = j}
i=1 1{c(i) = j}

fj =

We then set a signal threshold s and background threshold b to label signal (fj > s) and background (fj < b) centroids,
respectively. Then using only the centroids that passed the signal or background threshold cuts (i.e. the number of clusters
used in the testing phase kTest ≤ k), we cluster and label the testing data to get signal and background identiﬁcation
eﬃciencies:

eTest/Train
Signal

(s) =

1

mTest/Train

Signal

y(i)1{fc(i) > s}

mTest/Train

Signal(cid:88)
Background(cid:88)

i=1

mTest/Train

eTest/Train
Background(b) =

And the error  = 1 − e.

1

mTest/Train
Background

i=1

(1 − y(i))1{fc(i) < b}

By varying s and b, we get a ROC curve, which is shown in Figure 3.

3.4. Multivariate Analysis. There are correlations between diﬀerent kinematics variables in the dataset we consider.
For instance, both the Higgs jets and QCD background jets will split to two sub-jets of the same amount of energy.
Therefore, the pT of the sub-jets are roughly the same. To account for such correlations we implement more advanced
multivariate analysis tools BDT (Boosted Decision Tree) and k-NN (kth nearest neighborhood) which are widely used in
ATLAS Collaboration.

3.4.1. Boosted Decision Tree. The Boosted Decision Tree (BDT) is an ensemble of weak classiﬁers called decision trees.
The algorithm involves two steps: (1) building a forest of decision trees, and (2) adaptive boosting. A decision tree is
a multi-layer binary classiﬁer. The training starts with the root node and splits to two subsets, each of which searches
for the best variable and its corresponding cut based on calculation of the Gini Index, deﬁned as p(1 − p), where p is
the purity of the sample. If in a sample, signal and background are mixed equally then p = 0.5 and the Gini Index is
maximized, while it falls to zero if the sample is entirely signal or background. In this search step, we partition each
variable into 20 equal segments and decide the cut based on the maximum change between the parent node Gini Index

4

AVIV CUKIERMAN, ZIHAO JIANG

Figure 3. The ROC curves for the the modiﬁed k-clustering algorithm. The model improved with
increasing numbers of clusters, but low data statistics prevented making more clusters.

obtained, f (x) =(cid:80)T =200

and the sum of the daughter nodes Gini Indices. The tree keeps splitting until the resulting split subsets have fewer than
200 data points. Once we obtain 200 such weak classiﬁers hi, we pass them to the boosting step.

(cid:80)
The goal of the boosting is to construct a strong classiﬁer as a linear combination of the 200 weak classiﬁers we
αtht(x). For b ∈ {−1, 1}, we come up with a prediction model Hb(x) = 1{f (x) > b}. Given m
i 1{Hb(xi) (cid:54)= yi} is minimized. The minimization scheme we use
training samples, the α’s are chosen such that b = 1
m
is adaptive boosting, which estimates and minimizes an upper bound of . By varying b, we get a ROC curve, which can
be seen in Figure 4.

t=1

The BDT algorithm is also useful in determining which variables are most useful for separating signal and background.
We calculated the discriminating power of each feature as the number of splits occurred for that variable weighted by the
gain-squared and number of events in it. The gain is calculated as the change in information entropy H from a previous
stage to a new stage with more information. As seen in Table 2, we ﬁnd that the Higgs jet transverse momentum and
mass have the most discriminating power.

Importance Variable
Variable
0.19
jet pT
jet mass
0.15
sub-leading sub-jet mass 0.14
sub-leading sub-jet pT
0.13

leading sub-jet b-tagging score
sub-leading sub-jet b-tagging score
leading sub-jet mass
leading sub-jet pT

Importance
0.13
0.12
0.09
0.07

Table 2. Importance calculated by BDT algorithm. Most indicative variables are the jet pT and jet mass.

3.4.2. k-NN. The philosophy of k-NN is neighborhood majority vote. The label of a point is decided by the labels of the
majority of its neighborhood points. To formalize the problem, we need a metric to decide for two points to be close
or neighbors. For this problem, we use the conventional Euclidean metric. Given a test point p and a set of features
{f1, f2, ..., fn}, we calculate the metric R =
, where ai and bij are the ith feature value for p and jth
training point. Then we select the closest k training points (in our case, we use k = 20) and assign a probability of p
being signal Ps = ks
k , where ks is the number of signal points of the k-nearest points. The weight factor wi in the metric
is the width of feature i for all the data. By varying the cut on the signal probability, we come up with a ROC curve,
which can be seen in Figure 4.

|ai − bij|(cid:17) 1

(cid:16)(cid:80)n

2

1
w2
i

i

The results are detailed in the Models section. But we wish to compare the models used. Table 3 shows the minimum

test background error for each model with test signal error ∼ 5%.

4. Results

Model

Naive Bayes

Logistic Regression
Modiﬁed k-clustering

BDT
k-NN

Table 3. Results of the models used here for test signal error ∼ 5%.

Background Error

0.08
0.0966
0.1129
0.0265
0.0473

CLASSIFICATION OF HIGGS JETS AS DECAY PRODUCTS OF A RANDALL-SUNDRUM GRAVITON AT THE ATLAS EXPERIMENT 5

Figure 4. ROC curves for BDT and k-NN algorithms. The BDT algorithm outperforms k-NN at all
signal eﬃciencies.

As we can see, the BDT algorithm is the best performing.

5. Discussion

We examine both linear and non-linear classiﬁcation algorithms. In general we ﬁnd the non-linear classiﬁers to perform
better than the linear ones. The best performing linear classiﬁer was Naive Bayes, and the best performing classiﬁer
overall was BDT. We note that the ATLAS collaboration has generally agreed on using BDT to classify signal versus
background in a wide variety of problems, and our results support that decision.

We also designed a modiﬁed k-clustering algorithm. Our new algorithm was the worst performing of all the ones we
examined. However it was interesting to see that an algorithm with unsupervised components could still do a decent job
of classiﬁcation.

We also analyzed and measured the importance of each feature used in our algorithms. Both our Naive Bayes algorithm

and BDT ﬁnd that the two most important features are the Higgs jet mass and transverse momentum.

6. Future Work

Most of the algorithms discussed here have seen years of research and development. The k-clustering algorithm
developed here, on the other hand, shows some promise, even though it was the worst performing of all the algorithms.
With more time it would be interesting to look at this algorithm from a theoretical standpoint to understand its limitations
and under what model it performs best. If we were to work on this project for a few more months, we would also probably
look at other algorithms like SVM.

7. Conclusion

We ﬁnd that machine learning algorithms can be very powerful in distinguishing Higgs signal from background in
simulated data from ATLAS. Its particularly interesting that the algorithms themselves had no knowledge of the physics,
only of the data and training classiﬁcations. The prospect of discovering or ruling out a Randall-Sundrum graviton via
decay to two Higgs bosons at ATLAS seems feasible given the power of the classiﬁcation algorithms discovered here.
From a wider perspective, machine learning algorithms can be used in many physics analyses other than the speciﬁc one
studied here, such as searches for supersymmetric particles, dark matter, or exotic particles like axions. We believe that
the future of physics done at experiments like ATLAS inevitably lies with the use of machine learning techniques like the
ones presented here.

8. References

[1] ATLAS Collaboration (2012). “Observation of a new particle in the search for the Standard Model Higgs boson with
the ATLAS detector at the LHC. Phys. Lett. B 716 (39)
[2] M. Cacciari, G. Salam, G. Soyez (2008), “The Anti-Kt jet clustering algorithm”, JHEP 0804:063
[3] A. Hoecker et al. (2007). “TMVA - Toolkit for Multivariate Data Analysis, PoS ACAT 040
[4] A. Ng. “CS229 Lecture Notes, Retrieved from CS 229 Autumn 2014 Course Website, http://cs229.stanford.edu/materials.
[5] L. Randall, R. Sundrum (1999). “Large Mass Hierarchy from a Small Extra Dimension. Physical Review Letters 83
(17)
[6] T. Sjostrand, S. Mrenna and P. Skands (2007). “A Brief introduction to PYTHIA 8.1, arXiv:0710.3820

