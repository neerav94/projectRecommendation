Mac Malware Detection via Static File

Structure Analysis

Elizabeth Walkup
University of Stanford
ewalkup@stanford.edu

I.

Introduction

It is widely acknowledged in the security

community that the current signature-based
approach to virus detection is no longer ad-
equate. More recently, antivirus software has
been doing dynamic malicious behavior detec-
tion. While this is more effective, it is compu-
tationally expensive, so they cannot do very
much of it or the performance of the user’s
computer will suffer.

Static executable analysis offers a possible
solution to the problems of dynamic analysis.
Static analysis looks at the structures within
the executable that are necessary in order for
it to run. Since these structures are mandated
by the ﬁle type, they cannot be removed, en-
crypted (although their code contents may be),
or obfuscated easily. Also, because it simply
involves parsing structures, it is much less com-
putationally expensive than dynamic analysis.
Some research has already been done into static
executable analysis for Windows portable exe-
cutable (PE) ﬁles.

Although Mac malware (the general term
for malicious software) is not nearly as proliﬁc
as those for Windows, the number is steadily
increasing as Macs gain a greater market share.
Kaspersky has seen the amount of Mac mal-
ware double in the last two years, and increase
by a factor of ten in the last four years [10],
yielding an exponential increase curve. Of par-
ticular note, the last 4 or 5 years has seen a num-
ber of advanced persistent threat (APT) mal-
ware campaigns targeted speciﬁcally at Macs.
Researching tools to face these threats now en-
sures that we will be able to better handle the
increasing threat in the future.

II. Dataset

Malware samples were gathered from open
research sites like Contagio[8] and VirusTo-
tal[9]. Benign Mach-O ﬁles (for comparison)
were gathered from OS X Mavericks and open-
source programs for OS X. The samples them-
selves are Mach-O binary executable ﬁles. This
is the format of choice for OS X programs,
though there are one or two other formats
used in older systems. For legal reasons, open-
source malware repositories are hard to ﬁnd,
so increasing the malware samples by any sub-
stantial amount beyond this is difﬁcult.

The current data set consists of 420 malware
samples and 1000 goodware (benign programs)
samples. This data was randomly divided into
a training set (roughly 75%) and a testing set
(roughly 25%). By number, the samples are
divided as follows:

Malware
Goodware

Training Set Testing Set
87
202

333
798

III. Features

The Mach-O features are pulled out of the
binaries using a Python script that parses the
ﬁle structures. The information is then stored
in a SQLite database. Since machine learning
was done using Weka [7], another Python script
converts the information in the database into
a Weka-supported format. The features are
diverse: some are strings, some are continuous-
valued numbers, some are discrete numbers,
and some are boolean (0 or 1).
I. Structure Features

The script pulls 50 structural features out of
the database, mostly continuous valued, size-

1

Mac Malware Detection via Static File

Structure Analysis

Elizabeth Walkup
University of Stanford
ewalkup@stanford.edu

I.

Introduction

It is widely acknowledged in the security

community that the current signature-based
approach to virus detection is no longer ad-
equate. More recently, antivirus software has
been doing dynamic malicious behavior detec-
tion. While this is more effective, it is compu-
tationally expensive, so they cannot do very
much of it or the performance of the user’s
computer will suffer.

Static executable analysis offers a possible
solution to the problems of dynamic analysis.
Static analysis looks at the structures within
the executable that are necessary in order for
it to run. Since these structures are mandated
by the ﬁle type, they cannot be removed, en-
crypted (although their code contents may be),
or obfuscated easily. Also, because it simply
involves parsing structures, it is much less com-
putationally expensive than dynamic analysis.
Some research has already been done into static
executable analysis for Windows portable exe-
cutable (PE) ﬁles.

Although Mac malware (the general term
for malicious software) is not nearly as proliﬁc
as those for Windows, the number is steadily
increasing as Macs gain a greater market share.
Kaspersky has seen the amount of Mac mal-
ware double in the last two years, and increase
by a factor of ten in the last four years [10],
yielding an exponential increase curve. Of par-
ticular note, the last 4 or 5 years has seen a num-
ber of advanced persistent threat (APT) mal-
ware campaigns targeted speciﬁcally at Macs.
Researching tools to face these threats now en-
sures that we will be able to better handle the
increasing threat in the future.

II. Dataset

Malware samples were gathered from open
research sites like Contagio[8] and VirusTo-
tal[9]. Benign Mach-O ﬁles (for comparison)
were gathered from OS X Mavericks and open-
source programs for OS X. The samples them-
selves are Mach-O binary executable ﬁles. This
is the format of choice for OS X programs,
though there are one or two other formats
used in older systems. For legal reasons, open-
source malware repositories are hard to ﬁnd,
so increasing the malware samples by any sub-
stantial amount beyond this is difﬁcult.

The current data set consists of 420 malware
samples and 1000 goodware (benign programs)
samples. This data was randomly divided into
a training set (roughly 75%) and a testing set
(roughly 25%). By number, the samples are
divided as follows:

Malware
Goodware

Training Set Testing Set
87
202

333
798

III. Features

The Mach-O features are pulled out of the
binaries using a Python script that parses the
ﬁle structures. The information is then stored
in a SQLite database. Since machine learning
was done using Weka [7], another Python script
converts the information in the database into
a Weka-supported format. The features are
diverse: some are strings, some are continuous-
valued numbers, some are discrete numbers,
and some are boolean (0 or 1).
I. Structure Features

The script pulls 50 structural features out of
the database, mostly continuous valued, size-

1

based statistics of various structures. There are
other structural features within the database
that could be used, but these were chosen to
maximize information gain while decreasing
model size. Using only structural features, the
top model had a testing error of 7.3691%.
I.1 Mach-O File Structure

Mach-O executables[6] have three main
parts: a mach header, load commands, and
data. The data section is made up of the actual
executable code, which can be compressed or
encrypted, so no features are gathered from
there. The mach header contains information
about the platform the executable was built for,
as well as the ﬁle type (dynamic library, etc).
The majority of the structural features come
from the load commands. These contain head-
ers for each code segment in the data, as well as
information about import libraries and how to
load the ﬁle in memory. Each segment header
also contains information about a substructure
within the segment called a section.
II.

Import Features

Every executable ﬁle imports ﬁles and li-
braries to use within its program. These can-
not be encrypted, so their names can be ex-
tracted from the binary. However, there are
literally thousands of names that could be
present, which is not easy to incorporate into a
model. To compromise, feature selection was
performed on models built solely from import
features, and the top features were then in-
corporated back into the main set of features.
When these import features were added, the
testing error of the top model decreased from
7.3691% to 2.7682%, a nearly 5% improvement.
III. Feature Selection

Two methods were used to trim features:
forward search selection and information gain
ranking. These two algorithms had many top
features in common, particularly when used
to choose import features. By combing the top
results from these two methods, thousands of
possible import features were reduced to 60
dynamic library (dylib) imports and 117 func-
tion imports. Combined with the 50 structural
features, there were 227 features for all of the

2

models. Some of the top features are shown in
Table 1.

IV. Models

For the sake of brevity, only the top ﬁve
models will be described, although over 20
were tested.
I. Rotation Forest

Rotation Forest[1] builds an ensemble (or
forest) of decision trees, in this case C4.5 trees.
The feature set is split into K disjoint subsets.
In this case, K was 75, which corresponds to
roughly 3 features per set. For each classi-
ﬁer, principal component analysis (PCA) is run
using the subset of features chosen. The coefﬁ-
cients found using PCA are put into a sparse
rotation matrix, which is rearranged to match
the chosen features. This new matrix multi-
plied by the training data comprises the model.
To classify a new data point, the conﬁdence for
each individual model is calculated, and the
output class is the one with the highest conﬁ-
dence. This model had a conﬁdence factor of
0.25.
II. Random Forest

Random Forest[2], similar to Rotation For-
est, is an ensemble of different decision tree
models. However, instead of using a conﬁ-
dence measure, the different models vote on
the output class. The training sets for these
models are chosen randomly, but with the
same distribution. In this case, there were ten
decision tree models and the models used the
full feature set.
III. PART

PART[3] is a rule-based algorithm. The data
set is divided into several new sets. A partial
pruned C4.5 decision tree is built based on
each set. The leaf of this tree with the largest
data coverage (meaning it classiﬁes the most
instances) is made into a rule. A leaf must
cover at least two instances to be considered
for a rule.
IV. LMT

LMT[4] stands for Logistic Model Tree. This
is a basic decision tree except that instead of

Mac Malware Detection via Static File

Structure Analysis

Elizabeth Walkup
University of Stanford
ewalkup@stanford.edu

I.

Introduction

It is widely acknowledged in the security

community that the current signature-based
approach to virus detection is no longer ad-
equate. More recently, antivirus software has
been doing dynamic malicious behavior detec-
tion. While this is more effective, it is compu-
tationally expensive, so they cannot do very
much of it or the performance of the user’s
computer will suffer.

Static executable analysis offers a possible
solution to the problems of dynamic analysis.
Static analysis looks at the structures within
the executable that are necessary in order for
it to run. Since these structures are mandated
by the ﬁle type, they cannot be removed, en-
crypted (although their code contents may be),
or obfuscated easily. Also, because it simply
involves parsing structures, it is much less com-
putationally expensive than dynamic analysis.
Some research has already been done into static
executable analysis for Windows portable exe-
cutable (PE) ﬁles.

Although Mac malware (the general term
for malicious software) is not nearly as proliﬁc
as those for Windows, the number is steadily
increasing as Macs gain a greater market share.
Kaspersky has seen the amount of Mac mal-
ware double in the last two years, and increase
by a factor of ten in the last four years [10],
yielding an exponential increase curve. Of par-
ticular note, the last 4 or 5 years has seen a num-
ber of advanced persistent threat (APT) mal-
ware campaigns targeted speciﬁcally at Macs.
Researching tools to face these threats now en-
sures that we will be able to better handle the
increasing threat in the future.

II. Dataset

Malware samples were gathered from open
research sites like Contagio[8] and VirusTo-
tal[9]. Benign Mach-O ﬁles (for comparison)
were gathered from OS X Mavericks and open-
source programs for OS X. The samples them-
selves are Mach-O binary executable ﬁles. This
is the format of choice for OS X programs,
though there are one or two other formats
used in older systems. For legal reasons, open-
source malware repositories are hard to ﬁnd,
so increasing the malware samples by any sub-
stantial amount beyond this is difﬁcult.

The current data set consists of 420 malware
samples and 1000 goodware (benign programs)
samples. This data was randomly divided into
a training set (roughly 75%) and a testing set
(roughly 25%). By number, the samples are
divided as follows:

Malware
Goodware

Training Set Testing Set
87
202

333
798

III. Features

The Mach-O features are pulled out of the
binaries using a Python script that parses the
ﬁle structures. The information is then stored
in a SQLite database. Since machine learning
was done using Weka [7], another Python script
converts the information in the database into
a Weka-supported format. The features are
diverse: some are strings, some are continuous-
valued numbers, some are discrete numbers,
and some are boolean (0 or 1).
I. Structure Features

The script pulls 50 structural features out of
the database, mostly continuous valued, size-

1

based statistics of various structures. There are
other structural features within the database
that could be used, but these were chosen to
maximize information gain while decreasing
model size. Using only structural features, the
top model had a testing error of 7.3691%.
I.1 Mach-O File Structure

Mach-O executables[6] have three main
parts: a mach header, load commands, and
data. The data section is made up of the actual
executable code, which can be compressed or
encrypted, so no features are gathered from
there. The mach header contains information
about the platform the executable was built for,
as well as the ﬁle type (dynamic library, etc).
The majority of the structural features come
from the load commands. These contain head-
ers for each code segment in the data, as well as
information about import libraries and how to
load the ﬁle in memory. Each segment header
also contains information about a substructure
within the segment called a section.
II.

Import Features

Every executable ﬁle imports ﬁles and li-
braries to use within its program. These can-
not be encrypted, so their names can be ex-
tracted from the binary. However, there are
literally thousands of names that could be
present, which is not easy to incorporate into a
model. To compromise, feature selection was
performed on models built solely from import
features, and the top features were then in-
corporated back into the main set of features.
When these import features were added, the
testing error of the top model decreased from
7.3691% to 2.7682%, a nearly 5% improvement.
III. Feature Selection

Two methods were used to trim features:
forward search selection and information gain
ranking. These two algorithms had many top
features in common, particularly when used
to choose import features. By combing the top
results from these two methods, thousands of
possible import features were reduced to 60
dynamic library (dylib) imports and 117 func-
tion imports. Combined with the 50 structural
features, there were 227 features for all of the

2

models. Some of the top features are shown in
Table 1.

IV. Models

For the sake of brevity, only the top ﬁve
models will be described, although over 20
were tested.
I. Rotation Forest

Rotation Forest[1] builds an ensemble (or
forest) of decision trees, in this case C4.5 trees.
The feature set is split into K disjoint subsets.
In this case, K was 75, which corresponds to
roughly 3 features per set. For each classi-
ﬁer, principal component analysis (PCA) is run
using the subset of features chosen. The coefﬁ-
cients found using PCA are put into a sparse
rotation matrix, which is rearranged to match
the chosen features. This new matrix multi-
plied by the training data comprises the model.
To classify a new data point, the conﬁdence for
each individual model is calculated, and the
output class is the one with the highest conﬁ-
dence. This model had a conﬁdence factor of
0.25.
II. Random Forest

Random Forest[2], similar to Rotation For-
est, is an ensemble of different decision tree
models. However, instead of using a conﬁ-
dence measure, the different models vote on
the output class. The training sets for these
models are chosen randomly, but with the
same distribution. In this case, there were ten
decision tree models and the models used the
full feature set.
III. PART

PART[3] is a rule-based algorithm. The data
set is divided into several new sets. A partial
pruned C4.5 decision tree is built based on
each set. The leaf of this tree with the largest
data coverage (meaning it classiﬁes the most
instances) is made into a rule. A leaf must
cover at least two instances to be considered
for a rule.
IV. LMT

LMT[4] stands for Logistic Model Tree. This
is a basic decision tree except that instead of

Table 1: Top Ten Features by Informaton Gain

dylibs
mbukernel
mbuinstrument
QtCore
wlmstrings
mbustrings
MicrosoftComponentPlugin
QtGui
libstdc++
QtNetwork
Netlib

functions
_exit
_system
_kCFHTTPVersion1_1
_umask
_fread
_sysctlnametomib
_CFReadStreamCreateForHTTPRequest
_IORegistryEntryFromPath
_gmtime
_sysctl

structural
num_imports
std_segsize
ﬁle_type
avg_segsize
std_segvmsize
min_segsize
avg_r1
avg_segvmsize
std_secsize
avg_secalign

having classes at the leaf nodes, it has logis-
tic regression functions. A node must cover
at least ﬁfteen instances to have child nodes.
The functions at the leaf nodes model the class
probability:

Pr(G = j|X = x) =

Fj = α

eFj(x)
∑J
k=1 eFk(x)
0 + ∑
j
α
v∈VT

(1)

(2)

j
vv

where Vt is a subset of all the features, αi’s
are weights, and k is the number of possible
feature values.
V.

IBk

IBk[5] is an instance-based learning imple-
mentation of k-Nearest Neighbors (in this case
k=1). For each data point, it calculates a simi-
larity for each cluster. That data point is then
assigned to the cluster with the highest simi-
larity.

V. Results

Many different supervised learning algo-
rithms were evaluated to see which one could
classify the data the best. The algorithms were
trained on the full 227 attributes. The top ten
results are listed in Table 2. A false positive is
a goodware sample that was classiﬁed as mal-
ware, and a false negative is a malware sample
that was classiﬁed as goodware.

Some interesting facts that can be gleaned

from the data itself:

• Goodware tends to import more libraries

than malware

• Malware tends to have more segments
than goodware and use more varied seg-
ment ﬂags

• Malware and goodware have the same

distribution of load com- mand sizes

• Malware is more likely to use Java li-

braries

• Malware is more likely to use HTTP
stream functions (probably for con-
trol/exﬁltration)

• Goodware uses more memory access con-

trols

VI. Discussion

The ensemble methods had the best perfor-
mance, perhaps due to the complex nature of
the input features or the varied data set. Since
all of the import features were nominal fea-
tures, decision tree models were particularly
well suited for classiﬁcation.

The fact that the testing error can be re-
duced to a few percent means that there are
actually tangible differences in static structures
between malware and goodware. This was ex-
pected going into the project because substan-
tial results have been produced using a similar
approach with Windows portable executable
(PE) ﬁles and malware.

3

Mac Malware Detection via Static File

Structure Analysis

Elizabeth Walkup
University of Stanford
ewalkup@stanford.edu

I.

Introduction

It is widely acknowledged in the security

community that the current signature-based
approach to virus detection is no longer ad-
equate. More recently, antivirus software has
been doing dynamic malicious behavior detec-
tion. While this is more effective, it is compu-
tationally expensive, so they cannot do very
much of it or the performance of the user’s
computer will suffer.

Static executable analysis offers a possible
solution to the problems of dynamic analysis.
Static analysis looks at the structures within
the executable that are necessary in order for
it to run. Since these structures are mandated
by the ﬁle type, they cannot be removed, en-
crypted (although their code contents may be),
or obfuscated easily. Also, because it simply
involves parsing structures, it is much less com-
putationally expensive than dynamic analysis.
Some research has already been done into static
executable analysis for Windows portable exe-
cutable (PE) ﬁles.

Although Mac malware (the general term
for malicious software) is not nearly as proliﬁc
as those for Windows, the number is steadily
increasing as Macs gain a greater market share.
Kaspersky has seen the amount of Mac mal-
ware double in the last two years, and increase
by a factor of ten in the last four years [10],
yielding an exponential increase curve. Of par-
ticular note, the last 4 or 5 years has seen a num-
ber of advanced persistent threat (APT) mal-
ware campaigns targeted speciﬁcally at Macs.
Researching tools to face these threats now en-
sures that we will be able to better handle the
increasing threat in the future.

II. Dataset

Malware samples were gathered from open
research sites like Contagio[8] and VirusTo-
tal[9]. Benign Mach-O ﬁles (for comparison)
were gathered from OS X Mavericks and open-
source programs for OS X. The samples them-
selves are Mach-O binary executable ﬁles. This
is the format of choice for OS X programs,
though there are one or two other formats
used in older systems. For legal reasons, open-
source malware repositories are hard to ﬁnd,
so increasing the malware samples by any sub-
stantial amount beyond this is difﬁcult.

The current data set consists of 420 malware
samples and 1000 goodware (benign programs)
samples. This data was randomly divided into
a training set (roughly 75%) and a testing set
(roughly 25%). By number, the samples are
divided as follows:

Malware
Goodware

Training Set Testing Set
87
202

333
798

III. Features

The Mach-O features are pulled out of the
binaries using a Python script that parses the
ﬁle structures. The information is then stored
in a SQLite database. Since machine learning
was done using Weka [7], another Python script
converts the information in the database into
a Weka-supported format. The features are
diverse: some are strings, some are continuous-
valued numbers, some are discrete numbers,
and some are boolean (0 or 1).
I. Structure Features

The script pulls 50 structural features out of
the database, mostly continuous valued, size-

1

based statistics of various structures. There are
other structural features within the database
that could be used, but these were chosen to
maximize information gain while decreasing
model size. Using only structural features, the
top model had a testing error of 7.3691%.
I.1 Mach-O File Structure

Mach-O executables[6] have three main
parts: a mach header, load commands, and
data. The data section is made up of the actual
executable code, which can be compressed or
encrypted, so no features are gathered from
there. The mach header contains information
about the platform the executable was built for,
as well as the ﬁle type (dynamic library, etc).
The majority of the structural features come
from the load commands. These contain head-
ers for each code segment in the data, as well as
information about import libraries and how to
load the ﬁle in memory. Each segment header
also contains information about a substructure
within the segment called a section.
II.

Import Features

Every executable ﬁle imports ﬁles and li-
braries to use within its program. These can-
not be encrypted, so their names can be ex-
tracted from the binary. However, there are
literally thousands of names that could be
present, which is not easy to incorporate into a
model. To compromise, feature selection was
performed on models built solely from import
features, and the top features were then in-
corporated back into the main set of features.
When these import features were added, the
testing error of the top model decreased from
7.3691% to 2.7682%, a nearly 5% improvement.
III. Feature Selection

Two methods were used to trim features:
forward search selection and information gain
ranking. These two algorithms had many top
features in common, particularly when used
to choose import features. By combing the top
results from these two methods, thousands of
possible import features were reduced to 60
dynamic library (dylib) imports and 117 func-
tion imports. Combined with the 50 structural
features, there were 227 features for all of the

2

models. Some of the top features are shown in
Table 1.

IV. Models

For the sake of brevity, only the top ﬁve
models will be described, although over 20
were tested.
I. Rotation Forest

Rotation Forest[1] builds an ensemble (or
forest) of decision trees, in this case C4.5 trees.
The feature set is split into K disjoint subsets.
In this case, K was 75, which corresponds to
roughly 3 features per set. For each classi-
ﬁer, principal component analysis (PCA) is run
using the subset of features chosen. The coefﬁ-
cients found using PCA are put into a sparse
rotation matrix, which is rearranged to match
the chosen features. This new matrix multi-
plied by the training data comprises the model.
To classify a new data point, the conﬁdence for
each individual model is calculated, and the
output class is the one with the highest conﬁ-
dence. This model had a conﬁdence factor of
0.25.
II. Random Forest

Random Forest[2], similar to Rotation For-
est, is an ensemble of different decision tree
models. However, instead of using a conﬁ-
dence measure, the different models vote on
the output class. The training sets for these
models are chosen randomly, but with the
same distribution. In this case, there were ten
decision tree models and the models used the
full feature set.
III. PART

PART[3] is a rule-based algorithm. The data
set is divided into several new sets. A partial
pruned C4.5 decision tree is built based on
each set. The leaf of this tree with the largest
data coverage (meaning it classiﬁes the most
instances) is made into a rule. A leaf must
cover at least two instances to be considered
for a rule.
IV. LMT

LMT[4] stands for Logistic Model Tree. This
is a basic decision tree except that instead of

Table 1: Top Ten Features by Informaton Gain

dylibs
mbukernel
mbuinstrument
QtCore
wlmstrings
mbustrings
MicrosoftComponentPlugin
QtGui
libstdc++
QtNetwork
Netlib

functions
_exit
_system
_kCFHTTPVersion1_1
_umask
_fread
_sysctlnametomib
_CFReadStreamCreateForHTTPRequest
_IORegistryEntryFromPath
_gmtime
_sysctl

structural
num_imports
std_segsize
ﬁle_type
avg_segsize
std_segvmsize
min_segsize
avg_r1
avg_segvmsize
std_secsize
avg_secalign

having classes at the leaf nodes, it has logis-
tic regression functions. A node must cover
at least ﬁfteen instances to have child nodes.
The functions at the leaf nodes model the class
probability:

Pr(G = j|X = x) =

Fj = α

eFj(x)
∑J
k=1 eFk(x)
0 + ∑
j
α
v∈VT

(1)

(2)

j
vv

where Vt is a subset of all the features, αi’s
are weights, and k is the number of possible
feature values.
V.

IBk

IBk[5] is an instance-based learning imple-
mentation of k-Nearest Neighbors (in this case
k=1). For each data point, it calculates a simi-
larity for each cluster. That data point is then
assigned to the cluster with the highest simi-
larity.

V. Results

Many different supervised learning algo-
rithms were evaluated to see which one could
classify the data the best. The algorithms were
trained on the full 227 attributes. The top ten
results are listed in Table 2. A false positive is
a goodware sample that was classiﬁed as mal-
ware, and a false negative is a malware sample
that was classiﬁed as goodware.

Some interesting facts that can be gleaned

from the data itself:

• Goodware tends to import more libraries

than malware

• Malware tends to have more segments
than goodware and use more varied seg-
ment ﬂags

• Malware and goodware have the same

distribution of load com- mand sizes

• Malware is more likely to use Java li-

braries

• Malware is more likely to use HTTP
stream functions (probably for con-
trol/exﬁltration)

• Goodware uses more memory access con-

trols

VI. Discussion

The ensemble methods had the best perfor-
mance, perhaps due to the complex nature of
the input features or the varied data set. Since
all of the import features were nominal fea-
tures, decision tree models were particularly
well suited for classiﬁcation.

The fact that the testing error can be re-
duced to a few percent means that there are
actually tangible differences in static structures
between malware and goodware. This was ex-
pected going into the project because substan-
tial results have been produced using a similar
approach with Windows portable executable
(PE) ﬁles and malware.

3

Table 2: Top Ten Supervised Learning Algorithms Results

Algorithm
Rotation Forest
Random Forest
PART
LMT
IBk
SMO
FT Tree
J48/C4.5 Tree
Regression

Training Error Test Error
2.7682%
4.1522%
4.4983%
5.1903%
5.1903%
6.2284%
6.5744%
6.5744%
9.3426%

0.2653%
0.1768%
0.9726%
0.9726%
0%
3.8019%
1.4147%
1.1494%
3.4483%

False Positive
0.02
0.035
0.025
0.025
0.035
0.04
0.35
0.05
0.05

False Negative ROC Area
0.994
0.99
0.949
0.98
0.944
0.923
0.921
0.92
0.966

0.046
0.057
0.092
0.115
0.092
0.115
0.138
0.103
0.195

However, the results are not quite what
would be considered "production level" for mal-
ware detection. As a general rule, the goal is to
keep false positives below 0.001% and false neg-
atives below 1%. At best, among these models
the false positive was 2% and the false nega-
tive was 4.6%. False positives are especially
important to keep low, because customers will
not use an antivirus that blocks their legitimate
programs.

The models built in this project are still use-
ful as triaging tools. In cybersecurity, incident
response teams at businesses are swamped
with email security alerts every day, especially
with regards to suspicious email attachments.
Running these models on the applicable exe-
cutables can allow the incident response team
to focus only on those that have a high proba-
bility of being malware. So the conclusion is
that static analysis shows substantial promise,
even if it is not quite reﬁned enough to be
deployed as a standalone antivirus.
VII. Future Work

The one major roadblock to this project is
a lack of malware samples. The 420 samples
here took weeks to obtain, whereas, by contrast,
thousands of goodware samples can be gath-
ered in a day. Expanding the sample set would
probably cut down on the generalization er-
ror. One possible approach would be to catch
malware using a honeypot setup, though that
often comes with its own set of legal issues.

There are also more features that can be

4

used within the Mach-O ﬁles. Adding more
features could possibly increase the accuracy
of the classiﬁcation, but it comes at the price of
a larger model.

This static analysis approach could also be
used to try and classify malware into "fami-
lies" (for example, all polymorphic Flashback
executables), which is an open problem in the
security industry.

[1]

References

J. Rodriguez and L. Kuncheva, "Rota-
tion Forest: A New Classiﬁer Ensemble
Method", IEEE Trans. Pattern Analysis and
Machine Intelligence , vol. 28, no. 10, pp.
1619-1630, Oct. 2006.

[2] Leo Breiman, "Random Forests", Univer-

sity of California, Jan. 2001.

[3] Eibe Frank and Ian H. Witten, "Generat-
ing Accurate Rulesets Without Global Op-
timization", University of Waikato.

[4] N. Landwehr, M. Hall, and E. Frank, "Lo-
gistic Model Trees", 14th European Confer-
ence on Machine Learning, Jun. 2004.

[5] D. Aha, D. Kibler, and M. Albert,
"Instance-Based Learning Algorithms",
Machine Learning , vol. 6, pp. 37-66, 1991.

[6]

"OS X ABI Mach-O File Format
Reference". Mac Developer Library.
https://developer.apple.com/library/mac
/documentation/DeveloperTools/Con-
ceptual/MachORuntime/index.html.

Mac Malware Detection via Static File

Structure Analysis

Elizabeth Walkup
University of Stanford
ewalkup@stanford.edu

I.

Introduction

It is widely acknowledged in the security

community that the current signature-based
approach to virus detection is no longer ad-
equate. More recently, antivirus software has
been doing dynamic malicious behavior detec-
tion. While this is more effective, it is compu-
tationally expensive, so they cannot do very
much of it or the performance of the user’s
computer will suffer.

Static executable analysis offers a possible
solution to the problems of dynamic analysis.
Static analysis looks at the structures within
the executable that are necessary in order for
it to run. Since these structures are mandated
by the ﬁle type, they cannot be removed, en-
crypted (although their code contents may be),
or obfuscated easily. Also, because it simply
involves parsing structures, it is much less com-
putationally expensive than dynamic analysis.
Some research has already been done into static
executable analysis for Windows portable exe-
cutable (PE) ﬁles.

Although Mac malware (the general term
for malicious software) is not nearly as proliﬁc
as those for Windows, the number is steadily
increasing as Macs gain a greater market share.
Kaspersky has seen the amount of Mac mal-
ware double in the last two years, and increase
by a factor of ten in the last four years [10],
yielding an exponential increase curve. Of par-
ticular note, the last 4 or 5 years has seen a num-
ber of advanced persistent threat (APT) mal-
ware campaigns targeted speciﬁcally at Macs.
Researching tools to face these threats now en-
sures that we will be able to better handle the
increasing threat in the future.

II. Dataset

Malware samples were gathered from open
research sites like Contagio[8] and VirusTo-
tal[9]. Benign Mach-O ﬁles (for comparison)
were gathered from OS X Mavericks and open-
source programs for OS X. The samples them-
selves are Mach-O binary executable ﬁles. This
is the format of choice for OS X programs,
though there are one or two other formats
used in older systems. For legal reasons, open-
source malware repositories are hard to ﬁnd,
so increasing the malware samples by any sub-
stantial amount beyond this is difﬁcult.

The current data set consists of 420 malware
samples and 1000 goodware (benign programs)
samples. This data was randomly divided into
a training set (roughly 75%) and a testing set
(roughly 25%). By number, the samples are
divided as follows:

Malware
Goodware

Training Set Testing Set
87
202

333
798

III. Features

The Mach-O features are pulled out of the
binaries using a Python script that parses the
ﬁle structures. The information is then stored
in a SQLite database. Since machine learning
was done using Weka [7], another Python script
converts the information in the database into
a Weka-supported format. The features are
diverse: some are strings, some are continuous-
valued numbers, some are discrete numbers,
and some are boolean (0 or 1).
I. Structure Features

The script pulls 50 structural features out of
the database, mostly continuous valued, size-

1

based statistics of various structures. There are
other structural features within the database
that could be used, but these were chosen to
maximize information gain while decreasing
model size. Using only structural features, the
top model had a testing error of 7.3691%.
I.1 Mach-O File Structure

Mach-O executables[6] have three main
parts: a mach header, load commands, and
data. The data section is made up of the actual
executable code, which can be compressed or
encrypted, so no features are gathered from
there. The mach header contains information
about the platform the executable was built for,
as well as the ﬁle type (dynamic library, etc).
The majority of the structural features come
from the load commands. These contain head-
ers for each code segment in the data, as well as
information about import libraries and how to
load the ﬁle in memory. Each segment header
also contains information about a substructure
within the segment called a section.
II.

Import Features

Every executable ﬁle imports ﬁles and li-
braries to use within its program. These can-
not be encrypted, so their names can be ex-
tracted from the binary. However, there are
literally thousands of names that could be
present, which is not easy to incorporate into a
model. To compromise, feature selection was
performed on models built solely from import
features, and the top features were then in-
corporated back into the main set of features.
When these import features were added, the
testing error of the top model decreased from
7.3691% to 2.7682%, a nearly 5% improvement.
III. Feature Selection

Two methods were used to trim features:
forward search selection and information gain
ranking. These two algorithms had many top
features in common, particularly when used
to choose import features. By combing the top
results from these two methods, thousands of
possible import features were reduced to 60
dynamic library (dylib) imports and 117 func-
tion imports. Combined with the 50 structural
features, there were 227 features for all of the

2

models. Some of the top features are shown in
Table 1.

IV. Models

For the sake of brevity, only the top ﬁve
models will be described, although over 20
were tested.
I. Rotation Forest

Rotation Forest[1] builds an ensemble (or
forest) of decision trees, in this case C4.5 trees.
The feature set is split into K disjoint subsets.
In this case, K was 75, which corresponds to
roughly 3 features per set. For each classi-
ﬁer, principal component analysis (PCA) is run
using the subset of features chosen. The coefﬁ-
cients found using PCA are put into a sparse
rotation matrix, which is rearranged to match
the chosen features. This new matrix multi-
plied by the training data comprises the model.
To classify a new data point, the conﬁdence for
each individual model is calculated, and the
output class is the one with the highest conﬁ-
dence. This model had a conﬁdence factor of
0.25.
II. Random Forest

Random Forest[2], similar to Rotation For-
est, is an ensemble of different decision tree
models. However, instead of using a conﬁ-
dence measure, the different models vote on
the output class. The training sets for these
models are chosen randomly, but with the
same distribution. In this case, there were ten
decision tree models and the models used the
full feature set.
III. PART

PART[3] is a rule-based algorithm. The data
set is divided into several new sets. A partial
pruned C4.5 decision tree is built based on
each set. The leaf of this tree with the largest
data coverage (meaning it classiﬁes the most
instances) is made into a rule. A leaf must
cover at least two instances to be considered
for a rule.
IV. LMT

LMT[4] stands for Logistic Model Tree. This
is a basic decision tree except that instead of

Table 1: Top Ten Features by Informaton Gain

dylibs
mbukernel
mbuinstrument
QtCore
wlmstrings
mbustrings
MicrosoftComponentPlugin
QtGui
libstdc++
QtNetwork
Netlib

functions
_exit
_system
_kCFHTTPVersion1_1
_umask
_fread
_sysctlnametomib
_CFReadStreamCreateForHTTPRequest
_IORegistryEntryFromPath
_gmtime
_sysctl

structural
num_imports
std_segsize
ﬁle_type
avg_segsize
std_segvmsize
min_segsize
avg_r1
avg_segvmsize
std_secsize
avg_secalign

having classes at the leaf nodes, it has logis-
tic regression functions. A node must cover
at least ﬁfteen instances to have child nodes.
The functions at the leaf nodes model the class
probability:

Pr(G = j|X = x) =

Fj = α

eFj(x)
∑J
k=1 eFk(x)
0 + ∑
j
α
v∈VT

(1)

(2)

j
vv

where Vt is a subset of all the features, αi’s
are weights, and k is the number of possible
feature values.
V.

IBk

IBk[5] is an instance-based learning imple-
mentation of k-Nearest Neighbors (in this case
k=1). For each data point, it calculates a simi-
larity for each cluster. That data point is then
assigned to the cluster with the highest simi-
larity.

V. Results

Many different supervised learning algo-
rithms were evaluated to see which one could
classify the data the best. The algorithms were
trained on the full 227 attributes. The top ten
results are listed in Table 2. A false positive is
a goodware sample that was classiﬁed as mal-
ware, and a false negative is a malware sample
that was classiﬁed as goodware.

Some interesting facts that can be gleaned

from the data itself:

• Goodware tends to import more libraries

than malware

• Malware tends to have more segments
than goodware and use more varied seg-
ment ﬂags

• Malware and goodware have the same

distribution of load com- mand sizes

• Malware is more likely to use Java li-

braries

• Malware is more likely to use HTTP
stream functions (probably for con-
trol/exﬁltration)

• Goodware uses more memory access con-

trols

VI. Discussion

The ensemble methods had the best perfor-
mance, perhaps due to the complex nature of
the input features or the varied data set. Since
all of the import features were nominal fea-
tures, decision tree models were particularly
well suited for classiﬁcation.

The fact that the testing error can be re-
duced to a few percent means that there are
actually tangible differences in static structures
between malware and goodware. This was ex-
pected going into the project because substan-
tial results have been produced using a similar
approach with Windows portable executable
(PE) ﬁles and malware.

3

Table 2: Top Ten Supervised Learning Algorithms Results

Algorithm
Rotation Forest
Random Forest
PART
LMT
IBk
SMO
FT Tree
J48/C4.5 Tree
Regression

Training Error Test Error
2.7682%
4.1522%
4.4983%
5.1903%
5.1903%
6.2284%
6.5744%
6.5744%
9.3426%

0.2653%
0.1768%
0.9726%
0.9726%
0%
3.8019%
1.4147%
1.1494%
3.4483%

False Positive
0.02
0.035
0.025
0.025
0.035
0.04
0.35
0.05
0.05

False Negative ROC Area
0.994
0.99
0.949
0.98
0.944
0.923
0.921
0.92
0.966

0.046
0.057
0.092
0.115
0.092
0.115
0.138
0.103
0.195

However, the results are not quite what
would be considered "production level" for mal-
ware detection. As a general rule, the goal is to
keep false positives below 0.001% and false neg-
atives below 1%. At best, among these models
the false positive was 2% and the false nega-
tive was 4.6%. False positives are especially
important to keep low, because customers will
not use an antivirus that blocks their legitimate
programs.

The models built in this project are still use-
ful as triaging tools. In cybersecurity, incident
response teams at businesses are swamped
with email security alerts every day, especially
with regards to suspicious email attachments.
Running these models on the applicable exe-
cutables can allow the incident response team
to focus only on those that have a high proba-
bility of being malware. So the conclusion is
that static analysis shows substantial promise,
even if it is not quite reﬁned enough to be
deployed as a standalone antivirus.
VII. Future Work

The one major roadblock to this project is
a lack of malware samples. The 420 samples
here took weeks to obtain, whereas, by contrast,
thousands of goodware samples can be gath-
ered in a day. Expanding the sample set would
probably cut down on the generalization er-
ror. One possible approach would be to catch
malware using a honeypot setup, though that
often comes with its own set of legal issues.

There are also more features that can be

4

used within the Mach-O ﬁles. Adding more
features could possibly increase the accuracy
of the classiﬁcation, but it comes at the price of
a larger model.

This static analysis approach could also be
used to try and classify malware into "fami-
lies" (for example, all polymorphic Flashback
executables), which is an open problem in the
security industry.

[1]

References

J. Rodriguez and L. Kuncheva, "Rota-
tion Forest: A New Classiﬁer Ensemble
Method", IEEE Trans. Pattern Analysis and
Machine Intelligence , vol. 28, no. 10, pp.
1619-1630, Oct. 2006.

[2] Leo Breiman, "Random Forests", Univer-

sity of California, Jan. 2001.

[3] Eibe Frank and Ian H. Witten, "Generat-
ing Accurate Rulesets Without Global Op-
timization", University of Waikato.

[4] N. Landwehr, M. Hall, and E. Frank, "Lo-
gistic Model Trees", 14th European Confer-
ence on Machine Learning, Jun. 2004.

[5] D. Aha, D. Kibler, and M. Albert,
"Instance-Based Learning Algorithms",
Machine Learning , vol. 6, pp. 37-66, 1991.

[6]

"OS X ABI Mach-O File Format
Reference". Mac Developer Library.
https://developer.apple.com/library/mac
/documentation/DeveloperTools/Con-
ceptual/MachORuntime/index.html.

[7]

"Weka". The University of Waikato.
http://www.cs.waikato.ac.nz/ml/weka/.

[9]

"VirusTotal".
https://www.virustotal.com/.

[10] "Mac

Malware".

Press

Kasper-
2014.

[8]

"Contagio Malware Dump". Mila.
http://contagiodump.blogspot.com/.

sky
https://press.kaspersky.com/ﬁles/2014/11/Mac-
threats.png.

Release

5

