CS 229 Machine Learning

Stanford University

CS229 Final Project: k-Means Algorithm

Colin Wei and Alfred Xue

SUNet ID: colinwei axue

December 11, 2014

1 Notes

This project was done in conjuction with a similar project that explored k-means in CS 264.
Relevant parts of our paper are shared between the two. However, the majority of the two
papers are diﬀerent – the CS 264 paper focuses on theoritical research that has been done on
k-means, while this project focuses on the diﬀerences between k− means and single-link++,
with a more practical approach.

Introduction

2
The k-means algorithm is an algorithm used commonly for clustering points in Rn. While
this algorithm works quite well in practice, there are two aspects of this algorithm that are
hard to grasp theoretically.

First, it has been hard to prove any meaningful upper bound on the running time of
this algorithm. The worst case running time of k-means has been shown to be 2Ω(n) [7],
although in practice, k-means takes a sublinear number of iterations to converge for real
datasets. Smoothed analysis has given polynomial-time bounds to this problem, but even
these bounds are much higher than what has been observed empirically. We will summarize
some of the smoothed analysis work on k-means.

Second, it has been hard to quantify the accuracy of the solution that k-means converges
to. Although k-means has guaranteed convergence because each step of the algorithm per-
forms coordinate descent on the k-means objective, the algorithm rarely converges to the
exact optimal solution because it mostly gets stuck at local minima. Modiﬁcations of the k-
means algorithm with diﬀerent centroid initialization procedures, such as k-means++, have
improved the accuracy of convergence.

We are interested to see if k-means performance correlated with notions of stability we
discussed in class. We were interested in γ-perturbation stability in particular. Unfortu-
nately, the proof of the (c, )-stability discussed in [1] does not extend to γ pertubation
stability. Upon intital testing, we found little indication that gamma-stability increased
k-means performance. We also noticed that k-means seemed to get stuck on local mini-
mum more when the pertubation stability factor increased. To explore this, we implemented
k-means and compared its performance to single-link++.

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

1

CS 229 Machine Learning

Stanford University

CS229 Final Project: k-Means Algorithm

Colin Wei and Alfred Xue

SUNet ID: colinwei axue

December 11, 2014

1 Notes

This project was done in conjuction with a similar project that explored k-means in CS 264.
Relevant parts of our paper are shared between the two. However, the majority of the two
papers are diﬀerent – the CS 264 paper focuses on theoritical research that has been done on
k-means, while this project focuses on the diﬀerences between k− means and single-link++,
with a more practical approach.

Introduction

2
The k-means algorithm is an algorithm used commonly for clustering points in Rn. While
this algorithm works quite well in practice, there are two aspects of this algorithm that are
hard to grasp theoretically.

First, it has been hard to prove any meaningful upper bound on the running time of
this algorithm. The worst case running time of k-means has been shown to be 2Ω(n) [7],
although in practice, k-means takes a sublinear number of iterations to converge for real
datasets. Smoothed analysis has given polynomial-time bounds to this problem, but even
these bounds are much higher than what has been observed empirically. We will summarize
some of the smoothed analysis work on k-means.

Second, it has been hard to quantify the accuracy of the solution that k-means converges
to. Although k-means has guaranteed convergence because each step of the algorithm per-
forms coordinate descent on the k-means objective, the algorithm rarely converges to the
exact optimal solution because it mostly gets stuck at local minima. Modiﬁcations of the k-
means algorithm with diﬀerent centroid initialization procedures, such as k-means++, have
improved the accuracy of convergence.

We are interested to see if k-means performance correlated with notions of stability we
discussed in class. We were interested in γ-perturbation stability in particular. Unfortu-
nately, the proof of the (c, )-stability discussed in [1] does not extend to γ pertubation
stability. Upon intital testing, we found little indication that gamma-stability increased
k-means performance. We also noticed that k-means seemed to get stuck on local mini-
mum more when the pertubation stability factor increased. To explore this, we implemented
k-means and compared its performance to single-link++.

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

1

CS 229 Machine Learning

Stanford University

3 Deﬁnitions
The k-means clustering problem is as follows: given a set P of n points p1, ..., pn ∈ Rd, choose
a set C of k centers c1, ..., ck ∈ Rd to minimize the objective function

(cid:88)

p∈P

φC(P ) =

||p − c||2

min
c∈C

The k-means algorithm for this problem works as follows:
1. Randomly initialize cluster centroids c1, ..., ck ∈ Rd.

2. Until convergence, repeat:

i. For every point in the data set pi, let wi = arg minj ||pi − cj||.

ii. For every cluster centroid cj, set cj =

(cid:80)n
(cid:80)n
i=1 1{wi = j}pi
i=1 1{wi = j} .

The single-link++ clustering method is implmeneted as follows:

1. Construct a tree as follows

i. Place every point in its own tree.

ii. Select the two points not already connected with the smallest distance between them.

iii. Connect the heads of the two trees that corespond to these points.

(that is, we

construct a binary tree)

iv. Repeat step ii. and iii. until there is only one tree remaining.

2. Prune the tree by removing k links starting from the head of the tree, where a link can

only be broken from a head, that maximize the objective function

(cid:88)

p∈P

φC(P ) =

||p − c||2

min
c∈C

where the center of each cluster is the most optimal point for said cluster.

4

k-Means vs. Single-Link++

We wanted to test the performance of k-means and single-link++, due to [8] on data satisfy-
ing γ-perturbation stability to see how the two algorithms would compare. Furthermore, we
wanted to see if γ-perturbation stability was a particularly useful notion for k-means at all,
as it seemed like something that applied much better to hierarchical clustering algorithms.
We generated our own data to satisfy γ-perturbation stability as follows:
1. We let k = γ +1, and we placed cluster centers in R3 at (k, 0, 0), (0, k, 0), (0, 0, 0), (k, k, 0),

(k/2, k/2, k/

√
√
2), (k/2, k/2,−k/

2).

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

2

CS 229 Machine Learning

Stanford University

CS229 Final Project: k-Means Algorithm

Colin Wei and Alfred Xue

SUNet ID: colinwei axue

December 11, 2014

1 Notes

This project was done in conjuction with a similar project that explored k-means in CS 264.
Relevant parts of our paper are shared between the two. However, the majority of the two
papers are diﬀerent – the CS 264 paper focuses on theoritical research that has been done on
k-means, while this project focuses on the diﬀerences between k− means and single-link++,
with a more practical approach.

Introduction

2
The k-means algorithm is an algorithm used commonly for clustering points in Rn. While
this algorithm works quite well in practice, there are two aspects of this algorithm that are
hard to grasp theoretically.

First, it has been hard to prove any meaningful upper bound on the running time of
this algorithm. The worst case running time of k-means has been shown to be 2Ω(n) [7],
although in practice, k-means takes a sublinear number of iterations to converge for real
datasets. Smoothed analysis has given polynomial-time bounds to this problem, but even
these bounds are much higher than what has been observed empirically. We will summarize
some of the smoothed analysis work on k-means.

Second, it has been hard to quantify the accuracy of the solution that k-means converges
to. Although k-means has guaranteed convergence because each step of the algorithm per-
forms coordinate descent on the k-means objective, the algorithm rarely converges to the
exact optimal solution because it mostly gets stuck at local minima. Modiﬁcations of the k-
means algorithm with diﬀerent centroid initialization procedures, such as k-means++, have
improved the accuracy of convergence.

We are interested to see if k-means performance correlated with notions of stability we
discussed in class. We were interested in γ-perturbation stability in particular. Unfortu-
nately, the proof of the (c, )-stability discussed in [1] does not extend to γ pertubation
stability. Upon intital testing, we found little indication that gamma-stability increased
k-means performance. We also noticed that k-means seemed to get stuck on local mini-
mum more when the pertubation stability factor increased. To explore this, we implemented
k-means and compared its performance to single-link++.

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

1

CS 229 Machine Learning

Stanford University

3 Deﬁnitions
The k-means clustering problem is as follows: given a set P of n points p1, ..., pn ∈ Rd, choose
a set C of k centers c1, ..., ck ∈ Rd to minimize the objective function

(cid:88)

p∈P

φC(P ) =

||p − c||2

min
c∈C

The k-means algorithm for this problem works as follows:
1. Randomly initialize cluster centroids c1, ..., ck ∈ Rd.

2. Until convergence, repeat:

i. For every point in the data set pi, let wi = arg minj ||pi − cj||.

ii. For every cluster centroid cj, set cj =

(cid:80)n
(cid:80)n
i=1 1{wi = j}pi
i=1 1{wi = j} .

The single-link++ clustering method is implmeneted as follows:

1. Construct a tree as follows

i. Place every point in its own tree.

ii. Select the two points not already connected with the smallest distance between them.

iii. Connect the heads of the two trees that corespond to these points.

(that is, we

construct a binary tree)

iv. Repeat step ii. and iii. until there is only one tree remaining.

2. Prune the tree by removing k links starting from the head of the tree, where a link can

only be broken from a head, that maximize the objective function

(cid:88)

p∈P

φC(P ) =

||p − c||2

min
c∈C

where the center of each cluster is the most optimal point for said cluster.

4

k-Means vs. Single-Link++

We wanted to test the performance of k-means and single-link++, due to [8] on data satisfy-
ing γ-perturbation stability to see how the two algorithms would compare. Furthermore, we
wanted to see if γ-perturbation stability was a particularly useful notion for k-means at all,
as it seemed like something that applied much better to hierarchical clustering algorithms.
We generated our own data to satisfy γ-perturbation stability as follows:
1. We let k = γ +1, and we placed cluster centers in R3 at (k, 0, 0), (0, k, 0), (0, 0, 0), (k, k, 0),

(k/2, k/2, k/

√
√
2), (k/2, k/2,−k/

2).

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

2

CS 229 Machine Learning

Stanford University

2. For 1000 total points, we chose one of the 6 points and inserted a random point in a
sphere of radius one around that center point. These 1000 random points formed our
dataset.

Since our data was randomly generated, it might not have satisﬁed γ-perturbation stability
perfectly if one of the actual centers was oﬀ, but we believed it approximated this well enough
for our purposes. For each of the 10 values of γ from 0.6, 0.7, ..., 1.4, 1.5, we generated 40
such data sets and ran single-link++ and k-means on both data sets.
(Even though γ-
perturbation stability is not a valid notion for γ < 1, we still used it as a parameter for
controlling how separated our clusters were. The following chart shows the average value of
the objective function for those data sets:

γ
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5

SL++ Avg. k-Means Avg. SL++ Std. Dev. k-Means Std. Dev.
1860.34
2027.64
2180.089
2364.864
2569.541
2714.904
1845.208
798.727
597.335
594.659

18.11837
6.852872
9.915998
9.957035
40.21661
76.3374
54.38815
62.24314
96.25676
130.2924

575.0438
583.94
587.9218
593.7948
602.3708
613.0033
606.7595
606.5338
618.976
631.018

35.58476
37.38439
37.96719
31.50132
60.71504
155.5746
565.664
235.5272
6.602786
7.134685

We found it really interesting that the performance of single-link++ dropped so quickly;
we were surprised to see such a dramatic change. The other interesting thing we noticed
about our results was how k-means became more and more erratic as perturbation stability
increased, which seemed like a counterintuitive idea at ﬁrst. The standard deviation of the
k-means objective increased signiﬁcantly from γ = 0.6 to γ = 1.5. However, this makes

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

3

CS 229 Machine Learning

Stanford University

CS229 Final Project: k-Means Algorithm

Colin Wei and Alfred Xue

SUNet ID: colinwei axue

December 11, 2014

1 Notes

This project was done in conjuction with a similar project that explored k-means in CS 264.
Relevant parts of our paper are shared between the two. However, the majority of the two
papers are diﬀerent – the CS 264 paper focuses on theoritical research that has been done on
k-means, while this project focuses on the diﬀerences between k− means and single-link++,
with a more practical approach.

Introduction

2
The k-means algorithm is an algorithm used commonly for clustering points in Rn. While
this algorithm works quite well in practice, there are two aspects of this algorithm that are
hard to grasp theoretically.

First, it has been hard to prove any meaningful upper bound on the running time of
this algorithm. The worst case running time of k-means has been shown to be 2Ω(n) [7],
although in practice, k-means takes a sublinear number of iterations to converge for real
datasets. Smoothed analysis has given polynomial-time bounds to this problem, but even
these bounds are much higher than what has been observed empirically. We will summarize
some of the smoothed analysis work on k-means.

Second, it has been hard to quantify the accuracy of the solution that k-means converges
to. Although k-means has guaranteed convergence because each step of the algorithm per-
forms coordinate descent on the k-means objective, the algorithm rarely converges to the
exact optimal solution because it mostly gets stuck at local minima. Modiﬁcations of the k-
means algorithm with diﬀerent centroid initialization procedures, such as k-means++, have
improved the accuracy of convergence.

We are interested to see if k-means performance correlated with notions of stability we
discussed in class. We were interested in γ-perturbation stability in particular. Unfortu-
nately, the proof of the (c, )-stability discussed in [1] does not extend to γ pertubation
stability. Upon intital testing, we found little indication that gamma-stability increased
k-means performance. We also noticed that k-means seemed to get stuck on local mini-
mum more when the pertubation stability factor increased. To explore this, we implemented
k-means and compared its performance to single-link++.

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

1

CS 229 Machine Learning

Stanford University

3 Deﬁnitions
The k-means clustering problem is as follows: given a set P of n points p1, ..., pn ∈ Rd, choose
a set C of k centers c1, ..., ck ∈ Rd to minimize the objective function

(cid:88)

p∈P

φC(P ) =

||p − c||2

min
c∈C

The k-means algorithm for this problem works as follows:
1. Randomly initialize cluster centroids c1, ..., ck ∈ Rd.

2. Until convergence, repeat:

i. For every point in the data set pi, let wi = arg minj ||pi − cj||.

ii. For every cluster centroid cj, set cj =

(cid:80)n
(cid:80)n
i=1 1{wi = j}pi
i=1 1{wi = j} .

The single-link++ clustering method is implmeneted as follows:

1. Construct a tree as follows

i. Place every point in its own tree.

ii. Select the two points not already connected with the smallest distance between them.

iii. Connect the heads of the two trees that corespond to these points.

(that is, we

construct a binary tree)

iv. Repeat step ii. and iii. until there is only one tree remaining.

2. Prune the tree by removing k links starting from the head of the tree, where a link can

only be broken from a head, that maximize the objective function

(cid:88)

p∈P

φC(P ) =

||p − c||2

min
c∈C

where the center of each cluster is the most optimal point for said cluster.

4

k-Means vs. Single-Link++

We wanted to test the performance of k-means and single-link++, due to [8] on data satisfy-
ing γ-perturbation stability to see how the two algorithms would compare. Furthermore, we
wanted to see if γ-perturbation stability was a particularly useful notion for k-means at all,
as it seemed like something that applied much better to hierarchical clustering algorithms.
We generated our own data to satisfy γ-perturbation stability as follows:
1. We let k = γ +1, and we placed cluster centers in R3 at (k, 0, 0), (0, k, 0), (0, 0, 0), (k, k, 0),

(k/2, k/2, k/

√
√
2), (k/2, k/2,−k/

2).

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

2

CS 229 Machine Learning

Stanford University

2. For 1000 total points, we chose one of the 6 points and inserted a random point in a
sphere of radius one around that center point. These 1000 random points formed our
dataset.

Since our data was randomly generated, it might not have satisﬁed γ-perturbation stability
perfectly if one of the actual centers was oﬀ, but we believed it approximated this well enough
for our purposes. For each of the 10 values of γ from 0.6, 0.7, ..., 1.4, 1.5, we generated 40
such data sets and ran single-link++ and k-means on both data sets.
(Even though γ-
perturbation stability is not a valid notion for γ < 1, we still used it as a parameter for
controlling how separated our clusters were. The following chart shows the average value of
the objective function for those data sets:

γ
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5

SL++ Avg. k-Means Avg. SL++ Std. Dev. k-Means Std. Dev.
1860.34
2027.64
2180.089
2364.864
2569.541
2714.904
1845.208
798.727
597.335
594.659

18.11837
6.852872
9.915998
9.957035
40.21661
76.3374
54.38815
62.24314
96.25676
130.2924

575.0438
583.94
587.9218
593.7948
602.3708
613.0033
606.7595
606.5338
618.976
631.018

35.58476
37.38439
37.96719
31.50132
60.71504
155.5746
565.664
235.5272
6.602786
7.134685

We found it really interesting that the performance of single-link++ dropped so quickly;
we were surprised to see such a dramatic change. The other interesting thing we noticed
about our results was how k-means became more and more erratic as perturbation stability
increased, which seemed like a counterintuitive idea at ﬁrst. The standard deviation of the
k-means objective increased signiﬁcantly from γ = 0.6 to γ = 1.5. However, this makes

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

3

CS 229 Machine Learning

Stanford University

sense because as data becomes more and more well-separated, local maxima to the k-means
objective become worse relative to the optimal solution.

Before we ran these tests, we believed that stability notions would have some positive
impact on either k-means runtime or approximation ratio. However, our tests showed no
correlation between perturbation stability and the number of iterations k-means took and
also negative results for approximation in terms of increasing standard deviation. Granted,
our results could have been diﬀerent had we chosen to implement k-means++ instead of k-
means. However, it seems like in general, k-means is not very tractable even under stability
assumptions. This makes sense, as k-means is used a lot for real world data that isn’t
well-separated.

5 Conclusion

k-means does well through all data. As far as time eﬃcency goes, k-means is far superior, as
even in the worst case of its run times only had 40 iterations. Since our dataset was large,
even if k-means was run 25 diﬀerent times, k-means would still be superior in time eﬃcency.
We notice that for low values of γ, the mminimum objective value of sl++ steadily increases
until it reaches around 1.4. This can be explained by the idea that the number of misses
remain constant, but each miss is punished more due to the larger pertubation factor.
it
is likely the case that for pertubation factors above 1.4, sl++ returns the optimal value,
but has yet to be proven to be the case. The larger standard deviations as the pertubation
factor increases for k-means could either indicate that it fails more as the pertubation factor
increases, or simply that k-means is getting punished more for failed pertubations.

In conclusion, k-means seems to be the optimal method in practice, even for data with
large pertubation factors. SL++ remains a popular tool, however, for theoritical purposes
and bound exploration.

6 Future Research

Primary future research should focus on divising theory to support our researchw. Other
areas include exploring SL++ and (c, ) stability.

7 References

[1] Agarwal, Manu, Ragesh Jaiswal, and Arindam Pal. ”k-means++ under Approximation

Stability.” Theory and Applications of Models of Computation: 84.

[2] Arthur, David, Bodo Manthey, and H. Roglin. ”k-Means has polynomial smoothed
complexity.” Foundations of Computer Science, 2009. FOCS’09. 50th Annual IEEE
Symposium on. IEEE, 2009.

[3] Arthur, David, and Sergei Vassilvitskii. ”Worst-case and smoothed analysis of the ICP
algorithm, with an application to the k-means method.” Foundations of Computer Sci-
ence, 2006. FOCS’06. 47th Annual IEEE Symposium on. IEEE, 2006.

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

4

CS 229 Machine Learning

Stanford University

CS229 Final Project: k-Means Algorithm

Colin Wei and Alfred Xue

SUNet ID: colinwei axue

December 11, 2014

1 Notes

This project was done in conjuction with a similar project that explored k-means in CS 264.
Relevant parts of our paper are shared between the two. However, the majority of the two
papers are diﬀerent – the CS 264 paper focuses on theoritical research that has been done on
k-means, while this project focuses on the diﬀerences between k− means and single-link++,
with a more practical approach.

Introduction

2
The k-means algorithm is an algorithm used commonly for clustering points in Rn. While
this algorithm works quite well in practice, there are two aspects of this algorithm that are
hard to grasp theoretically.

First, it has been hard to prove any meaningful upper bound on the running time of
this algorithm. The worst case running time of k-means has been shown to be 2Ω(n) [7],
although in practice, k-means takes a sublinear number of iterations to converge for real
datasets. Smoothed analysis has given polynomial-time bounds to this problem, but even
these bounds are much higher than what has been observed empirically. We will summarize
some of the smoothed analysis work on k-means.

Second, it has been hard to quantify the accuracy of the solution that k-means converges
to. Although k-means has guaranteed convergence because each step of the algorithm per-
forms coordinate descent on the k-means objective, the algorithm rarely converges to the
exact optimal solution because it mostly gets stuck at local minima. Modiﬁcations of the k-
means algorithm with diﬀerent centroid initialization procedures, such as k-means++, have
improved the accuracy of convergence.

We are interested to see if k-means performance correlated with notions of stability we
discussed in class. We were interested in γ-perturbation stability in particular. Unfortu-
nately, the proof of the (c, )-stability discussed in [1] does not extend to γ pertubation
stability. Upon intital testing, we found little indication that gamma-stability increased
k-means performance. We also noticed that k-means seemed to get stuck on local mini-
mum more when the pertubation stability factor increased. To explore this, we implemented
k-means and compared its performance to single-link++.

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

1

CS 229 Machine Learning

Stanford University

3 Deﬁnitions
The k-means clustering problem is as follows: given a set P of n points p1, ..., pn ∈ Rd, choose
a set C of k centers c1, ..., ck ∈ Rd to minimize the objective function

(cid:88)

p∈P

φC(P ) =

||p − c||2

min
c∈C

The k-means algorithm for this problem works as follows:
1. Randomly initialize cluster centroids c1, ..., ck ∈ Rd.

2. Until convergence, repeat:

i. For every point in the data set pi, let wi = arg minj ||pi − cj||.

ii. For every cluster centroid cj, set cj =

(cid:80)n
(cid:80)n
i=1 1{wi = j}pi
i=1 1{wi = j} .

The single-link++ clustering method is implmeneted as follows:

1. Construct a tree as follows

i. Place every point in its own tree.

ii. Select the two points not already connected with the smallest distance between them.

iii. Connect the heads of the two trees that corespond to these points.

(that is, we

construct a binary tree)

iv. Repeat step ii. and iii. until there is only one tree remaining.

2. Prune the tree by removing k links starting from the head of the tree, where a link can

only be broken from a head, that maximize the objective function

(cid:88)

p∈P

φC(P ) =

||p − c||2

min
c∈C

where the center of each cluster is the most optimal point for said cluster.

4

k-Means vs. Single-Link++

We wanted to test the performance of k-means and single-link++, due to [8] on data satisfy-
ing γ-perturbation stability to see how the two algorithms would compare. Furthermore, we
wanted to see if γ-perturbation stability was a particularly useful notion for k-means at all,
as it seemed like something that applied much better to hierarchical clustering algorithms.
We generated our own data to satisfy γ-perturbation stability as follows:
1. We let k = γ +1, and we placed cluster centers in R3 at (k, 0, 0), (0, k, 0), (0, 0, 0), (k, k, 0),

(k/2, k/2, k/

√
√
2), (k/2, k/2,−k/

2).

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

2

CS 229 Machine Learning

Stanford University

2. For 1000 total points, we chose one of the 6 points and inserted a random point in a
sphere of radius one around that center point. These 1000 random points formed our
dataset.

Since our data was randomly generated, it might not have satisﬁed γ-perturbation stability
perfectly if one of the actual centers was oﬀ, but we believed it approximated this well enough
for our purposes. For each of the 10 values of γ from 0.6, 0.7, ..., 1.4, 1.5, we generated 40
such data sets and ran single-link++ and k-means on both data sets.
(Even though γ-
perturbation stability is not a valid notion for γ < 1, we still used it as a parameter for
controlling how separated our clusters were. The following chart shows the average value of
the objective function for those data sets:

γ
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5

SL++ Avg. k-Means Avg. SL++ Std. Dev. k-Means Std. Dev.
1860.34
2027.64
2180.089
2364.864
2569.541
2714.904
1845.208
798.727
597.335
594.659

18.11837
6.852872
9.915998
9.957035
40.21661
76.3374
54.38815
62.24314
96.25676
130.2924

575.0438
583.94
587.9218
593.7948
602.3708
613.0033
606.7595
606.5338
618.976
631.018

35.58476
37.38439
37.96719
31.50132
60.71504
155.5746
565.664
235.5272
6.602786
7.134685

We found it really interesting that the performance of single-link++ dropped so quickly;
we were surprised to see such a dramatic change. The other interesting thing we noticed
about our results was how k-means became more and more erratic as perturbation stability
increased, which seemed like a counterintuitive idea at ﬁrst. The standard deviation of the
k-means objective increased signiﬁcantly from γ = 0.6 to γ = 1.5. However, this makes

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

3

CS 229 Machine Learning

Stanford University

sense because as data becomes more and more well-separated, local maxima to the k-means
objective become worse relative to the optimal solution.

Before we ran these tests, we believed that stability notions would have some positive
impact on either k-means runtime or approximation ratio. However, our tests showed no
correlation between perturbation stability and the number of iterations k-means took and
also negative results for approximation in terms of increasing standard deviation. Granted,
our results could have been diﬀerent had we chosen to implement k-means++ instead of k-
means. However, it seems like in general, k-means is not very tractable even under stability
assumptions. This makes sense, as k-means is used a lot for real world data that isn’t
well-separated.

5 Conclusion

k-means does well through all data. As far as time eﬃcency goes, k-means is far superior, as
even in the worst case of its run times only had 40 iterations. Since our dataset was large,
even if k-means was run 25 diﬀerent times, k-means would still be superior in time eﬃcency.
We notice that for low values of γ, the mminimum objective value of sl++ steadily increases
until it reaches around 1.4. This can be explained by the idea that the number of misses
remain constant, but each miss is punished more due to the larger pertubation factor.
it
is likely the case that for pertubation factors above 1.4, sl++ returns the optimal value,
but has yet to be proven to be the case. The larger standard deviations as the pertubation
factor increases for k-means could either indicate that it fails more as the pertubation factor
increases, or simply that k-means is getting punished more for failed pertubations.

In conclusion, k-means seems to be the optimal method in practice, even for data with
large pertubation factors. SL++ remains a popular tool, however, for theoritical purposes
and bound exploration.

6 Future Research

Primary future research should focus on divising theory to support our researchw. Other
areas include exploring SL++ and (c, ) stability.

7 References

[1] Agarwal, Manu, Ragesh Jaiswal, and Arindam Pal. ”k-means++ under Approximation

Stability.” Theory and Applications of Models of Computation: 84.

[2] Arthur, David, Bodo Manthey, and H. Roglin. ”k-Means has polynomial smoothed
complexity.” Foundations of Computer Science, 2009. FOCS’09. 50th Annual IEEE
Symposium on. IEEE, 2009.

[3] Arthur, David, and Sergei Vassilvitskii. ”Worst-case and smoothed analysis of the ICP
algorithm, with an application to the k-means method.” Foundations of Computer Sci-
ence, 2006. FOCS’06. 47th Annual IEEE Symposium on. IEEE, 2006.

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

4

CS 229 Machine Learning

Stanford University

[4] Ragesh Jaiswal and Nitin Garg. Analysis of k-means++ for separable data.

In Pro-
ceedings of the 16th International Workshop on Randomization and Computation, pp.
591?602, 2012.

[5] David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seed-
ing. In Proceedings of the 18th annual ACM-SIAM symposium on Discrete Algorithms
(SODA?07), pp. 1027-1035, 2007

[6] Ostrovsky, Rafail, et al. ”The eﬀectiveness of Lloyd-type methods for the k-means prob-

lem.” Journal of the ACM (JACM) 59.6 (2012): 28.

[7] Andrea Vattani. k-means requires exponentially many iterations even in the plane. In
Proc. of the 25th ACM Symp. on Computational Geometry (SoCG), pages 324-332,
2009.

[8] Awasthi, Pranjal, Avrim Blum, and Or Sheﬀet. ”Center-based clustering under pertur-

bation stability.” Information Processing Letters 112.1 (2012): 49-54.

[9] Balcan, Maria-Florina, Avrim Blum, and Anupam Gupta. ”Approximate clustering with-
out the approximation.” Proceedings of the twentieth Annual ACM-SIAM Symposium
on Discrete Algorithms. Society for Industrial and Applied Mathematics, 2009.

Colin Wei and Alfred Xue: colinwei axue@stanford.edu

5

