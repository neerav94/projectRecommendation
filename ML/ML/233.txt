Language Identiﬁcation from Text Documents

Priyank Mathur
Adobe Systems

Stanford University

Arkajyoti Misra
Target Corporation
Stanford University

Stanford, California 94305

priyankm@stanford.edu

Stanford, California 94305

arkajyot@stanford.edu

Emrah Budur

Garanti Technology
Stanford University

Stanford, California 94305

emrah@stanford.edu

Abstract—The increase in the use of microblogging came along
with the rapid growth on short linguistic data. On the other
hand deep learning is considered to be the new frontier to
extract meaningful
information out of large amount of raw
data in an automated manner [1]. In this study, we engaged
these two emerging ﬁelds to come up with a robust language
identiﬁer on demand, namely Stanford Language Identiﬁcation
Engine (SLIDE). As a result, we achieved 95.12% accuracy in
Discriminating between Similar Languages (DSL) Shared Task
2015 dataset, beating the maximum reported accuracy achieved
so far [2].

Index Terms—Language identiﬁcation, LID, SLIDE.

I. INTRODUCTION

Automatic language detection is the ﬁrst step toward achiev-
ing a variety of tasks like detecting the source language for
machine translation, improving the search relevancy by per-
sonalizing the search results according to the query language
[3], providing uniform search box for a multilingual dictionary
[4], tagging data stream from Twitter with appropriate lan-
guage etc. While classifying languages belonging to disjoint
groups is not hard, disambiguation of languages originating
from the same source and dialects still pose a considerable
challenge in the area of natural language processing. Regular
classiﬁers based on word frequency only are inadequate in
making a correct prediction for such similar languages and
utilization of state of the art machine learning tools to capture
the structure of the language has become necessary to boost
the classiﬁer performance. In this work we took advantage
of recent advancement of deep neural network based models
showing stellar performance in many natural language pro-
cessing tasks to build a state of the art language classiﬁer.

We benchmarked our solution with the industry leaders and

achieved ﬁrst rank in the DSL test dataset.

II. PREVIOUS WORK

In the past, a variety of methods have been tried like
Naive Bayes [5], SVM [6], n-gram[7], graph-based n-gram[8],
prediction partial matching (PPM) [9], linear interpolation with
post independent weight optimization and majority voting for
combining multiple classiﬁers [10] etc. and the best accuracy
achieved are still in the lower ninety percents.

The researchers have worked on various critical tasks chal-
lenging the dimensions of the topic, including but not limited
to, supporting low resource languages, i.e. Nepali, Urdu, and
Icelandic [11], [12] handling user-generated unstructured short
texts, i.e. microblogs [11], [10] building a domain agnostic

engine [11], [8]. Existing benchmarking solutions approach
the LID problem in different ways where LogR [11] adopts a
discriminative approaches with regularized logistic regression,
TextCat and Google CLD[13] recruits N-gram-based algo-
rithm, langid.py [5] relies on a Naive Bayes classiﬁer with
a multinomial event model.

The outstanding results, of the time, suggested by Cavnar
and Trenkle became de facto standard of LID even today [7].
The signiﬁcant ingredient of their method is shown to use
a rank order statistic called ”out of place” distance measure
[14]. The problem in their approach is that they generated
n-grams out of words that requires tokenization. However,
many languages including Japanese and Chinese have no word
boundaries. Considering that Japanese is the second most
frequent language used in Twitter [12], there is a need for
better approach to scale the solution to all languages. As a
solution to their problem, Dunning came up with a better
approach with incorporating byte level n-grams of the whole
string instead of char level n-grams of the words [14].

After a rigorous literature survey, we found no prior study
that applied deep learning on language identiﬁcation of text.
On the other hand, there are a few number of studies that
applied deep learning to identify the language of speech [15],
[16], [17], [18]. We believe this study will be the ﬁrst in the
literature if published for LID in textual data by means of deep
learning.

III. DATASET DESCRIPTION

The data for this project work was obtained from ”Discrim-
inating between Similar Language (DSL) Shared Task 2015”
[19]. A set of 20000 instances per language (18000 training
(train.txt) and 2000 evaluation (test.txt)) was provided for 13
different world languages. The dataset also consisted of a
subset (devel.txt) of the overall training data which we utilized
for hyper-parameter tuning. The languages are grouped as
shown in Table I. The names of the groups will be frequently
referred in the subsequent sections.

Each entry in the dataset is a full sentence extracted from
journalistic corpora and written in one of the languages and
tagged with the language group and country of origin. A
similar set of mixed language instance was also provided to
add noise to the data. A separate gold test data was provided
for the ﬁnal evaluation (test-gold.txt).

We applied t-SNE algorithm to visualize the instances
in 3D euclidean space [20], [21]. For feature extraction,

Language Identiﬁcation from Text Documents

Priyank Mathur
Adobe Systems

Stanford University

Arkajyoti Misra
Target Corporation
Stanford University

Stanford, California 94305

priyankm@stanford.edu

Stanford, California 94305

arkajyot@stanford.edu

Emrah Budur

Garanti Technology
Stanford University

Stanford, California 94305

emrah@stanford.edu

Abstract—The increase in the use of microblogging came along
with the rapid growth on short linguistic data. On the other
hand deep learning is considered to be the new frontier to
extract meaningful
information out of large amount of raw
data in an automated manner [1]. In this study, we engaged
these two emerging ﬁelds to come up with a robust language
identiﬁer on demand, namely Stanford Language Identiﬁcation
Engine (SLIDE). As a result, we achieved 95.12% accuracy in
Discriminating between Similar Languages (DSL) Shared Task
2015 dataset, beating the maximum reported accuracy achieved
so far [2].

Index Terms—Language identiﬁcation, LID, SLIDE.

I. INTRODUCTION

Automatic language detection is the ﬁrst step toward achiev-
ing a variety of tasks like detecting the source language for
machine translation, improving the search relevancy by per-
sonalizing the search results according to the query language
[3], providing uniform search box for a multilingual dictionary
[4], tagging data stream from Twitter with appropriate lan-
guage etc. While classifying languages belonging to disjoint
groups is not hard, disambiguation of languages originating
from the same source and dialects still pose a considerable
challenge in the area of natural language processing. Regular
classiﬁers based on word frequency only are inadequate in
making a correct prediction for such similar languages and
utilization of state of the art machine learning tools to capture
the structure of the language has become necessary to boost
the classiﬁer performance. In this work we took advantage
of recent advancement of deep neural network based models
showing stellar performance in many natural language pro-
cessing tasks to build a state of the art language classiﬁer.

We benchmarked our solution with the industry leaders and

achieved ﬁrst rank in the DSL test dataset.

II. PREVIOUS WORK

In the past, a variety of methods have been tried like
Naive Bayes [5], SVM [6], n-gram[7], graph-based n-gram[8],
prediction partial matching (PPM) [9], linear interpolation with
post independent weight optimization and majority voting for
combining multiple classiﬁers [10] etc. and the best accuracy
achieved are still in the lower ninety percents.

The researchers have worked on various critical tasks chal-
lenging the dimensions of the topic, including but not limited
to, supporting low resource languages, i.e. Nepali, Urdu, and
Icelandic [11], [12] handling user-generated unstructured short
texts, i.e. microblogs [11], [10] building a domain agnostic

engine [11], [8]. Existing benchmarking solutions approach
the LID problem in different ways where LogR [11] adopts a
discriminative approaches with regularized logistic regression,
TextCat and Google CLD[13] recruits N-gram-based algo-
rithm, langid.py [5] relies on a Naive Bayes classiﬁer with
a multinomial event model.

The outstanding results, of the time, suggested by Cavnar
and Trenkle became de facto standard of LID even today [7].
The signiﬁcant ingredient of their method is shown to use
a rank order statistic called ”out of place” distance measure
[14]. The problem in their approach is that they generated
n-grams out of words that requires tokenization. However,
many languages including Japanese and Chinese have no word
boundaries. Considering that Japanese is the second most
frequent language used in Twitter [12], there is a need for
better approach to scale the solution to all languages. As a
solution to their problem, Dunning came up with a better
approach with incorporating byte level n-grams of the whole
string instead of char level n-grams of the words [14].

After a rigorous literature survey, we found no prior study
that applied deep learning on language identiﬁcation of text.
On the other hand, there are a few number of studies that
applied deep learning to identify the language of speech [15],
[16], [17], [18]. We believe this study will be the ﬁrst in the
literature if published for LID in textual data by means of deep
learning.

III. DATASET DESCRIPTION

The data for this project work was obtained from ”Discrim-
inating between Similar Language (DSL) Shared Task 2015”
[19]. A set of 20000 instances per language (18000 training
(train.txt) and 2000 evaluation (test.txt)) was provided for 13
different world languages. The dataset also consisted of a
subset (devel.txt) of the overall training data which we utilized
for hyper-parameter tuning. The languages are grouped as
shown in Table I. The names of the groups will be frequently
referred in the subsequent sections.

Each entry in the dataset is a full sentence extracted from
journalistic corpora and written in one of the languages and
tagged with the language group and country of origin. A
similar set of mixed language instance was also provided to
add noise to the data. A separate gold test data was provided
for the ﬁnal evaluation (test-gold.txt).

We applied t-SNE algorithm to visualize the instances
in 3D euclidean space [20], [21]. For feature extraction,

Group Name

Language Name

Language Code

South Eastern Slavic

South Western Slavic

West-Slavic

Ibero-Romance (Spanish)

Ibero-Romance (Portuguese)

Astronesian

Bulgarian
Macedonian
Bosnian
Croatian
Serbian
Czech
Slovak
Peninsular Spain
Argentinian Spanish
Brazilian Portuguese
European Portuguese
Indonesian
Malay

bg
mk
bs
hr
sr
cz
sk
es-ES
es-AR
pt-BR
pt-PT
id
my

TABLE I: Benchmark results of available solutions

(a) Easily separable
t-SNE visualization of

Fig. 1:
plots
http://SeeYourLanguage.info

including 3D animated plot

(b) Difﬁcult to separate

language groups. More
:

available

are

at

we vectorized each sentence over 1 to 5-grams of the to-
kens delimited by white space characters. Fig. 1 shows
the resulting plot. As can be seen on the plot,
the lan-
guages in the same group overlap a lot while the languages
in different groups can be linearly separable. A 3 dimen-
sional visualization of all the languages can be viewed at
https://www.youtube.com/watch?v=mhRdfC26q78.

A. Multinomial Naive Bayes

IV. METHODS

We created a baseline result by training a Multinomial
Naive Bayes model because it is quick to prototype, runs
fast and known to provide decent results in the ﬁeld of text
processing. We have done no pre-processing of the text com-
monly done in the ﬁeld like stemming or stop word removal
because we believe that could potentially remove important
signatures of a particular language, particularly when the same
language is spoken by two geographically disconnected group
of people (e.g Portuguese spoken in Portugal and Brazil).
We experimented with both word and character n-grams. The
character n-grams turned out to be particularly useful when
differentiating between two languages using mostly distinct
character sequences in their alphabet.

The character level n-gram behaves quite differently from
that of word level n-grams as shown in Fig. 2. Single char-

Fig. 2: Naive Bayes performance as a function of n for both
word and character n-grams.

acters carry little information and therefore the performance
for character n-gram improves quite sharply as the number
of characters is increased before saturating at about n=8. We
experimented with character n-grams both restricted at word
boundaries and spanning across word boundaries. The latter
has a marginal performance boost at the cost of longer training
time and memory pressure. The word n-gram model peaks
at n=2 and drops beyond that. While higher order n-grams
carry more structure of the language, they become increasingly
infrequent too and therefore the models don’t always get a
boost from it. Both the character level and word level n-gram
models show similar performance where they really excel
at certain languages (Czech, Slovak) and do poorly at other
(Bosnian, Croatian, Serbian).

B. Logistic Regression

We next tried a regularized logistic regression and here
too the character level n-gram performed a little better than
the word n-grams. Fig. 3 shows that the model was able to
completely ﬁt the training set but the performance on the
validation set plateaued close to 0.95. The best performance
was obtained by a character 9-gram model that includes all
n-grams up to n=9. These n-grams were truncated at
the
word boundaries, or in other words these n-grams did not
capture two or more consecutive words. Relaxing this criterion
signiﬁcantly increases the size of the term frequency matrix
and pushes the boundary of the computer memory but it does
improve the performance by a fraction of a percent.

C. Recurrent Neural Network

The MNB and LR approaches work really well in dis-
tinguishing two languages that have very little in common
because the set of n-grams will have very little overlap
between them. This approach does not work very well when
two languages are close to each other and share a lot of words
between them. Therefore, it becomes necessary to capture the
structure of a languages better to distinguish between similar
languages. We explored Recurrent Neural Networks (RNN)
for this purpose.

RNNs are a special kind of neural networks which possess
an internal state by virtue of a cycle in their hidden units. As

Language Identiﬁcation from Text Documents

Priyank Mathur
Adobe Systems

Stanford University

Arkajyoti Misra
Target Corporation
Stanford University

Stanford, California 94305

priyankm@stanford.edu

Stanford, California 94305

arkajyot@stanford.edu

Emrah Budur

Garanti Technology
Stanford University

Stanford, California 94305

emrah@stanford.edu

Abstract—The increase in the use of microblogging came along
with the rapid growth on short linguistic data. On the other
hand deep learning is considered to be the new frontier to
extract meaningful
information out of large amount of raw
data in an automated manner [1]. In this study, we engaged
these two emerging ﬁelds to come up with a robust language
identiﬁer on demand, namely Stanford Language Identiﬁcation
Engine (SLIDE). As a result, we achieved 95.12% accuracy in
Discriminating between Similar Languages (DSL) Shared Task
2015 dataset, beating the maximum reported accuracy achieved
so far [2].

Index Terms—Language identiﬁcation, LID, SLIDE.

I. INTRODUCTION

Automatic language detection is the ﬁrst step toward achiev-
ing a variety of tasks like detecting the source language for
machine translation, improving the search relevancy by per-
sonalizing the search results according to the query language
[3], providing uniform search box for a multilingual dictionary
[4], tagging data stream from Twitter with appropriate lan-
guage etc. While classifying languages belonging to disjoint
groups is not hard, disambiguation of languages originating
from the same source and dialects still pose a considerable
challenge in the area of natural language processing. Regular
classiﬁers based on word frequency only are inadequate in
making a correct prediction for such similar languages and
utilization of state of the art machine learning tools to capture
the structure of the language has become necessary to boost
the classiﬁer performance. In this work we took advantage
of recent advancement of deep neural network based models
showing stellar performance in many natural language pro-
cessing tasks to build a state of the art language classiﬁer.

We benchmarked our solution with the industry leaders and

achieved ﬁrst rank in the DSL test dataset.

II. PREVIOUS WORK

In the past, a variety of methods have been tried like
Naive Bayes [5], SVM [6], n-gram[7], graph-based n-gram[8],
prediction partial matching (PPM) [9], linear interpolation with
post independent weight optimization and majority voting for
combining multiple classiﬁers [10] etc. and the best accuracy
achieved are still in the lower ninety percents.

The researchers have worked on various critical tasks chal-
lenging the dimensions of the topic, including but not limited
to, supporting low resource languages, i.e. Nepali, Urdu, and
Icelandic [11], [12] handling user-generated unstructured short
texts, i.e. microblogs [11], [10] building a domain agnostic

engine [11], [8]. Existing benchmarking solutions approach
the LID problem in different ways where LogR [11] adopts a
discriminative approaches with regularized logistic regression,
TextCat and Google CLD[13] recruits N-gram-based algo-
rithm, langid.py [5] relies on a Naive Bayes classiﬁer with
a multinomial event model.

The outstanding results, of the time, suggested by Cavnar
and Trenkle became de facto standard of LID even today [7].
The signiﬁcant ingredient of their method is shown to use
a rank order statistic called ”out of place” distance measure
[14]. The problem in their approach is that they generated
n-grams out of words that requires tokenization. However,
many languages including Japanese and Chinese have no word
boundaries. Considering that Japanese is the second most
frequent language used in Twitter [12], there is a need for
better approach to scale the solution to all languages. As a
solution to their problem, Dunning came up with a better
approach with incorporating byte level n-grams of the whole
string instead of char level n-grams of the words [14].

After a rigorous literature survey, we found no prior study
that applied deep learning on language identiﬁcation of text.
On the other hand, there are a few number of studies that
applied deep learning to identify the language of speech [15],
[16], [17], [18]. We believe this study will be the ﬁrst in the
literature if published for LID in textual data by means of deep
learning.

III. DATASET DESCRIPTION

The data for this project work was obtained from ”Discrim-
inating between Similar Language (DSL) Shared Task 2015”
[19]. A set of 20000 instances per language (18000 training
(train.txt) and 2000 evaluation (test.txt)) was provided for 13
different world languages. The dataset also consisted of a
subset (devel.txt) of the overall training data which we utilized
for hyper-parameter tuning. The languages are grouped as
shown in Table I. The names of the groups will be frequently
referred in the subsequent sections.

Each entry in the dataset is a full sentence extracted from
journalistic corpora and written in one of the languages and
tagged with the language group and country of origin. A
similar set of mixed language instance was also provided to
add noise to the data. A separate gold test data was provided
for the ﬁnal evaluation (test-gold.txt).

We applied t-SNE algorithm to visualize the instances
in 3D euclidean space [20], [21]. For feature extraction,

Group Name

Language Name

Language Code

South Eastern Slavic

South Western Slavic

West-Slavic

Ibero-Romance (Spanish)

Ibero-Romance (Portuguese)

Astronesian

Bulgarian
Macedonian
Bosnian
Croatian
Serbian
Czech
Slovak
Peninsular Spain
Argentinian Spanish
Brazilian Portuguese
European Portuguese
Indonesian
Malay

bg
mk
bs
hr
sr
cz
sk
es-ES
es-AR
pt-BR
pt-PT
id
my

TABLE I: Benchmark results of available solutions

(a) Easily separable
t-SNE visualization of

Fig. 1:
plots
http://SeeYourLanguage.info

including 3D animated plot

(b) Difﬁcult to separate

language groups. More
:

available

are

at

we vectorized each sentence over 1 to 5-grams of the to-
kens delimited by white space characters. Fig. 1 shows
the resulting plot. As can be seen on the plot,
the lan-
guages in the same group overlap a lot while the languages
in different groups can be linearly separable. A 3 dimen-
sional visualization of all the languages can be viewed at
https://www.youtube.com/watch?v=mhRdfC26q78.

A. Multinomial Naive Bayes

IV. METHODS

We created a baseline result by training a Multinomial
Naive Bayes model because it is quick to prototype, runs
fast and known to provide decent results in the ﬁeld of text
processing. We have done no pre-processing of the text com-
monly done in the ﬁeld like stemming or stop word removal
because we believe that could potentially remove important
signatures of a particular language, particularly when the same
language is spoken by two geographically disconnected group
of people (e.g Portuguese spoken in Portugal and Brazil).
We experimented with both word and character n-grams. The
character n-grams turned out to be particularly useful when
differentiating between two languages using mostly distinct
character sequences in their alphabet.

The character level n-gram behaves quite differently from
that of word level n-grams as shown in Fig. 2. Single char-

Fig. 2: Naive Bayes performance as a function of n for both
word and character n-grams.

acters carry little information and therefore the performance
for character n-gram improves quite sharply as the number
of characters is increased before saturating at about n=8. We
experimented with character n-grams both restricted at word
boundaries and spanning across word boundaries. The latter
has a marginal performance boost at the cost of longer training
time and memory pressure. The word n-gram model peaks
at n=2 and drops beyond that. While higher order n-grams
carry more structure of the language, they become increasingly
infrequent too and therefore the models don’t always get a
boost from it. Both the character level and word level n-gram
models show similar performance where they really excel
at certain languages (Czech, Slovak) and do poorly at other
(Bosnian, Croatian, Serbian).

B. Logistic Regression

We next tried a regularized logistic regression and here
too the character level n-gram performed a little better than
the word n-grams. Fig. 3 shows that the model was able to
completely ﬁt the training set but the performance on the
validation set plateaued close to 0.95. The best performance
was obtained by a character 9-gram model that includes all
n-grams up to n=9. These n-grams were truncated at
the
word boundaries, or in other words these n-grams did not
capture two or more consecutive words. Relaxing this criterion
signiﬁcantly increases the size of the term frequency matrix
and pushes the boundary of the computer memory but it does
improve the performance by a fraction of a percent.

C. Recurrent Neural Network

The MNB and LR approaches work really well in dis-
tinguishing two languages that have very little in common
because the set of n-grams will have very little overlap
between them. This approach does not work very well when
two languages are close to each other and share a lot of words
between them. Therefore, it becomes necessary to capture the
structure of a languages better to distinguish between similar
languages. We explored Recurrent Neural Networks (RNN)
for this purpose.

RNNs are a special kind of neural networks which possess
an internal state by virtue of a cycle in their hidden units. As

of the process, we varied a single parameter while keeping
the other two constant. The plots below (ﬁg. 4) show the
performance of the resultant models on the validation dataset
as each parameter was changed.

Fig. 5: Variation of the accuracy on validation dataset as we
vary training epochs, number of hidden units and drop off

Fig. 6: Grid search over the best parameter values found in
the previous step

As we can see in the plot above in Fig. 4, increasing the
number of training epochs improves the model performance
up to a certain stage, after which it plateaus. Hence, for the
next stage of tuning, we ﬁxed the number of training epochs
to 20. Using the best values for the number of hidden units
and dropout found above, we performed grid search over
all combinations of these parameters. The result of the grid
search is visualized in Fig. 5. The (number of hidden units,
dropout) combinations (1280, 0.4) and (768, 0.45) gave us
the best performance on the validation set. The ﬁnal values
chosen for further experimentation were 768 hidden units and
0.45 dropout so as to avoid overﬁtting.

Training procedure: Our ﬁnal model is an ensemble of 5
RNNs, each built using a different feature set, namely, from
character 2-grams to character 5-grams and word unigrams. To
train our models, we divided our entire training data (train.txt)
into 90% training set and 10% validation set. Once trained, we
measured the performance of each model individually on the
validation set and is reported in Table II.

As seen in Fig. 6, to construct the ensemble, instead of
manually assigning weights to each model, we constructed a
Logistic Regression model to get the ﬁnal output. The features
for this LR model were the outputs from the 5 RNNs created
earlier and it was tuned using 5 fold cross validation over the
10% validation dataset.

For training the RNNs, we used a Python library called
passage [27], which is built on top of Theano. Although the

Fig. 3: The LR model was able to completely ﬁt the training
data but the accuracy on validation data peaked at about 94%
overall.

such, RNNs are able to record temporal dependencies among
the input sequence, as opposed to most other machine learning
algorithms where the inputs are considered independent of
each other. Hence, they are very well suited to natural lan-
guage processing tasks and have been successfully used for
applications like speech recognition, hand writing recognition
etc.

Fig. 4: Visualization of an un-rolled recurrent neural network
[22]

Until recently, RNNs were considered very difﬁcult

to
train because of the problem of exploding or vanishing
gradients[23] which makes it very difﬁcult for them to learn
long sequences of input. Few methods like gradient clipping
have been proposed to remedy this. Recent architectures like
Long Short Term Memory (LSTM) [24] and Gated Recurrent
Unit (GRU) [25] were also speciﬁcally designed to get around
this problem. In our experiments, we used single hidden
layer recurrent neural networks that used gated recurrent units.

Hyper-parameter tuning: In our single layer networks, we

had three model hyper parameters to search over

1) Epochs - the number of iterations over training data.

We generally try to train until the network saturates.

2) Hidden layer size - Number of hidden units in the

hidden layer.

3) Dropout - Deep neural networks with large number
of parameters are very powerful machines but are
extremely susceptible to overﬁtting. Dropout provides
a simple way to remedy this problem by randomly
dropping hidden units as each example propagates
through the network and back [26].

We used a subset of our overall training data (devel.txt) for
hyper parameter selection. This subset was further divided into
75% training data and 25% validation data. In the ﬁrst step

Language Identiﬁcation from Text Documents

Priyank Mathur
Adobe Systems

Stanford University

Arkajyoti Misra
Target Corporation
Stanford University

Stanford, California 94305

priyankm@stanford.edu

Stanford, California 94305

arkajyot@stanford.edu

Emrah Budur

Garanti Technology
Stanford University

Stanford, California 94305

emrah@stanford.edu

Abstract—The increase in the use of microblogging came along
with the rapid growth on short linguistic data. On the other
hand deep learning is considered to be the new frontier to
extract meaningful
information out of large amount of raw
data in an automated manner [1]. In this study, we engaged
these two emerging ﬁelds to come up with a robust language
identiﬁer on demand, namely Stanford Language Identiﬁcation
Engine (SLIDE). As a result, we achieved 95.12% accuracy in
Discriminating between Similar Languages (DSL) Shared Task
2015 dataset, beating the maximum reported accuracy achieved
so far [2].

Index Terms—Language identiﬁcation, LID, SLIDE.

I. INTRODUCTION

Automatic language detection is the ﬁrst step toward achiev-
ing a variety of tasks like detecting the source language for
machine translation, improving the search relevancy by per-
sonalizing the search results according to the query language
[3], providing uniform search box for a multilingual dictionary
[4], tagging data stream from Twitter with appropriate lan-
guage etc. While classifying languages belonging to disjoint
groups is not hard, disambiguation of languages originating
from the same source and dialects still pose a considerable
challenge in the area of natural language processing. Regular
classiﬁers based on word frequency only are inadequate in
making a correct prediction for such similar languages and
utilization of state of the art machine learning tools to capture
the structure of the language has become necessary to boost
the classiﬁer performance. In this work we took advantage
of recent advancement of deep neural network based models
showing stellar performance in many natural language pro-
cessing tasks to build a state of the art language classiﬁer.

We benchmarked our solution with the industry leaders and

achieved ﬁrst rank in the DSL test dataset.

II. PREVIOUS WORK

In the past, a variety of methods have been tried like
Naive Bayes [5], SVM [6], n-gram[7], graph-based n-gram[8],
prediction partial matching (PPM) [9], linear interpolation with
post independent weight optimization and majority voting for
combining multiple classiﬁers [10] etc. and the best accuracy
achieved are still in the lower ninety percents.

The researchers have worked on various critical tasks chal-
lenging the dimensions of the topic, including but not limited
to, supporting low resource languages, i.e. Nepali, Urdu, and
Icelandic [11], [12] handling user-generated unstructured short
texts, i.e. microblogs [11], [10] building a domain agnostic

engine [11], [8]. Existing benchmarking solutions approach
the LID problem in different ways where LogR [11] adopts a
discriminative approaches with regularized logistic regression,
TextCat and Google CLD[13] recruits N-gram-based algo-
rithm, langid.py [5] relies on a Naive Bayes classiﬁer with
a multinomial event model.

The outstanding results, of the time, suggested by Cavnar
and Trenkle became de facto standard of LID even today [7].
The signiﬁcant ingredient of their method is shown to use
a rank order statistic called ”out of place” distance measure
[14]. The problem in their approach is that they generated
n-grams out of words that requires tokenization. However,
many languages including Japanese and Chinese have no word
boundaries. Considering that Japanese is the second most
frequent language used in Twitter [12], there is a need for
better approach to scale the solution to all languages. As a
solution to their problem, Dunning came up with a better
approach with incorporating byte level n-grams of the whole
string instead of char level n-grams of the words [14].

After a rigorous literature survey, we found no prior study
that applied deep learning on language identiﬁcation of text.
On the other hand, there are a few number of studies that
applied deep learning to identify the language of speech [15],
[16], [17], [18]. We believe this study will be the ﬁrst in the
literature if published for LID in textual data by means of deep
learning.

III. DATASET DESCRIPTION

The data for this project work was obtained from ”Discrim-
inating between Similar Language (DSL) Shared Task 2015”
[19]. A set of 20000 instances per language (18000 training
(train.txt) and 2000 evaluation (test.txt)) was provided for 13
different world languages. The dataset also consisted of a
subset (devel.txt) of the overall training data which we utilized
for hyper-parameter tuning. The languages are grouped as
shown in Table I. The names of the groups will be frequently
referred in the subsequent sections.

Each entry in the dataset is a full sentence extracted from
journalistic corpora and written in one of the languages and
tagged with the language group and country of origin. A
similar set of mixed language instance was also provided to
add noise to the data. A separate gold test data was provided
for the ﬁnal evaluation (test-gold.txt).

We applied t-SNE algorithm to visualize the instances
in 3D euclidean space [20], [21]. For feature extraction,

Group Name

Language Name

Language Code

South Eastern Slavic

South Western Slavic

West-Slavic

Ibero-Romance (Spanish)

Ibero-Romance (Portuguese)

Astronesian

Bulgarian
Macedonian
Bosnian
Croatian
Serbian
Czech
Slovak
Peninsular Spain
Argentinian Spanish
Brazilian Portuguese
European Portuguese
Indonesian
Malay

bg
mk
bs
hr
sr
cz
sk
es-ES
es-AR
pt-BR
pt-PT
id
my

TABLE I: Benchmark results of available solutions

(a) Easily separable
t-SNE visualization of

Fig. 1:
plots
http://SeeYourLanguage.info

including 3D animated plot

(b) Difﬁcult to separate

language groups. More
:

available

are

at

we vectorized each sentence over 1 to 5-grams of the to-
kens delimited by white space characters. Fig. 1 shows
the resulting plot. As can be seen on the plot,
the lan-
guages in the same group overlap a lot while the languages
in different groups can be linearly separable. A 3 dimen-
sional visualization of all the languages can be viewed at
https://www.youtube.com/watch?v=mhRdfC26q78.

A. Multinomial Naive Bayes

IV. METHODS

We created a baseline result by training a Multinomial
Naive Bayes model because it is quick to prototype, runs
fast and known to provide decent results in the ﬁeld of text
processing. We have done no pre-processing of the text com-
monly done in the ﬁeld like stemming or stop word removal
because we believe that could potentially remove important
signatures of a particular language, particularly when the same
language is spoken by two geographically disconnected group
of people (e.g Portuguese spoken in Portugal and Brazil).
We experimented with both word and character n-grams. The
character n-grams turned out to be particularly useful when
differentiating between two languages using mostly distinct
character sequences in their alphabet.

The character level n-gram behaves quite differently from
that of word level n-grams as shown in Fig. 2. Single char-

Fig. 2: Naive Bayes performance as a function of n for both
word and character n-grams.

acters carry little information and therefore the performance
for character n-gram improves quite sharply as the number
of characters is increased before saturating at about n=8. We
experimented with character n-grams both restricted at word
boundaries and spanning across word boundaries. The latter
has a marginal performance boost at the cost of longer training
time and memory pressure. The word n-gram model peaks
at n=2 and drops beyond that. While higher order n-grams
carry more structure of the language, they become increasingly
infrequent too and therefore the models don’t always get a
boost from it. Both the character level and word level n-gram
models show similar performance where they really excel
at certain languages (Czech, Slovak) and do poorly at other
(Bosnian, Croatian, Serbian).

B. Logistic Regression

We next tried a regularized logistic regression and here
too the character level n-gram performed a little better than
the word n-grams. Fig. 3 shows that the model was able to
completely ﬁt the training set but the performance on the
validation set plateaued close to 0.95. The best performance
was obtained by a character 9-gram model that includes all
n-grams up to n=9. These n-grams were truncated at
the
word boundaries, or in other words these n-grams did not
capture two or more consecutive words. Relaxing this criterion
signiﬁcantly increases the size of the term frequency matrix
and pushes the boundary of the computer memory but it does
improve the performance by a fraction of a percent.

C. Recurrent Neural Network

The MNB and LR approaches work really well in dis-
tinguishing two languages that have very little in common
because the set of n-grams will have very little overlap
between them. This approach does not work very well when
two languages are close to each other and share a lot of words
between them. Therefore, it becomes necessary to capture the
structure of a languages better to distinguish between similar
languages. We explored Recurrent Neural Networks (RNN)
for this purpose.

RNNs are a special kind of neural networks which possess
an internal state by virtue of a cycle in their hidden units. As

of the process, we varied a single parameter while keeping
the other two constant. The plots below (ﬁg. 4) show the
performance of the resultant models on the validation dataset
as each parameter was changed.

Fig. 5: Variation of the accuracy on validation dataset as we
vary training epochs, number of hidden units and drop off

Fig. 6: Grid search over the best parameter values found in
the previous step

As we can see in the plot above in Fig. 4, increasing the
number of training epochs improves the model performance
up to a certain stage, after which it plateaus. Hence, for the
next stage of tuning, we ﬁxed the number of training epochs
to 20. Using the best values for the number of hidden units
and dropout found above, we performed grid search over
all combinations of these parameters. The result of the grid
search is visualized in Fig. 5. The (number of hidden units,
dropout) combinations (1280, 0.4) and (768, 0.45) gave us
the best performance on the validation set. The ﬁnal values
chosen for further experimentation were 768 hidden units and
0.45 dropout so as to avoid overﬁtting.

Training procedure: Our ﬁnal model is an ensemble of 5
RNNs, each built using a different feature set, namely, from
character 2-grams to character 5-grams and word unigrams. To
train our models, we divided our entire training data (train.txt)
into 90% training set and 10% validation set. Once trained, we
measured the performance of each model individually on the
validation set and is reported in Table II.

As seen in Fig. 6, to construct the ensemble, instead of
manually assigning weights to each model, we constructed a
Logistic Regression model to get the ﬁnal output. The features
for this LR model were the outputs from the 5 RNNs created
earlier and it was tuned using 5 fold cross validation over the
10% validation dataset.

For training the RNNs, we used a Python library called
passage [27], which is built on top of Theano. Although the

Fig. 3: The LR model was able to completely ﬁt the training
data but the accuracy on validation data peaked at about 94%
overall.

such, RNNs are able to record temporal dependencies among
the input sequence, as opposed to most other machine learning
algorithms where the inputs are considered independent of
each other. Hence, they are very well suited to natural lan-
guage processing tasks and have been successfully used for
applications like speech recognition, hand writing recognition
etc.

Fig. 4: Visualization of an un-rolled recurrent neural network
[22]

Until recently, RNNs were considered very difﬁcult

to
train because of the problem of exploding or vanishing
gradients[23] which makes it very difﬁcult for them to learn
long sequences of input. Few methods like gradient clipping
have been proposed to remedy this. Recent architectures like
Long Short Term Memory (LSTM) [24] and Gated Recurrent
Unit (GRU) [25] were also speciﬁcally designed to get around
this problem. In our experiments, we used single hidden
layer recurrent neural networks that used gated recurrent units.

Hyper-parameter tuning: In our single layer networks, we

had three model hyper parameters to search over

1) Epochs - the number of iterations over training data.

We generally try to train until the network saturates.

2) Hidden layer size - Number of hidden units in the

hidden layer.

3) Dropout - Deep neural networks with large number
of parameters are very powerful machines but are
extremely susceptible to overﬁtting. Dropout provides
a simple way to remedy this problem by randomly
dropping hidden units as each example propagates
through the network and back [26].

We used a subset of our overall training data (devel.txt) for
hyper parameter selection. This subset was further divided into
75% training data and 25% validation data. In the ﬁrst step

Model

MNB (char 9-gram)
LR (char 9-gram)
RNN (char 2-gram)
RNN (char 3-gram)
RNN (char 4-gram)
RNN (char 5-gram)
RNN (word uni-gram)
Ensemble of RNN model (SLIDE)

Accuracy

Validation Set

Test Set

0.9479
0.9486
0.9200
0.9328
0.9377
0.9347
0.9351
0.9533

0.9452
0.9449
0.9213
0.9338
0.9347
0.9316
0.9330
0.9512

TABLE II: Performance comparison of various models

Fig. 8: Confusion matrix

results that the biggest challenge consistently posed to our
classiﬁers is distinguishing the languages in South Western
Slavic group (bs, hr, cr). The training set revealed that among
all the words in bs, 48% are common to hr and 41% to sr.
Since Fig. 3 clearly showed we didn’t underﬁt the training
set, it made sense to augment the training data in these three
language categories. We incorporated a signiﬁcantly larger
labeled data for two of these languages and also downloaded
newspaper articles in bs, but the classiﬁcation accuracy in this
language group did not improve. Looking closer to some of
these external datasets revealed that none of the new words
could be uniquely associated to any of the three languages
and therefore, the additional data probably added more noise
than signal.

To understand the failure mechanism of the classiﬁer for
the South Western Slavic language group, we fed the LR
classiﬁer, which is the best of single models in validation set
according to Table II, different fractions of a document it failed
to classify correctly. For example, the following document
is in Bosnian(bs) but the classiﬁer predicts its language as
Serbian(sr): Usto se osvrnuo na ekonomsku situaciju u kojoj

Fig. 7: Training procedure for the RNN ensemble

library provides several tools for text pre-processing including
tokenization, it lacked the ability to generate character n-gram
level features. Therefore, we had to extend the library with
custom character level feature generators. In addition, training
neural networks on CPUs consumes a lot of time. Hence,
for our experiments, we leveraged AWS GPU (g2.2xlarge)
instances that provided a 10x boost in time required to train
one model.

V. RESULTS

Table II shows a comparison of the models we have ex-
perimented with. One surprising feature of the result is that
individual RNN models were not able to beat the performance
of the MNB and LR models, even though the latter models
have minimal knowledge of a language structure. However,
when we created an ensemble of RNN models, it turned out
to be the best model and crossed the 95% threshold for the ﬁrst
time. It should be noted that for a particular n-gram model,
MNB and LR models use all m-grams where 1 ≤ m ≤ n.
However, due to the very nature of an RNN architecture, a
combination of n-grams cannot be used because that will lead
to an overlapping sequence of content to be fed to the network.
Since any given n-gram captures only limited information
about a language, it was natural to try an ensemble of n-gram
RNN models with different values of n, so that structure of
the language can be captured at multiple different levels.

The boost in performance due to ensemble can also be
attributed to model combination, which aims to achieve at least
as good of a performance as the worst model in the ensemble.
This is because individual models can make mistakes on
different examples, and therefore, by using an ensemble we
are able to reduce this variance. While we tried other model
combination strategies like median and manual weighting,
building a Logistic Regression classiﬁer on top of RNNs really
helped us ﬁnd the optimal weight that should be given to each
individual model. We could not include RNN models beyond
character 5-gram in the ensemble because of memory limita-
tion and including the MNB or LR model in the ensemble did
not improve the performance of the model.

VI. DISCUSSION

The ﬁnal classiﬁcation for each language group is captured
in the confusion matrix in Fig. 8. It is quite evident from our

Language Identiﬁcation from Text Documents

Priyank Mathur
Adobe Systems

Stanford University

Arkajyoti Misra
Target Corporation
Stanford University

Stanford, California 94305

priyankm@stanford.edu

Stanford, California 94305

arkajyot@stanford.edu

Emrah Budur

Garanti Technology
Stanford University

Stanford, California 94305

emrah@stanford.edu

Abstract—The increase in the use of microblogging came along
with the rapid growth on short linguistic data. On the other
hand deep learning is considered to be the new frontier to
extract meaningful
information out of large amount of raw
data in an automated manner [1]. In this study, we engaged
these two emerging ﬁelds to come up with a robust language
identiﬁer on demand, namely Stanford Language Identiﬁcation
Engine (SLIDE). As a result, we achieved 95.12% accuracy in
Discriminating between Similar Languages (DSL) Shared Task
2015 dataset, beating the maximum reported accuracy achieved
so far [2].

Index Terms—Language identiﬁcation, LID, SLIDE.

I. INTRODUCTION

Automatic language detection is the ﬁrst step toward achiev-
ing a variety of tasks like detecting the source language for
machine translation, improving the search relevancy by per-
sonalizing the search results according to the query language
[3], providing uniform search box for a multilingual dictionary
[4], tagging data stream from Twitter with appropriate lan-
guage etc. While classifying languages belonging to disjoint
groups is not hard, disambiguation of languages originating
from the same source and dialects still pose a considerable
challenge in the area of natural language processing. Regular
classiﬁers based on word frequency only are inadequate in
making a correct prediction for such similar languages and
utilization of state of the art machine learning tools to capture
the structure of the language has become necessary to boost
the classiﬁer performance. In this work we took advantage
of recent advancement of deep neural network based models
showing stellar performance in many natural language pro-
cessing tasks to build a state of the art language classiﬁer.

We benchmarked our solution with the industry leaders and

achieved ﬁrst rank in the DSL test dataset.

II. PREVIOUS WORK

In the past, a variety of methods have been tried like
Naive Bayes [5], SVM [6], n-gram[7], graph-based n-gram[8],
prediction partial matching (PPM) [9], linear interpolation with
post independent weight optimization and majority voting for
combining multiple classiﬁers [10] etc. and the best accuracy
achieved are still in the lower ninety percents.

The researchers have worked on various critical tasks chal-
lenging the dimensions of the topic, including but not limited
to, supporting low resource languages, i.e. Nepali, Urdu, and
Icelandic [11], [12] handling user-generated unstructured short
texts, i.e. microblogs [11], [10] building a domain agnostic

engine [11], [8]. Existing benchmarking solutions approach
the LID problem in different ways where LogR [11] adopts a
discriminative approaches with regularized logistic regression,
TextCat and Google CLD[13] recruits N-gram-based algo-
rithm, langid.py [5] relies on a Naive Bayes classiﬁer with
a multinomial event model.

The outstanding results, of the time, suggested by Cavnar
and Trenkle became de facto standard of LID even today [7].
The signiﬁcant ingredient of their method is shown to use
a rank order statistic called ”out of place” distance measure
[14]. The problem in their approach is that they generated
n-grams out of words that requires tokenization. However,
many languages including Japanese and Chinese have no word
boundaries. Considering that Japanese is the second most
frequent language used in Twitter [12], there is a need for
better approach to scale the solution to all languages. As a
solution to their problem, Dunning came up with a better
approach with incorporating byte level n-grams of the whole
string instead of char level n-grams of the words [14].

After a rigorous literature survey, we found no prior study
that applied deep learning on language identiﬁcation of text.
On the other hand, there are a few number of studies that
applied deep learning to identify the language of speech [15],
[16], [17], [18]. We believe this study will be the ﬁrst in the
literature if published for LID in textual data by means of deep
learning.

III. DATASET DESCRIPTION

The data for this project work was obtained from ”Discrim-
inating between Similar Language (DSL) Shared Task 2015”
[19]. A set of 20000 instances per language (18000 training
(train.txt) and 2000 evaluation (test.txt)) was provided for 13
different world languages. The dataset also consisted of a
subset (devel.txt) of the overall training data which we utilized
for hyper-parameter tuning. The languages are grouped as
shown in Table I. The names of the groups will be frequently
referred in the subsequent sections.

Each entry in the dataset is a full sentence extracted from
journalistic corpora and written in one of the languages and
tagged with the language group and country of origin. A
similar set of mixed language instance was also provided to
add noise to the data. A separate gold test data was provided
for the ﬁnal evaluation (test-gold.txt).

We applied t-SNE algorithm to visualize the instances
in 3D euclidean space [20], [21]. For feature extraction,

Group Name

Language Name

Language Code

South Eastern Slavic

South Western Slavic

West-Slavic

Ibero-Romance (Spanish)

Ibero-Romance (Portuguese)

Astronesian

Bulgarian
Macedonian
Bosnian
Croatian
Serbian
Czech
Slovak
Peninsular Spain
Argentinian Spanish
Brazilian Portuguese
European Portuguese
Indonesian
Malay

bg
mk
bs
hr
sr
cz
sk
es-ES
es-AR
pt-BR
pt-PT
id
my

TABLE I: Benchmark results of available solutions

(a) Easily separable
t-SNE visualization of

Fig. 1:
plots
http://SeeYourLanguage.info

including 3D animated plot

(b) Difﬁcult to separate

language groups. More
:

available

are

at

we vectorized each sentence over 1 to 5-grams of the to-
kens delimited by white space characters. Fig. 1 shows
the resulting plot. As can be seen on the plot,
the lan-
guages in the same group overlap a lot while the languages
in different groups can be linearly separable. A 3 dimen-
sional visualization of all the languages can be viewed at
https://www.youtube.com/watch?v=mhRdfC26q78.

A. Multinomial Naive Bayes

IV. METHODS

We created a baseline result by training a Multinomial
Naive Bayes model because it is quick to prototype, runs
fast and known to provide decent results in the ﬁeld of text
processing. We have done no pre-processing of the text com-
monly done in the ﬁeld like stemming or stop word removal
because we believe that could potentially remove important
signatures of a particular language, particularly when the same
language is spoken by two geographically disconnected group
of people (e.g Portuguese spoken in Portugal and Brazil).
We experimented with both word and character n-grams. The
character n-grams turned out to be particularly useful when
differentiating between two languages using mostly distinct
character sequences in their alphabet.

The character level n-gram behaves quite differently from
that of word level n-grams as shown in Fig. 2. Single char-

Fig. 2: Naive Bayes performance as a function of n for both
word and character n-grams.

acters carry little information and therefore the performance
for character n-gram improves quite sharply as the number
of characters is increased before saturating at about n=8. We
experimented with character n-grams both restricted at word
boundaries and spanning across word boundaries. The latter
has a marginal performance boost at the cost of longer training
time and memory pressure. The word n-gram model peaks
at n=2 and drops beyond that. While higher order n-grams
carry more structure of the language, they become increasingly
infrequent too and therefore the models don’t always get a
boost from it. Both the character level and word level n-gram
models show similar performance where they really excel
at certain languages (Czech, Slovak) and do poorly at other
(Bosnian, Croatian, Serbian).

B. Logistic Regression

We next tried a regularized logistic regression and here
too the character level n-gram performed a little better than
the word n-grams. Fig. 3 shows that the model was able to
completely ﬁt the training set but the performance on the
validation set plateaued close to 0.95. The best performance
was obtained by a character 9-gram model that includes all
n-grams up to n=9. These n-grams were truncated at
the
word boundaries, or in other words these n-grams did not
capture two or more consecutive words. Relaxing this criterion
signiﬁcantly increases the size of the term frequency matrix
and pushes the boundary of the computer memory but it does
improve the performance by a fraction of a percent.

C. Recurrent Neural Network

The MNB and LR approaches work really well in dis-
tinguishing two languages that have very little in common
because the set of n-grams will have very little overlap
between them. This approach does not work very well when
two languages are close to each other and share a lot of words
between them. Therefore, it becomes necessary to capture the
structure of a languages better to distinguish between similar
languages. We explored Recurrent Neural Networks (RNN)
for this purpose.

RNNs are a special kind of neural networks which possess
an internal state by virtue of a cycle in their hidden units. As

of the process, we varied a single parameter while keeping
the other two constant. The plots below (ﬁg. 4) show the
performance of the resultant models on the validation dataset
as each parameter was changed.

Fig. 5: Variation of the accuracy on validation dataset as we
vary training epochs, number of hidden units and drop off

Fig. 6: Grid search over the best parameter values found in
the previous step

As we can see in the plot above in Fig. 4, increasing the
number of training epochs improves the model performance
up to a certain stage, after which it plateaus. Hence, for the
next stage of tuning, we ﬁxed the number of training epochs
to 20. Using the best values for the number of hidden units
and dropout found above, we performed grid search over
all combinations of these parameters. The result of the grid
search is visualized in Fig. 5. The (number of hidden units,
dropout) combinations (1280, 0.4) and (768, 0.45) gave us
the best performance on the validation set. The ﬁnal values
chosen for further experimentation were 768 hidden units and
0.45 dropout so as to avoid overﬁtting.

Training procedure: Our ﬁnal model is an ensemble of 5
RNNs, each built using a different feature set, namely, from
character 2-grams to character 5-grams and word unigrams. To
train our models, we divided our entire training data (train.txt)
into 90% training set and 10% validation set. Once trained, we
measured the performance of each model individually on the
validation set and is reported in Table II.

As seen in Fig. 6, to construct the ensemble, instead of
manually assigning weights to each model, we constructed a
Logistic Regression model to get the ﬁnal output. The features
for this LR model were the outputs from the 5 RNNs created
earlier and it was tuned using 5 fold cross validation over the
10% validation dataset.

For training the RNNs, we used a Python library called
passage [27], which is built on top of Theano. Although the

Fig. 3: The LR model was able to completely ﬁt the training
data but the accuracy on validation data peaked at about 94%
overall.

such, RNNs are able to record temporal dependencies among
the input sequence, as opposed to most other machine learning
algorithms where the inputs are considered independent of
each other. Hence, they are very well suited to natural lan-
guage processing tasks and have been successfully used for
applications like speech recognition, hand writing recognition
etc.

Fig. 4: Visualization of an un-rolled recurrent neural network
[22]

Until recently, RNNs were considered very difﬁcult

to
train because of the problem of exploding or vanishing
gradients[23] which makes it very difﬁcult for them to learn
long sequences of input. Few methods like gradient clipping
have been proposed to remedy this. Recent architectures like
Long Short Term Memory (LSTM) [24] and Gated Recurrent
Unit (GRU) [25] were also speciﬁcally designed to get around
this problem. In our experiments, we used single hidden
layer recurrent neural networks that used gated recurrent units.

Hyper-parameter tuning: In our single layer networks, we

had three model hyper parameters to search over

1) Epochs - the number of iterations over training data.

We generally try to train until the network saturates.

2) Hidden layer size - Number of hidden units in the

hidden layer.

3) Dropout - Deep neural networks with large number
of parameters are very powerful machines but are
extremely susceptible to overﬁtting. Dropout provides
a simple way to remedy this problem by randomly
dropping hidden units as each example propagates
through the network and back [26].

We used a subset of our overall training data (devel.txt) for
hyper parameter selection. This subset was further divided into
75% training data and 25% validation data. In the ﬁrst step

Model

MNB (char 9-gram)
LR (char 9-gram)
RNN (char 2-gram)
RNN (char 3-gram)
RNN (char 4-gram)
RNN (char 5-gram)
RNN (word uni-gram)
Ensemble of RNN model (SLIDE)

Accuracy

Validation Set

Test Set

0.9479
0.9486
0.9200
0.9328
0.9377
0.9347
0.9351
0.9533

0.9452
0.9449
0.9213
0.9338
0.9347
0.9316
0.9330
0.9512

TABLE II: Performance comparison of various models

Fig. 8: Confusion matrix

results that the biggest challenge consistently posed to our
classiﬁers is distinguishing the languages in South Western
Slavic group (bs, hr, cr). The training set revealed that among
all the words in bs, 48% are common to hr and 41% to sr.
Since Fig. 3 clearly showed we didn’t underﬁt the training
set, it made sense to augment the training data in these three
language categories. We incorporated a signiﬁcantly larger
labeled data for two of these languages and also downloaded
newspaper articles in bs, but the classiﬁcation accuracy in this
language group did not improve. Looking closer to some of
these external datasets revealed that none of the new words
could be uniquely associated to any of the three languages
and therefore, the additional data probably added more noise
than signal.

To understand the failure mechanism of the classiﬁer for
the South Western Slavic language group, we fed the LR
classiﬁer, which is the best of single models in validation set
according to Table II, different fractions of a document it failed
to classify correctly. For example, the following document
is in Bosnian(bs) but the classiﬁer predicts its language as
Serbian(sr): Usto se osvrnuo na ekonomsku situaciju u kojoj

Fig. 7: Training procedure for the RNN ensemble

library provides several tools for text pre-processing including
tokenization, it lacked the ability to generate character n-gram
level features. Therefore, we had to extend the library with
custom character level feature generators. In addition, training
neural networks on CPUs consumes a lot of time. Hence,
for our experiments, we leveraged AWS GPU (g2.2xlarge)
instances that provided a 10x boost in time required to train
one model.

V. RESULTS

Table II shows a comparison of the models we have ex-
perimented with. One surprising feature of the result is that
individual RNN models were not able to beat the performance
of the MNB and LR models, even though the latter models
have minimal knowledge of a language structure. However,
when we created an ensemble of RNN models, it turned out
to be the best model and crossed the 95% threshold for the ﬁrst
time. It should be noted that for a particular n-gram model,
MNB and LR models use all m-grams where 1 ≤ m ≤ n.
However, due to the very nature of an RNN architecture, a
combination of n-grams cannot be used because that will lead
to an overlapping sequence of content to be fed to the network.
Since any given n-gram captures only limited information
about a language, it was natural to try an ensemble of n-gram
RNN models with different values of n, so that structure of
the language can be captured at multiple different levels.

The boost in performance due to ensemble can also be
attributed to model combination, which aims to achieve at least
as good of a performance as the worst model in the ensemble.
This is because individual models can make mistakes on
different examples, and therefore, by using an ensemble we
are able to reduce this variance. While we tried other model
combination strategies like median and manual weighting,
building a Logistic Regression classiﬁer on top of RNNs really
helped us ﬁnd the optimal weight that should be given to each
individual model. We could not include RNN models beyond
character 5-gram in the ensemble because of memory limita-
tion and including the MNB or LR model in the ensemble did
not improve the performance of the model.

VI. DISCUSSION

The ﬁnal classiﬁcation for each language group is captured
in the confusion matrix in Fig. 8. It is quite evident from our

part of the corpus. We have relied on the ensemble of RNN
models to discover the structure unique to a speciﬁc language
but we could not engineer any additional feature due to lack
of knowledge in those speciﬁc languages. At this point, we
think, further improvement can only be achieved by designing
rule based features by talking to language experts or native
speakers.

ACKNOWLEDGMENT

We would like to thank David Jurgens of Department
of Computer Science, Stanford University for helping with
idea, dataset and previous research, Junjie Qin
the initial
for his mentoring and insightful comments that polished the
outcome of the study, AWS Educate Program for providing
EC2 credits and computing resources and Microsoft Azure
for Research Program for providing Azure credits and full
featured computing resources, and lastly Google, Yandex and
Basis Tech for providing free access to their language detection
APIs for our benchmarking analysis [28] [29] [30].

COMPARISON WITH OTHER SYSTEMS

We assessed the performance of SLIDE by comparing its
result with the domain leaders in an unfair test described
below.
We queried the test ﬁle of dataset of DSL Shared Task 2015
and accepted the resulting predictions even if

• the dialect of the language is not distinguished in Ibero-
Romance language group due to lack of support, i.e.
Google always predicts Portuguese for sentences both in
Brazilian Portuguese and European Portuguese.

• a certain language is not supported at all, i.e. Rosette

doesn’t support Bosnian.

Table III shows the resulting accuracies. Although SLIDE
it

had lack of competitive advantage in this unfair test,
surpassed the industry leaders in terms accuracy.

Solution
SLIDE
Google Translate API
Rosette Language API
langid.py
Yandex Translator API

Accuracy

95%
89%
86%
80%
79%

TABLE III: Benchmark results in non-increasing order of
accuracy

Fig. 9: Scenarios where the LR classiﬁer incorrectly predicted
the target language. (Detailed description in Section VI)

je veliki broj novinara u potrazi za poslom, na mizerne plae i
guranje etike strane profesije u zapeak. So we fed the classiﬁer
with ”Usto” and noted the prediction, then fed it with ”Usto
se” and noted the prediction, and so on until the full sentence is
fed. The classiﬁer prediction at different stages of the sentence
scan is plotted in Fig. (9). The top left panel of Fig. (9) shows
that the classiﬁer for the most part thinks the document to be
actually bs, until it saw the last word of the sentence when it
switched its prediction to sr. We think this is due to the fact
that the last word associated very uniquely to sr in the training
corpus. The bottom left panel of Fig. (9) shows a similar
scenario but in this case the classiﬁer switched back and forth
a couple of times. The ’confusion’ of the classiﬁer is very
high in the top right panel of the ﬁgure because the particular
sentence was made of words and phrases that are common to
all three languages. We believe that the correct classiﬁcation
of such documents needs creation of extra features based on
deeper understanding of this language group. Another possible
scenario where any classiﬁer can struggle is when the body
of the text contains a quotation of a different language. The
bottom right panel of Fig. (9) shows a scenario where a
document in Serbian had a comment in Portuguese, though
that was not the cause of the eventual classiﬁcation failure.
Removing quotes from a document is a potential option but
it can also have adverse effect if the quote is in the same
language as that of the main document.

VII. CONCLUSION AND NEXT STEPS

We have presented a deep neural network based language
identiﬁcation scheme that achieves near perfect accuracy in
classifying dissimilar languages and about 90% accuracy on
highly similar languages. Speciﬁcally, the languages in West-
ern Slavic Slavic group posed the highest challenge. And
expanding the corpus of these languages using external sources
did not help much mainly because no n-grams of words that
are unique to certain languages were ingested by the expanded

Language Identiﬁcation from Text Documents

Priyank Mathur
Adobe Systems

Stanford University

Arkajyoti Misra
Target Corporation
Stanford University

Stanford, California 94305

priyankm@stanford.edu

Stanford, California 94305

arkajyot@stanford.edu

Emrah Budur

Garanti Technology
Stanford University

Stanford, California 94305

emrah@stanford.edu

Abstract—The increase in the use of microblogging came along
with the rapid growth on short linguistic data. On the other
hand deep learning is considered to be the new frontier to
extract meaningful
information out of large amount of raw
data in an automated manner [1]. In this study, we engaged
these two emerging ﬁelds to come up with a robust language
identiﬁer on demand, namely Stanford Language Identiﬁcation
Engine (SLIDE). As a result, we achieved 95.12% accuracy in
Discriminating between Similar Languages (DSL) Shared Task
2015 dataset, beating the maximum reported accuracy achieved
so far [2].

Index Terms—Language identiﬁcation, LID, SLIDE.

I. INTRODUCTION

Automatic language detection is the ﬁrst step toward achiev-
ing a variety of tasks like detecting the source language for
machine translation, improving the search relevancy by per-
sonalizing the search results according to the query language
[3], providing uniform search box for a multilingual dictionary
[4], tagging data stream from Twitter with appropriate lan-
guage etc. While classifying languages belonging to disjoint
groups is not hard, disambiguation of languages originating
from the same source and dialects still pose a considerable
challenge in the area of natural language processing. Regular
classiﬁers based on word frequency only are inadequate in
making a correct prediction for such similar languages and
utilization of state of the art machine learning tools to capture
the structure of the language has become necessary to boost
the classiﬁer performance. In this work we took advantage
of recent advancement of deep neural network based models
showing stellar performance in many natural language pro-
cessing tasks to build a state of the art language classiﬁer.

We benchmarked our solution with the industry leaders and

achieved ﬁrst rank in the DSL test dataset.

II. PREVIOUS WORK

In the past, a variety of methods have been tried like
Naive Bayes [5], SVM [6], n-gram[7], graph-based n-gram[8],
prediction partial matching (PPM) [9], linear interpolation with
post independent weight optimization and majority voting for
combining multiple classiﬁers [10] etc. and the best accuracy
achieved are still in the lower ninety percents.

The researchers have worked on various critical tasks chal-
lenging the dimensions of the topic, including but not limited
to, supporting low resource languages, i.e. Nepali, Urdu, and
Icelandic [11], [12] handling user-generated unstructured short
texts, i.e. microblogs [11], [10] building a domain agnostic

engine [11], [8]. Existing benchmarking solutions approach
the LID problem in different ways where LogR [11] adopts a
discriminative approaches with regularized logistic regression,
TextCat and Google CLD[13] recruits N-gram-based algo-
rithm, langid.py [5] relies on a Naive Bayes classiﬁer with
a multinomial event model.

The outstanding results, of the time, suggested by Cavnar
and Trenkle became de facto standard of LID even today [7].
The signiﬁcant ingredient of their method is shown to use
a rank order statistic called ”out of place” distance measure
[14]. The problem in their approach is that they generated
n-grams out of words that requires tokenization. However,
many languages including Japanese and Chinese have no word
boundaries. Considering that Japanese is the second most
frequent language used in Twitter [12], there is a need for
better approach to scale the solution to all languages. As a
solution to their problem, Dunning came up with a better
approach with incorporating byte level n-grams of the whole
string instead of char level n-grams of the words [14].

After a rigorous literature survey, we found no prior study
that applied deep learning on language identiﬁcation of text.
On the other hand, there are a few number of studies that
applied deep learning to identify the language of speech [15],
[16], [17], [18]. We believe this study will be the ﬁrst in the
literature if published for LID in textual data by means of deep
learning.

III. DATASET DESCRIPTION

The data for this project work was obtained from ”Discrim-
inating between Similar Language (DSL) Shared Task 2015”
[19]. A set of 20000 instances per language (18000 training
(train.txt) and 2000 evaluation (test.txt)) was provided for 13
different world languages. The dataset also consisted of a
subset (devel.txt) of the overall training data which we utilized
for hyper-parameter tuning. The languages are grouped as
shown in Table I. The names of the groups will be frequently
referred in the subsequent sections.

Each entry in the dataset is a full sentence extracted from
journalistic corpora and written in one of the languages and
tagged with the language group and country of origin. A
similar set of mixed language instance was also provided to
add noise to the data. A separate gold test data was provided
for the ﬁnal evaluation (test-gold.txt).

We applied t-SNE algorithm to visualize the instances
in 3D euclidean space [20], [21]. For feature extraction,

Group Name

Language Name

Language Code

South Eastern Slavic

South Western Slavic

West-Slavic

Ibero-Romance (Spanish)

Ibero-Romance (Portuguese)

Astronesian

Bulgarian
Macedonian
Bosnian
Croatian
Serbian
Czech
Slovak
Peninsular Spain
Argentinian Spanish
Brazilian Portuguese
European Portuguese
Indonesian
Malay

bg
mk
bs
hr
sr
cz
sk
es-ES
es-AR
pt-BR
pt-PT
id
my

TABLE I: Benchmark results of available solutions

(a) Easily separable
t-SNE visualization of

Fig. 1:
plots
http://SeeYourLanguage.info

including 3D animated plot

(b) Difﬁcult to separate

language groups. More
:

available

are

at

we vectorized each sentence over 1 to 5-grams of the to-
kens delimited by white space characters. Fig. 1 shows
the resulting plot. As can be seen on the plot,
the lan-
guages in the same group overlap a lot while the languages
in different groups can be linearly separable. A 3 dimen-
sional visualization of all the languages can be viewed at
https://www.youtube.com/watch?v=mhRdfC26q78.

A. Multinomial Naive Bayes

IV. METHODS

We created a baseline result by training a Multinomial
Naive Bayes model because it is quick to prototype, runs
fast and known to provide decent results in the ﬁeld of text
processing. We have done no pre-processing of the text com-
monly done in the ﬁeld like stemming or stop word removal
because we believe that could potentially remove important
signatures of a particular language, particularly when the same
language is spoken by two geographically disconnected group
of people (e.g Portuguese spoken in Portugal and Brazil).
We experimented with both word and character n-grams. The
character n-grams turned out to be particularly useful when
differentiating between two languages using mostly distinct
character sequences in their alphabet.

The character level n-gram behaves quite differently from
that of word level n-grams as shown in Fig. 2. Single char-

Fig. 2: Naive Bayes performance as a function of n for both
word and character n-grams.

acters carry little information and therefore the performance
for character n-gram improves quite sharply as the number
of characters is increased before saturating at about n=8. We
experimented with character n-grams both restricted at word
boundaries and spanning across word boundaries. The latter
has a marginal performance boost at the cost of longer training
time and memory pressure. The word n-gram model peaks
at n=2 and drops beyond that. While higher order n-grams
carry more structure of the language, they become increasingly
infrequent too and therefore the models don’t always get a
boost from it. Both the character level and word level n-gram
models show similar performance where they really excel
at certain languages (Czech, Slovak) and do poorly at other
(Bosnian, Croatian, Serbian).

B. Logistic Regression

We next tried a regularized logistic regression and here
too the character level n-gram performed a little better than
the word n-grams. Fig. 3 shows that the model was able to
completely ﬁt the training set but the performance on the
validation set plateaued close to 0.95. The best performance
was obtained by a character 9-gram model that includes all
n-grams up to n=9. These n-grams were truncated at
the
word boundaries, or in other words these n-grams did not
capture two or more consecutive words. Relaxing this criterion
signiﬁcantly increases the size of the term frequency matrix
and pushes the boundary of the computer memory but it does
improve the performance by a fraction of a percent.

C. Recurrent Neural Network

The MNB and LR approaches work really well in dis-
tinguishing two languages that have very little in common
because the set of n-grams will have very little overlap
between them. This approach does not work very well when
two languages are close to each other and share a lot of words
between them. Therefore, it becomes necessary to capture the
structure of a languages better to distinguish between similar
languages. We explored Recurrent Neural Networks (RNN)
for this purpose.

RNNs are a special kind of neural networks which possess
an internal state by virtue of a cycle in their hidden units. As

of the process, we varied a single parameter while keeping
the other two constant. The plots below (ﬁg. 4) show the
performance of the resultant models on the validation dataset
as each parameter was changed.

Fig. 5: Variation of the accuracy on validation dataset as we
vary training epochs, number of hidden units and drop off

Fig. 6: Grid search over the best parameter values found in
the previous step

As we can see in the plot above in Fig. 4, increasing the
number of training epochs improves the model performance
up to a certain stage, after which it plateaus. Hence, for the
next stage of tuning, we ﬁxed the number of training epochs
to 20. Using the best values for the number of hidden units
and dropout found above, we performed grid search over
all combinations of these parameters. The result of the grid
search is visualized in Fig. 5. The (number of hidden units,
dropout) combinations (1280, 0.4) and (768, 0.45) gave us
the best performance on the validation set. The ﬁnal values
chosen for further experimentation were 768 hidden units and
0.45 dropout so as to avoid overﬁtting.

Training procedure: Our ﬁnal model is an ensemble of 5
RNNs, each built using a different feature set, namely, from
character 2-grams to character 5-grams and word unigrams. To
train our models, we divided our entire training data (train.txt)
into 90% training set and 10% validation set. Once trained, we
measured the performance of each model individually on the
validation set and is reported in Table II.

As seen in Fig. 6, to construct the ensemble, instead of
manually assigning weights to each model, we constructed a
Logistic Regression model to get the ﬁnal output. The features
for this LR model were the outputs from the 5 RNNs created
earlier and it was tuned using 5 fold cross validation over the
10% validation dataset.

For training the RNNs, we used a Python library called
passage [27], which is built on top of Theano. Although the

Fig. 3: The LR model was able to completely ﬁt the training
data but the accuracy on validation data peaked at about 94%
overall.

such, RNNs are able to record temporal dependencies among
the input sequence, as opposed to most other machine learning
algorithms where the inputs are considered independent of
each other. Hence, they are very well suited to natural lan-
guage processing tasks and have been successfully used for
applications like speech recognition, hand writing recognition
etc.

Fig. 4: Visualization of an un-rolled recurrent neural network
[22]

Until recently, RNNs were considered very difﬁcult

to
train because of the problem of exploding or vanishing
gradients[23] which makes it very difﬁcult for them to learn
long sequences of input. Few methods like gradient clipping
have been proposed to remedy this. Recent architectures like
Long Short Term Memory (LSTM) [24] and Gated Recurrent
Unit (GRU) [25] were also speciﬁcally designed to get around
this problem. In our experiments, we used single hidden
layer recurrent neural networks that used gated recurrent units.

Hyper-parameter tuning: In our single layer networks, we

had three model hyper parameters to search over

1) Epochs - the number of iterations over training data.

We generally try to train until the network saturates.

2) Hidden layer size - Number of hidden units in the

hidden layer.

3) Dropout - Deep neural networks with large number
of parameters are very powerful machines but are
extremely susceptible to overﬁtting. Dropout provides
a simple way to remedy this problem by randomly
dropping hidden units as each example propagates
through the network and back [26].

We used a subset of our overall training data (devel.txt) for
hyper parameter selection. This subset was further divided into
75% training data and 25% validation data. In the ﬁrst step

Model

MNB (char 9-gram)
LR (char 9-gram)
RNN (char 2-gram)
RNN (char 3-gram)
RNN (char 4-gram)
RNN (char 5-gram)
RNN (word uni-gram)
Ensemble of RNN model (SLIDE)

Accuracy

Validation Set

Test Set

0.9479
0.9486
0.9200
0.9328
0.9377
0.9347
0.9351
0.9533

0.9452
0.9449
0.9213
0.9338
0.9347
0.9316
0.9330
0.9512

TABLE II: Performance comparison of various models

Fig. 8: Confusion matrix

results that the biggest challenge consistently posed to our
classiﬁers is distinguishing the languages in South Western
Slavic group (bs, hr, cr). The training set revealed that among
all the words in bs, 48% are common to hr and 41% to sr.
Since Fig. 3 clearly showed we didn’t underﬁt the training
set, it made sense to augment the training data in these three
language categories. We incorporated a signiﬁcantly larger
labeled data for two of these languages and also downloaded
newspaper articles in bs, but the classiﬁcation accuracy in this
language group did not improve. Looking closer to some of
these external datasets revealed that none of the new words
could be uniquely associated to any of the three languages
and therefore, the additional data probably added more noise
than signal.

To understand the failure mechanism of the classiﬁer for
the South Western Slavic language group, we fed the LR
classiﬁer, which is the best of single models in validation set
according to Table II, different fractions of a document it failed
to classify correctly. For example, the following document
is in Bosnian(bs) but the classiﬁer predicts its language as
Serbian(sr): Usto se osvrnuo na ekonomsku situaciju u kojoj

Fig. 7: Training procedure for the RNN ensemble

library provides several tools for text pre-processing including
tokenization, it lacked the ability to generate character n-gram
level features. Therefore, we had to extend the library with
custom character level feature generators. In addition, training
neural networks on CPUs consumes a lot of time. Hence,
for our experiments, we leveraged AWS GPU (g2.2xlarge)
instances that provided a 10x boost in time required to train
one model.

V. RESULTS

Table II shows a comparison of the models we have ex-
perimented with. One surprising feature of the result is that
individual RNN models were not able to beat the performance
of the MNB and LR models, even though the latter models
have minimal knowledge of a language structure. However,
when we created an ensemble of RNN models, it turned out
to be the best model and crossed the 95% threshold for the ﬁrst
time. It should be noted that for a particular n-gram model,
MNB and LR models use all m-grams where 1 ≤ m ≤ n.
However, due to the very nature of an RNN architecture, a
combination of n-grams cannot be used because that will lead
to an overlapping sequence of content to be fed to the network.
Since any given n-gram captures only limited information
about a language, it was natural to try an ensemble of n-gram
RNN models with different values of n, so that structure of
the language can be captured at multiple different levels.

The boost in performance due to ensemble can also be
attributed to model combination, which aims to achieve at least
as good of a performance as the worst model in the ensemble.
This is because individual models can make mistakes on
different examples, and therefore, by using an ensemble we
are able to reduce this variance. While we tried other model
combination strategies like median and manual weighting,
building a Logistic Regression classiﬁer on top of RNNs really
helped us ﬁnd the optimal weight that should be given to each
individual model. We could not include RNN models beyond
character 5-gram in the ensemble because of memory limita-
tion and including the MNB or LR model in the ensemble did
not improve the performance of the model.

VI. DISCUSSION

The ﬁnal classiﬁcation for each language group is captured
in the confusion matrix in Fig. 8. It is quite evident from our

part of the corpus. We have relied on the ensemble of RNN
models to discover the structure unique to a speciﬁc language
but we could not engineer any additional feature due to lack
of knowledge in those speciﬁc languages. At this point, we
think, further improvement can only be achieved by designing
rule based features by talking to language experts or native
speakers.

ACKNOWLEDGMENT

We would like to thank David Jurgens of Department
of Computer Science, Stanford University for helping with
idea, dataset and previous research, Junjie Qin
the initial
for his mentoring and insightful comments that polished the
outcome of the study, AWS Educate Program for providing
EC2 credits and computing resources and Microsoft Azure
for Research Program for providing Azure credits and full
featured computing resources, and lastly Google, Yandex and
Basis Tech for providing free access to their language detection
APIs for our benchmarking analysis [28] [29] [30].

COMPARISON WITH OTHER SYSTEMS

We assessed the performance of SLIDE by comparing its
result with the domain leaders in an unfair test described
below.
We queried the test ﬁle of dataset of DSL Shared Task 2015
and accepted the resulting predictions even if

• the dialect of the language is not distinguished in Ibero-
Romance language group due to lack of support, i.e.
Google always predicts Portuguese for sentences both in
Brazilian Portuguese and European Portuguese.

• a certain language is not supported at all, i.e. Rosette

doesn’t support Bosnian.

Table III shows the resulting accuracies. Although SLIDE
it

had lack of competitive advantage in this unfair test,
surpassed the industry leaders in terms accuracy.

Solution
SLIDE
Google Translate API
Rosette Language API
langid.py
Yandex Translator API

Accuracy

95%
89%
86%
80%
79%

TABLE III: Benchmark results in non-increasing order of
accuracy

Fig. 9: Scenarios where the LR classiﬁer incorrectly predicted
the target language. (Detailed description in Section VI)

je veliki broj novinara u potrazi za poslom, na mizerne plae i
guranje etike strane profesije u zapeak. So we fed the classiﬁer
with ”Usto” and noted the prediction, then fed it with ”Usto
se” and noted the prediction, and so on until the full sentence is
fed. The classiﬁer prediction at different stages of the sentence
scan is plotted in Fig. (9). The top left panel of Fig. (9) shows
that the classiﬁer for the most part thinks the document to be
actually bs, until it saw the last word of the sentence when it
switched its prediction to sr. We think this is due to the fact
that the last word associated very uniquely to sr in the training
corpus. The bottom left panel of Fig. (9) shows a similar
scenario but in this case the classiﬁer switched back and forth
a couple of times. The ’confusion’ of the classiﬁer is very
high in the top right panel of the ﬁgure because the particular
sentence was made of words and phrases that are common to
all three languages. We believe that the correct classiﬁcation
of such documents needs creation of extra features based on
deeper understanding of this language group. Another possible
scenario where any classiﬁer can struggle is when the body
of the text contains a quotation of a different language. The
bottom right panel of Fig. (9) shows a scenario where a
document in Serbian had a comment in Portuguese, though
that was not the cause of the eventual classiﬁcation failure.
Removing quotes from a document is a potential option but
it can also have adverse effect if the quote is in the same
language as that of the main document.

VII. CONCLUSION AND NEXT STEPS

We have presented a deep neural network based language
identiﬁcation scheme that achieves near perfect accuracy in
classifying dissimilar languages and about 90% accuracy on
highly similar languages. Speciﬁcally, the languages in West-
ern Slavic Slavic group posed the highest challenge. And
expanding the corpus of these languages using external sources
did not help much mainly because no n-grams of words that
are unique to certain languages were ingested by the expanded

[29] Translator api yandex technologies.
[30] Rosette api — basis technology.

REFERENCES

[1] I Arel, D C Rose, and T P Karnowski. Deep machine learning - a
new frontier in artiﬁcial intelligence research [research frontier]. IEEE
Computational Intelligence Magazine, 5(4):13–18, 2010.

[2] Marcos Zampieri, Liling Tan, and Nikola Ljube. Overview of the dsl

shared task 2015. (2014), 2015.

[3] Juliane Stiller, Maria G¨ade, and Vivien Petras. Ambiguity of queries
and the challenges for query language detection. CLEF 2010 Labs and
Workshops Notebook Papers, 2010.

[4] Dong Nguyen and a Seza Do. Word level language identiﬁcation in

online multilingual communication. 23(October):857–862, 2013.

[5] Marco Lui and Timothy Baldwin. langid. py: An off-the-shelf language
identiﬁcation tool. Proceedings of the ACL 2012 System Demonstrations,
(July):25–30, 2012.

[6] Aditya Bhargava and Grzegorz Kondrak. Language identiﬁcation of

names with svms. Computational Linguistics, (June):693–696, 2010.

[7] William B Cavnar, John M Trenkle, and Ann Arbor Mi. N-gram-based
text categorization. In Proceedings of SDAIR-94, 3rd Annual Symposium
on Document Analysis and Information Retrieval, 1994.

[8] Erik Tromp and Mykola Pechenizkiy. Graph-based n-gram language
identiﬁcation on short texts. ”Proceedings of the 20th annual Belgian-
Dutch Conference on Machine Learning”, pages 27–34, 2011.

[9] Victoria Bobicev. Native language identiﬁcation with ppm. pages 180–

187.

[10] Simon Carter, Wouter Weerkamp, and Manos Tsagkias. Microblog
language identiﬁcation: overcoming the limitations of short, unedited
and idiomatic text. Language Resources and Evaluation, 47(1):195–
215, 2012.

[11] Shane Bergsma, Paul Mcnamee, Mossaab Bagdouri, Clayton Fink, and
Theresa Wilson. Language identiﬁcation for creating language-speciﬁc
twitter collections. ”Proceedings of the 2nd Workshop on Language in
Social Media at NAACL-HLT’12”, (Lsm 2012):65–74, 2012.

[12] Semicoast. Half of messages on twitter are not in english japanese is

the second most used language. Semiocast, page 75005, 2010.

[13] Dick Sites. Google compact language detector 2, 2013.
[14] Ted Dunning. Statistical identiﬁcation of language. Computing, (Novem-

ber), 1994.

[15] Ignacio Lopez-Moreno, Javier Gonzalez-Dominguez, Oldrich Plchot,
David Martinez, Joaquin Gonzalez-Rodriguez, and Pedro J. Moreno.
Automatic language identiﬁcation using deep neural networks. Icassp-
2014, pages 5337–5341, 2014.

[16] Gr´egoire Montavon. Deep learning for spoken language identiﬁcation.
NIPS Workshop on Deep Learning for Speech Recognition and Related
Applications, pages 1–4, 2009.

[17] Bing Jiang, Yan Song, Si Wei, Jun Hua Liu, Ian Vince McLoughlin,
and Li Rong Dai. Deep bottleneck features for spoken language
identiﬁcation. PLoS ONE, 9(7):3012–3016, 2014.

[18] Javier Gonzalez-Dominguez,

Ignacio Lopez-Moreno, Hasim Sak,
Joaquin Gonzalez-Rodriguez, and Pedro J. Moreno. Automatic language
identiﬁcation using long short-term memory recurrent neural networks.
In Interspeech-2014, pages 2155–2159, 2014.

[19] Lt4vardial workshop.
[20] t-sne laurens van der maaten.
[21] Laurens Van Der Maaten. Accelerating t-sne using tree-based algo-
rithms. The Journal of Machine Learning Research, 15(1):3221–3245,
1 2014.

[22] Understanding lstm networks – colah’s blog.
[23] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty
International Conference on

of training recurrent neural networks.
Machine Learning, (2):1310–1318, 2013.

[24] Sepp Hochreiter Schmidhuber and J¨urgen. Long short-term memory.

Neural Computation, 9(8):1735 – 1780, 1997.

[25] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase repre-
sentations using rnn encoder-decoder for statistical machine translation.
arXiv, pages 1724–1734, 2014.

[26] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,
and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural
networks from overﬁtting. The Journal of Machine Learning Research,
15(1):1929–1958, 1 2014.

[27] Indicodatasolutions/passage.
[28] Google translate api - programmatic translation services google cloud

platform.

