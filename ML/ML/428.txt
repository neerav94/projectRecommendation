Estimation of Word Representations Using Recurrent Neural

Networks and Its Application in Generating Business Fingerprints

Kuan Fang

kuanfang@stanford.edu

Abstract: Word vectors have been used as a useful
feature in many NLP related tasks.
In this project,
we proposed a modiﬁed recurrent neural network al-
gorithm to learn word representations from corpus in
a state-transition manner. Our state-transition RNN
could reach the results of state-of-the-art word embed-
ding algorithms in terms of comparing Euclidean dis-
tance. With smaller hidden size, the state-transition
RNN needs shorter time for training. As a testbench
for our model, we trained our model on Yelp review
corpus and implemented three diﬀerent applications
(business similarity, business search and business rec-
ommendation). Experimental results revealed that
our state-transition RNN and our Business ﬁngerprints
generation algorithms achieved expected results.

1. Introduction

Word vectors have been an important and basic fea-
ture for a variety of natural language processing tasks.
A reasonable set of word vectors should represent the
similarity of words by computing the distance (e.g.
cosine distance and Frobenius norm) of two diﬀerent
word vectors. Estimation algorithms of word represen-
tations such as skip-gram and GloVe have been imple-
mented in recent years. The skip-gram is an eﬃcient
linear training model considering a small part of con-
text words. And Glove is a log-bilinear model with
a weighted least-squares objective considering global
context. These existing models has shown good per-
formance on word analogy tasks, but they have deﬁ-
ciencies such as considering only local context words
or considering only the word-word co-occurrence in the
text.

In this project, we aim to propose unsupervised models
that can learn deeper relationships by using global con-
text words in sentences and paragraphs. We will be-
gin by implementing a modiﬁed recurrent neural net-
work (RNN) language model as our baseline algorithm.
Recurrent neural network has shown excellent perfor-
mance in the language modeling task by taking the
previous context words as input and predicting the tar-
get word. Since the word representation of the target

word should be learned by context words before and
after it, we attempt to implement a state-transition
RNN to extract global contexts and even the seman-
tics from the sentence.

With learned word vectors, we represent the each busi-
ness ﬁngerprint as a cluster of word vectors extracted
from the reviews. And three implemented applications
proved that these business ﬁngerprints are useful in
recommendation systems.

2. Prediction and Metrics

2.1. Learning Word Vectors

There is no direct way to evaluate the performance
of trained word vectors. Before we implemented more
sophisticated testbenches to test our word vectors on
dataset like word-analogy problems we take the Per-
plexity (PPL) of the language modeling as a metric of
our word vectors trained on State-Transition RNN.

2.2. Generating Business Fingerprints

Although a high performance of applications (e.g.
business category prediction) is not a direct metric of
ﬁngerprint quality, a good ﬁngerprint will be necessar-
ily good to be used for the classiﬁcation task. Based on
this argument, we can formulate an optimization goal
based on combinations of speciﬁc tasks. For exam-
ple, one evaluation metric is to loop through all pairs
of businesses and compute their distances, if the two
businesses have many categories in common, we hope
their distance is small; on the other hand if they do
not have category in common, we want their distance
to be large.

(cid:88)

(i,j)

(categorySim(i, j) − 0.5) · distance(i, j) (1)

min

θ

3. Dataset

We trained our data on Penn Tree Bank data set and
Yelp reviews corpus. The Penn Tree Bank data set is a
common dataset in natural language processing. And

Estimation of Word Representations Using Recurrent Neural

Networks and Its Application in Generating Business Fingerprints

Kuan Fang

kuanfang@stanford.edu

Abstract: Word vectors have been used as a useful
feature in many NLP related tasks.
In this project,
we proposed a modiﬁed recurrent neural network al-
gorithm to learn word representations from corpus in
a state-transition manner. Our state-transition RNN
could reach the results of state-of-the-art word embed-
ding algorithms in terms of comparing Euclidean dis-
tance. With smaller hidden size, the state-transition
RNN needs shorter time for training. As a testbench
for our model, we trained our model on Yelp review
corpus and implemented three diﬀerent applications
(business similarity, business search and business rec-
ommendation). Experimental results revealed that
our state-transition RNN and our Business ﬁngerprints
generation algorithms achieved expected results.

1. Introduction

Word vectors have been an important and basic fea-
ture for a variety of natural language processing tasks.
A reasonable set of word vectors should represent the
similarity of words by computing the distance (e.g.
cosine distance and Frobenius norm) of two diﬀerent
word vectors. Estimation algorithms of word represen-
tations such as skip-gram and GloVe have been imple-
mented in recent years. The skip-gram is an eﬃcient
linear training model considering a small part of con-
text words. And Glove is a log-bilinear model with
a weighted least-squares objective considering global
context. These existing models has shown good per-
formance on word analogy tasks, but they have deﬁ-
ciencies such as considering only local context words
or considering only the word-word co-occurrence in the
text.

In this project, we aim to propose unsupervised models
that can learn deeper relationships by using global con-
text words in sentences and paragraphs. We will be-
gin by implementing a modiﬁed recurrent neural net-
work (RNN) language model as our baseline algorithm.
Recurrent neural network has shown excellent perfor-
mance in the language modeling task by taking the
previous context words as input and predicting the tar-
get word. Since the word representation of the target

word should be learned by context words before and
after it, we attempt to implement a state-transition
RNN to extract global contexts and even the seman-
tics from the sentence.

With learned word vectors, we represent the each busi-
ness ﬁngerprint as a cluster of word vectors extracted
from the reviews. And three implemented applications
proved that these business ﬁngerprints are useful in
recommendation systems.

2. Prediction and Metrics

2.1. Learning Word Vectors

There is no direct way to evaluate the performance
of trained word vectors. Before we implemented more
sophisticated testbenches to test our word vectors on
dataset like word-analogy problems we take the Per-
plexity (PPL) of the language modeling as a metric of
our word vectors trained on State-Transition RNN.

2.2. Generating Business Fingerprints

Although a high performance of applications (e.g.
business category prediction) is not a direct metric of
ﬁngerprint quality, a good ﬁngerprint will be necessar-
ily good to be used for the classiﬁcation task. Based on
this argument, we can formulate an optimization goal
based on combinations of speciﬁc tasks. For exam-
ple, one evaluation metric is to loop through all pairs
of businesses and compute their distances, if the two
businesses have many categories in common, we hope
their distance is small; on the other hand if they do
not have category in common, we want their distance
to be large.

(cid:88)

(i,j)

(categorySim(i, j) − 0.5) · distance(i, j) (1)

min

θ

3. Dataset

We trained our data on Penn Tree Bank data set and
Yelp reviews corpus. The Penn Tree Bank data set is a
common dataset in natural language processing. And

CS229 Fall 2014 Project Final Writeup

the Yelp reviews corpus is cleaned by ourselves from
the Yelp Data Challenge data set.

4. Model

4.1. Word Representation Using Skip-gram

Model

Skip-gram Model is a distributed word representation
learning algorithm proposed by Mikolov et al in 2013.
Skip-gram learn the word representation vectors based
on a two-level neural network, taking the dot product
of the word vector as input to predict the occurrence
of words in a context window (usually 4 - 6 context
words). In the training, Skip-gram maximize the log
probability of context words.

T(cid:88)

1
T

(cid:88)

t=1

−c≤j≤c,j(cid:54)=0

log p(wt+j|wt)

(2)

The basic Skip-gram formulation deﬁnes p(wt+j|wt)
using the softmax function:

p(wt+j|wt) =

(cid:80)W

exp(v(cid:62)
wOvwI )
w=1 exp(v(cid:62)

w vwI )

Figure 1. State Transition Reccurent Neural Network

(3)

the semantics of the sentence as the following words
are read in one by one.

We modiﬁed Google’s word2vec toolkit implemented
by Mikolov et al (Mikolov et al., 2013a) and trained
Skip-gram on general text corpus and Yelp reviews
corpus respectively. In terms of plugging the resulting
word vectors into our recommendation model, we used
cosine distance and Frobenius norm to compute the
word distance respectively.

4.2. Word Representation Using

State-Transition Recurrent Neural
Network

We also proposed a recurrent neural network (RNN)
based model to learn word representation. One of the
disavantages of the word-word co-occurrence model is
that they can not learn the word vector based on the
semantics of the sentence. For example, “good” and
“bad” has a close cosine distance after trained by Skip-
gram. This is because these two words occurred in
many similar contexts in the training data. Thus we
turned to a modiﬁed RNN algorithm. Like existing
RNN, we learn word vectors in an unsupervised man-
ner taking the PPL of the language modeling as the
objective. In stead of taking the one-hot word code as
input, we model the semantics of the sentence using a
dense state vector. Each entry of the state vector rep-
resents a feature of the semantics evolved with words.
While each word maintains a transition matrix trans-
from the input state into the output state, evolving

s(0) = (1, 0, ..., 0)(cid:62)
s(t) = f (W (t) · s(t−1) + b(t))

(4)

(5)

To train this RNN unsupervisely, a ﬁxed matrix feed
forwards the output state layer into the prediction
layer, where the probability of the next word is repre-
sented by a one-hot vector.

y(t) = f (V · s(t))

(6)

The backpropagation steps of the model is as follows:

U (t) := U (t) + αe(t)
o

W (t) := W (t) + αe(t)

· s(t)(cid:62)
h · s(t−1)(cid:62)

where we plug following error expressions in:

o = d(t) − y(t)
e(t)
h = dh(e(t)(cid:62)
e(t)
V, t)
j (1 − s(t)
j )

o

dhj(x, t) = xs(t)

(7)

(8)

(9)

(10)

(11)

We expect this algorithm could extract deeper rela-
tionship between words, thus generating more reason-
able word vectors as long with lower PPL for the lan-
guage modeling objective. We implemented our code
based on Microsoft Research’s RNN language mod-
eling toolkit and modiﬁed almost all functions of it
according to our needs.

Estimation of Word Representations Using Recurrent Neural

Networks and Its Application in Generating Business Fingerprints

Kuan Fang

kuanfang@stanford.edu

Abstract: Word vectors have been used as a useful
feature in many NLP related tasks.
In this project,
we proposed a modiﬁed recurrent neural network al-
gorithm to learn word representations from corpus in
a state-transition manner. Our state-transition RNN
could reach the results of state-of-the-art word embed-
ding algorithms in terms of comparing Euclidean dis-
tance. With smaller hidden size, the state-transition
RNN needs shorter time for training. As a testbench
for our model, we trained our model on Yelp review
corpus and implemented three diﬀerent applications
(business similarity, business search and business rec-
ommendation). Experimental results revealed that
our state-transition RNN and our Business ﬁngerprints
generation algorithms achieved expected results.

1. Introduction

Word vectors have been an important and basic fea-
ture for a variety of natural language processing tasks.
A reasonable set of word vectors should represent the
similarity of words by computing the distance (e.g.
cosine distance and Frobenius norm) of two diﬀerent
word vectors. Estimation algorithms of word represen-
tations such as skip-gram and GloVe have been imple-
mented in recent years. The skip-gram is an eﬃcient
linear training model considering a small part of con-
text words. And Glove is a log-bilinear model with
a weighted least-squares objective considering global
context. These existing models has shown good per-
formance on word analogy tasks, but they have deﬁ-
ciencies such as considering only local context words
or considering only the word-word co-occurrence in the
text.

In this project, we aim to propose unsupervised models
that can learn deeper relationships by using global con-
text words in sentences and paragraphs. We will be-
gin by implementing a modiﬁed recurrent neural net-
work (RNN) language model as our baseline algorithm.
Recurrent neural network has shown excellent perfor-
mance in the language modeling task by taking the
previous context words as input and predicting the tar-
get word. Since the word representation of the target

word should be learned by context words before and
after it, we attempt to implement a state-transition
RNN to extract global contexts and even the seman-
tics from the sentence.

With learned word vectors, we represent the each busi-
ness ﬁngerprint as a cluster of word vectors extracted
from the reviews. And three implemented applications
proved that these business ﬁngerprints are useful in
recommendation systems.

2. Prediction and Metrics

2.1. Learning Word Vectors

There is no direct way to evaluate the performance
of trained word vectors. Before we implemented more
sophisticated testbenches to test our word vectors on
dataset like word-analogy problems we take the Per-
plexity (PPL) of the language modeling as a metric of
our word vectors trained on State-Transition RNN.

2.2. Generating Business Fingerprints

Although a high performance of applications (e.g.
business category prediction) is not a direct metric of
ﬁngerprint quality, a good ﬁngerprint will be necessar-
ily good to be used for the classiﬁcation task. Based on
this argument, we can formulate an optimization goal
based on combinations of speciﬁc tasks. For exam-
ple, one evaluation metric is to loop through all pairs
of businesses and compute their distances, if the two
businesses have many categories in common, we hope
their distance is small; on the other hand if they do
not have category in common, we want their distance
to be large.

(cid:88)

(i,j)

(categorySim(i, j) − 0.5) · distance(i, j) (1)

min

θ

3. Dataset

We trained our data on Penn Tree Bank data set and
Yelp reviews corpus. The Penn Tree Bank data set is a
common dataset in natural language processing. And

CS229 Fall 2014 Project Final Writeup

the Yelp reviews corpus is cleaned by ourselves from
the Yelp Data Challenge data set.

4. Model

4.1. Word Representation Using Skip-gram

Model

Skip-gram Model is a distributed word representation
learning algorithm proposed by Mikolov et al in 2013.
Skip-gram learn the word representation vectors based
on a two-level neural network, taking the dot product
of the word vector as input to predict the occurrence
of words in a context window (usually 4 - 6 context
words). In the training, Skip-gram maximize the log
probability of context words.

T(cid:88)

1
T

(cid:88)

t=1

−c≤j≤c,j(cid:54)=0

log p(wt+j|wt)

(2)

The basic Skip-gram formulation deﬁnes p(wt+j|wt)
using the softmax function:

p(wt+j|wt) =

(cid:80)W

exp(v(cid:62)
wOvwI )
w=1 exp(v(cid:62)

w vwI )

Figure 1. State Transition Reccurent Neural Network

(3)

the semantics of the sentence as the following words
are read in one by one.

We modiﬁed Google’s word2vec toolkit implemented
by Mikolov et al (Mikolov et al., 2013a) and trained
Skip-gram on general text corpus and Yelp reviews
corpus respectively. In terms of plugging the resulting
word vectors into our recommendation model, we used
cosine distance and Frobenius norm to compute the
word distance respectively.

4.2. Word Representation Using

State-Transition Recurrent Neural
Network

We also proposed a recurrent neural network (RNN)
based model to learn word representation. One of the
disavantages of the word-word co-occurrence model is
that they can not learn the word vector based on the
semantics of the sentence. For example, “good” and
“bad” has a close cosine distance after trained by Skip-
gram. This is because these two words occurred in
many similar contexts in the training data. Thus we
turned to a modiﬁed RNN algorithm. Like existing
RNN, we learn word vectors in an unsupervised man-
ner taking the PPL of the language modeling as the
objective. In stead of taking the one-hot word code as
input, we model the semantics of the sentence using a
dense state vector. Each entry of the state vector rep-
resents a feature of the semantics evolved with words.
While each word maintains a transition matrix trans-
from the input state into the output state, evolving

s(0) = (1, 0, ..., 0)(cid:62)
s(t) = f (W (t) · s(t−1) + b(t))

(4)

(5)

To train this RNN unsupervisely, a ﬁxed matrix feed
forwards the output state layer into the prediction
layer, where the probability of the next word is repre-
sented by a one-hot vector.

y(t) = f (V · s(t))

(6)

The backpropagation steps of the model is as follows:

U (t) := U (t) + αe(t)
o

W (t) := W (t) + αe(t)

· s(t)(cid:62)
h · s(t−1)(cid:62)

where we plug following error expressions in:

o = d(t) − y(t)
e(t)
h = dh(e(t)(cid:62)
e(t)
V, t)
j (1 − s(t)
j )

o

dhj(x, t) = xs(t)

(7)

(8)

(9)

(10)

(11)

We expect this algorithm could extract deeper rela-
tionship between words, thus generating more reason-
able word vectors as long with lower PPL for the lan-
guage modeling objective. We implemented our code
based on Microsoft Research’s RNN language mod-
eling toolkit and modiﬁed almost all functions of it
according to our needs.

CS229 Fall 2014 Project Final Writeup

5. Results

5.1. Unsupervised training of Word Vectors

We ﬁrst test our model on Penn Tree Bank dataset,
which is a hall mark in the natrual language process-
ing ﬁeld. We compare our results on State-Transition
RNN with the state-of-art RNN language modeling
implemented by Microsoft Research. Then we also
trained our State-Transition RNN on the Yelp reviews
dataset (cleaned by ourselves). Since Microsoft’s RNN
has much more parameters than we used, it could take
much more time to converge.

For ST-RNN, we compared the PPL performance of
diﬀerent state sizes from 3 to 99 on Penn Tree Bank
dataset. To speed up the training and prediction,
we followed Mikolov’s approach to categorize output
words according to their frequencies in the training
corpus. The results are shown in Figure. 2. We found
that for Penn Tree Bank dataset, the optimal state
size is around 30. For larger dataset like Yelp Data
Challenge, the optimal state size is around 35.

Model
RNN
RNN
ST-RNN
ST-RNN
ST-RNN
ST-RNN (Y)

Size words/sec
100
200
15
20
30
20

20000
8000
68000
62000
44000
62000

Iters PPL
9
9
8
8
8
7

167.14
153.62
188.31
183.78
177.34
117.80

Our State-Transition RNN achieves reasonable perfor-
mance. We will add BPTT later and test our model
on larger dataset.

5.2. Similar Word Vectors

We give some word examples here to compare the per-
formance of two word embedding algorithms. Both
examples are trained on the reviews data of Yelp Data
Challenge which are cleaned by ourselves. The ﬁrst
row is the input word, and the rest are the closest
words determined by cosine distance.

Table 2. Training State-Transition RNN on reviews

duck
chicken
burrito
tofu
salmon
octopus

burger
pizza
sandwich
burrito
smoothie
pho

noodle
pasta
tofu
vegetable
seafood
ramen

good
great
wonderful
decent
bad
fantastic

Table 3. Training Skip-gram on reviews

duck
pork
squab
chicken
conﬁt
beef

burger
hamburger
cheeseburger
burgers
burgeri
buger

noodle
noodles
tofu
noodlesi
wonton
vermecelli

good
decent
great
solid
tasty
alright

Figure 2. PPL Performance of Diﬀerent State Sizes.

The size column below indicates the hidden size for
RNN and state size for ST-RNN. Training State-
Transition RNN took 15 hours on a research server
in Stanford CS Department to converge. And the last
row is ST-RNN trained on Yelp reviews.

Table 1. Training Skip-gram on general corpus

Table 4. Training Skip-gram on general corpus
duck
fethry mcdonald
daﬀy
amuck mcdonalds
eider
coot

noodle
udon
toppings
steamed
ramen
kuroda

good
bad
natured
luck
virtuous
honest

burger

kfc

schalk
restaurant

As we can see from these examples, training word
vecters on domain-speciﬁc data (Yelp reviews in our
case) has much better performance. Our word embed-
ding algorithm is doing a good job, but it still cannot
achieve the ideal result we want. It is still assigning
antoynms closest vectors. We aspire to improve this
issue in our following work.

Estimation of Word Representations Using Recurrent Neural

Networks and Its Application in Generating Business Fingerprints

Kuan Fang

kuanfang@stanford.edu

Abstract: Word vectors have been used as a useful
feature in many NLP related tasks.
In this project,
we proposed a modiﬁed recurrent neural network al-
gorithm to learn word representations from corpus in
a state-transition manner. Our state-transition RNN
could reach the results of state-of-the-art word embed-
ding algorithms in terms of comparing Euclidean dis-
tance. With smaller hidden size, the state-transition
RNN needs shorter time for training. As a testbench
for our model, we trained our model on Yelp review
corpus and implemented three diﬀerent applications
(business similarity, business search and business rec-
ommendation). Experimental results revealed that
our state-transition RNN and our Business ﬁngerprints
generation algorithms achieved expected results.

1. Introduction

Word vectors have been an important and basic fea-
ture for a variety of natural language processing tasks.
A reasonable set of word vectors should represent the
similarity of words by computing the distance (e.g.
cosine distance and Frobenius norm) of two diﬀerent
word vectors. Estimation algorithms of word represen-
tations such as skip-gram and GloVe have been imple-
mented in recent years. The skip-gram is an eﬃcient
linear training model considering a small part of con-
text words. And Glove is a log-bilinear model with
a weighted least-squares objective considering global
context. These existing models has shown good per-
formance on word analogy tasks, but they have deﬁ-
ciencies such as considering only local context words
or considering only the word-word co-occurrence in the
text.

In this project, we aim to propose unsupervised models
that can learn deeper relationships by using global con-
text words in sentences and paragraphs. We will be-
gin by implementing a modiﬁed recurrent neural net-
work (RNN) language model as our baseline algorithm.
Recurrent neural network has shown excellent perfor-
mance in the language modeling task by taking the
previous context words as input and predicting the tar-
get word. Since the word representation of the target

word should be learned by context words before and
after it, we attempt to implement a state-transition
RNN to extract global contexts and even the seman-
tics from the sentence.

With learned word vectors, we represent the each busi-
ness ﬁngerprint as a cluster of word vectors extracted
from the reviews. And three implemented applications
proved that these business ﬁngerprints are useful in
recommendation systems.

2. Prediction and Metrics

2.1. Learning Word Vectors

There is no direct way to evaluate the performance
of trained word vectors. Before we implemented more
sophisticated testbenches to test our word vectors on
dataset like word-analogy problems we take the Per-
plexity (PPL) of the language modeling as a metric of
our word vectors trained on State-Transition RNN.

2.2. Generating Business Fingerprints

Although a high performance of applications (e.g.
business category prediction) is not a direct metric of
ﬁngerprint quality, a good ﬁngerprint will be necessar-
ily good to be used for the classiﬁcation task. Based on
this argument, we can formulate an optimization goal
based on combinations of speciﬁc tasks. For exam-
ple, one evaluation metric is to loop through all pairs
of businesses and compute their distances, if the two
businesses have many categories in common, we hope
their distance is small; on the other hand if they do
not have category in common, we want their distance
to be large.

(cid:88)

(i,j)

(categorySim(i, j) − 0.5) · distance(i, j) (1)

min

θ

3. Dataset

We trained our data on Penn Tree Bank data set and
Yelp reviews corpus. The Penn Tree Bank data set is a
common dataset in natural language processing. And

CS229 Fall 2014 Project Final Writeup

the Yelp reviews corpus is cleaned by ourselves from
the Yelp Data Challenge data set.

4. Model

4.1. Word Representation Using Skip-gram

Model

Skip-gram Model is a distributed word representation
learning algorithm proposed by Mikolov et al in 2013.
Skip-gram learn the word representation vectors based
on a two-level neural network, taking the dot product
of the word vector as input to predict the occurrence
of words in a context window (usually 4 - 6 context
words). In the training, Skip-gram maximize the log
probability of context words.

T(cid:88)

1
T

(cid:88)

t=1

−c≤j≤c,j(cid:54)=0

log p(wt+j|wt)

(2)

The basic Skip-gram formulation deﬁnes p(wt+j|wt)
using the softmax function:

p(wt+j|wt) =

(cid:80)W

exp(v(cid:62)
wOvwI )
w=1 exp(v(cid:62)

w vwI )

Figure 1. State Transition Reccurent Neural Network

(3)

the semantics of the sentence as the following words
are read in one by one.

We modiﬁed Google’s word2vec toolkit implemented
by Mikolov et al (Mikolov et al., 2013a) and trained
Skip-gram on general text corpus and Yelp reviews
corpus respectively. In terms of plugging the resulting
word vectors into our recommendation model, we used
cosine distance and Frobenius norm to compute the
word distance respectively.

4.2. Word Representation Using

State-Transition Recurrent Neural
Network

We also proposed a recurrent neural network (RNN)
based model to learn word representation. One of the
disavantages of the word-word co-occurrence model is
that they can not learn the word vector based on the
semantics of the sentence. For example, “good” and
“bad” has a close cosine distance after trained by Skip-
gram. This is because these two words occurred in
many similar contexts in the training data. Thus we
turned to a modiﬁed RNN algorithm. Like existing
RNN, we learn word vectors in an unsupervised man-
ner taking the PPL of the language modeling as the
objective. In stead of taking the one-hot word code as
input, we model the semantics of the sentence using a
dense state vector. Each entry of the state vector rep-
resents a feature of the semantics evolved with words.
While each word maintains a transition matrix trans-
from the input state into the output state, evolving

s(0) = (1, 0, ..., 0)(cid:62)
s(t) = f (W (t) · s(t−1) + b(t))

(4)

(5)

To train this RNN unsupervisely, a ﬁxed matrix feed
forwards the output state layer into the prediction
layer, where the probability of the next word is repre-
sented by a one-hot vector.

y(t) = f (V · s(t))

(6)

The backpropagation steps of the model is as follows:

U (t) := U (t) + αe(t)
o

W (t) := W (t) + αe(t)

· s(t)(cid:62)
h · s(t−1)(cid:62)

where we plug following error expressions in:

o = d(t) − y(t)
e(t)
h = dh(e(t)(cid:62)
e(t)
V, t)
j (1 − s(t)
j )

o

dhj(x, t) = xs(t)

(7)

(8)

(9)

(10)

(11)

We expect this algorithm could extract deeper rela-
tionship between words, thus generating more reason-
able word vectors as long with lower PPL for the lan-
guage modeling objective. We implemented our code
based on Microsoft Research’s RNN language mod-
eling toolkit and modiﬁed almost all functions of it
according to our needs.

CS229 Fall 2014 Project Final Writeup

5. Results

5.1. Unsupervised training of Word Vectors

We ﬁrst test our model on Penn Tree Bank dataset,
which is a hall mark in the natrual language process-
ing ﬁeld. We compare our results on State-Transition
RNN with the state-of-art RNN language modeling
implemented by Microsoft Research. Then we also
trained our State-Transition RNN on the Yelp reviews
dataset (cleaned by ourselves). Since Microsoft’s RNN
has much more parameters than we used, it could take
much more time to converge.

For ST-RNN, we compared the PPL performance of
diﬀerent state sizes from 3 to 99 on Penn Tree Bank
dataset. To speed up the training and prediction,
we followed Mikolov’s approach to categorize output
words according to their frequencies in the training
corpus. The results are shown in Figure. 2. We found
that for Penn Tree Bank dataset, the optimal state
size is around 30. For larger dataset like Yelp Data
Challenge, the optimal state size is around 35.

Model
RNN
RNN
ST-RNN
ST-RNN
ST-RNN
ST-RNN (Y)

Size words/sec
100
200
15
20
30
20

20000
8000
68000
62000
44000
62000

Iters PPL
9
9
8
8
8
7

167.14
153.62
188.31
183.78
177.34
117.80

Our State-Transition RNN achieves reasonable perfor-
mance. We will add BPTT later and test our model
on larger dataset.

5.2. Similar Word Vectors

We give some word examples here to compare the per-
formance of two word embedding algorithms. Both
examples are trained on the reviews data of Yelp Data
Challenge which are cleaned by ourselves. The ﬁrst
row is the input word, and the rest are the closest
words determined by cosine distance.

Table 2. Training State-Transition RNN on reviews

duck
chicken
burrito
tofu
salmon
octopus

burger
pizza
sandwich
burrito
smoothie
pho

noodle
pasta
tofu
vegetable
seafood
ramen

good
great
wonderful
decent
bad
fantastic

Table 3. Training Skip-gram on reviews

duck
pork
squab
chicken
conﬁt
beef

burger
hamburger
cheeseburger
burgers
burgeri
buger

noodle
noodles
tofu
noodlesi
wonton
vermecelli

good
decent
great
solid
tasty
alright

Figure 2. PPL Performance of Diﬀerent State Sizes.

The size column below indicates the hidden size for
RNN and state size for ST-RNN. Training State-
Transition RNN took 15 hours on a research server
in Stanford CS Department to converge. And the last
row is ST-RNN trained on Yelp reviews.

Table 1. Training Skip-gram on general corpus

Table 4. Training Skip-gram on general corpus
duck
fethry mcdonald
daﬀy
amuck mcdonalds
eider
coot

noodle
udon
toppings
steamed
ramen
kuroda

good
bad
natured
luck
virtuous
honest

burger

kfc

schalk
restaurant

As we can see from these examples, training word
vecters on domain-speciﬁc data (Yelp reviews in our
case) has much better performance. Our word embed-
ding algorithm is doing a good job, but it still cannot
achieve the ideal result we want. It is still assigning
antoynms closest vectors. We aspire to improve this
issue in our following work.

CS229 Fall 2014 Project Final Writeup

5.3. Business Similarity

We tested our current model on a small data set of
10 categories each of which has 100 businesses, thus
1000 businesses in total. Note that for our evaluation
method, smaller the costs are better our signatures
are. We can see that use more feature words achieves
lower cost, i.e. better signatures in terms of category
consistency.

The ground truth is the labeling from the Yelp data.
We count the number of co-existing words in the two
restaurants as a measure of ground truth similarity,
then computed the similarity based of business signa-
ture. We used both Euclidean distance and Cosine
distance to measure it. We tried diﬀerent length of
feature words to concatenate together to form a sig-
nature. As shown by the below plot, the larger the
number of words is chosen the more accurate the per-
formance.

Note that the smaller the Euclidean distance means
the larger Cosine distance, in both cases business ﬁn-
gerprint evaluation vector size = 20 using word2vec
domain speciﬁc training method gives the best perfor-
mance.

5.4. Business Search

Another application of building up business ﬁngerprint
is for semantic search. Traditional search is based on
the appearance of a word, then building a inverted
index. That is bad because if a word doesn’t appear in
a document but some similar word appears, it won’t be
captured. The word vector solved this problem in that
it can ﬁnd all the words that are close to the search
word, making the search result more meaningful. It’s
no longer a word by word search, but semantic search.
Here’s a list of results returned by the search engine.

For example, in the query ”morning light food”, appar-
ently the query wants to ﬁnd a place to have breakfast.
Although the query did not explicitly say breakfast,
but ”morning” and ”lightfood” expressed this notion.
By comparing the business signature the search en-
gine found the Beach House Lounge which is exactly
a place serving breakfast. And after digging into the
reason why this restaurant get returned, it shows the
word ’breakfast’ has the closest distance 1.2018 to the
query, so it gets returned.

Another example is the query ”beer nightlife club-
bing”. The query intends to ﬁnd a place to have some
beer and enjoy the nightlife by clubbing. The busi-
ness signature captures this underlying semantic and

Estimation of Word Representations Using Recurrent Neural

Networks and Its Application in Generating Business Fingerprints

Kuan Fang

kuanfang@stanford.edu

Abstract: Word vectors have been used as a useful
feature in many NLP related tasks.
In this project,
we proposed a modiﬁed recurrent neural network al-
gorithm to learn word representations from corpus in
a state-transition manner. Our state-transition RNN
could reach the results of state-of-the-art word embed-
ding algorithms in terms of comparing Euclidean dis-
tance. With smaller hidden size, the state-transition
RNN needs shorter time for training. As a testbench
for our model, we trained our model on Yelp review
corpus and implemented three diﬀerent applications
(business similarity, business search and business rec-
ommendation). Experimental results revealed that
our state-transition RNN and our Business ﬁngerprints
generation algorithms achieved expected results.

1. Introduction

Word vectors have been an important and basic fea-
ture for a variety of natural language processing tasks.
A reasonable set of word vectors should represent the
similarity of words by computing the distance (e.g.
cosine distance and Frobenius norm) of two diﬀerent
word vectors. Estimation algorithms of word represen-
tations such as skip-gram and GloVe have been imple-
mented in recent years. The skip-gram is an eﬃcient
linear training model considering a small part of con-
text words. And Glove is a log-bilinear model with
a weighted least-squares objective considering global
context. These existing models has shown good per-
formance on word analogy tasks, but they have deﬁ-
ciencies such as considering only local context words
or considering only the word-word co-occurrence in the
text.

In this project, we aim to propose unsupervised models
that can learn deeper relationships by using global con-
text words in sentences and paragraphs. We will be-
gin by implementing a modiﬁed recurrent neural net-
work (RNN) language model as our baseline algorithm.
Recurrent neural network has shown excellent perfor-
mance in the language modeling task by taking the
previous context words as input and predicting the tar-
get word. Since the word representation of the target

word should be learned by context words before and
after it, we attempt to implement a state-transition
RNN to extract global contexts and even the seman-
tics from the sentence.

With learned word vectors, we represent the each busi-
ness ﬁngerprint as a cluster of word vectors extracted
from the reviews. And three implemented applications
proved that these business ﬁngerprints are useful in
recommendation systems.

2. Prediction and Metrics

2.1. Learning Word Vectors

There is no direct way to evaluate the performance
of trained word vectors. Before we implemented more
sophisticated testbenches to test our word vectors on
dataset like word-analogy problems we take the Per-
plexity (PPL) of the language modeling as a metric of
our word vectors trained on State-Transition RNN.

2.2. Generating Business Fingerprints

Although a high performance of applications (e.g.
business category prediction) is not a direct metric of
ﬁngerprint quality, a good ﬁngerprint will be necessar-
ily good to be used for the classiﬁcation task. Based on
this argument, we can formulate an optimization goal
based on combinations of speciﬁc tasks. For exam-
ple, one evaluation metric is to loop through all pairs
of businesses and compute their distances, if the two
businesses have many categories in common, we hope
their distance is small; on the other hand if they do
not have category in common, we want their distance
to be large.

(cid:88)

(i,j)

(categorySim(i, j) − 0.5) · distance(i, j) (1)

min

θ

3. Dataset

We trained our data on Penn Tree Bank data set and
Yelp reviews corpus. The Penn Tree Bank data set is a
common dataset in natural language processing. And

CS229 Fall 2014 Project Final Writeup

the Yelp reviews corpus is cleaned by ourselves from
the Yelp Data Challenge data set.

4. Model

4.1. Word Representation Using Skip-gram

Model

Skip-gram Model is a distributed word representation
learning algorithm proposed by Mikolov et al in 2013.
Skip-gram learn the word representation vectors based
on a two-level neural network, taking the dot product
of the word vector as input to predict the occurrence
of words in a context window (usually 4 - 6 context
words). In the training, Skip-gram maximize the log
probability of context words.

T(cid:88)

1
T

(cid:88)

t=1

−c≤j≤c,j(cid:54)=0

log p(wt+j|wt)

(2)

The basic Skip-gram formulation deﬁnes p(wt+j|wt)
using the softmax function:

p(wt+j|wt) =

(cid:80)W

exp(v(cid:62)
wOvwI )
w=1 exp(v(cid:62)

w vwI )

Figure 1. State Transition Reccurent Neural Network

(3)

the semantics of the sentence as the following words
are read in one by one.

We modiﬁed Google’s word2vec toolkit implemented
by Mikolov et al (Mikolov et al., 2013a) and trained
Skip-gram on general text corpus and Yelp reviews
corpus respectively. In terms of plugging the resulting
word vectors into our recommendation model, we used
cosine distance and Frobenius norm to compute the
word distance respectively.

4.2. Word Representation Using

State-Transition Recurrent Neural
Network

We also proposed a recurrent neural network (RNN)
based model to learn word representation. One of the
disavantages of the word-word co-occurrence model is
that they can not learn the word vector based on the
semantics of the sentence. For example, “good” and
“bad” has a close cosine distance after trained by Skip-
gram. This is because these two words occurred in
many similar contexts in the training data. Thus we
turned to a modiﬁed RNN algorithm. Like existing
RNN, we learn word vectors in an unsupervised man-
ner taking the PPL of the language modeling as the
objective. In stead of taking the one-hot word code as
input, we model the semantics of the sentence using a
dense state vector. Each entry of the state vector rep-
resents a feature of the semantics evolved with words.
While each word maintains a transition matrix trans-
from the input state into the output state, evolving

s(0) = (1, 0, ..., 0)(cid:62)
s(t) = f (W (t) · s(t−1) + b(t))

(4)

(5)

To train this RNN unsupervisely, a ﬁxed matrix feed
forwards the output state layer into the prediction
layer, where the probability of the next word is repre-
sented by a one-hot vector.

y(t) = f (V · s(t))

(6)

The backpropagation steps of the model is as follows:

U (t) := U (t) + αe(t)
o

W (t) := W (t) + αe(t)

· s(t)(cid:62)
h · s(t−1)(cid:62)

where we plug following error expressions in:

o = d(t) − y(t)
e(t)
h = dh(e(t)(cid:62)
e(t)
V, t)
j (1 − s(t)
j )

o

dhj(x, t) = xs(t)

(7)

(8)

(9)

(10)

(11)

We expect this algorithm could extract deeper rela-
tionship between words, thus generating more reason-
able word vectors as long with lower PPL for the lan-
guage modeling objective. We implemented our code
based on Microsoft Research’s RNN language mod-
eling toolkit and modiﬁed almost all functions of it
according to our needs.

CS229 Fall 2014 Project Final Writeup

5. Results

5.1. Unsupervised training of Word Vectors

We ﬁrst test our model on Penn Tree Bank dataset,
which is a hall mark in the natrual language process-
ing ﬁeld. We compare our results on State-Transition
RNN with the state-of-art RNN language modeling
implemented by Microsoft Research. Then we also
trained our State-Transition RNN on the Yelp reviews
dataset (cleaned by ourselves). Since Microsoft’s RNN
has much more parameters than we used, it could take
much more time to converge.

For ST-RNN, we compared the PPL performance of
diﬀerent state sizes from 3 to 99 on Penn Tree Bank
dataset. To speed up the training and prediction,
we followed Mikolov’s approach to categorize output
words according to their frequencies in the training
corpus. The results are shown in Figure. 2. We found
that for Penn Tree Bank dataset, the optimal state
size is around 30. For larger dataset like Yelp Data
Challenge, the optimal state size is around 35.

Model
RNN
RNN
ST-RNN
ST-RNN
ST-RNN
ST-RNN (Y)

Size words/sec
100
200
15
20
30
20

20000
8000
68000
62000
44000
62000

Iters PPL
9
9
8
8
8
7

167.14
153.62
188.31
183.78
177.34
117.80

Our State-Transition RNN achieves reasonable perfor-
mance. We will add BPTT later and test our model
on larger dataset.

5.2. Similar Word Vectors

We give some word examples here to compare the per-
formance of two word embedding algorithms. Both
examples are trained on the reviews data of Yelp Data
Challenge which are cleaned by ourselves. The ﬁrst
row is the input word, and the rest are the closest
words determined by cosine distance.

Table 2. Training State-Transition RNN on reviews

duck
chicken
burrito
tofu
salmon
octopus

burger
pizza
sandwich
burrito
smoothie
pho

noodle
pasta
tofu
vegetable
seafood
ramen

good
great
wonderful
decent
bad
fantastic

Table 3. Training Skip-gram on reviews

duck
pork
squab
chicken
conﬁt
beef

burger
hamburger
cheeseburger
burgers
burgeri
buger

noodle
noodles
tofu
noodlesi
wonton
vermecelli

good
decent
great
solid
tasty
alright

Figure 2. PPL Performance of Diﬀerent State Sizes.

The size column below indicates the hidden size for
RNN and state size for ST-RNN. Training State-
Transition RNN took 15 hours on a research server
in Stanford CS Department to converge. And the last
row is ST-RNN trained on Yelp reviews.

Table 1. Training Skip-gram on general corpus

Table 4. Training Skip-gram on general corpus
duck
fethry mcdonald
daﬀy
amuck mcdonalds
eider
coot

noodle
udon
toppings
steamed
ramen
kuroda

good
bad
natured
luck
virtuous
honest

burger

kfc

schalk
restaurant

As we can see from these examples, training word
vecters on domain-speciﬁc data (Yelp reviews in our
case) has much better performance. Our word embed-
ding algorithm is doing a good job, but it still cannot
achieve the ideal result we want. It is still assigning
antoynms closest vectors. We aspire to improve this
issue in our following work.

CS229 Fall 2014 Project Final Writeup

5.3. Business Similarity

We tested our current model on a small data set of
10 categories each of which has 100 businesses, thus
1000 businesses in total. Note that for our evaluation
method, smaller the costs are better our signatures
are. We can see that use more feature words achieves
lower cost, i.e. better signatures in terms of category
consistency.

The ground truth is the labeling from the Yelp data.
We count the number of co-existing words in the two
restaurants as a measure of ground truth similarity,
then computed the similarity based of business signa-
ture. We used both Euclidean distance and Cosine
distance to measure it. We tried diﬀerent length of
feature words to concatenate together to form a sig-
nature. As shown by the below plot, the larger the
number of words is chosen the more accurate the per-
formance.

Note that the smaller the Euclidean distance means
the larger Cosine distance, in both cases business ﬁn-
gerprint evaluation vector size = 20 using word2vec
domain speciﬁc training method gives the best perfor-
mance.

5.4. Business Search

Another application of building up business ﬁngerprint
is for semantic search. Traditional search is based on
the appearance of a word, then building a inverted
index. That is bad because if a word doesn’t appear in
a document but some similar word appears, it won’t be
captured. The word vector solved this problem in that
it can ﬁnd all the words that are close to the search
word, making the search result more meaningful. It’s
no longer a word by word search, but semantic search.
Here’s a list of results returned by the search engine.

For example, in the query ”morning light food”, appar-
ently the query wants to ﬁnd a place to have breakfast.
Although the query did not explicitly say breakfast,
but ”morning” and ”lightfood” expressed this notion.
By comparing the business signature the search en-
gine found the Beach House Lounge which is exactly
a place serving breakfast. And after digging into the
reason why this restaurant get returned, it shows the
word ’breakfast’ has the closest distance 1.2018 to the
query, so it gets returned.

Another example is the query ”beer nightlife club-
bing”. The query intends to ﬁnd a place to have some
beer and enjoy the nightlife by clubbing. The busi-
ness signature captures this underlying semantic and

CS229 Fall 2014 Project Final Writeup

returned the Tip Top Tavern Bar, which is exactly a
night bar.

testbed. With our word vectors, we could classify and
search businesses in these applications as we expected.

Result
Query
”morning light food”
Beach House Lounge
”beer nightlife clubbing” Tip Top Tavern Bar
”fastfood”
”healthy vegetarian”

Cheeseburger in Paradise
Monty’s Blue Plate Diner

5.5. Business Recommendation

The last application we built on top of the business sig-
nature is business recommendation, which is an item
similarity based recommendation. The user speciﬁes
a restaurant he likes, then the system will return a
restaurant that’s similar to the speciﬁed restaurant.
Basically the recommendation system is calculating
the Euclidean distance between he business signatures
and return the one with the smallest distance.

For example, the recommendation for ”Uno Pizzeria
and Grill” is ”Benvenuto’s Italian Grill”, because they
are both selling grills. Another example is the recom-
mendation for ”Flaming Wok” is ”Gourmet House of
Hong Kong”, because they are both Chinese restau-
rants.

This business signature based recommendation system
captures the intrinsic characteristics of a restaurant
and provides a fair way to compare the similarity based
on the vector distance. Overall the system provides a
quantitative way to compare the businesses.

6. Discussion

The PPL of ST-RNNs approaches the PPL of tradi-
tional RNN, but ST-RNNs were trained at much faster
speed.
In the three business recommendation appli-
cations, ST-RNNs have similar performance with the
Skip-gram when comparing Euclidean distance, while
its performance is worse when comparing cosine dis-
tance. An explanation is that the Skip-gram algorithm
contains an inner product in its model, so the algo-
rithm is based on consine distance.

7. Conclusion

We designed state-transition RNN for word embed-
ding. Comparing with traditional RNNs and the
word2vec toolkit, our algorithm could approach the
state-of-the-art results of word embedding and lan-
guage modelling. With smaller state size, our model
could be trained much faster than traditional RNNs.
With the learned word vectors, we implemented three
applications of business ﬁngerprints generation as a

8. Future Work

The back propagation through time (BPTT) algo-
rithm did not work well for our ST-RNN. We assumed
that this kind of neural networks with self-generated
parameter matrices need other optimization algorithm
to improve the performance. For example, we may
need to use second order methods (e.g. Hessian-free
optimization). In some related works, factorizing the
parameter matrices to a product of several matrices
might also help. We expected to explore these algo-
rithms in our next step.

Acknowledgement

Some parts of this project also serves in my CS221
project: Learning Business Fingerprints from Texts.
In my CS221 project, we have another two team mem-
bers (Han Song and Charles Qi) working together. The
word representation part was all done by myself. I also
worked on the design of the three applications of gen-
erating business ﬁngerprints.

References

Mikolov, Tomas, Karaﬁ´at, Martin, Burget, Lukas,
Cernock`y, Jan, and Khudanpur, Sanjeev. Recurrent
neural network based language model. In INTER-
SPEECH, pp. 1045–1048, 2010.

Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean,
Jeﬀrey. Eﬃcient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781,
2013a.

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeﬀ. Distributed representa-
tions of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems, pp. 3111–3119, 2013b.

Sutskever, Ilya, Martens, James, and Hinton, Geof-
frey E. Generating text with recurrent neural net-
works. In Proceedings of the 28th International Con-
ference on Machine Learning (ICML-11), pp. 1017–
1024, 2011.

Taylor, Graham W, Hinton, Geoﬀrey E, and Roweis,
Sam T. Two distributed-state models for generat-
ing high-dimensional time series. The Journal of
Machine Learning Research, 12:1025–1068, 2011.

