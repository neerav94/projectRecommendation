STANFORD UNIVERSITY

1

Predicting quality of wine based on chemical

attributes

Amelia Lemionet, Yi Liu, Zhenxiang Zhou

Abstract—Ordinal data structure is difﬁcult to leverage upon
due to the fact that it has both the properties of regression and
classiﬁcation. In this paper, we devised a simple new method
that leverages upon ordinal data structure of the data: Additive
Logistic Regression (ALR). This method is used to predict the
quality of Portuguese white wine based on the chemical attributes
of the wine. We compare ALR to non-parametric methods. The
results showed that our method, albeit simple, out-performs the
non-parametric methods.

I. INTRODUCTION AND RELATED WORK

O RDINAL regression arises frequently in social sciences

and information retrieval where human preferences play
a major role [8]. Ordinal responses are comprised of both a
ranking structure of real numbers and a discrete structure of
classiﬁcation. This therefore makes the response more difﬁcult
to model than real numbers or categorical variables alone.
Work has been done recently which makes use of support
vector machines to regress on ordinal responses [8] [1], How-
ever, we feel that these method have a complex mathematical
structure and we believe that a simple and elegant solution can
be found.

The data that we decided to test our method on is from the
UCI machine learning repository. In this data, the response
is the quality of Portuguese white wine determined by wine
connoisseurs . There are altogether eleven chemical attributes
serving as potential predictors. All predictors are continuous
while the response is a categorical variable which takes values
from 1 to 10.

In this paper, we propose a new method known as an
additive logistic regression and compare its performance to
that of weighted linear regression and k-Nearest-Neighbors.
The authors acknowledge the fact that work has been done in
ﬁeld of ordinal logisitic regression (order logit)1 which tackles
the problem with similar approaches [2]. However, we like to
argue that ordinal logistic regression requires the assumption
that the separating hyperplanes between classes must be non-
intersecting. While this assumption is theoretically true, it
limits the practical application of this method. Additive logistic
regression solves this problem and therefore in practice is
a simpler method than order logit. In Section II, we will
develop the theoretical foundations of the three methods that
we will compare in this work: k-nearest neighbors regression,
weighted linear regression and additive logistic regression.

Advisor: Derek - Professor: Andrew Ng
1Ordinal logistic regression ﬁnds the probability of the point belonging
in each class and assigns the point to the class where the probability is the
highest.

In Section III, we will analyze the data through exploratory
analysis. Afterwards, in Sections IV and V, we implement,
adjust, test and compare the three methods at issue. Finally,
Section VI and Section VII exhibit the main conclusions and
open discussion to other issues of interest.

II. THEORETICAL FOUNDATIONS

Before tackling the wine-quality problem, it is important
to introduce the concepts and ideas that we will be using
throughout this work.

A. Notation and Formal statement of the problem

To be consistent, let:
• m be the number of examples in the dataset (this may
change as we divide our whole data set into training and
testing sub-sets).

• n = 11 be the number of features.
• y be the array containing ”wine quality” for each of the
• X be the m × n matrix containing one example on each

m examples.

row and one feature on each column.

Further on, we will note ˆy(i) the prediction we make of y(i).

B. K-Nearest Neighbor Classiﬁcation

One method used in ordinal classiﬁcation in the industry is
k-nearest neighbors. An ordinary k-nearest neighbors involves
ﬁnding the k nearest neighbors of the test data in the variable
space and obtain the class for the test data through majority of
votes. However, in the case of ordinal classiﬁcation, in order
to make better use of the ordinal structure of the data, we
have decided to take the mean of the responses instead of the
majority of votes. This is illustrated in the diagram below.

Fig. 1. Diagram Illustration

STANFORD UNIVERSITY

1

Predicting quality of wine based on chemical

attributes

Amelia Lemionet, Yi Liu, Zhenxiang Zhou

Abstract—Ordinal data structure is difﬁcult to leverage upon
due to the fact that it has both the properties of regression and
classiﬁcation. In this paper, we devised a simple new method
that leverages upon ordinal data structure of the data: Additive
Logistic Regression (ALR). This method is used to predict the
quality of Portuguese white wine based on the chemical attributes
of the wine. We compare ALR to non-parametric methods. The
results showed that our method, albeit simple, out-performs the
non-parametric methods.

I. INTRODUCTION AND RELATED WORK

O RDINAL regression arises frequently in social sciences

and information retrieval where human preferences play
a major role [8]. Ordinal responses are comprised of both a
ranking structure of real numbers and a discrete structure of
classiﬁcation. This therefore makes the response more difﬁcult
to model than real numbers or categorical variables alone.
Work has been done recently which makes use of support
vector machines to regress on ordinal responses [8] [1], How-
ever, we feel that these method have a complex mathematical
structure and we believe that a simple and elegant solution can
be found.

The data that we decided to test our method on is from the
UCI machine learning repository. In this data, the response
is the quality of Portuguese white wine determined by wine
connoisseurs . There are altogether eleven chemical attributes
serving as potential predictors. All predictors are continuous
while the response is a categorical variable which takes values
from 1 to 10.

In this paper, we propose a new method known as an
additive logistic regression and compare its performance to
that of weighted linear regression and k-Nearest-Neighbors.
The authors acknowledge the fact that work has been done in
ﬁeld of ordinal logisitic regression (order logit)1 which tackles
the problem with similar approaches [2]. However, we like to
argue that ordinal logistic regression requires the assumption
that the separating hyperplanes between classes must be non-
intersecting. While this assumption is theoretically true, it
limits the practical application of this method. Additive logistic
regression solves this problem and therefore in practice is
a simpler method than order logit. In Section II, we will
develop the theoretical foundations of the three methods that
we will compare in this work: k-nearest neighbors regression,
weighted linear regression and additive logistic regression.

Advisor: Derek - Professor: Andrew Ng
1Ordinal logistic regression ﬁnds the probability of the point belonging
in each class and assigns the point to the class where the probability is the
highest.

In Section III, we will analyze the data through exploratory
analysis. Afterwards, in Sections IV and V, we implement,
adjust, test and compare the three methods at issue. Finally,
Section VI and Section VII exhibit the main conclusions and
open discussion to other issues of interest.

II. THEORETICAL FOUNDATIONS

Before tackling the wine-quality problem, it is important
to introduce the concepts and ideas that we will be using
throughout this work.

A. Notation and Formal statement of the problem

To be consistent, let:
• m be the number of examples in the dataset (this may
change as we divide our whole data set into training and
testing sub-sets).

• n = 11 be the number of features.
• y be the array containing ”wine quality” for each of the
• X be the m × n matrix containing one example on each

m examples.

row and one feature on each column.

Further on, we will note ˆy(i) the prediction we make of y(i).

B. K-Nearest Neighbor Classiﬁcation

One method used in ordinal classiﬁcation in the industry is
k-nearest neighbors. An ordinary k-nearest neighbors involves
ﬁnding the k nearest neighbors of the test data in the variable
space and obtain the class for the test data through majority of
votes. However, in the case of ordinal classiﬁcation, in order
to make better use of the ordinal structure of the data, we
have decided to take the mean of the responses instead of the
majority of votes. This is illustrated in the diagram below.

Fig. 1. Diagram Illustration

STANFORD UNIVERSITY

2

Consider the star in the diagram as the test point. An the
points inside the dotted regions as the k-nearest neighbors (k =
4 in this example). Note that if we consider the majority votes
of the nearest neighbors, we would have predicted that this
is class 3 instead of class 4. However, if we take the mean,
we will have class 3.75 which is much closer to the actual
class of the model. There are various ways in which k (the
number of neighbors) can be selected. In this work we will
use cross validation over the training data and chose k such
that the cross validation mean squared error is minimized.

C. Weighted-Linear Regression

Another method we used in this project is weighted linear
regression, which is an non-parametric algorithm. It can often
be used to maximize the efﬁciency of parameter estimation
by attempting to give each data point its proper amount of
inﬂuence over the parameter estimates.
The ﬁrst weight function we used is:

W1i = exp(− (xi − x)2

2τ 2

)

(1)

Where x is the input test point. This weight function captures
the distance between the input point and all training points,
giving more weights to the training point that is closer to the
test point because we assume that similar attributes of wine
will have similar quality scores.
The second weight function we used is:
max(yi)

W2i =

yi

This weight function considers the magnitude of the response
variable, giving more weight to the training response variables
with high quality scores.
The last weight function we used is:

(cid:80)n

1
n

W3i =

i=1 frequency of yi
frequency of yi

(2)

(3)

This weight function takes into account the nature of the
distribution of the response variable. Because the quality
scores are not uniformly distributed, there are fewer cases for
the scores that are extremely low or extremely high. Hence,
this function gives more weight to the training points with low
frequency scores.

D. Additive logistic regression

We generally consider the y to take the form:

y = f (x) + 

with  a zero mean noise. Hence we can predict y as

E[y(i)|X] = f (x).

(4)
In the case of ordinal responses, we discover a special property
of y that we can leverage. To do that, we need to prove the
following lemma:
y(i) ∈ {0, 1, 2, ..., n}

Lemma 2.1: Suppose y is a multinomial variable such that

E[y(i)|X] =

P(y(i) ≥ j | X)

(5)

n(cid:88)

j=1

Proof:

E[y(i)|X] =

=

=

j=1

n(cid:88)
n(cid:88)
n(cid:88)

j=1

n(cid:88)

y(i)P(y(i) = j | X)

P(y(i) = k | X)

k=j

P(y(i) ≥ j | X).

j=1

So essentially, we are turning a multinomial classiﬁcation
problem into a series of binary classiﬁcation problems. This
allows us make use of logistic regression as a method to
estimate P(y ≥ j | X). Then we propose the following
algorithm (Algorithm 1).

Algorithm 1 Additive Logistic Regression Algorithm
1: for j = 0 to n do
Divide the Training data into y(i) ≥ j and y(i) < j
2:
Estimate P (y(i) ≥ j | x(i)) using logistic regression
3:
j = ˆP (y(i) ≥ j | x(i))
φ(i)
4:
5: end for

6: ˆy(i) =(cid:80)m

j φ(i)

j

There are two reasons why we choose logistic regression
over other methods. First, we note that logistic regression is
simple and computationally inexpensive. This is very useful
since we are producing n logistic regression models at the
same time. If we use more computationally expensive methods
such as neural-network, the methods can be too computation-
ally expensive and the time taken for model construction, even
with improvement in the results in the end, cannot be justiﬁed.
Also, using a linear decision boundary prevents overﬁtting [3]
and increases the robustness of our model. This makes logistic
regression a better choice compared to the non-linear methods.
Second, logistic regression is directly modeling P (y | X)
and does not make any assumption on the true distribution
of the response. As the model between each class is highly
correlated, it is hard to justify the use of generative models
since we have to justify why the probability distribution
remains the same from one class to another. Hence, we have
decided to forgo the idea of using generative models.

Although ˆy(i) will be a continuous real number (and not
a discrete class), this method provides an intuitive classiﬁ-
cation boundary since it classiﬁes x(i) to be in class C if

and only if ˆy(i) ∈ (cid:2)C − 1

(cid:3). 2 The reason that this

boundary is used is that we generally classify a point to
two different classes at the 0.5 point. However, like binary

2 , C + 1

2

2Note that this method differs greatly from the ordinal logistic regression
in the sense that ordinal logistic regression ﬁnds the probability of the point
belonging in each class and assigns the point to the class where the probability
is the highest. This requires the separating hyperplanes of each of the classes
to be perfectly parallel otherwise the method will fail since it might produce
negative probabilities (when the probability of the point being lower than i is
higher than the probability of being lower than (i + 1)). Our method however
does not rely on the separating hyperplanes being parallel since we are adding
the probabilities together and hence the effectiveness of its performance is not
affected by the separating hyperplanes.

STANFORD UNIVERSITY

1

Predicting quality of wine based on chemical

attributes

Amelia Lemionet, Yi Liu, Zhenxiang Zhou

Abstract—Ordinal data structure is difﬁcult to leverage upon
due to the fact that it has both the properties of regression and
classiﬁcation. In this paper, we devised a simple new method
that leverages upon ordinal data structure of the data: Additive
Logistic Regression (ALR). This method is used to predict the
quality of Portuguese white wine based on the chemical attributes
of the wine. We compare ALR to non-parametric methods. The
results showed that our method, albeit simple, out-performs the
non-parametric methods.

I. INTRODUCTION AND RELATED WORK

O RDINAL regression arises frequently in social sciences

and information retrieval where human preferences play
a major role [8]. Ordinal responses are comprised of both a
ranking structure of real numbers and a discrete structure of
classiﬁcation. This therefore makes the response more difﬁcult
to model than real numbers or categorical variables alone.
Work has been done recently which makes use of support
vector machines to regress on ordinal responses [8] [1], How-
ever, we feel that these method have a complex mathematical
structure and we believe that a simple and elegant solution can
be found.

The data that we decided to test our method on is from the
UCI machine learning repository. In this data, the response
is the quality of Portuguese white wine determined by wine
connoisseurs . There are altogether eleven chemical attributes
serving as potential predictors. All predictors are continuous
while the response is a categorical variable which takes values
from 1 to 10.

In this paper, we propose a new method known as an
additive logistic regression and compare its performance to
that of weighted linear regression and k-Nearest-Neighbors.
The authors acknowledge the fact that work has been done in
ﬁeld of ordinal logisitic regression (order logit)1 which tackles
the problem with similar approaches [2]. However, we like to
argue that ordinal logistic regression requires the assumption
that the separating hyperplanes between classes must be non-
intersecting. While this assumption is theoretically true, it
limits the practical application of this method. Additive logistic
regression solves this problem and therefore in practice is
a simpler method than order logit. In Section II, we will
develop the theoretical foundations of the three methods that
we will compare in this work: k-nearest neighbors regression,
weighted linear regression and additive logistic regression.

Advisor: Derek - Professor: Andrew Ng
1Ordinal logistic regression ﬁnds the probability of the point belonging
in each class and assigns the point to the class where the probability is the
highest.

In Section III, we will analyze the data through exploratory
analysis. Afterwards, in Sections IV and V, we implement,
adjust, test and compare the three methods at issue. Finally,
Section VI and Section VII exhibit the main conclusions and
open discussion to other issues of interest.

II. THEORETICAL FOUNDATIONS

Before tackling the wine-quality problem, it is important
to introduce the concepts and ideas that we will be using
throughout this work.

A. Notation and Formal statement of the problem

To be consistent, let:
• m be the number of examples in the dataset (this may
change as we divide our whole data set into training and
testing sub-sets).

• n = 11 be the number of features.
• y be the array containing ”wine quality” for each of the
• X be the m × n matrix containing one example on each

m examples.

row and one feature on each column.

Further on, we will note ˆy(i) the prediction we make of y(i).

B. K-Nearest Neighbor Classiﬁcation

One method used in ordinal classiﬁcation in the industry is
k-nearest neighbors. An ordinary k-nearest neighbors involves
ﬁnding the k nearest neighbors of the test data in the variable
space and obtain the class for the test data through majority of
votes. However, in the case of ordinal classiﬁcation, in order
to make better use of the ordinal structure of the data, we
have decided to take the mean of the responses instead of the
majority of votes. This is illustrated in the diagram below.

Fig. 1. Diagram Illustration

STANFORD UNIVERSITY

2

Consider the star in the diagram as the test point. An the
points inside the dotted regions as the k-nearest neighbors (k =
4 in this example). Note that if we consider the majority votes
of the nearest neighbors, we would have predicted that this
is class 3 instead of class 4. However, if we take the mean,
we will have class 3.75 which is much closer to the actual
class of the model. There are various ways in which k (the
number of neighbors) can be selected. In this work we will
use cross validation over the training data and chose k such
that the cross validation mean squared error is minimized.

C. Weighted-Linear Regression

Another method we used in this project is weighted linear
regression, which is an non-parametric algorithm. It can often
be used to maximize the efﬁciency of parameter estimation
by attempting to give each data point its proper amount of
inﬂuence over the parameter estimates.
The ﬁrst weight function we used is:

W1i = exp(− (xi − x)2

2τ 2

)

(1)

Where x is the input test point. This weight function captures
the distance between the input point and all training points,
giving more weights to the training point that is closer to the
test point because we assume that similar attributes of wine
will have similar quality scores.
The second weight function we used is:
max(yi)

W2i =

yi

This weight function considers the magnitude of the response
variable, giving more weight to the training response variables
with high quality scores.
The last weight function we used is:

(cid:80)n

1
n

W3i =

i=1 frequency of yi
frequency of yi

(2)

(3)

This weight function takes into account the nature of the
distribution of the response variable. Because the quality
scores are not uniformly distributed, there are fewer cases for
the scores that are extremely low or extremely high. Hence,
this function gives more weight to the training points with low
frequency scores.

D. Additive logistic regression

We generally consider the y to take the form:

y = f (x) + 

with  a zero mean noise. Hence we can predict y as

E[y(i)|X] = f (x).

(4)
In the case of ordinal responses, we discover a special property
of y that we can leverage. To do that, we need to prove the
following lemma:
y(i) ∈ {0, 1, 2, ..., n}

Lemma 2.1: Suppose y is a multinomial variable such that

E[y(i)|X] =

P(y(i) ≥ j | X)

(5)

n(cid:88)

j=1

Proof:

E[y(i)|X] =

=

=

j=1

n(cid:88)
n(cid:88)
n(cid:88)

j=1

n(cid:88)

y(i)P(y(i) = j | X)

P(y(i) = k | X)

k=j

P(y(i) ≥ j | X).

j=1

So essentially, we are turning a multinomial classiﬁcation
problem into a series of binary classiﬁcation problems. This
allows us make use of logistic regression as a method to
estimate P(y ≥ j | X). Then we propose the following
algorithm (Algorithm 1).

Algorithm 1 Additive Logistic Regression Algorithm
1: for j = 0 to n do
Divide the Training data into y(i) ≥ j and y(i) < j
2:
Estimate P (y(i) ≥ j | x(i)) using logistic regression
3:
j = ˆP (y(i) ≥ j | x(i))
φ(i)
4:
5: end for

6: ˆy(i) =(cid:80)m

j φ(i)

j

There are two reasons why we choose logistic regression
over other methods. First, we note that logistic regression is
simple and computationally inexpensive. This is very useful
since we are producing n logistic regression models at the
same time. If we use more computationally expensive methods
such as neural-network, the methods can be too computation-
ally expensive and the time taken for model construction, even
with improvement in the results in the end, cannot be justiﬁed.
Also, using a linear decision boundary prevents overﬁtting [3]
and increases the robustness of our model. This makes logistic
regression a better choice compared to the non-linear methods.
Second, logistic regression is directly modeling P (y | X)
and does not make any assumption on the true distribution
of the response. As the model between each class is highly
correlated, it is hard to justify the use of generative models
since we have to justify why the probability distribution
remains the same from one class to another. Hence, we have
decided to forgo the idea of using generative models.

Although ˆy(i) will be a continuous real number (and not
a discrete class), this method provides an intuitive classiﬁ-
cation boundary since it classiﬁes x(i) to be in class C if

and only if ˆy(i) ∈ (cid:2)C − 1

(cid:3). 2 The reason that this

boundary is used is that we generally classify a point to
two different classes at the 0.5 point. However, like binary

2 , C + 1

2

2Note that this method differs greatly from the ordinal logistic regression
in the sense that ordinal logistic regression ﬁnds the probability of the point
belonging in each class and assigns the point to the class where the probability
is the highest. This requires the separating hyperplanes of each of the classes
to be perfectly parallel otherwise the method will fail since it might produce
negative probabilities (when the probability of the point being lower than i is
higher than the probability of being lower than (i + 1)). Our method however
does not rely on the separating hyperplanes being parallel since we are adding
the probabilities together and hence the effectiveness of its performance is not
affected by the separating hyperplanes.

STANFORD UNIVERSITY

3

2 ).

2 , 1

classiﬁcation, there might be different cost associated with
over-estimating and under-estimating ˆy. This thought brought
us to the possibility of changing the decision boundary of

2 − δ(cid:3) where δ ∈ (− 1

yi ∈(cid:2)C − 1

2 − δ, C + 1

Before we test the method on a real data set, we decided that
we should test this method on a arbitrary data-set with no
noise. This helped us understand how the method performs in
ideal settings and is a good test of concept. As illustrated in
the diagram below, this method has proven to yield desirable
results when there is no noise in the data. In a 1-dimensional
naive case3, we can observe that the resulting model ﬁts the
data satisfactorily.
Another thing that we have to note is that in the ideal setting,
we can observe the number of points in each class is different.
The ﬁtting of the graph signals that the method is robust even
when having different number of points in each class.

occur naturally in the grapes or are created through the
fermentation process.

• Volatile acidity: a measure of steam distillable acids
present in a wine. In theory, our palates are quite sensitive
to the presence of volatile acids and for that reason a good
wine should keep their concentrations as low as possible.
• Citric acid: one of the many acids that are measured to

obtained ﬁxed acidity.

• Residual sugar: measurement of any natural grape sugars
that are leftover after fermentation ceases. In theory
residual sugar can help wines age well.

• Chlorides: the amount of salt in the wine.
• Free sulfuric dioxide: the free form of SO2 exists in
equilibrium between molecular SO2 (as a dissolved gas)
and bisulﬁte ion; it prevents microbial growth and the
oxidation of wine.

• Total sulfuric dioxide: amount of free and bound forms of
SO2; in low concentrations, SO2 is mostly undetectable
in wine, but at free SO2 concentrations over 50 ppm, SO2
becomes evident in the nose and taste of wine.

• Density: measure of density of wine.
• pH: value for pH.
• Sulfates: a wine additive which can contribute to sulfur
dioxide gas (S02) levels, which acts as an antimicrobial
and antioxidant.

• Alcohol: the percentage of alcohol present in the wine.
• Quality: subjective measurement ranging from 1 to 10

(although the observed data ranges from 3 to 8).

The following table (TABLE I) shows the summary for each
variable. Note that all variables are continuous except for the
response variable which is categorical.

Fig. 2. One dimension illustration

To the authors’ limited knowledge, while similar methods
exists [1][2][8], the algorithm proposed is not found in litera-
ture that we know about.

III. DATASET AND FEATURES

Our analyses focus in a Portuguese white wine database
consisting of 4, 898 observations. The data set contains eleven
explanatory variables that measure wine attributes and one
response variable: ”wine quality”. In more detail,

• Fixed acidity: a measurement of the total concentration
of titratable acids and free hydrogen ions present in the
wine. Theoretically, having a low acidity will result in
a ﬂat and boring wine while having too much acid can
lead to tartness or even a sour wine. These acids either

3The splitting point is generated at random

SUMMARY OF FEATURES AND RESPONSE VARIABLE

TABLE I

The histogram shown in Figure 3, shows that the values
for the response variable are not uniformly distributed, and
that we have a very high concentration of ”average” wines,
whereas very low quality and very high quality wines are
under-represented.

STANFORD UNIVERSITY

1

Predicting quality of wine based on chemical

attributes

Amelia Lemionet, Yi Liu, Zhenxiang Zhou

Abstract—Ordinal data structure is difﬁcult to leverage upon
due to the fact that it has both the properties of regression and
classiﬁcation. In this paper, we devised a simple new method
that leverages upon ordinal data structure of the data: Additive
Logistic Regression (ALR). This method is used to predict the
quality of Portuguese white wine based on the chemical attributes
of the wine. We compare ALR to non-parametric methods. The
results showed that our method, albeit simple, out-performs the
non-parametric methods.

I. INTRODUCTION AND RELATED WORK

O RDINAL regression arises frequently in social sciences

and information retrieval where human preferences play
a major role [8]. Ordinal responses are comprised of both a
ranking structure of real numbers and a discrete structure of
classiﬁcation. This therefore makes the response more difﬁcult
to model than real numbers or categorical variables alone.
Work has been done recently which makes use of support
vector machines to regress on ordinal responses [8] [1], How-
ever, we feel that these method have a complex mathematical
structure and we believe that a simple and elegant solution can
be found.

The data that we decided to test our method on is from the
UCI machine learning repository. In this data, the response
is the quality of Portuguese white wine determined by wine
connoisseurs . There are altogether eleven chemical attributes
serving as potential predictors. All predictors are continuous
while the response is a categorical variable which takes values
from 1 to 10.

In this paper, we propose a new method known as an
additive logistic regression and compare its performance to
that of weighted linear regression and k-Nearest-Neighbors.
The authors acknowledge the fact that work has been done in
ﬁeld of ordinal logisitic regression (order logit)1 which tackles
the problem with similar approaches [2]. However, we like to
argue that ordinal logistic regression requires the assumption
that the separating hyperplanes between classes must be non-
intersecting. While this assumption is theoretically true, it
limits the practical application of this method. Additive logistic
regression solves this problem and therefore in practice is
a simpler method than order logit. In Section II, we will
develop the theoretical foundations of the three methods that
we will compare in this work: k-nearest neighbors regression,
weighted linear regression and additive logistic regression.

Advisor: Derek - Professor: Andrew Ng
1Ordinal logistic regression ﬁnds the probability of the point belonging
in each class and assigns the point to the class where the probability is the
highest.

In Section III, we will analyze the data through exploratory
analysis. Afterwards, in Sections IV and V, we implement,
adjust, test and compare the three methods at issue. Finally,
Section VI and Section VII exhibit the main conclusions and
open discussion to other issues of interest.

II. THEORETICAL FOUNDATIONS

Before tackling the wine-quality problem, it is important
to introduce the concepts and ideas that we will be using
throughout this work.

A. Notation and Formal statement of the problem

To be consistent, let:
• m be the number of examples in the dataset (this may
change as we divide our whole data set into training and
testing sub-sets).

• n = 11 be the number of features.
• y be the array containing ”wine quality” for each of the
• X be the m × n matrix containing one example on each

m examples.

row and one feature on each column.

Further on, we will note ˆy(i) the prediction we make of y(i).

B. K-Nearest Neighbor Classiﬁcation

One method used in ordinal classiﬁcation in the industry is
k-nearest neighbors. An ordinary k-nearest neighbors involves
ﬁnding the k nearest neighbors of the test data in the variable
space and obtain the class for the test data through majority of
votes. However, in the case of ordinal classiﬁcation, in order
to make better use of the ordinal structure of the data, we
have decided to take the mean of the responses instead of the
majority of votes. This is illustrated in the diagram below.

Fig. 1. Diagram Illustration

STANFORD UNIVERSITY

2

Consider the star in the diagram as the test point. An the
points inside the dotted regions as the k-nearest neighbors (k =
4 in this example). Note that if we consider the majority votes
of the nearest neighbors, we would have predicted that this
is class 3 instead of class 4. However, if we take the mean,
we will have class 3.75 which is much closer to the actual
class of the model. There are various ways in which k (the
number of neighbors) can be selected. In this work we will
use cross validation over the training data and chose k such
that the cross validation mean squared error is minimized.

C. Weighted-Linear Regression

Another method we used in this project is weighted linear
regression, which is an non-parametric algorithm. It can often
be used to maximize the efﬁciency of parameter estimation
by attempting to give each data point its proper amount of
inﬂuence over the parameter estimates.
The ﬁrst weight function we used is:

W1i = exp(− (xi − x)2

2τ 2

)

(1)

Where x is the input test point. This weight function captures
the distance between the input point and all training points,
giving more weights to the training point that is closer to the
test point because we assume that similar attributes of wine
will have similar quality scores.
The second weight function we used is:
max(yi)

W2i =

yi

This weight function considers the magnitude of the response
variable, giving more weight to the training response variables
with high quality scores.
The last weight function we used is:

(cid:80)n

1
n

W3i =

i=1 frequency of yi
frequency of yi

(2)

(3)

This weight function takes into account the nature of the
distribution of the response variable. Because the quality
scores are not uniformly distributed, there are fewer cases for
the scores that are extremely low or extremely high. Hence,
this function gives more weight to the training points with low
frequency scores.

D. Additive logistic regression

We generally consider the y to take the form:

y = f (x) + 

with  a zero mean noise. Hence we can predict y as

E[y(i)|X] = f (x).

(4)
In the case of ordinal responses, we discover a special property
of y that we can leverage. To do that, we need to prove the
following lemma:
y(i) ∈ {0, 1, 2, ..., n}

Lemma 2.1: Suppose y is a multinomial variable such that

E[y(i)|X] =

P(y(i) ≥ j | X)

(5)

n(cid:88)

j=1

Proof:

E[y(i)|X] =

=

=

j=1

n(cid:88)
n(cid:88)
n(cid:88)

j=1

n(cid:88)

y(i)P(y(i) = j | X)

P(y(i) = k | X)

k=j

P(y(i) ≥ j | X).

j=1

So essentially, we are turning a multinomial classiﬁcation
problem into a series of binary classiﬁcation problems. This
allows us make use of logistic regression as a method to
estimate P(y ≥ j | X). Then we propose the following
algorithm (Algorithm 1).

Algorithm 1 Additive Logistic Regression Algorithm
1: for j = 0 to n do
Divide the Training data into y(i) ≥ j and y(i) < j
2:
Estimate P (y(i) ≥ j | x(i)) using logistic regression
3:
j = ˆP (y(i) ≥ j | x(i))
φ(i)
4:
5: end for

6: ˆy(i) =(cid:80)m

j φ(i)

j

There are two reasons why we choose logistic regression
over other methods. First, we note that logistic regression is
simple and computationally inexpensive. This is very useful
since we are producing n logistic regression models at the
same time. If we use more computationally expensive methods
such as neural-network, the methods can be too computation-
ally expensive and the time taken for model construction, even
with improvement in the results in the end, cannot be justiﬁed.
Also, using a linear decision boundary prevents overﬁtting [3]
and increases the robustness of our model. This makes logistic
regression a better choice compared to the non-linear methods.
Second, logistic regression is directly modeling P (y | X)
and does not make any assumption on the true distribution
of the response. As the model between each class is highly
correlated, it is hard to justify the use of generative models
since we have to justify why the probability distribution
remains the same from one class to another. Hence, we have
decided to forgo the idea of using generative models.

Although ˆy(i) will be a continuous real number (and not
a discrete class), this method provides an intuitive classiﬁ-
cation boundary since it classiﬁes x(i) to be in class C if

and only if ˆy(i) ∈ (cid:2)C − 1

(cid:3). 2 The reason that this

boundary is used is that we generally classify a point to
two different classes at the 0.5 point. However, like binary

2 , C + 1

2

2Note that this method differs greatly from the ordinal logistic regression
in the sense that ordinal logistic regression ﬁnds the probability of the point
belonging in each class and assigns the point to the class where the probability
is the highest. This requires the separating hyperplanes of each of the classes
to be perfectly parallel otherwise the method will fail since it might produce
negative probabilities (when the probability of the point being lower than i is
higher than the probability of being lower than (i + 1)). Our method however
does not rely on the separating hyperplanes being parallel since we are adding
the probabilities together and hence the effectiveness of its performance is not
affected by the separating hyperplanes.

STANFORD UNIVERSITY

3

2 ).

2 , 1

classiﬁcation, there might be different cost associated with
over-estimating and under-estimating ˆy. This thought brought
us to the possibility of changing the decision boundary of

2 − δ(cid:3) where δ ∈ (− 1

yi ∈(cid:2)C − 1

2 − δ, C + 1

Before we test the method on a real data set, we decided that
we should test this method on a arbitrary data-set with no
noise. This helped us understand how the method performs in
ideal settings and is a good test of concept. As illustrated in
the diagram below, this method has proven to yield desirable
results when there is no noise in the data. In a 1-dimensional
naive case3, we can observe that the resulting model ﬁts the
data satisfactorily.
Another thing that we have to note is that in the ideal setting,
we can observe the number of points in each class is different.
The ﬁtting of the graph signals that the method is robust even
when having different number of points in each class.

occur naturally in the grapes or are created through the
fermentation process.

• Volatile acidity: a measure of steam distillable acids
present in a wine. In theory, our palates are quite sensitive
to the presence of volatile acids and for that reason a good
wine should keep their concentrations as low as possible.
• Citric acid: one of the many acids that are measured to

obtained ﬁxed acidity.

• Residual sugar: measurement of any natural grape sugars
that are leftover after fermentation ceases. In theory
residual sugar can help wines age well.

• Chlorides: the amount of salt in the wine.
• Free sulfuric dioxide: the free form of SO2 exists in
equilibrium between molecular SO2 (as a dissolved gas)
and bisulﬁte ion; it prevents microbial growth and the
oxidation of wine.

• Total sulfuric dioxide: amount of free and bound forms of
SO2; in low concentrations, SO2 is mostly undetectable
in wine, but at free SO2 concentrations over 50 ppm, SO2
becomes evident in the nose and taste of wine.

• Density: measure of density of wine.
• pH: value for pH.
• Sulfates: a wine additive which can contribute to sulfur
dioxide gas (S02) levels, which acts as an antimicrobial
and antioxidant.

• Alcohol: the percentage of alcohol present in the wine.
• Quality: subjective measurement ranging from 1 to 10

(although the observed data ranges from 3 to 8).

The following table (TABLE I) shows the summary for each
variable. Note that all variables are continuous except for the
response variable which is categorical.

Fig. 2. One dimension illustration

To the authors’ limited knowledge, while similar methods
exists [1][2][8], the algorithm proposed is not found in litera-
ture that we know about.

III. DATASET AND FEATURES

Our analyses focus in a Portuguese white wine database
consisting of 4, 898 observations. The data set contains eleven
explanatory variables that measure wine attributes and one
response variable: ”wine quality”. In more detail,

• Fixed acidity: a measurement of the total concentration
of titratable acids and free hydrogen ions present in the
wine. Theoretically, having a low acidity will result in
a ﬂat and boring wine while having too much acid can
lead to tartness or even a sour wine. These acids either

3The splitting point is generated at random

SUMMARY OF FEATURES AND RESPONSE VARIABLE

TABLE I

The histogram shown in Figure 3, shows that the values
for the response variable are not uniformly distributed, and
that we have a very high concentration of ”average” wines,
whereas very low quality and very high quality wines are
under-represented.

STANFORD UNIVERSITY

4

weight function (1), there are different choices for the tuning
parameter τ, which is the bandwidth parameter. This parameter
controls how quickly the weight of a training point drops as a
function of the distance between xi and the test point x. The
larger τ is, the wider the bandwidth around the test point x.
To determine the optimal value for τ, we used 10-fold cross
validation on the training set and found τ = 10 after taking
the minimum CV error.

Fig. 3. Distribution of values for ”wine quality”.

IV. FITTING THE ALGORITHMS

To test the algorithm, we ﬁrst divided the data into a test set
and a training set. We set aside 898 data-points as the ultimate
test set and the rest of the data points (4, 000) as the training
set. Since 4, 000 data points does not constitute a giant data
set, we used cross-validation to ﬁnd the respective optimal
tuning parameters for k-nearest neighbors and weighted linear
regression.

We did not normalize the data because most of the data are
concentrations measured at the same level and hence the scale
is important to the data set.

A. K-Nearest Neighbors

The ﬁrst step in adjusting the k-nearest neighbors model
was to ﬁx the number of neighbors k. For that, we used 10-
fold cross validation and chose k such that the CV residual
mean squared error (RMSE) is minimized. This yielded to a
result of k = 9.

To adapt the mean k-nearest neighbors regression to our
ordinal data, we rounded the resulting value to obtain a integer
number.

Fig. 5. Error rate for different weight functions. Normal linear regression
for reference.

C. Additive Logistic Regression

It is important to note that the smallest class is supposed to
be 1. In this case, however, the smallest class is 3 which is not
ideal. However, a simple solution is to do a parallel shift to the
response variable in order to make a prediction and then shift
the response back after we make the prediction. This method
is mathematically justiﬁed since adding a constant does not
change the probability.4

V. RESULTS

In classiﬁcation, one of the key attributes that we consider
is the test error rate. In this case, since we are not dealing with
binary classiﬁcation, it is not possible to use the ROC curve
or AUC as a criteria to assess our performance. However, in
the case of ordinal classiﬁcation, we can leverage the ordinal
nature of the response to consider not only the error rate, but
also the percentage of response that is under-estimated and
percentage of response that is over-estimated. In this case, the
number of times we predicted that the wine had worse quality
than it actually has or vice-versa. These are the three key
assessment criteria that we will be looking at.

Fig. 4.
(RMSE).

k = 9 minimizes the cross validation residual mean squared error

Fig. 6. Additive Logistic Regression minimizes the test error. The Error
represents how many times we miss-classiﬁed an observation. The results
shown for weighted linear regression (WLR) are the ones corresponding to
the weights W1 (τ = 10).

B. Weighted-Linear Regression

In weighted linear regression, since it is a regression al-
gorithm and produces quantitative outputs, we rounded the
outputs to the nearest integers to make our ﬁnal predictions.
In weight function (2),(3), it is quite straightforward to ﬁt the
model because the weights only depend on the training set
and there are no tuning parameters to choose. However, in

To analyze the previous table, it is important to keep in mind
the ”naive” scenario. As seen in Fig. 3, the naive predictor
would classify all wines as being quality 6 since that is the
mode class. If we were to use the naive predictor, the error
rate would be of ∼ 55%. Compared to the naive scenario, all
three methods represent a better alternative.

4i.e.E(x + a) = E(x) + a

STANFORD UNIVERSITY

1

Predicting quality of wine based on chemical

attributes

Amelia Lemionet, Yi Liu, Zhenxiang Zhou

Abstract—Ordinal data structure is difﬁcult to leverage upon
due to the fact that it has both the properties of regression and
classiﬁcation. In this paper, we devised a simple new method
that leverages upon ordinal data structure of the data: Additive
Logistic Regression (ALR). This method is used to predict the
quality of Portuguese white wine based on the chemical attributes
of the wine. We compare ALR to non-parametric methods. The
results showed that our method, albeit simple, out-performs the
non-parametric methods.

I. INTRODUCTION AND RELATED WORK

O RDINAL regression arises frequently in social sciences

and information retrieval where human preferences play
a major role [8]. Ordinal responses are comprised of both a
ranking structure of real numbers and a discrete structure of
classiﬁcation. This therefore makes the response more difﬁcult
to model than real numbers or categorical variables alone.
Work has been done recently which makes use of support
vector machines to regress on ordinal responses [8] [1], How-
ever, we feel that these method have a complex mathematical
structure and we believe that a simple and elegant solution can
be found.

The data that we decided to test our method on is from the
UCI machine learning repository. In this data, the response
is the quality of Portuguese white wine determined by wine
connoisseurs . There are altogether eleven chemical attributes
serving as potential predictors. All predictors are continuous
while the response is a categorical variable which takes values
from 1 to 10.

In this paper, we propose a new method known as an
additive logistic regression and compare its performance to
that of weighted linear regression and k-Nearest-Neighbors.
The authors acknowledge the fact that work has been done in
ﬁeld of ordinal logisitic regression (order logit)1 which tackles
the problem with similar approaches [2]. However, we like to
argue that ordinal logistic regression requires the assumption
that the separating hyperplanes between classes must be non-
intersecting. While this assumption is theoretically true, it
limits the practical application of this method. Additive logistic
regression solves this problem and therefore in practice is
a simpler method than order logit. In Section II, we will
develop the theoretical foundations of the three methods that
we will compare in this work: k-nearest neighbors regression,
weighted linear regression and additive logistic regression.

Advisor: Derek - Professor: Andrew Ng
1Ordinal logistic regression ﬁnds the probability of the point belonging
in each class and assigns the point to the class where the probability is the
highest.

In Section III, we will analyze the data through exploratory
analysis. Afterwards, in Sections IV and V, we implement,
adjust, test and compare the three methods at issue. Finally,
Section VI and Section VII exhibit the main conclusions and
open discussion to other issues of interest.

II. THEORETICAL FOUNDATIONS

Before tackling the wine-quality problem, it is important
to introduce the concepts and ideas that we will be using
throughout this work.

A. Notation and Formal statement of the problem

To be consistent, let:
• m be the number of examples in the dataset (this may
change as we divide our whole data set into training and
testing sub-sets).

• n = 11 be the number of features.
• y be the array containing ”wine quality” for each of the
• X be the m × n matrix containing one example on each

m examples.

row and one feature on each column.

Further on, we will note ˆy(i) the prediction we make of y(i).

B. K-Nearest Neighbor Classiﬁcation

One method used in ordinal classiﬁcation in the industry is
k-nearest neighbors. An ordinary k-nearest neighbors involves
ﬁnding the k nearest neighbors of the test data in the variable
space and obtain the class for the test data through majority of
votes. However, in the case of ordinal classiﬁcation, in order
to make better use of the ordinal structure of the data, we
have decided to take the mean of the responses instead of the
majority of votes. This is illustrated in the diagram below.

Fig. 1. Diagram Illustration

STANFORD UNIVERSITY

2

Consider the star in the diagram as the test point. An the
points inside the dotted regions as the k-nearest neighbors (k =
4 in this example). Note that if we consider the majority votes
of the nearest neighbors, we would have predicted that this
is class 3 instead of class 4. However, if we take the mean,
we will have class 3.75 which is much closer to the actual
class of the model. There are various ways in which k (the
number of neighbors) can be selected. In this work we will
use cross validation over the training data and chose k such
that the cross validation mean squared error is minimized.

C. Weighted-Linear Regression

Another method we used in this project is weighted linear
regression, which is an non-parametric algorithm. It can often
be used to maximize the efﬁciency of parameter estimation
by attempting to give each data point its proper amount of
inﬂuence over the parameter estimates.
The ﬁrst weight function we used is:

W1i = exp(− (xi − x)2

2τ 2

)

(1)

Where x is the input test point. This weight function captures
the distance between the input point and all training points,
giving more weights to the training point that is closer to the
test point because we assume that similar attributes of wine
will have similar quality scores.
The second weight function we used is:
max(yi)

W2i =

yi

This weight function considers the magnitude of the response
variable, giving more weight to the training response variables
with high quality scores.
The last weight function we used is:

(cid:80)n

1
n

W3i =

i=1 frequency of yi
frequency of yi

(2)

(3)

This weight function takes into account the nature of the
distribution of the response variable. Because the quality
scores are not uniformly distributed, there are fewer cases for
the scores that are extremely low or extremely high. Hence,
this function gives more weight to the training points with low
frequency scores.

D. Additive logistic regression

We generally consider the y to take the form:

y = f (x) + 

with  a zero mean noise. Hence we can predict y as

E[y(i)|X] = f (x).

(4)
In the case of ordinal responses, we discover a special property
of y that we can leverage. To do that, we need to prove the
following lemma:
y(i) ∈ {0, 1, 2, ..., n}

Lemma 2.1: Suppose y is a multinomial variable such that

E[y(i)|X] =

P(y(i) ≥ j | X)

(5)

n(cid:88)

j=1

Proof:

E[y(i)|X] =

=

=

j=1

n(cid:88)
n(cid:88)
n(cid:88)

j=1

n(cid:88)

y(i)P(y(i) = j | X)

P(y(i) = k | X)

k=j

P(y(i) ≥ j | X).

j=1

So essentially, we are turning a multinomial classiﬁcation
problem into a series of binary classiﬁcation problems. This
allows us make use of logistic regression as a method to
estimate P(y ≥ j | X). Then we propose the following
algorithm (Algorithm 1).

Algorithm 1 Additive Logistic Regression Algorithm
1: for j = 0 to n do
Divide the Training data into y(i) ≥ j and y(i) < j
2:
Estimate P (y(i) ≥ j | x(i)) using logistic regression
3:
j = ˆP (y(i) ≥ j | x(i))
φ(i)
4:
5: end for

6: ˆy(i) =(cid:80)m

j φ(i)

j

There are two reasons why we choose logistic regression
over other methods. First, we note that logistic regression is
simple and computationally inexpensive. This is very useful
since we are producing n logistic regression models at the
same time. If we use more computationally expensive methods
such as neural-network, the methods can be too computation-
ally expensive and the time taken for model construction, even
with improvement in the results in the end, cannot be justiﬁed.
Also, using a linear decision boundary prevents overﬁtting [3]
and increases the robustness of our model. This makes logistic
regression a better choice compared to the non-linear methods.
Second, logistic regression is directly modeling P (y | X)
and does not make any assumption on the true distribution
of the response. As the model between each class is highly
correlated, it is hard to justify the use of generative models
since we have to justify why the probability distribution
remains the same from one class to another. Hence, we have
decided to forgo the idea of using generative models.

Although ˆy(i) will be a continuous real number (and not
a discrete class), this method provides an intuitive classiﬁ-
cation boundary since it classiﬁes x(i) to be in class C if

and only if ˆy(i) ∈ (cid:2)C − 1

(cid:3). 2 The reason that this

boundary is used is that we generally classify a point to
two different classes at the 0.5 point. However, like binary

2 , C + 1

2

2Note that this method differs greatly from the ordinal logistic regression
in the sense that ordinal logistic regression ﬁnds the probability of the point
belonging in each class and assigns the point to the class where the probability
is the highest. This requires the separating hyperplanes of each of the classes
to be perfectly parallel otherwise the method will fail since it might produce
negative probabilities (when the probability of the point being lower than i is
higher than the probability of being lower than (i + 1)). Our method however
does not rely on the separating hyperplanes being parallel since we are adding
the probabilities together and hence the effectiveness of its performance is not
affected by the separating hyperplanes.

STANFORD UNIVERSITY

3

2 ).

2 , 1

classiﬁcation, there might be different cost associated with
over-estimating and under-estimating ˆy. This thought brought
us to the possibility of changing the decision boundary of

2 − δ(cid:3) where δ ∈ (− 1

yi ∈(cid:2)C − 1

2 − δ, C + 1

Before we test the method on a real data set, we decided that
we should test this method on a arbitrary data-set with no
noise. This helped us understand how the method performs in
ideal settings and is a good test of concept. As illustrated in
the diagram below, this method has proven to yield desirable
results when there is no noise in the data. In a 1-dimensional
naive case3, we can observe that the resulting model ﬁts the
data satisfactorily.
Another thing that we have to note is that in the ideal setting,
we can observe the number of points in each class is different.
The ﬁtting of the graph signals that the method is robust even
when having different number of points in each class.

occur naturally in the grapes or are created through the
fermentation process.

• Volatile acidity: a measure of steam distillable acids
present in a wine. In theory, our palates are quite sensitive
to the presence of volatile acids and for that reason a good
wine should keep their concentrations as low as possible.
• Citric acid: one of the many acids that are measured to

obtained ﬁxed acidity.

• Residual sugar: measurement of any natural grape sugars
that are leftover after fermentation ceases. In theory
residual sugar can help wines age well.

• Chlorides: the amount of salt in the wine.
• Free sulfuric dioxide: the free form of SO2 exists in
equilibrium between molecular SO2 (as a dissolved gas)
and bisulﬁte ion; it prevents microbial growth and the
oxidation of wine.

• Total sulfuric dioxide: amount of free and bound forms of
SO2; in low concentrations, SO2 is mostly undetectable
in wine, but at free SO2 concentrations over 50 ppm, SO2
becomes evident in the nose and taste of wine.

• Density: measure of density of wine.
• pH: value for pH.
• Sulfates: a wine additive which can contribute to sulfur
dioxide gas (S02) levels, which acts as an antimicrobial
and antioxidant.

• Alcohol: the percentage of alcohol present in the wine.
• Quality: subjective measurement ranging from 1 to 10

(although the observed data ranges from 3 to 8).

The following table (TABLE I) shows the summary for each
variable. Note that all variables are continuous except for the
response variable which is categorical.

Fig. 2. One dimension illustration

To the authors’ limited knowledge, while similar methods
exists [1][2][8], the algorithm proposed is not found in litera-
ture that we know about.

III. DATASET AND FEATURES

Our analyses focus in a Portuguese white wine database
consisting of 4, 898 observations. The data set contains eleven
explanatory variables that measure wine attributes and one
response variable: ”wine quality”. In more detail,

• Fixed acidity: a measurement of the total concentration
of titratable acids and free hydrogen ions present in the
wine. Theoretically, having a low acidity will result in
a ﬂat and boring wine while having too much acid can
lead to tartness or even a sour wine. These acids either

3The splitting point is generated at random

SUMMARY OF FEATURES AND RESPONSE VARIABLE

TABLE I

The histogram shown in Figure 3, shows that the values
for the response variable are not uniformly distributed, and
that we have a very high concentration of ”average” wines,
whereas very low quality and very high quality wines are
under-represented.

STANFORD UNIVERSITY

4

weight function (1), there are different choices for the tuning
parameter τ, which is the bandwidth parameter. This parameter
controls how quickly the weight of a training point drops as a
function of the distance between xi and the test point x. The
larger τ is, the wider the bandwidth around the test point x.
To determine the optimal value for τ, we used 10-fold cross
validation on the training set and found τ = 10 after taking
the minimum CV error.

Fig. 3. Distribution of values for ”wine quality”.

IV. FITTING THE ALGORITHMS

To test the algorithm, we ﬁrst divided the data into a test set
and a training set. We set aside 898 data-points as the ultimate
test set and the rest of the data points (4, 000) as the training
set. Since 4, 000 data points does not constitute a giant data
set, we used cross-validation to ﬁnd the respective optimal
tuning parameters for k-nearest neighbors and weighted linear
regression.

We did not normalize the data because most of the data are
concentrations measured at the same level and hence the scale
is important to the data set.

A. K-Nearest Neighbors

The ﬁrst step in adjusting the k-nearest neighbors model
was to ﬁx the number of neighbors k. For that, we used 10-
fold cross validation and chose k such that the CV residual
mean squared error (RMSE) is minimized. This yielded to a
result of k = 9.

To adapt the mean k-nearest neighbors regression to our
ordinal data, we rounded the resulting value to obtain a integer
number.

Fig. 5. Error rate for different weight functions. Normal linear regression
for reference.

C. Additive Logistic Regression

It is important to note that the smallest class is supposed to
be 1. In this case, however, the smallest class is 3 which is not
ideal. However, a simple solution is to do a parallel shift to the
response variable in order to make a prediction and then shift
the response back after we make the prediction. This method
is mathematically justiﬁed since adding a constant does not
change the probability.4

V. RESULTS

In classiﬁcation, one of the key attributes that we consider
is the test error rate. In this case, since we are not dealing with
binary classiﬁcation, it is not possible to use the ROC curve
or AUC as a criteria to assess our performance. However, in
the case of ordinal classiﬁcation, we can leverage the ordinal
nature of the response to consider not only the error rate, but
also the percentage of response that is under-estimated and
percentage of response that is over-estimated. In this case, the
number of times we predicted that the wine had worse quality
than it actually has or vice-versa. These are the three key
assessment criteria that we will be looking at.

Fig. 4.
(RMSE).

k = 9 minimizes the cross validation residual mean squared error

Fig. 6. Additive Logistic Regression minimizes the test error. The Error
represents how many times we miss-classiﬁed an observation. The results
shown for weighted linear regression (WLR) are the ones corresponding to
the weights W1 (τ = 10).

B. Weighted-Linear Regression

In weighted linear regression, since it is a regression al-
gorithm and produces quantitative outputs, we rounded the
outputs to the nearest integers to make our ﬁnal predictions.
In weight function (2),(3), it is quite straightforward to ﬁt the
model because the weights only depend on the training set
and there are no tuning parameters to choose. However, in

To analyze the previous table, it is important to keep in mind
the ”naive” scenario. As seen in Fig. 3, the naive predictor
would classify all wines as being quality 6 since that is the
mode class. If we were to use the naive predictor, the error
rate would be of ∼ 55%. Compared to the naive scenario, all
three methods represent a better alternative.

4i.e.E(x + a) = E(x) + a

STANFORD UNIVERSITY

5

Now, when we compare the three methods against each
other, additive logistic regression outperforms both k-nearest
neighbors and weighted linear regression. Regarding the per-
centage of over and underestimation, all methods seem to have
a tendency to over estimate the response variable.

Fig. 7. Predicted values respect order in response variable but most values
are concentrated near 6.

VI. DISCUSSION

In light of all of the methods that we have applied, we
are encouraged by the fact that the method that we proposed
is performing much better than the other methods. In this
Section, we will discuss in detail some of the potential reasons
for this. It is surprising that additive logistic regression actually
performs much better than k-nearest neighbor classiﬁcation
as k-nearest neighbor can be highly ﬂexible[3]. In this case,
we believe that additive logistic regression does better at
leveraging the ordinal structure of the data and hence produces
better results.

As for weighted linear regression, we need to note that
weighted linear regression performs well when the number of
predictors is small [3]. In the case of 10 variables, the predictor
space may be too sparse to generate good results. (This can
also be explained by the curse of dimensionality).

In addition to better accuracy in prediction, we like to intro-
duce an additional beneﬁt using additive logistic regression: its
good interpretability. As we go through the ranks, we notice
that the importance of each of variables can change. We will
illustrate this in the case of our wine data.
We consider the logistic regression model of P (y ≥ 5 | X)

Fig. 8. The coefﬁcients for the model P (y ≥ 5 | X)

Notice that the coefﬁcient of ﬁxed acidity in this logistic
regression is -0.837105. Fixed acidity in this case is also very
important since its absolute z-value is large.
However, consider the logistic regression model of P (y ≥
7 | x)

Fig. 9. The coefﬁcients for the model P (y ≥ 7 | X)

We notice that

the coefﬁcient for ﬁxed acidity here is
actually positive and close to zero. This means that the impact
of ﬁxed acidity is changing from one class to the other. Due
to the special nature of logistic regression, we will be able to
observe these changes and therefore peek into the nature of
wine tasting through looking at these coefﬁcients. This highly
interpretability is not available for the other non-parametric
methods that we used in this paper.

In addition, we need to observe that the result separating
hyperplane for each class is far from parallel. Although this
is counter intuitive, we need to note that that might be other
unknown variables (In this case, the year and brand of the
wine) which, when working together with our variable, could
produce a more or less parallel hyperplane. However, if we use
probability of the test point being in a class as the criteria of
assigning into that class. (i.e. assign to class with the highest
probability) We might face the problem of having negative
probability which is not mathematically justiﬁed. Additive
logistic regression circumscribe this problem and make sure
that the predicted results lies within the range of the responses.
However, we would also like to discuss one key part that
is lost in additive logistic regression. We have lost the log-
odds interpretation of each coefﬁcient, meaning that we cannot
translate the coefﬁcients directly to probability.

VII. CONCLUSIONS AND FUTURE WORK

In conclusion, additive logistic regression proves to be
more accurate method in terms of prediction accuracy and
model interpretability. This is mainly due to the fact that
additive logistic regression leverages ordinal nature of the data.
However, we still believe that more work can be done in
the future. We can add bagging procedure into our methods.
Also more work can be done on picking the classiﬁcation
boundary instead of the naive one that we have tried. We
believe that such methods would improve the results in terms
of prediction accuracy. In addition, we can also treat additive
logistic regression as part of general additive models [3]. This
might further improves the predictability of our model.

STANFORD UNIVERSITY

1

Predicting quality of wine based on chemical

attributes

Amelia Lemionet, Yi Liu, Zhenxiang Zhou

Abstract—Ordinal data structure is difﬁcult to leverage upon
due to the fact that it has both the properties of regression and
classiﬁcation. In this paper, we devised a simple new method
that leverages upon ordinal data structure of the data: Additive
Logistic Regression (ALR). This method is used to predict the
quality of Portuguese white wine based on the chemical attributes
of the wine. We compare ALR to non-parametric methods. The
results showed that our method, albeit simple, out-performs the
non-parametric methods.

I. INTRODUCTION AND RELATED WORK

O RDINAL regression arises frequently in social sciences

and information retrieval where human preferences play
a major role [8]. Ordinal responses are comprised of both a
ranking structure of real numbers and a discrete structure of
classiﬁcation. This therefore makes the response more difﬁcult
to model than real numbers or categorical variables alone.
Work has been done recently which makes use of support
vector machines to regress on ordinal responses [8] [1], How-
ever, we feel that these method have a complex mathematical
structure and we believe that a simple and elegant solution can
be found.

The data that we decided to test our method on is from the
UCI machine learning repository. In this data, the response
is the quality of Portuguese white wine determined by wine
connoisseurs . There are altogether eleven chemical attributes
serving as potential predictors. All predictors are continuous
while the response is a categorical variable which takes values
from 1 to 10.

In this paper, we propose a new method known as an
additive logistic regression and compare its performance to
that of weighted linear regression and k-Nearest-Neighbors.
The authors acknowledge the fact that work has been done in
ﬁeld of ordinal logisitic regression (order logit)1 which tackles
the problem with similar approaches [2]. However, we like to
argue that ordinal logistic regression requires the assumption
that the separating hyperplanes between classes must be non-
intersecting. While this assumption is theoretically true, it
limits the practical application of this method. Additive logistic
regression solves this problem and therefore in practice is
a simpler method than order logit. In Section II, we will
develop the theoretical foundations of the three methods that
we will compare in this work: k-nearest neighbors regression,
weighted linear regression and additive logistic regression.

Advisor: Derek - Professor: Andrew Ng
1Ordinal logistic regression ﬁnds the probability of the point belonging
in each class and assigns the point to the class where the probability is the
highest.

In Section III, we will analyze the data through exploratory
analysis. Afterwards, in Sections IV and V, we implement,
adjust, test and compare the three methods at issue. Finally,
Section VI and Section VII exhibit the main conclusions and
open discussion to other issues of interest.

II. THEORETICAL FOUNDATIONS

Before tackling the wine-quality problem, it is important
to introduce the concepts and ideas that we will be using
throughout this work.

A. Notation and Formal statement of the problem

To be consistent, let:
• m be the number of examples in the dataset (this may
change as we divide our whole data set into training and
testing sub-sets).

• n = 11 be the number of features.
• y be the array containing ”wine quality” for each of the
• X be the m × n matrix containing one example on each

m examples.

row and one feature on each column.

Further on, we will note ˆy(i) the prediction we make of y(i).

B. K-Nearest Neighbor Classiﬁcation

One method used in ordinal classiﬁcation in the industry is
k-nearest neighbors. An ordinary k-nearest neighbors involves
ﬁnding the k nearest neighbors of the test data in the variable
space and obtain the class for the test data through majority of
votes. However, in the case of ordinal classiﬁcation, in order
to make better use of the ordinal structure of the data, we
have decided to take the mean of the responses instead of the
majority of votes. This is illustrated in the diagram below.

Fig. 1. Diagram Illustration

STANFORD UNIVERSITY

2

Consider the star in the diagram as the test point. An the
points inside the dotted regions as the k-nearest neighbors (k =
4 in this example). Note that if we consider the majority votes
of the nearest neighbors, we would have predicted that this
is class 3 instead of class 4. However, if we take the mean,
we will have class 3.75 which is much closer to the actual
class of the model. There are various ways in which k (the
number of neighbors) can be selected. In this work we will
use cross validation over the training data and chose k such
that the cross validation mean squared error is minimized.

C. Weighted-Linear Regression

Another method we used in this project is weighted linear
regression, which is an non-parametric algorithm. It can often
be used to maximize the efﬁciency of parameter estimation
by attempting to give each data point its proper amount of
inﬂuence over the parameter estimates.
The ﬁrst weight function we used is:

W1i = exp(− (xi − x)2

2τ 2

)

(1)

Where x is the input test point. This weight function captures
the distance between the input point and all training points,
giving more weights to the training point that is closer to the
test point because we assume that similar attributes of wine
will have similar quality scores.
The second weight function we used is:
max(yi)

W2i =

yi

This weight function considers the magnitude of the response
variable, giving more weight to the training response variables
with high quality scores.
The last weight function we used is:

(cid:80)n

1
n

W3i =

i=1 frequency of yi
frequency of yi

(2)

(3)

This weight function takes into account the nature of the
distribution of the response variable. Because the quality
scores are not uniformly distributed, there are fewer cases for
the scores that are extremely low or extremely high. Hence,
this function gives more weight to the training points with low
frequency scores.

D. Additive logistic regression

We generally consider the y to take the form:

y = f (x) + 

with  a zero mean noise. Hence we can predict y as

E[y(i)|X] = f (x).

(4)
In the case of ordinal responses, we discover a special property
of y that we can leverage. To do that, we need to prove the
following lemma:
y(i) ∈ {0, 1, 2, ..., n}

Lemma 2.1: Suppose y is a multinomial variable such that

E[y(i)|X] =

P(y(i) ≥ j | X)

(5)

n(cid:88)

j=1

Proof:

E[y(i)|X] =

=

=

j=1

n(cid:88)
n(cid:88)
n(cid:88)

j=1

n(cid:88)

y(i)P(y(i) = j | X)

P(y(i) = k | X)

k=j

P(y(i) ≥ j | X).

j=1

So essentially, we are turning a multinomial classiﬁcation
problem into a series of binary classiﬁcation problems. This
allows us make use of logistic regression as a method to
estimate P(y ≥ j | X). Then we propose the following
algorithm (Algorithm 1).

Algorithm 1 Additive Logistic Regression Algorithm
1: for j = 0 to n do
Divide the Training data into y(i) ≥ j and y(i) < j
2:
Estimate P (y(i) ≥ j | x(i)) using logistic regression
3:
j = ˆP (y(i) ≥ j | x(i))
φ(i)
4:
5: end for

6: ˆy(i) =(cid:80)m

j φ(i)

j

There are two reasons why we choose logistic regression
over other methods. First, we note that logistic regression is
simple and computationally inexpensive. This is very useful
since we are producing n logistic regression models at the
same time. If we use more computationally expensive methods
such as neural-network, the methods can be too computation-
ally expensive and the time taken for model construction, even
with improvement in the results in the end, cannot be justiﬁed.
Also, using a linear decision boundary prevents overﬁtting [3]
and increases the robustness of our model. This makes logistic
regression a better choice compared to the non-linear methods.
Second, logistic regression is directly modeling P (y | X)
and does not make any assumption on the true distribution
of the response. As the model between each class is highly
correlated, it is hard to justify the use of generative models
since we have to justify why the probability distribution
remains the same from one class to another. Hence, we have
decided to forgo the idea of using generative models.

Although ˆy(i) will be a continuous real number (and not
a discrete class), this method provides an intuitive classiﬁ-
cation boundary since it classiﬁes x(i) to be in class C if

and only if ˆy(i) ∈ (cid:2)C − 1

(cid:3). 2 The reason that this

boundary is used is that we generally classify a point to
two different classes at the 0.5 point. However, like binary

2 , C + 1

2

2Note that this method differs greatly from the ordinal logistic regression
in the sense that ordinal logistic regression ﬁnds the probability of the point
belonging in each class and assigns the point to the class where the probability
is the highest. This requires the separating hyperplanes of each of the classes
to be perfectly parallel otherwise the method will fail since it might produce
negative probabilities (when the probability of the point being lower than i is
higher than the probability of being lower than (i + 1)). Our method however
does not rely on the separating hyperplanes being parallel since we are adding
the probabilities together and hence the effectiveness of its performance is not
affected by the separating hyperplanes.

STANFORD UNIVERSITY

3

2 ).

2 , 1

classiﬁcation, there might be different cost associated with
over-estimating and under-estimating ˆy. This thought brought
us to the possibility of changing the decision boundary of

2 − δ(cid:3) where δ ∈ (− 1

yi ∈(cid:2)C − 1

2 − δ, C + 1

Before we test the method on a real data set, we decided that
we should test this method on a arbitrary data-set with no
noise. This helped us understand how the method performs in
ideal settings and is a good test of concept. As illustrated in
the diagram below, this method has proven to yield desirable
results when there is no noise in the data. In a 1-dimensional
naive case3, we can observe that the resulting model ﬁts the
data satisfactorily.
Another thing that we have to note is that in the ideal setting,
we can observe the number of points in each class is different.
The ﬁtting of the graph signals that the method is robust even
when having different number of points in each class.

occur naturally in the grapes or are created through the
fermentation process.

• Volatile acidity: a measure of steam distillable acids
present in a wine. In theory, our palates are quite sensitive
to the presence of volatile acids and for that reason a good
wine should keep their concentrations as low as possible.
• Citric acid: one of the many acids that are measured to

obtained ﬁxed acidity.

• Residual sugar: measurement of any natural grape sugars
that are leftover after fermentation ceases. In theory
residual sugar can help wines age well.

• Chlorides: the amount of salt in the wine.
• Free sulfuric dioxide: the free form of SO2 exists in
equilibrium between molecular SO2 (as a dissolved gas)
and bisulﬁte ion; it prevents microbial growth and the
oxidation of wine.

• Total sulfuric dioxide: amount of free and bound forms of
SO2; in low concentrations, SO2 is mostly undetectable
in wine, but at free SO2 concentrations over 50 ppm, SO2
becomes evident in the nose and taste of wine.

• Density: measure of density of wine.
• pH: value for pH.
• Sulfates: a wine additive which can contribute to sulfur
dioxide gas (S02) levels, which acts as an antimicrobial
and antioxidant.

• Alcohol: the percentage of alcohol present in the wine.
• Quality: subjective measurement ranging from 1 to 10

(although the observed data ranges from 3 to 8).

The following table (TABLE I) shows the summary for each
variable. Note that all variables are continuous except for the
response variable which is categorical.

Fig. 2. One dimension illustration

To the authors’ limited knowledge, while similar methods
exists [1][2][8], the algorithm proposed is not found in litera-
ture that we know about.

III. DATASET AND FEATURES

Our analyses focus in a Portuguese white wine database
consisting of 4, 898 observations. The data set contains eleven
explanatory variables that measure wine attributes and one
response variable: ”wine quality”. In more detail,

• Fixed acidity: a measurement of the total concentration
of titratable acids and free hydrogen ions present in the
wine. Theoretically, having a low acidity will result in
a ﬂat and boring wine while having too much acid can
lead to tartness or even a sour wine. These acids either

3The splitting point is generated at random

SUMMARY OF FEATURES AND RESPONSE VARIABLE

TABLE I

The histogram shown in Figure 3, shows that the values
for the response variable are not uniformly distributed, and
that we have a very high concentration of ”average” wines,
whereas very low quality and very high quality wines are
under-represented.

STANFORD UNIVERSITY

4

weight function (1), there are different choices for the tuning
parameter τ, which is the bandwidth parameter. This parameter
controls how quickly the weight of a training point drops as a
function of the distance between xi and the test point x. The
larger τ is, the wider the bandwidth around the test point x.
To determine the optimal value for τ, we used 10-fold cross
validation on the training set and found τ = 10 after taking
the minimum CV error.

Fig. 3. Distribution of values for ”wine quality”.

IV. FITTING THE ALGORITHMS

To test the algorithm, we ﬁrst divided the data into a test set
and a training set. We set aside 898 data-points as the ultimate
test set and the rest of the data points (4, 000) as the training
set. Since 4, 000 data points does not constitute a giant data
set, we used cross-validation to ﬁnd the respective optimal
tuning parameters for k-nearest neighbors and weighted linear
regression.

We did not normalize the data because most of the data are
concentrations measured at the same level and hence the scale
is important to the data set.

A. K-Nearest Neighbors

The ﬁrst step in adjusting the k-nearest neighbors model
was to ﬁx the number of neighbors k. For that, we used 10-
fold cross validation and chose k such that the CV residual
mean squared error (RMSE) is minimized. This yielded to a
result of k = 9.

To adapt the mean k-nearest neighbors regression to our
ordinal data, we rounded the resulting value to obtain a integer
number.

Fig. 5. Error rate for different weight functions. Normal linear regression
for reference.

C. Additive Logistic Regression

It is important to note that the smallest class is supposed to
be 1. In this case, however, the smallest class is 3 which is not
ideal. However, a simple solution is to do a parallel shift to the
response variable in order to make a prediction and then shift
the response back after we make the prediction. This method
is mathematically justiﬁed since adding a constant does not
change the probability.4

V. RESULTS

In classiﬁcation, one of the key attributes that we consider
is the test error rate. In this case, since we are not dealing with
binary classiﬁcation, it is not possible to use the ROC curve
or AUC as a criteria to assess our performance. However, in
the case of ordinal classiﬁcation, we can leverage the ordinal
nature of the response to consider not only the error rate, but
also the percentage of response that is under-estimated and
percentage of response that is over-estimated. In this case, the
number of times we predicted that the wine had worse quality
than it actually has or vice-versa. These are the three key
assessment criteria that we will be looking at.

Fig. 4.
(RMSE).

k = 9 minimizes the cross validation residual mean squared error

Fig. 6. Additive Logistic Regression minimizes the test error. The Error
represents how many times we miss-classiﬁed an observation. The results
shown for weighted linear regression (WLR) are the ones corresponding to
the weights W1 (τ = 10).

B. Weighted-Linear Regression

In weighted linear regression, since it is a regression al-
gorithm and produces quantitative outputs, we rounded the
outputs to the nearest integers to make our ﬁnal predictions.
In weight function (2),(3), it is quite straightforward to ﬁt the
model because the weights only depend on the training set
and there are no tuning parameters to choose. However, in

To analyze the previous table, it is important to keep in mind
the ”naive” scenario. As seen in Fig. 3, the naive predictor
would classify all wines as being quality 6 since that is the
mode class. If we were to use the naive predictor, the error
rate would be of ∼ 55%. Compared to the naive scenario, all
three methods represent a better alternative.

4i.e.E(x + a) = E(x) + a

STANFORD UNIVERSITY

5

Now, when we compare the three methods against each
other, additive logistic regression outperforms both k-nearest
neighbors and weighted linear regression. Regarding the per-
centage of over and underestimation, all methods seem to have
a tendency to over estimate the response variable.

Fig. 7. Predicted values respect order in response variable but most values
are concentrated near 6.

VI. DISCUSSION

In light of all of the methods that we have applied, we
are encouraged by the fact that the method that we proposed
is performing much better than the other methods. In this
Section, we will discuss in detail some of the potential reasons
for this. It is surprising that additive logistic regression actually
performs much better than k-nearest neighbor classiﬁcation
as k-nearest neighbor can be highly ﬂexible[3]. In this case,
we believe that additive logistic regression does better at
leveraging the ordinal structure of the data and hence produces
better results.

As for weighted linear regression, we need to note that
weighted linear regression performs well when the number of
predictors is small [3]. In the case of 10 variables, the predictor
space may be too sparse to generate good results. (This can
also be explained by the curse of dimensionality).

In addition to better accuracy in prediction, we like to intro-
duce an additional beneﬁt using additive logistic regression: its
good interpretability. As we go through the ranks, we notice
that the importance of each of variables can change. We will
illustrate this in the case of our wine data.
We consider the logistic regression model of P (y ≥ 5 | X)

Fig. 8. The coefﬁcients for the model P (y ≥ 5 | X)

Notice that the coefﬁcient of ﬁxed acidity in this logistic
regression is -0.837105. Fixed acidity in this case is also very
important since its absolute z-value is large.
However, consider the logistic regression model of P (y ≥
7 | x)

Fig. 9. The coefﬁcients for the model P (y ≥ 7 | X)

We notice that

the coefﬁcient for ﬁxed acidity here is
actually positive and close to zero. This means that the impact
of ﬁxed acidity is changing from one class to the other. Due
to the special nature of logistic regression, we will be able to
observe these changes and therefore peek into the nature of
wine tasting through looking at these coefﬁcients. This highly
interpretability is not available for the other non-parametric
methods that we used in this paper.

In addition, we need to observe that the result separating
hyperplane for each class is far from parallel. Although this
is counter intuitive, we need to note that that might be other
unknown variables (In this case, the year and brand of the
wine) which, when working together with our variable, could
produce a more or less parallel hyperplane. However, if we use
probability of the test point being in a class as the criteria of
assigning into that class. (i.e. assign to class with the highest
probability) We might face the problem of having negative
probability which is not mathematically justiﬁed. Additive
logistic regression circumscribe this problem and make sure
that the predicted results lies within the range of the responses.
However, we would also like to discuss one key part that
is lost in additive logistic regression. We have lost the log-
odds interpretation of each coefﬁcient, meaning that we cannot
translate the coefﬁcients directly to probability.

VII. CONCLUSIONS AND FUTURE WORK

In conclusion, additive logistic regression proves to be
more accurate method in terms of prediction accuracy and
model interpretability. This is mainly due to the fact that
additive logistic regression leverages ordinal nature of the data.
However, we still believe that more work can be done in
the future. We can add bagging procedure into our methods.
Also more work can be done on picking the classiﬁcation
boundary instead of the naive one that we have tried. We
believe that such methods would improve the results in terms
of prediction accuracy. In addition, we can also treat additive
logistic regression as part of general additive models [3]. This
might further improves the predictability of our model.

STANFORD UNIVERSITY

6

REFERENCES

[1] Jaime S. Cardoso, Joaquim F. Pinto da Costa. Learning to Classify
Ordinal Data: The Data Replication Method Journal of Machine Learning
Research 8 (2007) 1393-1429

[2] Harmen J. Dikkers, Support vector machines in ordinal classiﬁcation, A
revision of the abn amro corporate credit rating system August 2005
http://www.kbs.twi.tudelft.nl/docs/MSc/2005/Dikkers H J/thesis.pdf.

[3] Trevor Hastie, Robert Tibshirani, Jerome Friedman Elements of Statistical
Learning:Data Mining, Inference, and Prediction.. Springer, 2nd edition,
2009.

[4] UC Irvine Machine Learning Repository http://archive.ics.uci.edu/ml/.
[5] Winemaker’s

understanding

Academy,

acidity.

wine

http:

//winemakersacademy.com/understanding-wine-acidity/.

[6] Wine Jargon: What

is Residual Sugar?. http://drinks.seriouseats.com/

2013/04/wine-jargon-what-is-residual-sugar-riesling-fermentation-steven-grubbs.
html.

[7] Red and White Wine Quality https://rpubs.com/Daria/57835.
[8] Wei Chu, S. Sathiya Keerthi, Support Vector Ordinal Regression http:

//www.gatsby.ucl.ac.uk/∼chuwei/paper/svor.pdf

[9] Brian Ripley [aut, cre, cph], William Venables [cph] class: Functions for

Classiﬁcation http://www.stats.ox.ac.uk/pub/MASS4/

[10] Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani, ISLR:
Data for An Introduction to Statistical Learning with Applications in R,
http://www.StatLearning.com

[11] Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre
Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, Zachary Mayer,
Brenton Kenkel, the R Core Team, Michael Benesty, Reynald Lescarbeau,
Andrew Ziem, Luca Scrucca, Yuan Tang, and Can Candan., caret:
Classiﬁcation and Regression Training, https://github.com/topepo/caret/

