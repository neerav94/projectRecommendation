Acoustic Identiﬁcation of Cardiomyocytes

Alex Lemon

1

Introduction
Pluripotent stem cells can be made to differentiate into cardiomyocytes (heart-muscle cells). However,
there are three different types of cardiomyocytes: atrial, ventricular, and nodal, and it is difﬁcult to force
cells to differentiate into a particular type. In therapeutic applications it is extremely important to use the
correct type of cardiomyocytes – implanting the wrong type of cell will not restore function as desired,
and may lead to cancer. Thus, it is extremely important to be able to accurately identify different types
of cardiomyocytes. Moreover, the identiﬁcation process must be minimally invasive if the cells are to be
subsequently used for therapy. In this paper we use noninvasive acoustic measurements of heart-muscle
cells to classify the cells as atrial, ventricular, or nodal; we apply several classiﬁcation methods to the data,
including K-nearest-neighbors, multinomial regression, discriminant analysis, support-vector machines,
and tree-based classiﬁers.
2 Related work

Junyi et al showed that the different types of cardiomyocytes can be identiﬁed using the electrical sig-
nals associated with action potentials [4]. These measurements are obtained using an invasive technique
called patch clamping, which renders the cells unﬁt for subsequent therapeutic use. More recently a team
of researchers at Stanford has been using a photonic-crystal hydrophone to take acoustic measurements
of murine cardiomyocytes. (This work is currently unpublished, although technical details of the mea-
surement device are available [3].) Catherine Jan (a doctoral student in the Department of Electrical En-
gineering, Stanford University) and Sally Kim (a postdoctoral researcher in the Department of Psychiatry,
Stanford University) provided their acoustic measurements for our analysis.
3 Data and preprocessing

The data set consists of thirty-four time traces ranging in length from thirty seconds to two minutes.
The original data was sampled at 10 kHz, while the features of interest occur in a frequency range that is an
order of magnitude slower. I used 5× downsampling with averaging to reduce the size of the data, while
preserving the features of interest.
After downsampling the data, I developed a method for extracting the individual pulses. This method

was based on three assumptions about the nature of the data.

(1) Each pulse can be approximated as a triangle wave of width w = 10 ms; we let φt denote the triangle

wave that starts at time t (see ﬁgure 1).

(2) The pulses corresponding to different cells are added to give the observed signal.

different pulses cannot cancel each other out.)

(In particular,

(3) Pulses begin at relatively few of the time sample points.

Figure 1 – a triangle wave

Based on these assumptions, we propose extracting the individual pulses by solving the following opti-
mization problem:

where ρ > 0 is a regularization parameter. Intuitively, we approximate the time trace as a weighted sum
of triangle waves. The ﬁrst term in the objective is the approximation error, which we want to be small.

(cid:80)
τ ατ φτ(cid:107)2
: (cid:107)x −
minimize
subject to : α ≥ 0,

α

2 + ρ(cid:107)α(cid:107)1

1

t1t+wAcoustic Identiﬁcation of Cardiomyocytes

Alex Lemon

1

Introduction
Pluripotent stem cells can be made to differentiate into cardiomyocytes (heart-muscle cells). However,
there are three different types of cardiomyocytes: atrial, ventricular, and nodal, and it is difﬁcult to force
cells to differentiate into a particular type. In therapeutic applications it is extremely important to use the
correct type of cardiomyocytes – implanting the wrong type of cell will not restore function as desired,
and may lead to cancer. Thus, it is extremely important to be able to accurately identify different types
of cardiomyocytes. Moreover, the identiﬁcation process must be minimally invasive if the cells are to be
subsequently used for therapy. In this paper we use noninvasive acoustic measurements of heart-muscle
cells to classify the cells as atrial, ventricular, or nodal; we apply several classiﬁcation methods to the data,
including K-nearest-neighbors, multinomial regression, discriminant analysis, support-vector machines,
and tree-based classiﬁers.
2 Related work

Junyi et al showed that the different types of cardiomyocytes can be identiﬁed using the electrical sig-
nals associated with action potentials [4]. These measurements are obtained using an invasive technique
called patch clamping, which renders the cells unﬁt for subsequent therapeutic use. More recently a team
of researchers at Stanford has been using a photonic-crystal hydrophone to take acoustic measurements
of murine cardiomyocytes. (This work is currently unpublished, although technical details of the mea-
surement device are available [3].) Catherine Jan (a doctoral student in the Department of Electrical En-
gineering, Stanford University) and Sally Kim (a postdoctoral researcher in the Department of Psychiatry,
Stanford University) provided their acoustic measurements for our analysis.
3 Data and preprocessing

The data set consists of thirty-four time traces ranging in length from thirty seconds to two minutes.
The original data was sampled at 10 kHz, while the features of interest occur in a frequency range that is an
order of magnitude slower. I used 5× downsampling with averaging to reduce the size of the data, while
preserving the features of interest.
After downsampling the data, I developed a method for extracting the individual pulses. This method

was based on three assumptions about the nature of the data.

(1) Each pulse can be approximated as a triangle wave of width w = 10 ms; we let φt denote the triangle

wave that starts at time t (see ﬁgure 1).

(2) The pulses corresponding to different cells are added to give the observed signal.

different pulses cannot cancel each other out.)

(In particular,

(3) Pulses begin at relatively few of the time sample points.

Figure 1 – a triangle wave

Based on these assumptions, we propose extracting the individual pulses by solving the following opti-
mization problem:

where ρ > 0 is a regularization parameter. Intuitively, we approximate the time trace as a weighted sum
of triangle waves. The ﬁrst term in the objective is the approximation error, which we want to be small.

(cid:80)
τ ατ φτ(cid:107)2
: (cid:107)x −
minimize
subject to : α ≥ 0,

α

2 + ρ(cid:107)α(cid:107)1

1

t1t+wThe penalty term (cid:107)α(cid:107)1 is used to promote sparsity, corresponding to the assumption that pulses start at
relatively few of the time sample points. The constraint α ≥ 0 models the assumption that the measured
signal is obtained by adding the pulses corresponding to different cells, and there is no cancellation. Thus,
the optimization problem above is a form of sparse, nonnegative regression. An example of the results are
shown in ﬁgure 2, where we see that the method is effective at extracting the pulses.

Figure 2 – approximating the measured signal using triangle waves

Having extracted the pulses from the measurements, we manually labeled 1220 observations. Repre-
sentative examples of the pulses corresponding to the three different types of cells are given in ﬁgure 3. We
labeled all of the observations twice in order to assess the quality of our labels. The two labels differed for
approximately one quarter of the examples, indicating that the problem is difﬁcult, and suggesting that an
error rate of about 25 % should be considered a success.

(a) atrial

(b) nodal

(c) ventricular

Figure 3 – pulses corresponding to the three different types of cardiomyocytes

4 Classiﬁcation methods

The following classiﬁcation methods were applied to the data. More detailed explanations of these

methods are given in James et al [2] and Hastie et al [1].

(a) K-nearest neighbors. In this method we classify an observation with predictor vector x0 by identifying
the K training examples whose predictor vectors are closest to x0, and using a majority vote among
these neighboring training examples. (We used the Euclidean distance to measure closeness, but it is
also possible to use other metrics.) The parameter K must be chosen to balance bias (which increases
with K) and variance (which decreases with K).

(b) Multinomial logistic regression. An extension of the usual two-class logistic regression, K-class multi-

nomial logistic regression assumes that the conditional class probabilities are

P(Y = k | X) =
P(Y = K | X) =

1 X)

exp(βT
k=1 exp(βT

k X)

k=1 exp(βT

k X)

1 +(cid:80)K−1
1 +(cid:80)K−1

1

2

k = 1, . . . K − 1,

,

,

signal,x(t)time,t00.20.40.60.81−0.1−0.0500.050.10.150.20.25Acoustic Identiﬁcation of Cardiomyocytes

Alex Lemon

1

Introduction
Pluripotent stem cells can be made to differentiate into cardiomyocytes (heart-muscle cells). However,
there are three different types of cardiomyocytes: atrial, ventricular, and nodal, and it is difﬁcult to force
cells to differentiate into a particular type. In therapeutic applications it is extremely important to use the
correct type of cardiomyocytes – implanting the wrong type of cell will not restore function as desired,
and may lead to cancer. Thus, it is extremely important to be able to accurately identify different types
of cardiomyocytes. Moreover, the identiﬁcation process must be minimally invasive if the cells are to be
subsequently used for therapy. In this paper we use noninvasive acoustic measurements of heart-muscle
cells to classify the cells as atrial, ventricular, or nodal; we apply several classiﬁcation methods to the data,
including K-nearest-neighbors, multinomial regression, discriminant analysis, support-vector machines,
and tree-based classiﬁers.
2 Related work

Junyi et al showed that the different types of cardiomyocytes can be identiﬁed using the electrical sig-
nals associated with action potentials [4]. These measurements are obtained using an invasive technique
called patch clamping, which renders the cells unﬁt for subsequent therapeutic use. More recently a team
of researchers at Stanford has been using a photonic-crystal hydrophone to take acoustic measurements
of murine cardiomyocytes. (This work is currently unpublished, although technical details of the mea-
surement device are available [3].) Catherine Jan (a doctoral student in the Department of Electrical En-
gineering, Stanford University) and Sally Kim (a postdoctoral researcher in the Department of Psychiatry,
Stanford University) provided their acoustic measurements for our analysis.
3 Data and preprocessing

The data set consists of thirty-four time traces ranging in length from thirty seconds to two minutes.
The original data was sampled at 10 kHz, while the features of interest occur in a frequency range that is an
order of magnitude slower. I used 5× downsampling with averaging to reduce the size of the data, while
preserving the features of interest.
After downsampling the data, I developed a method for extracting the individual pulses. This method

was based on three assumptions about the nature of the data.

(1) Each pulse can be approximated as a triangle wave of width w = 10 ms; we let φt denote the triangle

wave that starts at time t (see ﬁgure 1).

(2) The pulses corresponding to different cells are added to give the observed signal.

different pulses cannot cancel each other out.)

(In particular,

(3) Pulses begin at relatively few of the time sample points.

Figure 1 – a triangle wave

Based on these assumptions, we propose extracting the individual pulses by solving the following opti-
mization problem:

where ρ > 0 is a regularization parameter. Intuitively, we approximate the time trace as a weighted sum
of triangle waves. The ﬁrst term in the objective is the approximation error, which we want to be small.

(cid:80)
τ ατ φτ(cid:107)2
: (cid:107)x −
minimize
subject to : α ≥ 0,

α

2 + ρ(cid:107)α(cid:107)1

1

t1t+wThe penalty term (cid:107)α(cid:107)1 is used to promote sparsity, corresponding to the assumption that pulses start at
relatively few of the time sample points. The constraint α ≥ 0 models the assumption that the measured
signal is obtained by adding the pulses corresponding to different cells, and there is no cancellation. Thus,
the optimization problem above is a form of sparse, nonnegative regression. An example of the results are
shown in ﬁgure 2, where we see that the method is effective at extracting the pulses.

Figure 2 – approximating the measured signal using triangle waves

Having extracted the pulses from the measurements, we manually labeled 1220 observations. Repre-
sentative examples of the pulses corresponding to the three different types of cells are given in ﬁgure 3. We
labeled all of the observations twice in order to assess the quality of our labels. The two labels differed for
approximately one quarter of the examples, indicating that the problem is difﬁcult, and suggesting that an
error rate of about 25 % should be considered a success.

(a) atrial

(b) nodal

(c) ventricular

Figure 3 – pulses corresponding to the three different types of cardiomyocytes

4 Classiﬁcation methods

The following classiﬁcation methods were applied to the data. More detailed explanations of these

methods are given in James et al [2] and Hastie et al [1].

(a) K-nearest neighbors. In this method we classify an observation with predictor vector x0 by identifying
the K training examples whose predictor vectors are closest to x0, and using a majority vote among
these neighboring training examples. (We used the Euclidean distance to measure closeness, but it is
also possible to use other metrics.) The parameter K must be chosen to balance bias (which increases
with K) and variance (which decreases with K).

(b) Multinomial logistic regression. An extension of the usual two-class logistic regression, K-class multi-

nomial logistic regression assumes that the conditional class probabilities are

P(Y = k | X) =
P(Y = K | X) =

1 X)

exp(βT
k=1 exp(βT

k X)

k=1 exp(βT

k X)

1 +(cid:80)K−1
1 +(cid:80)K−1

1

2

k = 1, . . . K − 1,

,

,

signal,x(t)time,t00.20.40.60.81−0.1−0.0500.050.10.150.20.25where β1, . . . , βK−1 are parameters, which we can ﬁt using the method of maximum likelihood.

(c) Support-vector machine (SVM). A support-vector machine partitions the predictor space in order to
maximize the distance of the observations from the decision boundaries, while keeping the classiﬁca-
tion errors small. More concretely, we ﬁt an SVM by solving the following optimization problem:

maximize

βj , i, M
subject to :

: M

(cid:80)p
yi(β0 +(cid:80)p
(cid:80)n

j=1 β2

j = 1

j=1 βjxj) ≥ M (1 − i)

i ≥ 0
i=1 i ≤ C.

Important considerations for SVMs are the kernel (that is, the measure of similarity between predic-
tors) and the cost parameter C. In addition, because our classiﬁcation problem has more than two
classes, we need to decide whether to use one-versus-one comparisons or one-versus-all compar-
isons. (In a one-versus-one comparison, we ﬁt an SVM for all pairs of classes, and predict the class
that appeared most in the pairwise classiﬁcations; in a one-versus-all comparison, we ﬁt an SVM for
each class against all of the other classes, and predict the class with the highest conﬁdence.) We used
the standard radial kernel and one-versus-alls comparisons in this project.

(d) Discriminant analysis. In discriminant analysis we assume that the predictors from each class come
from a multivariate normal distribution, with possibly different parameters for each class. We esti-
mate the distribution parameters using their sample analogs (that is, the sample mean and sample
variance), and we classify a new observation to the class with the highest probability density at the
observed value of the predictor vector.

(i) Linear discriminant analysis (LDA). If we assume that the variance matrix is the same for all classes,

then we obtain linear decision boundaries.

(ii) Quadratic discriminant analysis (QDA). If we assume that the variance matrix is different for dif-
ferent classes, then we obtain quadratic decision boundaries. We tend to obtain better results
with QDA than LDA if the variance matrix differs substantially across classes, and worse results
otherwise.

(e) Tree-based methods.

(i) Classiﬁcation trees. In a classiﬁcation tree, we recursively partition the training set using binary
decisions based on the predictors. In order to classify a new observation, we apply the same
sequence of binary decisions to the predictors of the new observation, and label the observation
using majority vote when we reach a terminal node. Trees are very prone to overﬁtting, which
can be ameliorated by tuning the number of terminal nodes.

(ii) Random forests. A random forest is an ensemble of classiﬁcation trees. The trees in the ensemble
are decorrelated by bootstrapping the data used to generate the trees, and randomly selecting
the predictors that can be used for decisions at each node. Classiﬁcation is performed using a
majority vote of the trees in the ensemble. Important parameters are the number of trees in the
forest, and the number of predictors selected at each node.

(iii) Boosting. Like random forests, boosting also uses an ensemble of trees. However, instead of
producing a decorrelated ensemble, boosting instead uses subsequent trees to ﬁt the errors in
previous trees. The key parameters are the number of trees and the learning rate.

5 Results

All algorithm parameters were chosen using ten-fold cross-validation repeated ten times (that is, with
ten different random groupings for cross-validation; repeating cross-validation gives better estimates of the
error, and decreases the standard error of these estimates). In particular, we chose the number of neighbors
for K-nearest neighbors, the number of terminal nodes in the classiﬁcation tree, the number of predictors

3

Acoustic Identiﬁcation of Cardiomyocytes

Alex Lemon

1

Introduction
Pluripotent stem cells can be made to differentiate into cardiomyocytes (heart-muscle cells). However,
there are three different types of cardiomyocytes: atrial, ventricular, and nodal, and it is difﬁcult to force
cells to differentiate into a particular type. In therapeutic applications it is extremely important to use the
correct type of cardiomyocytes – implanting the wrong type of cell will not restore function as desired,
and may lead to cancer. Thus, it is extremely important to be able to accurately identify different types
of cardiomyocytes. Moreover, the identiﬁcation process must be minimally invasive if the cells are to be
subsequently used for therapy. In this paper we use noninvasive acoustic measurements of heart-muscle
cells to classify the cells as atrial, ventricular, or nodal; we apply several classiﬁcation methods to the data,
including K-nearest-neighbors, multinomial regression, discriminant analysis, support-vector machines,
and tree-based classiﬁers.
2 Related work

Junyi et al showed that the different types of cardiomyocytes can be identiﬁed using the electrical sig-
nals associated with action potentials [4]. These measurements are obtained using an invasive technique
called patch clamping, which renders the cells unﬁt for subsequent therapeutic use. More recently a team
of researchers at Stanford has been using a photonic-crystal hydrophone to take acoustic measurements
of murine cardiomyocytes. (This work is currently unpublished, although technical details of the mea-
surement device are available [3].) Catherine Jan (a doctoral student in the Department of Electrical En-
gineering, Stanford University) and Sally Kim (a postdoctoral researcher in the Department of Psychiatry,
Stanford University) provided their acoustic measurements for our analysis.
3 Data and preprocessing

The data set consists of thirty-four time traces ranging in length from thirty seconds to two minutes.
The original data was sampled at 10 kHz, while the features of interest occur in a frequency range that is an
order of magnitude slower. I used 5× downsampling with averaging to reduce the size of the data, while
preserving the features of interest.
After downsampling the data, I developed a method for extracting the individual pulses. This method

was based on three assumptions about the nature of the data.

(1) Each pulse can be approximated as a triangle wave of width w = 10 ms; we let φt denote the triangle

wave that starts at time t (see ﬁgure 1).

(2) The pulses corresponding to different cells are added to give the observed signal.

different pulses cannot cancel each other out.)

(In particular,

(3) Pulses begin at relatively few of the time sample points.

Figure 1 – a triangle wave

Based on these assumptions, we propose extracting the individual pulses by solving the following opti-
mization problem:

where ρ > 0 is a regularization parameter. Intuitively, we approximate the time trace as a weighted sum
of triangle waves. The ﬁrst term in the objective is the approximation error, which we want to be small.

(cid:80)
τ ατ φτ(cid:107)2
: (cid:107)x −
minimize
subject to : α ≥ 0,

α

2 + ρ(cid:107)α(cid:107)1

1

t1t+wThe penalty term (cid:107)α(cid:107)1 is used to promote sparsity, corresponding to the assumption that pulses start at
relatively few of the time sample points. The constraint α ≥ 0 models the assumption that the measured
signal is obtained by adding the pulses corresponding to different cells, and there is no cancellation. Thus,
the optimization problem above is a form of sparse, nonnegative regression. An example of the results are
shown in ﬁgure 2, where we see that the method is effective at extracting the pulses.

Figure 2 – approximating the measured signal using triangle waves

Having extracted the pulses from the measurements, we manually labeled 1220 observations. Repre-
sentative examples of the pulses corresponding to the three different types of cells are given in ﬁgure 3. We
labeled all of the observations twice in order to assess the quality of our labels. The two labels differed for
approximately one quarter of the examples, indicating that the problem is difﬁcult, and suggesting that an
error rate of about 25 % should be considered a success.

(a) atrial

(b) nodal

(c) ventricular

Figure 3 – pulses corresponding to the three different types of cardiomyocytes

4 Classiﬁcation methods

The following classiﬁcation methods were applied to the data. More detailed explanations of these

methods are given in James et al [2] and Hastie et al [1].

(a) K-nearest neighbors. In this method we classify an observation with predictor vector x0 by identifying
the K training examples whose predictor vectors are closest to x0, and using a majority vote among
these neighboring training examples. (We used the Euclidean distance to measure closeness, but it is
also possible to use other metrics.) The parameter K must be chosen to balance bias (which increases
with K) and variance (which decreases with K).

(b) Multinomial logistic regression. An extension of the usual two-class logistic regression, K-class multi-

nomial logistic regression assumes that the conditional class probabilities are

P(Y = k | X) =
P(Y = K | X) =

1 X)

exp(βT
k=1 exp(βT

k X)

k=1 exp(βT

k X)

1 +(cid:80)K−1
1 +(cid:80)K−1

1

2

k = 1, . . . K − 1,

,

,

signal,x(t)time,t00.20.40.60.81−0.1−0.0500.050.10.150.20.25where β1, . . . , βK−1 are parameters, which we can ﬁt using the method of maximum likelihood.

(c) Support-vector machine (SVM). A support-vector machine partitions the predictor space in order to
maximize the distance of the observations from the decision boundaries, while keeping the classiﬁca-
tion errors small. More concretely, we ﬁt an SVM by solving the following optimization problem:

maximize

βj , i, M
subject to :

: M

(cid:80)p
yi(β0 +(cid:80)p
(cid:80)n

j=1 β2

j = 1

j=1 βjxj) ≥ M (1 − i)

i ≥ 0
i=1 i ≤ C.

Important considerations for SVMs are the kernel (that is, the measure of similarity between predic-
tors) and the cost parameter C. In addition, because our classiﬁcation problem has more than two
classes, we need to decide whether to use one-versus-one comparisons or one-versus-all compar-
isons. (In a one-versus-one comparison, we ﬁt an SVM for all pairs of classes, and predict the class
that appeared most in the pairwise classiﬁcations; in a one-versus-all comparison, we ﬁt an SVM for
each class against all of the other classes, and predict the class with the highest conﬁdence.) We used
the standard radial kernel and one-versus-alls comparisons in this project.

(d) Discriminant analysis. In discriminant analysis we assume that the predictors from each class come
from a multivariate normal distribution, with possibly different parameters for each class. We esti-
mate the distribution parameters using their sample analogs (that is, the sample mean and sample
variance), and we classify a new observation to the class with the highest probability density at the
observed value of the predictor vector.

(i) Linear discriminant analysis (LDA). If we assume that the variance matrix is the same for all classes,

then we obtain linear decision boundaries.

(ii) Quadratic discriminant analysis (QDA). If we assume that the variance matrix is different for dif-
ferent classes, then we obtain quadratic decision boundaries. We tend to obtain better results
with QDA than LDA if the variance matrix differs substantially across classes, and worse results
otherwise.

(e) Tree-based methods.

(i) Classiﬁcation trees. In a classiﬁcation tree, we recursively partition the training set using binary
decisions based on the predictors. In order to classify a new observation, we apply the same
sequence of binary decisions to the predictors of the new observation, and label the observation
using majority vote when we reach a terminal node. Trees are very prone to overﬁtting, which
can be ameliorated by tuning the number of terminal nodes.

(ii) Random forests. A random forest is an ensemble of classiﬁcation trees. The trees in the ensemble
are decorrelated by bootstrapping the data used to generate the trees, and randomly selecting
the predictors that can be used for decisions at each node. Classiﬁcation is performed using a
majority vote of the trees in the ensemble. Important parameters are the number of trees in the
forest, and the number of predictors selected at each node.

(iii) Boosting. Like random forests, boosting also uses an ensemble of trees. However, instead of
producing a decorrelated ensemble, boosting instead uses subsequent trees to ﬁt the errors in
previous trees. The key parameters are the number of trees and the learning rate.

5 Results

All algorithm parameters were chosen using ten-fold cross-validation repeated ten times (that is, with
ten different random groupings for cross-validation; repeating cross-validation gives better estimates of the
error, and decreases the standard error of these estimates). In particular, we chose the number of neighbors
for K-nearest neighbors, the number of terminal nodes in the classiﬁcation tree, the number of predictors

3

chosen for each node in the random forest, and the number of trees in the boosting ensemble. The cross-
validation curves are shown in ﬁgure 4. The vertical red lines in these plots mark the minimum error, while
the horizontal red lines are one standard error above the minimum. The ﬁnal ﬁtted models were chosen
using the one-standard-error rule: that is, we selected the simplest model that was within one standard
error of the minimum cross-validation error.

Performance metrics for the different classiﬁcation algorithms are given in table 1. Many of the meth-
ods exhibit substantial overﬁtting (that is, a training error much lower than the cross-validation error). I
attempted to reduce overﬁtting by using the one-standard-error rule to select less complex models, and this
seemed particularly effective for K-nearest neighbors, where we see a U-shaped cross-validation curve.
Nevertheless, substantial overﬁtting remains, particularly for the random forest. I think much of this over-
ﬁtting can be attributed to the noisy labels in the data. As mentioned above, I labeled all of the data twice,
and the labels were inconsistent for about one quarter of the observations. With such a relatively low
signal-to-noise ratio in the training data, overﬁtting is very difﬁcult to avoid.

Several algorithms came close to the 25 % error rate that is a lower bound on the accuracy due to the
noisy labels.
In particular, the random forest achieved an error rate of 28 %. The multinomial logistic
regression did not perform well, indicating that linear decision boundaries are probably not appropriate.
Both forms of discriminant analysis performed especially poorly, which is likely a result of the fact that
the assumptions of these generative models are not even approximately correct. (Recall that these models
assume that the predictor vector for each class has a multivariate normal distribution; there is no reason to
think our data satisfy this assumption.)

(a) KNN

(b) tree

(c) random forest

(d) boosted tree

Figure 4 – CV curves for choosing algorithm parameters

6 Conclusion

We used acoustic measurements to classify cardiomyocytes as atrial, nodal, or ventricular; such a nonin-
vasive classiﬁcation procedure is essential for practical cardiac stem-cell therapy. The classiﬁcation problem
is very difﬁcult – a human only managed to achieve an error rate of about 25 %. We obtained an error rate

4

lllllllllllllllllllllllll5101520250.340.350.360.370.38KNNKcrossvalidation misclassification errorlllllllllllll24681012140.350.400.450.500.55treetree sizeCrossvalidation misclassification errorllllllllllllllllllll51015200.2800.2820.2840.2860.2880.290random forestmCrossvalidation misclassification errorllllllllllllllllllll501001502000.3500.3520.3540.3560.3580.360n.treescrossvalidation misclassification errorAcoustic Identiﬁcation of Cardiomyocytes

Alex Lemon

1

Introduction
Pluripotent stem cells can be made to differentiate into cardiomyocytes (heart-muscle cells). However,
there are three different types of cardiomyocytes: atrial, ventricular, and nodal, and it is difﬁcult to force
cells to differentiate into a particular type. In therapeutic applications it is extremely important to use the
correct type of cardiomyocytes – implanting the wrong type of cell will not restore function as desired,
and may lead to cancer. Thus, it is extremely important to be able to accurately identify different types
of cardiomyocytes. Moreover, the identiﬁcation process must be minimally invasive if the cells are to be
subsequently used for therapy. In this paper we use noninvasive acoustic measurements of heart-muscle
cells to classify the cells as atrial, ventricular, or nodal; we apply several classiﬁcation methods to the data,
including K-nearest-neighbors, multinomial regression, discriminant analysis, support-vector machines,
and tree-based classiﬁers.
2 Related work

Junyi et al showed that the different types of cardiomyocytes can be identiﬁed using the electrical sig-
nals associated with action potentials [4]. These measurements are obtained using an invasive technique
called patch clamping, which renders the cells unﬁt for subsequent therapeutic use. More recently a team
of researchers at Stanford has been using a photonic-crystal hydrophone to take acoustic measurements
of murine cardiomyocytes. (This work is currently unpublished, although technical details of the mea-
surement device are available [3].) Catherine Jan (a doctoral student in the Department of Electrical En-
gineering, Stanford University) and Sally Kim (a postdoctoral researcher in the Department of Psychiatry,
Stanford University) provided their acoustic measurements for our analysis.
3 Data and preprocessing

The data set consists of thirty-four time traces ranging in length from thirty seconds to two minutes.
The original data was sampled at 10 kHz, while the features of interest occur in a frequency range that is an
order of magnitude slower. I used 5× downsampling with averaging to reduce the size of the data, while
preserving the features of interest.
After downsampling the data, I developed a method for extracting the individual pulses. This method

was based on three assumptions about the nature of the data.

(1) Each pulse can be approximated as a triangle wave of width w = 10 ms; we let φt denote the triangle

wave that starts at time t (see ﬁgure 1).

(2) The pulses corresponding to different cells are added to give the observed signal.

different pulses cannot cancel each other out.)

(In particular,

(3) Pulses begin at relatively few of the time sample points.

Figure 1 – a triangle wave

Based on these assumptions, we propose extracting the individual pulses by solving the following opti-
mization problem:

where ρ > 0 is a regularization parameter. Intuitively, we approximate the time trace as a weighted sum
of triangle waves. The ﬁrst term in the objective is the approximation error, which we want to be small.

(cid:80)
τ ατ φτ(cid:107)2
: (cid:107)x −
minimize
subject to : α ≥ 0,

α

2 + ρ(cid:107)α(cid:107)1

1

t1t+wThe penalty term (cid:107)α(cid:107)1 is used to promote sparsity, corresponding to the assumption that pulses start at
relatively few of the time sample points. The constraint α ≥ 0 models the assumption that the measured
signal is obtained by adding the pulses corresponding to different cells, and there is no cancellation. Thus,
the optimization problem above is a form of sparse, nonnegative regression. An example of the results are
shown in ﬁgure 2, where we see that the method is effective at extracting the pulses.

Figure 2 – approximating the measured signal using triangle waves

Having extracted the pulses from the measurements, we manually labeled 1220 observations. Repre-
sentative examples of the pulses corresponding to the three different types of cells are given in ﬁgure 3. We
labeled all of the observations twice in order to assess the quality of our labels. The two labels differed for
approximately one quarter of the examples, indicating that the problem is difﬁcult, and suggesting that an
error rate of about 25 % should be considered a success.

(a) atrial

(b) nodal

(c) ventricular

Figure 3 – pulses corresponding to the three different types of cardiomyocytes

4 Classiﬁcation methods

The following classiﬁcation methods were applied to the data. More detailed explanations of these

methods are given in James et al [2] and Hastie et al [1].

(a) K-nearest neighbors. In this method we classify an observation with predictor vector x0 by identifying
the K training examples whose predictor vectors are closest to x0, and using a majority vote among
these neighboring training examples. (We used the Euclidean distance to measure closeness, but it is
also possible to use other metrics.) The parameter K must be chosen to balance bias (which increases
with K) and variance (which decreases with K).

(b) Multinomial logistic regression. An extension of the usual two-class logistic regression, K-class multi-

nomial logistic regression assumes that the conditional class probabilities are

P(Y = k | X) =
P(Y = K | X) =

1 X)

exp(βT
k=1 exp(βT

k X)

k=1 exp(βT

k X)

1 +(cid:80)K−1
1 +(cid:80)K−1

1

2

k = 1, . . . K − 1,

,

,

signal,x(t)time,t00.20.40.60.81−0.1−0.0500.050.10.150.20.25where β1, . . . , βK−1 are parameters, which we can ﬁt using the method of maximum likelihood.

(c) Support-vector machine (SVM). A support-vector machine partitions the predictor space in order to
maximize the distance of the observations from the decision boundaries, while keeping the classiﬁca-
tion errors small. More concretely, we ﬁt an SVM by solving the following optimization problem:

maximize

βj , i, M
subject to :

: M

(cid:80)p
yi(β0 +(cid:80)p
(cid:80)n

j=1 β2

j = 1

j=1 βjxj) ≥ M (1 − i)

i ≥ 0
i=1 i ≤ C.

Important considerations for SVMs are the kernel (that is, the measure of similarity between predic-
tors) and the cost parameter C. In addition, because our classiﬁcation problem has more than two
classes, we need to decide whether to use one-versus-one comparisons or one-versus-all compar-
isons. (In a one-versus-one comparison, we ﬁt an SVM for all pairs of classes, and predict the class
that appeared most in the pairwise classiﬁcations; in a one-versus-all comparison, we ﬁt an SVM for
each class against all of the other classes, and predict the class with the highest conﬁdence.) We used
the standard radial kernel and one-versus-alls comparisons in this project.

(d) Discriminant analysis. In discriminant analysis we assume that the predictors from each class come
from a multivariate normal distribution, with possibly different parameters for each class. We esti-
mate the distribution parameters using their sample analogs (that is, the sample mean and sample
variance), and we classify a new observation to the class with the highest probability density at the
observed value of the predictor vector.

(i) Linear discriminant analysis (LDA). If we assume that the variance matrix is the same for all classes,

then we obtain linear decision boundaries.

(ii) Quadratic discriminant analysis (QDA). If we assume that the variance matrix is different for dif-
ferent classes, then we obtain quadratic decision boundaries. We tend to obtain better results
with QDA than LDA if the variance matrix differs substantially across classes, and worse results
otherwise.

(e) Tree-based methods.

(i) Classiﬁcation trees. In a classiﬁcation tree, we recursively partition the training set using binary
decisions based on the predictors. In order to classify a new observation, we apply the same
sequence of binary decisions to the predictors of the new observation, and label the observation
using majority vote when we reach a terminal node. Trees are very prone to overﬁtting, which
can be ameliorated by tuning the number of terminal nodes.

(ii) Random forests. A random forest is an ensemble of classiﬁcation trees. The trees in the ensemble
are decorrelated by bootstrapping the data used to generate the trees, and randomly selecting
the predictors that can be used for decisions at each node. Classiﬁcation is performed using a
majority vote of the trees in the ensemble. Important parameters are the number of trees in the
forest, and the number of predictors selected at each node.

(iii) Boosting. Like random forests, boosting also uses an ensemble of trees. However, instead of
producing a decorrelated ensemble, boosting instead uses subsequent trees to ﬁt the errors in
previous trees. The key parameters are the number of trees and the learning rate.

5 Results

All algorithm parameters were chosen using ten-fold cross-validation repeated ten times (that is, with
ten different random groupings for cross-validation; repeating cross-validation gives better estimates of the
error, and decreases the standard error of these estimates). In particular, we chose the number of neighbors
for K-nearest neighbors, the number of terminal nodes in the classiﬁcation tree, the number of predictors

3

chosen for each node in the random forest, and the number of trees in the boosting ensemble. The cross-
validation curves are shown in ﬁgure 4. The vertical red lines in these plots mark the minimum error, while
the horizontal red lines are one standard error above the minimum. The ﬁnal ﬁtted models were chosen
using the one-standard-error rule: that is, we selected the simplest model that was within one standard
error of the minimum cross-validation error.

Performance metrics for the different classiﬁcation algorithms are given in table 1. Many of the meth-
ods exhibit substantial overﬁtting (that is, a training error much lower than the cross-validation error). I
attempted to reduce overﬁtting by using the one-standard-error rule to select less complex models, and this
seemed particularly effective for K-nearest neighbors, where we see a U-shaped cross-validation curve.
Nevertheless, substantial overﬁtting remains, particularly for the random forest. I think much of this over-
ﬁtting can be attributed to the noisy labels in the data. As mentioned above, I labeled all of the data twice,
and the labels were inconsistent for about one quarter of the observations. With such a relatively low
signal-to-noise ratio in the training data, overﬁtting is very difﬁcult to avoid.

Several algorithms came close to the 25 % error rate that is a lower bound on the accuracy due to the
noisy labels.
In particular, the random forest achieved an error rate of 28 %. The multinomial logistic
regression did not perform well, indicating that linear decision boundaries are probably not appropriate.
Both forms of discriminant analysis performed especially poorly, which is likely a result of the fact that
the assumptions of these generative models are not even approximately correct. (Recall that these models
assume that the predictor vector for each class has a multivariate normal distribution; there is no reason to
think our data satisfy this assumption.)

(a) KNN

(b) tree

(c) random forest

(d) boosted tree

Figure 4 – CV curves for choosing algorithm parameters

6 Conclusion

We used acoustic measurements to classify cardiomyocytes as atrial, nodal, or ventricular; such a nonin-
vasive classiﬁcation procedure is essential for practical cardiac stem-cell therapy. The classiﬁcation problem
is very difﬁcult – a human only managed to achieve an error rate of about 25 %. We obtained an error rate

4

lllllllllllllllllllllllll5101520250.340.350.360.370.38KNNKcrossvalidation misclassification errorlllllllllllll24681012140.350.400.450.500.55treetree sizeCrossvalidation misclassification errorllllllllllllllllllll51015200.2800.2820.2840.2860.2880.290random forestmCrossvalidation misclassification errorllllllllllllllllllll501001502000.3500.3520.3540.3560.3580.360n.treescrossvalidation misclassification erroralgorithm

error

KNN

0.2639

multinomial

0.3770

SVM

0.2902

LDA

0.3852

QDA

0.4115

tree

0.3139

random forest

0.0008

boosted tree

0.3262

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

training
confusion matrix

error

 0.3434
 0.3939
 0.3301
 0.4008
 0.4447

 0.2801
 0.3525

0.3610

cross-validation










confusion matrix

A

N

V

355.5
145.5
39.5

100.5
197.0
46.9

25.0
62.5
247.6

A

N

V

355.5
156.7
66.6

80.7
179.9
63.4

44.8
68.4
204.0

A

N

V

399.6
160.4
42.3

50.3
160.7
34.7

31.1
83.9
257.0

A

N

V

343.0
152.2
65.0

96.0
186.4
67.4

42.0
66.4
201.6

A

N

V

215.5
73.5
31.5

240.0
298.3
138.8

25.5
33.2
163.7

A

N

V

366.2
145.9
53.4

70.9
194.2
65.8

43.9
64.9
214.8

A

N

V

386.5
116.0
33.4

73.1
235.4
45.9

21.4
53.6
254.7

A

N

V

414.2
189.4
75.5

45.8
158.8
41.2

21.0
56.8
217.3










A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

N
71
241
34
N
75
186
57
N
42
183
34
N
85
189
65
N
236
321
135
N
65
23
71
N
0
405
1
N
44
167
30

V
18
51
265
V
44
66
212
V
22
83
266
V
42
66
207
V
21
27
173
V
44
48
232
V
0
0
333
V
16
52
234

417
139
34

354
150
62

224
57
26

392
113
35

362
153
65

 A
 A
 A
 A
 A
 A
 A
 A

481
0
0

421
186
70

372
124
31

Table 1 – performance of classiﬁcation algorithms

of 28 % using a random forest, and also had success with K-nearest-neighbors, support-vector machines,
classiﬁcation trees, and boosted trees; multinomial regression and discriminant analysis performed poorly.
The largest source of error for most methods was classifying nodal cells as atrial.

The most important step for future work is obtaining more accurate training data. The labels used
in this project were not assigned by a biologist; if a domain expert were to assign the labels, then we
could obtain more accurate training data, which should reduce the overﬁtting problems that we observed.
Many of the methods can probably be improved by further tuning. For example, we did not use cross-
validation to adjust the cost parameter in the SVM, or the number of trees in the boosting algorithm. It may
also be worth investigating speciﬁc methods for differentiating between these two types of cells, perhaps
adding special features that can identify this difference. Finally, another interesting direction of research
is using multiple sensors simultaneously. If we have one sensor for each clump of cells, then we can use
independent-components analysis (ICA) to separate the signals corresponding to the different clumps.

5

Acoustic Identiﬁcation of Cardiomyocytes

Alex Lemon

1

Introduction
Pluripotent stem cells can be made to differentiate into cardiomyocytes (heart-muscle cells). However,
there are three different types of cardiomyocytes: atrial, ventricular, and nodal, and it is difﬁcult to force
cells to differentiate into a particular type. In therapeutic applications it is extremely important to use the
correct type of cardiomyocytes – implanting the wrong type of cell will not restore function as desired,
and may lead to cancer. Thus, it is extremely important to be able to accurately identify different types
of cardiomyocytes. Moreover, the identiﬁcation process must be minimally invasive if the cells are to be
subsequently used for therapy. In this paper we use noninvasive acoustic measurements of heart-muscle
cells to classify the cells as atrial, ventricular, or nodal; we apply several classiﬁcation methods to the data,
including K-nearest-neighbors, multinomial regression, discriminant analysis, support-vector machines,
and tree-based classiﬁers.
2 Related work

Junyi et al showed that the different types of cardiomyocytes can be identiﬁed using the electrical sig-
nals associated with action potentials [4]. These measurements are obtained using an invasive technique
called patch clamping, which renders the cells unﬁt for subsequent therapeutic use. More recently a team
of researchers at Stanford has been using a photonic-crystal hydrophone to take acoustic measurements
of murine cardiomyocytes. (This work is currently unpublished, although technical details of the mea-
surement device are available [3].) Catherine Jan (a doctoral student in the Department of Electrical En-
gineering, Stanford University) and Sally Kim (a postdoctoral researcher in the Department of Psychiatry,
Stanford University) provided their acoustic measurements for our analysis.
3 Data and preprocessing

The data set consists of thirty-four time traces ranging in length from thirty seconds to two minutes.
The original data was sampled at 10 kHz, while the features of interest occur in a frequency range that is an
order of magnitude slower. I used 5× downsampling with averaging to reduce the size of the data, while
preserving the features of interest.
After downsampling the data, I developed a method for extracting the individual pulses. This method

was based on three assumptions about the nature of the data.

(1) Each pulse can be approximated as a triangle wave of width w = 10 ms; we let φt denote the triangle

wave that starts at time t (see ﬁgure 1).

(2) The pulses corresponding to different cells are added to give the observed signal.

different pulses cannot cancel each other out.)

(In particular,

(3) Pulses begin at relatively few of the time sample points.

Figure 1 – a triangle wave

Based on these assumptions, we propose extracting the individual pulses by solving the following opti-
mization problem:

where ρ > 0 is a regularization parameter. Intuitively, we approximate the time trace as a weighted sum
of triangle waves. The ﬁrst term in the objective is the approximation error, which we want to be small.

(cid:80)
τ ατ φτ(cid:107)2
: (cid:107)x −
minimize
subject to : α ≥ 0,

α

2 + ρ(cid:107)α(cid:107)1

1

t1t+wThe penalty term (cid:107)α(cid:107)1 is used to promote sparsity, corresponding to the assumption that pulses start at
relatively few of the time sample points. The constraint α ≥ 0 models the assumption that the measured
signal is obtained by adding the pulses corresponding to different cells, and there is no cancellation. Thus,
the optimization problem above is a form of sparse, nonnegative regression. An example of the results are
shown in ﬁgure 2, where we see that the method is effective at extracting the pulses.

Figure 2 – approximating the measured signal using triangle waves

Having extracted the pulses from the measurements, we manually labeled 1220 observations. Repre-
sentative examples of the pulses corresponding to the three different types of cells are given in ﬁgure 3. We
labeled all of the observations twice in order to assess the quality of our labels. The two labels differed for
approximately one quarter of the examples, indicating that the problem is difﬁcult, and suggesting that an
error rate of about 25 % should be considered a success.

(a) atrial

(b) nodal

(c) ventricular

Figure 3 – pulses corresponding to the three different types of cardiomyocytes

4 Classiﬁcation methods

The following classiﬁcation methods were applied to the data. More detailed explanations of these

methods are given in James et al [2] and Hastie et al [1].

(a) K-nearest neighbors. In this method we classify an observation with predictor vector x0 by identifying
the K training examples whose predictor vectors are closest to x0, and using a majority vote among
these neighboring training examples. (We used the Euclidean distance to measure closeness, but it is
also possible to use other metrics.) The parameter K must be chosen to balance bias (which increases
with K) and variance (which decreases with K).

(b) Multinomial logistic regression. An extension of the usual two-class logistic regression, K-class multi-

nomial logistic regression assumes that the conditional class probabilities are

P(Y = k | X) =
P(Y = K | X) =

1 X)

exp(βT
k=1 exp(βT

k X)

k=1 exp(βT

k X)

1 +(cid:80)K−1
1 +(cid:80)K−1

1

2

k = 1, . . . K − 1,

,

,

signal,x(t)time,t00.20.40.60.81−0.1−0.0500.050.10.150.20.25where β1, . . . , βK−1 are parameters, which we can ﬁt using the method of maximum likelihood.

(c) Support-vector machine (SVM). A support-vector machine partitions the predictor space in order to
maximize the distance of the observations from the decision boundaries, while keeping the classiﬁca-
tion errors small. More concretely, we ﬁt an SVM by solving the following optimization problem:

maximize

βj , i, M
subject to :

: M

(cid:80)p
yi(β0 +(cid:80)p
(cid:80)n

j=1 β2

j = 1

j=1 βjxj) ≥ M (1 − i)

i ≥ 0
i=1 i ≤ C.

Important considerations for SVMs are the kernel (that is, the measure of similarity between predic-
tors) and the cost parameter C. In addition, because our classiﬁcation problem has more than two
classes, we need to decide whether to use one-versus-one comparisons or one-versus-all compar-
isons. (In a one-versus-one comparison, we ﬁt an SVM for all pairs of classes, and predict the class
that appeared most in the pairwise classiﬁcations; in a one-versus-all comparison, we ﬁt an SVM for
each class against all of the other classes, and predict the class with the highest conﬁdence.) We used
the standard radial kernel and one-versus-alls comparisons in this project.

(d) Discriminant analysis. In discriminant analysis we assume that the predictors from each class come
from a multivariate normal distribution, with possibly different parameters for each class. We esti-
mate the distribution parameters using their sample analogs (that is, the sample mean and sample
variance), and we classify a new observation to the class with the highest probability density at the
observed value of the predictor vector.

(i) Linear discriminant analysis (LDA). If we assume that the variance matrix is the same for all classes,

then we obtain linear decision boundaries.

(ii) Quadratic discriminant analysis (QDA). If we assume that the variance matrix is different for dif-
ferent classes, then we obtain quadratic decision boundaries. We tend to obtain better results
with QDA than LDA if the variance matrix differs substantially across classes, and worse results
otherwise.

(e) Tree-based methods.

(i) Classiﬁcation trees. In a classiﬁcation tree, we recursively partition the training set using binary
decisions based on the predictors. In order to classify a new observation, we apply the same
sequence of binary decisions to the predictors of the new observation, and label the observation
using majority vote when we reach a terminal node. Trees are very prone to overﬁtting, which
can be ameliorated by tuning the number of terminal nodes.

(ii) Random forests. A random forest is an ensemble of classiﬁcation trees. The trees in the ensemble
are decorrelated by bootstrapping the data used to generate the trees, and randomly selecting
the predictors that can be used for decisions at each node. Classiﬁcation is performed using a
majority vote of the trees in the ensemble. Important parameters are the number of trees in the
forest, and the number of predictors selected at each node.

(iii) Boosting. Like random forests, boosting also uses an ensemble of trees. However, instead of
producing a decorrelated ensemble, boosting instead uses subsequent trees to ﬁt the errors in
previous trees. The key parameters are the number of trees and the learning rate.

5 Results

All algorithm parameters were chosen using ten-fold cross-validation repeated ten times (that is, with
ten different random groupings for cross-validation; repeating cross-validation gives better estimates of the
error, and decreases the standard error of these estimates). In particular, we chose the number of neighbors
for K-nearest neighbors, the number of terminal nodes in the classiﬁcation tree, the number of predictors

3

chosen for each node in the random forest, and the number of trees in the boosting ensemble. The cross-
validation curves are shown in ﬁgure 4. The vertical red lines in these plots mark the minimum error, while
the horizontal red lines are one standard error above the minimum. The ﬁnal ﬁtted models were chosen
using the one-standard-error rule: that is, we selected the simplest model that was within one standard
error of the minimum cross-validation error.

Performance metrics for the different classiﬁcation algorithms are given in table 1. Many of the meth-
ods exhibit substantial overﬁtting (that is, a training error much lower than the cross-validation error). I
attempted to reduce overﬁtting by using the one-standard-error rule to select less complex models, and this
seemed particularly effective for K-nearest neighbors, where we see a U-shaped cross-validation curve.
Nevertheless, substantial overﬁtting remains, particularly for the random forest. I think much of this over-
ﬁtting can be attributed to the noisy labels in the data. As mentioned above, I labeled all of the data twice,
and the labels were inconsistent for about one quarter of the observations. With such a relatively low
signal-to-noise ratio in the training data, overﬁtting is very difﬁcult to avoid.

Several algorithms came close to the 25 % error rate that is a lower bound on the accuracy due to the
noisy labels.
In particular, the random forest achieved an error rate of 28 %. The multinomial logistic
regression did not perform well, indicating that linear decision boundaries are probably not appropriate.
Both forms of discriminant analysis performed especially poorly, which is likely a result of the fact that
the assumptions of these generative models are not even approximately correct. (Recall that these models
assume that the predictor vector for each class has a multivariate normal distribution; there is no reason to
think our data satisfy this assumption.)

(a) KNN

(b) tree

(c) random forest

(d) boosted tree

Figure 4 – CV curves for choosing algorithm parameters

6 Conclusion

We used acoustic measurements to classify cardiomyocytes as atrial, nodal, or ventricular; such a nonin-
vasive classiﬁcation procedure is essential for practical cardiac stem-cell therapy. The classiﬁcation problem
is very difﬁcult – a human only managed to achieve an error rate of about 25 %. We obtained an error rate

4

lllllllllllllllllllllllll5101520250.340.350.360.370.38KNNKcrossvalidation misclassification errorlllllllllllll24681012140.350.400.450.500.55treetree sizeCrossvalidation misclassification errorllllllllllllllllllll51015200.2800.2820.2840.2860.2880.290random forestmCrossvalidation misclassification errorllllllllllllllllllll501001502000.3500.3520.3540.3560.3580.360n.treescrossvalidation misclassification erroralgorithm

error

KNN

0.2639

multinomial

0.3770

SVM

0.2902

LDA

0.3852

QDA

0.4115

tree

0.3139

random forest

0.0008

boosted tree

0.3262

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

training
confusion matrix

error

 0.3434
 0.3939
 0.3301
 0.4008
 0.4447

 0.2801
 0.3525

0.3610

cross-validation










confusion matrix

A

N

V

355.5
145.5
39.5

100.5
197.0
46.9

25.0
62.5
247.6

A

N

V

355.5
156.7
66.6

80.7
179.9
63.4

44.8
68.4
204.0

A

N

V

399.6
160.4
42.3

50.3
160.7
34.7

31.1
83.9
257.0

A

N

V

343.0
152.2
65.0

96.0
186.4
67.4

42.0
66.4
201.6

A

N

V

215.5
73.5
31.5

240.0
298.3
138.8

25.5
33.2
163.7

A

N

V

366.2
145.9
53.4

70.9
194.2
65.8

43.9
64.9
214.8

A

N

V

386.5
116.0
33.4

73.1
235.4
45.9

21.4
53.6
254.7

A

N

V

414.2
189.4
75.5

45.8
158.8
41.2

21.0
56.8
217.3










A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

A

N

V

N
71
241
34
N
75
186
57
N
42
183
34
N
85
189
65
N
236
321
135
N
65
23
71
N
0
405
1
N
44
167
30

V
18
51
265
V
44
66
212
V
22
83
266
V
42
66
207
V
21
27
173
V
44
48
232
V
0
0
333
V
16
52
234

417
139
34

354
150
62

224
57
26

392
113
35

362
153
65

 A
 A
 A
 A
 A
 A
 A
 A

481
0
0

421
186
70

372
124
31

Table 1 – performance of classiﬁcation algorithms

of 28 % using a random forest, and also had success with K-nearest-neighbors, support-vector machines,
classiﬁcation trees, and boosted trees; multinomial regression and discriminant analysis performed poorly.
The largest source of error for most methods was classifying nodal cells as atrial.

The most important step for future work is obtaining more accurate training data. The labels used
in this project were not assigned by a biologist; if a domain expert were to assign the labels, then we
could obtain more accurate training data, which should reduce the overﬁtting problems that we observed.
Many of the methods can probably be improved by further tuning. For example, we did not use cross-
validation to adjust the cost parameter in the SVM, or the number of trees in the boosting algorithm. It may
also be worth investigating speciﬁc methods for differentiating between these two types of cells, perhaps
adding special features that can identify this difference. Finally, another interesting direction of research
is using multiple sensors simultaneously. If we have one sensor for each clump of cells, then we can use
independent-components analysis (ICA) to separate the signals corresponding to the different clumps.

5

References
[1] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer, 2

edition, 2009.

[2] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learn-

ing with Applications in R. Springer, 2013.

[3] Catherine Jan, Wonuk Jo, Michel J. F. Digonnet, and Olav Solgaard. Photonic-crystal-based ﬁber hy-
IEEE Photonics Technology Letters, 28(2):123 –

drophone with sub-100 µPa/√Hz pressure resolution.
126, 2016.

[4] Junyi Ma, Liang Guo, Steve J. Fiene, Blake D. Anson, James A. Thomson, Timothy J. Kamp, Kyle L.
Kolaja, Bradley J. Swanson, and Craig T. January. High purity human-induced pluripotent stem cell-
derived cardiomyocytes: electrophysiological properties of action potentials and ionic currents. Ameri-
can Journal of Physiology: Hearty and Circulatory Physiology, 301(5):H2006 – H2017, 2011.

6

