Predict Inﬂuencers in the Social Network

Ruishan Liu, Yang Zhao and Liuyu Zhou

Email: rliu2, yzhao12, lyzhou@stanford.edu

Department of Electrical Engineering, Stanford University

Abstract—Given two persons and their social network
features, our job is to predict which one is more inﬂuential.
In our project, we collect training samples from Kaggle
based on human judgement. We use several different
models to make predictions, such as Logistic Regression,
SVM, Naive Bayes and Neural Network. We also use some
auxiliary techniques like cross validation, feature selection
and data preprocessing. In the results section, we compare
the performances of different models and provide analysis
and suggestions for future works. We implement different
learning models using Matlab.

I. INTRODUCTION

In recent years, social network plays an increas-
ingly signiﬁcant role in our daily lives. We share our
experiences and opinions with our friends on Face-
book, Twitter, Instagram and so on. When you’re
browsing your friends’ posts, you may ﬁnd that
some of them are more inﬂuential than others and
we call them inﬂuencers. According to the research
in Sociology, inﬂuencers have a great impact on
other people’s lives because people always tend to
follow the opinions of these inﬂuencers in social
networks. Therefore, it is important for us to ﬁgure
out which persons are inﬂuencers and how they
shape public opinions. In our project, the goal is
to ﬁnd inﬂuencers in a speciﬁc social network—
Twitter.

II. DATASET

A. Original Dataset

We use the dataset from Kaggle provided by
PeerIndex, consisting of a standard, pair-wise
preference
learning task [1]. Each datapoint
describes two individuals, A and B. For each
person,
there are 11 pre-computed, non-negative
numeric features based on Twitter activity provided,
which include:

1. # followers
4. # mentions received
7. # retweets sent
10. # network feature 2

2. # followings
5. # retweets received
8. # posts
11. # network feature 3

3. #listed
6. # mentions sent
9. #network feature 1

Since we have two persons in each data sample,
we have 22 features in total. What’s more, there is a
binary label representing a human judgement about
which of the two individuals is more inﬂuential in
each training sample. Label 1 means A is more in-
ﬂuential than B. Label 0 means B is more inﬂuential
than A. There are 3000 training samples and 2500
testing samples in our dataset. Given a test sample,
our job is to predict which individual in this test
sample is more inﬂuential.
B. Data Preprocessing

Before applying different models on our data, we
may want to preprocess it ﬁrst. For linear models,
the hypothesis function has the following form:

hθ(x) = g(θT x), θ, x ∈ R22

(1)

which results in a linear decision boundary,
i.e.
when θT x ≥ b, we predict 1; otherwise, we predict
0. Note that for a training example, if we change the
order A and B (that is, exchange the last 11 features
of the training example with its ﬁrst 11 features),
the label should also be reversed (1 becomes 0,
and 0 becomes 1). Thus the coefﬁcients of the
ﬁrst 11 features must be opposite numbers of the
coefﬁcients of the last 11 features, i.e.
θj = −θj+11, j = 1, 2, ..., 11

(2)
where θj is the jth entry of θ. So there are only 11
independent parameters in θ, and thus we can use
only 11 features to represent an example. We choose
z as our new representation of training example as
follows:

Predict Inﬂuencers in the Social Network

Ruishan Liu, Yang Zhao and Liuyu Zhou

Email: rliu2, yzhao12, lyzhou@stanford.edu

Department of Electrical Engineering, Stanford University

Abstract—Given two persons and their social network
features, our job is to predict which one is more inﬂuential.
In our project, we collect training samples from Kaggle
based on human judgement. We use several different
models to make predictions, such as Logistic Regression,
SVM, Naive Bayes and Neural Network. We also use some
auxiliary techniques like cross validation, feature selection
and data preprocessing. In the results section, we compare
the performances of different models and provide analysis
and suggestions for future works. We implement different
learning models using Matlab.

I. INTRODUCTION

In recent years, social network plays an increas-
ingly signiﬁcant role in our daily lives. We share our
experiences and opinions with our friends on Face-
book, Twitter, Instagram and so on. When you’re
browsing your friends’ posts, you may ﬁnd that
some of them are more inﬂuential than others and
we call them inﬂuencers. According to the research
in Sociology, inﬂuencers have a great impact on
other people’s lives because people always tend to
follow the opinions of these inﬂuencers in social
networks. Therefore, it is important for us to ﬁgure
out which persons are inﬂuencers and how they
shape public opinions. In our project, the goal is
to ﬁnd inﬂuencers in a speciﬁc social network—
Twitter.

II. DATASET

A. Original Dataset

We use the dataset from Kaggle provided by
PeerIndex, consisting of a standard, pair-wise
preference
learning task [1]. Each datapoint
describes two individuals, A and B. For each
person,
there are 11 pre-computed, non-negative
numeric features based on Twitter activity provided,
which include:

1. # followers
4. # mentions received
7. # retweets sent
10. # network feature 2

2. # followings
5. # retweets received
8. # posts
11. # network feature 3

3. #listed
6. # mentions sent
9. #network feature 1

Since we have two persons in each data sample,
we have 22 features in total. What’s more, there is a
binary label representing a human judgement about
which of the two individuals is more inﬂuential in
each training sample. Label 1 means A is more in-
ﬂuential than B. Label 0 means B is more inﬂuential
than A. There are 3000 training samples and 2500
testing samples in our dataset. Given a test sample,
our job is to predict which individual in this test
sample is more inﬂuential.
B. Data Preprocessing

Before applying different models on our data, we
may want to preprocess it ﬁrst. For linear models,
the hypothesis function has the following form:

hθ(x) = g(θT x), θ, x ∈ R22

(1)

which results in a linear decision boundary,
i.e.
when θT x ≥ b, we predict 1; otherwise, we predict
0. Note that for a training example, if we change the
order A and B (that is, exchange the last 11 features
of the training example with its ﬁrst 11 features),
the label should also be reversed (1 becomes 0,
and 0 becomes 1). Thus the coefﬁcients of the
ﬁrst 11 features must be opposite numbers of the
coefﬁcients of the last 11 features, i.e.
θj = −θj+11, j = 1, 2, ..., 11

(2)
where θj is the jth entry of θ. So there are only 11
independent parameters in θ, and thus we can use
only 11 features to represent an example. We choose
z as our new representation of training example as
follows:

zj = xj − xj+11, j = 1, 2, ..., 11

(3)

where zj and xj are the jth attribute of z and x
respectively. Note that this preprocessing method
will only be used in linear models, i.e. Logistic
Regression and SVM. Besides this method, we
also use other preprocessing methods such K-means
algorithm, which will be discussed in their own
models.

III. SYSTEM MODELS

A. Follower Count Benchmark

than B. Using this method,

First, we tried a quite straightforward and trivial
method based only on the number of followers,
i.e.
if A has more followers than B, then A is
more inﬂuential
the
test accuracy is about 70.2%. This result shows
that the number of followers is a strong indicator
of the inﬂuencers. However, 70.2% is not good
enough for our prediction and this result will be
mainly considered as a benchmark. After adding
more features and using more general models, we
hope to get a better prediction on the test samples.

B. Logistic Regression

First, we preprocess the original dataset using the
previous method. After preprocessing the original
dataset, different attributes have different ranges
which vary a lot. Thus the ﬁrst thing to do is to
handle the data to make it more uniform and easy
to deal with. So in order to achieve this, we have a
normalization on the dataset. For each feature, we
do the following normalization

¯z(i)
j =

j(cid:113)(cid:80)m

z(i)
i=1 z(i)2

j

where m is the number of training examples and
n = 11 is the number of features.

Because our job is to predict who is more inﬂuen-
tial given two persons, the number of followers, as
we know, plays a signiﬁcant rule in the prediction.
A person with more followers than the other is more
likely to be judged inﬂuential by users. So we will
multiply the normalization of the ﬁrst feature, i.e.
number of followers, by a positive constant factor

which is greater than 1 to make it more inﬂuential
than other features on the prediction.

For this logistic regression problem, we have two
choices— gradient ascent and Newton’s method
to achieve the parameter θ. Since the number of
features n = 11 is not very large, so it will not take
much time to compute the inverse of a n×n matrix.
Thus Newton’s method should converge faster in
this problem and we choose Newton’s method as our
implementation. The update rule of the Newton’s
method is:

θ := θ − H−1∇θl(θ)

−(cid:80)m

i=1 hθ(¯z(i))(1 − hθ(¯z(i)))¯z(i)

(5)
where H ∈ Rn×n is the Hessian matrix and Hjk =
k , ∇θl(θ) =
¯Z((cid:126)y − hθ( ¯Z)).
Furthermore, we will add cross validation and
feature selection to this model, which will be dis-
cussed in details in section IV. We write our own
code to implement the Newton’s method.
C. SVM

j ¯z(i)

in

II-B. We
a

First, we also use the preprocessing method
mentioned
implement
SVM
through libsvm,
at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm/,
with
function [2]
l2
[3]. Using SVM model, the problem formulation
becomes:

regularization and linear kernel

library offered online

m(cid:88)

(cid:107)w(cid:107)2 + C

1
2

min
w,b,ξ
s.t. y(i)(wT ¯z(i) + b) ≥ 1 − ξi, i = 1, 2, ..., m

i=1

ξ2
i

(6)

ξi ≥ 0, i = 1, 2, ..., m

we let C = 1 as its default value.

Moreover, we add cross validation and feature
selection to this model, which refers to section IV.
D. Naive Bayes

After using discriminative learning algorithms,
we may want to try some generative learning al-
gorithms. The distribution of the original data is
probably not Gaussian distribution, so Gaussian
discriminant analysis can hardly work. Therefore,
we choose the Naive Bayes model. In this case, we

, z = 1, 2, ..., n

(4)

The box constraint C in (6) is customizable and

Predict Inﬂuencers in the Social Network

Ruishan Liu, Yang Zhao and Liuyu Zhou

Email: rliu2, yzhao12, lyzhou@stanford.edu

Department of Electrical Engineering, Stanford University

Abstract—Given two persons and their social network
features, our job is to predict which one is more inﬂuential.
In our project, we collect training samples from Kaggle
based on human judgement. We use several different
models to make predictions, such as Logistic Regression,
SVM, Naive Bayes and Neural Network. We also use some
auxiliary techniques like cross validation, feature selection
and data preprocessing. In the results section, we compare
the performances of different models and provide analysis
and suggestions for future works. We implement different
learning models using Matlab.

I. INTRODUCTION

In recent years, social network plays an increas-
ingly signiﬁcant role in our daily lives. We share our
experiences and opinions with our friends on Face-
book, Twitter, Instagram and so on. When you’re
browsing your friends’ posts, you may ﬁnd that
some of them are more inﬂuential than others and
we call them inﬂuencers. According to the research
in Sociology, inﬂuencers have a great impact on
other people’s lives because people always tend to
follow the opinions of these inﬂuencers in social
networks. Therefore, it is important for us to ﬁgure
out which persons are inﬂuencers and how they
shape public opinions. In our project, the goal is
to ﬁnd inﬂuencers in a speciﬁc social network—
Twitter.

II. DATASET

A. Original Dataset

We use the dataset from Kaggle provided by
PeerIndex, consisting of a standard, pair-wise
preference
learning task [1]. Each datapoint
describes two individuals, A and B. For each
person,
there are 11 pre-computed, non-negative
numeric features based on Twitter activity provided,
which include:

1. # followers
4. # mentions received
7. # retweets sent
10. # network feature 2

2. # followings
5. # retweets received
8. # posts
11. # network feature 3

3. #listed
6. # mentions sent
9. #network feature 1

Since we have two persons in each data sample,
we have 22 features in total. What’s more, there is a
binary label representing a human judgement about
which of the two individuals is more inﬂuential in
each training sample. Label 1 means A is more in-
ﬂuential than B. Label 0 means B is more inﬂuential
than A. There are 3000 training samples and 2500
testing samples in our dataset. Given a test sample,
our job is to predict which individual in this test
sample is more inﬂuential.
B. Data Preprocessing

Before applying different models on our data, we
may want to preprocess it ﬁrst. For linear models,
the hypothesis function has the following form:

hθ(x) = g(θT x), θ, x ∈ R22

(1)

which results in a linear decision boundary,
i.e.
when θT x ≥ b, we predict 1; otherwise, we predict
0. Note that for a training example, if we change the
order A and B (that is, exchange the last 11 features
of the training example with its ﬁrst 11 features),
the label should also be reversed (1 becomes 0,
and 0 becomes 1). Thus the coefﬁcients of the
ﬁrst 11 features must be opposite numbers of the
coefﬁcients of the last 11 features, i.e.
θj = −θj+11, j = 1, 2, ..., 11

(2)
where θj is the jth entry of θ. So there are only 11
independent parameters in θ, and thus we can use
only 11 features to represent an example. We choose
z as our new representation of training example as
follows:

zj = xj − xj+11, j = 1, 2, ..., 11

(3)

where zj and xj are the jth attribute of z and x
respectively. Note that this preprocessing method
will only be used in linear models, i.e. Logistic
Regression and SVM. Besides this method, we
also use other preprocessing methods such K-means
algorithm, which will be discussed in their own
models.

III. SYSTEM MODELS

A. Follower Count Benchmark

than B. Using this method,

First, we tried a quite straightforward and trivial
method based only on the number of followers,
i.e.
if A has more followers than B, then A is
more inﬂuential
the
test accuracy is about 70.2%. This result shows
that the number of followers is a strong indicator
of the inﬂuencers. However, 70.2% is not good
enough for our prediction and this result will be
mainly considered as a benchmark. After adding
more features and using more general models, we
hope to get a better prediction on the test samples.

B. Logistic Regression

First, we preprocess the original dataset using the
previous method. After preprocessing the original
dataset, different attributes have different ranges
which vary a lot. Thus the ﬁrst thing to do is to
handle the data to make it more uniform and easy
to deal with. So in order to achieve this, we have a
normalization on the dataset. For each feature, we
do the following normalization

¯z(i)
j =

j(cid:113)(cid:80)m

z(i)
i=1 z(i)2

j

where m is the number of training examples and
n = 11 is the number of features.

Because our job is to predict who is more inﬂuen-
tial given two persons, the number of followers, as
we know, plays a signiﬁcant rule in the prediction.
A person with more followers than the other is more
likely to be judged inﬂuential by users. So we will
multiply the normalization of the ﬁrst feature, i.e.
number of followers, by a positive constant factor

which is greater than 1 to make it more inﬂuential
than other features on the prediction.

For this logistic regression problem, we have two
choices— gradient ascent and Newton’s method
to achieve the parameter θ. Since the number of
features n = 11 is not very large, so it will not take
much time to compute the inverse of a n×n matrix.
Thus Newton’s method should converge faster in
this problem and we choose Newton’s method as our
implementation. The update rule of the Newton’s
method is:

θ := θ − H−1∇θl(θ)

−(cid:80)m

i=1 hθ(¯z(i))(1 − hθ(¯z(i)))¯z(i)

(5)
where H ∈ Rn×n is the Hessian matrix and Hjk =
k , ∇θl(θ) =
¯Z((cid:126)y − hθ( ¯Z)).
Furthermore, we will add cross validation and
feature selection to this model, which will be dis-
cussed in details in section IV. We write our own
code to implement the Newton’s method.
C. SVM

j ¯z(i)

in

II-B. We
a

First, we also use the preprocessing method
mentioned
implement
SVM
through libsvm,
at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm/,
with
function [2]
l2
[3]. Using SVM model, the problem formulation
becomes:

regularization and linear kernel

library offered online

m(cid:88)

(cid:107)w(cid:107)2 + C

1
2

min
w,b,ξ
s.t. y(i)(wT ¯z(i) + b) ≥ 1 − ξi, i = 1, 2, ..., m

i=1

ξ2
i

(6)

ξi ≥ 0, i = 1, 2, ..., m

we let C = 1 as its default value.

Moreover, we add cross validation and feature
selection to this model, which refers to section IV.
D. Naive Bayes

After using discriminative learning algorithms,
we may want to try some generative learning al-
gorithms. The distribution of the original data is
probably not Gaussian distribution, so Gaussian
discriminant analysis can hardly work. Therefore,
we choose the Naive Bayes model. In this case, we

, z = 1, 2, ..., n

(4)

The box constraint C in (6) is customizable and

use the multinomial Naive Bayes model, which is
more general.

Before we apply Naive Bayes algorithm, we need
to discretize our dataset ﬁrst. For this purpose,
we choose K-means algorithm to discretize each
attribute into several clusters and each cluster cor-
responds to a class. Applying clustering algorithm
like K-means can help us distinguish users from
different levels. For instance, a ﬁlm star may have
a million followers while a normal user can only
have hundreds of followers and in this case, their
difference can’t be ignored and we need to put them
into different classes.

In the original dataset, some attributes have very
large ranges. Thus, we consider using logarithm
function on the original dataset before applying K-
means. We will compare the performance using
logarithm function to not using it in section V.

Once we have discretized the data, we can apply
multinomial Naive Bayes model to it. Assume the
jth attribute has cj classes {1, 2, ..., cj}, we give
the parameters achieving maximum likelihood as
follows:

(cid:80)m
i=1 I{y(i) = 1}

φy =

m

(cid:80)m
(cid:80)m
j = k ∩ y(i) = 1}
i=1 I{x(i)
i=1 I{y(i) = 1}

(7)

, k = 1, 2, ..., cj

φjk|y=1 =

(8)
where j = 1, 2, ..., 22 corresponding to 22 features.
φy is probability of y = 1; φjk|y=1 is the probability
that the jth attribute is class k given y = 1; φjk|y=0
can be derived similarly and thus we don’t give its
expression here. With the above parameters and a
test sample y(i), we predict 1 if and only if:

Using the formula in (9), we can predict on
the testing set. However, one thing we haven’t
speciﬁed is the numbers of classes of each attribute
and changing this parameter, we may get different
testing accuracies. This parameter can be regarded
as a 22-dimensional vector, i.e. c = [c1, c2, ..., c22]T .
If we use a deterministic initial state instead of
random initialization in the K-means algorithm, the
clustering result is also deterministic given the num-
ber of clusters. In this case, all parameters are deter-
mined by the vector c and we call it discretization
parameter. Thus, the testing accuracy is a function of
c, say f (c). By changing c, we may achieve different
accuracies and thus we can optimize the accuracy.
We provide a coordinate ascent based algorithm as
follows:

• Initialize c = [2, 2, ..., 2]T .
• Repeat until convergence:

For j = 1, 2, ..., 22

cj := arg maxˆcj f (c1, c2, ..., ˆcj, ..., c22)
where ˆcj = cj − d, ..., cj, ..., cj + d

The parameter d denotes the maximum step we
can take in one iteration, which is adjustable. This
algorithm has high computational complexity and
thus we haven’t tested and improved it, which could
be considered as a choice in future work. In section
V, we choose some speciﬁc values of c and illustrate
the performance of Naive Bayes. We write our own
code to implement the K-means and Naive Bayes
algorithm.

E. Neural Network

The Naive Bayes model introduced in the pre-
vious section is a nonlinear model. However, this
results from our nonlinear preprocessing methods,
which makes Naive Bayes itself not that ”nonlin-
ear”. Besides linear models, we still want to try
some nonlinear models with high capacity. For this
reason, Neural Network might be a good choice.

Fig. 1. Neural Network Architecture in Matlab

22(cid:89)

j=1

p(y(i) = 1|x(i)) ≥ p(y(i) = 0|x(i)) ⇔

p(x(i)|y(i) = 1)p(y(i) = 1) ≥ p(x(i)|y(i) = 0)p(y(i) = 0)

⇔ φy

|y=1 ≥ (1 − φy)

φjy(i)

j

φjy(i)

j

|y=0

(9)
The decision boundary in (9) is linear. However,
this model is nonlinear because K-means algorithm
and logarithm function are nonlinear mappings.

22(cid:89)

j=1

Predict Inﬂuencers in the Social Network

Ruishan Liu, Yang Zhao and Liuyu Zhou

Email: rliu2, yzhao12, lyzhou@stanford.edu

Department of Electrical Engineering, Stanford University

Abstract—Given two persons and their social network
features, our job is to predict which one is more inﬂuential.
In our project, we collect training samples from Kaggle
based on human judgement. We use several different
models to make predictions, such as Logistic Regression,
SVM, Naive Bayes and Neural Network. We also use some
auxiliary techniques like cross validation, feature selection
and data preprocessing. In the results section, we compare
the performances of different models and provide analysis
and suggestions for future works. We implement different
learning models using Matlab.

I. INTRODUCTION

In recent years, social network plays an increas-
ingly signiﬁcant role in our daily lives. We share our
experiences and opinions with our friends on Face-
book, Twitter, Instagram and so on. When you’re
browsing your friends’ posts, you may ﬁnd that
some of them are more inﬂuential than others and
we call them inﬂuencers. According to the research
in Sociology, inﬂuencers have a great impact on
other people’s lives because people always tend to
follow the opinions of these inﬂuencers in social
networks. Therefore, it is important for us to ﬁgure
out which persons are inﬂuencers and how they
shape public opinions. In our project, the goal is
to ﬁnd inﬂuencers in a speciﬁc social network—
Twitter.

II. DATASET

A. Original Dataset

We use the dataset from Kaggle provided by
PeerIndex, consisting of a standard, pair-wise
preference
learning task [1]. Each datapoint
describes two individuals, A and B. For each
person,
there are 11 pre-computed, non-negative
numeric features based on Twitter activity provided,
which include:

1. # followers
4. # mentions received
7. # retweets sent
10. # network feature 2

2. # followings
5. # retweets received
8. # posts
11. # network feature 3

3. #listed
6. # mentions sent
9. #network feature 1

Since we have two persons in each data sample,
we have 22 features in total. What’s more, there is a
binary label representing a human judgement about
which of the two individuals is more inﬂuential in
each training sample. Label 1 means A is more in-
ﬂuential than B. Label 0 means B is more inﬂuential
than A. There are 3000 training samples and 2500
testing samples in our dataset. Given a test sample,
our job is to predict which individual in this test
sample is more inﬂuential.
B. Data Preprocessing

Before applying different models on our data, we
may want to preprocess it ﬁrst. For linear models,
the hypothesis function has the following form:

hθ(x) = g(θT x), θ, x ∈ R22

(1)

which results in a linear decision boundary,
i.e.
when θT x ≥ b, we predict 1; otherwise, we predict
0. Note that for a training example, if we change the
order A and B (that is, exchange the last 11 features
of the training example with its ﬁrst 11 features),
the label should also be reversed (1 becomes 0,
and 0 becomes 1). Thus the coefﬁcients of the
ﬁrst 11 features must be opposite numbers of the
coefﬁcients of the last 11 features, i.e.
θj = −θj+11, j = 1, 2, ..., 11

(2)
where θj is the jth entry of θ. So there are only 11
independent parameters in θ, and thus we can use
only 11 features to represent an example. We choose
z as our new representation of training example as
follows:

zj = xj − xj+11, j = 1, 2, ..., 11

(3)

where zj and xj are the jth attribute of z and x
respectively. Note that this preprocessing method
will only be used in linear models, i.e. Logistic
Regression and SVM. Besides this method, we
also use other preprocessing methods such K-means
algorithm, which will be discussed in their own
models.

III. SYSTEM MODELS

A. Follower Count Benchmark

than B. Using this method,

First, we tried a quite straightforward and trivial
method based only on the number of followers,
i.e.
if A has more followers than B, then A is
more inﬂuential
the
test accuracy is about 70.2%. This result shows
that the number of followers is a strong indicator
of the inﬂuencers. However, 70.2% is not good
enough for our prediction and this result will be
mainly considered as a benchmark. After adding
more features and using more general models, we
hope to get a better prediction on the test samples.

B. Logistic Regression

First, we preprocess the original dataset using the
previous method. After preprocessing the original
dataset, different attributes have different ranges
which vary a lot. Thus the ﬁrst thing to do is to
handle the data to make it more uniform and easy
to deal with. So in order to achieve this, we have a
normalization on the dataset. For each feature, we
do the following normalization

¯z(i)
j =

j(cid:113)(cid:80)m

z(i)
i=1 z(i)2

j

where m is the number of training examples and
n = 11 is the number of features.

Because our job is to predict who is more inﬂuen-
tial given two persons, the number of followers, as
we know, plays a signiﬁcant rule in the prediction.
A person with more followers than the other is more
likely to be judged inﬂuential by users. So we will
multiply the normalization of the ﬁrst feature, i.e.
number of followers, by a positive constant factor

which is greater than 1 to make it more inﬂuential
than other features on the prediction.

For this logistic regression problem, we have two
choices— gradient ascent and Newton’s method
to achieve the parameter θ. Since the number of
features n = 11 is not very large, so it will not take
much time to compute the inverse of a n×n matrix.
Thus Newton’s method should converge faster in
this problem and we choose Newton’s method as our
implementation. The update rule of the Newton’s
method is:

θ := θ − H−1∇θl(θ)

−(cid:80)m

i=1 hθ(¯z(i))(1 − hθ(¯z(i)))¯z(i)

(5)
where H ∈ Rn×n is the Hessian matrix and Hjk =
k , ∇θl(θ) =
¯Z((cid:126)y − hθ( ¯Z)).
Furthermore, we will add cross validation and
feature selection to this model, which will be dis-
cussed in details in section IV. We write our own
code to implement the Newton’s method.
C. SVM

j ¯z(i)

in

II-B. We
a

First, we also use the preprocessing method
mentioned
implement
SVM
through libsvm,
at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm/,
with
function [2]
l2
[3]. Using SVM model, the problem formulation
becomes:

regularization and linear kernel

library offered online

m(cid:88)

(cid:107)w(cid:107)2 + C

1
2

min
w,b,ξ
s.t. y(i)(wT ¯z(i) + b) ≥ 1 − ξi, i = 1, 2, ..., m

i=1

ξ2
i

(6)

ξi ≥ 0, i = 1, 2, ..., m

we let C = 1 as its default value.

Moreover, we add cross validation and feature
selection to this model, which refers to section IV.
D. Naive Bayes

After using discriminative learning algorithms,
we may want to try some generative learning al-
gorithms. The distribution of the original data is
probably not Gaussian distribution, so Gaussian
discriminant analysis can hardly work. Therefore,
we choose the Naive Bayes model. In this case, we

, z = 1, 2, ..., n

(4)

The box constraint C in (6) is customizable and

use the multinomial Naive Bayes model, which is
more general.

Before we apply Naive Bayes algorithm, we need
to discretize our dataset ﬁrst. For this purpose,
we choose K-means algorithm to discretize each
attribute into several clusters and each cluster cor-
responds to a class. Applying clustering algorithm
like K-means can help us distinguish users from
different levels. For instance, a ﬁlm star may have
a million followers while a normal user can only
have hundreds of followers and in this case, their
difference can’t be ignored and we need to put them
into different classes.

In the original dataset, some attributes have very
large ranges. Thus, we consider using logarithm
function on the original dataset before applying K-
means. We will compare the performance using
logarithm function to not using it in section V.

Once we have discretized the data, we can apply
multinomial Naive Bayes model to it. Assume the
jth attribute has cj classes {1, 2, ..., cj}, we give
the parameters achieving maximum likelihood as
follows:

(cid:80)m
i=1 I{y(i) = 1}

φy =

m

(cid:80)m
(cid:80)m
j = k ∩ y(i) = 1}
i=1 I{x(i)
i=1 I{y(i) = 1}

(7)

, k = 1, 2, ..., cj

φjk|y=1 =

(8)
where j = 1, 2, ..., 22 corresponding to 22 features.
φy is probability of y = 1; φjk|y=1 is the probability
that the jth attribute is class k given y = 1; φjk|y=0
can be derived similarly and thus we don’t give its
expression here. With the above parameters and a
test sample y(i), we predict 1 if and only if:

Using the formula in (9), we can predict on
the testing set. However, one thing we haven’t
speciﬁed is the numbers of classes of each attribute
and changing this parameter, we may get different
testing accuracies. This parameter can be regarded
as a 22-dimensional vector, i.e. c = [c1, c2, ..., c22]T .
If we use a deterministic initial state instead of
random initialization in the K-means algorithm, the
clustering result is also deterministic given the num-
ber of clusters. In this case, all parameters are deter-
mined by the vector c and we call it discretization
parameter. Thus, the testing accuracy is a function of
c, say f (c). By changing c, we may achieve different
accuracies and thus we can optimize the accuracy.
We provide a coordinate ascent based algorithm as
follows:

• Initialize c = [2, 2, ..., 2]T .
• Repeat until convergence:

For j = 1, 2, ..., 22

cj := arg maxˆcj f (c1, c2, ..., ˆcj, ..., c22)
where ˆcj = cj − d, ..., cj, ..., cj + d

The parameter d denotes the maximum step we
can take in one iteration, which is adjustable. This
algorithm has high computational complexity and
thus we haven’t tested and improved it, which could
be considered as a choice in future work. In section
V, we choose some speciﬁc values of c and illustrate
the performance of Naive Bayes. We write our own
code to implement the K-means and Naive Bayes
algorithm.

E. Neural Network

The Naive Bayes model introduced in the pre-
vious section is a nonlinear model. However, this
results from our nonlinear preprocessing methods,
which makes Naive Bayes itself not that ”nonlin-
ear”. Besides linear models, we still want to try
some nonlinear models with high capacity. For this
reason, Neural Network might be a good choice.

Fig. 1. Neural Network Architecture in Matlab

22(cid:89)

j=1

p(y(i) = 1|x(i)) ≥ p(y(i) = 0|x(i)) ⇔

p(x(i)|y(i) = 1)p(y(i) = 1) ≥ p(x(i)|y(i) = 0)p(y(i) = 0)

⇔ φy

|y=1 ≥ (1 − φy)

φjy(i)

j

φjy(i)

j

|y=0

(9)
The decision boundary in (9) is linear. However,
this model is nonlinear because K-means algorithm
and logarithm function are nonlinear mappings.

22(cid:89)

j=1

We use the Matlab Neural Network Pattern
Recognition toolbox to implement the Neural Net-
work algorithm, which is designed for classiﬁcation
problems. The network architecture is given in Fig.
1, which is a two-layer feed forward network, with
sigmoid hidden neurons and softmax output neuron
[4].The number of neurons in the hidden layer is
customizable and we will compare the performances
using different numbers of hidden neurons in section
V.

IV. MODEL SELECTION

A. Cross Validation

We use hold-out cross validation and 30% of the
training set are considered as validation set. What’s
more, we randomly split the training set for 100
times (we get 100 hypothesis functions) and pick
the one with the smallest cross validation error. We
also use k-fold cross validation and we will assign
different values to k in section V in order to choose
the best one.
B. Feature Selection

In our dataset, it is fairly possible that some fea-
tures are much more indicative than other features,
e.g. the number of followers is a strong indicator.
We have 11 features for the linear models as stated
above. Although the number of features are not very
large, the test error is large and there may be only
a subset of these 11 features that are relevant to the
result.

In this problem, we use forward search as our
feature selection algorithm, which is designed to
ﬁnd the most relevant features. In addition,
the
maximum number of features is regarded as an
input argument in our implementation. Thus, we can
change this parameter to optimize the performance
and sort features from the most relevant to the least
relevant, which will be discussed in section V.

V. RESULTS & DISCUSSIONS

A. Logistic Regression & SVM

In this section, we apply cross validation and
feature selection to the linear models, i.e. Logistic
Regression and SVM. The following ﬁgures com-
pare the performance among different models.

In Fig. 2, we use different cross validation options
on linear models, such as hold-out and k-fold,

Fig. 2. Test accuracy vs. cross validation options for linear models

and compare their test accuracies. It is shown that
cross validation can improve the test accuracy of
SVM, while having no strong effect on Logistic
Regression. What’s more, the performance of SVM
is better than Logistic Regression using cross vali-
dation.

Fig. 3. Test accuracy vs. discretization parameter for linear models

Fig. 3 shows the test accuracies of SVM and Lo-
gistic Regression with different numbers of features
selected. As is shown above, the performance of
SVM is better, compared with logistic regression.
The best performance of SVM is achieved when
4 features are selected, which shows that some
features are weak indicators. After feature selection,
we sort the features from the most relevant to the
least relevant as follows:

SVM 9
LR
3

6
6

7
8

10
2

8
9

5
5

3
4

2
7

1
1

4
11

11
10

Predict Inﬂuencers in the Social Network

Ruishan Liu, Yang Zhao and Liuyu Zhou

Email: rliu2, yzhao12, lyzhou@stanford.edu

Department of Electrical Engineering, Stanford University

Abstract—Given two persons and their social network
features, our job is to predict which one is more inﬂuential.
In our project, we collect training samples from Kaggle
based on human judgement. We use several different
models to make predictions, such as Logistic Regression,
SVM, Naive Bayes and Neural Network. We also use some
auxiliary techniques like cross validation, feature selection
and data preprocessing. In the results section, we compare
the performances of different models and provide analysis
and suggestions for future works. We implement different
learning models using Matlab.

I. INTRODUCTION

In recent years, social network plays an increas-
ingly signiﬁcant role in our daily lives. We share our
experiences and opinions with our friends on Face-
book, Twitter, Instagram and so on. When you’re
browsing your friends’ posts, you may ﬁnd that
some of them are more inﬂuential than others and
we call them inﬂuencers. According to the research
in Sociology, inﬂuencers have a great impact on
other people’s lives because people always tend to
follow the opinions of these inﬂuencers in social
networks. Therefore, it is important for us to ﬁgure
out which persons are inﬂuencers and how they
shape public opinions. In our project, the goal is
to ﬁnd inﬂuencers in a speciﬁc social network—
Twitter.

II. DATASET

A. Original Dataset

We use the dataset from Kaggle provided by
PeerIndex, consisting of a standard, pair-wise
preference
learning task [1]. Each datapoint
describes two individuals, A and B. For each
person,
there are 11 pre-computed, non-negative
numeric features based on Twitter activity provided,
which include:

1. # followers
4. # mentions received
7. # retweets sent
10. # network feature 2

2. # followings
5. # retweets received
8. # posts
11. # network feature 3

3. #listed
6. # mentions sent
9. #network feature 1

Since we have two persons in each data sample,
we have 22 features in total. What’s more, there is a
binary label representing a human judgement about
which of the two individuals is more inﬂuential in
each training sample. Label 1 means A is more in-
ﬂuential than B. Label 0 means B is more inﬂuential
than A. There are 3000 training samples and 2500
testing samples in our dataset. Given a test sample,
our job is to predict which individual in this test
sample is more inﬂuential.
B. Data Preprocessing

Before applying different models on our data, we
may want to preprocess it ﬁrst. For linear models,
the hypothesis function has the following form:

hθ(x) = g(θT x), θ, x ∈ R22

(1)

which results in a linear decision boundary,
i.e.
when θT x ≥ b, we predict 1; otherwise, we predict
0. Note that for a training example, if we change the
order A and B (that is, exchange the last 11 features
of the training example with its ﬁrst 11 features),
the label should also be reversed (1 becomes 0,
and 0 becomes 1). Thus the coefﬁcients of the
ﬁrst 11 features must be opposite numbers of the
coefﬁcients of the last 11 features, i.e.
θj = −θj+11, j = 1, 2, ..., 11

(2)
where θj is the jth entry of θ. So there are only 11
independent parameters in θ, and thus we can use
only 11 features to represent an example. We choose
z as our new representation of training example as
follows:

zj = xj − xj+11, j = 1, 2, ..., 11

(3)

where zj and xj are the jth attribute of z and x
respectively. Note that this preprocessing method
will only be used in linear models, i.e. Logistic
Regression and SVM. Besides this method, we
also use other preprocessing methods such K-means
algorithm, which will be discussed in their own
models.

III. SYSTEM MODELS

A. Follower Count Benchmark

than B. Using this method,

First, we tried a quite straightforward and trivial
method based only on the number of followers,
i.e.
if A has more followers than B, then A is
more inﬂuential
the
test accuracy is about 70.2%. This result shows
that the number of followers is a strong indicator
of the inﬂuencers. However, 70.2% is not good
enough for our prediction and this result will be
mainly considered as a benchmark. After adding
more features and using more general models, we
hope to get a better prediction on the test samples.

B. Logistic Regression

First, we preprocess the original dataset using the
previous method. After preprocessing the original
dataset, different attributes have different ranges
which vary a lot. Thus the ﬁrst thing to do is to
handle the data to make it more uniform and easy
to deal with. So in order to achieve this, we have a
normalization on the dataset. For each feature, we
do the following normalization

¯z(i)
j =

j(cid:113)(cid:80)m

z(i)
i=1 z(i)2

j

where m is the number of training examples and
n = 11 is the number of features.

Because our job is to predict who is more inﬂuen-
tial given two persons, the number of followers, as
we know, plays a signiﬁcant rule in the prediction.
A person with more followers than the other is more
likely to be judged inﬂuential by users. So we will
multiply the normalization of the ﬁrst feature, i.e.
number of followers, by a positive constant factor

which is greater than 1 to make it more inﬂuential
than other features on the prediction.

For this logistic regression problem, we have two
choices— gradient ascent and Newton’s method
to achieve the parameter θ. Since the number of
features n = 11 is not very large, so it will not take
much time to compute the inverse of a n×n matrix.
Thus Newton’s method should converge faster in
this problem and we choose Newton’s method as our
implementation. The update rule of the Newton’s
method is:

θ := θ − H−1∇θl(θ)

−(cid:80)m

i=1 hθ(¯z(i))(1 − hθ(¯z(i)))¯z(i)

(5)
where H ∈ Rn×n is the Hessian matrix and Hjk =
k , ∇θl(θ) =
¯Z((cid:126)y − hθ( ¯Z)).
Furthermore, we will add cross validation and
feature selection to this model, which will be dis-
cussed in details in section IV. We write our own
code to implement the Newton’s method.
C. SVM

j ¯z(i)

in

II-B. We
a

First, we also use the preprocessing method
mentioned
implement
SVM
through libsvm,
at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm/,
with
function [2]
l2
[3]. Using SVM model, the problem formulation
becomes:

regularization and linear kernel

library offered online

m(cid:88)

(cid:107)w(cid:107)2 + C

1
2

min
w,b,ξ
s.t. y(i)(wT ¯z(i) + b) ≥ 1 − ξi, i = 1, 2, ..., m

i=1

ξ2
i

(6)

ξi ≥ 0, i = 1, 2, ..., m

we let C = 1 as its default value.

Moreover, we add cross validation and feature
selection to this model, which refers to section IV.
D. Naive Bayes

After using discriminative learning algorithms,
we may want to try some generative learning al-
gorithms. The distribution of the original data is
probably not Gaussian distribution, so Gaussian
discriminant analysis can hardly work. Therefore,
we choose the Naive Bayes model. In this case, we

, z = 1, 2, ..., n

(4)

The box constraint C in (6) is customizable and

use the multinomial Naive Bayes model, which is
more general.

Before we apply Naive Bayes algorithm, we need
to discretize our dataset ﬁrst. For this purpose,
we choose K-means algorithm to discretize each
attribute into several clusters and each cluster cor-
responds to a class. Applying clustering algorithm
like K-means can help us distinguish users from
different levels. For instance, a ﬁlm star may have
a million followers while a normal user can only
have hundreds of followers and in this case, their
difference can’t be ignored and we need to put them
into different classes.

In the original dataset, some attributes have very
large ranges. Thus, we consider using logarithm
function on the original dataset before applying K-
means. We will compare the performance using
logarithm function to not using it in section V.

Once we have discretized the data, we can apply
multinomial Naive Bayes model to it. Assume the
jth attribute has cj classes {1, 2, ..., cj}, we give
the parameters achieving maximum likelihood as
follows:

(cid:80)m
i=1 I{y(i) = 1}

φy =

m

(cid:80)m
(cid:80)m
j = k ∩ y(i) = 1}
i=1 I{x(i)
i=1 I{y(i) = 1}

(7)

, k = 1, 2, ..., cj

φjk|y=1 =

(8)
where j = 1, 2, ..., 22 corresponding to 22 features.
φy is probability of y = 1; φjk|y=1 is the probability
that the jth attribute is class k given y = 1; φjk|y=0
can be derived similarly and thus we don’t give its
expression here. With the above parameters and a
test sample y(i), we predict 1 if and only if:

Using the formula in (9), we can predict on
the testing set. However, one thing we haven’t
speciﬁed is the numbers of classes of each attribute
and changing this parameter, we may get different
testing accuracies. This parameter can be regarded
as a 22-dimensional vector, i.e. c = [c1, c2, ..., c22]T .
If we use a deterministic initial state instead of
random initialization in the K-means algorithm, the
clustering result is also deterministic given the num-
ber of clusters. In this case, all parameters are deter-
mined by the vector c and we call it discretization
parameter. Thus, the testing accuracy is a function of
c, say f (c). By changing c, we may achieve different
accuracies and thus we can optimize the accuracy.
We provide a coordinate ascent based algorithm as
follows:

• Initialize c = [2, 2, ..., 2]T .
• Repeat until convergence:

For j = 1, 2, ..., 22

cj := arg maxˆcj f (c1, c2, ..., ˆcj, ..., c22)
where ˆcj = cj − d, ..., cj, ..., cj + d

The parameter d denotes the maximum step we
can take in one iteration, which is adjustable. This
algorithm has high computational complexity and
thus we haven’t tested and improved it, which could
be considered as a choice in future work. In section
V, we choose some speciﬁc values of c and illustrate
the performance of Naive Bayes. We write our own
code to implement the K-means and Naive Bayes
algorithm.

E. Neural Network

The Naive Bayes model introduced in the pre-
vious section is a nonlinear model. However, this
results from our nonlinear preprocessing methods,
which makes Naive Bayes itself not that ”nonlin-
ear”. Besides linear models, we still want to try
some nonlinear models with high capacity. For this
reason, Neural Network might be a good choice.

Fig. 1. Neural Network Architecture in Matlab

22(cid:89)

j=1

p(y(i) = 1|x(i)) ≥ p(y(i) = 0|x(i)) ⇔

p(x(i)|y(i) = 1)p(y(i) = 1) ≥ p(x(i)|y(i) = 0)p(y(i) = 0)

⇔ φy

|y=1 ≥ (1 − φy)

φjy(i)

j

φjy(i)

j

|y=0

(9)
The decision boundary in (9) is linear. However,
this model is nonlinear because K-means algorithm
and logarithm function are nonlinear mappings.

22(cid:89)

j=1

We use the Matlab Neural Network Pattern
Recognition toolbox to implement the Neural Net-
work algorithm, which is designed for classiﬁcation
problems. The network architecture is given in Fig.
1, which is a two-layer feed forward network, with
sigmoid hidden neurons and softmax output neuron
[4].The number of neurons in the hidden layer is
customizable and we will compare the performances
using different numbers of hidden neurons in section
V.

IV. MODEL SELECTION

A. Cross Validation

We use hold-out cross validation and 30% of the
training set are considered as validation set. What’s
more, we randomly split the training set for 100
times (we get 100 hypothesis functions) and pick
the one with the smallest cross validation error. We
also use k-fold cross validation and we will assign
different values to k in section V in order to choose
the best one.
B. Feature Selection

In our dataset, it is fairly possible that some fea-
tures are much more indicative than other features,
e.g. the number of followers is a strong indicator.
We have 11 features for the linear models as stated
above. Although the number of features are not very
large, the test error is large and there may be only
a subset of these 11 features that are relevant to the
result.

In this problem, we use forward search as our
feature selection algorithm, which is designed to
ﬁnd the most relevant features. In addition,
the
maximum number of features is regarded as an
input argument in our implementation. Thus, we can
change this parameter to optimize the performance
and sort features from the most relevant to the least
relevant, which will be discussed in section V.

V. RESULTS & DISCUSSIONS

A. Logistic Regression & SVM

In this section, we apply cross validation and
feature selection to the linear models, i.e. Logistic
Regression and SVM. The following ﬁgures com-
pare the performance among different models.

In Fig. 2, we use different cross validation options
on linear models, such as hold-out and k-fold,

Fig. 2. Test accuracy vs. cross validation options for linear models

and compare their test accuracies. It is shown that
cross validation can improve the test accuracy of
SVM, while having no strong effect on Logistic
Regression. What’s more, the performance of SVM
is better than Logistic Regression using cross vali-
dation.

Fig. 3. Test accuracy vs. discretization parameter for linear models

Fig. 3 shows the test accuracies of SVM and Lo-
gistic Regression with different numbers of features
selected. As is shown above, the performance of
SVM is better, compared with logistic regression.
The best performance of SVM is achieved when
4 features are selected, which shows that some
features are weak indicators. After feature selection,
we sort the features from the most relevant to the
least relevant as follows:

SVM 9
LR
3

6
6

7
8

10
2

8
9

5
5

3
4

2
7

1
1

4
11

11
10

where the corresponding feature name refers to sec-
tion II-A. From this table, we can see that although
the sort results shown above are quite different for
these two methods, their test accuracies are very
close. Using linear models, the best accuracy we
can achieve is 76.00% (highest test accuracy in Fig.
2 and Fig. 3).

B. Naive Bayes

In this model, we compare the performances of
different discretization parameters. Here we assume
that all features have the same number of classes
(the coordinate ascent based algorithm takes too
much time). We also mentioned in section III-D that
logarithm function is used to preprocess the data.

Fig. 4. Test accuracies vs. number of classes of each feature

In Fig. 4, we also compare the performance with
logarithm preprocessing and the one without loga-
rithm processing. We can see that logarithm process-
ing helps increase the test accuracy and when the
number of classes of each feature increases, their
performances approaches. In addition, it turns out
that different discretization parameters don’t have
much impact on test accuracy. In this case, the best
test accuracy is 76.48%.

C. Neural Network

In this model, we use 50 neurons in the hidden
layer. We also apply hold-out cross validation with
30% as the validation set. The ROC (Receiver
Operating Characteristic) curves are shown in Fig.
5. After training the two-layer network, we get the
test accuracy 79.04%, which corresponds to the
area under the test ROC curve.

Fig. 5. Receiver operating characteristic

VI. CONCLUSION & FUTURE WORKS

From the above results, we can conclude that the
best accuracy we can achieve is about 76% using
linear models, which is not much better than our
benchmark 70.2%. This suggests that the testing
examples might not be linearly separable. The test
accuracy of Naive Bayes is close to linear models.
However, if we can apply the coordinate ascent
based algorithm, we may achieve much better per-
formance, which is a good choice for future works.
Furthermore, the accuracy of nonlinear models such
as Neural Network is better than linear models,
although it’s not as good as we expected. Moreover,
there might be data corruptions since sometimes
human judgement can be highly biased. In order to
achieve better performance, we can either try more
nonlinear models or use decision trees to improve
it.

REFERENCES

[1] J. Furnkranz and E. Hullermeier. Preference Learning: A Tutorial

Introducton, DS 2011, Espoo, Finland, Oct 2011.

Morgan Kaufmann, 1996.

[2] Hsu C W, Chang C C, Lin C J. A practical guide to support

vector classiﬁcation[J]. 2003.

[3] Chang C C, Lin C J. LIBSVM: a library for support vector
machines[J]. ACM Transactions on Intelligent Systems and Tech-
nology (TIST), 2011, 2(3): 27.

[4] Swingler K. Applying neural networks: a practical guide[M].

23456789100.620.640.660.680.70.720.740.760.780.8# Classes of each attributeAccuracy  With LogarithmWithout Logarithm00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Positive RateTrue Positive RateTraining ROC00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Positive RateTrue Positive RateValidation ROC00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Positive RateTrue Positive RateTest ROC00.20.40.60.8100.10.20.30.40.50.60.70.80.91False Positive RateTrue Positive RateAll ROC