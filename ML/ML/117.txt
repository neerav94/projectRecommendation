Predicting Political Ideology Using Campaign

Finance Data

Keri A. McKiernan and Joe A. Napoli

December 11, 2015

Introduction

Public interest has become increasingly focused on the role of money in politics. Speciﬁcally, the Citizens
United v. FEC Supreme Court ruling maintained the legality of unrestricted political expenditures by
corporate and union entities. As a result, there has been a proliferation of super PAC, or ‘political action
committee’, organizations. While these organizations are not permitted to make contributions directly to
campaigns, they may engage in unlimited independent spending and there is no restriction on the amount
of funds they can accept from donors. Such legal developments beg the question: Can one predict generally
the inﬂuence of donors over politicians to whom they give? As the ﬂux of money into the political system
continues to increase, this remains a critical question. Our project builds largely on previous research by
Poole and Rosenthal and makes use of freely available databases of campaign ﬁnance information. We aim to
predict quantitative scores of political ideology using campaign contribution proﬁles; speciﬁcally, the input
to our algorithm is a matrix of normalized vectors of campaign contribution data, partitioned by industry,
the learning algorithm used is Support Vector Regression (SVR), and the output is a score for each candidate
in the interval [−1, 1] indicating a position on the political ideological spectrum.

Related Work

The research of Poole and Rosenthal has focused on quantifying the political ideology of politicians [6]. In
particular, they actively develop methods for calculating ‘ideal points’ for candidates. For example, the
DW-NOMINATE method calculates a legislator’s overall probability of voting ‘yea’ on a piece of legislation
as the sum of a deterministic utility value and a normally distributed random error [2]. It calculates ‘ideal
point’ coordinates for legislators by maximizing the log likelihood function

T(cid:88)

pt(cid:88)

L =

qt(cid:88)

2(cid:88)

Cijτ t ln Pijτ t

t=1

i=1

j=1

τ =1

where Pijτ t is the probability of voting for choice τ (yes or no) and Cijτ t = 1 if that probability accurately
predicts the vote [2]. Indexes j, i, and t sum over roll call votes, legislators, and legislative sessions, respec-
tively. These ideal points are constrained to lie within the interval [−1, 1] and are two-dimensional quantities.
A common interpretation of the ﬁrst coordinate is that it reﬂects the divide between the Republican and
Democratic parties, whereas the second coordinate is more highly correlated with intra-party division. It
is important to emphasize that these methods seek to estimate ideal points using roll call voting records.
While a full congressional voting record would be unavailable for candidates who are new to oﬃce, campaign
ﬁnance data are, ideally, readily available. Thus, we think it would be helpful to be able to predict ideal
points for candidates even before they have established a congressional voting record. One could then use
these scores to predict how legislators will vote on particular pieces of legislation and to draw quantitative
comparisons between them.

Much previous work has focused on connecting political contributions to speciﬁc votes cast in congress [9,
8]. Furthermore, Adam Bonica of Stanford has developed a correspondence analysis model for predicting
DW-NOMINATE scores using data at the resolution of individual contributions to PACs [1]. We think this
approach has the deﬁciency of being computationally expensive compared to a coarser graining of the space
of contributions, and we posit that we can recover the ideal point of a candidate just by using a contribution
proﬁle partitioned by sector. We have collated freely available campaign ﬁnance [5] and DW-NOMINATE

1

Predicting Political Ideology Using Campaign

Finance Data

Keri A. McKiernan and Joe A. Napoli

December 11, 2015

Introduction

Public interest has become increasingly focused on the role of money in politics. Speciﬁcally, the Citizens
United v. FEC Supreme Court ruling maintained the legality of unrestricted political expenditures by
corporate and union entities. As a result, there has been a proliferation of super PAC, or ‘political action
committee’, organizations. While these organizations are not permitted to make contributions directly to
campaigns, they may engage in unlimited independent spending and there is no restriction on the amount
of funds they can accept from donors. Such legal developments beg the question: Can one predict generally
the inﬂuence of donors over politicians to whom they give? As the ﬂux of money into the political system
continues to increase, this remains a critical question. Our project builds largely on previous research by
Poole and Rosenthal and makes use of freely available databases of campaign ﬁnance information. We aim to
predict quantitative scores of political ideology using campaign contribution proﬁles; speciﬁcally, the input
to our algorithm is a matrix of normalized vectors of campaign contribution data, partitioned by industry,
the learning algorithm used is Support Vector Regression (SVR), and the output is a score for each candidate
in the interval [−1, 1] indicating a position on the political ideological spectrum.

Related Work

The research of Poole and Rosenthal has focused on quantifying the political ideology of politicians [6]. In
particular, they actively develop methods for calculating ‘ideal points’ for candidates. For example, the
DW-NOMINATE method calculates a legislator’s overall probability of voting ‘yea’ on a piece of legislation
as the sum of a deterministic utility value and a normally distributed random error [2]. It calculates ‘ideal
point’ coordinates for legislators by maximizing the log likelihood function

T(cid:88)

pt(cid:88)

L =

qt(cid:88)

2(cid:88)

Cijτ t ln Pijτ t

t=1

i=1

j=1

τ =1

where Pijτ t is the probability of voting for choice τ (yes or no) and Cijτ t = 1 if that probability accurately
predicts the vote [2]. Indexes j, i, and t sum over roll call votes, legislators, and legislative sessions, respec-
tively. These ideal points are constrained to lie within the interval [−1, 1] and are two-dimensional quantities.
A common interpretation of the ﬁrst coordinate is that it reﬂects the divide between the Republican and
Democratic parties, whereas the second coordinate is more highly correlated with intra-party division. It
is important to emphasize that these methods seek to estimate ideal points using roll call voting records.
While a full congressional voting record would be unavailable for candidates who are new to oﬃce, campaign
ﬁnance data are, ideally, readily available. Thus, we think it would be helpful to be able to predict ideal
points for candidates even before they have established a congressional voting record. One could then use
these scores to predict how legislators will vote on particular pieces of legislation and to draw quantitative
comparisons between them.

Much previous work has focused on connecting political contributions to speciﬁc votes cast in congress [9,
8]. Furthermore, Adam Bonica of Stanford has developed a correspondence analysis model for predicting
DW-NOMINATE scores using data at the resolution of individual contributions to PACs [1]. We think this
approach has the deﬁciency of being computationally expensive compared to a coarser graining of the space
of contributions, and we posit that we can recover the ideal point of a candidate just by using a contribution
proﬁle partitioned by sector. We have collated freely available campaign ﬁnance [5] and DW-NOMINATE

1

ideal point [4] datasets. The original scope of our project has been narrowed due to a reduction in the size of
our group. We originally planned to investigate anomalous voting behavior and the ability to predict it solely
by examining campaign ﬁnance data, but shifted our focus to training a model to predict DW-NOMINATE
ideal point coordinates.

Dataset and Features

Campaign ﬁnance data for 375 candidates (as determined by sampling from the DW-NOMINATE scores)
over years 2010, 2012, and 2014 were obtained using the Open Secrets python API [5]. Of the available
data, we chose to featurize by sector. For each sector, individual and PAC contributions were combined and
normalized on a per candidate basis. This amounted to 12 features in total. A heatmap of the total feature
set is illustrated below.

We also include a plot of the distribution of DW-NOMINATE scores, labeling Democrats and Republi-
cans using their traditional party colors. We note that the dataset appears linearly separable in the ﬁrst
dimension of the ideal point coordinates. This suggests that a classiﬁcation algorithm may eﬀectively dis-
tinguish between the parties in case regression algorithm fails to quantitatively capture the distribution of
scores. Finally, we note that for each party the second dimension of the DW-NOMINATE scores is not as
easily separable and instead spans the interval [−1, 1]. This would be a natural extension of the present work.

Figure 1: Heatmap of complete feature set

Figure 2: DW-NOMINATE score distribu-
tion

Methods

The learning algorithm used in this work was epsilon-Support Vector Regression (-SVR), using a radial basis
function (RBF) kernel. In -SVR, a nonlinear function is constructed through the training of a linear func-
tion, f (x), in a higher dimensional inner product space deﬁned by a kernel function, K(xi, xj) = φ(xi)T φ(xj).
The objective of the function training is to ensure all training data lies within  of all target data. Without
supplementary information about a particular data set, SV learning algorithms are consider the best “oﬀ-
the-shelf” supervised learning algorithm [3].
Given a training set, S = {(x0, y0), ..., (xn, yn)|xi ∈ R}, -SVR can be formulated into the following convex
optimization problem, given here in its primal form:

n(cid:88)

i=1

(ζi + ζ∗
i )

min

w,b,ζ,ζ∗

1
2

wT w + C

2

Predicting Political Ideology Using Campaign

Finance Data

Keri A. McKiernan and Joe A. Napoli

December 11, 2015

Introduction

Public interest has become increasingly focused on the role of money in politics. Speciﬁcally, the Citizens
United v. FEC Supreme Court ruling maintained the legality of unrestricted political expenditures by
corporate and union entities. As a result, there has been a proliferation of super PAC, or ‘political action
committee’, organizations. While these organizations are not permitted to make contributions directly to
campaigns, they may engage in unlimited independent spending and there is no restriction on the amount
of funds they can accept from donors. Such legal developments beg the question: Can one predict generally
the inﬂuence of donors over politicians to whom they give? As the ﬂux of money into the political system
continues to increase, this remains a critical question. Our project builds largely on previous research by
Poole and Rosenthal and makes use of freely available databases of campaign ﬁnance information. We aim to
predict quantitative scores of political ideology using campaign contribution proﬁles; speciﬁcally, the input
to our algorithm is a matrix of normalized vectors of campaign contribution data, partitioned by industry,
the learning algorithm used is Support Vector Regression (SVR), and the output is a score for each candidate
in the interval [−1, 1] indicating a position on the political ideological spectrum.

Related Work

The research of Poole and Rosenthal has focused on quantifying the political ideology of politicians [6]. In
particular, they actively develop methods for calculating ‘ideal points’ for candidates. For example, the
DW-NOMINATE method calculates a legislator’s overall probability of voting ‘yea’ on a piece of legislation
as the sum of a deterministic utility value and a normally distributed random error [2]. It calculates ‘ideal
point’ coordinates for legislators by maximizing the log likelihood function

T(cid:88)

pt(cid:88)

L =

qt(cid:88)

2(cid:88)

Cijτ t ln Pijτ t

t=1

i=1

j=1

τ =1

where Pijτ t is the probability of voting for choice τ (yes or no) and Cijτ t = 1 if that probability accurately
predicts the vote [2]. Indexes j, i, and t sum over roll call votes, legislators, and legislative sessions, respec-
tively. These ideal points are constrained to lie within the interval [−1, 1] and are two-dimensional quantities.
A common interpretation of the ﬁrst coordinate is that it reﬂects the divide between the Republican and
Democratic parties, whereas the second coordinate is more highly correlated with intra-party division. It
is important to emphasize that these methods seek to estimate ideal points using roll call voting records.
While a full congressional voting record would be unavailable for candidates who are new to oﬃce, campaign
ﬁnance data are, ideally, readily available. Thus, we think it would be helpful to be able to predict ideal
points for candidates even before they have established a congressional voting record. One could then use
these scores to predict how legislators will vote on particular pieces of legislation and to draw quantitative
comparisons between them.

Much previous work has focused on connecting political contributions to speciﬁc votes cast in congress [9,
8]. Furthermore, Adam Bonica of Stanford has developed a correspondence analysis model for predicting
DW-NOMINATE scores using data at the resolution of individual contributions to PACs [1]. We think this
approach has the deﬁciency of being computationally expensive compared to a coarser graining of the space
of contributions, and we posit that we can recover the ideal point of a candidate just by using a contribution
proﬁle partitioned by sector. We have collated freely available campaign ﬁnance [5] and DW-NOMINATE

1

ideal point [4] datasets. The original scope of our project has been narrowed due to a reduction in the size of
our group. We originally planned to investigate anomalous voting behavior and the ability to predict it solely
by examining campaign ﬁnance data, but shifted our focus to training a model to predict DW-NOMINATE
ideal point coordinates.

Dataset and Features

Campaign ﬁnance data for 375 candidates (as determined by sampling from the DW-NOMINATE scores)
over years 2010, 2012, and 2014 were obtained using the Open Secrets python API [5]. Of the available
data, we chose to featurize by sector. For each sector, individual and PAC contributions were combined and
normalized on a per candidate basis. This amounted to 12 features in total. A heatmap of the total feature
set is illustrated below.

We also include a plot of the distribution of DW-NOMINATE scores, labeling Democrats and Republi-
cans using their traditional party colors. We note that the dataset appears linearly separable in the ﬁrst
dimension of the ideal point coordinates. This suggests that a classiﬁcation algorithm may eﬀectively dis-
tinguish between the parties in case regression algorithm fails to quantitatively capture the distribution of
scores. Finally, we note that for each party the second dimension of the DW-NOMINATE scores is not as
easily separable and instead spans the interval [−1, 1]. This would be a natural extension of the present work.

Figure 1: Heatmap of complete feature set

Figure 2: DW-NOMINATE score distribu-
tion

Methods

The learning algorithm used in this work was epsilon-Support Vector Regression (-SVR), using a radial basis
function (RBF) kernel. In -SVR, a nonlinear function is constructed through the training of a linear func-
tion, f (x), in a higher dimensional inner product space deﬁned by a kernel function, K(xi, xj) = φ(xi)T φ(xj).
The objective of the function training is to ensure all training data lies within  of all target data. Without
supplementary information about a particular data set, SV learning algorithms are consider the best “oﬀ-
the-shelf” supervised learning algorithm [3].
Given a training set, S = {(x0, y0), ..., (xn, yn)|xi ∈ R}, -SVR can be formulated into the following convex
optimization problem, given here in its primal form:

n(cid:88)

i=1

(ζi + ζ∗
i )

min

w,b,ζ,ζ∗

1
2

wT w + C

2

s.t. yi − wT φ(xi) − b ≤  + ζi,
wT φ(xi) + b − yi ≤  + ζ∗
i ,
ζi, ζ∗

i ≥ 0, i = 1, ..., n

where φ(x) is the mapping to the higher dimension space, C determines the degree of regularization (lower
C leads to a smoother solution), and ζ, ζ∗ are slack variables which allow for constraint relaxation in the
case it is required. This problem is often simpler to solve in its dual form (derived through the method of
Lagrange multipliers):

min
α,α∗

1
2

(α − α∗)T Q(α − α∗) + εeT (α + α∗) − yT (α − α∗)

s.t. (α − α∗) = 0

0 ≤ αi, α∗

i ≤ C, i = 1, ..., n

where Qij ≡ K(xi, xj), and Q is the matrix of these values. This problem can be solved to yield the following:

n(cid:88)

f (x) =

(αi − α∗

i )K(xi, x) + b

(1)

(2)

where the RBF kernel is given by the following:

K(xi, xj) = exp(cid:2)−γ|xi − xj|2(cid:3)

i=1

and where γ describes the inﬂuence of each individual training example [7].

Experiments/Results/Discussion

We chose a ﬁnal model after performing a randomized optimization in the space of hyperparameters for
-SVR, for multiple kernel types. This required specifying distributions over which to sample the hyper-
parameters. We speciﬁed a separate exponential distribution for each hyperparameter, and estimated the
decay scale of each distribution by ﬁrst performing an exhaustive grid search over values of the hyper-
parameters spanning several orders of magnitude.
In both the grid and randomized search schemes, the
coeﬃcient of determination (R2) was used as the scoring metric for ranking all sampled models and was
estimated for each candidate model using 5-fold cross-validation on a randomly sampled set of training data
that comprised 80% of the data set. Each randomized hyperparameter search proceeded for 30,000 iterations.

RBF kernel Linear kerel

C

γ

Training score

Test score

10.77
0.047
9.97
0.82
0.83

55.33
0.31
n/a
0.70
0.64

Figure 3: Summary of hyperparameters and model scores

The ﬁnal hyperparameters for the linear and radial basis function (RBF) kernels are summarized in the
table below alongside their corresponding training and test scores. We omit results for the polynomial kernel
because the calculation did not converge and thus results could only be obtained via a cruder randomized
search. We also note that of the best models identiﬁed by the randomized search, for each kernel type
there were several distinct sets of hyperparameters that yielded very similar training scores. For the RBF
kernel speciﬁcally, the hypermarameters we selected represent a compromise between the values of C and

3

Predicting Political Ideology Using Campaign

Finance Data

Keri A. McKiernan and Joe A. Napoli

December 11, 2015

Introduction

Public interest has become increasingly focused on the role of money in politics. Speciﬁcally, the Citizens
United v. FEC Supreme Court ruling maintained the legality of unrestricted political expenditures by
corporate and union entities. As a result, there has been a proliferation of super PAC, or ‘political action
committee’, organizations. While these organizations are not permitted to make contributions directly to
campaigns, they may engage in unlimited independent spending and there is no restriction on the amount
of funds they can accept from donors. Such legal developments beg the question: Can one predict generally
the inﬂuence of donors over politicians to whom they give? As the ﬂux of money into the political system
continues to increase, this remains a critical question. Our project builds largely on previous research by
Poole and Rosenthal and makes use of freely available databases of campaign ﬁnance information. We aim to
predict quantitative scores of political ideology using campaign contribution proﬁles; speciﬁcally, the input
to our algorithm is a matrix of normalized vectors of campaign contribution data, partitioned by industry,
the learning algorithm used is Support Vector Regression (SVR), and the output is a score for each candidate
in the interval [−1, 1] indicating a position on the political ideological spectrum.

Related Work

The research of Poole and Rosenthal has focused on quantifying the political ideology of politicians [6]. In
particular, they actively develop methods for calculating ‘ideal points’ for candidates. For example, the
DW-NOMINATE method calculates a legislator’s overall probability of voting ‘yea’ on a piece of legislation
as the sum of a deterministic utility value and a normally distributed random error [2]. It calculates ‘ideal
point’ coordinates for legislators by maximizing the log likelihood function

T(cid:88)

pt(cid:88)

L =

qt(cid:88)

2(cid:88)

Cijτ t ln Pijτ t

t=1

i=1

j=1

τ =1

where Pijτ t is the probability of voting for choice τ (yes or no) and Cijτ t = 1 if that probability accurately
predicts the vote [2]. Indexes j, i, and t sum over roll call votes, legislators, and legislative sessions, respec-
tively. These ideal points are constrained to lie within the interval [−1, 1] and are two-dimensional quantities.
A common interpretation of the ﬁrst coordinate is that it reﬂects the divide between the Republican and
Democratic parties, whereas the second coordinate is more highly correlated with intra-party division. It
is important to emphasize that these methods seek to estimate ideal points using roll call voting records.
While a full congressional voting record would be unavailable for candidates who are new to oﬃce, campaign
ﬁnance data are, ideally, readily available. Thus, we think it would be helpful to be able to predict ideal
points for candidates even before they have established a congressional voting record. One could then use
these scores to predict how legislators will vote on particular pieces of legislation and to draw quantitative
comparisons between them.

Much previous work has focused on connecting political contributions to speciﬁc votes cast in congress [9,
8]. Furthermore, Adam Bonica of Stanford has developed a correspondence analysis model for predicting
DW-NOMINATE scores using data at the resolution of individual contributions to PACs [1]. We think this
approach has the deﬁciency of being computationally expensive compared to a coarser graining of the space
of contributions, and we posit that we can recover the ideal point of a candidate just by using a contribution
proﬁle partitioned by sector. We have collated freely available campaign ﬁnance [5] and DW-NOMINATE

1

ideal point [4] datasets. The original scope of our project has been narrowed due to a reduction in the size of
our group. We originally planned to investigate anomalous voting behavior and the ability to predict it solely
by examining campaign ﬁnance data, but shifted our focus to training a model to predict DW-NOMINATE
ideal point coordinates.

Dataset and Features

Campaign ﬁnance data for 375 candidates (as determined by sampling from the DW-NOMINATE scores)
over years 2010, 2012, and 2014 were obtained using the Open Secrets python API [5]. Of the available
data, we chose to featurize by sector. For each sector, individual and PAC contributions were combined and
normalized on a per candidate basis. This amounted to 12 features in total. A heatmap of the total feature
set is illustrated below.

We also include a plot of the distribution of DW-NOMINATE scores, labeling Democrats and Republi-
cans using their traditional party colors. We note that the dataset appears linearly separable in the ﬁrst
dimension of the ideal point coordinates. This suggests that a classiﬁcation algorithm may eﬀectively dis-
tinguish between the parties in case regression algorithm fails to quantitatively capture the distribution of
scores. Finally, we note that for each party the second dimension of the DW-NOMINATE scores is not as
easily separable and instead spans the interval [−1, 1]. This would be a natural extension of the present work.

Figure 1: Heatmap of complete feature set

Figure 2: DW-NOMINATE score distribu-
tion

Methods

The learning algorithm used in this work was epsilon-Support Vector Regression (-SVR), using a radial basis
function (RBF) kernel. In -SVR, a nonlinear function is constructed through the training of a linear func-
tion, f (x), in a higher dimensional inner product space deﬁned by a kernel function, K(xi, xj) = φ(xi)T φ(xj).
The objective of the function training is to ensure all training data lies within  of all target data. Without
supplementary information about a particular data set, SV learning algorithms are consider the best “oﬀ-
the-shelf” supervised learning algorithm [3].
Given a training set, S = {(x0, y0), ..., (xn, yn)|xi ∈ R}, -SVR can be formulated into the following convex
optimization problem, given here in its primal form:

n(cid:88)

i=1

(ζi + ζ∗
i )

min

w,b,ζ,ζ∗

1
2

wT w + C

2

s.t. yi − wT φ(xi) − b ≤  + ζi,
wT φ(xi) + b − yi ≤  + ζ∗
i ,
ζi, ζ∗

i ≥ 0, i = 1, ..., n

where φ(x) is the mapping to the higher dimension space, C determines the degree of regularization (lower
C leads to a smoother solution), and ζ, ζ∗ are slack variables which allow for constraint relaxation in the
case it is required. This problem is often simpler to solve in its dual form (derived through the method of
Lagrange multipliers):

min
α,α∗

1
2

(α − α∗)T Q(α − α∗) + εeT (α + α∗) − yT (α − α∗)

s.t. (α − α∗) = 0

0 ≤ αi, α∗

i ≤ C, i = 1, ..., n

where Qij ≡ K(xi, xj), and Q is the matrix of these values. This problem can be solved to yield the following:

n(cid:88)

f (x) =

(αi − α∗

i )K(xi, x) + b

(1)

(2)

where the RBF kernel is given by the following:

K(xi, xj) = exp(cid:2)−γ|xi − xj|2(cid:3)

i=1

and where γ describes the inﬂuence of each individual training example [7].

Experiments/Results/Discussion

We chose a ﬁnal model after performing a randomized optimization in the space of hyperparameters for
-SVR, for multiple kernel types. This required specifying distributions over which to sample the hyper-
parameters. We speciﬁed a separate exponential distribution for each hyperparameter, and estimated the
decay scale of each distribution by ﬁrst performing an exhaustive grid search over values of the hyper-
parameters spanning several orders of magnitude.
In both the grid and randomized search schemes, the
coeﬃcient of determination (R2) was used as the scoring metric for ranking all sampled models and was
estimated for each candidate model using 5-fold cross-validation on a randomly sampled set of training data
that comprised 80% of the data set. Each randomized hyperparameter search proceeded for 30,000 iterations.

RBF kernel Linear kerel

C

γ

Training score

Test score

10.77
0.047
9.97
0.82
0.83

55.33
0.31
n/a
0.70
0.64

Figure 3: Summary of hyperparameters and model scores

The ﬁnal hyperparameters for the linear and radial basis function (RBF) kernels are summarized in the
table below alongside their corresponding training and test scores. We omit results for the polynomial kernel
because the calculation did not converge and thus results could only be obtained via a cruder randomized
search. We also note that of the best models identiﬁed by the randomized search, for each kernel type
there were several distinct sets of hyperparameters that yielded very similar training scores. For the RBF
kernel speciﬁcally, the hypermarameters we selected represent a compromise between the values of C and

3

γ. While relatively high values of C correspond to overﬁtting by overpenalizing large deviations from the
target, constraining C too aggressively would encourage the bandwidth of the kernel function, γ, to expand
and potentially overﬁt the data. Therefore, cross-validation is critical in evaluating our ﬁnal selection.

We ﬁnd that the RBF kernel is better able to capture the nonlinearities and regional variation of the data
by virtue of its implicit inﬁnite-dimensional feature mapping. In order to characterize the performance of
the ﬁnal model further and to compare it to the linear kernel model, we compute learning curves for both
showing the test score as a function of the fraction of data used for training. Deﬁciencies of the linear kernel
are readily apparent; its score saturates at about 0.65 to 0.7, within error, while the RBF kernel acheives
0.8 to 0.85, within error.

Figure 4: Learning curve comparison

In order to evaluate the structure of the feature data set, and gain qualitative insight into our model, principal
component analysis (PCA) was performed on the full training set. The top 3 principal components are
depicted below, along with supplementary data regarding the per party distributions of campaign donations
by sector.

Figure 5: Top 3 principal components

4

Predicting Political Ideology Using Campaign

Finance Data

Keri A. McKiernan and Joe A. Napoli

December 11, 2015

Introduction

Public interest has become increasingly focused on the role of money in politics. Speciﬁcally, the Citizens
United v. FEC Supreme Court ruling maintained the legality of unrestricted political expenditures by
corporate and union entities. As a result, there has been a proliferation of super PAC, or ‘political action
committee’, organizations. While these organizations are not permitted to make contributions directly to
campaigns, they may engage in unlimited independent spending and there is no restriction on the amount
of funds they can accept from donors. Such legal developments beg the question: Can one predict generally
the inﬂuence of donors over politicians to whom they give? As the ﬂux of money into the political system
continues to increase, this remains a critical question. Our project builds largely on previous research by
Poole and Rosenthal and makes use of freely available databases of campaign ﬁnance information. We aim to
predict quantitative scores of political ideology using campaign contribution proﬁles; speciﬁcally, the input
to our algorithm is a matrix of normalized vectors of campaign contribution data, partitioned by industry,
the learning algorithm used is Support Vector Regression (SVR), and the output is a score for each candidate
in the interval [−1, 1] indicating a position on the political ideological spectrum.

Related Work

The research of Poole and Rosenthal has focused on quantifying the political ideology of politicians [6]. In
particular, they actively develop methods for calculating ‘ideal points’ for candidates. For example, the
DW-NOMINATE method calculates a legislator’s overall probability of voting ‘yea’ on a piece of legislation
as the sum of a deterministic utility value and a normally distributed random error [2]. It calculates ‘ideal
point’ coordinates for legislators by maximizing the log likelihood function

T(cid:88)

pt(cid:88)

L =

qt(cid:88)

2(cid:88)

Cijτ t ln Pijτ t

t=1

i=1

j=1

τ =1

where Pijτ t is the probability of voting for choice τ (yes or no) and Cijτ t = 1 if that probability accurately
predicts the vote [2]. Indexes j, i, and t sum over roll call votes, legislators, and legislative sessions, respec-
tively. These ideal points are constrained to lie within the interval [−1, 1] and are two-dimensional quantities.
A common interpretation of the ﬁrst coordinate is that it reﬂects the divide between the Republican and
Democratic parties, whereas the second coordinate is more highly correlated with intra-party division. It
is important to emphasize that these methods seek to estimate ideal points using roll call voting records.
While a full congressional voting record would be unavailable for candidates who are new to oﬃce, campaign
ﬁnance data are, ideally, readily available. Thus, we think it would be helpful to be able to predict ideal
points for candidates even before they have established a congressional voting record. One could then use
these scores to predict how legislators will vote on particular pieces of legislation and to draw quantitative
comparisons between them.

Much previous work has focused on connecting political contributions to speciﬁc votes cast in congress [9,
8]. Furthermore, Adam Bonica of Stanford has developed a correspondence analysis model for predicting
DW-NOMINATE scores using data at the resolution of individual contributions to PACs [1]. We think this
approach has the deﬁciency of being computationally expensive compared to a coarser graining of the space
of contributions, and we posit that we can recover the ideal point of a candidate just by using a contribution
proﬁle partitioned by sector. We have collated freely available campaign ﬁnance [5] and DW-NOMINATE

1

ideal point [4] datasets. The original scope of our project has been narrowed due to a reduction in the size of
our group. We originally planned to investigate anomalous voting behavior and the ability to predict it solely
by examining campaign ﬁnance data, but shifted our focus to training a model to predict DW-NOMINATE
ideal point coordinates.

Dataset and Features

Campaign ﬁnance data for 375 candidates (as determined by sampling from the DW-NOMINATE scores)
over years 2010, 2012, and 2014 were obtained using the Open Secrets python API [5]. Of the available
data, we chose to featurize by sector. For each sector, individual and PAC contributions were combined and
normalized on a per candidate basis. This amounted to 12 features in total. A heatmap of the total feature
set is illustrated below.

We also include a plot of the distribution of DW-NOMINATE scores, labeling Democrats and Republi-
cans using their traditional party colors. We note that the dataset appears linearly separable in the ﬁrst
dimension of the ideal point coordinates. This suggests that a classiﬁcation algorithm may eﬀectively dis-
tinguish between the parties in case regression algorithm fails to quantitatively capture the distribution of
scores. Finally, we note that for each party the second dimension of the DW-NOMINATE scores is not as
easily separable and instead spans the interval [−1, 1]. This would be a natural extension of the present work.

Figure 1: Heatmap of complete feature set

Figure 2: DW-NOMINATE score distribu-
tion

Methods

The learning algorithm used in this work was epsilon-Support Vector Regression (-SVR), using a radial basis
function (RBF) kernel. In -SVR, a nonlinear function is constructed through the training of a linear func-
tion, f (x), in a higher dimensional inner product space deﬁned by a kernel function, K(xi, xj) = φ(xi)T φ(xj).
The objective of the function training is to ensure all training data lies within  of all target data. Without
supplementary information about a particular data set, SV learning algorithms are consider the best “oﬀ-
the-shelf” supervised learning algorithm [3].
Given a training set, S = {(x0, y0), ..., (xn, yn)|xi ∈ R}, -SVR can be formulated into the following convex
optimization problem, given here in its primal form:

n(cid:88)

i=1

(ζi + ζ∗
i )

min

w,b,ζ,ζ∗

1
2

wT w + C

2

s.t. yi − wT φ(xi) − b ≤  + ζi,
wT φ(xi) + b − yi ≤  + ζ∗
i ,
ζi, ζ∗

i ≥ 0, i = 1, ..., n

where φ(x) is the mapping to the higher dimension space, C determines the degree of regularization (lower
C leads to a smoother solution), and ζ, ζ∗ are slack variables which allow for constraint relaxation in the
case it is required. This problem is often simpler to solve in its dual form (derived through the method of
Lagrange multipliers):

min
α,α∗

1
2

(α − α∗)T Q(α − α∗) + εeT (α + α∗) − yT (α − α∗)

s.t. (α − α∗) = 0

0 ≤ αi, α∗

i ≤ C, i = 1, ..., n

where Qij ≡ K(xi, xj), and Q is the matrix of these values. This problem can be solved to yield the following:

n(cid:88)

f (x) =

(αi − α∗

i )K(xi, x) + b

(1)

(2)

where the RBF kernel is given by the following:

K(xi, xj) = exp(cid:2)−γ|xi − xj|2(cid:3)

i=1

and where γ describes the inﬂuence of each individual training example [7].

Experiments/Results/Discussion

We chose a ﬁnal model after performing a randomized optimization in the space of hyperparameters for
-SVR, for multiple kernel types. This required specifying distributions over which to sample the hyper-
parameters. We speciﬁed a separate exponential distribution for each hyperparameter, and estimated the
decay scale of each distribution by ﬁrst performing an exhaustive grid search over values of the hyper-
parameters spanning several orders of magnitude.
In both the grid and randomized search schemes, the
coeﬃcient of determination (R2) was used as the scoring metric for ranking all sampled models and was
estimated for each candidate model using 5-fold cross-validation on a randomly sampled set of training data
that comprised 80% of the data set. Each randomized hyperparameter search proceeded for 30,000 iterations.

RBF kernel Linear kerel

C

γ

Training score

Test score

10.77
0.047
9.97
0.82
0.83

55.33
0.31
n/a
0.70
0.64

Figure 3: Summary of hyperparameters and model scores

The ﬁnal hyperparameters for the linear and radial basis function (RBF) kernels are summarized in the
table below alongside their corresponding training and test scores. We omit results for the polynomial kernel
because the calculation did not converge and thus results could only be obtained via a cruder randomized
search. We also note that of the best models identiﬁed by the randomized search, for each kernel type
there were several distinct sets of hyperparameters that yielded very similar training scores. For the RBF
kernel speciﬁcally, the hypermarameters we selected represent a compromise between the values of C and

3

γ. While relatively high values of C correspond to overﬁtting by overpenalizing large deviations from the
target, constraining C too aggressively would encourage the bandwidth of the kernel function, γ, to expand
and potentially overﬁt the data. Therefore, cross-validation is critical in evaluating our ﬁnal selection.

We ﬁnd that the RBF kernel is better able to capture the nonlinearities and regional variation of the data
by virtue of its implicit inﬁnite-dimensional feature mapping. In order to characterize the performance of
the ﬁnal model further and to compare it to the linear kernel model, we compute learning curves for both
showing the test score as a function of the fraction of data used for training. Deﬁciencies of the linear kernel
are readily apparent; its score saturates at about 0.65 to 0.7, within error, while the RBF kernel acheives
0.8 to 0.85, within error.

Figure 4: Learning curve comparison

In order to evaluate the structure of the feature data set, and gain qualitative insight into our model, principal
component analysis (PCA) was performed on the full training set. The top 3 principal components are
depicted below, along with supplementary data regarding the per party distributions of campaign donations
by sector.

Figure 5: Top 3 principal components

4

Figure 6: Per party campaign contributions [5]

It appears the ﬁnancial and labor sectors specify the dimensions of largest variance. It also appears that
donations from the ﬁnance sector are heavily skewed towards Republican candidates, while donations from
the labor sector are heavily skewed towards Democrats. Although SVR models are abstract to interpret, this
analysis suggests that these two categories had the largest inﬂuence in the prediction of political ideology.

Conclusion/Future Work

We found that the Support Vector regression algorithm with the radial basis function kernel outperformed
all other models, achieving a score of 0.8 to 0.85, within error. We think the model would have been even
better if we had access to additionl years of data. The nonlinearity of this problem is not especially surprising
given the regional variability of both political ideology and of the contributing industries. We note that the
normalization procedure employed here likely washed out some of the variability, as some candidates surely
received greater amounts of contributions overall compared to others; however, the unnormalized feature ma-
trix was prohibitively costly to ﬁt to. One future direction may be to look at incorporating this variability
in the model. More time could be invested in exploring alternative and more representative normalization
methods. Additionally, a way to circumvent the feature set normalization issue would be to use an unsuper-
vised learning algorithm such as a neural network.

Had we had an additional group memeber, we would have liked to explore the correlation of ‘anomylous’
voting behavior, i.e. votes that are not well predicted by DW-NOMINATE scores, and a higher-resolution
feature set of individual contributions. Such an analysis would probably also beneﬁt from further degmen-
tation of donations into personal and PAC contributions.

References

[1] Adam Bonica. “Ideology and interests in the political marketplace”. In: American Journal of Political

Science 57.2 (2013), pp. 294–311.

[2] Royce Carroll et al. “Measuring bias and uncertainty in DW-NOMINATE ideal point estimates via the

parametric bootstrap”. In: Political Analysis 17.3 (2009), pp. 261–275.

[3] CS229 class notes on Support Vector Machines. cs229.stanford.edu/materials/notes/cs229-notes3.pdf.

Accessed: 2015-12-11.

5

Predicting Political Ideology Using Campaign

Finance Data

Keri A. McKiernan and Joe A. Napoli

December 11, 2015

Introduction

Public interest has become increasingly focused on the role of money in politics. Speciﬁcally, the Citizens
United v. FEC Supreme Court ruling maintained the legality of unrestricted political expenditures by
corporate and union entities. As a result, there has been a proliferation of super PAC, or ‘political action
committee’, organizations. While these organizations are not permitted to make contributions directly to
campaigns, they may engage in unlimited independent spending and there is no restriction on the amount
of funds they can accept from donors. Such legal developments beg the question: Can one predict generally
the inﬂuence of donors over politicians to whom they give? As the ﬂux of money into the political system
continues to increase, this remains a critical question. Our project builds largely on previous research by
Poole and Rosenthal and makes use of freely available databases of campaign ﬁnance information. We aim to
predict quantitative scores of political ideology using campaign contribution proﬁles; speciﬁcally, the input
to our algorithm is a matrix of normalized vectors of campaign contribution data, partitioned by industry,
the learning algorithm used is Support Vector Regression (SVR), and the output is a score for each candidate
in the interval [−1, 1] indicating a position on the political ideological spectrum.

Related Work

The research of Poole and Rosenthal has focused on quantifying the political ideology of politicians [6]. In
particular, they actively develop methods for calculating ‘ideal points’ for candidates. For example, the
DW-NOMINATE method calculates a legislator’s overall probability of voting ‘yea’ on a piece of legislation
as the sum of a deterministic utility value and a normally distributed random error [2]. It calculates ‘ideal
point’ coordinates for legislators by maximizing the log likelihood function

T(cid:88)

pt(cid:88)

L =

qt(cid:88)

2(cid:88)

Cijτ t ln Pijτ t

t=1

i=1

j=1

τ =1

where Pijτ t is the probability of voting for choice τ (yes or no) and Cijτ t = 1 if that probability accurately
predicts the vote [2]. Indexes j, i, and t sum over roll call votes, legislators, and legislative sessions, respec-
tively. These ideal points are constrained to lie within the interval [−1, 1] and are two-dimensional quantities.
A common interpretation of the ﬁrst coordinate is that it reﬂects the divide between the Republican and
Democratic parties, whereas the second coordinate is more highly correlated with intra-party division. It
is important to emphasize that these methods seek to estimate ideal points using roll call voting records.
While a full congressional voting record would be unavailable for candidates who are new to oﬃce, campaign
ﬁnance data are, ideally, readily available. Thus, we think it would be helpful to be able to predict ideal
points for candidates even before they have established a congressional voting record. One could then use
these scores to predict how legislators will vote on particular pieces of legislation and to draw quantitative
comparisons between them.

Much previous work has focused on connecting political contributions to speciﬁc votes cast in congress [9,
8]. Furthermore, Adam Bonica of Stanford has developed a correspondence analysis model for predicting
DW-NOMINATE scores using data at the resolution of individual contributions to PACs [1]. We think this
approach has the deﬁciency of being computationally expensive compared to a coarser graining of the space
of contributions, and we posit that we can recover the ideal point of a candidate just by using a contribution
proﬁle partitioned by sector. We have collated freely available campaign ﬁnance [5] and DW-NOMINATE

1

ideal point [4] datasets. The original scope of our project has been narrowed due to a reduction in the size of
our group. We originally planned to investigate anomalous voting behavior and the ability to predict it solely
by examining campaign ﬁnance data, but shifted our focus to training a model to predict DW-NOMINATE
ideal point coordinates.

Dataset and Features

Campaign ﬁnance data for 375 candidates (as determined by sampling from the DW-NOMINATE scores)
over years 2010, 2012, and 2014 were obtained using the Open Secrets python API [5]. Of the available
data, we chose to featurize by sector. For each sector, individual and PAC contributions were combined and
normalized on a per candidate basis. This amounted to 12 features in total. A heatmap of the total feature
set is illustrated below.

We also include a plot of the distribution of DW-NOMINATE scores, labeling Democrats and Republi-
cans using their traditional party colors. We note that the dataset appears linearly separable in the ﬁrst
dimension of the ideal point coordinates. This suggests that a classiﬁcation algorithm may eﬀectively dis-
tinguish between the parties in case regression algorithm fails to quantitatively capture the distribution of
scores. Finally, we note that for each party the second dimension of the DW-NOMINATE scores is not as
easily separable and instead spans the interval [−1, 1]. This would be a natural extension of the present work.

Figure 1: Heatmap of complete feature set

Figure 2: DW-NOMINATE score distribu-
tion

Methods

The learning algorithm used in this work was epsilon-Support Vector Regression (-SVR), using a radial basis
function (RBF) kernel. In -SVR, a nonlinear function is constructed through the training of a linear func-
tion, f (x), in a higher dimensional inner product space deﬁned by a kernel function, K(xi, xj) = φ(xi)T φ(xj).
The objective of the function training is to ensure all training data lies within  of all target data. Without
supplementary information about a particular data set, SV learning algorithms are consider the best “oﬀ-
the-shelf” supervised learning algorithm [3].
Given a training set, S = {(x0, y0), ..., (xn, yn)|xi ∈ R}, -SVR can be formulated into the following convex
optimization problem, given here in its primal form:

n(cid:88)

i=1

(ζi + ζ∗
i )

min

w,b,ζ,ζ∗

1
2

wT w + C

2

s.t. yi − wT φ(xi) − b ≤  + ζi,
wT φ(xi) + b − yi ≤  + ζ∗
i ,
ζi, ζ∗

i ≥ 0, i = 1, ..., n

where φ(x) is the mapping to the higher dimension space, C determines the degree of regularization (lower
C leads to a smoother solution), and ζ, ζ∗ are slack variables which allow for constraint relaxation in the
case it is required. This problem is often simpler to solve in its dual form (derived through the method of
Lagrange multipliers):

min
α,α∗

1
2

(α − α∗)T Q(α − α∗) + εeT (α + α∗) − yT (α − α∗)

s.t. (α − α∗) = 0

0 ≤ αi, α∗

i ≤ C, i = 1, ..., n

where Qij ≡ K(xi, xj), and Q is the matrix of these values. This problem can be solved to yield the following:

n(cid:88)

f (x) =

(αi − α∗

i )K(xi, x) + b

(1)

(2)

where the RBF kernel is given by the following:

K(xi, xj) = exp(cid:2)−γ|xi − xj|2(cid:3)

i=1

and where γ describes the inﬂuence of each individual training example [7].

Experiments/Results/Discussion

We chose a ﬁnal model after performing a randomized optimization in the space of hyperparameters for
-SVR, for multiple kernel types. This required specifying distributions over which to sample the hyper-
parameters. We speciﬁed a separate exponential distribution for each hyperparameter, and estimated the
decay scale of each distribution by ﬁrst performing an exhaustive grid search over values of the hyper-
parameters spanning several orders of magnitude.
In both the grid and randomized search schemes, the
coeﬃcient of determination (R2) was used as the scoring metric for ranking all sampled models and was
estimated for each candidate model using 5-fold cross-validation on a randomly sampled set of training data
that comprised 80% of the data set. Each randomized hyperparameter search proceeded for 30,000 iterations.

RBF kernel Linear kerel

C

γ

Training score

Test score

10.77
0.047
9.97
0.82
0.83

55.33
0.31
n/a
0.70
0.64

Figure 3: Summary of hyperparameters and model scores

The ﬁnal hyperparameters for the linear and radial basis function (RBF) kernels are summarized in the
table below alongside their corresponding training and test scores. We omit results for the polynomial kernel
because the calculation did not converge and thus results could only be obtained via a cruder randomized
search. We also note that of the best models identiﬁed by the randomized search, for each kernel type
there were several distinct sets of hyperparameters that yielded very similar training scores. For the RBF
kernel speciﬁcally, the hypermarameters we selected represent a compromise between the values of C and

3

γ. While relatively high values of C correspond to overﬁtting by overpenalizing large deviations from the
target, constraining C too aggressively would encourage the bandwidth of the kernel function, γ, to expand
and potentially overﬁt the data. Therefore, cross-validation is critical in evaluating our ﬁnal selection.

We ﬁnd that the RBF kernel is better able to capture the nonlinearities and regional variation of the data
by virtue of its implicit inﬁnite-dimensional feature mapping. In order to characterize the performance of
the ﬁnal model further and to compare it to the linear kernel model, we compute learning curves for both
showing the test score as a function of the fraction of data used for training. Deﬁciencies of the linear kernel
are readily apparent; its score saturates at about 0.65 to 0.7, within error, while the RBF kernel acheives
0.8 to 0.85, within error.

Figure 4: Learning curve comparison

In order to evaluate the structure of the feature data set, and gain qualitative insight into our model, principal
component analysis (PCA) was performed on the full training set. The top 3 principal components are
depicted below, along with supplementary data regarding the per party distributions of campaign donations
by sector.

Figure 5: Top 3 principal components

4

Figure 6: Per party campaign contributions [5]

It appears the ﬁnancial and labor sectors specify the dimensions of largest variance. It also appears that
donations from the ﬁnance sector are heavily skewed towards Republican candidates, while donations from
the labor sector are heavily skewed towards Democrats. Although SVR models are abstract to interpret, this
analysis suggests that these two categories had the largest inﬂuence in the prediction of political ideology.

Conclusion/Future Work

We found that the Support Vector regression algorithm with the radial basis function kernel outperformed
all other models, achieving a score of 0.8 to 0.85, within error. We think the model would have been even
better if we had access to additionl years of data. The nonlinearity of this problem is not especially surprising
given the regional variability of both political ideology and of the contributing industries. We note that the
normalization procedure employed here likely washed out some of the variability, as some candidates surely
received greater amounts of contributions overall compared to others; however, the unnormalized feature ma-
trix was prohibitively costly to ﬁt to. One future direction may be to look at incorporating this variability
in the model. More time could be invested in exploring alternative and more representative normalization
methods. Additionally, a way to circumvent the feature set normalization issue would be to use an unsuper-
vised learning algorithm such as a neural network.

Had we had an additional group memeber, we would have liked to explore the correlation of ‘anomylous’
voting behavior, i.e. votes that are not well predicted by DW-NOMINATE scores, and a higher-resolution
feature set of individual contributions. Such an analysis would probably also beneﬁt from further degmen-
tation of donations into personal and PAC contributions.

References

[1] Adam Bonica. “Ideology and interests in the political marketplace”. In: American Journal of Political

Science 57.2 (2013), pp. 294–311.

[2] Royce Carroll et al. “Measuring bias and uncertainty in DW-NOMINATE ideal point estimates via the

parametric bootstrap”. In: Political Analysis 17.3 (2009), pp. 261–275.

[3] CS229 class notes on Support Vector Machines. cs229.stanford.edu/materials/notes/cs229-notes3.pdf.

Accessed: 2015-12-11.

5

[4] DW-NOMINATE Scores With Bootstrapped Standard Errors. http://voteview.com/dwnomin.htm.

Accessed: 2015.10.01.

[5] OpenSecrets.org. http://opensecrets.org. Accessed: 2015-12-11.

[6] K.T. Poole and H. Rosenthal. Congress: A Political-Economic History of Roll Call Voting. Oxford

University Press, 2000.

[7] Scikit-learn SVR implementation documentation. http://scikit-learn.org/stable/modules/svm.htmlsvm-

implementation-details. Accessed: 2015-12-11.

[8] Thomas Stratmann. “Campaign contributions and congressional voting: does the timing of contributions

matter?” In: The Review of Economics and Statistics (1995), pp. 127–136.

[9] Thomas Stratmann. “What do campaign contributions buy? Deciphering causal eﬀects of money and

votes”. In: Southern Economic Journal (1991), pp. 606–620.

6

