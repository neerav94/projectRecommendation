Pose Estimation Based on 3D Models

Chuiwen Ma, Liang Shi

1

Introduction

This project aims to estimate the pose of an object
in the image. Pose estimation problem is known to
be an open problem and also a crucial problem in
computer vision ﬁeld. Many real-world tasks depend
heavily on or can be improved by a good pose
estimation. For example, by knowing the exact pose
of an object, robots will know where to sit on, how
to grasp, or avoid collision when walking around.
Besides, pose estimation is also applicable to auto-
matic driving. With good pose estimation of cars,
automatic driving system will know how to manip-
ulate itself accordingly. Moreover, pose estimation
can also beneﬁt image searching, 3D reconstruction
and has a large potential impact on many other ﬁelds.

Previously, most pose estimation works were im-
plemented by training on manually labeled dataset.
However, to create such a dataset is extremely time-
consuming, laborsome, and also error-prone because
the labelization might be subjective. Therefore, the
training datasets in existing works are either too
small or too vague for training an eﬀective classiﬁer.
In this project, we instead utilized the power of
3D shape models. To be speciﬁc, we built a large,
balanced and precisely labeled training dataset from
ShapeNet [6], a large 3D model pool which contains
millions of 3D shape models in thousands of object
categories. By rendering 3D models into 2D images
from diﬀerent viewpoints, we can easily control the
size, the pose distribution, and the precision of the
dataset. A learning model trained on this dataset
will help us better solve the pose estimation task.

In this work, we built a pose estimation system of
chairs based on a rendered image training set, which
predicts the pose of the chair in a real image. Our
pose estimation system takes a properly cropped
image as input, and outputs a probability vector on
pose space. Given a test image, we ﬁrst divide it into
a N × N patch grid. For each patch, a multi-class
classiﬁer is trained to estimate the probability of this
patch to be pose v. Then, scores from all patches
are combined to generate a probability vector for the
whole image.

Although we built a larger and more precise train-

ing dataset from rendered images, there is an obvious
drawback of this approach — the statistical prop-
erty of the training set and the test set are diﬀer-
ent. For instance, in the real world, there exists a
prior probability distribution of poses, which might
be non-uniform. Furthermore, even for features from
the same pose, real image features might be more di-
verse than rendered image features.
In this paper,
we proposed a method to revise the inﬂuence of the
diﬀerence in prior probability distribution. Detailed
methods and experiment results are shown in the fol-
lowing sections.

2 Dataset, Features and

Preprocessing

2.1 Training Data

As we mentioned in Section 1, we collected our train-
ing data from ShapeNet, a 3D shape model database,
which contains 5057 chair models. For each model,
we rendered it on 16 viewpoints, evenly distributed
on the horizontal circle, shown in Figure 1.

Figure 1: Chair models and rendering process

We chose 4000 models, accordingly 64,000 images
to build the training dataset, and leave the rest 1057
models to be our rendered image test set. When ex-
tracting image features, we ﬁrst resize the images to
112 × 112 pixels, and then divide it into 6 × 6 over-
lapped patch grid, with patch size 32 × 32 and patch
stride 16 on both axes. After that, we extract a 576
dimensional HoG features [3] for each patch, so the
whole image can be represented by a 20736 dimen-
sional feature vector. Those 64,000 feature vectors
constituted our training dataset.

1

Pose Estimation Based on 3D Models

Chuiwen Ma, Liang Shi

1

Introduction

This project aims to estimate the pose of an object
in the image. Pose estimation problem is known to
be an open problem and also a crucial problem in
computer vision ﬁeld. Many real-world tasks depend
heavily on or can be improved by a good pose
estimation. For example, by knowing the exact pose
of an object, robots will know where to sit on, how
to grasp, or avoid collision when walking around.
Besides, pose estimation is also applicable to auto-
matic driving. With good pose estimation of cars,
automatic driving system will know how to manip-
ulate itself accordingly. Moreover, pose estimation
can also beneﬁt image searching, 3D reconstruction
and has a large potential impact on many other ﬁelds.

Previously, most pose estimation works were im-
plemented by training on manually labeled dataset.
However, to create such a dataset is extremely time-
consuming, laborsome, and also error-prone because
the labelization might be subjective. Therefore, the
training datasets in existing works are either too
small or too vague for training an eﬀective classiﬁer.
In this project, we instead utilized the power of
3D shape models. To be speciﬁc, we built a large,
balanced and precisely labeled training dataset from
ShapeNet [6], a large 3D model pool which contains
millions of 3D shape models in thousands of object
categories. By rendering 3D models into 2D images
from diﬀerent viewpoints, we can easily control the
size, the pose distribution, and the precision of the
dataset. A learning model trained on this dataset
will help us better solve the pose estimation task.

In this work, we built a pose estimation system of
chairs based on a rendered image training set, which
predicts the pose of the chair in a real image. Our
pose estimation system takes a properly cropped
image as input, and outputs a probability vector on
pose space. Given a test image, we ﬁrst divide it into
a N × N patch grid. For each patch, a multi-class
classiﬁer is trained to estimate the probability of this
patch to be pose v. Then, scores from all patches
are combined to generate a probability vector for the
whole image.

Although we built a larger and more precise train-

ing dataset from rendered images, there is an obvious
drawback of this approach — the statistical prop-
erty of the training set and the test set are diﬀer-
ent. For instance, in the real world, there exists a
prior probability distribution of poses, which might
be non-uniform. Furthermore, even for features from
the same pose, real image features might be more di-
verse than rendered image features.
In this paper,
we proposed a method to revise the inﬂuence of the
diﬀerence in prior probability distribution. Detailed
methods and experiment results are shown in the fol-
lowing sections.

2 Dataset, Features and

Preprocessing

2.1 Training Data

As we mentioned in Section 1, we collected our train-
ing data from ShapeNet, a 3D shape model database,
which contains 5057 chair models. For each model,
we rendered it on 16 viewpoints, evenly distributed
on the horizontal circle, shown in Figure 1.

Figure 1: Chair models and rendering process

We chose 4000 models, accordingly 64,000 images
to build the training dataset, and leave the rest 1057
models to be our rendered image test set. When ex-
tracting image features, we ﬁrst resize the images to
112 × 112 pixels, and then divide it into 6 × 6 over-
lapped patch grid, with patch size 32 × 32 and patch
stride 16 on both axes. After that, we extract a 576
dimensional HoG features [3] for each patch, so the
whole image can be represented by a 20736 dimen-
sional feature vector. Those 64,000 feature vectors
constituted our training dataset.

1

2.2 Test Data

To better evaluate the performance of our learning
algorithm, we built three diﬀerent test sets with
increasing test diﬃculty. They are rendered image
test set, clean background real image test set and
cluttered background real image test set.

Rendered image test set consists of 1057 × 16
rendered images, which also comes from ShapeNet.
Clean background and cluttered background real im-
age test sets are collected from ImageNet [4], con-
taining 1309 and 1000 images respectively, both with
manually labeled pose ground truth. Some sample
images are shown in Figure 2. Obviously, these three
datasets are increasingly noisy and diﬃcult to tackle.

Figure 2: Clean background & cluttered background

For the test sets, we used the same scheme to pro-
cess the image as the training set. That is, convert
each image into a 20736-dimension HoG feature.

3 Model

Rather than using global image feature as the input
of classiﬁcation, our pose estimation model is patch-
based. By dividing image into patches and training
a classiﬁer for each patch, our model can be more
robust to occlusion and background noise. Also,
this approach reduced the feature dimension, thus
reduced the sample complexity for each classiﬁer.
Actually, we did try the global method, while the
classiﬁcation accuracy is 20% lower than patch based
method. The mathematical representation of our
patch based model is as follows.

¯v = arg max

v

In sum, our model takes Fi, i = 1,··· , N 2 as input,

and outputs P (v|I) and ¯v.

4 Methods

4.1 Learning Algorithms

4.1.1 Random Forest

In this project, we choose random forest [1] as a pri-
mary classiﬁcation algorithm based on following con-
siderations:

• Suitable for multiclass classiﬁcation.
• Non-parametric, easy to tune.
• Fast, easy to parallel.
• Robust, due to randomness.
During classiﬁcation, 36 random forest classiﬁers
are trained for 36 patches. As a trade oﬀ between
spatio-temporal complexity and performance, we set
the forest size to be 100 trees. We also tuned the
maximum depth of trees using cross-validation, where
the optimal depth is 20. When classiﬁcation, each
random forest outputs a probability vector P (v|Fi).
After Laplace smoothing, we calculated P (v|I), esti-
mated the pose to be ¯v = arg max

P (v|I).

v

4.1.2 Multiclass SVM

In binary classiﬁcation, SVM constructs a hyperplane
or set of hyperplanes in a high- or inﬁnite-dimensional
space that will separate data with diﬀerent labels as
wide as possible. In C-SVM model, the hyperplane
is generated by maximizing the following function

assume P (v|I) ∝ N 2(cid:81)

For each patch, we build a classiﬁer, which gives a
prediction of the conditional probability P (v|Fi).
To respresent P (v|I) in P (v|Fi), i = 1,··· , N 2, we
P (v|Fi). So, we can calculate
P (v|I) and the according ¯v using the following
formula:

i=1

P (v|I) =

N 2(cid:81)
N 2(cid:81)
V(cid:80)

i=1

v=1

i=1

P (v|Fi)

P (v|Fi)
P (v|I)

Deﬁne Fi as

the HoG feature of patch i,
I = (F1,··· , FN 2 ) to be the HoG feature of the
whole image, V = {1,··· , V } to be the pose space.

minγ,w,b

||w||2 + C

1
2

2

m(cid:88)

i=1

ξi

Pose Estimation Based on 3D Models

Chuiwen Ma, Liang Shi

1

Introduction

This project aims to estimate the pose of an object
in the image. Pose estimation problem is known to
be an open problem and also a crucial problem in
computer vision ﬁeld. Many real-world tasks depend
heavily on or can be improved by a good pose
estimation. For example, by knowing the exact pose
of an object, robots will know where to sit on, how
to grasp, or avoid collision when walking around.
Besides, pose estimation is also applicable to auto-
matic driving. With good pose estimation of cars,
automatic driving system will know how to manip-
ulate itself accordingly. Moreover, pose estimation
can also beneﬁt image searching, 3D reconstruction
and has a large potential impact on many other ﬁelds.

Previously, most pose estimation works were im-
plemented by training on manually labeled dataset.
However, to create such a dataset is extremely time-
consuming, laborsome, and also error-prone because
the labelization might be subjective. Therefore, the
training datasets in existing works are either too
small or too vague for training an eﬀective classiﬁer.
In this project, we instead utilized the power of
3D shape models. To be speciﬁc, we built a large,
balanced and precisely labeled training dataset from
ShapeNet [6], a large 3D model pool which contains
millions of 3D shape models in thousands of object
categories. By rendering 3D models into 2D images
from diﬀerent viewpoints, we can easily control the
size, the pose distribution, and the precision of the
dataset. A learning model trained on this dataset
will help us better solve the pose estimation task.

In this work, we built a pose estimation system of
chairs based on a rendered image training set, which
predicts the pose of the chair in a real image. Our
pose estimation system takes a properly cropped
image as input, and outputs a probability vector on
pose space. Given a test image, we ﬁrst divide it into
a N × N patch grid. For each patch, a multi-class
classiﬁer is trained to estimate the probability of this
patch to be pose v. Then, scores from all patches
are combined to generate a probability vector for the
whole image.

Although we built a larger and more precise train-

ing dataset from rendered images, there is an obvious
drawback of this approach — the statistical prop-
erty of the training set and the test set are diﬀer-
ent. For instance, in the real world, there exists a
prior probability distribution of poses, which might
be non-uniform. Furthermore, even for features from
the same pose, real image features might be more di-
verse than rendered image features.
In this paper,
we proposed a method to revise the inﬂuence of the
diﬀerence in prior probability distribution. Detailed
methods and experiment results are shown in the fol-
lowing sections.

2 Dataset, Features and

Preprocessing

2.1 Training Data

As we mentioned in Section 1, we collected our train-
ing data from ShapeNet, a 3D shape model database,
which contains 5057 chair models. For each model,
we rendered it on 16 viewpoints, evenly distributed
on the horizontal circle, shown in Figure 1.

Figure 1: Chair models and rendering process

We chose 4000 models, accordingly 64,000 images
to build the training dataset, and leave the rest 1057
models to be our rendered image test set. When ex-
tracting image features, we ﬁrst resize the images to
112 × 112 pixels, and then divide it into 6 × 6 over-
lapped patch grid, with patch size 32 × 32 and patch
stride 16 on both axes. After that, we extract a 576
dimensional HoG features [3] for each patch, so the
whole image can be represented by a 20736 dimen-
sional feature vector. Those 64,000 feature vectors
constituted our training dataset.

1

2.2 Test Data

To better evaluate the performance of our learning
algorithm, we built three diﬀerent test sets with
increasing test diﬃculty. They are rendered image
test set, clean background real image test set and
cluttered background real image test set.

Rendered image test set consists of 1057 × 16
rendered images, which also comes from ShapeNet.
Clean background and cluttered background real im-
age test sets are collected from ImageNet [4], con-
taining 1309 and 1000 images respectively, both with
manually labeled pose ground truth. Some sample
images are shown in Figure 2. Obviously, these three
datasets are increasingly noisy and diﬃcult to tackle.

Figure 2: Clean background & cluttered background

For the test sets, we used the same scheme to pro-
cess the image as the training set. That is, convert
each image into a 20736-dimension HoG feature.

3 Model

Rather than using global image feature as the input
of classiﬁcation, our pose estimation model is patch-
based. By dividing image into patches and training
a classiﬁer for each patch, our model can be more
robust to occlusion and background noise. Also,
this approach reduced the feature dimension, thus
reduced the sample complexity for each classiﬁer.
Actually, we did try the global method, while the
classiﬁcation accuracy is 20% lower than patch based
method. The mathematical representation of our
patch based model is as follows.

¯v = arg max

v

In sum, our model takes Fi, i = 1,··· , N 2 as input,

and outputs P (v|I) and ¯v.

4 Methods

4.1 Learning Algorithms

4.1.1 Random Forest

In this project, we choose random forest [1] as a pri-
mary classiﬁcation algorithm based on following con-
siderations:

• Suitable for multiclass classiﬁcation.
• Non-parametric, easy to tune.
• Fast, easy to parallel.
• Robust, due to randomness.
During classiﬁcation, 36 random forest classiﬁers
are trained for 36 patches. As a trade oﬀ between
spatio-temporal complexity and performance, we set
the forest size to be 100 trees. We also tuned the
maximum depth of trees using cross-validation, where
the optimal depth is 20. When classiﬁcation, each
random forest outputs a probability vector P (v|Fi).
After Laplace smoothing, we calculated P (v|I), esti-
mated the pose to be ¯v = arg max

P (v|I).

v

4.1.2 Multiclass SVM

In binary classiﬁcation, SVM constructs a hyperplane
or set of hyperplanes in a high- or inﬁnite-dimensional
space that will separate data with diﬀerent labels as
wide as possible. In C-SVM model, the hyperplane
is generated by maximizing the following function

assume P (v|I) ∝ N 2(cid:81)

For each patch, we build a classiﬁer, which gives a
prediction of the conditional probability P (v|Fi).
To respresent P (v|I) in P (v|Fi), i = 1,··· , N 2, we
P (v|Fi). So, we can calculate
P (v|I) and the according ¯v using the following
formula:

i=1

P (v|I) =

N 2(cid:81)
N 2(cid:81)
V(cid:80)

i=1

v=1

i=1

P (v|Fi)

P (v|Fi)
P (v|I)

Deﬁne Fi as

the HoG feature of patch i,
I = (F1,··· , FN 2 ) to be the HoG feature of the
whole image, V = {1,··· , V } to be the pose space.

minγ,w,b

||w||2 + C

1
2

2

m(cid:88)

i=1

ξi

s.t.

y(i)(wT x(i) + b) ≥ 1 − ξi,
ξi ≥ 0, i = 1 . . . , m.

two important factors will shape the outcome of
model — kernel and soft margin parameter C.
Kernel deﬁnes a mapping that projects the input
attributes to higher dimension features, which could
often convert the non-separable data into separable.
C determines the trade-oﬀ between the training error
and VC dimension of the model, the smaller C is,
the less eﬀect will the outliers exert on the classiﬁer.

In our problem, we face a multiple classiﬁcation
problem which can not be addressed by building a
single SVM model. Here, we apply two methods to
solve it: 1. One-versus-Rest (OvR) 2. One-versus-
One (OvO). Both these two methods reduce the sin-
gle multiclass problem into multiple binary classiﬁca-
tion problems. OvR method train classiﬁers by sep-
arating data into one exact label and the rest, which
results in n models for n-label data, then it predicts
the result as the highest output. In OvO approach,
classiﬁcation is done by a max-wins voting strategy.
For n-label data, n(n − 1)/2 classiﬁers are trained,
with each trained by picking 2-label data from entire
n-label data. In prediction, each classiﬁer assigns the
input to one of the two labels it trained with, and
ﬁnally the label with the most votes determines the
instance classiﬁcation.

4.2 Optimization

Constructing training dataset from rendered images
has many advantages, but there are also drawbacks.
As I mentioned in Section 1, the prior probability of
pose in real images can be highly diﬀerent from that
in rendered images. As we know, pose distribution in
the training set is uniform, however, in real images,
there are far more front view chairs than back view.
Fortunately, this diﬀerence can be analyzed and mod-
eled as follows.

4.2.1 Probability Calibration

In classiﬁcation step, each classiﬁer Ci will output a
probability vector ˜P (v|Fi). Using Bayesian formula,
we have:

˜P (v|Fi) =

˜P (v) ˜P (Fi|v)

˜P (Fi)

˜P (v|Fi). Here, P (v), P (Fi|v) and P (Fi) are distri-
butions in the test set.
P (v|Fi) =

P (v)P (Fi|v)

P (Fi)

Assume the training data and the test data have at
least some similarity. Speciﬁcally speaking, assume
P (Fi|v) = ˜P (Fi|v), P (Fi) = ˜P (Fi), then we have:

P (v|Fi) = ˜P (v|Fi)

P (v)
˜P (v)

∝ ˜P (v|Fi)P (v)

To recover P (v|Fi), we just need to achieve a good
estimation of P (v). One possible method might be
randomly choosing some samples from the test set,
and manually label the ground truth of pose, regard
the ground truth pose distribution of samples as an
estimation of global P (v). However, we still need to
do some “labor work”.

Noticing the above formula can also be written as:

P (v|Fi)
P (v)

=

˜P (v|Fi)
˜P (v)

; ˜P (v) =

, ∀v ∈ V

1
V

we came up with another idea to automatically
improve the classiﬁcation result. For ˜P (v|Fi), we
have:

P (v) >

P (v) <

1
V
1
V

⇒ ˜P (v|Fi) < P (v|Fi)
⇒ ˜P (v|Fi) > P (v|Fi)

That means, when testing,

frequently appeared
poses are underestimated, while uncommon poses are
overestimated. Here, we will propose an iterative
method to counterbalance this eﬀect. Basically, we
will use ˜P (v|Fi) to generate an estimation ˜P (v) of
the prior distribution; assume P (v) and ˜P (v) have
similar common views and uncommon views (in other
words, P (v) and ˜P (v) have the same trend); smooth
˜P (v) to keep the trend while reduce ﬂuctuation range;
multiply the original ˜P (v|Fi) by smoothed ˜P (v); and
iteratively repeat the above steps. Finally, due to
the damping eﬀect in combination step, ˜P (v) will
converge, and ˜P (v|Fi) gets closer to P (v|Fi). For-
mulation of this iterative algorithm is as follows:
1. Calculate ˜P (v|I (j)), j = 1,··· , m.

Here, ˜P (v), ˜P (F|v) and ˜P (F ) are learned from train-
ing data. Whereas, the real P (v|Fi), which satis-
ﬁes the following formula, could be diﬀerent from

˜P (v|I (j)) =

3

N 2(cid:81)
N 2(cid:81)
V(cid:80)

i=1

v=1

i=1

˜P (v|F (j)

)

i

˜P (v|F (j)

i

)

Pose Estimation Based on 3D Models

Chuiwen Ma, Liang Shi

1

Introduction

This project aims to estimate the pose of an object
in the image. Pose estimation problem is known to
be an open problem and also a crucial problem in
computer vision ﬁeld. Many real-world tasks depend
heavily on or can be improved by a good pose
estimation. For example, by knowing the exact pose
of an object, robots will know where to sit on, how
to grasp, or avoid collision when walking around.
Besides, pose estimation is also applicable to auto-
matic driving. With good pose estimation of cars,
automatic driving system will know how to manip-
ulate itself accordingly. Moreover, pose estimation
can also beneﬁt image searching, 3D reconstruction
and has a large potential impact on many other ﬁelds.

Previously, most pose estimation works were im-
plemented by training on manually labeled dataset.
However, to create such a dataset is extremely time-
consuming, laborsome, and also error-prone because
the labelization might be subjective. Therefore, the
training datasets in existing works are either too
small or too vague for training an eﬀective classiﬁer.
In this project, we instead utilized the power of
3D shape models. To be speciﬁc, we built a large,
balanced and precisely labeled training dataset from
ShapeNet [6], a large 3D model pool which contains
millions of 3D shape models in thousands of object
categories. By rendering 3D models into 2D images
from diﬀerent viewpoints, we can easily control the
size, the pose distribution, and the precision of the
dataset. A learning model trained on this dataset
will help us better solve the pose estimation task.

In this work, we built a pose estimation system of
chairs based on a rendered image training set, which
predicts the pose of the chair in a real image. Our
pose estimation system takes a properly cropped
image as input, and outputs a probability vector on
pose space. Given a test image, we ﬁrst divide it into
a N × N patch grid. For each patch, a multi-class
classiﬁer is trained to estimate the probability of this
patch to be pose v. Then, scores from all patches
are combined to generate a probability vector for the
whole image.

Although we built a larger and more precise train-

ing dataset from rendered images, there is an obvious
drawback of this approach — the statistical prop-
erty of the training set and the test set are diﬀer-
ent. For instance, in the real world, there exists a
prior probability distribution of poses, which might
be non-uniform. Furthermore, even for features from
the same pose, real image features might be more di-
verse than rendered image features.
In this paper,
we proposed a method to revise the inﬂuence of the
diﬀerence in prior probability distribution. Detailed
methods and experiment results are shown in the fol-
lowing sections.

2 Dataset, Features and

Preprocessing

2.1 Training Data

As we mentioned in Section 1, we collected our train-
ing data from ShapeNet, a 3D shape model database,
which contains 5057 chair models. For each model,
we rendered it on 16 viewpoints, evenly distributed
on the horizontal circle, shown in Figure 1.

Figure 1: Chair models and rendering process

We chose 4000 models, accordingly 64,000 images
to build the training dataset, and leave the rest 1057
models to be our rendered image test set. When ex-
tracting image features, we ﬁrst resize the images to
112 × 112 pixels, and then divide it into 6 × 6 over-
lapped patch grid, with patch size 32 × 32 and patch
stride 16 on both axes. After that, we extract a 576
dimensional HoG features [3] for each patch, so the
whole image can be represented by a 20736 dimen-
sional feature vector. Those 64,000 feature vectors
constituted our training dataset.

1

2.2 Test Data

To better evaluate the performance of our learning
algorithm, we built three diﬀerent test sets with
increasing test diﬃculty. They are rendered image
test set, clean background real image test set and
cluttered background real image test set.

Rendered image test set consists of 1057 × 16
rendered images, which also comes from ShapeNet.
Clean background and cluttered background real im-
age test sets are collected from ImageNet [4], con-
taining 1309 and 1000 images respectively, both with
manually labeled pose ground truth. Some sample
images are shown in Figure 2. Obviously, these three
datasets are increasingly noisy and diﬃcult to tackle.

Figure 2: Clean background & cluttered background

For the test sets, we used the same scheme to pro-
cess the image as the training set. That is, convert
each image into a 20736-dimension HoG feature.

3 Model

Rather than using global image feature as the input
of classiﬁcation, our pose estimation model is patch-
based. By dividing image into patches and training
a classiﬁer for each patch, our model can be more
robust to occlusion and background noise. Also,
this approach reduced the feature dimension, thus
reduced the sample complexity for each classiﬁer.
Actually, we did try the global method, while the
classiﬁcation accuracy is 20% lower than patch based
method. The mathematical representation of our
patch based model is as follows.

¯v = arg max

v

In sum, our model takes Fi, i = 1,··· , N 2 as input,

and outputs P (v|I) and ¯v.

4 Methods

4.1 Learning Algorithms

4.1.1 Random Forest

In this project, we choose random forest [1] as a pri-
mary classiﬁcation algorithm based on following con-
siderations:

• Suitable for multiclass classiﬁcation.
• Non-parametric, easy to tune.
• Fast, easy to parallel.
• Robust, due to randomness.
During classiﬁcation, 36 random forest classiﬁers
are trained for 36 patches. As a trade oﬀ between
spatio-temporal complexity and performance, we set
the forest size to be 100 trees. We also tuned the
maximum depth of trees using cross-validation, where
the optimal depth is 20. When classiﬁcation, each
random forest outputs a probability vector P (v|Fi).
After Laplace smoothing, we calculated P (v|I), esti-
mated the pose to be ¯v = arg max

P (v|I).

v

4.1.2 Multiclass SVM

In binary classiﬁcation, SVM constructs a hyperplane
or set of hyperplanes in a high- or inﬁnite-dimensional
space that will separate data with diﬀerent labels as
wide as possible. In C-SVM model, the hyperplane
is generated by maximizing the following function

assume P (v|I) ∝ N 2(cid:81)

For each patch, we build a classiﬁer, which gives a
prediction of the conditional probability P (v|Fi).
To respresent P (v|I) in P (v|Fi), i = 1,··· , N 2, we
P (v|Fi). So, we can calculate
P (v|I) and the according ¯v using the following
formula:

i=1

P (v|I) =

N 2(cid:81)
N 2(cid:81)
V(cid:80)

i=1

v=1

i=1

P (v|Fi)

P (v|Fi)
P (v|I)

Deﬁne Fi as

the HoG feature of patch i,
I = (F1,··· , FN 2 ) to be the HoG feature of the
whole image, V = {1,··· , V } to be the pose space.

minγ,w,b

||w||2 + C

1
2

2

m(cid:88)

i=1

ξi

s.t.

y(i)(wT x(i) + b) ≥ 1 − ξi,
ξi ≥ 0, i = 1 . . . , m.

two important factors will shape the outcome of
model — kernel and soft margin parameter C.
Kernel deﬁnes a mapping that projects the input
attributes to higher dimension features, which could
often convert the non-separable data into separable.
C determines the trade-oﬀ between the training error
and VC dimension of the model, the smaller C is,
the less eﬀect will the outliers exert on the classiﬁer.

In our problem, we face a multiple classiﬁcation
problem which can not be addressed by building a
single SVM model. Here, we apply two methods to
solve it: 1. One-versus-Rest (OvR) 2. One-versus-
One (OvO). Both these two methods reduce the sin-
gle multiclass problem into multiple binary classiﬁca-
tion problems. OvR method train classiﬁers by sep-
arating data into one exact label and the rest, which
results in n models for n-label data, then it predicts
the result as the highest output. In OvO approach,
classiﬁcation is done by a max-wins voting strategy.
For n-label data, n(n − 1)/2 classiﬁers are trained,
with each trained by picking 2-label data from entire
n-label data. In prediction, each classiﬁer assigns the
input to one of the two labels it trained with, and
ﬁnally the label with the most votes determines the
instance classiﬁcation.

4.2 Optimization

Constructing training dataset from rendered images
has many advantages, but there are also drawbacks.
As I mentioned in Section 1, the prior probability of
pose in real images can be highly diﬀerent from that
in rendered images. As we know, pose distribution in
the training set is uniform, however, in real images,
there are far more front view chairs than back view.
Fortunately, this diﬀerence can be analyzed and mod-
eled as follows.

4.2.1 Probability Calibration

In classiﬁcation step, each classiﬁer Ci will output a
probability vector ˜P (v|Fi). Using Bayesian formula,
we have:

˜P (v|Fi) =

˜P (v) ˜P (Fi|v)

˜P (Fi)

˜P (v|Fi). Here, P (v), P (Fi|v) and P (Fi) are distri-
butions in the test set.
P (v|Fi) =

P (v)P (Fi|v)

P (Fi)

Assume the training data and the test data have at
least some similarity. Speciﬁcally speaking, assume
P (Fi|v) = ˜P (Fi|v), P (Fi) = ˜P (Fi), then we have:

P (v|Fi) = ˜P (v|Fi)

P (v)
˜P (v)

∝ ˜P (v|Fi)P (v)

To recover P (v|Fi), we just need to achieve a good
estimation of P (v). One possible method might be
randomly choosing some samples from the test set,
and manually label the ground truth of pose, regard
the ground truth pose distribution of samples as an
estimation of global P (v). However, we still need to
do some “labor work”.

Noticing the above formula can also be written as:

P (v|Fi)
P (v)

=

˜P (v|Fi)
˜P (v)

; ˜P (v) =

, ∀v ∈ V

1
V

we came up with another idea to automatically
improve the classiﬁcation result. For ˜P (v|Fi), we
have:

P (v) >

P (v) <

1
V
1
V

⇒ ˜P (v|Fi) < P (v|Fi)
⇒ ˜P (v|Fi) > P (v|Fi)

That means, when testing,

frequently appeared
poses are underestimated, while uncommon poses are
overestimated. Here, we will propose an iterative
method to counterbalance this eﬀect. Basically, we
will use ˜P (v|Fi) to generate an estimation ˜P (v) of
the prior distribution; assume P (v) and ˜P (v) have
similar common views and uncommon views (in other
words, P (v) and ˜P (v) have the same trend); smooth
˜P (v) to keep the trend while reduce ﬂuctuation range;
multiply the original ˜P (v|Fi) by smoothed ˜P (v); and
iteratively repeat the above steps. Finally, due to
the damping eﬀect in combination step, ˜P (v) will
converge, and ˜P (v|Fi) gets closer to P (v|Fi). For-
mulation of this iterative algorithm is as follows:
1. Calculate ˜P (v|I (j)), j = 1,··· , m.

Here, ˜P (v), ˜P (F|v) and ˜P (F ) are learned from train-
ing data. Whereas, the real P (v|Fi), which satis-
ﬁes the following formula, could be diﬀerent from

˜P (v|I (j)) =

3

N 2(cid:81)
N 2(cid:81)
V(cid:80)

i=1

v=1

i=1

˜P (v|F (j)

)

i

˜P (v|F (j)

i

)

2. Accumulate ˜P (v|I (j)) on all test samples to cal-

culate ˜P (v).

m(cid:88)

j=1

˜P (v) =

1
m

˜P (v|I (j))

3. Smooth ˜P (v) by factor α.

˜Ps(v) =

˜P (v) + α
1 + 16α

4. Estimate P (v|Fi) by letting:

¯P (v|Fi) = ˜P (v|Fi) ˜Ps(v)

5. Use ¯P (v|Fi) to re-calculate ˜P (v|I (j)) in step 1,
while remain ˜P (v|Fi) in step 4 unchanged, repeat
the above steps.

After several iterations, the algorithm will converge,
and we’ll get a ﬁnal estimation ¯P (v|Fi) of P (v|Fi).

4.2.2 Parameter Automatic Selection

However, diﬀerent α will lead to far diﬀerent converg-
ing results, as shown in Figure 3. From experiment
results in Figure 4 we observed that if α is too small,
viewpoint with the highest probability ˜P (v) will soon
beat other viewpoints, and ˜P (v) converges to a to-
tally biased distribution. While, if α is too large,
smoothing eﬀect is too strong to make any change of
¯P (v|Fi). However, there exists an intermediate value
of α to maximize the classiﬁcation accuracy and re-
sult in an optimal estimation ¯P (v|Fi).
In Figure 3
and 4, it is 0.8.

Figure 4: Stable distribution ˜P (v) w.r.t. α

is almost monotonically increasing with respect to α,
such as blue curves, some are monotonically decreas-
ing, such as the black curve, while others will decrease
after ﬁrst increase, such as the red curves. Recall
the distribution change with α in Figure 4, we found
˜P (v) will ﬁrst approximate P (v) then be smoothed.
So, patterns with turning points are reﬂection of this
trend. Sum on those components, we get Figure 6,
and take the turning point of the curve as our esti-
mated ¯α. Here ¯α is 1, very close the optimal value
0.8.

Figure 5: ˜P (vj) curve with respect to α

Figure 3: Classiﬁcation accuracy change w.r.t. α

To solve the optimal α, we conducted deep analysis
to the relationship between stable ˜P (v) and α. We
found three patterns of relationship between ˜P (vj)
and α, shown in Figure 5. For some viewpoints, ˜P (v)

5 Results and Discussion

5.1 Classiﬁcation Performance

Table 1 shows a promising classiﬁcation results on
all three test sets. Under our scheme, OvO SVM

4

−1−0.8−0.6−0.4−0.200.20.40.60.81505560657075808590Acc=89.3%α=0.8logαAccuracy24681012141600.10.20.30.40.50.60.70.80.91PoseConverged˜P(v)  Ground Trutha = 0a = 0.8a = 10−1−0.500.5100.010.020.030.040.05V1−1−0.500.5100.050.10.150.20.6V2−1−0.500.5100.050.10.150.20.9V3−1−0.500.5100.020.040.06V4−1−0.500.5100.511.522.5x 10−3V5−1−0.500.5100.0050.010.015V6−1−0.500.5100.010.020.030.04V7−1−0.500.5100.010.020.030.040.05V8−1−0.500.5102468x 10−3V9−1−0.500.5100.020.040.06V10−1−0.500.5100.010.020.03V11−1−0.500.5100.010.020.03V12−1−0.500.51012345x 10−3V13−1−0.500.5100.020.040.060.080.1V14−1−0.500.5100.20.40.60.81V15−1−0.500.5100.050.10.150.21.4V16Pose Estimation Based on 3D Models

Chuiwen Ma, Liang Shi

1

Introduction

This project aims to estimate the pose of an object
in the image. Pose estimation problem is known to
be an open problem and also a crucial problem in
computer vision ﬁeld. Many real-world tasks depend
heavily on or can be improved by a good pose
estimation. For example, by knowing the exact pose
of an object, robots will know where to sit on, how
to grasp, or avoid collision when walking around.
Besides, pose estimation is also applicable to auto-
matic driving. With good pose estimation of cars,
automatic driving system will know how to manip-
ulate itself accordingly. Moreover, pose estimation
can also beneﬁt image searching, 3D reconstruction
and has a large potential impact on many other ﬁelds.

Previously, most pose estimation works were im-
plemented by training on manually labeled dataset.
However, to create such a dataset is extremely time-
consuming, laborsome, and also error-prone because
the labelization might be subjective. Therefore, the
training datasets in existing works are either too
small or too vague for training an eﬀective classiﬁer.
In this project, we instead utilized the power of
3D shape models. To be speciﬁc, we built a large,
balanced and precisely labeled training dataset from
ShapeNet [6], a large 3D model pool which contains
millions of 3D shape models in thousands of object
categories. By rendering 3D models into 2D images
from diﬀerent viewpoints, we can easily control the
size, the pose distribution, and the precision of the
dataset. A learning model trained on this dataset
will help us better solve the pose estimation task.

In this work, we built a pose estimation system of
chairs based on a rendered image training set, which
predicts the pose of the chair in a real image. Our
pose estimation system takes a properly cropped
image as input, and outputs a probability vector on
pose space. Given a test image, we ﬁrst divide it into
a N × N patch grid. For each patch, a multi-class
classiﬁer is trained to estimate the probability of this
patch to be pose v. Then, scores from all patches
are combined to generate a probability vector for the
whole image.

Although we built a larger and more precise train-

ing dataset from rendered images, there is an obvious
drawback of this approach — the statistical prop-
erty of the training set and the test set are diﬀer-
ent. For instance, in the real world, there exists a
prior probability distribution of poses, which might
be non-uniform. Furthermore, even for features from
the same pose, real image features might be more di-
verse than rendered image features.
In this paper,
we proposed a method to revise the inﬂuence of the
diﬀerence in prior probability distribution. Detailed
methods and experiment results are shown in the fol-
lowing sections.

2 Dataset, Features and

Preprocessing

2.1 Training Data

As we mentioned in Section 1, we collected our train-
ing data from ShapeNet, a 3D shape model database,
which contains 5057 chair models. For each model,
we rendered it on 16 viewpoints, evenly distributed
on the horizontal circle, shown in Figure 1.

Figure 1: Chair models and rendering process

We chose 4000 models, accordingly 64,000 images
to build the training dataset, and leave the rest 1057
models to be our rendered image test set. When ex-
tracting image features, we ﬁrst resize the images to
112 × 112 pixels, and then divide it into 6 × 6 over-
lapped patch grid, with patch size 32 × 32 and patch
stride 16 on both axes. After that, we extract a 576
dimensional HoG features [3] for each patch, so the
whole image can be represented by a 20736 dimen-
sional feature vector. Those 64,000 feature vectors
constituted our training dataset.

1

2.2 Test Data

To better evaluate the performance of our learning
algorithm, we built three diﬀerent test sets with
increasing test diﬃculty. They are rendered image
test set, clean background real image test set and
cluttered background real image test set.

Rendered image test set consists of 1057 × 16
rendered images, which also comes from ShapeNet.
Clean background and cluttered background real im-
age test sets are collected from ImageNet [4], con-
taining 1309 and 1000 images respectively, both with
manually labeled pose ground truth. Some sample
images are shown in Figure 2. Obviously, these three
datasets are increasingly noisy and diﬃcult to tackle.

Figure 2: Clean background & cluttered background

For the test sets, we used the same scheme to pro-
cess the image as the training set. That is, convert
each image into a 20736-dimension HoG feature.

3 Model

Rather than using global image feature as the input
of classiﬁcation, our pose estimation model is patch-
based. By dividing image into patches and training
a classiﬁer for each patch, our model can be more
robust to occlusion and background noise. Also,
this approach reduced the feature dimension, thus
reduced the sample complexity for each classiﬁer.
Actually, we did try the global method, while the
classiﬁcation accuracy is 20% lower than patch based
method. The mathematical representation of our
patch based model is as follows.

¯v = arg max

v

In sum, our model takes Fi, i = 1,··· , N 2 as input,

and outputs P (v|I) and ¯v.

4 Methods

4.1 Learning Algorithms

4.1.1 Random Forest

In this project, we choose random forest [1] as a pri-
mary classiﬁcation algorithm based on following con-
siderations:

• Suitable for multiclass classiﬁcation.
• Non-parametric, easy to tune.
• Fast, easy to parallel.
• Robust, due to randomness.
During classiﬁcation, 36 random forest classiﬁers
are trained for 36 patches. As a trade oﬀ between
spatio-temporal complexity and performance, we set
the forest size to be 100 trees. We also tuned the
maximum depth of trees using cross-validation, where
the optimal depth is 20. When classiﬁcation, each
random forest outputs a probability vector P (v|Fi).
After Laplace smoothing, we calculated P (v|I), esti-
mated the pose to be ¯v = arg max

P (v|I).

v

4.1.2 Multiclass SVM

In binary classiﬁcation, SVM constructs a hyperplane
or set of hyperplanes in a high- or inﬁnite-dimensional
space that will separate data with diﬀerent labels as
wide as possible. In C-SVM model, the hyperplane
is generated by maximizing the following function

assume P (v|I) ∝ N 2(cid:81)

For each patch, we build a classiﬁer, which gives a
prediction of the conditional probability P (v|Fi).
To respresent P (v|I) in P (v|Fi), i = 1,··· , N 2, we
P (v|Fi). So, we can calculate
P (v|I) and the according ¯v using the following
formula:

i=1

P (v|I) =

N 2(cid:81)
N 2(cid:81)
V(cid:80)

i=1

v=1

i=1

P (v|Fi)

P (v|Fi)
P (v|I)

Deﬁne Fi as

the HoG feature of patch i,
I = (F1,··· , FN 2 ) to be the HoG feature of the
whole image, V = {1,··· , V } to be the pose space.

minγ,w,b

||w||2 + C

1
2

2

m(cid:88)

i=1

ξi

s.t.

y(i)(wT x(i) + b) ≥ 1 − ξi,
ξi ≥ 0, i = 1 . . . , m.

two important factors will shape the outcome of
model — kernel and soft margin parameter C.
Kernel deﬁnes a mapping that projects the input
attributes to higher dimension features, which could
often convert the non-separable data into separable.
C determines the trade-oﬀ between the training error
and VC dimension of the model, the smaller C is,
the less eﬀect will the outliers exert on the classiﬁer.

In our problem, we face a multiple classiﬁcation
problem which can not be addressed by building a
single SVM model. Here, we apply two methods to
solve it: 1. One-versus-Rest (OvR) 2. One-versus-
One (OvO). Both these two methods reduce the sin-
gle multiclass problem into multiple binary classiﬁca-
tion problems. OvR method train classiﬁers by sep-
arating data into one exact label and the rest, which
results in n models for n-label data, then it predicts
the result as the highest output. In OvO approach,
classiﬁcation is done by a max-wins voting strategy.
For n-label data, n(n − 1)/2 classiﬁers are trained,
with each trained by picking 2-label data from entire
n-label data. In prediction, each classiﬁer assigns the
input to one of the two labels it trained with, and
ﬁnally the label with the most votes determines the
instance classiﬁcation.

4.2 Optimization

Constructing training dataset from rendered images
has many advantages, but there are also drawbacks.
As I mentioned in Section 1, the prior probability of
pose in real images can be highly diﬀerent from that
in rendered images. As we know, pose distribution in
the training set is uniform, however, in real images,
there are far more front view chairs than back view.
Fortunately, this diﬀerence can be analyzed and mod-
eled as follows.

4.2.1 Probability Calibration

In classiﬁcation step, each classiﬁer Ci will output a
probability vector ˜P (v|Fi). Using Bayesian formula,
we have:

˜P (v|Fi) =

˜P (v) ˜P (Fi|v)

˜P (Fi)

˜P (v|Fi). Here, P (v), P (Fi|v) and P (Fi) are distri-
butions in the test set.
P (v|Fi) =

P (v)P (Fi|v)

P (Fi)

Assume the training data and the test data have at
least some similarity. Speciﬁcally speaking, assume
P (Fi|v) = ˜P (Fi|v), P (Fi) = ˜P (Fi), then we have:

P (v|Fi) = ˜P (v|Fi)

P (v)
˜P (v)

∝ ˜P (v|Fi)P (v)

To recover P (v|Fi), we just need to achieve a good
estimation of P (v). One possible method might be
randomly choosing some samples from the test set,
and manually label the ground truth of pose, regard
the ground truth pose distribution of samples as an
estimation of global P (v). However, we still need to
do some “labor work”.

Noticing the above formula can also be written as:

P (v|Fi)
P (v)

=

˜P (v|Fi)
˜P (v)

; ˜P (v) =

, ∀v ∈ V

1
V

we came up with another idea to automatically
improve the classiﬁcation result. For ˜P (v|Fi), we
have:

P (v) >

P (v) <

1
V
1
V

⇒ ˜P (v|Fi) < P (v|Fi)
⇒ ˜P (v|Fi) > P (v|Fi)

That means, when testing,

frequently appeared
poses are underestimated, while uncommon poses are
overestimated. Here, we will propose an iterative
method to counterbalance this eﬀect. Basically, we
will use ˜P (v|Fi) to generate an estimation ˜P (v) of
the prior distribution; assume P (v) and ˜P (v) have
similar common views and uncommon views (in other
words, P (v) and ˜P (v) have the same trend); smooth
˜P (v) to keep the trend while reduce ﬂuctuation range;
multiply the original ˜P (v|Fi) by smoothed ˜P (v); and
iteratively repeat the above steps. Finally, due to
the damping eﬀect in combination step, ˜P (v) will
converge, and ˜P (v|Fi) gets closer to P (v|Fi). For-
mulation of this iterative algorithm is as follows:
1. Calculate ˜P (v|I (j)), j = 1,··· , m.

Here, ˜P (v), ˜P (F|v) and ˜P (F ) are learned from train-
ing data. Whereas, the real P (v|Fi), which satis-
ﬁes the following formula, could be diﬀerent from

˜P (v|I (j)) =

3

N 2(cid:81)
N 2(cid:81)
V(cid:80)

i=1

v=1

i=1

˜P (v|F (j)

)

i

˜P (v|F (j)

i

)

2. Accumulate ˜P (v|I (j)) on all test samples to cal-

culate ˜P (v).

m(cid:88)

j=1

˜P (v) =

1
m

˜P (v|I (j))

3. Smooth ˜P (v) by factor α.

˜Ps(v) =

˜P (v) + α
1 + 16α

4. Estimate P (v|Fi) by letting:

¯P (v|Fi) = ˜P (v|Fi) ˜Ps(v)

5. Use ¯P (v|Fi) to re-calculate ˜P (v|I (j)) in step 1,
while remain ˜P (v|Fi) in step 4 unchanged, repeat
the above steps.

After several iterations, the algorithm will converge,
and we’ll get a ﬁnal estimation ¯P (v|Fi) of P (v|Fi).

4.2.2 Parameter Automatic Selection

However, diﬀerent α will lead to far diﬀerent converg-
ing results, as shown in Figure 3. From experiment
results in Figure 4 we observed that if α is too small,
viewpoint with the highest probability ˜P (v) will soon
beat other viewpoints, and ˜P (v) converges to a to-
tally biased distribution. While, if α is too large,
smoothing eﬀect is too strong to make any change of
¯P (v|Fi). However, there exists an intermediate value
of α to maximize the classiﬁcation accuracy and re-
sult in an optimal estimation ¯P (v|Fi).
In Figure 3
and 4, it is 0.8.

Figure 4: Stable distribution ˜P (v) w.r.t. α

is almost monotonically increasing with respect to α,
such as blue curves, some are monotonically decreas-
ing, such as the black curve, while others will decrease
after ﬁrst increase, such as the red curves. Recall
the distribution change with α in Figure 4, we found
˜P (v) will ﬁrst approximate P (v) then be smoothed.
So, patterns with turning points are reﬂection of this
trend. Sum on those components, we get Figure 6,
and take the turning point of the curve as our esti-
mated ¯α. Here ¯α is 1, very close the optimal value
0.8.

Figure 5: ˜P (vj) curve with respect to α

Figure 3: Classiﬁcation accuracy change w.r.t. α

To solve the optimal α, we conducted deep analysis
to the relationship between stable ˜P (v) and α. We
found three patterns of relationship between ˜P (vj)
and α, shown in Figure 5. For some viewpoints, ˜P (v)

5 Results and Discussion

5.1 Classiﬁcation Performance

Table 1 shows a promising classiﬁcation results on
all three test sets. Under our scheme, OvO SVM

4

−1−0.8−0.6−0.4−0.200.20.40.60.81505560657075808590Acc=89.3%α=0.8logαAccuracy24681012141600.10.20.30.40.50.60.70.80.91PoseConverged˜P(v)  Ground Trutha = 0a = 0.8a = 10−1−0.500.5100.010.020.030.040.05V1−1−0.500.5100.050.10.150.20.6V2−1−0.500.5100.050.10.150.20.9V3−1−0.500.5100.020.040.06V4−1−0.500.5100.511.522.5x 10−3V5−1−0.500.5100.0050.010.015V6−1−0.500.5100.010.020.030.04V7−1−0.500.5100.010.020.030.040.05V8−1−0.500.5102468x 10−3V9−1−0.500.5100.020.040.06V10−1−0.500.5100.010.020.03V11−1−0.500.5100.010.020.03V12−1−0.500.51012345x 10−3V13−1−0.500.5100.020.040.060.080.1V14−1−0.500.5100.20.40.60.81V15−1−0.500.5100.050.10.150.21.4V16Figure 7: Confusion matrix on rendered, clean, clut-
tered test sets

Figure 6: Estimated α

results veriﬁed the eﬀectiveness of our scheme. More
experiment details are omitted due to page limit.

achieves 83% accuracy on clean background real im-
age test set, and 78% on cluttered background test
set, which beats other algorithms. After calibrating
the conditional probability ˜P (v|Fi) using automat-
ically selected α, performance on clean test set is
boosted by 6%, as well 2% on cluttered set. The
relatively low improvement on cluttered test set may
result from our assumption of ˜P (Fi|v) = P (Fi|v) and
˜P (Fi) = P (Fi) are too strong for cluttered images.

RF(%)

RFopt(%)
OvO(%)

OvOopt(%)

OvR(%)

OvRopt(%)

—

95.89

—

93.42

—

Render Clean Cluttered
96.16

80.67
88.90
83.21
89.12
76.93
83.21

76.80
78.70
78.10
79.94
70.33
72.98

Table 1: Classiﬁcation accuracy on three test sets

Figure 7 shows the confusion matrix on three test
sets respectively. From left to right, as test diﬃ-
culty increases, confusion matrix becomes increas-
ingly scattered. On rendered image test set, an in-
teresting phenomenon is that some poses are often
misclassiﬁed to poses with 90◦ diﬀerence with them,
one possible explanation is that the shape of some
chairs are like a square. Also, front view and back-
view are often misclassiﬁed, because they have similar
appearance in feature space.

6 Conclusion

In this paper, we proposed a novel pose estimation
approach — learn from 3D models. We explained
our model in Bayesian framework, and raised a new
optimization method to transmit information from
test set to training set. The promising experiment

5

7 Future Work

Our ideas for the future work are described as follows:
• Take into consideration the foreground and back-
ground information in the image, fully utilize the
information in rendered images.

• Further model the diﬀerence between three

datasets, revise our inaccurate assumption.

• Learn the discriminativeness of patches, give dif-

ferent weight for diﬀerent patches.

References

[1] Breiman, Leo. “Random forests.” Machine learn-

ing 45.1 (2001): 5-32.

[2] Chang, Chih-Chung, and Chih-Jen Lin. “LIB-
SVM: a library for support vector machines.”
ACM Transactions on Intelligent Systems and
Technology (TIST) 2.3 (2011): 27.

[3] Dalal, Navneet, and Bill Triggs. “Histograms of
oriented gradients for human detection.” Com-
puter Vision and Pattern Recognition, 2005.
CVPR 2005. IEEE Computer Society Conference
on. Vol. 1. IEEE, 2005.

[4] Deng, Jia, et al. “Imagenet: A large-scale hier-
archical image database.” Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE
Conference on. IEEE, 2009.

[5] Pedregosa, Fabian, et al. “Scikit-learn: Machine
learning in Python.” The Journal of Machine
Learning Research 12 (2011): 2825-2830.

[6] Su, Hao, Qixing Huang and Guibas Leonidas.
“Shapenet.” <http://shapenet.cs.stanford.edu>

−1−0.8−0.6−0.4−0.200.20.40.60.8100.050.10.150.20.250.30.350.40.450.5logαProbability¯α=1