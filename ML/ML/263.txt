A BIGRAM EXTENSION TO WORD VECTOR REPRESENTATION

ADRIAN SANBORN AND JACEK SKRYZALIN

1. Background

GloVe is an algorithm which associates a vector to each word such that the dot product of two words
corresponds to the likelihood they appear together in a large corpus ([PSM14]). GloVe vectors achieve
state-of-the-art performance on word analogy tasks (v(king) − v(man) + v(woman) ≈ v(queen)), but
they are limited to capturing meanings of individual words. In our project, we develop “biGloVe,” a
version of GloVe that learns vector representations of bigrams. Using the full English Wikipedia text
as our training corpus, we compute 1.2 million bigram vectors in 150 dimensions.

To evaluate the quality of our biGloVe vectors, we apply them to two machine learning tasks.
The ﬁrst task is a 2012 SemEval challenge where one must determine the semantic similarity of two
sentences or phrases. We used logistic regression using as features the cosine similarity of the average
sentence (bi)GloVe vectors and found slightly better performance in one challenge when GloVe and
biGlove were combined, but generally, the usage of biGloVe vectors did not increase performance.
Second, we applied biGloVe vectors to classify the sentiment of movie reviews, training with naive
Bayes using bag-of-words, SVMs, and random forests. We found that naive Bayes or an SVM with
GloVe vectors performed the best.

Applications of biGloVe vectors were hindered by insuﬃcient bigram coverage, despite training 1.2
million vectors. At the same time, examination of nearest neighbors revealed that biGloVe vectors were
indeed capturing semantic relationships unique to bigrams, suggesting that the method has promise.
Training new vectors on a much larger corpus such as Common Crawl is likely to improve performance
of biGloVe vectors in tasks.

2. Bigram Word Vectors

The GloVe Algorithm: GloVe associates to each word w a vector v(w) ∈ Rn, where typically n
takes values between 50 and 300. This collection of vectors is trained ﬁrst by computing Xij, the
number of times word i occurs in the context of word j, within some large corpus of text C (we used
the English Wikipedia):

(cid:88)

Xij =

c,d∈[1,m]

1{C[c] = wi} · 1{C[d] = wj} · 1{0 < |c − d| ≤ 15}

|c − d|

,

where C[k] denotes the kth word in C, m is the number of words in C, and wi represents the ith word
in the lexicon obtained by collecting all words which occur in C. Note that the word C[k] cannot occur
in the context of itself, that we consider the context of a word to be all words that appear at most
15 places before or after the word in question, and we weight contextuality by the inverse distance
between the words in the article.
Let wi ∈ W be the ith word in the set of words W . GloVe uses gradient descent to minimize over
all possible assignments of v(wi), ˜v(wi) ∈ Rn and bi, ˜bj ∈ R the quantity

J =

Xij

v(wi) · ˜v(wj) + bi + ˜bj − log Xij

.

|W|(cid:88)

3/4(cid:16)

i,j=1

1

(cid:17)2

A BIGRAM EXTENSION TO WORD VECTOR REPRESENTATION

ADRIAN SANBORN AND JACEK SKRYZALIN

1. Background

GloVe is an algorithm which associates a vector to each word such that the dot product of two words
corresponds to the likelihood they appear together in a large corpus ([PSM14]). GloVe vectors achieve
state-of-the-art performance on word analogy tasks (v(king) − v(man) + v(woman) ≈ v(queen)), but
they are limited to capturing meanings of individual words. In our project, we develop “biGloVe,” a
version of GloVe that learns vector representations of bigrams. Using the full English Wikipedia text
as our training corpus, we compute 1.2 million bigram vectors in 150 dimensions.

To evaluate the quality of our biGloVe vectors, we apply them to two machine learning tasks.
The ﬁrst task is a 2012 SemEval challenge where one must determine the semantic similarity of two
sentences or phrases. We used logistic regression using as features the cosine similarity of the average
sentence (bi)GloVe vectors and found slightly better performance in one challenge when GloVe and
biGlove were combined, but generally, the usage of biGloVe vectors did not increase performance.
Second, we applied biGloVe vectors to classify the sentiment of movie reviews, training with naive
Bayes using bag-of-words, SVMs, and random forests. We found that naive Bayes or an SVM with
GloVe vectors performed the best.

Applications of biGloVe vectors were hindered by insuﬃcient bigram coverage, despite training 1.2
million vectors. At the same time, examination of nearest neighbors revealed that biGloVe vectors were
indeed capturing semantic relationships unique to bigrams, suggesting that the method has promise.
Training new vectors on a much larger corpus such as Common Crawl is likely to improve performance
of biGloVe vectors in tasks.

2. Bigram Word Vectors

The GloVe Algorithm: GloVe associates to each word w a vector v(w) ∈ Rn, where typically n
takes values between 50 and 300. This collection of vectors is trained ﬁrst by computing Xij, the
number of times word i occurs in the context of word j, within some large corpus of text C (we used
the English Wikipedia):

(cid:88)

Xij =

c,d∈[1,m]

1{C[c] = wi} · 1{C[d] = wj} · 1{0 < |c − d| ≤ 15}

|c − d|

,

where C[k] denotes the kth word in C, m is the number of words in C, and wi represents the ith word
in the lexicon obtained by collecting all words which occur in C. Note that the word C[k] cannot occur
in the context of itself, that we consider the context of a word to be all words that appear at most
15 places before or after the word in question, and we weight contextuality by the inverse distance
between the words in the article.
Let wi ∈ W be the ith word in the set of words W . GloVe uses gradient descent to minimize over
all possible assignments of v(wi), ˜v(wi) ∈ Rn and bi, ˜bj ∈ R the quantity

J =

Xij

v(wi) · ˜v(wj) + bi + ˜bj − log Xij

.

|W|(cid:88)

3/4(cid:16)

i,j=1

1

(cid:17)2

2

ADRIAN SANBORN AND JACEK SKRYZALIN

biGloVe: We develop a technique called biGloVe (bigram GloVe). biGloVe associates to each bigram
b a vector v(b) ∈ Rn (we choose n = 150 for this project). We hypothesize that by associating a vector
to each bigram we can capture new bigram-speciﬁc meanings.

To train biGloVe, we download all articles in English Wikipedia, removed all non-article content (i.e.,
pictures, urls, captions, metadata), converted all letters to lowercase, and removed all numbers. With
some experimentation, we noticed that the bigram frequency was strongly biased towards bigrams
containing one signiﬁcant word (e.g., “removal”, “tendency”) and one insigniﬁcant word (e.g., “the“,
“of”). In order to increase the information density of our bigrams, we removed 68 common stop words
which do not add to the meaning of a sentence. Finally, we trained biGloVe via the same gradient
descent algorithm as GloVe, except that the cooccurrence Xij is calculated via:

(cid:88)

Xij =

c,d∈[1,m−1]

1{C[c] = bi} · 1{C[d] = bj} · 1{0 < |c − d| ≤ 7}

,

|c − d|

where C[k] now denotes the kth bigram in C (which consists of the kth and (k + 1)th word in C),
and where bi represents the ith bigram in the lexicon obtained by collecting all bigrams which occur
in C. We performed ﬁfty iterations of stochastic gradient descent, which took around 40 hours on a
16 CPU node with 64GB memory.

Bigram Vector Space. We trained vectors for any bigram occurring more than 150 times in the
corpus, resulting in 1,240,128 vectors. To probe the semantic structure captured in these vectors, we
computed the ten nearest neighbors for several hundred of the most frequent bigrams, measured by
cosine similarity of the corresponding vectors, v1· v2/(cid:107)v1(cid:107)(cid:107)v2(cid:107). We observed that many neighbors cap-
tured new bigram-speciﬁc meaning. For example, nearest neighbors of “united nations” included “se-
curity council”, “secretary general”, “human rights”; nearest neighbors of “st louis” included “kansas
city” and “saint louis”; and nearest neighbors of “major league” included “professional baseball” and
“baseball player”.

3. Semantic Similarity Prediction

2 θ(cid:62)θ + C(cid:80)n

1

We use GloVe and biGloVe to produce feature vectors for a semantic similarity task. The data, down-
loaded from http://ixa2.si.ehu.es/sts/data/trial.tgz, was used for the SemEval-2012 task 17
challenge. The challenge contains 750 pairs of sentences in each of three groups, annotated with a
numerical label ranging from 0, indicating that the sentences have no overlapping meaning, to 5,
indicating that the sentences have the exact same meaning.

i=1 log(exp(−yi(X(cid:62)

We used logistic regression using a small feature vector, minimizing over all θ and c the quantity
i θ + c)) + 1) where we set C = 10,000 for weak regularization. We
considered four features: the lengths of the two sentences, and the cosine similarity between the
two sentences using the GloVe/biGlove vectors obtained by averaging the GloVe/biGloVe vectors of
all words/bigrams in the sentences. We tested three versions: one just using GloVe, one just using
biGloVe, and one using both GloVe and biGloVe. The percentage of correct similarity rankings is
given below:

Group 1

Group 2

Group 3

Train success Test success Train success Test success Train success Test success

GLV
bGLV

GLV+bGLV

0.347
0.220
0.325

0.306
0.189
0.341

0.713
0.289
0.736

0.716
0.295
0.718

0.684
0.620
0.702

0.403
0.085
0.361

Figure 1. Success rates using various algorithms and feature vectors for semantic similarity

We ﬁnd that biGloVe vectors alone perform poorly, suggesting insuﬃcient coverage of bigram vocab-
ulary. However, adding biGloVe to GloVe did improve performance slightly for one group of sentences.

A BIGRAM EXTENSION TO WORD VECTOR REPRESENTATION

ADRIAN SANBORN AND JACEK SKRYZALIN

1. Background

GloVe is an algorithm which associates a vector to each word such that the dot product of two words
corresponds to the likelihood they appear together in a large corpus ([PSM14]). GloVe vectors achieve
state-of-the-art performance on word analogy tasks (v(king) − v(man) + v(woman) ≈ v(queen)), but
they are limited to capturing meanings of individual words. In our project, we develop “biGloVe,” a
version of GloVe that learns vector representations of bigrams. Using the full English Wikipedia text
as our training corpus, we compute 1.2 million bigram vectors in 150 dimensions.

To evaluate the quality of our biGloVe vectors, we apply them to two machine learning tasks.
The ﬁrst task is a 2012 SemEval challenge where one must determine the semantic similarity of two
sentences or phrases. We used logistic regression using as features the cosine similarity of the average
sentence (bi)GloVe vectors and found slightly better performance in one challenge when GloVe and
biGlove were combined, but generally, the usage of biGloVe vectors did not increase performance.
Second, we applied biGloVe vectors to classify the sentiment of movie reviews, training with naive
Bayes using bag-of-words, SVMs, and random forests. We found that naive Bayes or an SVM with
GloVe vectors performed the best.

Applications of biGloVe vectors were hindered by insuﬃcient bigram coverage, despite training 1.2
million vectors. At the same time, examination of nearest neighbors revealed that biGloVe vectors were
indeed capturing semantic relationships unique to bigrams, suggesting that the method has promise.
Training new vectors on a much larger corpus such as Common Crawl is likely to improve performance
of biGloVe vectors in tasks.

2. Bigram Word Vectors

The GloVe Algorithm: GloVe associates to each word w a vector v(w) ∈ Rn, where typically n
takes values between 50 and 300. This collection of vectors is trained ﬁrst by computing Xij, the
number of times word i occurs in the context of word j, within some large corpus of text C (we used
the English Wikipedia):

(cid:88)

Xij =

c,d∈[1,m]

1{C[c] = wi} · 1{C[d] = wj} · 1{0 < |c − d| ≤ 15}

|c − d|

,

where C[k] denotes the kth word in C, m is the number of words in C, and wi represents the ith word
in the lexicon obtained by collecting all words which occur in C. Note that the word C[k] cannot occur
in the context of itself, that we consider the context of a word to be all words that appear at most
15 places before or after the word in question, and we weight contextuality by the inverse distance
between the words in the article.
Let wi ∈ W be the ith word in the set of words W . GloVe uses gradient descent to minimize over
all possible assignments of v(wi), ˜v(wi) ∈ Rn and bi, ˜bj ∈ R the quantity

J =

Xij

v(wi) · ˜v(wj) + bi + ˜bj − log Xij

.

|W|(cid:88)

3/4(cid:16)

i,j=1

1

(cid:17)2

2

ADRIAN SANBORN AND JACEK SKRYZALIN

biGloVe: We develop a technique called biGloVe (bigram GloVe). biGloVe associates to each bigram
b a vector v(b) ∈ Rn (we choose n = 150 for this project). We hypothesize that by associating a vector
to each bigram we can capture new bigram-speciﬁc meanings.

To train biGloVe, we download all articles in English Wikipedia, removed all non-article content (i.e.,
pictures, urls, captions, metadata), converted all letters to lowercase, and removed all numbers. With
some experimentation, we noticed that the bigram frequency was strongly biased towards bigrams
containing one signiﬁcant word (e.g., “removal”, “tendency”) and one insigniﬁcant word (e.g., “the“,
“of”). In order to increase the information density of our bigrams, we removed 68 common stop words
which do not add to the meaning of a sentence. Finally, we trained biGloVe via the same gradient
descent algorithm as GloVe, except that the cooccurrence Xij is calculated via:

(cid:88)

Xij =

c,d∈[1,m−1]

1{C[c] = bi} · 1{C[d] = bj} · 1{0 < |c − d| ≤ 7}

,

|c − d|

where C[k] now denotes the kth bigram in C (which consists of the kth and (k + 1)th word in C),
and where bi represents the ith bigram in the lexicon obtained by collecting all bigrams which occur
in C. We performed ﬁfty iterations of stochastic gradient descent, which took around 40 hours on a
16 CPU node with 64GB memory.

Bigram Vector Space. We trained vectors for any bigram occurring more than 150 times in the
corpus, resulting in 1,240,128 vectors. To probe the semantic structure captured in these vectors, we
computed the ten nearest neighbors for several hundred of the most frequent bigrams, measured by
cosine similarity of the corresponding vectors, v1· v2/(cid:107)v1(cid:107)(cid:107)v2(cid:107). We observed that many neighbors cap-
tured new bigram-speciﬁc meaning. For example, nearest neighbors of “united nations” included “se-
curity council”, “secretary general”, “human rights”; nearest neighbors of “st louis” included “kansas
city” and “saint louis”; and nearest neighbors of “major league” included “professional baseball” and
“baseball player”.

3. Semantic Similarity Prediction

2 θ(cid:62)θ + C(cid:80)n

1

We use GloVe and biGloVe to produce feature vectors for a semantic similarity task. The data, down-
loaded from http://ixa2.si.ehu.es/sts/data/trial.tgz, was used for the SemEval-2012 task 17
challenge. The challenge contains 750 pairs of sentences in each of three groups, annotated with a
numerical label ranging from 0, indicating that the sentences have no overlapping meaning, to 5,
indicating that the sentences have the exact same meaning.

i=1 log(exp(−yi(X(cid:62)

We used logistic regression using a small feature vector, minimizing over all θ and c the quantity
i θ + c)) + 1) where we set C = 10,000 for weak regularization. We
considered four features: the lengths of the two sentences, and the cosine similarity between the
two sentences using the GloVe/biGlove vectors obtained by averaging the GloVe/biGloVe vectors of
all words/bigrams in the sentences. We tested three versions: one just using GloVe, one just using
biGloVe, and one using both GloVe and biGloVe. The percentage of correct similarity rankings is
given below:

Group 1

Group 2

Group 3

Train success Test success Train success Test success Train success Test success

GLV
bGLV

GLV+bGLV

0.347
0.220
0.325

0.306
0.189
0.341

0.713
0.289
0.736

0.716
0.295
0.718

0.684
0.620
0.702

0.403
0.085
0.361

Figure 1. Success rates using various algorithms and feature vectors for semantic similarity

We ﬁnd that biGloVe vectors alone perform poorly, suggesting insuﬃcient coverage of bigram vocab-
ulary. However, adding biGloVe to GloVe did improve performance slightly for one group of sentences.

A BIGRAM EXTENSION TO WORD VECTOR REPRESENTATION

3

4. Sentiment Analysis

The Data: The data, downloaded from http://nlp.stanford.edu/sentiment/ ([SPW+13]), con-
sists of 11855 movie reviews with a train/dev/test split of 8544/2210/1101. Each movie review is
assigned a score from 0 to 1. The interval [0, 1] is split evenly into ﬁfths, and each ﬁfth is assigned a
sentiment (i.e., very-negative, negative, neutral, positive, and very-positive). It is our goal to correctly
categorize each review into its designated category. We record the percentage of reviews that have
been classiﬁed correctly.

We also consider a two-category system in which we consider reviews with assigned scores in the
interval [0, 0.4] to be “negative” and those with assigned scores in the range [0.6, 1] to be “positive”.
We apply all algorithms to both the ﬁve-category and two-category schemes; algorithmic adaptations
necessitated by having a non-binary classiﬁcation scheme are detailed below.

Before applying all algorithms, we ﬁlter all text as done in bigram vector training. We consider three
featurization schemes: bag-of-words, GloVe, and biGloVe. The bag-of-words (BOW) model associates
to each review r a sparse vector φ(r) indexed on the words in the article. The wth entry of φ(r) is
the number of times the word w appears in r. To construct GloVe and biGloVe feature vectors, we
average all word / bigram vectors corresponding to words in the review.

Naive Bayes: We use a multinomial naive Bayes (NB) model with Laplace smoothing. We use the
bag-of-words feature vectors only with this algorithm. To train the model with training set Ttrain, we
calculate the values

p(word x | category = c) =

1 +(cid:80)|Ttrain|
|vocabulary| +(cid:80)|Ttrain|
(cid:80)|Ttrain|

k=1

k=1

1{c = category(rk)} · φ(rk)[x]

1{c = category(rk)} · nk

p(category c) =

k=1

1{c = category(rk)}

,

|Ttrain|

where rk denotes the kth review in Ttrain, category(r) gives the category associated to review r, and
nk gives the number of words in the kth review rk. Given a test review r with BOW feature vector
φ(r), we predict the category with the highest posterior probability.

Support Vector Machines: We experimented with a variety of support vector machines (SVMs)
and found that a ν-SVM (as constructed in [SSWB00]) with a Gaussian kernel demonstrated optimal
performance. The parameter ν is an upper bound on the fraction of training errors and a lower bound
on the proportion of support vectors.

Concretely, given a training set Ttrain consisting of pairs(cid:0)φ(i), y(i)(cid:1), where y(i) = ±1, we solve the

Given a new feature vector φ, we assign the category determined by

where SV is the set of all support vectors.

i=1

|Ttrain|(cid:88)

following optimization problem:

W (α) = − 1
2

maximize

i,j=1

subject to

|Ttrain|(cid:88)
where K(φ, ψ) = exp(cid:2)−γ(cid:107)φ − ψ(cid:107)2(cid:3), where γ =

0 ≤ 1
αi

|Ttrain|

≤

1

αiαjy(i)y(j)K(φ(i), φ(j))

i=1

αiy(i) = 0

(cid:40)
K(φ, φ(i)) − 1

0.05 bag-of-words used
0.2

*GloVe used

.

(cid:88)

|SV |

φsv∈SV

K(φsv, φ(i))

 ,

|Ttrain|(cid:88)

sign

αiy(i)

|Ttrain|(cid:88)

i=1

αi ≥ ν =

(cid:40)

0.5 bag-of-words used
0.6

*GloVe used

A BIGRAM EXTENSION TO WORD VECTOR REPRESENTATION

ADRIAN SANBORN AND JACEK SKRYZALIN

1. Background

GloVe is an algorithm which associates a vector to each word such that the dot product of two words
corresponds to the likelihood they appear together in a large corpus ([PSM14]). GloVe vectors achieve
state-of-the-art performance on word analogy tasks (v(king) − v(man) + v(woman) ≈ v(queen)), but
they are limited to capturing meanings of individual words. In our project, we develop “biGloVe,” a
version of GloVe that learns vector representations of bigrams. Using the full English Wikipedia text
as our training corpus, we compute 1.2 million bigram vectors in 150 dimensions.

To evaluate the quality of our biGloVe vectors, we apply them to two machine learning tasks.
The ﬁrst task is a 2012 SemEval challenge where one must determine the semantic similarity of two
sentences or phrases. We used logistic regression using as features the cosine similarity of the average
sentence (bi)GloVe vectors and found slightly better performance in one challenge when GloVe and
biGlove were combined, but generally, the usage of biGloVe vectors did not increase performance.
Second, we applied biGloVe vectors to classify the sentiment of movie reviews, training with naive
Bayes using bag-of-words, SVMs, and random forests. We found that naive Bayes or an SVM with
GloVe vectors performed the best.

Applications of biGloVe vectors were hindered by insuﬃcient bigram coverage, despite training 1.2
million vectors. At the same time, examination of nearest neighbors revealed that biGloVe vectors were
indeed capturing semantic relationships unique to bigrams, suggesting that the method has promise.
Training new vectors on a much larger corpus such as Common Crawl is likely to improve performance
of biGloVe vectors in tasks.

2. Bigram Word Vectors

The GloVe Algorithm: GloVe associates to each word w a vector v(w) ∈ Rn, where typically n
takes values between 50 and 300. This collection of vectors is trained ﬁrst by computing Xij, the
number of times word i occurs in the context of word j, within some large corpus of text C (we used
the English Wikipedia):

(cid:88)

Xij =

c,d∈[1,m]

1{C[c] = wi} · 1{C[d] = wj} · 1{0 < |c − d| ≤ 15}

|c − d|

,

where C[k] denotes the kth word in C, m is the number of words in C, and wi represents the ith word
in the lexicon obtained by collecting all words which occur in C. Note that the word C[k] cannot occur
in the context of itself, that we consider the context of a word to be all words that appear at most
15 places before or after the word in question, and we weight contextuality by the inverse distance
between the words in the article.
Let wi ∈ W be the ith word in the set of words W . GloVe uses gradient descent to minimize over
all possible assignments of v(wi), ˜v(wi) ∈ Rn and bi, ˜bj ∈ R the quantity

J =

Xij

v(wi) · ˜v(wj) + bi + ˜bj − log Xij

.

|W|(cid:88)

3/4(cid:16)

i,j=1

1

(cid:17)2

2

ADRIAN SANBORN AND JACEK SKRYZALIN

biGloVe: We develop a technique called biGloVe (bigram GloVe). biGloVe associates to each bigram
b a vector v(b) ∈ Rn (we choose n = 150 for this project). We hypothesize that by associating a vector
to each bigram we can capture new bigram-speciﬁc meanings.

To train biGloVe, we download all articles in English Wikipedia, removed all non-article content (i.e.,
pictures, urls, captions, metadata), converted all letters to lowercase, and removed all numbers. With
some experimentation, we noticed that the bigram frequency was strongly biased towards bigrams
containing one signiﬁcant word (e.g., “removal”, “tendency”) and one insigniﬁcant word (e.g., “the“,
“of”). In order to increase the information density of our bigrams, we removed 68 common stop words
which do not add to the meaning of a sentence. Finally, we trained biGloVe via the same gradient
descent algorithm as GloVe, except that the cooccurrence Xij is calculated via:

(cid:88)

Xij =

c,d∈[1,m−1]

1{C[c] = bi} · 1{C[d] = bj} · 1{0 < |c − d| ≤ 7}

,

|c − d|

where C[k] now denotes the kth bigram in C (which consists of the kth and (k + 1)th word in C),
and where bi represents the ith bigram in the lexicon obtained by collecting all bigrams which occur
in C. We performed ﬁfty iterations of stochastic gradient descent, which took around 40 hours on a
16 CPU node with 64GB memory.

Bigram Vector Space. We trained vectors for any bigram occurring more than 150 times in the
corpus, resulting in 1,240,128 vectors. To probe the semantic structure captured in these vectors, we
computed the ten nearest neighbors for several hundred of the most frequent bigrams, measured by
cosine similarity of the corresponding vectors, v1· v2/(cid:107)v1(cid:107)(cid:107)v2(cid:107). We observed that many neighbors cap-
tured new bigram-speciﬁc meaning. For example, nearest neighbors of “united nations” included “se-
curity council”, “secretary general”, “human rights”; nearest neighbors of “st louis” included “kansas
city” and “saint louis”; and nearest neighbors of “major league” included “professional baseball” and
“baseball player”.

3. Semantic Similarity Prediction

2 θ(cid:62)θ + C(cid:80)n

1

We use GloVe and biGloVe to produce feature vectors for a semantic similarity task. The data, down-
loaded from http://ixa2.si.ehu.es/sts/data/trial.tgz, was used for the SemEval-2012 task 17
challenge. The challenge contains 750 pairs of sentences in each of three groups, annotated with a
numerical label ranging from 0, indicating that the sentences have no overlapping meaning, to 5,
indicating that the sentences have the exact same meaning.

i=1 log(exp(−yi(X(cid:62)

We used logistic regression using a small feature vector, minimizing over all θ and c the quantity
i θ + c)) + 1) where we set C = 10,000 for weak regularization. We
considered four features: the lengths of the two sentences, and the cosine similarity between the
two sentences using the GloVe/biGlove vectors obtained by averaging the GloVe/biGloVe vectors of
all words/bigrams in the sentences. We tested three versions: one just using GloVe, one just using
biGloVe, and one using both GloVe and biGloVe. The percentage of correct similarity rankings is
given below:

Group 1

Group 2

Group 3

Train success Test success Train success Test success Train success Test success

GLV
bGLV

GLV+bGLV

0.347
0.220
0.325

0.306
0.189
0.341

0.713
0.289
0.736

0.716
0.295
0.718

0.684
0.620
0.702

0.403
0.085
0.361

Figure 1. Success rates using various algorithms and feature vectors for semantic similarity

We ﬁnd that biGloVe vectors alone perform poorly, suggesting insuﬃcient coverage of bigram vocab-
ulary. However, adding biGloVe to GloVe did improve performance slightly for one group of sentences.

A BIGRAM EXTENSION TO WORD VECTOR REPRESENTATION

3

4. Sentiment Analysis

The Data: The data, downloaded from http://nlp.stanford.edu/sentiment/ ([SPW+13]), con-
sists of 11855 movie reviews with a train/dev/test split of 8544/2210/1101. Each movie review is
assigned a score from 0 to 1. The interval [0, 1] is split evenly into ﬁfths, and each ﬁfth is assigned a
sentiment (i.e., very-negative, negative, neutral, positive, and very-positive). It is our goal to correctly
categorize each review into its designated category. We record the percentage of reviews that have
been classiﬁed correctly.

We also consider a two-category system in which we consider reviews with assigned scores in the
interval [0, 0.4] to be “negative” and those with assigned scores in the range [0.6, 1] to be “positive”.
We apply all algorithms to both the ﬁve-category and two-category schemes; algorithmic adaptations
necessitated by having a non-binary classiﬁcation scheme are detailed below.

Before applying all algorithms, we ﬁlter all text as done in bigram vector training. We consider three
featurization schemes: bag-of-words, GloVe, and biGloVe. The bag-of-words (BOW) model associates
to each review r a sparse vector φ(r) indexed on the words in the article. The wth entry of φ(r) is
the number of times the word w appears in r. To construct GloVe and biGloVe feature vectors, we
average all word / bigram vectors corresponding to words in the review.

Naive Bayes: We use a multinomial naive Bayes (NB) model with Laplace smoothing. We use the
bag-of-words feature vectors only with this algorithm. To train the model with training set Ttrain, we
calculate the values

p(word x | category = c) =

1 +(cid:80)|Ttrain|
|vocabulary| +(cid:80)|Ttrain|
(cid:80)|Ttrain|

k=1

k=1

1{c = category(rk)} · φ(rk)[x]

1{c = category(rk)} · nk

p(category c) =

k=1

1{c = category(rk)}

,

|Ttrain|

where rk denotes the kth review in Ttrain, category(r) gives the category associated to review r, and
nk gives the number of words in the kth review rk. Given a test review r with BOW feature vector
φ(r), we predict the category with the highest posterior probability.

Support Vector Machines: We experimented with a variety of support vector machines (SVMs)
and found that a ν-SVM (as constructed in [SSWB00]) with a Gaussian kernel demonstrated optimal
performance. The parameter ν is an upper bound on the fraction of training errors and a lower bound
on the proportion of support vectors.

Concretely, given a training set Ttrain consisting of pairs(cid:0)φ(i), y(i)(cid:1), where y(i) = ±1, we solve the

Given a new feature vector φ, we assign the category determined by

where SV is the set of all support vectors.

i=1

|Ttrain|(cid:88)

following optimization problem:

W (α) = − 1
2

maximize

i,j=1

subject to

|Ttrain|(cid:88)
where K(φ, ψ) = exp(cid:2)−γ(cid:107)φ − ψ(cid:107)2(cid:3), where γ =

0 ≤ 1
αi

|Ttrain|

≤

1

αiαjy(i)y(j)K(φ(i), φ(j))

i=1

αiy(i) = 0

(cid:40)
K(φ, φ(i)) − 1

0.05 bag-of-words used
0.2

*GloVe used

.

(cid:88)

|SV |

φsv∈SV

K(φsv, φ(i))

 ,

|Ttrain|(cid:88)

sign

αiy(i)

|Ttrain|(cid:88)

i=1

αi ≥ ν =

(cid:40)

0.5 bag-of-words used
0.6

*GloVe used

4

ADRIAN SANBORN AND JACEK SKRYZALIN

For the case where we have 5 classes, we use the “one-vs-one” approach: we construct 10 SVMs to
separate each pair of classes. Upon receiving a new feature vector φ, we test φ against each SVM and
tally the number of times that φ is assigned to each class. We classify φ according to the category to
which it is most frequently assigned. If “ties” occur, φ is assigned the class based on the classiﬁcation
provided by the furthest hyperplane.

Random forests: To train our random forests (RF), we create 250 decision trees (as outlined in
[Bre01]). We create the decision trees via the CART algorithm, minimizing Gini impurity. To test
our random forest, we output the most common decision of the 250 decision trees.

Results: We provide the computed success rates for the various (learning algorithm, feature vector)
pairs. The best test success rates for each classiﬁcation scheme are bold and underlined.

Two sentiments

Five sentiments

Train success Test success Train success Test success

NB

SVM

RF

BOW
BOW
GLV
bGLV

GLV+bGLV

GLV
bGLV

GLV+bGLV

0.944
0.987
0.850
0.916
0.990

1

0.932

1

0.818
0.813
0.793
0.652
0.763
0.769
0.643
0.762

0.807
0.955
0.741
0.842
0.980
0.999
0.882
0.999

0.399
0.401
0.415
0.295
0.382
0.397
0.317
0.403

Figure 2. Success rates using various algorithms and feature vectors for sentiment analysis

One of the most important messages gleaned from these results is that naive Bayes, which is both
simple and easily implemented, is also highly eﬀective. Also important is that ν-SVMs can improve
success rates over regular SVMs: using a ν-SVM produced success rates around 4% higher than those
of a standard SVM using GloVe features (using ν-SVMs with the GloVe feature model produced higher
success rates than naive Bayes!), and success rates around 17% higher than those of a standard SVM
using BOW features (standard SVM results not shown). It can be concluded that ν-SVMs allow us
to harness the power of SVMs when the data is sparse and/or very high dimensional.

Interestingly, we found that neural networks generally produced relatively low success rates (success
rates not shown here). This comes as a surprise: other experiments not reported here involving the 20
Newsgroups dataset showed that using neural networks with the GloVe feature model can rival naive
Bayes on document classiﬁcation tasks.

5. Discussion

We eﬀectively implemented biGloVe and trained bigram vectors on the full English Wikipedia text,
and the resulting vectors captured new bigram-speciﬁc meaning. However, when our biGloVe vectors
were used in semantic similarity and sentiment analysis tasks, they performed poorly on their own
and did not substantially improve performance in combination with GloVe vectors.

Because the number of bigrams needed is on the order of the square of the English vocabulary
size, achieving ample vocabulary coverage is a particular challenge in training biGloVe vectors. Upon
closer inspection, despite training over 1.2 million bigrams, we found that algorithm performance was
hindered by low vocabulary coverage of bigrams. For example, over 50% of bigrams in the sentiment
analysis movie reviews were not in our biGloVe dictionary. Furthermore, expanding the vocabulary by
lowering the corpus co-occurrence threshold is not suﬃcient to achieve signiﬁcantly greater coverage:
training a vector for any bigram occurring more than 50 times in the Wikipedia corpus (rather than

A BIGRAM EXTENSION TO WORD VECTOR REPRESENTATION

ADRIAN SANBORN AND JACEK SKRYZALIN

1. Background

GloVe is an algorithm which associates a vector to each word such that the dot product of two words
corresponds to the likelihood they appear together in a large corpus ([PSM14]). GloVe vectors achieve
state-of-the-art performance on word analogy tasks (v(king) − v(man) + v(woman) ≈ v(queen)), but
they are limited to capturing meanings of individual words. In our project, we develop “biGloVe,” a
version of GloVe that learns vector representations of bigrams. Using the full English Wikipedia text
as our training corpus, we compute 1.2 million bigram vectors in 150 dimensions.

To evaluate the quality of our biGloVe vectors, we apply them to two machine learning tasks.
The ﬁrst task is a 2012 SemEval challenge where one must determine the semantic similarity of two
sentences or phrases. We used logistic regression using as features the cosine similarity of the average
sentence (bi)GloVe vectors and found slightly better performance in one challenge when GloVe and
biGlove were combined, but generally, the usage of biGloVe vectors did not increase performance.
Second, we applied biGloVe vectors to classify the sentiment of movie reviews, training with naive
Bayes using bag-of-words, SVMs, and random forests. We found that naive Bayes or an SVM with
GloVe vectors performed the best.

Applications of biGloVe vectors were hindered by insuﬃcient bigram coverage, despite training 1.2
million vectors. At the same time, examination of nearest neighbors revealed that biGloVe vectors were
indeed capturing semantic relationships unique to bigrams, suggesting that the method has promise.
Training new vectors on a much larger corpus such as Common Crawl is likely to improve performance
of biGloVe vectors in tasks.

2. Bigram Word Vectors

The GloVe Algorithm: GloVe associates to each word w a vector v(w) ∈ Rn, where typically n
takes values between 50 and 300. This collection of vectors is trained ﬁrst by computing Xij, the
number of times word i occurs in the context of word j, within some large corpus of text C (we used
the English Wikipedia):

(cid:88)

Xij =

c,d∈[1,m]

1{C[c] = wi} · 1{C[d] = wj} · 1{0 < |c − d| ≤ 15}

|c − d|

,

where C[k] denotes the kth word in C, m is the number of words in C, and wi represents the ith word
in the lexicon obtained by collecting all words which occur in C. Note that the word C[k] cannot occur
in the context of itself, that we consider the context of a word to be all words that appear at most
15 places before or after the word in question, and we weight contextuality by the inverse distance
between the words in the article.
Let wi ∈ W be the ith word in the set of words W . GloVe uses gradient descent to minimize over
all possible assignments of v(wi), ˜v(wi) ∈ Rn and bi, ˜bj ∈ R the quantity

J =

Xij

v(wi) · ˜v(wj) + bi + ˜bj − log Xij

.

|W|(cid:88)

3/4(cid:16)

i,j=1

1

(cid:17)2

2

ADRIAN SANBORN AND JACEK SKRYZALIN

biGloVe: We develop a technique called biGloVe (bigram GloVe). biGloVe associates to each bigram
b a vector v(b) ∈ Rn (we choose n = 150 for this project). We hypothesize that by associating a vector
to each bigram we can capture new bigram-speciﬁc meanings.

To train biGloVe, we download all articles in English Wikipedia, removed all non-article content (i.e.,
pictures, urls, captions, metadata), converted all letters to lowercase, and removed all numbers. With
some experimentation, we noticed that the bigram frequency was strongly biased towards bigrams
containing one signiﬁcant word (e.g., “removal”, “tendency”) and one insigniﬁcant word (e.g., “the“,
“of”). In order to increase the information density of our bigrams, we removed 68 common stop words
which do not add to the meaning of a sentence. Finally, we trained biGloVe via the same gradient
descent algorithm as GloVe, except that the cooccurrence Xij is calculated via:

(cid:88)

Xij =

c,d∈[1,m−1]

1{C[c] = bi} · 1{C[d] = bj} · 1{0 < |c − d| ≤ 7}

,

|c − d|

where C[k] now denotes the kth bigram in C (which consists of the kth and (k + 1)th word in C),
and where bi represents the ith bigram in the lexicon obtained by collecting all bigrams which occur
in C. We performed ﬁfty iterations of stochastic gradient descent, which took around 40 hours on a
16 CPU node with 64GB memory.

Bigram Vector Space. We trained vectors for any bigram occurring more than 150 times in the
corpus, resulting in 1,240,128 vectors. To probe the semantic structure captured in these vectors, we
computed the ten nearest neighbors for several hundred of the most frequent bigrams, measured by
cosine similarity of the corresponding vectors, v1· v2/(cid:107)v1(cid:107)(cid:107)v2(cid:107). We observed that many neighbors cap-
tured new bigram-speciﬁc meaning. For example, nearest neighbors of “united nations” included “se-
curity council”, “secretary general”, “human rights”; nearest neighbors of “st louis” included “kansas
city” and “saint louis”; and nearest neighbors of “major league” included “professional baseball” and
“baseball player”.

3. Semantic Similarity Prediction

2 θ(cid:62)θ + C(cid:80)n

1

We use GloVe and biGloVe to produce feature vectors for a semantic similarity task. The data, down-
loaded from http://ixa2.si.ehu.es/sts/data/trial.tgz, was used for the SemEval-2012 task 17
challenge. The challenge contains 750 pairs of sentences in each of three groups, annotated with a
numerical label ranging from 0, indicating that the sentences have no overlapping meaning, to 5,
indicating that the sentences have the exact same meaning.

i=1 log(exp(−yi(X(cid:62)

We used logistic regression using a small feature vector, minimizing over all θ and c the quantity
i θ + c)) + 1) where we set C = 10,000 for weak regularization. We
considered four features: the lengths of the two sentences, and the cosine similarity between the
two sentences using the GloVe/biGlove vectors obtained by averaging the GloVe/biGloVe vectors of
all words/bigrams in the sentences. We tested three versions: one just using GloVe, one just using
biGloVe, and one using both GloVe and biGloVe. The percentage of correct similarity rankings is
given below:

Group 1

Group 2

Group 3

Train success Test success Train success Test success Train success Test success

GLV
bGLV

GLV+bGLV

0.347
0.220
0.325

0.306
0.189
0.341

0.713
0.289
0.736

0.716
0.295
0.718

0.684
0.620
0.702

0.403
0.085
0.361

Figure 1. Success rates using various algorithms and feature vectors for semantic similarity

We ﬁnd that biGloVe vectors alone perform poorly, suggesting insuﬃcient coverage of bigram vocab-
ulary. However, adding biGloVe to GloVe did improve performance slightly for one group of sentences.

A BIGRAM EXTENSION TO WORD VECTOR REPRESENTATION

3

4. Sentiment Analysis

The Data: The data, downloaded from http://nlp.stanford.edu/sentiment/ ([SPW+13]), con-
sists of 11855 movie reviews with a train/dev/test split of 8544/2210/1101. Each movie review is
assigned a score from 0 to 1. The interval [0, 1] is split evenly into ﬁfths, and each ﬁfth is assigned a
sentiment (i.e., very-negative, negative, neutral, positive, and very-positive). It is our goal to correctly
categorize each review into its designated category. We record the percentage of reviews that have
been classiﬁed correctly.

We also consider a two-category system in which we consider reviews with assigned scores in the
interval [0, 0.4] to be “negative” and those with assigned scores in the range [0.6, 1] to be “positive”.
We apply all algorithms to both the ﬁve-category and two-category schemes; algorithmic adaptations
necessitated by having a non-binary classiﬁcation scheme are detailed below.

Before applying all algorithms, we ﬁlter all text as done in bigram vector training. We consider three
featurization schemes: bag-of-words, GloVe, and biGloVe. The bag-of-words (BOW) model associates
to each review r a sparse vector φ(r) indexed on the words in the article. The wth entry of φ(r) is
the number of times the word w appears in r. To construct GloVe and biGloVe feature vectors, we
average all word / bigram vectors corresponding to words in the review.

Naive Bayes: We use a multinomial naive Bayes (NB) model with Laplace smoothing. We use the
bag-of-words feature vectors only with this algorithm. To train the model with training set Ttrain, we
calculate the values

p(word x | category = c) =

1 +(cid:80)|Ttrain|
|vocabulary| +(cid:80)|Ttrain|
(cid:80)|Ttrain|

k=1

k=1

1{c = category(rk)} · φ(rk)[x]

1{c = category(rk)} · nk

p(category c) =

k=1

1{c = category(rk)}

,

|Ttrain|

where rk denotes the kth review in Ttrain, category(r) gives the category associated to review r, and
nk gives the number of words in the kth review rk. Given a test review r with BOW feature vector
φ(r), we predict the category with the highest posterior probability.

Support Vector Machines: We experimented with a variety of support vector machines (SVMs)
and found that a ν-SVM (as constructed in [SSWB00]) with a Gaussian kernel demonstrated optimal
performance. The parameter ν is an upper bound on the fraction of training errors and a lower bound
on the proportion of support vectors.

Concretely, given a training set Ttrain consisting of pairs(cid:0)φ(i), y(i)(cid:1), where y(i) = ±1, we solve the

Given a new feature vector φ, we assign the category determined by

where SV is the set of all support vectors.

i=1

|Ttrain|(cid:88)

following optimization problem:

W (α) = − 1
2

maximize

i,j=1

subject to

|Ttrain|(cid:88)
where K(φ, ψ) = exp(cid:2)−γ(cid:107)φ − ψ(cid:107)2(cid:3), where γ =

0 ≤ 1
αi

|Ttrain|

≤

1

αiαjy(i)y(j)K(φ(i), φ(j))

i=1

αiy(i) = 0

(cid:40)
K(φ, φ(i)) − 1

0.05 bag-of-words used
0.2

*GloVe used

.

(cid:88)

|SV |

φsv∈SV

K(φsv, φ(i))

 ,

|Ttrain|(cid:88)

sign

αiy(i)

|Ttrain|(cid:88)

i=1

αi ≥ ν =

(cid:40)

0.5 bag-of-words used
0.6

*GloVe used

4

ADRIAN SANBORN AND JACEK SKRYZALIN

For the case where we have 5 classes, we use the “one-vs-one” approach: we construct 10 SVMs to
separate each pair of classes. Upon receiving a new feature vector φ, we test φ against each SVM and
tally the number of times that φ is assigned to each class. We classify φ according to the category to
which it is most frequently assigned. If “ties” occur, φ is assigned the class based on the classiﬁcation
provided by the furthest hyperplane.

Random forests: To train our random forests (RF), we create 250 decision trees (as outlined in
[Bre01]). We create the decision trees via the CART algorithm, minimizing Gini impurity. To test
our random forest, we output the most common decision of the 250 decision trees.

Results: We provide the computed success rates for the various (learning algorithm, feature vector)
pairs. The best test success rates for each classiﬁcation scheme are bold and underlined.

Two sentiments

Five sentiments

Train success Test success Train success Test success

NB

SVM

RF

BOW
BOW
GLV
bGLV

GLV+bGLV

GLV
bGLV

GLV+bGLV

0.944
0.987
0.850
0.916
0.990

1

0.932

1

0.818
0.813
0.793
0.652
0.763
0.769
0.643
0.762

0.807
0.955
0.741
0.842
0.980
0.999
0.882
0.999

0.399
0.401
0.415
0.295
0.382
0.397
0.317
0.403

Figure 2. Success rates using various algorithms and feature vectors for sentiment analysis

One of the most important messages gleaned from these results is that naive Bayes, which is both
simple and easily implemented, is also highly eﬀective. Also important is that ν-SVMs can improve
success rates over regular SVMs: using a ν-SVM produced success rates around 4% higher than those
of a standard SVM using GloVe features (using ν-SVMs with the GloVe feature model produced higher
success rates than naive Bayes!), and success rates around 17% higher than those of a standard SVM
using BOW features (standard SVM results not shown). It can be concluded that ν-SVMs allow us
to harness the power of SVMs when the data is sparse and/or very high dimensional.

Interestingly, we found that neural networks generally produced relatively low success rates (success
rates not shown here). This comes as a surprise: other experiments not reported here involving the 20
Newsgroups dataset showed that using neural networks with the GloVe feature model can rival naive
Bayes on document classiﬁcation tasks.

5. Discussion

We eﬀectively implemented biGloVe and trained bigram vectors on the full English Wikipedia text,
and the resulting vectors captured new bigram-speciﬁc meaning. However, when our biGloVe vectors
were used in semantic similarity and sentiment analysis tasks, they performed poorly on their own
and did not substantially improve performance in combination with GloVe vectors.

Because the number of bigrams needed is on the order of the square of the English vocabulary
size, achieving ample vocabulary coverage is a particular challenge in training biGloVe vectors. Upon
closer inspection, despite training over 1.2 million bigrams, we found that algorithm performance was
hindered by low vocabulary coverage of bigrams. For example, over 50% of bigrams in the sentiment
analysis movie reviews were not in our biGloVe dictionary. Furthermore, expanding the vocabulary by
lowering the corpus co-occurrence threshold is not suﬃcient to achieve signiﬁcantly greater coverage:
training a vector for any bigram occurring more than 50 times in the Wikipedia corpus (rather than

A BIGRAM EXTENSION TO WORD VECTOR REPRESENTATION

5

150) would produce about 3.7 million vectors but only reduce the percentage of missing bigrams from
59% to 49% (see ﬁgure).

Figure 3. Bigram coverage of movie review text as a function of training corpus
occurrence threshold

By comparing the movie review text and biGloVe vocabulary, it also became evident that some not-
uncommon colloquial bigrams were missing from the vocabulary. It appears that the formal nature of
the Wikipedia text also biased the biGloVe vocabulary away from the text in our applications.

Although our implementation of biGloVe didn’t perform as well as expected, we nonetheless feel
that the algorithm is sound, and that examination of biGloVe neighbors suggests that the model still
has potential. As such, as a next step, we would like to train biGloVe on a much larger corpus, ideally
the combination of Wikipedia, Common Crawl, and Gigaword 5. We estimate that a vocabulary
of around 10 million biGloVe vectors should provide suﬃcient coverage for our applications, and we
would expect then to see improved performance when biGloVe is combined with GloVe. Additionally,
the semantic space of bigrams is more complex than the space of individual words, and we expect that
biGloVe performance will continue to improve as the vector dimension is increased, at least to 300
dimensions, and perhaps to 1000 dimensions.

Once second-generation biGloVe vectors have been trained, we would also like to experiment with
other statistical methods for using the vectors in applications. For example, instead of averaging all
word vectors in order to create a vector for the entire review, it would be interesting to instead analyze
the distribution of all word vectors in a review. Using more advanced statistical techniques, one might
then be able to correlate certain distributions of word vectors with certain sentiment classes.

References

[Bre01]
[PSM14]

Leo Breiman. Random forests. Mach. Learn., 45(1):5–32, October 2001.
Jeﬀrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word represen-
tation. In Proceedings of EMNLP, 2014.

[SPW+13] Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and

Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. 2013.

[SSWB00] Bernhard Sch¨olkopf, Alex J. Smola, Robert C. Williamson, and Peter L. Bartlett. New support vector algo-

rithms. Neural Comput., 12(5):1207–1245, May 2000.

E-mail address: asanborn@stanford.edu

E-mail address: jskryzal@stanford.edu

