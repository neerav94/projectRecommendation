Predicting and Evaluating the Popularity of

Online News

He Ren

Department of Electrical Engineering

heren@stanford.edu

Quan Yang

Department of Electrical Engineering

quanyang@stanford.edu

Abstract—With the expansion of the Internet, more
and more people enjoys reading and sharing online
news articles. The number of shares under a news
article indicates how popular the news is. In this
project, we intend to ﬁnd the best model and set
of feature to predict the popularity of online news,
using machine learning techniques. Our data comes
from Mashable, a well-known online news website.
We implemented 10 diﬀerent learning algorithms on
the dataset, ranging from various regressions to SVM
and Random Forest. Their performances are recorded
and compared. Feature selection methods are used to
improve performance and reduce features. Random
Forest turns out to be the best model for prediction,
and it can achieve an accuracy of 70% with optimal
parameters. Our work can help online news companies
to predict news popularity before publication.

Keywords - Machine learning; Classiﬁcation; Popu-

larity prediction; Feature selection; Model selection

I.

Introduction

In this information era, reading and sharing
news have become the center of people’s entertain-
ment lives. Therefore, it would be greatly helpful if
we could accurately predict the popularity of news
prior to its publication, for social media workers
(authors, advertisers, etc). For the purpose of this
paper, we intend to make use of a largely and
recently collected dataset with over 39000 articles
from Mashable website, to ﬁrst select informative
features and then analyze and compare the perfor-
mance of several machine learning algorithms.

Some prediction approaches are based on ana-
lyzing early users’ comments [1], or features about
post contents and domains [2]. Another proposed
method [3] predicted the article’s popularity not
only based on its own appeal, but also other ar-
ticles that it is competing with. Prediction models
with SVMs, Ranking SVMs [3], Naive Bayes [2]

are investigated, and more advanced algorithms
such as Random Forest, Adaptive Boosting [4] could
increase the precision. This paper however, incorpo-
rates a broader and more abstracter set of features,
and starts with basic regression and classiﬁcation
models to advanced ones, with elaborations about
eﬀective feature selection.

This paper has the following structure. Section II
introduces our dataset and feature selection. Section
III gives our implementation of various learning
algorithms. We analyze the result and compare the
performances in Section IV. In Section V, we discuss
possible future work.

II. Dataset & Feature Selection

A. Data collection

Our dataset is provided by UCI machine learn-
ing repository [4], originally acquired and pre-
processed by K.Fernandes et al. It extracts 59 at-
tributes (as numerical values) describing diﬀerent
aspects of each article, from a total of 39644 articles
published in the last two years from Mashable
website. The full feature set is mainly categorized
as in Table 1.

B. Features

A full feature set may include much noise. We
ﬁrst attempted PCA for dimension reduction but it
did not provide any improvements for our models,
then we used ﬁlter methods (mutual information,
and Fisher criterion) for feature selection and im-
proved our prediction accuracy.

PCA is a commonly used dimensionality re-
duction algorithm, which could give us a lower-
dimensional approximation for original dataset
while preserving as much variability as possible.
However, the PCA results could only made our

Predicting and Evaluating the Popularity of

Online News

He Ren

Department of Electrical Engineering

heren@stanford.edu

Quan Yang

Department of Electrical Engineering

quanyang@stanford.edu

Abstract—With the expansion of the Internet, more
and more people enjoys reading and sharing online
news articles. The number of shares under a news
article indicates how popular the news is. In this
project, we intend to ﬁnd the best model and set
of feature to predict the popularity of online news,
using machine learning techniques. Our data comes
from Mashable, a well-known online news website.
We implemented 10 diﬀerent learning algorithms on
the dataset, ranging from various regressions to SVM
and Random Forest. Their performances are recorded
and compared. Feature selection methods are used to
improve performance and reduce features. Random
Forest turns out to be the best model for prediction,
and it can achieve an accuracy of 70% with optimal
parameters. Our work can help online news companies
to predict news popularity before publication.

Keywords - Machine learning; Classiﬁcation; Popu-

larity prediction; Feature selection; Model selection

I.

Introduction

In this information era, reading and sharing
news have become the center of people’s entertain-
ment lives. Therefore, it would be greatly helpful if
we could accurately predict the popularity of news
prior to its publication, for social media workers
(authors, advertisers, etc). For the purpose of this
paper, we intend to make use of a largely and
recently collected dataset with over 39000 articles
from Mashable website, to ﬁrst select informative
features and then analyze and compare the perfor-
mance of several machine learning algorithms.

Some prediction approaches are based on ana-
lyzing early users’ comments [1], or features about
post contents and domains [2]. Another proposed
method [3] predicted the article’s popularity not
only based on its own appeal, but also other ar-
ticles that it is competing with. Prediction models
with SVMs, Ranking SVMs [3], Naive Bayes [2]

are investigated, and more advanced algorithms
such as Random Forest, Adaptive Boosting [4] could
increase the precision. This paper however, incorpo-
rates a broader and more abstracter set of features,
and starts with basic regression and classiﬁcation
models to advanced ones, with elaborations about
eﬀective feature selection.

This paper has the following structure. Section II
introduces our dataset and feature selection. Section
III gives our implementation of various learning
algorithms. We analyze the result and compare the
performances in Section IV. In Section V, we discuss
possible future work.

II. Dataset & Feature Selection

A. Data collection

Our dataset is provided by UCI machine learn-
ing repository [4], originally acquired and pre-
processed by K.Fernandes et al. It extracts 59 at-
tributes (as numerical values) describing diﬀerent
aspects of each article, from a total of 39644 articles
published in the last two years from Mashable
website. The full feature set is mainly categorized
as in Table 1.

B. Features

A full feature set may include much noise. We
ﬁrst attempted PCA for dimension reduction but it
did not provide any improvements for our models,
then we used ﬁlter methods (mutual information,
and Fisher criterion) for feature selection and im-
proved our prediction accuracy.

PCA is a commonly used dimensionality re-
duction algorithm, which could give us a lower-
dimensional approximation for original dataset
while preserving as much variability as possible.
However, the PCA results could only made our

TABLE I.

All Available Features

Features

Number of words of the title/content;

Average word length;

Rate of unique/non-stop words of contents

Aspects

Words

Links

Number of links;

Number of links to other articles in Mashable

Digital Media

Number of images/videos

Publication Time

Day of the week/weekend

Keywords

NLP

Number of keywords;

Worst/best/average keywords (#shares);

Article category

Closeness to ﬁve LDA topics;
Title/Text polarity/subjectivity;

Rate and polarity of positive/negative words;

Absolute subjectivity/polarity level

Target

Number of shares at Mashable

models perform worse. This is because the original
feature set is well-designed and correlated informa-
tion between features is limited.

Filter Methods

1) Mutual information: We calculate the mutual
information M I(xi, y) between features and class
labels to be the score to rank features, which can be
expressed as the Kullback-Leibler(KL) divergence:

M I(xi, y) = KL(p(xi, y)||p(xi)p(y))

2) Fisher criterion: Fisher criterion is another ef-
fective way in feature ranking. The Fish score (for
data with two classes) for jth feature is given by:

F (j) =

j − ¯x2
j )2
(¯x1
(s1
j )2 + (s2
j )2

where

(sk

j )2 =

(xj − ¯xk
j )2

(cid:88)

x∈X k

The numerator indicates the discrimination be-
tween popular and unpopular news, and the de-
nominator indicates the scatter within each class.
The larger the F-score is, the more likely this fea-
ture is more discriminative. Then we used crossed
validation (with logistic regression) ﬁnding that a
feature size of k = 20 (see Fig. 1) using F-score gives

us better performance than other values of k and
mutual-information-based criterion. This observa-
tion also applies to other of our models. Therefore,
we used this feature set as default for the rest of
project.

Fig. 1. Top 20 features with highest Fisher scores in feature
selection

Fig. 2. 2D histogram of all data with respect to top 2 features

III. Machine Learning Approaches

A. Linear Regression

First we used linear regression to get a quick
start. Linear regression represents a least-square ﬁt
of the response to the data. It chooses the hypoth-
esis

n(cid:88)

i=0

hθ(x) =

θixi

Predicting and Evaluating the Popularity of

Online News

He Ren

Department of Electrical Engineering

heren@stanford.edu

Quan Yang

Department of Electrical Engineering

quanyang@stanford.edu

Abstract—With the expansion of the Internet, more
and more people enjoys reading and sharing online
news articles. The number of shares under a news
article indicates how popular the news is. In this
project, we intend to ﬁnd the best model and set
of feature to predict the popularity of online news,
using machine learning techniques. Our data comes
from Mashable, a well-known online news website.
We implemented 10 diﬀerent learning algorithms on
the dataset, ranging from various regressions to SVM
and Random Forest. Their performances are recorded
and compared. Feature selection methods are used to
improve performance and reduce features. Random
Forest turns out to be the best model for prediction,
and it can achieve an accuracy of 70% with optimal
parameters. Our work can help online news companies
to predict news popularity before publication.

Keywords - Machine learning; Classiﬁcation; Popu-

larity prediction; Feature selection; Model selection

I.

Introduction

In this information era, reading and sharing
news have become the center of people’s entertain-
ment lives. Therefore, it would be greatly helpful if
we could accurately predict the popularity of news
prior to its publication, for social media workers
(authors, advertisers, etc). For the purpose of this
paper, we intend to make use of a largely and
recently collected dataset with over 39000 articles
from Mashable website, to ﬁrst select informative
features and then analyze and compare the perfor-
mance of several machine learning algorithms.

Some prediction approaches are based on ana-
lyzing early users’ comments [1], or features about
post contents and domains [2]. Another proposed
method [3] predicted the article’s popularity not
only based on its own appeal, but also other ar-
ticles that it is competing with. Prediction models
with SVMs, Ranking SVMs [3], Naive Bayes [2]

are investigated, and more advanced algorithms
such as Random Forest, Adaptive Boosting [4] could
increase the precision. This paper however, incorpo-
rates a broader and more abstracter set of features,
and starts with basic regression and classiﬁcation
models to advanced ones, with elaborations about
eﬀective feature selection.

This paper has the following structure. Section II
introduces our dataset and feature selection. Section
III gives our implementation of various learning
algorithms. We analyze the result and compare the
performances in Section IV. In Section V, we discuss
possible future work.

II. Dataset & Feature Selection

A. Data collection

Our dataset is provided by UCI machine learn-
ing repository [4], originally acquired and pre-
processed by K.Fernandes et al. It extracts 59 at-
tributes (as numerical values) describing diﬀerent
aspects of each article, from a total of 39644 articles
published in the last two years from Mashable
website. The full feature set is mainly categorized
as in Table 1.

B. Features

A full feature set may include much noise. We
ﬁrst attempted PCA for dimension reduction but it
did not provide any improvements for our models,
then we used ﬁlter methods (mutual information,
and Fisher criterion) for feature selection and im-
proved our prediction accuracy.

PCA is a commonly used dimensionality re-
duction algorithm, which could give us a lower-
dimensional approximation for original dataset
while preserving as much variability as possible.
However, the PCA results could only made our

TABLE I.

All Available Features

Features

Number of words of the title/content;

Average word length;

Rate of unique/non-stop words of contents

Aspects

Words

Links

Number of links;

Number of links to other articles in Mashable

Digital Media

Number of images/videos

Publication Time

Day of the week/weekend

Keywords

NLP

Number of keywords;

Worst/best/average keywords (#shares);

Article category

Closeness to ﬁve LDA topics;
Title/Text polarity/subjectivity;

Rate and polarity of positive/negative words;

Absolute subjectivity/polarity level

Target

Number of shares at Mashable

models perform worse. This is because the original
feature set is well-designed and correlated informa-
tion between features is limited.

Filter Methods

1) Mutual information: We calculate the mutual
information M I(xi, y) between features and class
labels to be the score to rank features, which can be
expressed as the Kullback-Leibler(KL) divergence:

M I(xi, y) = KL(p(xi, y)||p(xi)p(y))

2) Fisher criterion: Fisher criterion is another ef-
fective way in feature ranking. The Fish score (for
data with two classes) for jth feature is given by:

F (j) =

j − ¯x2
j )2
(¯x1
(s1
j )2 + (s2
j )2

where

(sk

j )2 =

(xj − ¯xk
j )2

(cid:88)

x∈X k

The numerator indicates the discrimination be-
tween popular and unpopular news, and the de-
nominator indicates the scatter within each class.
The larger the F-score is, the more likely this fea-
ture is more discriminative. Then we used crossed
validation (with logistic regression) ﬁnding that a
feature size of k = 20 (see Fig. 1) using F-score gives

us better performance than other values of k and
mutual-information-based criterion. This observa-
tion also applies to other of our models. Therefore,
we used this feature set as default for the rest of
project.

Fig. 1. Top 20 features with highest Fisher scores in feature
selection

Fig. 2. 2D histogram of all data with respect to top 2 features

III. Machine Learning Approaches

A. Linear Regression

First we used linear regression to get a quick
start. Linear regression represents a least-square ﬁt
of the response to the data. It chooses the hypoth-
esis

n(cid:88)

i=0

hθ(x) =

θixi

by minimizing the cost function

m(cid:88)

i=1

J(θ) =

1
2

(hθ(x(i)) − y(i))2

Due to the high variance of the target variable
(number of shares), direct application of linear re-
gression was not acceptable, speciﬁcally, on testing
samples, only 20% prediction values (number of
shares) are within 1000 of the actual results. We
discretized the target value to binary categories (as
in Table 2), and consider a prediction correct if
its value and the actual result have the same sign
(both + or -). It gave us 66% accuracy. Although
we applied the regression model on a classiﬁcation
problem, the result is quite desirable.

TABLE II.

Categories and classifications for linear rg.

and logistic rg.

Number of shares

0 − 1400 > 1400

Linear Rg. Categories

Logistic Rg. Classiﬁcation

-1
0

1
1

B. Logistic Regression

We then use classiﬁcation model trying to im-
prove our accuracy further. For logistic regression,
the hypothesis is

hθ(x) = g(θT x) =

1

1 + e−θT x

and parameters are chosen as to maximize their
likelihood

m(cid:89)

i=1

p(y(i)|x(i); θ)

We classiﬁed the data as in Table 2 and used
stochastic gradient ascent rule to implement it, and
we got similar result as linear regression model.

For multinomial classiﬁcations (say k classes),
the model uses logarithmic function to estimate the
relative probability of each category with reference
to the kth category. For example, for k = 3 (i. e.
we classify the data into 3 categories, “unpopular",
“popular", “very popular"), we comparing the fol-
lows:

(cid:32)
(cid:32)

log

log

P (y(i) = 1)
P (y(i) = 3)

P (y(i) = 2)
P (y(i) = 3)

(cid:33)
(cid:33)

n(cid:88)
n(cid:88)

k=1

k=1

=

=

βk,1x(i)
k−1

βk,2x(i)
k−1

and the prediction result is the class with the
largest probability. Both generalization and training
error increases with the increasing k, (e.g., for k =
3, logistic regression gives 51.6% accuracy). Since
we mainly focused on predicting whether a news
would be popular or not, we did not go further in
multiclass problems.

C. Support Vector Machine

We started SVM with linear kernel, which use

the following formula to make predictions:

(cid:32) m(cid:88)
αiy(i)(cid:68)
m(cid:88)

i=1

i=1

(cid:33)T
(cid:69)

x(i), x

+ b

αiy(i)x(i)

x + b

wT x + b =

=

In the equation, the kernel can be replaced with

more complex ones, as long as:

K(x, y) = φ(x)T φ(y)

TABLE III.

SVM Kernels we used

Kernel

Parameter

Expression

Linear

None

K(x, y) = xT y

Polynomial

Degree d

K(x, y) = (xT y + 1)d

Gaussian

σ

K(x, y) = exp(

−|x−y|2

2σ2

)

The kernels we used are in Table 3. The reason
we use diﬀerent kernels is because linear kernel
has high bias problem. Polynomial and gaussian
kernels can operate in a high-dimensional, implicit
feature space without computing the coordinates of
the data in that space. In this way, they can oﬀer
more ﬂexible decision boundaries.

D. Random Forest

In bagging (Bootstrap Aggregation), numerous
replicates of
the original dataset are created
to reduce the variance in prediction. Random
Forest use multiple decision trees which are built
on separate sets of examples drawn from the
dataset. In each tree, we can use a subset of all
the features we have. By using more decision
trees and averaging the result, the variance of the
model can be greatly lowered. Given a training set

Predicting and Evaluating the Popularity of

Online News

He Ren

Department of Electrical Engineering

heren@stanford.edu

Quan Yang

Department of Electrical Engineering

quanyang@stanford.edu

Abstract—With the expansion of the Internet, more
and more people enjoys reading and sharing online
news articles. The number of shares under a news
article indicates how popular the news is. In this
project, we intend to ﬁnd the best model and set
of feature to predict the popularity of online news,
using machine learning techniques. Our data comes
from Mashable, a well-known online news website.
We implemented 10 diﬀerent learning algorithms on
the dataset, ranging from various regressions to SVM
and Random Forest. Their performances are recorded
and compared. Feature selection methods are used to
improve performance and reduce features. Random
Forest turns out to be the best model for prediction,
and it can achieve an accuracy of 70% with optimal
parameters. Our work can help online news companies
to predict news popularity before publication.

Keywords - Machine learning; Classiﬁcation; Popu-

larity prediction; Feature selection; Model selection

I.

Introduction

In this information era, reading and sharing
news have become the center of people’s entertain-
ment lives. Therefore, it would be greatly helpful if
we could accurately predict the popularity of news
prior to its publication, for social media workers
(authors, advertisers, etc). For the purpose of this
paper, we intend to make use of a largely and
recently collected dataset with over 39000 articles
from Mashable website, to ﬁrst select informative
features and then analyze and compare the perfor-
mance of several machine learning algorithms.

Some prediction approaches are based on ana-
lyzing early users’ comments [1], or features about
post contents and domains [2]. Another proposed
method [3] predicted the article’s popularity not
only based on its own appeal, but also other ar-
ticles that it is competing with. Prediction models
with SVMs, Ranking SVMs [3], Naive Bayes [2]

are investigated, and more advanced algorithms
such as Random Forest, Adaptive Boosting [4] could
increase the precision. This paper however, incorpo-
rates a broader and more abstracter set of features,
and starts with basic regression and classiﬁcation
models to advanced ones, with elaborations about
eﬀective feature selection.

This paper has the following structure. Section II
introduces our dataset and feature selection. Section
III gives our implementation of various learning
algorithms. We analyze the result and compare the
performances in Section IV. In Section V, we discuss
possible future work.

II. Dataset & Feature Selection

A. Data collection

Our dataset is provided by UCI machine learn-
ing repository [4], originally acquired and pre-
processed by K.Fernandes et al. It extracts 59 at-
tributes (as numerical values) describing diﬀerent
aspects of each article, from a total of 39644 articles
published in the last two years from Mashable
website. The full feature set is mainly categorized
as in Table 1.

B. Features

A full feature set may include much noise. We
ﬁrst attempted PCA for dimension reduction but it
did not provide any improvements for our models,
then we used ﬁlter methods (mutual information,
and Fisher criterion) for feature selection and im-
proved our prediction accuracy.

PCA is a commonly used dimensionality re-
duction algorithm, which could give us a lower-
dimensional approximation for original dataset
while preserving as much variability as possible.
However, the PCA results could only made our

TABLE I.

All Available Features

Features

Number of words of the title/content;

Average word length;

Rate of unique/non-stop words of contents

Aspects

Words

Links

Number of links;

Number of links to other articles in Mashable

Digital Media

Number of images/videos

Publication Time

Day of the week/weekend

Keywords

NLP

Number of keywords;

Worst/best/average keywords (#shares);

Article category

Closeness to ﬁve LDA topics;
Title/Text polarity/subjectivity;

Rate and polarity of positive/negative words;

Absolute subjectivity/polarity level

Target

Number of shares at Mashable

models perform worse. This is because the original
feature set is well-designed and correlated informa-
tion between features is limited.

Filter Methods

1) Mutual information: We calculate the mutual
information M I(xi, y) between features and class
labels to be the score to rank features, which can be
expressed as the Kullback-Leibler(KL) divergence:

M I(xi, y) = KL(p(xi, y)||p(xi)p(y))

2) Fisher criterion: Fisher criterion is another ef-
fective way in feature ranking. The Fish score (for
data with two classes) for jth feature is given by:

F (j) =

j − ¯x2
j )2
(¯x1
(s1
j )2 + (s2
j )2

where

(sk

j )2 =

(xj − ¯xk
j )2

(cid:88)

x∈X k

The numerator indicates the discrimination be-
tween popular and unpopular news, and the de-
nominator indicates the scatter within each class.
The larger the F-score is, the more likely this fea-
ture is more discriminative. Then we used crossed
validation (with logistic regression) ﬁnding that a
feature size of k = 20 (see Fig. 1) using F-score gives

us better performance than other values of k and
mutual-information-based criterion. This observa-
tion also applies to other of our models. Therefore,
we used this feature set as default for the rest of
project.

Fig. 1. Top 20 features with highest Fisher scores in feature
selection

Fig. 2. 2D histogram of all data with respect to top 2 features

III. Machine Learning Approaches

A. Linear Regression

First we used linear regression to get a quick
start. Linear regression represents a least-square ﬁt
of the response to the data. It chooses the hypoth-
esis

n(cid:88)

i=0

hθ(x) =

θixi

by minimizing the cost function

m(cid:88)

i=1

J(θ) =

1
2

(hθ(x(i)) − y(i))2

Due to the high variance of the target variable
(number of shares), direct application of linear re-
gression was not acceptable, speciﬁcally, on testing
samples, only 20% prediction values (number of
shares) are within 1000 of the actual results. We
discretized the target value to binary categories (as
in Table 2), and consider a prediction correct if
its value and the actual result have the same sign
(both + or -). It gave us 66% accuracy. Although
we applied the regression model on a classiﬁcation
problem, the result is quite desirable.

TABLE II.

Categories and classifications for linear rg.

and logistic rg.

Number of shares

0 − 1400 > 1400

Linear Rg. Categories

Logistic Rg. Classiﬁcation

-1
0

1
1

B. Logistic Regression

We then use classiﬁcation model trying to im-
prove our accuracy further. For logistic regression,
the hypothesis is

hθ(x) = g(θT x) =

1

1 + e−θT x

and parameters are chosen as to maximize their
likelihood

m(cid:89)

i=1

p(y(i)|x(i); θ)

We classiﬁed the data as in Table 2 and used
stochastic gradient ascent rule to implement it, and
we got similar result as linear regression model.

For multinomial classiﬁcations (say k classes),
the model uses logarithmic function to estimate the
relative probability of each category with reference
to the kth category. For example, for k = 3 (i. e.
we classify the data into 3 categories, “unpopular",
“popular", “very popular"), we comparing the fol-
lows:

(cid:32)
(cid:32)

log

log

P (y(i) = 1)
P (y(i) = 3)

P (y(i) = 2)
P (y(i) = 3)

(cid:33)
(cid:33)

n(cid:88)
n(cid:88)

k=1

k=1

=

=

βk,1x(i)
k−1

βk,2x(i)
k−1

and the prediction result is the class with the
largest probability. Both generalization and training
error increases with the increasing k, (e.g., for k =
3, logistic regression gives 51.6% accuracy). Since
we mainly focused on predicting whether a news
would be popular or not, we did not go further in
multiclass problems.

C. Support Vector Machine

We started SVM with linear kernel, which use

the following formula to make predictions:

(cid:32) m(cid:88)
αiy(i)(cid:68)
m(cid:88)

i=1

i=1

(cid:33)T
(cid:69)

x(i), x

+ b

αiy(i)x(i)

x + b

wT x + b =

=

In the equation, the kernel can be replaced with

more complex ones, as long as:

K(x, y) = φ(x)T φ(y)

TABLE III.

SVM Kernels we used

Kernel

Parameter

Expression

Linear

None

K(x, y) = xT y

Polynomial

Degree d

K(x, y) = (xT y + 1)d

Gaussian

σ

K(x, y) = exp(

−|x−y|2

2σ2

)

The kernels we used are in Table 3. The reason
we use diﬀerent kernels is because linear kernel
has high bias problem. Polynomial and gaussian
kernels can operate in a high-dimensional, implicit
feature space without computing the coordinates of
the data in that space. In this way, they can oﬀer
more ﬂexible decision boundaries.

D. Random Forest

In bagging (Bootstrap Aggregation), numerous
replicates of
the original dataset are created
to reduce the variance in prediction. Random
Forest use multiple decision trees which are built
on separate sets of examples drawn from the
dataset. In each tree, we can use a subset of all
the features we have. By using more decision
trees and averaging the result, the variance of the
model can be greatly lowered. Given a training set

x(1), x(2),··· , x(n) with responses y(1), y(2),··· , y(n),
bagging repeatedly (B times) selects a random
sample with replacement of the training set and
ﬁts treees to these examples [6]:

• For b = 1,··· , B:
examples called Xb, Yb

1. Sample, with replacement, n training

2. Train a decision or regression tree fb on

Xb, Yb

• After training, predictions for unseen examples
x(cid:48) can be made by averaging the predictions from
all the individual regression trees on x(cid:48):

B(cid:88)

b=1

ˆf =

1
B

ˆfb(x(cid:48))

or by taking the majority vote in the case of
decision trees.

there are

For Random Forest,

two main
parameters to be considered: number of trees and
number of features they select at each decision
point. Theoretically, accuracy will
increase with
more trees making decision. We use cross validation
to see how the performance changes with these
parameters. We ensured that every value within
a certain range that our computer can support is
tested, and the result is plotted. In this case we
are able to see exactly the relationship between
performance and parameters.

IV. Results

In this project, we implemented 10 diﬀerent ma-
chine learning models. In this section, we apply 5-
fold cross validation to models and compare their
performances. Their accuracy and recall (sensitivity)
are listed in Table 4. We went deeper into how pa-
rameter aﬀects performance for SVM and Random
Forest, since they have more parameters to consider.
logistic regression
achieves a decent accuracy, better than most of the
models. Its Receiver Operating Characteristic Curve
is shown in Fig. 3. The AUC value is 0.74, which
means logistic regression gives fairly good result.

As a classiﬁcation model,

By observing training and test error, we saw
that SVM with linear kernel has high bias prob-
lem. Therefore we used more complex kernels and
trained SVM on full feature set to solve the problem
[5]. The result is shown in Table 5.

TABLE IV.

Performance of different algorithms

Algorithms

Accuracy

Recall

Linear Regression

Logistic Regression

SVM (d = 9 Poly Kernel)
Random Forest (500 Trees)
k-Nearest Neighbors (k = 5)

SVR (Linear Kernel)

REPTree

Kernel Partial Least Square

Kernel Perceptron (Max loop 100)

C4.5 Algorithm

0.66

0.66

0.55

0.69
0.56

0.52

0.67

0.58

0.45

0.58

0.67

0.70

0.45

0.71
0.47

0.59

0.62

0.60

0.99

0.59

Fig. 3.
bounds

ROC curve of logistic regression with conﬁdence

Even if we use high degree polynomial kernels,
high bias problem is slightly mitigated while accu-
racy seems to reach a bottleneck. As we plot the
training set on various combinations of features,
examples from two classes always mix together, and
it shows no potential boundary. We infer that the
data is not separable enough for SVM to handle,
even for extremely high degree polynomial kernels.

For SVM with 9-degree polynomial kernel,
which is empirically an optimal setting here for
SVM, Fig. 4 shows how test error and training
error change with increasing number of training

Predicting and Evaluating the Popularity of

Online News

He Ren

Department of Electrical Engineering

heren@stanford.edu

Quan Yang

Department of Electrical Engineering

quanyang@stanford.edu

Abstract—With the expansion of the Internet, more
and more people enjoys reading and sharing online
news articles. The number of shares under a news
article indicates how popular the news is. In this
project, we intend to ﬁnd the best model and set
of feature to predict the popularity of online news,
using machine learning techniques. Our data comes
from Mashable, a well-known online news website.
We implemented 10 diﬀerent learning algorithms on
the dataset, ranging from various regressions to SVM
and Random Forest. Their performances are recorded
and compared. Feature selection methods are used to
improve performance and reduce features. Random
Forest turns out to be the best model for prediction,
and it can achieve an accuracy of 70% with optimal
parameters. Our work can help online news companies
to predict news popularity before publication.

Keywords - Machine learning; Classiﬁcation; Popu-

larity prediction; Feature selection; Model selection

I.

Introduction

In this information era, reading and sharing
news have become the center of people’s entertain-
ment lives. Therefore, it would be greatly helpful if
we could accurately predict the popularity of news
prior to its publication, for social media workers
(authors, advertisers, etc). For the purpose of this
paper, we intend to make use of a largely and
recently collected dataset with over 39000 articles
from Mashable website, to ﬁrst select informative
features and then analyze and compare the perfor-
mance of several machine learning algorithms.

Some prediction approaches are based on ana-
lyzing early users’ comments [1], or features about
post contents and domains [2]. Another proposed
method [3] predicted the article’s popularity not
only based on its own appeal, but also other ar-
ticles that it is competing with. Prediction models
with SVMs, Ranking SVMs [3], Naive Bayes [2]

are investigated, and more advanced algorithms
such as Random Forest, Adaptive Boosting [4] could
increase the precision. This paper however, incorpo-
rates a broader and more abstracter set of features,
and starts with basic regression and classiﬁcation
models to advanced ones, with elaborations about
eﬀective feature selection.

This paper has the following structure. Section II
introduces our dataset and feature selection. Section
III gives our implementation of various learning
algorithms. We analyze the result and compare the
performances in Section IV. In Section V, we discuss
possible future work.

II. Dataset & Feature Selection

A. Data collection

Our dataset is provided by UCI machine learn-
ing repository [4], originally acquired and pre-
processed by K.Fernandes et al. It extracts 59 at-
tributes (as numerical values) describing diﬀerent
aspects of each article, from a total of 39644 articles
published in the last two years from Mashable
website. The full feature set is mainly categorized
as in Table 1.

B. Features

A full feature set may include much noise. We
ﬁrst attempted PCA for dimension reduction but it
did not provide any improvements for our models,
then we used ﬁlter methods (mutual information,
and Fisher criterion) for feature selection and im-
proved our prediction accuracy.

PCA is a commonly used dimensionality re-
duction algorithm, which could give us a lower-
dimensional approximation for original dataset
while preserving as much variability as possible.
However, the PCA results could only made our

TABLE I.

All Available Features

Features

Number of words of the title/content;

Average word length;

Rate of unique/non-stop words of contents

Aspects

Words

Links

Number of links;

Number of links to other articles in Mashable

Digital Media

Number of images/videos

Publication Time

Day of the week/weekend

Keywords

NLP

Number of keywords;

Worst/best/average keywords (#shares);

Article category

Closeness to ﬁve LDA topics;
Title/Text polarity/subjectivity;

Rate and polarity of positive/negative words;

Absolute subjectivity/polarity level

Target

Number of shares at Mashable

models perform worse. This is because the original
feature set is well-designed and correlated informa-
tion between features is limited.

Filter Methods

1) Mutual information: We calculate the mutual
information M I(xi, y) between features and class
labels to be the score to rank features, which can be
expressed as the Kullback-Leibler(KL) divergence:

M I(xi, y) = KL(p(xi, y)||p(xi)p(y))

2) Fisher criterion: Fisher criterion is another ef-
fective way in feature ranking. The Fish score (for
data with two classes) for jth feature is given by:

F (j) =

j − ¯x2
j )2
(¯x1
(s1
j )2 + (s2
j )2

where

(sk

j )2 =

(xj − ¯xk
j )2

(cid:88)

x∈X k

The numerator indicates the discrimination be-
tween popular and unpopular news, and the de-
nominator indicates the scatter within each class.
The larger the F-score is, the more likely this fea-
ture is more discriminative. Then we used crossed
validation (with logistic regression) ﬁnding that a
feature size of k = 20 (see Fig. 1) using F-score gives

us better performance than other values of k and
mutual-information-based criterion. This observa-
tion also applies to other of our models. Therefore,
we used this feature set as default for the rest of
project.

Fig. 1. Top 20 features with highest Fisher scores in feature
selection

Fig. 2. 2D histogram of all data with respect to top 2 features

III. Machine Learning Approaches

A. Linear Regression

First we used linear regression to get a quick
start. Linear regression represents a least-square ﬁt
of the response to the data. It chooses the hypoth-
esis

n(cid:88)

i=0

hθ(x) =

θixi

by minimizing the cost function

m(cid:88)

i=1

J(θ) =

1
2

(hθ(x(i)) − y(i))2

Due to the high variance of the target variable
(number of shares), direct application of linear re-
gression was not acceptable, speciﬁcally, on testing
samples, only 20% prediction values (number of
shares) are within 1000 of the actual results. We
discretized the target value to binary categories (as
in Table 2), and consider a prediction correct if
its value and the actual result have the same sign
(both + or -). It gave us 66% accuracy. Although
we applied the regression model on a classiﬁcation
problem, the result is quite desirable.

TABLE II.

Categories and classifications for linear rg.

and logistic rg.

Number of shares

0 − 1400 > 1400

Linear Rg. Categories

Logistic Rg. Classiﬁcation

-1
0

1
1

B. Logistic Regression

We then use classiﬁcation model trying to im-
prove our accuracy further. For logistic regression,
the hypothesis is

hθ(x) = g(θT x) =

1

1 + e−θT x

and parameters are chosen as to maximize their
likelihood

m(cid:89)

i=1

p(y(i)|x(i); θ)

We classiﬁed the data as in Table 2 and used
stochastic gradient ascent rule to implement it, and
we got similar result as linear regression model.

For multinomial classiﬁcations (say k classes),
the model uses logarithmic function to estimate the
relative probability of each category with reference
to the kth category. For example, for k = 3 (i. e.
we classify the data into 3 categories, “unpopular",
“popular", “very popular"), we comparing the fol-
lows:

(cid:32)
(cid:32)

log

log

P (y(i) = 1)
P (y(i) = 3)

P (y(i) = 2)
P (y(i) = 3)

(cid:33)
(cid:33)

n(cid:88)
n(cid:88)

k=1

k=1

=

=

βk,1x(i)
k−1

βk,2x(i)
k−1

and the prediction result is the class with the
largest probability. Both generalization and training
error increases with the increasing k, (e.g., for k =
3, logistic regression gives 51.6% accuracy). Since
we mainly focused on predicting whether a news
would be popular or not, we did not go further in
multiclass problems.

C. Support Vector Machine

We started SVM with linear kernel, which use

the following formula to make predictions:

(cid:32) m(cid:88)
αiy(i)(cid:68)
m(cid:88)

i=1

i=1

(cid:33)T
(cid:69)

x(i), x

+ b

αiy(i)x(i)

x + b

wT x + b =

=

In the equation, the kernel can be replaced with

more complex ones, as long as:

K(x, y) = φ(x)T φ(y)

TABLE III.

SVM Kernels we used

Kernel

Parameter

Expression

Linear

None

K(x, y) = xT y

Polynomial

Degree d

K(x, y) = (xT y + 1)d

Gaussian

σ

K(x, y) = exp(

−|x−y|2

2σ2

)

The kernels we used are in Table 3. The reason
we use diﬀerent kernels is because linear kernel
has high bias problem. Polynomial and gaussian
kernels can operate in a high-dimensional, implicit
feature space without computing the coordinates of
the data in that space. In this way, they can oﬀer
more ﬂexible decision boundaries.

D. Random Forest

In bagging (Bootstrap Aggregation), numerous
replicates of
the original dataset are created
to reduce the variance in prediction. Random
Forest use multiple decision trees which are built
on separate sets of examples drawn from the
dataset. In each tree, we can use a subset of all
the features we have. By using more decision
trees and averaging the result, the variance of the
model can be greatly lowered. Given a training set

x(1), x(2),··· , x(n) with responses y(1), y(2),··· , y(n),
bagging repeatedly (B times) selects a random
sample with replacement of the training set and
ﬁts treees to these examples [6]:

• For b = 1,··· , B:
examples called Xb, Yb

1. Sample, with replacement, n training

2. Train a decision or regression tree fb on

Xb, Yb

• After training, predictions for unseen examples
x(cid:48) can be made by averaging the predictions from
all the individual regression trees on x(cid:48):

B(cid:88)

b=1

ˆf =

1
B

ˆfb(x(cid:48))

or by taking the majority vote in the case of
decision trees.

there are

For Random Forest,

two main
parameters to be considered: number of trees and
number of features they select at each decision
point. Theoretically, accuracy will
increase with
more trees making decision. We use cross validation
to see how the performance changes with these
parameters. We ensured that every value within
a certain range that our computer can support is
tested, and the result is plotted. In this case we
are able to see exactly the relationship between
performance and parameters.

IV. Results

In this project, we implemented 10 diﬀerent ma-
chine learning models. In this section, we apply 5-
fold cross validation to models and compare their
performances. Their accuracy and recall (sensitivity)
are listed in Table 4. We went deeper into how pa-
rameter aﬀects performance for SVM and Random
Forest, since they have more parameters to consider.
logistic regression
achieves a decent accuracy, better than most of the
models. Its Receiver Operating Characteristic Curve
is shown in Fig. 3. The AUC value is 0.74, which
means logistic regression gives fairly good result.

As a classiﬁcation model,

By observing training and test error, we saw
that SVM with linear kernel has high bias prob-
lem. Therefore we used more complex kernels and
trained SVM on full feature set to solve the problem
[5]. The result is shown in Table 5.

TABLE IV.

Performance of different algorithms

Algorithms

Accuracy

Recall

Linear Regression

Logistic Regression

SVM (d = 9 Poly Kernel)
Random Forest (500 Trees)
k-Nearest Neighbors (k = 5)

SVR (Linear Kernel)

REPTree

Kernel Partial Least Square

Kernel Perceptron (Max loop 100)

C4.5 Algorithm

0.66

0.66

0.55

0.69
0.56

0.52

0.67

0.58

0.45

0.58

0.67

0.70

0.45

0.71
0.47

0.59

0.62

0.60

0.99

0.59

Fig. 3.
bounds

ROC curve of logistic regression with conﬁdence

Even if we use high degree polynomial kernels,
high bias problem is slightly mitigated while accu-
racy seems to reach a bottleneck. As we plot the
training set on various combinations of features,
examples from two classes always mix together, and
it shows no potential boundary. We infer that the
data is not separable enough for SVM to handle,
even for extremely high degree polynomial kernels.

For SVM with 9-degree polynomial kernel,
which is empirically an optimal setting here for
SVM, Fig. 4 shows how test error and training
error change with increasing number of training

TABLE V.

SVM Results with various kernels

Kernel

Linear

Poly (d = 7)

Poly (d = 20) Gaussian

Test Error

Training Error

0.48
0.45

0.47
0.45

0.45
0.45

0.46
0.47

Fig. 4. SVM error with increasing number of training examples

examples. This is the best result SVM can give, and
its accuracy doesn’t improve with more training
examples.

Random Forest has the best result

for this
classiﬁcation problem. It can have diﬀerent number
of decision trees and diﬀerent number of features
used for each decision point. The number of
training examples can also change. Therefore,
implementation should be done in a systematic
way. We change only one variable at a time. We
ﬁrst use a default setting for Random Forest and
increase the number of
training examples. The
error decreases to a certain level as shown in Fig.
5 (left). Then we set the number of trees to be
constant, and change the number of features used
for decision. It turns out that log(Nmax) is the
best value. Finally, we change the number of trees
continuously from 5 to 500, and plot the result in
Fig. 5 (right). The accuracy reaches a limit of 69%,
which is the best among all algorithms.

V. Future Work

As is seen from the result, no algorithm can
reach 70% accuracy given the data set we have,
even though they are state-of-the-art. To improve
accuracy, there is little room in model selection but
much room in feature selection. In the preprocess-

Fig. 5.
Random Forest training and test error with respect
to number of training examples (left) and number of decision
trees (right)

ing round, 59 features were extracted from news
articles, and our later work is based on these fea-
tures. However, the content of news articles hasn’t
been fully explored. Some features are related to
the content, such as LDA topics (feature #39 -
#43), which are convenient to use for learning, but
reﬂect only a small portion of information about the
content.

In the future, we could directly treat all the
words in an article as additional features, and then
apply machine learning algorithms like Naive Bayes
and SVM. In this way, what the article really talks
about is taken into account, and this approach
should improve the accuracy of prediction if com-
bined with our current work.

References

[1] Tatar, Alexandru, et al. "Predicting the popularity of online
articles based on user comments." Proceedings of the
International Conference on Web Intelligence, Mining and
Semantics. ACM, 2011.
"Predicting the Popularity of Social News Posts." 2013
cs229 projects. Joe Maguire Scott Michelson.

[2]

[3] Hensinger, Elena, Ilias Flaounas, and Nello Cristianini.
"Modelling and predicting news popularity." Pattern Anal-
ysis and Applications 16.4 (2013): 623-635.

[4] K. Fernandes, P. Vinagre and P. Cortez. A Proactive In-
telligent Decision Support System for Predicting the Pop-
ularity of Online News. Proceedings of the 17th EPIA 2015
- Portuguese Conference on Artiﬁcial Intelligence, September,
Coimbra, Portugal.

[5] Chang, Chih-Chung, and Chih-Jen Lin. "LIBSVM: A li-
brary for support vector machines." ACM Transactions on
Intelligent Systems and Technology (TIST) 2.3 (2011): 27.
James, Gareth, et al. An introduction to statistical learning.
New York: springer, 2013.

[6]

