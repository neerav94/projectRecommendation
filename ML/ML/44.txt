Machine Learning in Automatic Music

Chords Generation

Ziheng Chen

Department of Music
zihengc@stanford.edu

Jie Qi

Department of Electrical Engineering

qijie@stanford.edu

Yifei Zhou

Department of Statistics
yfzhou@stanford.edu

I. INTRODUCTION

III. DATASET

Melodies and chords compose a basic music piece.
Assigning chords for a music melody is an important step
in music composition. This project is to apply machine
learning techniques to assign chords to follow several
measures of melody and generate pleasant music pieces.
In real music chord assignment process, to choose
which chord to use for a measure, musicians normally
consider the notes in this measure and how chords
are progressed. So our aim is to learn the relationship
between notes and chords, as well as the relationship
between adjacent chords, and use that to assign chords
for a new melody.

The input to our algorithm is a music piece with
several measures. We then try different models to out-
put a predicted chord for each measure. We will use
basic models taught in the class (Logistic Regression,
Naive Bayes, Support Vector Machine) as well as some
advanced models (Random Forest, Boosting, Hidden
Markov Model), and compare their performance for our
problem.

II. RELATED WORK

There have been some previous works using machine
learning techniques to generate music chords. Cunha and
Ramalho[1] set a neural network and combined it with
a rule-based approach to select accompanying chords
for a melody, while Legaspi et al.[2] use a genetic
algorithm to build chords. Chuan and Chew[3] use a
data-driven HMM combined with a series of musical
rules to generate chord progression. Simon et al.[4] use
HMM and 60 types of chords to made an interactive
product, which would generate chord accompaniment for
human voice input. Paiement et al.[5] use a multilevel
graphical model to generate chord progressions for a
given melody.

A. Data Source

We collected 43 lead sheets as our dataset. The chosen
lead sheets have several properties: a) virtually each
measure in the dataset has and has only one chord; b) the
chords in the dataset are mainly scale tone chords, which
are the most basic and commonly used chords in music
pieces. The data we use are in MusicXML format, which
is a digital sheet music format for common Western
music notation. Measure information can be extracted
from this format by MATLAB.

B. Data Preprocessing

To simplify our further analysis, we did some prepro-

cessing of the dataset before training:

1) The key of each song is shifted to key C. The
key of a song can determine the note and a set of
common chord types the song uses. A song written
in one key can be easily shifted to another key
by simply increasing or decreasing all the pitches
in notes and chords equally, without affecting its
subjective character. Therefore, without any loss
of music information, we can shift the key of each
song to key C to make the dataset more organized
as well as decrease the number of class types
(chord types) in training process.

2) The chord types (class types) are restricted only
to scale tone chords in key C, which are 7 types:
C major, D minor, E minor, F major, G major, A
minor, and G dominant. Other types of chords (like
C Dominant, G suspended, etc.) are transformed to
their most similar scale tone chords.

3) Some measures in the dataset have no chords or no
notes (like rest measure). We simply delete these
measures from the dataset.

4) A small number of measures in our dataset are
assigned two chords continuously to accompany

Machine Learning in Automatic Music

Chords Generation

Ziheng Chen

Department of Music
zihengc@stanford.edu

Jie Qi

Department of Electrical Engineering

qijie@stanford.edu

Yifei Zhou

Department of Statistics
yfzhou@stanford.edu

I. INTRODUCTION

III. DATASET

Melodies and chords compose a basic music piece.
Assigning chords for a music melody is an important step
in music composition. This project is to apply machine
learning techniques to assign chords to follow several
measures of melody and generate pleasant music pieces.
In real music chord assignment process, to choose
which chord to use for a measure, musicians normally
consider the notes in this measure and how chords
are progressed. So our aim is to learn the relationship
between notes and chords, as well as the relationship
between adjacent chords, and use that to assign chords
for a new melody.

The input to our algorithm is a music piece with
several measures. We then try different models to out-
put a predicted chord for each measure. We will use
basic models taught in the class (Logistic Regression,
Naive Bayes, Support Vector Machine) as well as some
advanced models (Random Forest, Boosting, Hidden
Markov Model), and compare their performance for our
problem.

II. RELATED WORK

There have been some previous works using machine
learning techniques to generate music chords. Cunha and
Ramalho[1] set a neural network and combined it with
a rule-based approach to select accompanying chords
for a melody, while Legaspi et al.[2] use a genetic
algorithm to build chords. Chuan and Chew[3] use a
data-driven HMM combined with a series of musical
rules to generate chord progression. Simon et al.[4] use
HMM and 60 types of chords to made an interactive
product, which would generate chord accompaniment for
human voice input. Paiement et al.[5] use a multilevel
graphical model to generate chord progressions for a
given melody.

A. Data Source

We collected 43 lead sheets as our dataset. The chosen
lead sheets have several properties: a) virtually each
measure in the dataset has and has only one chord; b) the
chords in the dataset are mainly scale tone chords, which
are the most basic and commonly used chords in music
pieces. The data we use are in MusicXML format, which
is a digital sheet music format for common Western
music notation. Measure information can be extracted
from this format by MATLAB.

B. Data Preprocessing

To simplify our further analysis, we did some prepro-

cessing of the dataset before training:

1) The key of each song is shifted to key C. The
key of a song can determine the note and a set of
common chord types the song uses. A song written
in one key can be easily shifted to another key
by simply increasing or decreasing all the pitches
in notes and chords equally, without affecting its
subjective character. Therefore, without any loss
of music information, we can shift the key of each
song to key C to make the dataset more organized
as well as decrease the number of class types
(chord types) in training process.

2) The chord types (class types) are restricted only
to scale tone chords in key C, which are 7 types:
C major, D minor, E minor, F major, G major, A
minor, and G dominant. Other types of chords (like
C Dominant, G suspended, etc.) are transformed to
their most similar scale tone chords.

3) Some measures in the dataset have no chords or no
notes (like rest measure). We simply delete these
measures from the dataset.

4) A small number of measures in our dataset are
assigned two chords continuously to accompany

different notes in the measure. To simplify the
learning process, we regard the second chord as
the chord of this measure and delete the ﬁrst one.
From an audiences perspective, it will sound better
than deleting the second chord.

After preprocessing, the 43 lead sheets we chose have

813 measures in total.

The 7 chord labels are shown in TABLE I. They are
the labels we are trying to assign. Now, our project turns
into a multi-class classiﬁcation problem.

Label
Chord

2

1
7
C Dm Em F G Am G7

3

4

5

6

TABLE I: Labels for different chord types

IV. FEATURE SELECTION

We started by extracting some indicative features. The
basic units for prediction are the notes inside a measure.
Then we chose our initial features from the following
three aspects:

1) Note itself: whether a note is present in the mea-
sure; take value of 0 or 1. A chord is inﬂuenced by
the notes that appear to create a sense of harmony.
2) Note vs. Beat: the notes on the beat in the measure

since chords should accompany the beat tones.

3) Note vs. Duration: the longest notes in the measure
since the long notes need to be satisﬁed by the
assigned chords.

To represent a note in a measure, we quantiﬁed it as
shown in TABLE II. It is labeled 1 to 12. Note that
we don’t consider which octave the music note lies in,
since the octave basically will not affect the chord type
we choose (E.g. C4 and C5 are both labeled 1).

Label
Note
Label
Note

1
C
7

F#/Gb

2

C#/Db

8
G

3
D
9

G#/Ab

4

D#/Eb

10
A

5
E
11

A#/Bb

6
F
12
B

TABLE II: Labels for different note types

We chose notes on the 4/4 beats and 2 longest notes.
In addition to the 12 type (1) features, we initially have
18 features.

To get the true effective features, we ran Forward
Feature Selection based on Random Forest (which will
be discussed further in the next section), by adding one
feature at a time and selecting the current optimal one.
The result is shown in Figure 1.

Fig. 1: Forward feature selection result

We can see the accuracy does not improve much after
6 features. Finally, we selected the next 6 features in our
following analysis: the note pitches on the 4 beats and
the 2 longest note pitches.

V. MODELS

A. Logistic Regression

We used multinomial logistic regression, i.e., softmax

regression in R, with the log-likelihood as

m(cid:88)

(cid:32)

k(cid:89)

(cid:96)(θ) =

log

i=1

l=1

(cid:80)k

l x(i)

eθT
j=1 eθT

l x(i)

(cid:33)1{y(i)=l}

(1)

We could achieve the maximum likelihood estimate

by using Newton’s method:

θ := θ − H−1∇θ(cid:96)(θ)

(2)

since our feature size is small (n = 6) and it is easy to
compute the inverse of the Hessian.

B. Naive Bayes

We applied Naive Bayes by using the Statistics and
Machine Learning Toolbox in MATLAB. We used the
multinomial event model with multiple classes. For any
class c, the maximum likelihood estimate gives
j = k ∧ y(i) = c}

(cid:80)m

i=1

φk|y=c =

(cid:80)ni
(cid:80)m
j=1 1{x(i)
i=1 1{y(i) = c}ni
(cid:80)m
i=1 1{y(i) = c}

(3)

(4)

φy=c =

m

We then made the prediction on the posterior com-

puted with the probabilities above.

Machine Learning in Automatic Music

Chords Generation

Ziheng Chen

Department of Music
zihengc@stanford.edu

Jie Qi

Department of Electrical Engineering

qijie@stanford.edu

Yifei Zhou

Department of Statistics
yfzhou@stanford.edu

I. INTRODUCTION

III. DATASET

Melodies and chords compose a basic music piece.
Assigning chords for a music melody is an important step
in music composition. This project is to apply machine
learning techniques to assign chords to follow several
measures of melody and generate pleasant music pieces.
In real music chord assignment process, to choose
which chord to use for a measure, musicians normally
consider the notes in this measure and how chords
are progressed. So our aim is to learn the relationship
between notes and chords, as well as the relationship
between adjacent chords, and use that to assign chords
for a new melody.

The input to our algorithm is a music piece with
several measures. We then try different models to out-
put a predicted chord for each measure. We will use
basic models taught in the class (Logistic Regression,
Naive Bayes, Support Vector Machine) as well as some
advanced models (Random Forest, Boosting, Hidden
Markov Model), and compare their performance for our
problem.

II. RELATED WORK

There have been some previous works using machine
learning techniques to generate music chords. Cunha and
Ramalho[1] set a neural network and combined it with
a rule-based approach to select accompanying chords
for a melody, while Legaspi et al.[2] use a genetic
algorithm to build chords. Chuan and Chew[3] use a
data-driven HMM combined with a series of musical
rules to generate chord progression. Simon et al.[4] use
HMM and 60 types of chords to made an interactive
product, which would generate chord accompaniment for
human voice input. Paiement et al.[5] use a multilevel
graphical model to generate chord progressions for a
given melody.

A. Data Source

We collected 43 lead sheets as our dataset. The chosen
lead sheets have several properties: a) virtually each
measure in the dataset has and has only one chord; b) the
chords in the dataset are mainly scale tone chords, which
are the most basic and commonly used chords in music
pieces. The data we use are in MusicXML format, which
is a digital sheet music format for common Western
music notation. Measure information can be extracted
from this format by MATLAB.

B. Data Preprocessing

To simplify our further analysis, we did some prepro-

cessing of the dataset before training:

1) The key of each song is shifted to key C. The
key of a song can determine the note and a set of
common chord types the song uses. A song written
in one key can be easily shifted to another key
by simply increasing or decreasing all the pitches
in notes and chords equally, without affecting its
subjective character. Therefore, without any loss
of music information, we can shift the key of each
song to key C to make the dataset more organized
as well as decrease the number of class types
(chord types) in training process.

2) The chord types (class types) are restricted only
to scale tone chords in key C, which are 7 types:
C major, D minor, E minor, F major, G major, A
minor, and G dominant. Other types of chords (like
C Dominant, G suspended, etc.) are transformed to
their most similar scale tone chords.

3) Some measures in the dataset have no chords or no
notes (like rest measure). We simply delete these
measures from the dataset.

4) A small number of measures in our dataset are
assigned two chords continuously to accompany

different notes in the measure. To simplify the
learning process, we regard the second chord as
the chord of this measure and delete the ﬁrst one.
From an audiences perspective, it will sound better
than deleting the second chord.

After preprocessing, the 43 lead sheets we chose have

813 measures in total.

The 7 chord labels are shown in TABLE I. They are
the labels we are trying to assign. Now, our project turns
into a multi-class classiﬁcation problem.

Label
Chord

2

1
7
C Dm Em F G Am G7

3

4

5

6

TABLE I: Labels for different chord types

IV. FEATURE SELECTION

We started by extracting some indicative features. The
basic units for prediction are the notes inside a measure.
Then we chose our initial features from the following
three aspects:

1) Note itself: whether a note is present in the mea-
sure; take value of 0 or 1. A chord is inﬂuenced by
the notes that appear to create a sense of harmony.
2) Note vs. Beat: the notes on the beat in the measure

since chords should accompany the beat tones.

3) Note vs. Duration: the longest notes in the measure
since the long notes need to be satisﬁed by the
assigned chords.

To represent a note in a measure, we quantiﬁed it as
shown in TABLE II. It is labeled 1 to 12. Note that
we don’t consider which octave the music note lies in,
since the octave basically will not affect the chord type
we choose (E.g. C4 and C5 are both labeled 1).

Label
Note
Label
Note

1
C
7

F#/Gb

2

C#/Db

8
G

3
D
9

G#/Ab

4

D#/Eb

10
A

5
E
11

A#/Bb

6
F
12
B

TABLE II: Labels for different note types

We chose notes on the 4/4 beats and 2 longest notes.
In addition to the 12 type (1) features, we initially have
18 features.

To get the true effective features, we ran Forward
Feature Selection based on Random Forest (which will
be discussed further in the next section), by adding one
feature at a time and selecting the current optimal one.
The result is shown in Figure 1.

Fig. 1: Forward feature selection result

We can see the accuracy does not improve much after
6 features. Finally, we selected the next 6 features in our
following analysis: the note pitches on the 4 beats and
the 2 longest note pitches.

V. MODELS

A. Logistic Regression

We used multinomial logistic regression, i.e., softmax

regression in R, with the log-likelihood as

m(cid:88)

(cid:32)

k(cid:89)

(cid:96)(θ) =

log

i=1

l=1

(cid:80)k

l x(i)

eθT
j=1 eθT

l x(i)

(cid:33)1{y(i)=l}

(1)

We could achieve the maximum likelihood estimate

by using Newton’s method:

θ := θ − H−1∇θ(cid:96)(θ)

(2)

since our feature size is small (n = 6) and it is easy to
compute the inverse of the Hessian.

B. Naive Bayes

We applied Naive Bayes by using the Statistics and
Machine Learning Toolbox in MATLAB. We used the
multinomial event model with multiple classes. For any
class c, the maximum likelihood estimate gives
j = k ∧ y(i) = c}

(cid:80)m

i=1

φk|y=c =

(cid:80)ni
(cid:80)m
j=1 1{x(i)
i=1 1{y(i) = c}ni
(cid:80)m
i=1 1{y(i) = c}

(3)

(4)

φy=c =

m

We then made the prediction on the posterior com-

puted with the probabilities above.

C. Support Vector Machine (SVM)

E. Boosting

SVM is one of the supervised learning algorithms
which is to maximize the distance between training
example and classiﬁcation hyperplane by ﬁnding (use
hard-margin as an example):

min
γ,w,b

||w||2
1
2
s.t. y(i)(wT x(i) + b) ≥ 1, i = 1, ..., m

(5)

By using kernel
trick, SVM can efﬁciently perform
non-linear classiﬁcation or data with high-dimensional
features. In this problem, we tried the following three
kernels in R:

1) Linear kernel

K(xi, xj) =

2) Polynomial kernel

(cid:32)

p(cid:88)

k=1

K(xi, xj) =

1 +

xipxjp

(cid:33)d

p(cid:88)

k=1

xipxjp

3) Radial kernel

K(xi, xj) = e−γ(cid:80)p

k=1(xip−xjp)2

D. Random Forest

Random forest is based on bagging which is a kind
of decision tree with bootstrapping and can decrease
variance. For a classiﬁcation problems with p features,
√
p features are used in each split in order to decrease
the correlation of the trees[6]. We applied this model
using R package.

After applying the model, we can generate the impor-
tance plot for the features as the ﬁgure below. All the
features we are using have the similar importance level,
which reinforces the conclusion from feature selection.

Fig. 2: Variable importance

(6)

(7)

(8)

Boosting is a meta-algorithm to learn slowly to ﬁt
the new model from the residual of the current model.
Its parameters include the number of trees to split, the
shrinkage parameter and the depth of the tree[6]. We also
applied this model in R.

Speciﬁcally, we tune these parameters using cross vali-
dation to make sure the model has the best performance.
As a result, the number of trees is 200, the shrinkage
parameter is 0.2 and the depth is 4.

F. Hidden Markov Model (HMM)

Up till now, these ﬁve models above make prediction
based on the information of a single measure. We wanted
to incorporate the relationship between measures. So, we
also tried HMM to make prediction based on a sequence
of measures.
In HMM,

the system is being modeled to be a
Markov process which has a series of observed states
x = {x1, x2, ..., xT} and a series of related unobserved
(hidden) states z = {z1, z2, ..., zT} (T is the number
of the states). Suppose there are S types of observed
states and Z types of hidden states. An S × S Transi-
tion Matrix denotes the transition probabilities between
adjacent hidden states. And an S × Z Emission Matrix
denotes the probabilities of each hidden state emit each
observed state. Given an observed series of outputs, if
we know the Transition Matrix and Emission Matrix,
we can compute the most likely hidden series using the
Viterbi Algorithm[7].

In our chord assignment problem, the ﬁrst note of each
measure was considered as an observed state, and the
chord of each measure was considered as a hidden state.
Using our dataset, we computed the transition probability
and emission probability and formed transition matrix
and emission matrix. And then we computed the most
likely chord progression using hmmviterbi function in
Statistics and Machine Learning Toolbox in MATLAB.

VI. RESULTS & ANALYSIS

A. Cross Validation

We started at comparing different machine learning
models to indicate how they perform. We used hold-out
cross validation and 30% of the data as the validation
set. In addition, we also tried k-fold cross validation.
The cross validation results are used to represent the test
accuracy and evaluate how our models perform.

Machine Learning in Automatic Music

Chords Generation

Ziheng Chen

Department of Music
zihengc@stanford.edu

Jie Qi

Department of Electrical Engineering

qijie@stanford.edu

Yifei Zhou

Department of Statistics
yfzhou@stanford.edu

I. INTRODUCTION

III. DATASET

Melodies and chords compose a basic music piece.
Assigning chords for a music melody is an important step
in music composition. This project is to apply machine
learning techniques to assign chords to follow several
measures of melody and generate pleasant music pieces.
In real music chord assignment process, to choose
which chord to use for a measure, musicians normally
consider the notes in this measure and how chords
are progressed. So our aim is to learn the relationship
between notes and chords, as well as the relationship
between adjacent chords, and use that to assign chords
for a new melody.

The input to our algorithm is a music piece with
several measures. We then try different models to out-
put a predicted chord for each measure. We will use
basic models taught in the class (Logistic Regression,
Naive Bayes, Support Vector Machine) as well as some
advanced models (Random Forest, Boosting, Hidden
Markov Model), and compare their performance for our
problem.

II. RELATED WORK

There have been some previous works using machine
learning techniques to generate music chords. Cunha and
Ramalho[1] set a neural network and combined it with
a rule-based approach to select accompanying chords
for a melody, while Legaspi et al.[2] use a genetic
algorithm to build chords. Chuan and Chew[3] use a
data-driven HMM combined with a series of musical
rules to generate chord progression. Simon et al.[4] use
HMM and 60 types of chords to made an interactive
product, which would generate chord accompaniment for
human voice input. Paiement et al.[5] use a multilevel
graphical model to generate chord progressions for a
given melody.

A. Data Source

We collected 43 lead sheets as our dataset. The chosen
lead sheets have several properties: a) virtually each
measure in the dataset has and has only one chord; b) the
chords in the dataset are mainly scale tone chords, which
are the most basic and commonly used chords in music
pieces. The data we use are in MusicXML format, which
is a digital sheet music format for common Western
music notation. Measure information can be extracted
from this format by MATLAB.

B. Data Preprocessing

To simplify our further analysis, we did some prepro-

cessing of the dataset before training:

1) The key of each song is shifted to key C. The
key of a song can determine the note and a set of
common chord types the song uses. A song written
in one key can be easily shifted to another key
by simply increasing or decreasing all the pitches
in notes and chords equally, without affecting its
subjective character. Therefore, without any loss
of music information, we can shift the key of each
song to key C to make the dataset more organized
as well as decrease the number of class types
(chord types) in training process.

2) The chord types (class types) are restricted only
to scale tone chords in key C, which are 7 types:
C major, D minor, E minor, F major, G major, A
minor, and G dominant. Other types of chords (like
C Dominant, G suspended, etc.) are transformed to
their most similar scale tone chords.

3) Some measures in the dataset have no chords or no
notes (like rest measure). We simply delete these
measures from the dataset.

4) A small number of measures in our dataset are
assigned two chords continuously to accompany

different notes in the measure. To simplify the
learning process, we regard the second chord as
the chord of this measure and delete the ﬁrst one.
From an audiences perspective, it will sound better
than deleting the second chord.

After preprocessing, the 43 lead sheets we chose have

813 measures in total.

The 7 chord labels are shown in TABLE I. They are
the labels we are trying to assign. Now, our project turns
into a multi-class classiﬁcation problem.

Label
Chord

2

1
7
C Dm Em F G Am G7

3

4

5

6

TABLE I: Labels for different chord types

IV. FEATURE SELECTION

We started by extracting some indicative features. The
basic units for prediction are the notes inside a measure.
Then we chose our initial features from the following
three aspects:

1) Note itself: whether a note is present in the mea-
sure; take value of 0 or 1. A chord is inﬂuenced by
the notes that appear to create a sense of harmony.
2) Note vs. Beat: the notes on the beat in the measure

since chords should accompany the beat tones.

3) Note vs. Duration: the longest notes in the measure
since the long notes need to be satisﬁed by the
assigned chords.

To represent a note in a measure, we quantiﬁed it as
shown in TABLE II. It is labeled 1 to 12. Note that
we don’t consider which octave the music note lies in,
since the octave basically will not affect the chord type
we choose (E.g. C4 and C5 are both labeled 1).

Label
Note
Label
Note

1
C
7

F#/Gb

2

C#/Db

8
G

3
D
9

G#/Ab

4

D#/Eb

10
A

5
E
11

A#/Bb

6
F
12
B

TABLE II: Labels for different note types

We chose notes on the 4/4 beats and 2 longest notes.
In addition to the 12 type (1) features, we initially have
18 features.

To get the true effective features, we ran Forward
Feature Selection based on Random Forest (which will
be discussed further in the next section), by adding one
feature at a time and selecting the current optimal one.
The result is shown in Figure 1.

Fig. 1: Forward feature selection result

We can see the accuracy does not improve much after
6 features. Finally, we selected the next 6 features in our
following analysis: the note pitches on the 4 beats and
the 2 longest note pitches.

V. MODELS

A. Logistic Regression

We used multinomial logistic regression, i.e., softmax

regression in R, with the log-likelihood as

m(cid:88)

(cid:32)

k(cid:89)

(cid:96)(θ) =

log

i=1

l=1

(cid:80)k

l x(i)

eθT
j=1 eθT

l x(i)

(cid:33)1{y(i)=l}

(1)

We could achieve the maximum likelihood estimate

by using Newton’s method:

θ := θ − H−1∇θ(cid:96)(θ)

(2)

since our feature size is small (n = 6) and it is easy to
compute the inverse of the Hessian.

B. Naive Bayes

We applied Naive Bayes by using the Statistics and
Machine Learning Toolbox in MATLAB. We used the
multinomial event model with multiple classes. For any
class c, the maximum likelihood estimate gives
j = k ∧ y(i) = c}

(cid:80)m

i=1

φk|y=c =

(cid:80)ni
(cid:80)m
j=1 1{x(i)
i=1 1{y(i) = c}ni
(cid:80)m
i=1 1{y(i) = c}

(3)

(4)

φy=c =

m

We then made the prediction on the posterior com-

puted with the probabilities above.

C. Support Vector Machine (SVM)

E. Boosting

SVM is one of the supervised learning algorithms
which is to maximize the distance between training
example and classiﬁcation hyperplane by ﬁnding (use
hard-margin as an example):

min
γ,w,b

||w||2
1
2
s.t. y(i)(wT x(i) + b) ≥ 1, i = 1, ..., m

(5)

By using kernel
trick, SVM can efﬁciently perform
non-linear classiﬁcation or data with high-dimensional
features. In this problem, we tried the following three
kernels in R:

1) Linear kernel

K(xi, xj) =

2) Polynomial kernel

(cid:32)

p(cid:88)

k=1

K(xi, xj) =

1 +

xipxjp

(cid:33)d

p(cid:88)

k=1

xipxjp

3) Radial kernel

K(xi, xj) = e−γ(cid:80)p

k=1(xip−xjp)2

D. Random Forest

Random forest is based on bagging which is a kind
of decision tree with bootstrapping and can decrease
variance. For a classiﬁcation problems with p features,
√
p features are used in each split in order to decrease
the correlation of the trees[6]. We applied this model
using R package.

After applying the model, we can generate the impor-
tance plot for the features as the ﬁgure below. All the
features we are using have the similar importance level,
which reinforces the conclusion from feature selection.

Fig. 2: Variable importance

(6)

(7)

(8)

Boosting is a meta-algorithm to learn slowly to ﬁt
the new model from the residual of the current model.
Its parameters include the number of trees to split, the
shrinkage parameter and the depth of the tree[6]. We also
applied this model in R.

Speciﬁcally, we tune these parameters using cross vali-
dation to make sure the model has the best performance.
As a result, the number of trees is 200, the shrinkage
parameter is 0.2 and the depth is 4.

F. Hidden Markov Model (HMM)

Up till now, these ﬁve models above make prediction
based on the information of a single measure. We wanted
to incorporate the relationship between measures. So, we
also tried HMM to make prediction based on a sequence
of measures.
In HMM,

the system is being modeled to be a
Markov process which has a series of observed states
x = {x1, x2, ..., xT} and a series of related unobserved
(hidden) states z = {z1, z2, ..., zT} (T is the number
of the states). Suppose there are S types of observed
states and Z types of hidden states. An S × S Transi-
tion Matrix denotes the transition probabilities between
adjacent hidden states. And an S × Z Emission Matrix
denotes the probabilities of each hidden state emit each
observed state. Given an observed series of outputs, if
we know the Transition Matrix and Emission Matrix,
we can compute the most likely hidden series using the
Viterbi Algorithm[7].

In our chord assignment problem, the ﬁrst note of each
measure was considered as an observed state, and the
chord of each measure was considered as a hidden state.
Using our dataset, we computed the transition probability
and emission probability and formed transition matrix
and emission matrix. And then we computed the most
likely chord progression using hmmviterbi function in
Statistics and Machine Learning Toolbox in MATLAB.

VI. RESULTS & ANALYSIS

A. Cross Validation

We started at comparing different machine learning
models to indicate how they perform. We used hold-out
cross validation and 30% of the data as the validation
set. In addition, we also tried k-fold cross validation.
The cross validation results are used to represent the test
accuracy and evaluate how our models perform.

B. Prediction on a Single Measure

The cross validation accuracy for the ﬁve different
models we used is shown in TABLE III. (k = 5, 10
for k-fold)

Model

Logistic Regression

Naive Bayes
SVM Linear
SVM Poly
SVM Radial
Random Forest

Boosting

70%/30% 10-fold
43.75%
53.57%
48.21%
56.25%
65.17%
62.50%
63.39%

5-fold
39.15% 38.42%
55.34% 53.67%
36.97% 38.35%
52.80% 51.63%
61.39% 63.13%
64.06% 63.73%
64.99% 62.29%

TABLE III: Accuracy for different models

bias/low variance classiﬁers (e.g., Naive Bayes) could
have a better performance. However, Naive Bayes also
assumes that the features are conditionally independent.
In reality, the four notes on the beat could inﬂuence each
other, so such strong assumption could limit the per-
formance of Naive Bayes. Random forest and Boosting
perform best in our case since they are both complex
and have a reduction in model variance.
C. Prediction on Sequential Measures

The following is the song (cid:109)“silent night ” that is

assigned chords by our HMM model.

To get an insight of the results above, we can have
a look at the confusion matrix. Take SVM with radial
kernel as an example:



6
0
0

1 6 2
45 0 1
0 0 0
5 0
0
0 0
0
0 0 0
2 0 13 1 3 1
5
4 0 0
1 1
2
3 3 1
0 0
1
0
0 0
0 0 3



(9)

1
1
0

In the confusion matrix, the rows represent the predic-
tion results, the columns represent the true labels and the
diagonal values are the correct predictions. We can see
the data is highly imbalanced, i.e., most of the labels are
1 or 4. In fact, for key C, the frequent chords are exactly
C major (1) and F major (4). This data distribution will
cause our models to suffer.

Now let’s visualize the accuracy results in the ﬁgure

below.

Fig. 4: Generated lead sheet based on HMM predictions

The result we get for HMM varies greatly for different
songs. The overall accuracy of HMM is 48.44% but for
some pieces, it can achieve an accuracy over 70%. This
could be caused by the limited information provided by
the ﬁrst note pitch observed. To add more pitches and
regard a group of notes in the measure as an observation,
the result can be improved but it will greatly complicate
the model with a much larger state space.

The second reason is that, assigning chord is a more
subjective than objective process. Two composers may
choose different chord types for the same measure and
both of them can sound pleasant. There is no single norm
to decide if the chord is assigned correct or not.

Fig. 3: Visualization of accuracy for different models

As the ﬁgure shows, random forest and boosting are
the most solid models. Apart from the imbalanced data,
logistic regression and SVM with linear kernel have bad
performance, mainly because the relation is highly non-
linear and these models cannot ﬁt this dataset while high

Machine Learning in Automatic Music

Chords Generation

Ziheng Chen

Department of Music
zihengc@stanford.edu

Jie Qi

Department of Electrical Engineering

qijie@stanford.edu

Yifei Zhou

Department of Statistics
yfzhou@stanford.edu

I. INTRODUCTION

III. DATASET

Melodies and chords compose a basic music piece.
Assigning chords for a music melody is an important step
in music composition. This project is to apply machine
learning techniques to assign chords to follow several
measures of melody and generate pleasant music pieces.
In real music chord assignment process, to choose
which chord to use for a measure, musicians normally
consider the notes in this measure and how chords
are progressed. So our aim is to learn the relationship
between notes and chords, as well as the relationship
between adjacent chords, and use that to assign chords
for a new melody.

The input to our algorithm is a music piece with
several measures. We then try different models to out-
put a predicted chord for each measure. We will use
basic models taught in the class (Logistic Regression,
Naive Bayes, Support Vector Machine) as well as some
advanced models (Random Forest, Boosting, Hidden
Markov Model), and compare their performance for our
problem.

II. RELATED WORK

There have been some previous works using machine
learning techniques to generate music chords. Cunha and
Ramalho[1] set a neural network and combined it with
a rule-based approach to select accompanying chords
for a melody, while Legaspi et al.[2] use a genetic
algorithm to build chords. Chuan and Chew[3] use a
data-driven HMM combined with a series of musical
rules to generate chord progression. Simon et al.[4] use
HMM and 60 types of chords to made an interactive
product, which would generate chord accompaniment for
human voice input. Paiement et al.[5] use a multilevel
graphical model to generate chord progressions for a
given melody.

A. Data Source

We collected 43 lead sheets as our dataset. The chosen
lead sheets have several properties: a) virtually each
measure in the dataset has and has only one chord; b) the
chords in the dataset are mainly scale tone chords, which
are the most basic and commonly used chords in music
pieces. The data we use are in MusicXML format, which
is a digital sheet music format for common Western
music notation. Measure information can be extracted
from this format by MATLAB.

B. Data Preprocessing

To simplify our further analysis, we did some prepro-

cessing of the dataset before training:

1) The key of each song is shifted to key C. The
key of a song can determine the note and a set of
common chord types the song uses. A song written
in one key can be easily shifted to another key
by simply increasing or decreasing all the pitches
in notes and chords equally, without affecting its
subjective character. Therefore, without any loss
of music information, we can shift the key of each
song to key C to make the dataset more organized
as well as decrease the number of class types
(chord types) in training process.

2) The chord types (class types) are restricted only
to scale tone chords in key C, which are 7 types:
C major, D minor, E minor, F major, G major, A
minor, and G dominant. Other types of chords (like
C Dominant, G suspended, etc.) are transformed to
their most similar scale tone chords.

3) Some measures in the dataset have no chords or no
notes (like rest measure). We simply delete these
measures from the dataset.

4) A small number of measures in our dataset are
assigned two chords continuously to accompany

different notes in the measure. To simplify the
learning process, we regard the second chord as
the chord of this measure and delete the ﬁrst one.
From an audiences perspective, it will sound better
than deleting the second chord.

After preprocessing, the 43 lead sheets we chose have

813 measures in total.

The 7 chord labels are shown in TABLE I. They are
the labels we are trying to assign. Now, our project turns
into a multi-class classiﬁcation problem.

Label
Chord

2

1
7
C Dm Em F G Am G7

3

4

5

6

TABLE I: Labels for different chord types

IV. FEATURE SELECTION

We started by extracting some indicative features. The
basic units for prediction are the notes inside a measure.
Then we chose our initial features from the following
three aspects:

1) Note itself: whether a note is present in the mea-
sure; take value of 0 or 1. A chord is inﬂuenced by
the notes that appear to create a sense of harmony.
2) Note vs. Beat: the notes on the beat in the measure

since chords should accompany the beat tones.

3) Note vs. Duration: the longest notes in the measure
since the long notes need to be satisﬁed by the
assigned chords.

To represent a note in a measure, we quantiﬁed it as
shown in TABLE II. It is labeled 1 to 12. Note that
we don’t consider which octave the music note lies in,
since the octave basically will not affect the chord type
we choose (E.g. C4 and C5 are both labeled 1).

Label
Note
Label
Note

1
C
7

F#/Gb

2

C#/Db

8
G

3
D
9

G#/Ab

4

D#/Eb

10
A

5
E
11

A#/Bb

6
F
12
B

TABLE II: Labels for different note types

We chose notes on the 4/4 beats and 2 longest notes.
In addition to the 12 type (1) features, we initially have
18 features.

To get the true effective features, we ran Forward
Feature Selection based on Random Forest (which will
be discussed further in the next section), by adding one
feature at a time and selecting the current optimal one.
The result is shown in Figure 1.

Fig. 1: Forward feature selection result

We can see the accuracy does not improve much after
6 features. Finally, we selected the next 6 features in our
following analysis: the note pitches on the 4 beats and
the 2 longest note pitches.

V. MODELS

A. Logistic Regression

We used multinomial logistic regression, i.e., softmax

regression in R, with the log-likelihood as

m(cid:88)

(cid:32)

k(cid:89)

(cid:96)(θ) =

log

i=1

l=1

(cid:80)k

l x(i)

eθT
j=1 eθT

l x(i)

(cid:33)1{y(i)=l}

(1)

We could achieve the maximum likelihood estimate

by using Newton’s method:

θ := θ − H−1∇θ(cid:96)(θ)

(2)

since our feature size is small (n = 6) and it is easy to
compute the inverse of the Hessian.

B. Naive Bayes

We applied Naive Bayes by using the Statistics and
Machine Learning Toolbox in MATLAB. We used the
multinomial event model with multiple classes. For any
class c, the maximum likelihood estimate gives
j = k ∧ y(i) = c}

(cid:80)m

i=1

φk|y=c =

(cid:80)ni
(cid:80)m
j=1 1{x(i)
i=1 1{y(i) = c}ni
(cid:80)m
i=1 1{y(i) = c}

(3)

(4)

φy=c =

m

We then made the prediction on the posterior com-

puted with the probabilities above.

C. Support Vector Machine (SVM)

E. Boosting

SVM is one of the supervised learning algorithms
which is to maximize the distance between training
example and classiﬁcation hyperplane by ﬁnding (use
hard-margin as an example):

min
γ,w,b

||w||2
1
2
s.t. y(i)(wT x(i) + b) ≥ 1, i = 1, ..., m

(5)

By using kernel
trick, SVM can efﬁciently perform
non-linear classiﬁcation or data with high-dimensional
features. In this problem, we tried the following three
kernels in R:

1) Linear kernel

K(xi, xj) =

2) Polynomial kernel

(cid:32)

p(cid:88)

k=1

K(xi, xj) =

1 +

xipxjp

(cid:33)d

p(cid:88)

k=1

xipxjp

3) Radial kernel

K(xi, xj) = e−γ(cid:80)p

k=1(xip−xjp)2

D. Random Forest

Random forest is based on bagging which is a kind
of decision tree with bootstrapping and can decrease
variance. For a classiﬁcation problems with p features,
√
p features are used in each split in order to decrease
the correlation of the trees[6]. We applied this model
using R package.

After applying the model, we can generate the impor-
tance plot for the features as the ﬁgure below. All the
features we are using have the similar importance level,
which reinforces the conclusion from feature selection.

Fig. 2: Variable importance

(6)

(7)

(8)

Boosting is a meta-algorithm to learn slowly to ﬁt
the new model from the residual of the current model.
Its parameters include the number of trees to split, the
shrinkage parameter and the depth of the tree[6]. We also
applied this model in R.

Speciﬁcally, we tune these parameters using cross vali-
dation to make sure the model has the best performance.
As a result, the number of trees is 200, the shrinkage
parameter is 0.2 and the depth is 4.

F. Hidden Markov Model (HMM)

Up till now, these ﬁve models above make prediction
based on the information of a single measure. We wanted
to incorporate the relationship between measures. So, we
also tried HMM to make prediction based on a sequence
of measures.
In HMM,

the system is being modeled to be a
Markov process which has a series of observed states
x = {x1, x2, ..., xT} and a series of related unobserved
(hidden) states z = {z1, z2, ..., zT} (T is the number
of the states). Suppose there are S types of observed
states and Z types of hidden states. An S × S Transi-
tion Matrix denotes the transition probabilities between
adjacent hidden states. And an S × Z Emission Matrix
denotes the probabilities of each hidden state emit each
observed state. Given an observed series of outputs, if
we know the Transition Matrix and Emission Matrix,
we can compute the most likely hidden series using the
Viterbi Algorithm[7].

In our chord assignment problem, the ﬁrst note of each
measure was considered as an observed state, and the
chord of each measure was considered as a hidden state.
Using our dataset, we computed the transition probability
and emission probability and formed transition matrix
and emission matrix. And then we computed the most
likely chord progression using hmmviterbi function in
Statistics and Machine Learning Toolbox in MATLAB.

VI. RESULTS & ANALYSIS

A. Cross Validation

We started at comparing different machine learning
models to indicate how they perform. We used hold-out
cross validation and 30% of the data as the validation
set. In addition, we also tried k-fold cross validation.
The cross validation results are used to represent the test
accuracy and evaluate how our models perform.

B. Prediction on a Single Measure

The cross validation accuracy for the ﬁve different
models we used is shown in TABLE III. (k = 5, 10
for k-fold)

Model

Logistic Regression

Naive Bayes
SVM Linear
SVM Poly
SVM Radial
Random Forest

Boosting

70%/30% 10-fold
43.75%
53.57%
48.21%
56.25%
65.17%
62.50%
63.39%

5-fold
39.15% 38.42%
55.34% 53.67%
36.97% 38.35%
52.80% 51.63%
61.39% 63.13%
64.06% 63.73%
64.99% 62.29%

TABLE III: Accuracy for different models

bias/low variance classiﬁers (e.g., Naive Bayes) could
have a better performance. However, Naive Bayes also
assumes that the features are conditionally independent.
In reality, the four notes on the beat could inﬂuence each
other, so such strong assumption could limit the per-
formance of Naive Bayes. Random forest and Boosting
perform best in our case since they are both complex
and have a reduction in model variance.
C. Prediction on Sequential Measures

The following is the song (cid:109)“silent night ” that is

assigned chords by our HMM model.

To get an insight of the results above, we can have
a look at the confusion matrix. Take SVM with radial
kernel as an example:



6
0
0

1 6 2
45 0 1
0 0 0
5 0
0
0 0
0
0 0 0
2 0 13 1 3 1
5
4 0 0
1 1
2
3 3 1
0 0
1
0
0 0
0 0 3



(9)

1
1
0

In the confusion matrix, the rows represent the predic-
tion results, the columns represent the true labels and the
diagonal values are the correct predictions. We can see
the data is highly imbalanced, i.e., most of the labels are
1 or 4. In fact, for key C, the frequent chords are exactly
C major (1) and F major (4). This data distribution will
cause our models to suffer.

Now let’s visualize the accuracy results in the ﬁgure

below.

Fig. 4: Generated lead sheet based on HMM predictions

The result we get for HMM varies greatly for different
songs. The overall accuracy of HMM is 48.44% but for
some pieces, it can achieve an accuracy over 70%. This
could be caused by the limited information provided by
the ﬁrst note pitch observed. To add more pitches and
regard a group of notes in the measure as an observation,
the result can be improved but it will greatly complicate
the model with a much larger state space.

The second reason is that, assigning chord is a more
subjective than objective process. Two composers may
choose different chord types for the same measure and
both of them can sound pleasant. There is no single norm
to decide if the chord is assigned correct or not.

Fig. 3: Visualization of accuracy for different models

As the ﬁgure shows, random forest and boosting are
the most solid models. Apart from the imbalanced data,
logistic regression and SVM with linear kernel have bad
performance, mainly because the relation is highly non-
linear and these models cannot ﬁt this dataset while high

VII. CONCLUSION & FUTURE WORK

From the results above, we can conclude that Random
forest and Boosting perform best
in prediction with
single measure. HMM could also achieve a good result
if we include more information as our observation states.
But the highest accuracy we can achieve is only about
70%. This is caused by the subjectivity in our dataset.
This can be improved by choosing data from one
composer with one genre. However, it is typically hard
to achieve. A better way is to design an experiment to
use human judgements to evaluate the performance of
our model instead of only relying on the given labels.

REFERENCES

[1] Cunha, U. S., & Ramalho, G. (1999). An intelligent hybrid model

for chord prediction. Organised Sound, 4(02), 115-119.

[2] Legaspi, R., Hashimoto, Y., Moriyama, K., Kurihara, S., &
Numao, M. (2007, January). Music compositional intelligence
with an affective ﬂavor. In Proceedings of the 12th international
conference on Intelligent user interfaces (pp. 216-224). ACM.

[3] Chuan, C. H., & Chew, E. (2007, June). A hybrid system for
automatic generation of style-speciﬁc accompaniment. In 4th Intl
Joint Workshop on Computational Creativity.

[4] Simon, I., Morris, D., & Basu, S. (2008, April). MySong:
automatic accompaniment generation for vocal melodies. In
Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems (pp. 725-734). ACM.

[5] Paiement, J. F., Eck, D., & Bengio, S. (2006). Probabilistic
melodic harmonization. In Advances in Artiﬁcial Intelligence
(pp. 218-229). Springer Berlin Heidelberg.

[6] James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An

introduction to statistical learning (p. 6). New York: springer.

[7] Viterbi, A. J. (1967). Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm. Information
Theory, IEEE Transactions on, 13(2), 260-269.

