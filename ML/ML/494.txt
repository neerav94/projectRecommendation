Classification and Regression Approaches to Predicting United States Senate Elections 

Rohan Sampath, Yue Teng 

 

Abstract 

The  United  States  Senate  is  arguably  the  finest 
democratic  institution  for  debate  and deliberation  in 
the world. It also provides a fascinating opportunity to 
understand  the  complex  dynamics  that  go  into 
determining  the  outcome  of  Senate  elections,  using 
Machine Learning.  

Motivation 

Are  elections  decided  even  before  they  begin?  Can 
political  fundamentals  predict  elections  regardless  of 
candidates/campaigns? 

Our goal is to get a birdâ€™s eye, forward-looking view 
of  Senate  elections  using  data  available  well  in 
advance. We believe that if we reliably predict Senate 
elections  well  before  they  happen,  that  has  several 
significant implications for various stakeholders, since 
individual  senators  wield  a  tremendous  amount  of 
legislative power.  

Introduction 

We use: 

ï‚·  A modified version of the LMS algorithm, called 
a discount-weighted least-mean squares algorithm 
to  predict  the  margin  of  victory  of  Senate 
elections. 

ï‚·  An ordinary Support Vector Machine classifier to 

predict the outcome of Senate elections. 

ï‚·  Random  Forest  to  also  predict  the  outcome  of 

Senate elections. 

Data 

Our  data  set  consists  of  all  biennial  Senate  elections 
that were held from 1998 to 2014; this data is publicly 
available. 

Preprocessing:  We  preprocess  the  data  to  weed  out 
elections where: 

ï‚·  There wasnâ€™t exactly one Republican and exactly 

one Democratic candidate. 

ï‚·  A third Party candidate either won the election or 
distorted the election by winning more than 20% 
of  the  vote  (i.e.  a  third  party  candidate  was  a 
significant player). 

After preprocessing, we are left with 273 data points. 
(There were 300 regularly scheduled Senate elections 
in the period 1998-2014, of which 27 were eliminated 
in preprocessing.) 

The  fundamental  challenge  we  face  is  one  of  limited 
data. Senate elections, by their very nature, are limited 
â€“ only around 33 happen every two years. Therefore, 
we had to keep in mind that an inescapable part of this 
project was having limited data.   

Features 

We  use  a  feature  vector  of  71  features.  These  71 
features  include  original  sourced  features  (such  as 
margin  of  victory,  unemployment  rate  etc.)  and 
derived features (such as change in unemployment rate 
over a period of time). 

The features are described below: 

ï‚·  Margin of victory in the Senate election held six 
and  twelve  years  previously.  (Note:  Senators 
serve six-year terms.) 

ï‚·  Margin  of  victory  in  the  state  in  the  last  three 

Presidential Elections 

ï‚·  Presidential Approval in the State. 
ï‚·  Annualized Changes in the Presidential Approval 

in the state. 

ï‚·  Percent  African-American 

population 
extrapolated from the most recent Census) 

ï‚·  Percent  Hispanic/Latino 

population 
extrapolated from the most recent Census) 

(as 

((as 

ï‚·  Changes  in  the  above  demographic  factors  over 

time. 

ï‚·  Three-month average unemployment rates in the 

year before the election.  

Classification and Regression Approaches to Predicting United States Senate Elections 

Rohan Sampath, Yue Teng 

 

Abstract 

The  United  States  Senate  is  arguably  the  finest 
democratic  institution  for  debate  and deliberation  in 
the world. It also provides a fascinating opportunity to 
understand  the  complex  dynamics  that  go  into 
determining  the  outcome  of  Senate  elections,  using 
Machine Learning.  

Motivation 

Are  elections  decided  even  before  they  begin?  Can 
political  fundamentals  predict  elections  regardless  of 
candidates/campaigns? 

Our goal is to get a birdâ€™s eye, forward-looking view 
of  Senate  elections  using  data  available  well  in 
advance. We believe that if we reliably predict Senate 
elections  well  before  they  happen,  that  has  several 
significant implications for various stakeholders, since 
individual  senators  wield  a  tremendous  amount  of 
legislative power.  

Introduction 

We use: 

ï‚·  A modified version of the LMS algorithm, called 
a discount-weighted least-mean squares algorithm 
to  predict  the  margin  of  victory  of  Senate 
elections. 

ï‚·  An ordinary Support Vector Machine classifier to 

predict the outcome of Senate elections. 

ï‚·  Random  Forest  to  also  predict  the  outcome  of 

Senate elections. 

Data 

Our  data  set  consists  of  all  biennial  Senate  elections 
that were held from 1998 to 2014; this data is publicly 
available. 

Preprocessing:  We  preprocess  the  data  to  weed  out 
elections where: 

ï‚·  There wasnâ€™t exactly one Republican and exactly 

one Democratic candidate. 

ï‚·  A third Party candidate either won the election or 
distorted the election by winning more than 20% 
of  the  vote  (i.e.  a  third  party  candidate  was  a 
significant player). 

After preprocessing, we are left with 273 data points. 
(There were 300 regularly scheduled Senate elections 
in the period 1998-2014, of which 27 were eliminated 
in preprocessing.) 

The  fundamental  challenge  we  face  is  one  of  limited 
data. Senate elections, by their very nature, are limited 
â€“ only around 33 happen every two years. Therefore, 
we had to keep in mind that an inescapable part of this 
project was having limited data.   

Features 

We  use  a  feature  vector  of  71  features.  These  71 
features  include  original  sourced  features  (such  as 
margin  of  victory,  unemployment  rate  etc.)  and 
derived features (such as change in unemployment rate 
over a period of time). 

The features are described below: 

ï‚·  Margin of victory in the Senate election held six 
and  twelve  years  previously.  (Note:  Senators 
serve six-year terms.) 

ï‚·  Margin  of  victory  in  the  state  in  the  last  three 

Presidential Elections 

ï‚·  Presidential Approval in the State. 
ï‚·  Annualized Changes in the Presidential Approval 

in the state. 

ï‚·  Percent  African-American 

population 
extrapolated from the most recent Census) 

ï‚·  Percent  Hispanic/Latino 

population 
extrapolated from the most recent Census) 

(as 

((as 

ï‚·  Changes  in  the  above  demographic  factors  over 

time. 

ï‚·  Three-month average unemployment rates in the 

year before the election.  

ï‚· 

6-month, 12-month, 18-month 24-month changes 
in unemployment rate1 

ï‚·  Partisan  Voting  Index  (PVI)  over  the  past  three 

Presidential elections2  

ï‚·  Change 

in 

the  PVI  from 

the  second-last 

presidential election to the last one. 

ï‚·  Median income in the state. 
ï‚·  Variation in the median income in the state. 
ï‚· 

Indicator variable: Whether Republican candidate 
is the incumbent senator. 

ï‚· 

Indicator variable: Whether Democratic candidate 
is the incumbent senator. 

ï‚·  Number of years of incumbency for the President. 
ï‚· 

Indicator  variable:  Whether  the  election  was  a 
midterm election or not. 

Convention: In all cases, a positive result for the 
Republican is recorded as positive, and vice-versa. 
Example: A reduction of the unemployment rate 
during a Democratic Presidentâ€™s term is means that 
the feature data point is (-), since itâ€™s good for the 
Democrats. 

Cross-Validation 

We use cross-validation frequently through the project. 
Our  perusal  of  literature  suggested  that  a  direct-
application  of  k-fold  cross-validation  was  not 
appropriate  for  time-series  data  â€“  it  would  not  be 
appropriate  to  train  on  2012  data,  for  example,  and 
validate on a hold-out data point that happened before 
2012!  

Hence,  we  use  a  modified  version  called  forward 
chaining.  For  example,  say  we  have  a  training  set 
consisting  of  data  from  years  2000,  2002,  2004  and 
2006; we then design the folds as follows: 

ï‚·  Fold 1: train [2000], hold-out validation [2002] 
ï‚·  Fold  2:  train  [2000,  2002],  hold-out  validation 

[2004] 

ï‚·  Fold  3:  train  [2000,  2002,  2004],  hold-out 

validation [2006] 

                                                           
1 A reduction is positive under incumbent Republican 
President,  while  negative  under  an 
incumbent 
Democratic President; vice-versa for an increase. 

Principal Component Analysis 

Motivation:  clear  interdependencies  between  certain 
variables: PVI and Previous US Presidential election 
result, for example. 

In  order  to  choose  an  appropriate  ğ‘˜-dimension 
spanned by the first ğ‘˜ principal components subspace 
(for  ğ‘˜âˆˆ(1,  10))  and  thus  determine  the  ğ‘˜  principal 
factors,  we  use  the  Scree  Plot  and  the  Cumulative 
Variance Plot. 

(The plot below is for the 183 data points from 2002 
to 2012. First 10 principal components are shown.) 

Scree Plot

1.4
1.2
1
0.8
0.6
0.4
0.2
0

)
0
0
0
'
(
 

l

e
u
a
v
n
e
g
i
E

1

2

4

5

3
Factor/Component

7

6

8

9

10

 

 

Cumulative Variance Plot

e
c
n
a

 

i
r
a
V
e
v
i
t
a
u
m
u
C

l

110.00%

100.00%

90.00%

80.00%

70.00%

60.00%

1 2 3 4 5 6 7 8 9 10

Factor/Component

2  PVI  of  a  state:  On  average,  how  much  more 
Republican  was  the  state  in  the  last  two  presidential 
elections as compared to the nation as a whole) 

Classification and Regression Approaches to Predicting United States Senate Elections 

Rohan Sampath, Yue Teng 

 

Abstract 

The  United  States  Senate  is  arguably  the  finest 
democratic  institution  for  debate  and deliberation  in 
the world. It also provides a fascinating opportunity to 
understand  the  complex  dynamics  that  go  into 
determining  the  outcome  of  Senate  elections,  using 
Machine Learning.  

Motivation 

Are  elections  decided  even  before  they  begin?  Can 
political  fundamentals  predict  elections  regardless  of 
candidates/campaigns? 

Our goal is to get a birdâ€™s eye, forward-looking view 
of  Senate  elections  using  data  available  well  in 
advance. We believe that if we reliably predict Senate 
elections  well  before  they  happen,  that  has  several 
significant implications for various stakeholders, since 
individual  senators  wield  a  tremendous  amount  of 
legislative power.  

Introduction 

We use: 

ï‚·  A modified version of the LMS algorithm, called 
a discount-weighted least-mean squares algorithm 
to  predict  the  margin  of  victory  of  Senate 
elections. 

ï‚·  An ordinary Support Vector Machine classifier to 

predict the outcome of Senate elections. 

ï‚·  Random  Forest  to  also  predict  the  outcome  of 

Senate elections. 

Data 

Our  data  set  consists  of  all  biennial  Senate  elections 
that were held from 1998 to 2014; this data is publicly 
available. 

Preprocessing:  We  preprocess  the  data  to  weed  out 
elections where: 

ï‚·  There wasnâ€™t exactly one Republican and exactly 

one Democratic candidate. 

ï‚·  A third Party candidate either won the election or 
distorted the election by winning more than 20% 
of  the  vote  (i.e.  a  third  party  candidate  was  a 
significant player). 

After preprocessing, we are left with 273 data points. 
(There were 300 regularly scheduled Senate elections 
in the period 1998-2014, of which 27 were eliminated 
in preprocessing.) 

The  fundamental  challenge  we  face  is  one  of  limited 
data. Senate elections, by their very nature, are limited 
â€“ only around 33 happen every two years. Therefore, 
we had to keep in mind that an inescapable part of this 
project was having limited data.   

Features 

We  use  a  feature  vector  of  71  features.  These  71 
features  include  original  sourced  features  (such  as 
margin  of  victory,  unemployment  rate  etc.)  and 
derived features (such as change in unemployment rate 
over a period of time). 

The features are described below: 

ï‚·  Margin of victory in the Senate election held six 
and  twelve  years  previously.  (Note:  Senators 
serve six-year terms.) 

ï‚·  Margin  of  victory  in  the  state  in  the  last  three 

Presidential Elections 

ï‚·  Presidential Approval in the State. 
ï‚·  Annualized Changes in the Presidential Approval 

in the state. 

ï‚·  Percent  African-American 

population 
extrapolated from the most recent Census) 

ï‚·  Percent  Hispanic/Latino 

population 
extrapolated from the most recent Census) 

(as 

((as 

ï‚·  Changes  in  the  above  demographic  factors  over 

time. 

ï‚·  Three-month average unemployment rates in the 

year before the election.  

ï‚· 

6-month, 12-month, 18-month 24-month changes 
in unemployment rate1 

ï‚·  Partisan  Voting  Index  (PVI)  over  the  past  three 

Presidential elections2  

ï‚·  Change 

in 

the  PVI  from 

the  second-last 

presidential election to the last one. 

ï‚·  Median income in the state. 
ï‚·  Variation in the median income in the state. 
ï‚· 

Indicator variable: Whether Republican candidate 
is the incumbent senator. 

ï‚· 

Indicator variable: Whether Democratic candidate 
is the incumbent senator. 

ï‚·  Number of years of incumbency for the President. 
ï‚· 

Indicator  variable:  Whether  the  election  was  a 
midterm election or not. 

Convention: In all cases, a positive result for the 
Republican is recorded as positive, and vice-versa. 
Example: A reduction of the unemployment rate 
during a Democratic Presidentâ€™s term is means that 
the feature data point is (-), since itâ€™s good for the 
Democrats. 

Cross-Validation 

We use cross-validation frequently through the project. 
Our  perusal  of  literature  suggested  that  a  direct-
application  of  k-fold  cross-validation  was  not 
appropriate  for  time-series  data  â€“  it  would  not  be 
appropriate  to  train  on  2012  data,  for  example,  and 
validate on a hold-out data point that happened before 
2012!  

Hence,  we  use  a  modified  version  called  forward 
chaining.  For  example,  say  we  have  a  training  set 
consisting  of  data  from  years  2000,  2002,  2004  and 
2006; we then design the folds as follows: 

ï‚·  Fold 1: train [2000], hold-out validation [2002] 
ï‚·  Fold  2:  train  [2000,  2002],  hold-out  validation 

[2004] 

ï‚·  Fold  3:  train  [2000,  2002,  2004],  hold-out 

validation [2006] 

                                                           
1 A reduction is positive under incumbent Republican 
President,  while  negative  under  an 
incumbent 
Democratic President; vice-versa for an increase. 

Principal Component Analysis 

Motivation:  clear  interdependencies  between  certain 
variables: PVI and Previous US Presidential election 
result, for example. 

In  order  to  choose  an  appropriate  ğ‘˜-dimension 
spanned by the first ğ‘˜ principal components subspace 
(for  ğ‘˜âˆˆ(1,  10))  and  thus  determine  the  ğ‘˜  principal 
factors,  we  use  the  Scree  Plot  and  the  Cumulative 
Variance Plot. 

(The plot below is for the 183 data points from 2002 
to 2012. First 10 principal components are shown.) 

Scree Plot

1.4
1.2
1
0.8
0.6
0.4
0.2
0

)
0
0
0
'
(
 

l

e
u
a
v
n
e
g
i
E

1

2

4

5

3
Factor/Component

7

6

8

9

10

 

 

Cumulative Variance Plot

e
c
n
a

 

i
r
a
V
e
v
i
t
a
u
m
u
C

l

110.00%

100.00%

90.00%

80.00%

70.00%

60.00%

1 2 3 4 5 6 7 8 9 10

Factor/Component

2  PVI  of  a  state:  On  average,  how  much  more 
Republican  was  the  state  in  the  last  two  presidential 
elections as compared to the nation as a whole) 

Support Vector Machine (SVM) 

Classification 

to  implement  SVM  classification  with  a  Gaussian 
kernel function. 

We  solve  3  classification  problems  using  standard 
SVM classification:  

Results for SVM Classification 

(Training Data Set) 

Years Trained 

Upon 

1998-2008 

2000-2010 

2002-2012 

 

(Test Data Set) 

N 

183 

183 

182 

N Correctly 

Training 

classified 

Error 

179 

180 

178 

2.19% 

1.64% 

2.20% 

Year  

(Using Training 

N 

N Correctly 

classified 

Test Error 

Data from) 

2010  

(1998-2008) 

2012  

(2000-2010) 

2014  

(2002-2012) 

 

30 

30 

30 

27 

26 

27 

10.00 % 

13.33% 

10.00% 

ï‚· 
ï‚· 

ï‚· 

classifying 2014 after learning on 2002-2012, 

classifying 2012 after learning on 2000-2010, 
and 

classifying 2010 after learning on 1998-2008.  

The fundamental motivation behind SVM is carrying 
out  binary  classification  in  a  high-dimension  feature 
space  efficiently,  by  using  the  kernel  trick  (i.e.  by 
mapping  input  data  via  a  non-linear  function).  The 
SVM  algorithm  can  perform 
this  computation 
efficiently  because  it  considers  a  small  number  of 
training points and ignores all training points that are  
close (within a threshold ğœ–) to the model prediction.  

The primal optimization problem is given by: 

 

min   

1
2

ğ‘š

â€–ğ‘¤2â€– + ğ¶ âˆ‘(Î¾ğ‘– + Î¾ğ‘–

âˆ—)
 

ğ‘–=1

subject to:     {

ğ‘¦(ğ‘–)âˆ’ < w, ğ“(ğ‘–) > âˆ’ğ‘ â‰¤ ğœ– + Î¾ğ‘– 
âˆ—
< w, ğ“(ğ‘–) > + ğ‘ âˆ’ ğ‘¦(ğ‘–) â‰¤ ğœ– + Î¾ğ‘–

Î¾ğ‘–,  Î¾ğ‘–

âˆ— â‰¥ 0 

The  norm â€–ğ‘¤2â€– measures  the  flatness  of  the  proxy, 
and the constraints force the model to approximate all 
âˆ— are 
training points within an absolute margin ğœ–. Î¾ğ‘–, Î¾ğ‘–
slack  variables that allow  for  compliance  with  the   ğœ– 
margin constraints and the flatness of the proxy. ğ¶ is 
the penalty for violating the constraints. 

The corresponding dual problem is given by: 

max     âˆ’

1
2

ğ‘š
âˆ‘ (ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
ğ‘–,ğ‘—=1

âˆ—)
âˆ—)(ğ›¼ğ‘— âˆ’ ğ›¼ğ‘—

(< ğ“(ğ‘–), ğ“(ğ‘—) >)

ğ‘š

ğ‘š

âˆ’ ğœ– âˆ‘(ğ›¼ğ‘– + ğ›¼ğ‘–

âˆ—) âˆ’ ğ‘¦(ğ‘–) âˆ‘(ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
âˆ—)
 

ğ‘–=1

ğ‘–=1

subject to:{

ğ‘š
âˆ‘ (ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
ğ‘–=1
0 â‰¤ ğ›¼ğ‘–, ğ›¼ğ‘–

âˆ—) = 1
 
âˆ— â‰¤ ğ¶

The  dual  optimization  is  convex  and  can  easily  be 
solved with optimization software. We use LIBSVM 

Discount-Weighted Least-Means Square 

 

Regression 

 
Once again, we solve three regression problems for the 
years 2010, 2012 and 2014. 

Given that the composition and voting intentions of a 
state evolve rapidly, we thought it would be beneficial 
to give less weight to earlier training data as compared 
to later ones. 

The basic premise of this time discount rate algorithm, 
which  has  been  adapted  from  Harrison  and Johnston 
[5], is to use a â€˜discount factorâ€™ which conveys the rate 
of decay of the information content of an observation. 

The  discount-weighted  LMS  algorithm  had  a  lower 
generalization  error  than  a  standard  LMS  algorithm 
when forward-chaining cross-validation was used. 

We used a discount factor of the form: 

Classification and Regression Approaches to Predicting United States Senate Elections 

Rohan Sampath, Yue Teng 

 

Abstract 

The  United  States  Senate  is  arguably  the  finest 
democratic  institution  for  debate  and deliberation  in 
the world. It also provides a fascinating opportunity to 
understand  the  complex  dynamics  that  go  into 
determining  the  outcome  of  Senate  elections,  using 
Machine Learning.  

Motivation 

Are  elections  decided  even  before  they  begin?  Can 
political  fundamentals  predict  elections  regardless  of 
candidates/campaigns? 

Our goal is to get a birdâ€™s eye, forward-looking view 
of  Senate  elections  using  data  available  well  in 
advance. We believe that if we reliably predict Senate 
elections  well  before  they  happen,  that  has  several 
significant implications for various stakeholders, since 
individual  senators  wield  a  tremendous  amount  of 
legislative power.  

Introduction 

We use: 

ï‚·  A modified version of the LMS algorithm, called 
a discount-weighted least-mean squares algorithm 
to  predict  the  margin  of  victory  of  Senate 
elections. 

ï‚·  An ordinary Support Vector Machine classifier to 

predict the outcome of Senate elections. 

ï‚·  Random  Forest  to  also  predict  the  outcome  of 

Senate elections. 

Data 

Our  data  set  consists  of  all  biennial  Senate  elections 
that were held from 1998 to 2014; this data is publicly 
available. 

Preprocessing:  We  preprocess  the  data  to  weed  out 
elections where: 

ï‚·  There wasnâ€™t exactly one Republican and exactly 

one Democratic candidate. 

ï‚·  A third Party candidate either won the election or 
distorted the election by winning more than 20% 
of  the  vote  (i.e.  a  third  party  candidate  was  a 
significant player). 

After preprocessing, we are left with 273 data points. 
(There were 300 regularly scheduled Senate elections 
in the period 1998-2014, of which 27 were eliminated 
in preprocessing.) 

The  fundamental  challenge  we  face  is  one  of  limited 
data. Senate elections, by their very nature, are limited 
â€“ only around 33 happen every two years. Therefore, 
we had to keep in mind that an inescapable part of this 
project was having limited data.   

Features 

We  use  a  feature  vector  of  71  features.  These  71 
features  include  original  sourced  features  (such  as 
margin  of  victory,  unemployment  rate  etc.)  and 
derived features (such as change in unemployment rate 
over a period of time). 

The features are described below: 

ï‚·  Margin of victory in the Senate election held six 
and  twelve  years  previously.  (Note:  Senators 
serve six-year terms.) 

ï‚·  Margin  of  victory  in  the  state  in  the  last  three 

Presidential Elections 

ï‚·  Presidential Approval in the State. 
ï‚·  Annualized Changes in the Presidential Approval 

in the state. 

ï‚·  Percent  African-American 

population 
extrapolated from the most recent Census) 

ï‚·  Percent  Hispanic/Latino 

population 
extrapolated from the most recent Census) 

(as 

((as 

ï‚·  Changes  in  the  above  demographic  factors  over 

time. 

ï‚·  Three-month average unemployment rates in the 

year before the election.  

ï‚· 

6-month, 12-month, 18-month 24-month changes 
in unemployment rate1 

ï‚·  Partisan  Voting  Index  (PVI)  over  the  past  three 

Presidential elections2  

ï‚·  Change 

in 

the  PVI  from 

the  second-last 

presidential election to the last one. 

ï‚·  Median income in the state. 
ï‚·  Variation in the median income in the state. 
ï‚· 

Indicator variable: Whether Republican candidate 
is the incumbent senator. 

ï‚· 

Indicator variable: Whether Democratic candidate 
is the incumbent senator. 

ï‚·  Number of years of incumbency for the President. 
ï‚· 

Indicator  variable:  Whether  the  election  was  a 
midterm election or not. 

Convention: In all cases, a positive result for the 
Republican is recorded as positive, and vice-versa. 
Example: A reduction of the unemployment rate 
during a Democratic Presidentâ€™s term is means that 
the feature data point is (-), since itâ€™s good for the 
Democrats. 

Cross-Validation 

We use cross-validation frequently through the project. 
Our  perusal  of  literature  suggested  that  a  direct-
application  of  k-fold  cross-validation  was  not 
appropriate  for  time-series  data  â€“  it  would  not  be 
appropriate  to  train  on  2012  data,  for  example,  and 
validate on a hold-out data point that happened before 
2012!  

Hence,  we  use  a  modified  version  called  forward 
chaining.  For  example,  say  we  have  a  training  set 
consisting  of  data  from  years  2000,  2002,  2004  and 
2006; we then design the folds as follows: 

ï‚·  Fold 1: train [2000], hold-out validation [2002] 
ï‚·  Fold  2:  train  [2000,  2002],  hold-out  validation 

[2004] 

ï‚·  Fold  3:  train  [2000,  2002,  2004],  hold-out 

validation [2006] 

                                                           
1 A reduction is positive under incumbent Republican 
President,  while  negative  under  an 
incumbent 
Democratic President; vice-versa for an increase. 

Principal Component Analysis 

Motivation:  clear  interdependencies  between  certain 
variables: PVI and Previous US Presidential election 
result, for example. 

In  order  to  choose  an  appropriate  ğ‘˜-dimension 
spanned by the first ğ‘˜ principal components subspace 
(for  ğ‘˜âˆˆ(1,  10))  and  thus  determine  the  ğ‘˜  principal 
factors,  we  use  the  Scree  Plot  and  the  Cumulative 
Variance Plot. 

(The plot below is for the 183 data points from 2002 
to 2012. First 10 principal components are shown.) 

Scree Plot

1.4
1.2
1
0.8
0.6
0.4
0.2
0

)
0
0
0
'
(
 

l

e
u
a
v
n
e
g
i
E

1

2

4

5

3
Factor/Component

7

6

8

9

10

 

 

Cumulative Variance Plot

e
c
n
a

 

i
r
a
V
e
v
i
t
a
u
m
u
C

l

110.00%

100.00%

90.00%

80.00%

70.00%

60.00%

1 2 3 4 5 6 7 8 9 10

Factor/Component

2  PVI  of  a  state:  On  average,  how  much  more 
Republican  was  the  state  in  the  last  two  presidential 
elections as compared to the nation as a whole) 

Support Vector Machine (SVM) 

Classification 

to  implement  SVM  classification  with  a  Gaussian 
kernel function. 

We  solve  3  classification  problems  using  standard 
SVM classification:  

Results for SVM Classification 

(Training Data Set) 

Years Trained 

Upon 

1998-2008 

2000-2010 

2002-2012 

 

(Test Data Set) 

N 

183 

183 

182 

N Correctly 

Training 

classified 

Error 

179 

180 

178 

2.19% 

1.64% 

2.20% 

Year  

(Using Training 

N 

N Correctly 

classified 

Test Error 

Data from) 

2010  

(1998-2008) 

2012  

(2000-2010) 

2014  

(2002-2012) 

 

30 

30 

30 

27 

26 

27 

10.00 % 

13.33% 

10.00% 

ï‚· 
ï‚· 

ï‚· 

classifying 2014 after learning on 2002-2012, 

classifying 2012 after learning on 2000-2010, 
and 

classifying 2010 after learning on 1998-2008.  

The fundamental motivation behind SVM is carrying 
out  binary  classification  in  a  high-dimension  feature 
space  efficiently,  by  using  the  kernel  trick  (i.e.  by 
mapping  input  data  via  a  non-linear  function).  The 
SVM  algorithm  can  perform 
this  computation 
efficiently  because  it  considers  a  small  number  of 
training points and ignores all training points that are  
close (within a threshold ğœ–) to the model prediction.  

The primal optimization problem is given by: 

 

min   

1
2

ğ‘š

â€–ğ‘¤2â€– + ğ¶ âˆ‘(Î¾ğ‘– + Î¾ğ‘–

âˆ—)
 

ğ‘–=1

subject to:     {

ğ‘¦(ğ‘–)âˆ’ < w, ğ“(ğ‘–) > âˆ’ğ‘ â‰¤ ğœ– + Î¾ğ‘– 
âˆ—
< w, ğ“(ğ‘–) > + ğ‘ âˆ’ ğ‘¦(ğ‘–) â‰¤ ğœ– + Î¾ğ‘–

Î¾ğ‘–,  Î¾ğ‘–

âˆ— â‰¥ 0 

The  norm â€–ğ‘¤2â€– measures  the  flatness  of  the  proxy, 
and the constraints force the model to approximate all 
âˆ— are 
training points within an absolute margin ğœ–. Î¾ğ‘–, Î¾ğ‘–
slack  variables that allow  for  compliance  with  the   ğœ– 
margin constraints and the flatness of the proxy. ğ¶ is 
the penalty for violating the constraints. 

The corresponding dual problem is given by: 

max     âˆ’

1
2

ğ‘š
âˆ‘ (ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
ğ‘–,ğ‘—=1

âˆ—)
âˆ—)(ğ›¼ğ‘— âˆ’ ğ›¼ğ‘—

(< ğ“(ğ‘–), ğ“(ğ‘—) >)

ğ‘š

ğ‘š

âˆ’ ğœ– âˆ‘(ğ›¼ğ‘– + ğ›¼ğ‘–

âˆ—) âˆ’ ğ‘¦(ğ‘–) âˆ‘(ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
âˆ—)
 

ğ‘–=1

ğ‘–=1

subject to:{

ğ‘š
âˆ‘ (ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
ğ‘–=1
0 â‰¤ ğ›¼ğ‘–, ğ›¼ğ‘–

âˆ—) = 1
 
âˆ— â‰¤ ğ¶

The  dual  optimization  is  convex  and  can  easily  be 
solved with optimization software. We use LIBSVM 

Discount-Weighted Least-Means Square 

 

Regression 

 
Once again, we solve three regression problems for the 
years 2010, 2012 and 2014. 

Given that the composition and voting intentions of a 
state evolve rapidly, we thought it would be beneficial 
to give less weight to earlier training data as compared 
to later ones. 

The basic premise of this time discount rate algorithm, 
which  has  been  adapted  from  Harrison  and Johnston 
[5], is to use a â€˜discount factorâ€™ which conveys the rate 
of decay of the information content of an observation. 

The  discount-weighted  LMS  algorithm  had  a  lower 
generalization  error  than  a  standard  LMS  algorithm 
when forward-chaining cross-validation was used. 

We used a discount factor of the form: 

ğ›¿ğ‘¡ =

âˆš2ğ›¼ğ‘¡ âˆ’ 1
2ğ›¼ğ‘¡ + 1

â„

âˆš2ğ›¼ğ‘‡ âˆ’ 1
2ğ›¼ğ‘‡ + 1

Random Forests 

 

We  also  implemented  Random  Forests  classification 
on the original data set. 

Where ğ›¿1 is  the  discount  factor  for  the  earliest  time 
period  and  T  is  the  number  of  time  periods  (i.e. ğ‘¡ =
1,   â€¦ . ,  ğ‘‡ ). ğ›¼ is  a  parameter  than  can  be  optimized. 
Clearly, ğ›¿ğ‘‡ is always = 1.  

Discount factor for various alphas is shown below: 

Random forests use decision trees as the basic building 
block to enable prediction. A decision tree uses a tree-
like graph or model of decisions to split up the feature 
space into separate regions. Each data point falls into 
exactly  one  region,  and  in  the  case  of  classification, 
the most common class is the predicted class. 

Discount factors for various alphas 

)
a
t
l
e
d
(
 
r
o
t
c
a
f
 
t
n
u
o
c
s
i
D

1.1

1

0.9

0.8

0.7

0.6

0.5

 

(T = 5)

1

2

3

4

5

Time period (t)

alpha=1

alpha=2

alpha=3

alpha=4

Results for Discount Weighted LMS 

(Training Data Set) 

Years 
Trained 
Upon 

N 

Mean 
margin 
of error 

N 

Correctly 
classified 

1998-2008  183 

4.20% 

179 

Training 

Classification 

Error 

2.19% 

2000-2010  183 

4.61% 

180 

1.64% 

2002-2012  182 

4.43% 

178 

2.20% 

 

(Test Data Set) 

Year  
(Using 

Training Data 

N 

from) 
2010  

(1998-2008) 

2012  

(2000-2010) 

2014  

(2002-2012) 

Mean 
margin 
of error 

N 

Test 

Correctly 
classified 

Classifica
tion Error 

30 

7.48% 

30 

6.52% 

30 

7.10% 

27 

26 

27 

10.00 % 

13.33% 

10.00% 

Random  forests  use  multiple  decision  trees,  and  the 
reasoning  behind  this  is  to  reduce  the  chances  of 
overfitting to the data. Each tree is built on a separate 
dataset  where  each  dataset  is  sampled  from  the 
original distribution. However, since we do not know, 
or  have  access  to,  the  original  distribution,  we  build 
each dataset by sampling with replacement using the 
is  known  as  bootstrap 
original  dataset.  This 
aggregation,  since  we  now  have  multiple  decision 
trees  which  are  all  fit  to  an  approximation  of  the 
original  distribution.  By  using  multiple  trees  we  can 
lower  the  variance  of  the  model  at  the  cost  of 
increasing the bias.  

Although  bootstrap  aggregation  helps  to  reduce  the 
variance  of  the  model,  it  does  not  fix  an  important 
problem  which  is  that  every  tree  may  be  highly 
correlated to each other. In that case, it does not matter 
how  many  trees  we  average  our  predictions  over  if 
each tree is exactly the same, since the variance of the 
model  will  not  decrease  at  all.  In  order  to  prevent 
highly  similar  trees,  we  will  only  consider  a  random 
subset of the features at each split. Often the number 
of  features  considered,  ğ‘š ,  is  much  lower  than  ğ‘ , 
where ğ‘ is the original number of predictors.  

There  are  two  parameters  to  tune  over  in  random 
forests: ğµ, the number of decision trees to create, and 
ğ‘š, the number of predictors to consider at each split. 
Increasing ğµ will prevent the model from overfitting, 
but  may  also  prevent  accurately  capturing 
the 
relationship between the training data and the output. 
Increasing ğ‘š will increase the chances of overfitting, 
but  may  allow  a  better  fit  to  the  training  data. 
Appropriate  choices  for ğµ and ğ‘š can  be  selected  by 
using cross validation. Choices for ğµ and ğ‘š that were 
optimal  in  our  three  tests  hovered  around ğ‘© â‰ˆ100 
and ğ’ â‰ˆ ğ’‘/7 â‰ˆ10.   

Classification and Regression Approaches to Predicting United States Senate Elections 

Rohan Sampath, Yue Teng 

 

Abstract 

The  United  States  Senate  is  arguably  the  finest 
democratic  institution  for  debate  and deliberation  in 
the world. It also provides a fascinating opportunity to 
understand  the  complex  dynamics  that  go  into 
determining  the  outcome  of  Senate  elections,  using 
Machine Learning.  

Motivation 

Are  elections  decided  even  before  they  begin?  Can 
political  fundamentals  predict  elections  regardless  of 
candidates/campaigns? 

Our goal is to get a birdâ€™s eye, forward-looking view 
of  Senate  elections  using  data  available  well  in 
advance. We believe that if we reliably predict Senate 
elections  well  before  they  happen,  that  has  several 
significant implications for various stakeholders, since 
individual  senators  wield  a  tremendous  amount  of 
legislative power.  

Introduction 

We use: 

ï‚·  A modified version of the LMS algorithm, called 
a discount-weighted least-mean squares algorithm 
to  predict  the  margin  of  victory  of  Senate 
elections. 

ï‚·  An ordinary Support Vector Machine classifier to 

predict the outcome of Senate elections. 

ï‚·  Random  Forest  to  also  predict  the  outcome  of 

Senate elections. 

Data 

Our  data  set  consists  of  all  biennial  Senate  elections 
that were held from 1998 to 2014; this data is publicly 
available. 

Preprocessing:  We  preprocess  the  data  to  weed  out 
elections where: 

ï‚·  There wasnâ€™t exactly one Republican and exactly 

one Democratic candidate. 

ï‚·  A third Party candidate either won the election or 
distorted the election by winning more than 20% 
of  the  vote  (i.e.  a  third  party  candidate  was  a 
significant player). 

After preprocessing, we are left with 273 data points. 
(There were 300 regularly scheduled Senate elections 
in the period 1998-2014, of which 27 were eliminated 
in preprocessing.) 

The  fundamental  challenge  we  face  is  one  of  limited 
data. Senate elections, by their very nature, are limited 
â€“ only around 33 happen every two years. Therefore, 
we had to keep in mind that an inescapable part of this 
project was having limited data.   

Features 

We  use  a  feature  vector  of  71  features.  These  71 
features  include  original  sourced  features  (such  as 
margin  of  victory,  unemployment  rate  etc.)  and 
derived features (such as change in unemployment rate 
over a period of time). 

The features are described below: 

ï‚·  Margin of victory in the Senate election held six 
and  twelve  years  previously.  (Note:  Senators 
serve six-year terms.) 

ï‚·  Margin  of  victory  in  the  state  in  the  last  three 

Presidential Elections 

ï‚·  Presidential Approval in the State. 
ï‚·  Annualized Changes in the Presidential Approval 

in the state. 

ï‚·  Percent  African-American 

population 
extrapolated from the most recent Census) 

ï‚·  Percent  Hispanic/Latino 

population 
extrapolated from the most recent Census) 

(as 

((as 

ï‚·  Changes  in  the  above  demographic  factors  over 

time. 

ï‚·  Three-month average unemployment rates in the 

year before the election.  

ï‚· 

6-month, 12-month, 18-month 24-month changes 
in unemployment rate1 

ï‚·  Partisan  Voting  Index  (PVI)  over  the  past  three 

Presidential elections2  

ï‚·  Change 

in 

the  PVI  from 

the  second-last 

presidential election to the last one. 

ï‚·  Median income in the state. 
ï‚·  Variation in the median income in the state. 
ï‚· 

Indicator variable: Whether Republican candidate 
is the incumbent senator. 

ï‚· 

Indicator variable: Whether Democratic candidate 
is the incumbent senator. 

ï‚·  Number of years of incumbency for the President. 
ï‚· 

Indicator  variable:  Whether  the  election  was  a 
midterm election or not. 

Convention: In all cases, a positive result for the 
Republican is recorded as positive, and vice-versa. 
Example: A reduction of the unemployment rate 
during a Democratic Presidentâ€™s term is means that 
the feature data point is (-), since itâ€™s good for the 
Democrats. 

Cross-Validation 

We use cross-validation frequently through the project. 
Our  perusal  of  literature  suggested  that  a  direct-
application  of  k-fold  cross-validation  was  not 
appropriate  for  time-series  data  â€“  it  would  not  be 
appropriate  to  train  on  2012  data,  for  example,  and 
validate on a hold-out data point that happened before 
2012!  

Hence,  we  use  a  modified  version  called  forward 
chaining.  For  example,  say  we  have  a  training  set 
consisting  of  data  from  years  2000,  2002,  2004  and 
2006; we then design the folds as follows: 

ï‚·  Fold 1: train [2000], hold-out validation [2002] 
ï‚·  Fold  2:  train  [2000,  2002],  hold-out  validation 

[2004] 

ï‚·  Fold  3:  train  [2000,  2002,  2004],  hold-out 

validation [2006] 

                                                           
1 A reduction is positive under incumbent Republican 
President,  while  negative  under  an 
incumbent 
Democratic President; vice-versa for an increase. 

Principal Component Analysis 

Motivation:  clear  interdependencies  between  certain 
variables: PVI and Previous US Presidential election 
result, for example. 

In  order  to  choose  an  appropriate  ğ‘˜-dimension 
spanned by the first ğ‘˜ principal components subspace 
(for  ğ‘˜âˆˆ(1,  10))  and  thus  determine  the  ğ‘˜  principal 
factors,  we  use  the  Scree  Plot  and  the  Cumulative 
Variance Plot. 

(The plot below is for the 183 data points from 2002 
to 2012. First 10 principal components are shown.) 

Scree Plot

1.4
1.2
1
0.8
0.6
0.4
0.2
0

)
0
0
0
'
(
 

l

e
u
a
v
n
e
g
i
E

1

2

4

5

3
Factor/Component

7

6

8

9

10

 

 

Cumulative Variance Plot

e
c
n
a

 

i
r
a
V
e
v
i
t
a
u
m
u
C

l

110.00%

100.00%

90.00%

80.00%

70.00%

60.00%

1 2 3 4 5 6 7 8 9 10

Factor/Component

2  PVI  of  a  state:  On  average,  how  much  more 
Republican  was  the  state  in  the  last  two  presidential 
elections as compared to the nation as a whole) 

Support Vector Machine (SVM) 

Classification 

to  implement  SVM  classification  with  a  Gaussian 
kernel function. 

We  solve  3  classification  problems  using  standard 
SVM classification:  

Results for SVM Classification 

(Training Data Set) 

Years Trained 

Upon 

1998-2008 

2000-2010 

2002-2012 

 

(Test Data Set) 

N 

183 

183 

182 

N Correctly 

Training 

classified 

Error 

179 

180 

178 

2.19% 

1.64% 

2.20% 

Year  

(Using Training 

N 

N Correctly 

classified 

Test Error 

Data from) 

2010  

(1998-2008) 

2012  

(2000-2010) 

2014  

(2002-2012) 

 

30 

30 

30 

27 

26 

27 

10.00 % 

13.33% 

10.00% 

ï‚· 
ï‚· 

ï‚· 

classifying 2014 after learning on 2002-2012, 

classifying 2012 after learning on 2000-2010, 
and 

classifying 2010 after learning on 1998-2008.  

The fundamental motivation behind SVM is carrying 
out  binary  classification  in  a  high-dimension  feature 
space  efficiently,  by  using  the  kernel  trick  (i.e.  by 
mapping  input  data  via  a  non-linear  function).  The 
SVM  algorithm  can  perform 
this  computation 
efficiently  because  it  considers  a  small  number  of 
training points and ignores all training points that are  
close (within a threshold ğœ–) to the model prediction.  

The primal optimization problem is given by: 

 

min   

1
2

ğ‘š

â€–ğ‘¤2â€– + ğ¶ âˆ‘(Î¾ğ‘– + Î¾ğ‘–

âˆ—)
 

ğ‘–=1

subject to:     {

ğ‘¦(ğ‘–)âˆ’ < w, ğ“(ğ‘–) > âˆ’ğ‘ â‰¤ ğœ– + Î¾ğ‘– 
âˆ—
< w, ğ“(ğ‘–) > + ğ‘ âˆ’ ğ‘¦(ğ‘–) â‰¤ ğœ– + Î¾ğ‘–

Î¾ğ‘–,  Î¾ğ‘–

âˆ— â‰¥ 0 

The  norm â€–ğ‘¤2â€– measures  the  flatness  of  the  proxy, 
and the constraints force the model to approximate all 
âˆ— are 
training points within an absolute margin ğœ–. Î¾ğ‘–, Î¾ğ‘–
slack  variables that allow  for  compliance  with  the   ğœ– 
margin constraints and the flatness of the proxy. ğ¶ is 
the penalty for violating the constraints. 

The corresponding dual problem is given by: 

max     âˆ’

1
2

ğ‘š
âˆ‘ (ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
ğ‘–,ğ‘—=1

âˆ—)
âˆ—)(ğ›¼ğ‘— âˆ’ ğ›¼ğ‘—

(< ğ“(ğ‘–), ğ“(ğ‘—) >)

ğ‘š

ğ‘š

âˆ’ ğœ– âˆ‘(ğ›¼ğ‘– + ğ›¼ğ‘–

âˆ—) âˆ’ ğ‘¦(ğ‘–) âˆ‘(ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
âˆ—)
 

ğ‘–=1

ğ‘–=1

subject to:{

ğ‘š
âˆ‘ (ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
ğ‘–=1
0 â‰¤ ğ›¼ğ‘–, ğ›¼ğ‘–

âˆ—) = 1
 
âˆ— â‰¤ ğ¶

The  dual  optimization  is  convex  and  can  easily  be 
solved with optimization software. We use LIBSVM 

Discount-Weighted Least-Means Square 

 

Regression 

 
Once again, we solve three regression problems for the 
years 2010, 2012 and 2014. 

Given that the composition and voting intentions of a 
state evolve rapidly, we thought it would be beneficial 
to give less weight to earlier training data as compared 
to later ones. 

The basic premise of this time discount rate algorithm, 
which  has  been  adapted  from  Harrison  and Johnston 
[5], is to use a â€˜discount factorâ€™ which conveys the rate 
of decay of the information content of an observation. 

The  discount-weighted  LMS  algorithm  had  a  lower 
generalization  error  than  a  standard  LMS  algorithm 
when forward-chaining cross-validation was used. 

We used a discount factor of the form: 

ğ›¿ğ‘¡ =

âˆš2ğ›¼ğ‘¡ âˆ’ 1
2ğ›¼ğ‘¡ + 1

â„

âˆš2ğ›¼ğ‘‡ âˆ’ 1
2ğ›¼ğ‘‡ + 1

Random Forests 

 

We  also  implemented  Random  Forests  classification 
on the original data set. 

Where ğ›¿1 is  the  discount  factor  for  the  earliest  time 
period  and  T  is  the  number  of  time  periods  (i.e. ğ‘¡ =
1,   â€¦ . ,  ğ‘‡ ). ğ›¼ is  a  parameter  than  can  be  optimized. 
Clearly, ğ›¿ğ‘‡ is always = 1.  

Discount factor for various alphas is shown below: 

Random forests use decision trees as the basic building 
block to enable prediction. A decision tree uses a tree-
like graph or model of decisions to split up the feature 
space into separate regions. Each data point falls into 
exactly  one  region,  and  in  the  case  of  classification, 
the most common class is the predicted class. 

Discount factors for various alphas 

)
a
t
l
e
d
(
 
r
o
t
c
a
f
 
t
n
u
o
c
s
i
D

1.1

1

0.9

0.8

0.7

0.6

0.5

 

(T = 5)

1

2

3

4

5

Time period (t)

alpha=1

alpha=2

alpha=3

alpha=4

Results for Discount Weighted LMS 

(Training Data Set) 

Years 
Trained 
Upon 

N 

Mean 
margin 
of error 

N 

Correctly 
classified 

1998-2008  183 

4.20% 

179 

Training 

Classification 

Error 

2.19% 

2000-2010  183 

4.61% 

180 

1.64% 

2002-2012  182 

4.43% 

178 

2.20% 

 

(Test Data Set) 

Year  
(Using 

Training Data 

N 

from) 
2010  

(1998-2008) 

2012  

(2000-2010) 

2014  

(2002-2012) 

Mean 
margin 
of error 

N 

Test 

Correctly 
classified 

Classifica
tion Error 

30 

7.48% 

30 

6.52% 

30 

7.10% 

27 

26 

27 

10.00 % 

13.33% 

10.00% 

Random  forests  use  multiple  decision  trees,  and  the 
reasoning  behind  this  is  to  reduce  the  chances  of 
overfitting to the data. Each tree is built on a separate 
dataset  where  each  dataset  is  sampled  from  the 
original distribution. However, since we do not know, 
or  have  access  to,  the  original  distribution,  we  build 
each dataset by sampling with replacement using the 
is  known  as  bootstrap 
original  dataset.  This 
aggregation,  since  we  now  have  multiple  decision 
trees  which  are  all  fit  to  an  approximation  of  the 
original  distribution.  By  using  multiple  trees  we  can 
lower  the  variance  of  the  model  at  the  cost  of 
increasing the bias.  

Although  bootstrap  aggregation  helps  to  reduce  the 
variance  of  the  model,  it  does  not  fix  an  important 
problem  which  is  that  every  tree  may  be  highly 
correlated to each other. In that case, it does not matter 
how  many  trees  we  average  our  predictions  over  if 
each tree is exactly the same, since the variance of the 
model  will  not  decrease  at  all.  In  order  to  prevent 
highly  similar  trees,  we  will  only  consider  a  random 
subset of the features at each split. Often the number 
of  features  considered,  ğ‘š ,  is  much  lower  than  ğ‘ , 
where ğ‘ is the original number of predictors.  

There  are  two  parameters  to  tune  over  in  random 
forests: ğµ, the number of decision trees to create, and 
ğ‘š, the number of predictors to consider at each split. 
Increasing ğµ will prevent the model from overfitting, 
but  may  also  prevent  accurately  capturing 
the 
relationship between the training data and the output. 
Increasing ğ‘š will increase the chances of overfitting, 
but  may  allow  a  better  fit  to  the  training  data. 
Appropriate  choices  for ğµ and ğ‘š can  be  selected  by 
using cross validation. Choices for ğµ and ğ‘š that were 
optimal  in  our  three  tests  hovered  around ğ‘© â‰ˆ100 
and ğ’ â‰ˆ ğ’‘/7 â‰ˆ10.   

Results for Random Forests 

 (Test Data Set) 

Year  

(Using Training 

N 

Data from) 

2010  

(1998-2008) 

2012  

(2000-2010) 

2014  

(2002-2012) 

Conclusions 

30 

30 

30 

 

N Correctly 

classified 

Test Error 

28 

27 

27 

6.67 % 

10.00% 

10.00% 

Random  Forests  clearly  works  better  than  the  SVM 
classifier while attempting binary classification with a 
small  number  of  data  points  (and  hence  a  high 
possibility of over-fitting). The average classification 
test error rate for Random Forests is 8.9%, while for 
the other two algorithms it is 11.1%.  

Most importantly, we conclude that we predicted the 
2010,  2012  and  2014  Senate  elections  with  a 
reasonable  amount  of  accuracy  with  data  that  was 

mostly available at least two years in advance of those 
elections. That is, except for unemployment statistics 
(for which we can use forecasts), we have enough data 
to predict the 2016 election too (we do just that in the 
Appendix)!  

While a lot of attention is directed towards Presidential 
Elections, individual Senators have tremendous power 
over  legislation.  Therefore,  we  believe  that  having  a 
birdâ€™s eye estimation of what the Senate might shape 
up to be two years in the future could be very useful 
for a lot of stakeholders, such as: 

ï‚·  Stakeholders in key bills: If Senator X loses, will 

the â€˜AJKL billâ€™ fail in the next Congress? 

ï‚·  Lobbyists: Can the threat of being vulnerable help 

persuade Senator X to support Z? 

ï‚·  Speculators:  Can I  shape  my  investments  with a 
reasonable  amount  of  confidence  in  having  a 
Republican/ Democratic Senate 2 years from now? 

 

 

ï‚·  Party  machinery:  Senator  Z  is  vulnerable.  We 
must  begin  directing  resources  towards  his/her 
campaign IMMEDIATELY. 

And  therein  lies  the  practical  utility  of  our  exercise. 
Weâ€™re  excited  that  we  were  able  to  get  reasonably 
good results with publicly available data and machine-
learning  approaches  â€“  clearly,  elections  can  be 
predictable!  Weâ€™re  eager  to  build  on  some  of  these 
approaches,  especially  Random  Forest,  and  explore 
new techniques as well. 

 

Data Sources 

All data is publicly available: 

ï‚·  Election  Results  are  sourced  from  the  Federal 

Election Commission website (www.fec.gov) 

ï‚·  Unemployment  Rate  Statistics  are  sourced  from 

the Bureau of Labor Statistics (www.bls.gov) 

ï‚·  Demographic  Statistics  are  sourced  from  the 

United States Census bureau (www.census.gov) 

 

References 

[1] Drucker, H., Burges, C. J., Kaufman, L., Smola, 
A., & Vapnik, V. (1997). Support vector regression 
machines. Advances in neural information processing 
systems, 9, 155-161. 

[2] Basak, D., Pal, S., & Patranabis, D. C. (2007). 
Support vector regression. Neural Information 
Processing-Letters and Reviews, 11(10), 203-224. 

[3] J. Smolatand Bernhard Scholkof. A tutorial on 
support vector regression. 2004 

[4]  Friedman,  Jerome,  Trevor  Hastie,  and  Robert 
Tibshirani. The elements of statistical learning. Vol. 1. 
New York: Springer Series in Statistics, 2001. 

[5]  Harrison,  P.  J.,  and  F.  R.  Johnston.  "Discount 
weighted  regression."  Journal  of  the  Operational 
Research Society (1984): 923-932. 

Classification and Regression Approaches to Predicting United States Senate Elections 

Rohan Sampath, Yue Teng 

 

Abstract 

The  United  States  Senate  is  arguably  the  finest 
democratic  institution  for  debate  and deliberation  in 
the world. It also provides a fascinating opportunity to 
understand  the  complex  dynamics  that  go  into 
determining  the  outcome  of  Senate  elections,  using 
Machine Learning.  

Motivation 

Are  elections  decided  even  before  they  begin?  Can 
political  fundamentals  predict  elections  regardless  of 
candidates/campaigns? 

Our goal is to get a birdâ€™s eye, forward-looking view 
of  Senate  elections  using  data  available  well  in 
advance. We believe that if we reliably predict Senate 
elections  well  before  they  happen,  that  has  several 
significant implications for various stakeholders, since 
individual  senators  wield  a  tremendous  amount  of 
legislative power.  

Introduction 

We use: 

ï‚·  A modified version of the LMS algorithm, called 
a discount-weighted least-mean squares algorithm 
to  predict  the  margin  of  victory  of  Senate 
elections. 

ï‚·  An ordinary Support Vector Machine classifier to 

predict the outcome of Senate elections. 

ï‚·  Random  Forest  to  also  predict  the  outcome  of 

Senate elections. 

Data 

Our  data  set  consists  of  all  biennial  Senate  elections 
that were held from 1998 to 2014; this data is publicly 
available. 

Preprocessing:  We  preprocess  the  data  to  weed  out 
elections where: 

ï‚·  There wasnâ€™t exactly one Republican and exactly 

one Democratic candidate. 

ï‚·  A third Party candidate either won the election or 
distorted the election by winning more than 20% 
of  the  vote  (i.e.  a  third  party  candidate  was  a 
significant player). 

After preprocessing, we are left with 273 data points. 
(There were 300 regularly scheduled Senate elections 
in the period 1998-2014, of which 27 were eliminated 
in preprocessing.) 

The  fundamental  challenge  we  face  is  one  of  limited 
data. Senate elections, by their very nature, are limited 
â€“ only around 33 happen every two years. Therefore, 
we had to keep in mind that an inescapable part of this 
project was having limited data.   

Features 

We  use  a  feature  vector  of  71  features.  These  71 
features  include  original  sourced  features  (such  as 
margin  of  victory,  unemployment  rate  etc.)  and 
derived features (such as change in unemployment rate 
over a period of time). 

The features are described below: 

ï‚·  Margin of victory in the Senate election held six 
and  twelve  years  previously.  (Note:  Senators 
serve six-year terms.) 

ï‚·  Margin  of  victory  in  the  state  in  the  last  three 

Presidential Elections 

ï‚·  Presidential Approval in the State. 
ï‚·  Annualized Changes in the Presidential Approval 

in the state. 

ï‚·  Percent  African-American 

population 
extrapolated from the most recent Census) 

ï‚·  Percent  Hispanic/Latino 

population 
extrapolated from the most recent Census) 

(as 

((as 

ï‚·  Changes  in  the  above  demographic  factors  over 

time. 

ï‚·  Three-month average unemployment rates in the 

year before the election.  

ï‚· 

6-month, 12-month, 18-month 24-month changes 
in unemployment rate1 

ï‚·  Partisan  Voting  Index  (PVI)  over  the  past  three 

Presidential elections2  

ï‚·  Change 

in 

the  PVI  from 

the  second-last 

presidential election to the last one. 

ï‚·  Median income in the state. 
ï‚·  Variation in the median income in the state. 
ï‚· 

Indicator variable: Whether Republican candidate 
is the incumbent senator. 

ï‚· 

Indicator variable: Whether Democratic candidate 
is the incumbent senator. 

ï‚·  Number of years of incumbency for the President. 
ï‚· 

Indicator  variable:  Whether  the  election  was  a 
midterm election or not. 

Convention: In all cases, a positive result for the 
Republican is recorded as positive, and vice-versa. 
Example: A reduction of the unemployment rate 
during a Democratic Presidentâ€™s term is means that 
the feature data point is (-), since itâ€™s good for the 
Democrats. 

Cross-Validation 

We use cross-validation frequently through the project. 
Our  perusal  of  literature  suggested  that  a  direct-
application  of  k-fold  cross-validation  was  not 
appropriate  for  time-series  data  â€“  it  would  not  be 
appropriate  to  train  on  2012  data,  for  example,  and 
validate on a hold-out data point that happened before 
2012!  

Hence,  we  use  a  modified  version  called  forward 
chaining.  For  example,  say  we  have  a  training  set 
consisting  of  data  from  years  2000,  2002,  2004  and 
2006; we then design the folds as follows: 

ï‚·  Fold 1: train [2000], hold-out validation [2002] 
ï‚·  Fold  2:  train  [2000,  2002],  hold-out  validation 

[2004] 

ï‚·  Fold  3:  train  [2000,  2002,  2004],  hold-out 

validation [2006] 

                                                           
1 A reduction is positive under incumbent Republican 
President,  while  negative  under  an 
incumbent 
Democratic President; vice-versa for an increase. 

Principal Component Analysis 

Motivation:  clear  interdependencies  between  certain 
variables: PVI and Previous US Presidential election 
result, for example. 

In  order  to  choose  an  appropriate  ğ‘˜-dimension 
spanned by the first ğ‘˜ principal components subspace 
(for  ğ‘˜âˆˆ(1,  10))  and  thus  determine  the  ğ‘˜  principal 
factors,  we  use  the  Scree  Plot  and  the  Cumulative 
Variance Plot. 

(The plot below is for the 183 data points from 2002 
to 2012. First 10 principal components are shown.) 

Scree Plot

1.4
1.2
1
0.8
0.6
0.4
0.2
0

)
0
0
0
'
(
 

l

e
u
a
v
n
e
g
i
E

1

2

4

5

3
Factor/Component

7

6

8

9

10

 

 

Cumulative Variance Plot

e
c
n
a

 

i
r
a
V
e
v
i
t
a
u
m
u
C

l

110.00%

100.00%

90.00%

80.00%

70.00%

60.00%

1 2 3 4 5 6 7 8 9 10

Factor/Component

2  PVI  of  a  state:  On  average,  how  much  more 
Republican  was  the  state  in  the  last  two  presidential 
elections as compared to the nation as a whole) 

Support Vector Machine (SVM) 

Classification 

to  implement  SVM  classification  with  a  Gaussian 
kernel function. 

We  solve  3  classification  problems  using  standard 
SVM classification:  

Results for SVM Classification 

(Training Data Set) 

Years Trained 

Upon 

1998-2008 

2000-2010 

2002-2012 

 

(Test Data Set) 

N 

183 

183 

182 

N Correctly 

Training 

classified 

Error 

179 

180 

178 

2.19% 

1.64% 

2.20% 

Year  

(Using Training 

N 

N Correctly 

classified 

Test Error 

Data from) 

2010  

(1998-2008) 

2012  

(2000-2010) 

2014  

(2002-2012) 

 

30 

30 

30 

27 

26 

27 

10.00 % 

13.33% 

10.00% 

ï‚· 
ï‚· 

ï‚· 

classifying 2014 after learning on 2002-2012, 

classifying 2012 after learning on 2000-2010, 
and 

classifying 2010 after learning on 1998-2008.  

The fundamental motivation behind SVM is carrying 
out  binary  classification  in  a  high-dimension  feature 
space  efficiently,  by  using  the  kernel  trick  (i.e.  by 
mapping  input  data  via  a  non-linear  function).  The 
SVM  algorithm  can  perform 
this  computation 
efficiently  because  it  considers  a  small  number  of 
training points and ignores all training points that are  
close (within a threshold ğœ–) to the model prediction.  

The primal optimization problem is given by: 

 

min   

1
2

ğ‘š

â€–ğ‘¤2â€– + ğ¶ âˆ‘(Î¾ğ‘– + Î¾ğ‘–

âˆ—)
 

ğ‘–=1

subject to:     {

ğ‘¦(ğ‘–)âˆ’ < w, ğ“(ğ‘–) > âˆ’ğ‘ â‰¤ ğœ– + Î¾ğ‘– 
âˆ—
< w, ğ“(ğ‘–) > + ğ‘ âˆ’ ğ‘¦(ğ‘–) â‰¤ ğœ– + Î¾ğ‘–

Î¾ğ‘–,  Î¾ğ‘–

âˆ— â‰¥ 0 

The  norm â€–ğ‘¤2â€– measures  the  flatness  of  the  proxy, 
and the constraints force the model to approximate all 
âˆ— are 
training points within an absolute margin ğœ–. Î¾ğ‘–, Î¾ğ‘–
slack  variables that allow  for  compliance  with  the   ğœ– 
margin constraints and the flatness of the proxy. ğ¶ is 
the penalty for violating the constraints. 

The corresponding dual problem is given by: 

max     âˆ’

1
2

ğ‘š
âˆ‘ (ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
ğ‘–,ğ‘—=1

âˆ—)
âˆ—)(ğ›¼ğ‘— âˆ’ ğ›¼ğ‘—

(< ğ“(ğ‘–), ğ“(ğ‘—) >)

ğ‘š

ğ‘š

âˆ’ ğœ– âˆ‘(ğ›¼ğ‘– + ğ›¼ğ‘–

âˆ—) âˆ’ ğ‘¦(ğ‘–) âˆ‘(ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
âˆ—)
 

ğ‘–=1

ğ‘–=1

subject to:{

ğ‘š
âˆ‘ (ğ›¼ğ‘– âˆ’ ğ›¼ğ‘–
ğ‘–=1
0 â‰¤ ğ›¼ğ‘–, ğ›¼ğ‘–

âˆ—) = 1
 
âˆ— â‰¤ ğ¶

The  dual  optimization  is  convex  and  can  easily  be 
solved with optimization software. We use LIBSVM 

Discount-Weighted Least-Means Square 

 

Regression 

 
Once again, we solve three regression problems for the 
years 2010, 2012 and 2014. 

Given that the composition and voting intentions of a 
state evolve rapidly, we thought it would be beneficial 
to give less weight to earlier training data as compared 
to later ones. 

The basic premise of this time discount rate algorithm, 
which  has  been  adapted  from  Harrison  and Johnston 
[5], is to use a â€˜discount factorâ€™ which conveys the rate 
of decay of the information content of an observation. 

The  discount-weighted  LMS  algorithm  had  a  lower 
generalization  error  than  a  standard  LMS  algorithm 
when forward-chaining cross-validation was used. 

We used a discount factor of the form: 

ğ›¿ğ‘¡ =

âˆš2ğ›¼ğ‘¡ âˆ’ 1
2ğ›¼ğ‘¡ + 1

â„

âˆš2ğ›¼ğ‘‡ âˆ’ 1
2ğ›¼ğ‘‡ + 1

Random Forests 

 

We  also  implemented  Random  Forests  classification 
on the original data set. 

Where ğ›¿1 is  the  discount  factor  for  the  earliest  time 
period  and  T  is  the  number  of  time  periods  (i.e. ğ‘¡ =
1,   â€¦ . ,  ğ‘‡ ). ğ›¼ is  a  parameter  than  can  be  optimized. 
Clearly, ğ›¿ğ‘‡ is always = 1.  

Discount factor for various alphas is shown below: 

Random forests use decision trees as the basic building 
block to enable prediction. A decision tree uses a tree-
like graph or model of decisions to split up the feature 
space into separate regions. Each data point falls into 
exactly  one  region,  and  in  the  case  of  classification, 
the most common class is the predicted class. 

Discount factors for various alphas 

)
a
t
l
e
d
(
 
r
o
t
c
a
f
 
t
n
u
o
c
s
i
D

1.1

1

0.9

0.8

0.7

0.6

0.5

 

(T = 5)

1

2

3

4

5

Time period (t)

alpha=1

alpha=2

alpha=3

alpha=4

Results for Discount Weighted LMS 

(Training Data Set) 

Years 
Trained 
Upon 

N 

Mean 
margin 
of error 

N 

Correctly 
classified 

1998-2008  183 

4.20% 

179 

Training 

Classification 

Error 

2.19% 

2000-2010  183 

4.61% 

180 

1.64% 

2002-2012  182 

4.43% 

178 

2.20% 

 

(Test Data Set) 

Year  
(Using 

Training Data 

N 

from) 
2010  

(1998-2008) 

2012  

(2000-2010) 

2014  

(2002-2012) 

Mean 
margin 
of error 

N 

Test 

Correctly 
classified 

Classifica
tion Error 

30 

7.48% 

30 

6.52% 

30 

7.10% 

27 

26 

27 

10.00 % 

13.33% 

10.00% 

Random  forests  use  multiple  decision  trees,  and  the 
reasoning  behind  this  is  to  reduce  the  chances  of 
overfitting to the data. Each tree is built on a separate 
dataset  where  each  dataset  is  sampled  from  the 
original distribution. However, since we do not know, 
or  have  access  to,  the  original  distribution,  we  build 
each dataset by sampling with replacement using the 
is  known  as  bootstrap 
original  dataset.  This 
aggregation,  since  we  now  have  multiple  decision 
trees  which  are  all  fit  to  an  approximation  of  the 
original  distribution.  By  using  multiple  trees  we  can 
lower  the  variance  of  the  model  at  the  cost  of 
increasing the bias.  

Although  bootstrap  aggregation  helps  to  reduce  the 
variance  of  the  model,  it  does  not  fix  an  important 
problem  which  is  that  every  tree  may  be  highly 
correlated to each other. In that case, it does not matter 
how  many  trees  we  average  our  predictions  over  if 
each tree is exactly the same, since the variance of the 
model  will  not  decrease  at  all.  In  order  to  prevent 
highly  similar  trees,  we  will  only  consider  a  random 
subset of the features at each split. Often the number 
of  features  considered,  ğ‘š ,  is  much  lower  than  ğ‘ , 
where ğ‘ is the original number of predictors.  

There  are  two  parameters  to  tune  over  in  random 
forests: ğµ, the number of decision trees to create, and 
ğ‘š, the number of predictors to consider at each split. 
Increasing ğµ will prevent the model from overfitting, 
but  may  also  prevent  accurately  capturing 
the 
relationship between the training data and the output. 
Increasing ğ‘š will increase the chances of overfitting, 
but  may  allow  a  better  fit  to  the  training  data. 
Appropriate  choices  for ğµ and ğ‘š can  be  selected  by 
using cross validation. Choices for ğµ and ğ‘š that were 
optimal  in  our  three  tests  hovered  around ğ‘© â‰ˆ100 
and ğ’ â‰ˆ ğ’‘/7 â‰ˆ10.   

Results for Random Forests 

 (Test Data Set) 

Year  

(Using Training 

N 

Data from) 

2010  

(1998-2008) 

2012  

(2000-2010) 

2014  

(2002-2012) 

Conclusions 

30 

30 

30 

 

N Correctly 

classified 

Test Error 

28 

27 

27 

6.67 % 

10.00% 

10.00% 

Random  Forests  clearly  works  better  than  the  SVM 
classifier while attempting binary classification with a 
small  number  of  data  points  (and  hence  a  high 
possibility of over-fitting). The average classification 
test error rate for Random Forests is 8.9%, while for 
the other two algorithms it is 11.1%.  

Most importantly, we conclude that we predicted the 
2010,  2012  and  2014  Senate  elections  with  a 
reasonable  amount  of  accuracy  with  data  that  was 

mostly available at least two years in advance of those 
elections. That is, except for unemployment statistics 
(for which we can use forecasts), we have enough data 
to predict the 2016 election too (we do just that in the 
Appendix)!  

While a lot of attention is directed towards Presidential 
Elections, individual Senators have tremendous power 
over  legislation.  Therefore,  we  believe  that  having  a 
birdâ€™s eye estimation of what the Senate might shape 
up to be two years in the future could be very useful 
for a lot of stakeholders, such as: 

ï‚·  Stakeholders in key bills: If Senator X loses, will 

the â€˜AJKL billâ€™ fail in the next Congress? 

ï‚·  Lobbyists: Can the threat of being vulnerable help 

persuade Senator X to support Z? 

ï‚·  Speculators:  Can I  shape  my  investments  with a 
reasonable  amount  of  confidence  in  having  a 
Republican/ Democratic Senate 2 years from now? 

 

 

ï‚·  Party  machinery:  Senator  Z  is  vulnerable.  We 
must  begin  directing  resources  towards  his/her 
campaign IMMEDIATELY. 

And  therein  lies  the  practical  utility  of  our  exercise. 
Weâ€™re  excited  that  we  were  able  to  get  reasonably 
good results with publicly available data and machine-
learning  approaches  â€“  clearly,  elections  can  be 
predictable!  Weâ€™re  eager  to  build  on  some  of  these 
approaches,  especially  Random  Forest,  and  explore 
new techniques as well. 

 

Data Sources 

All data is publicly available: 

ï‚·  Election  Results  are  sourced  from  the  Federal 

Election Commission website (www.fec.gov) 

ï‚·  Unemployment  Rate  Statistics  are  sourced  from 

the Bureau of Labor Statistics (www.bls.gov) 

ï‚·  Demographic  Statistics  are  sourced  from  the 

United States Census bureau (www.census.gov) 

 

References 

[1] Drucker, H., Burges, C. J., Kaufman, L., Smola, 
A., & Vapnik, V. (1997). Support vector regression 
machines. Advances in neural information processing 
systems, 9, 155-161. 

[2] Basak, D., Pal, S., & Patranabis, D. C. (2007). 
Support vector regression. Neural Information 
Processing-Letters and Reviews, 11(10), 203-224. 

[3] J. Smolatand Bernhard Scholkof. A tutorial on 
support vector regression. 2004 

[4]  Friedman,  Jerome,  Trevor  Hastie,  and  Robert 
Tibshirani. The elements of statistical learning. Vol. 1. 
New York: Springer Series in Statistics, 2001. 

[5]  Harrison,  P.  J.,  and  F.  R.  Johnston.  "Discount 
weighted  regression."  Journal  of  the  Operational 
Research Society (1984): 923-932. 

Appendix: Our Prediction for the 2016 Senate Elections 

 

The Republicans lose two seats, but hold on to the Senate, 52-48! 

 

 

