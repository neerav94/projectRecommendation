On the Recognition of Handwritten Math Equations

Quan Nguyen, Maximillian Wang, Le Cheng Fan

December 12, 2014

1

Introduction

1.1 Background

Handwriting recognition is a classic machine learning problem that has led to extensive research and a
number of highly accurate solutions. Some handwriting recognition algorithms are achieving near-human
level performance.

We employ tools used in handwriting recognition to handwritten math equations. While typing is gener-
ally much faster than writing by hand, math equations have the opposite property: writing equations by hand
is more eﬃcient than typesetting them. Furthermore, whereas handwritten equations are human-readable,
the “code” of typesetting languages are often highly nested and diﬃcult to edit. These problems of type-
setting present an opportunity for improving the workﬂow of writing math equations digitally. In addition,
recognizing mathematical equations is important for digitalizing past scientiﬁc works.

1.2 Goals and Outline

Though typesetting equations is diﬃcult, storing equations in typeset form allows for concise storage that
is relatively ﬂexible for future editing. Since LATEXis the most popular typesetting tool, our team’s long-
term goal is to produce a system to recognize images of handwritten equations and output the corresponding
characters in LATEX. Under the given time constraints, building the full system is infeasible, so our team chose
a more reasonable goal of constructing a system for recognizing individual math symbols, with the possibility
of building some functionality necessary to recognize multi-symbol expression. In the next section, we discuss
the data collection process to obtain a wide array of characters. Afterward, we discuss preprocessing and
algorithmic methods used to implement the system. Finally, we discuss our results and compare each of the
diﬀerent approaches we took.

2 Methods

2.1 Process

We run all of our images for both training and testing through a sequence of steps in order to improve the
eﬀectiveness of the system as a whole.

We begin by reading in each image and preprocessing it as outlined below in order to reduce the variation
between diﬀerent examples. This allows the system to create a much clearer picture of the distinctions
between diﬀerent labels. The next step is to detect features in the images and place them in a feature vector.
Finally, we train and test several diﬀerent machine learning models on our generated features.

We use a ”pipeline analysis” approach in our research. Since our system consists of a series of separate
steps, the overall performance is aﬀected by several factors. At each step of the process, we compare diﬀerent
methods used for preprocessing, feature detection, and learning. In doing so, we can achieve a more complete
understanding of how each step aﬀects the system. (See Figure 1)

1

On the Recognition of Handwritten Math Equations

Quan Nguyen, Maximillian Wang, Le Cheng Fan

December 12, 2014

1

Introduction

1.1 Background

Handwriting recognition is a classic machine learning problem that has led to extensive research and a
number of highly accurate solutions. Some handwriting recognition algorithms are achieving near-human
level performance.

We employ tools used in handwriting recognition to handwritten math equations. While typing is gener-
ally much faster than writing by hand, math equations have the opposite property: writing equations by hand
is more eﬃcient than typesetting them. Furthermore, whereas handwritten equations are human-readable,
the “code” of typesetting languages are often highly nested and diﬃcult to edit. These problems of type-
setting present an opportunity for improving the workﬂow of writing math equations digitally. In addition,
recognizing mathematical equations is important for digitalizing past scientiﬁc works.

1.2 Goals and Outline

Though typesetting equations is diﬃcult, storing equations in typeset form allows for concise storage that
is relatively ﬂexible for future editing. Since LATEXis the most popular typesetting tool, our team’s long-
term goal is to produce a system to recognize images of handwritten equations and output the corresponding
characters in LATEX. Under the given time constraints, building the full system is infeasible, so our team chose
a more reasonable goal of constructing a system for recognizing individual math symbols, with the possibility
of building some functionality necessary to recognize multi-symbol expression. In the next section, we discuss
the data collection process to obtain a wide array of characters. Afterward, we discuss preprocessing and
algorithmic methods used to implement the system. Finally, we discuss our results and compare each of the
diﬀerent approaches we took.

2 Methods

2.1 Process

We run all of our images for both training and testing through a sequence of steps in order to improve the
eﬀectiveness of the system as a whole.

We begin by reading in each image and preprocessing it as outlined below in order to reduce the variation
between diﬀerent examples. This allows the system to create a much clearer picture of the distinctions
between diﬀerent labels. The next step is to detect features in the images and place them in a feature vector.
Finally, we train and test several diﬀerent machine learning models on our generated features.

We use a ”pipeline analysis” approach in our research. Since our system consists of a series of separate
steps, the overall performance is aﬀected by several factors. At each step of the process, we compare diﬀerent
methods used for preprocessing, feature detection, and learning. In doing so, we can achieve a more complete
understanding of how each step aﬀects the system. (See Figure 1)

1

Figure 1: Pipeline steps

2.2 Software

Our main tool for performing all preprocessing, feature detection, and learning algorithms is the OpenCV
library for C++. This software package includes utilities for every computer vision and Machine learning
functionality that we require for this project.

2.3 Data Set

All the training and testing data we use are handwritten Greek letters produced in Photoshop with varied
brush sizes and hardness values. The dataset consists of 128x128 grayscale images of the following letters:
α, β, λ, µ, φ, π, and ζ. There are 100 images of each Greek letter, and hold-out cross validation is used to
separate the data set into training/testing sets and to compare training and generalization error.

In order to reduce variations between images, we implement two diﬀerent preprocessing techniques. In
M10
M00

the ﬁrst method, we calculate the centroid, (¯x, ¯y), of the image via moments. These are given by ¯x =

and ¯y =

M01
M00

, where

Mpq =

128(cid:88)

128(cid:88)

xp · yq · ImageIntensity(x, y).

x=1

y=1

Once we ﬁnd the centroid, we then translate the entire image inside the 128x128 square such that the centroid
lies on the center of the image, namely at (64, 64). This corrects for diﬀerences in symbol positioning in the
hand-drawn images.

Our second technique is to eliminate as much background ”white space” in the image as possible. This is
done by drawing a bounding box around the actual symbol, and then expanding that section of the image to
ﬁll the entire square. The technique corrects for size diﬀerences in each image. It also adjusts for positioning
since the end result is tightly snug in the 128x128 square. Since the centroid of the original image is generally
not the same as the centroid of the blown-up image, we only perform one of these preprocessing techniques at
a given time. We also run the system without preprocessing to see how the techniques aﬀect the performance.
(See Figure 2)

2.4 Feature Detection

We compare two diﬀerent types of feature vectors for our learning models. The ﬁrst is treating the grayscale
image as a matrix of pixel values, and then reformatting this matrix into a single row vector. This vector
thus encodes the each of the 128 × 128 pixel values of the image. The second type of feature detection
method we use is the Harris operator for detecting corners in an image. We use an abstracted OpenCV
implementation which assigns a ”conﬁdence level” for each of the detected corners, and then returns a vector
of the coordinates of the k top scores. In our research, we compare the performance when using 5 corners, 7
corners, and raw pixel values.

2

On the Recognition of Handwritten Math Equations

Quan Nguyen, Maximillian Wang, Le Cheng Fan

December 12, 2014

1

Introduction

1.1 Background

Handwriting recognition is a classic machine learning problem that has led to extensive research and a
number of highly accurate solutions. Some handwriting recognition algorithms are achieving near-human
level performance.

We employ tools used in handwriting recognition to handwritten math equations. While typing is gener-
ally much faster than writing by hand, math equations have the opposite property: writing equations by hand
is more eﬃcient than typesetting them. Furthermore, whereas handwritten equations are human-readable,
the “code” of typesetting languages are often highly nested and diﬃcult to edit. These problems of type-
setting present an opportunity for improving the workﬂow of writing math equations digitally. In addition,
recognizing mathematical equations is important for digitalizing past scientiﬁc works.

1.2 Goals and Outline

Though typesetting equations is diﬃcult, storing equations in typeset form allows for concise storage that
is relatively ﬂexible for future editing. Since LATEXis the most popular typesetting tool, our team’s long-
term goal is to produce a system to recognize images of handwritten equations and output the corresponding
characters in LATEX. Under the given time constraints, building the full system is infeasible, so our team chose
a more reasonable goal of constructing a system for recognizing individual math symbols, with the possibility
of building some functionality necessary to recognize multi-symbol expression. In the next section, we discuss
the data collection process to obtain a wide array of characters. Afterward, we discuss preprocessing and
algorithmic methods used to implement the system. Finally, we discuss our results and compare each of the
diﬀerent approaches we took.

2 Methods

2.1 Process

We run all of our images for both training and testing through a sequence of steps in order to improve the
eﬀectiveness of the system as a whole.

We begin by reading in each image and preprocessing it as outlined below in order to reduce the variation
between diﬀerent examples. This allows the system to create a much clearer picture of the distinctions
between diﬀerent labels. The next step is to detect features in the images and place them in a feature vector.
Finally, we train and test several diﬀerent machine learning models on our generated features.

We use a ”pipeline analysis” approach in our research. Since our system consists of a series of separate
steps, the overall performance is aﬀected by several factors. At each step of the process, we compare diﬀerent
methods used for preprocessing, feature detection, and learning. In doing so, we can achieve a more complete
understanding of how each step aﬀects the system. (See Figure 1)

1

Figure 1: Pipeline steps

2.2 Software

Our main tool for performing all preprocessing, feature detection, and learning algorithms is the OpenCV
library for C++. This software package includes utilities for every computer vision and Machine learning
functionality that we require for this project.

2.3 Data Set

All the training and testing data we use are handwritten Greek letters produced in Photoshop with varied
brush sizes and hardness values. The dataset consists of 128x128 grayscale images of the following letters:
α, β, λ, µ, φ, π, and ζ. There are 100 images of each Greek letter, and hold-out cross validation is used to
separate the data set into training/testing sets and to compare training and generalization error.

In order to reduce variations between images, we implement two diﬀerent preprocessing techniques. In
M10
M00

the ﬁrst method, we calculate the centroid, (¯x, ¯y), of the image via moments. These are given by ¯x =

and ¯y =

M01
M00

, where

Mpq =

128(cid:88)

128(cid:88)

xp · yq · ImageIntensity(x, y).

x=1

y=1

Once we ﬁnd the centroid, we then translate the entire image inside the 128x128 square such that the centroid
lies on the center of the image, namely at (64, 64). This corrects for diﬀerences in symbol positioning in the
hand-drawn images.

Our second technique is to eliminate as much background ”white space” in the image as possible. This is
done by drawing a bounding box around the actual symbol, and then expanding that section of the image to
ﬁll the entire square. The technique corrects for size diﬀerences in each image. It also adjusts for positioning
since the end result is tightly snug in the 128x128 square. Since the centroid of the original image is generally
not the same as the centroid of the blown-up image, we only perform one of these preprocessing techniques at
a given time. We also run the system without preprocessing to see how the techniques aﬀect the performance.
(See Figure 2)

2.4 Feature Detection

We compare two diﬀerent types of feature vectors for our learning models. The ﬁrst is treating the grayscale
image as a matrix of pixel values, and then reformatting this matrix into a single row vector. This vector
thus encodes the each of the 128 × 128 pixel values of the image. The second type of feature detection
method we use is the Harris operator for detecting corners in an image. We use an abstracted OpenCV
implementation which assigns a ”conﬁdence level” for each of the detected corners, and then returns a vector
of the coordinates of the k top scores. In our research, we compare the performance when using 5 corners, 7
corners, and raw pixel values.

2

(a) Original

(b) Centered

(c) Resized

Figure 2: Data set with preprocessing

3 Models

We use three diﬀerent learning algorithms in our project: Support Vector Machines, K-Nearest-Neighbors,
and Random Forest. Here we outline each of these algorithms.

3.1 Support Vector Machine

A support vector machine attempts to ﬁnd a hyperplane that divides training examples of two diﬀerent types,
such that the distance between the plane and each of the training examples is maximized. When training
data is linearly separable, this hyperplane will perfectly divide the points into groups by label. However, in
most cases, data is not perfectly separable, so L2 regularization is used. More precisely, ﬁnd

maxα

ai − 1
2

y(i)y(j)αiαj(cid:104)x(i)x(j)(cid:105)

m(cid:88)

i,j=1

m(cid:88)
m(cid:88)

i=1

i=1

m(cid:88)

αiy(i)x(i).

i=1

such that 0 ≤ αi ≤ C, i = 1, ..., m and

αiy(i) = 0. And then calculating w as

For our implementation, we tested a few diﬀerent kernels for diﬀerences in performance. Since the SVM
is only used for binary classiﬁcations, the OpenCV implementation creates one SVM for each pair of labels
in order to generalize the algorithm to multiclass situations (like our own research).

3.2 K-Nearest-Neighbors

The K-Nearest-Neighbors is a non-parametric method which classiﬁes a test data point by a ”majority vote”
of the closest training example (by Euclidean distance of feature vectors). Given a test point, the algorithm
ﬁnds the k nearest neighbors to that point, and predicts the new point’s label as the mode of the labels of
the neighboring points. The algorithm breaks ties arbitrarily.

3.3 Random Forest

The random forest model bootstraps by selecting, with replacement, a random subset of the data for training
each new decision tree (bagging). Furthermore, a random subset of the data is selected at each decision
point. In total, 21 decision trees were constructed for our forest.

3

On the Recognition of Handwritten Math Equations

Quan Nguyen, Maximillian Wang, Le Cheng Fan

December 12, 2014

1

Introduction

1.1 Background

Handwriting recognition is a classic machine learning problem that has led to extensive research and a
number of highly accurate solutions. Some handwriting recognition algorithms are achieving near-human
level performance.

We employ tools used in handwriting recognition to handwritten math equations. While typing is gener-
ally much faster than writing by hand, math equations have the opposite property: writing equations by hand
is more eﬃcient than typesetting them. Furthermore, whereas handwritten equations are human-readable,
the “code” of typesetting languages are often highly nested and diﬃcult to edit. These problems of type-
setting present an opportunity for improving the workﬂow of writing math equations digitally. In addition,
recognizing mathematical equations is important for digitalizing past scientiﬁc works.

1.2 Goals and Outline

Though typesetting equations is diﬃcult, storing equations in typeset form allows for concise storage that
is relatively ﬂexible for future editing. Since LATEXis the most popular typesetting tool, our team’s long-
term goal is to produce a system to recognize images of handwritten equations and output the corresponding
characters in LATEX. Under the given time constraints, building the full system is infeasible, so our team chose
a more reasonable goal of constructing a system for recognizing individual math symbols, with the possibility
of building some functionality necessary to recognize multi-symbol expression. In the next section, we discuss
the data collection process to obtain a wide array of characters. Afterward, we discuss preprocessing and
algorithmic methods used to implement the system. Finally, we discuss our results and compare each of the
diﬀerent approaches we took.

2 Methods

2.1 Process

We run all of our images for both training and testing through a sequence of steps in order to improve the
eﬀectiveness of the system as a whole.

We begin by reading in each image and preprocessing it as outlined below in order to reduce the variation
between diﬀerent examples. This allows the system to create a much clearer picture of the distinctions
between diﬀerent labels. The next step is to detect features in the images and place them in a feature vector.
Finally, we train and test several diﬀerent machine learning models on our generated features.

We use a ”pipeline analysis” approach in our research. Since our system consists of a series of separate
steps, the overall performance is aﬀected by several factors. At each step of the process, we compare diﬀerent
methods used for preprocessing, feature detection, and learning. In doing so, we can achieve a more complete
understanding of how each step aﬀects the system. (See Figure 1)

1

Figure 1: Pipeline steps

2.2 Software

Our main tool for performing all preprocessing, feature detection, and learning algorithms is the OpenCV
library for C++. This software package includes utilities for every computer vision and Machine learning
functionality that we require for this project.

2.3 Data Set

All the training and testing data we use are handwritten Greek letters produced in Photoshop with varied
brush sizes and hardness values. The dataset consists of 128x128 grayscale images of the following letters:
α, β, λ, µ, φ, π, and ζ. There are 100 images of each Greek letter, and hold-out cross validation is used to
separate the data set into training/testing sets and to compare training and generalization error.

In order to reduce variations between images, we implement two diﬀerent preprocessing techniques. In
M10
M00

the ﬁrst method, we calculate the centroid, (¯x, ¯y), of the image via moments. These are given by ¯x =

and ¯y =

M01
M00

, where

Mpq =

128(cid:88)

128(cid:88)

xp · yq · ImageIntensity(x, y).

x=1

y=1

Once we ﬁnd the centroid, we then translate the entire image inside the 128x128 square such that the centroid
lies on the center of the image, namely at (64, 64). This corrects for diﬀerences in symbol positioning in the
hand-drawn images.

Our second technique is to eliminate as much background ”white space” in the image as possible. This is
done by drawing a bounding box around the actual symbol, and then expanding that section of the image to
ﬁll the entire square. The technique corrects for size diﬀerences in each image. It also adjusts for positioning
since the end result is tightly snug in the 128x128 square. Since the centroid of the original image is generally
not the same as the centroid of the blown-up image, we only perform one of these preprocessing techniques at
a given time. We also run the system without preprocessing to see how the techniques aﬀect the performance.
(See Figure 2)

2.4 Feature Detection

We compare two diﬀerent types of feature vectors for our learning models. The ﬁrst is treating the grayscale
image as a matrix of pixel values, and then reformatting this matrix into a single row vector. This vector
thus encodes the each of the 128 × 128 pixel values of the image. The second type of feature detection
method we use is the Harris operator for detecting corners in an image. We use an abstracted OpenCV
implementation which assigns a ”conﬁdence level” for each of the detected corners, and then returns a vector
of the coordinates of the k top scores. In our research, we compare the performance when using 5 corners, 7
corners, and raw pixel values.

2

(a) Original

(b) Centered

(c) Resized

Figure 2: Data set with preprocessing

3 Models

We use three diﬀerent learning algorithms in our project: Support Vector Machines, K-Nearest-Neighbors,
and Random Forest. Here we outline each of these algorithms.

3.1 Support Vector Machine

A support vector machine attempts to ﬁnd a hyperplane that divides training examples of two diﬀerent types,
such that the distance between the plane and each of the training examples is maximized. When training
data is linearly separable, this hyperplane will perfectly divide the points into groups by label. However, in
most cases, data is not perfectly separable, so L2 regularization is used. More precisely, ﬁnd

maxα

ai − 1
2

y(i)y(j)αiαj(cid:104)x(i)x(j)(cid:105)

m(cid:88)

i,j=1

m(cid:88)
m(cid:88)

i=1

i=1

m(cid:88)

αiy(i)x(i).

i=1

such that 0 ≤ αi ≤ C, i = 1, ..., m and

αiy(i) = 0. And then calculating w as

For our implementation, we tested a few diﬀerent kernels for diﬀerences in performance. Since the SVM
is only used for binary classiﬁcations, the OpenCV implementation creates one SVM for each pair of labels
in order to generalize the algorithm to multiclass situations (like our own research).

3.2 K-Nearest-Neighbors

The K-Nearest-Neighbors is a non-parametric method which classiﬁes a test data point by a ”majority vote”
of the closest training example (by Euclidean distance of feature vectors). Given a test point, the algorithm
ﬁnds the k nearest neighbors to that point, and predicts the new point’s label as the mode of the labels of
the neighboring points. The algorithm breaks ties arbitrarily.

3.3 Random Forest

The random forest model bootstraps by selecting, with replacement, a random subset of the data for training
each new decision tree (bagging). Furthermore, a random subset of the data is selected at each decision
point. In total, 21 decision trees were constructed for our forest.

3

4 Results and Analysis

4.1 Results

We found that the best performance came from a combination of either centering or resizing, raw pixel value
features, and a support vector machine model. Figure 3 below shows our results when using a linear kernel.
We were later able to achieve slightly better results using a degree-3 polynomial kernel, yielding approx-
imately 1.5% generalization error rate when centering the images and using raw pixel values. Other kernel
types, such as radial basis function and higher-order polynomials, performed signiﬁcantly worse due to over-
ﬁtting, as shown by a training error of 0% and generalization error of greater than 80%. We also found
that the use of centering and resizing did not yield signiﬁcantly diﬀerent results, although they both were
signiﬁcantly more eﬀective than no preprocessing. In addition, the corner detection method produced very
high error rates, proving infeasible for our desired application.

(a) Comparison of feature detection (with centering)

(b) Comparison of preprocessing (with pixel detection

Figure 3: Comparison of results

4.2 Analysis

Our results were somewhat unexpected. We had hypothesized that the corner detection method would yield
features that were more unique to each symbol, and would therefore be more eﬀective at classifying them.
Our results disagreed with this hypothesis. We believe that the high error rate associated with the corner
detection method is due to the relatively low dimension of the feature space, meaning that less information
is encoded in the feature vectors.
In addition, the corner detection technique is sensitive to individual
handwriting habits. While the dark pixels will generally occur in a similar pattern when preprocessed, the
corners detected depend strongly on where the writer draws a rounded edge or a pointed one.

While initial eﬀorts were funneled toward testing diﬀerent types of learning algorithms, the most signiﬁcant
results came from preprocessing (Figure 3). Using the centered or resized images increased all algorithms
to at least acceptable performance. It is interesting to observe that the relative diﬀerence between each the
algorithms remained approximately constant: KNN had about twice the error rate for un-processed, resized,
and centered images. Preprocessing decreases noise in the data due to variations in diﬀerent positions and
symbol sizes.

Lastly, the training and testing error curves for the diﬀerent models (Figure 4) show that our choice of
70 training examples is suﬃcient, and that adding more training examples will not decrease generalization
error signiﬁcantly. The curve shows that training and testing error converge past 60 training examples for
both SVM and RF. The small diﬀerence between training and testing error (< 5%), and the low training
error for SVM, also implies that both bias and variance are low. We cannot do much better by generating a
larger training set, so we must look elsewhere for improvements.

4

On the Recognition of Handwritten Math Equations

Quan Nguyen, Maximillian Wang, Le Cheng Fan

December 12, 2014

1

Introduction

1.1 Background

Handwriting recognition is a classic machine learning problem that has led to extensive research and a
number of highly accurate solutions. Some handwriting recognition algorithms are achieving near-human
level performance.

We employ tools used in handwriting recognition to handwritten math equations. While typing is gener-
ally much faster than writing by hand, math equations have the opposite property: writing equations by hand
is more eﬃcient than typesetting them. Furthermore, whereas handwritten equations are human-readable,
the “code” of typesetting languages are often highly nested and diﬃcult to edit. These problems of type-
setting present an opportunity for improving the workﬂow of writing math equations digitally. In addition,
recognizing mathematical equations is important for digitalizing past scientiﬁc works.

1.2 Goals and Outline

Though typesetting equations is diﬃcult, storing equations in typeset form allows for concise storage that
is relatively ﬂexible for future editing. Since LATEXis the most popular typesetting tool, our team’s long-
term goal is to produce a system to recognize images of handwritten equations and output the corresponding
characters in LATEX. Under the given time constraints, building the full system is infeasible, so our team chose
a more reasonable goal of constructing a system for recognizing individual math symbols, with the possibility
of building some functionality necessary to recognize multi-symbol expression. In the next section, we discuss
the data collection process to obtain a wide array of characters. Afterward, we discuss preprocessing and
algorithmic methods used to implement the system. Finally, we discuss our results and compare each of the
diﬀerent approaches we took.

2 Methods

2.1 Process

We run all of our images for both training and testing through a sequence of steps in order to improve the
eﬀectiveness of the system as a whole.

We begin by reading in each image and preprocessing it as outlined below in order to reduce the variation
between diﬀerent examples. This allows the system to create a much clearer picture of the distinctions
between diﬀerent labels. The next step is to detect features in the images and place them in a feature vector.
Finally, we train and test several diﬀerent machine learning models on our generated features.

We use a ”pipeline analysis” approach in our research. Since our system consists of a series of separate
steps, the overall performance is aﬀected by several factors. At each step of the process, we compare diﬀerent
methods used for preprocessing, feature detection, and learning. In doing so, we can achieve a more complete
understanding of how each step aﬀects the system. (See Figure 1)

1

Figure 1: Pipeline steps

2.2 Software

Our main tool for performing all preprocessing, feature detection, and learning algorithms is the OpenCV
library for C++. This software package includes utilities for every computer vision and Machine learning
functionality that we require for this project.

2.3 Data Set

All the training and testing data we use are handwritten Greek letters produced in Photoshop with varied
brush sizes and hardness values. The dataset consists of 128x128 grayscale images of the following letters:
α, β, λ, µ, φ, π, and ζ. There are 100 images of each Greek letter, and hold-out cross validation is used to
separate the data set into training/testing sets and to compare training and generalization error.

In order to reduce variations between images, we implement two diﬀerent preprocessing techniques. In
M10
M00

the ﬁrst method, we calculate the centroid, (¯x, ¯y), of the image via moments. These are given by ¯x =

and ¯y =

M01
M00

, where

Mpq =

128(cid:88)

128(cid:88)

xp · yq · ImageIntensity(x, y).

x=1

y=1

Once we ﬁnd the centroid, we then translate the entire image inside the 128x128 square such that the centroid
lies on the center of the image, namely at (64, 64). This corrects for diﬀerences in symbol positioning in the
hand-drawn images.

Our second technique is to eliminate as much background ”white space” in the image as possible. This is
done by drawing a bounding box around the actual symbol, and then expanding that section of the image to
ﬁll the entire square. The technique corrects for size diﬀerences in each image. It also adjusts for positioning
since the end result is tightly snug in the 128x128 square. Since the centroid of the original image is generally
not the same as the centroid of the blown-up image, we only perform one of these preprocessing techniques at
a given time. We also run the system without preprocessing to see how the techniques aﬀect the performance.
(See Figure 2)

2.4 Feature Detection

We compare two diﬀerent types of feature vectors for our learning models. The ﬁrst is treating the grayscale
image as a matrix of pixel values, and then reformatting this matrix into a single row vector. This vector
thus encodes the each of the 128 × 128 pixel values of the image. The second type of feature detection
method we use is the Harris operator for detecting corners in an image. We use an abstracted OpenCV
implementation which assigns a ”conﬁdence level” for each of the detected corners, and then returns a vector
of the coordinates of the k top scores. In our research, we compare the performance when using 5 corners, 7
corners, and raw pixel values.

2

(a) Original

(b) Centered

(c) Resized

Figure 2: Data set with preprocessing

3 Models

We use three diﬀerent learning algorithms in our project: Support Vector Machines, K-Nearest-Neighbors,
and Random Forest. Here we outline each of these algorithms.

3.1 Support Vector Machine

A support vector machine attempts to ﬁnd a hyperplane that divides training examples of two diﬀerent types,
such that the distance between the plane and each of the training examples is maximized. When training
data is linearly separable, this hyperplane will perfectly divide the points into groups by label. However, in
most cases, data is not perfectly separable, so L2 regularization is used. More precisely, ﬁnd

maxα

ai − 1
2

y(i)y(j)αiαj(cid:104)x(i)x(j)(cid:105)

m(cid:88)

i,j=1

m(cid:88)
m(cid:88)

i=1

i=1

m(cid:88)

αiy(i)x(i).

i=1

such that 0 ≤ αi ≤ C, i = 1, ..., m and

αiy(i) = 0. And then calculating w as

For our implementation, we tested a few diﬀerent kernels for diﬀerences in performance. Since the SVM
is only used for binary classiﬁcations, the OpenCV implementation creates one SVM for each pair of labels
in order to generalize the algorithm to multiclass situations (like our own research).

3.2 K-Nearest-Neighbors

The K-Nearest-Neighbors is a non-parametric method which classiﬁes a test data point by a ”majority vote”
of the closest training example (by Euclidean distance of feature vectors). Given a test point, the algorithm
ﬁnds the k nearest neighbors to that point, and predicts the new point’s label as the mode of the labels of
the neighboring points. The algorithm breaks ties arbitrarily.

3.3 Random Forest

The random forest model bootstraps by selecting, with replacement, a random subset of the data for training
each new decision tree (bagging). Furthermore, a random subset of the data is selected at each decision
point. In total, 21 decision trees were constructed for our forest.

3

4 Results and Analysis

4.1 Results

We found that the best performance came from a combination of either centering or resizing, raw pixel value
features, and a support vector machine model. Figure 3 below shows our results when using a linear kernel.
We were later able to achieve slightly better results using a degree-3 polynomial kernel, yielding approx-
imately 1.5% generalization error rate when centering the images and using raw pixel values. Other kernel
types, such as radial basis function and higher-order polynomials, performed signiﬁcantly worse due to over-
ﬁtting, as shown by a training error of 0% and generalization error of greater than 80%. We also found
that the use of centering and resizing did not yield signiﬁcantly diﬀerent results, although they both were
signiﬁcantly more eﬀective than no preprocessing. In addition, the corner detection method produced very
high error rates, proving infeasible for our desired application.

(a) Comparison of feature detection (with centering)

(b) Comparison of preprocessing (with pixel detection

Figure 3: Comparison of results

4.2 Analysis

Our results were somewhat unexpected. We had hypothesized that the corner detection method would yield
features that were more unique to each symbol, and would therefore be more eﬀective at classifying them.
Our results disagreed with this hypothesis. We believe that the high error rate associated with the corner
detection method is due to the relatively low dimension of the feature space, meaning that less information
is encoded in the feature vectors.
In addition, the corner detection technique is sensitive to individual
handwriting habits. While the dark pixels will generally occur in a similar pattern when preprocessed, the
corners detected depend strongly on where the writer draws a rounded edge or a pointed one.

While initial eﬀorts were funneled toward testing diﬀerent types of learning algorithms, the most signiﬁcant
results came from preprocessing (Figure 3). Using the centered or resized images increased all algorithms
to at least acceptable performance. It is interesting to observe that the relative diﬀerence between each the
algorithms remained approximately constant: KNN had about twice the error rate for un-processed, resized,
and centered images. Preprocessing decreases noise in the data due to variations in diﬀerent positions and
symbol sizes.

Lastly, the training and testing error curves for the diﬀerent models (Figure 4) show that our choice of
70 training examples is suﬃcient, and that adding more training examples will not decrease generalization
error signiﬁcantly. The curve shows that training and testing error converge past 60 training examples for
both SVM and RF. The small diﬀerence between training and testing error (< 5%), and the low training
error for SVM, also implies that both bias and variance are low. We cannot do much better by generating a
larger training set, so we must look elsewhere for improvements.

4

(a) Testing and training error

Figure 4: Comparison of results

5 Future Work

We will extend our framework to work for full handwritten math equations. The primary addition will be
a segmentation algorithm for dividing a full expression into individual symbols, which may be identiﬁed
using our current work. In addition, we hope to test our system on photographs taken of real handwriting
rather than Photoshop drawings. We predict that this use case will require some additional pre-processing
techniques, such as contrast boosting, to achieve current performance levels.

Finally, we will further study neural networks as an option to outperform our other learning models.
We tested neural networks with all preprocessing and feature selection permutations during our research.
However, due to problems with overﬁtting, we did not achieve useful results. Some successful preliminary
results have shown an error rate of 2% for 2 classes with a training set of 20 examples.

6 References

A. Graves, et. al. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition.

IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009.

Ciresan, Dan, Ueli Meier, and Jurgen Schmidhuber. ”Multi-column Deep Learning Neural Networks for
Image Classiﬁcation.”. IEEE Conference on Computer Vision and Pattern Recognition. (2012): 3642-649.
IEEE. Web.

Hsu, Chih-Wei, and Chih-Jen Lin. ”A Comparison of Methods for Multiclass Support Vector Machines.”.

IEEE Transactions on Neural Networks. 13.2 (2002): 415-25. Web.

Knerr, S., L. Personnaz, and G. Dreyfus. ”Single-layer Learning Revisited: A Stepwise Procedure for

Building and Training a Neural Network.”Springer. 68 (1990): 41-50. Web.

Liu, Cheng-Lin, Kazuki Nakashima, Hiroshi Sako, and Hiromichi Fujisawa. ”Handwritten Digit Recogni-
tion: Investigation of Normalization and Feature Extraction Techniques.”. Pattern Recognition. 37.2 (2004):
265-79. Web.

5

