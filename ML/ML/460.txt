CS229 FALL 2014

1

A Binary Classiﬁcation of Beatles Song Authorship

Miles Bennett, Casey Haaland, and Atsu Kobashi

I. INTRODUCTION

M ACHINE learning algorithms have a long history with

the problem of text classiﬁcation which has been used
extensively for a variety of applications including spam classi-
ﬁcation as well as the identiﬁcation of the disputed authorship
of several Federalist Papers [1][2]. In our project, we attempt
to use the tools of machine learning to identify the primary
author of a given Beatles’ song using the lyrics as features.
Since a large majority of Beatles songs were written by either
John Lennon or Paul McCartney, we restricted the scope of
our project to a simple binary classiﬁcation problem. Though
text classiﬁcation is a well known and widely used application
of machine learning, the aim of this project is to analyze the
performance of various classiﬁcation algorithms and quantify
their suitability for this problem.

II. METHODOLOGY

A. Data Collection and Preprocessing

After converting a PDF of Beatles lyrics to plain text format
we ﬁrst separated the set of lyrics into separate text ﬁles for
each song [3]. In the ﬁrst iteration of testing, we also removed
words we believed were content free, in an attempt to improve
the accuracy of our classiﬁer. Since some basic words were
likely to appear in a vast majority of the songs, we felt it was
likely that these words would not be informative features in
determining authorship so we would be justiﬁed in removing
them. As such, we deﬁned the following words which would
not be included in the feature space:

1) “the”
2) “a”
3) “and”

4) “or”
5) “but”
6) “if”

In addition to removing these so called “stop words”, we
also lemmatized the song lyrics using a library provided by the
Natural Language Toolkit (NLTK), an open source module for
python [4]. The lemmatization process converts a noun, verb,
or adjective to the canonical form of the word. For example,
the plural noun “dogs” would be converted to the singular
noun “dog” and the word “went” would be converted to “go.”
By lemmatizing the aggregate list of all words that appear in
the corpus of Beatles songs, the size of the vocabulary was
reduced from 1435 to 1142. This is a similar preprocessing
technique that was used in Problem Set #2.

Lastly, because there are only 93 songs written by either
Lennon or McCartney, our data set was relatively small. For
this reason, in the algorithms to follow, all accuracies are
reported under leave one out cross validation. Speciﬁcally, we

evaluated our experimental accuracy as:

n(cid:88)

i=1

(cid:16)

x(i)(cid:17) (cid:54)= y(i)(cid:111)

(cid:110)

1

hθ

ˆτLOOCV = 1 − 1
n

Since 43 of the songs were by McCartney and 50 by
Lennon, there was very little class imbalance and this seemed
to be an appropriate metric for the classiﬁer.

B. Algorithms

There are a variety of well known algorithms in the ﬁeld of
machine learning for the purpose of binary text classiﬁcation.
One possibility that our group hypothesized was that songs
written by Lennon and songs written by McCartney would
lie close together in a cluster. For this reason we wanted to
explore the use of non-parametric clustering algorithms such
as k-means or k-Nearest Neighbors as well as investigate the
effect of using different distance metrics (e.g. (cid:96)1 and (cid:96)2) on
our classiﬁcation accuracy.

Another well known algorithm considered to be both simple
and effective for text classiﬁcation problems is Naive Bayes.
The large volume of literature concerning the use of Naive
Bayes for textual analysis seems to suggest both the Bernoulli
and Multinomial event model would be well suited for our
classiﬁcation problem.

The last

two algorithms we seek to investigate in this
project are Support Vector Machines and regularized logistic
regression. SVMs have many desirable properties such as
large margin, the ability to be kernelized, and a relatively low
tendency to overﬁt, all of which would allow it to perform well
on this problem. Logistic regression is generally accepted as an
algorithm that performs well on binary classiﬁcation. With the
addition of a regularization term we can obviate the problem
of overﬁtting that logistic regression tends to suffer from.

A. Initial Classiﬁcation Approach

III. RESULTS

Before attempting the algorithms listed above, we ﬁrst
attempted to see if there was a simple feature space in which
we could visualize and perhaps ﬁnd structure in the data. One
feature that was often cited as being useful for the purpose
of text classiﬁcation was the number of unique words per
document. In accordance with this, we created a scatter plot
of the number of unique words vs. song length.

As evidenced by Figure 1, this two dimensional feature
space was very densely populated with little discernible pattern
that would enable us to distinguish Lennon from McCartney.
Therefore, we found that lack of linear separability or distinct
clustering meant this feature space was not rich enough to
accurately perform the classiﬁcation. This observation was

CS229 FALL 2014

1

A Binary Classiﬁcation of Beatles Song Authorship

Miles Bennett, Casey Haaland, and Atsu Kobashi

I. INTRODUCTION

M ACHINE learning algorithms have a long history with

the problem of text classiﬁcation which has been used
extensively for a variety of applications including spam classi-
ﬁcation as well as the identiﬁcation of the disputed authorship
of several Federalist Papers [1][2]. In our project, we attempt
to use the tools of machine learning to identify the primary
author of a given Beatles’ song using the lyrics as features.
Since a large majority of Beatles songs were written by either
John Lennon or Paul McCartney, we restricted the scope of
our project to a simple binary classiﬁcation problem. Though
text classiﬁcation is a well known and widely used application
of machine learning, the aim of this project is to analyze the
performance of various classiﬁcation algorithms and quantify
their suitability for this problem.

II. METHODOLOGY

A. Data Collection and Preprocessing

After converting a PDF of Beatles lyrics to plain text format
we ﬁrst separated the set of lyrics into separate text ﬁles for
each song [3]. In the ﬁrst iteration of testing, we also removed
words we believed were content free, in an attempt to improve
the accuracy of our classiﬁer. Since some basic words were
likely to appear in a vast majority of the songs, we felt it was
likely that these words would not be informative features in
determining authorship so we would be justiﬁed in removing
them. As such, we deﬁned the following words which would
not be included in the feature space:

1) “the”
2) “a”
3) “and”

4) “or”
5) “but”
6) “if”

In addition to removing these so called “stop words”, we
also lemmatized the song lyrics using a library provided by the
Natural Language Toolkit (NLTK), an open source module for
python [4]. The lemmatization process converts a noun, verb,
or adjective to the canonical form of the word. For example,
the plural noun “dogs” would be converted to the singular
noun “dog” and the word “went” would be converted to “go.”
By lemmatizing the aggregate list of all words that appear in
the corpus of Beatles songs, the size of the vocabulary was
reduced from 1435 to 1142. This is a similar preprocessing
technique that was used in Problem Set #2.

Lastly, because there are only 93 songs written by either
Lennon or McCartney, our data set was relatively small. For
this reason, in the algorithms to follow, all accuracies are
reported under leave one out cross validation. Speciﬁcally, we

evaluated our experimental accuracy as:

n(cid:88)

i=1

(cid:16)

x(i)(cid:17) (cid:54)= y(i)(cid:111)

(cid:110)

1

hθ

ˆτLOOCV = 1 − 1
n

Since 43 of the songs were by McCartney and 50 by
Lennon, there was very little class imbalance and this seemed
to be an appropriate metric for the classiﬁer.

B. Algorithms

There are a variety of well known algorithms in the ﬁeld of
machine learning for the purpose of binary text classiﬁcation.
One possibility that our group hypothesized was that songs
written by Lennon and songs written by McCartney would
lie close together in a cluster. For this reason we wanted to
explore the use of non-parametric clustering algorithms such
as k-means or k-Nearest Neighbors as well as investigate the
effect of using different distance metrics (e.g. (cid:96)1 and (cid:96)2) on
our classiﬁcation accuracy.

Another well known algorithm considered to be both simple
and effective for text classiﬁcation problems is Naive Bayes.
The large volume of literature concerning the use of Naive
Bayes for textual analysis seems to suggest both the Bernoulli
and Multinomial event model would be well suited for our
classiﬁcation problem.

The last

two algorithms we seek to investigate in this
project are Support Vector Machines and regularized logistic
regression. SVMs have many desirable properties such as
large margin, the ability to be kernelized, and a relatively low
tendency to overﬁt, all of which would allow it to perform well
on this problem. Logistic regression is generally accepted as an
algorithm that performs well on binary classiﬁcation. With the
addition of a regularization term we can obviate the problem
of overﬁtting that logistic regression tends to suffer from.

A. Initial Classiﬁcation Approach

III. RESULTS

Before attempting the algorithms listed above, we ﬁrst
attempted to see if there was a simple feature space in which
we could visualize and perhaps ﬁnd structure in the data. One
feature that was often cited as being useful for the purpose
of text classiﬁcation was the number of unique words per
document. In accordance with this, we created a scatter plot
of the number of unique words vs. song length.

As evidenced by Figure 1, this two dimensional feature
space was very densely populated with little discernible pattern
that would enable us to distinguish Lennon from McCartney.
Therefore, we found that lack of linear separability or distinct
clustering meant this feature space was not rich enough to
accurately perform the classiﬁcation. This observation was

CS229 FALL 2014

2

Fig. 1. The circles denote songs composed by John Lennon while the x’s
denote ones written by Paul McCartney

Fig. 2. Graphical depiction of the k-nearest neighbors algorithm. The dashed
circle corresponds to the chosen distance metric

veriﬁed by our initial attempts using k-NN which yielded an
accuracy of 45.16%.

In hopes of better capturing more information that could be
used to discriminate between the two artists, we chose to create
a design matrix X ∈ R93×1435 and a classiﬁcation vector
y ∈ R93

 − x(1) −

− x(93) −

...





 y(1)

...
y(93)

X =

y =

where x(i)
gives the number of times the jth word in our
j
vocabulary V appears in the ith Beatles’ song in our training
set where y(i) = 1 if McCartney was the artist of song i and
y(i) = 0 if Lennon was the artist.1

B. k-means & k-Nearest Neighbors

The k-means algorithm is an unsupervised learning algo-
rithm that clusters data based on a chosen distance metric.
This algorithm is ideal in the case that the convex hulls of
the points from each class are mutually disjoint.2 The k-
Nearest Neighbors (k-NN) algorithm is another non-parametric
algorithm in which the class label of a new example is
predicted by a majority vote of the k nearest neighbors (where
nearest was measured by either the (cid:96)1 or (cid:96)2 norm). We chose
to break ties by using the class label of the closest example to
the test point. It is also worth noting that k-NN exploits local
clustering of data and as a result tends to perform well on data
in which classes lie in a number of dense clusters [5].

1For the case of the SVM, the class designations are swtiched so that
2The convex hull of a set of points S = {x1, · · · , xn} is simply the set

y(i) = −1 identiﬁes a song composed by Lennon rather than 0
of all convex combinations of points in the set (i.e. conv(S) = {λT x|
0 ≺ λ, λT 1 = 1}

Initially, we supposed that the data for the Beatles’ songs
written by each author might lie in two distinct clusters of
our feature space. This situation might occur if, for example,
McCartney used one subset of the vocabulary much more
frequently than Lennon did. Another hypothesis for the struc-
ture of our data that we thought was reasonable was that the
classiﬁcations might lie close together in a number of smaller
clusters in the feature space. Since both of these seemed to be
plausible assumptions about the data, we used the k-means and
k-NN MATLAB implementations, iterating over the number of
neighbors and utilizing the 1-norm and 2-norm, the results of
which are shown in the table.

Algorithm k Accuracy Metric
k-means
k-NN
k-NN
k-NN
k-NN
k-NN
k-NN

53.76%
54.84%
53.76%
37.63%
25.81%
49.46%
50.54%

(cid:96)2
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1

2
1
1
2
2
5
5

As evidenced by the poor performance of k-means, it was
clear the data did not lie in two distinct clusters of our chosen
feature space. This result might be explained by Lennon and
McCartney having similar writing styles and therefore the
different classes were highly interspersed amongst each other.
The negative results of both algorithms could also be attributed
to the curse of dimensionality, as the feature space is over 1400
dimensional while the number of test examples was just 93.

C. Naive Bayes

Naive Bayes is a simple generative learning algorithm that
is easily applied to binary classiﬁcation problems. The key
assumption of the algorithm is the conditional independence
of each of the words given the document’s classiﬁcation (i.e.
p(xi, xj|y) = p(xi|y)p(xj|y). The algorithm then seeks to
learn a model by computing

max

θ

p(x, y) = max

θ

p(y)

where θ is the vector of parameters p(y = 1), p(xi|y = 1), and
p(xi|y = 0) which are to be estimated. In our initial attempts

n(cid:89)

i=1

p(xi|y)

CS229 FALL 2014

1

A Binary Classiﬁcation of Beatles Song Authorship

Miles Bennett, Casey Haaland, and Atsu Kobashi

I. INTRODUCTION

M ACHINE learning algorithms have a long history with

the problem of text classiﬁcation which has been used
extensively for a variety of applications including spam classi-
ﬁcation as well as the identiﬁcation of the disputed authorship
of several Federalist Papers [1][2]. In our project, we attempt
to use the tools of machine learning to identify the primary
author of a given Beatles’ song using the lyrics as features.
Since a large majority of Beatles songs were written by either
John Lennon or Paul McCartney, we restricted the scope of
our project to a simple binary classiﬁcation problem. Though
text classiﬁcation is a well known and widely used application
of machine learning, the aim of this project is to analyze the
performance of various classiﬁcation algorithms and quantify
their suitability for this problem.

II. METHODOLOGY

A. Data Collection and Preprocessing

After converting a PDF of Beatles lyrics to plain text format
we ﬁrst separated the set of lyrics into separate text ﬁles for
each song [3]. In the ﬁrst iteration of testing, we also removed
words we believed were content free, in an attempt to improve
the accuracy of our classiﬁer. Since some basic words were
likely to appear in a vast majority of the songs, we felt it was
likely that these words would not be informative features in
determining authorship so we would be justiﬁed in removing
them. As such, we deﬁned the following words which would
not be included in the feature space:

1) “the”
2) “a”
3) “and”

4) “or”
5) “but”
6) “if”

In addition to removing these so called “stop words”, we
also lemmatized the song lyrics using a library provided by the
Natural Language Toolkit (NLTK), an open source module for
python [4]. The lemmatization process converts a noun, verb,
or adjective to the canonical form of the word. For example,
the plural noun “dogs” would be converted to the singular
noun “dog” and the word “went” would be converted to “go.”
By lemmatizing the aggregate list of all words that appear in
the corpus of Beatles songs, the size of the vocabulary was
reduced from 1435 to 1142. This is a similar preprocessing
technique that was used in Problem Set #2.

Lastly, because there are only 93 songs written by either
Lennon or McCartney, our data set was relatively small. For
this reason, in the algorithms to follow, all accuracies are
reported under leave one out cross validation. Speciﬁcally, we

evaluated our experimental accuracy as:

n(cid:88)

i=1

(cid:16)

x(i)(cid:17) (cid:54)= y(i)(cid:111)

(cid:110)

1

hθ

ˆτLOOCV = 1 − 1
n

Since 43 of the songs were by McCartney and 50 by
Lennon, there was very little class imbalance and this seemed
to be an appropriate metric for the classiﬁer.

B. Algorithms

There are a variety of well known algorithms in the ﬁeld of
machine learning for the purpose of binary text classiﬁcation.
One possibility that our group hypothesized was that songs
written by Lennon and songs written by McCartney would
lie close together in a cluster. For this reason we wanted to
explore the use of non-parametric clustering algorithms such
as k-means or k-Nearest Neighbors as well as investigate the
effect of using different distance metrics (e.g. (cid:96)1 and (cid:96)2) on
our classiﬁcation accuracy.

Another well known algorithm considered to be both simple
and effective for text classiﬁcation problems is Naive Bayes.
The large volume of literature concerning the use of Naive
Bayes for textual analysis seems to suggest both the Bernoulli
and Multinomial event model would be well suited for our
classiﬁcation problem.

The last

two algorithms we seek to investigate in this
project are Support Vector Machines and regularized logistic
regression. SVMs have many desirable properties such as
large margin, the ability to be kernelized, and a relatively low
tendency to overﬁt, all of which would allow it to perform well
on this problem. Logistic regression is generally accepted as an
algorithm that performs well on binary classiﬁcation. With the
addition of a regularization term we can obviate the problem
of overﬁtting that logistic regression tends to suffer from.

A. Initial Classiﬁcation Approach

III. RESULTS

Before attempting the algorithms listed above, we ﬁrst
attempted to see if there was a simple feature space in which
we could visualize and perhaps ﬁnd structure in the data. One
feature that was often cited as being useful for the purpose
of text classiﬁcation was the number of unique words per
document. In accordance with this, we created a scatter plot
of the number of unique words vs. song length.

As evidenced by Figure 1, this two dimensional feature
space was very densely populated with little discernible pattern
that would enable us to distinguish Lennon from McCartney.
Therefore, we found that lack of linear separability or distinct
clustering meant this feature space was not rich enough to
accurately perform the classiﬁcation. This observation was

CS229 FALL 2014

2

Fig. 1. The circles denote songs composed by John Lennon while the x’s
denote ones written by Paul McCartney

Fig. 2. Graphical depiction of the k-nearest neighbors algorithm. The dashed
circle corresponds to the chosen distance metric

veriﬁed by our initial attempts using k-NN which yielded an
accuracy of 45.16%.

In hopes of better capturing more information that could be
used to discriminate between the two artists, we chose to create
a design matrix X ∈ R93×1435 and a classiﬁcation vector
y ∈ R93

 − x(1) −

− x(93) −

...





 y(1)

...
y(93)

X =

y =

where x(i)
gives the number of times the jth word in our
j
vocabulary V appears in the ith Beatles’ song in our training
set where y(i) = 1 if McCartney was the artist of song i and
y(i) = 0 if Lennon was the artist.1

B. k-means & k-Nearest Neighbors

The k-means algorithm is an unsupervised learning algo-
rithm that clusters data based on a chosen distance metric.
This algorithm is ideal in the case that the convex hulls of
the points from each class are mutually disjoint.2 The k-
Nearest Neighbors (k-NN) algorithm is another non-parametric
algorithm in which the class label of a new example is
predicted by a majority vote of the k nearest neighbors (where
nearest was measured by either the (cid:96)1 or (cid:96)2 norm). We chose
to break ties by using the class label of the closest example to
the test point. It is also worth noting that k-NN exploits local
clustering of data and as a result tends to perform well on data
in which classes lie in a number of dense clusters [5].

1For the case of the SVM, the class designations are swtiched so that
2The convex hull of a set of points S = {x1, · · · , xn} is simply the set

y(i) = −1 identiﬁes a song composed by Lennon rather than 0
of all convex combinations of points in the set (i.e. conv(S) = {λT x|
0 ≺ λ, λT 1 = 1}

Initially, we supposed that the data for the Beatles’ songs
written by each author might lie in two distinct clusters of
our feature space. This situation might occur if, for example,
McCartney used one subset of the vocabulary much more
frequently than Lennon did. Another hypothesis for the struc-
ture of our data that we thought was reasonable was that the
classiﬁcations might lie close together in a number of smaller
clusters in the feature space. Since both of these seemed to be
plausible assumptions about the data, we used the k-means and
k-NN MATLAB implementations, iterating over the number of
neighbors and utilizing the 1-norm and 2-norm, the results of
which are shown in the table.

Algorithm k Accuracy Metric
k-means
k-NN
k-NN
k-NN
k-NN
k-NN
k-NN

53.76%
54.84%
53.76%
37.63%
25.81%
49.46%
50.54%

(cid:96)2
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1

2
1
1
2
2
5
5

As evidenced by the poor performance of k-means, it was
clear the data did not lie in two distinct clusters of our chosen
feature space. This result might be explained by Lennon and
McCartney having similar writing styles and therefore the
different classes were highly interspersed amongst each other.
The negative results of both algorithms could also be attributed
to the curse of dimensionality, as the feature space is over 1400
dimensional while the number of test examples was just 93.

C. Naive Bayes

Naive Bayes is a simple generative learning algorithm that
is easily applied to binary classiﬁcation problems. The key
assumption of the algorithm is the conditional independence
of each of the words given the document’s classiﬁcation (i.e.
p(xi, xj|y) = p(xi|y)p(xj|y). The algorithm then seeks to
learn a model by computing

max

θ

p(x, y) = max

θ

p(y)

where θ is the vector of parameters p(y = 1), p(xi|y = 1), and
p(xi|y = 0) which are to be estimated. In our initial attempts

n(cid:89)

i=1

p(xi|y)

CS229 FALL 2014

3

to apply Naive Bayes we implemented both the Bernoulli
and Multinomial event models with Laplace smoothing. The
results of which are summarized below.

Event Model Accuracy
62.37%
53.76%

Bernoulli
Multinomial

As was the case with the previous learning algorithms, Naive
Bayes only slightly outperformed random guessing. The rea-
sons for the failure of this particular learning algorithm is most
likely due to the small training set as well as the conditional
independence assumption being violated.

D. SVMs & Regularized Logistic Regression

Algorithm
Logistic

Loss

log-loss

SVM
SVM
SVM

(cid:96)2
(cid:96)1
(cid:96)2

ˆλ

Penalty Accuracy
55.69%
0.0015
58.06% 6.9 ×10−4
61.29%
63.44%

0.002
0.02

(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)1

IV. AN ALTERNATIVE FEATURE SPACE

In light of the poor performance of many of the algorithms
we used to perform the classiﬁcation, we sought alternative
feature mappings for the lyrical data. In the course of our
research, we discovered term frequency - inverse document
frequency (tf-idf ) is a standard feature mapping that performs
well in text classiﬁcation. To perform the feature mapping we
deﬁne the following quantities [7]

The last two algorithms that we attempted were Support
Vector Machines and regularized logistic regression, both of
which are supported by the LIBLINEAR package [6]. In order
to fully investigate the range of classiﬁers available, we used
a variety of objectives for the SVM algorithm such as (cid:96)2 loss
with (cid:96)2 penalty:

J1 = 1/2(cid:107)w(cid:107)2
as well as (cid:96)2 loss with (cid:96)1 penalty:
J2 = 1/2(cid:107)w(cid:107)2

2 + λ(cid:107)ξ(cid:107)2

2

2 + λ(cid:107)ξ(cid:107)1

among other options supported by the software package. For
the logistic regression learning algorithm we also experi-
mented with values of the regularization term for the objective

(cid:16)

m(cid:88)

i=1

JLR (θ) =

log

1 + exp

(cid:16)−y(i)θT ˜x(i)(cid:17)(cid:17)
(cid:105)T
(cid:104)(cid:0)x(i)(cid:1)T

1

+ λ(cid:107)θ(cid:107)2

2

with θ, ˜x(i) ∈ R94 where ˜x(i) =
. Assuming we
have a cost function J for the unregularized algorithm, our
objective for the regularized learning algorithms take the form

J + λ · fp ((cid:107)x(cid:107)p)

where (cid:107)x(cid:107)p is deﬁned for p ≥ 1 as

(cid:33)1/p

(cid:107)x(cid:107)p (cid:44)

|xi|p

and fp : R+ → R+ is the mapping

(cid:32) n(cid:88)
(cid:40)

i=1

z
z2

f (z) (cid:44)

if p = 1
if p = 2

The value λ ∈ R is the regularization parameter which helps
stabilize the objective J [7]. To choose the value of the reg-
ularization parameter, we iterated over a logarithmic spacing
of values from 0.0001 to 5000 and computed the resulting
accuracies for each value of λ. The accuracies reported in
table below are for

ˆλ ≈ arg max

λ

ˆτLOOCV

Speciﬁcally, we selected the value of λ that produced the
maximum accuracy on our set of examples under Leave One
Out Cross Validation.

nw(x) = number of times word w appears in doc x

f (w|x) =

= term frequency

(cid:80)(cid:48)

nw(x)
w nw(cid:48)(x)

(cid:20)

φw(x) = f (w|x) log

# of docs + 1

# of docs with word w + 1

(cid:21)

The “term frequency” portion of the mapping gives a weight-
ing corresponding to how often word w appears in document
x while the “inverse document frequency” portion serves to
deemphasize words which appear in a large fraction of the
documents. The addition of one in both the numerator and
denominator of the idf term acts as a form of smoothing and
prevents division by zero in the case that a word does not
appear [8].

Using this new feature mapping we reran a selection of our
previous algorithms to see if the new feature mapping φw(x)
garnered any improvement in classiﬁcation accuracy and per-
haps be worth pursuing. The following table summarizes the
results of implementing the tf-idf frequency feature mapping.

Algorithm
Logistic

SVM

k-means

Loss

log-loss

(cid:96)1
-

Penalty Accuracy

ˆλ
60.22% 2.5
10
58.06%
44.09 %
-

(cid:96)2
(cid:96)2
-

We speculate that the reasons for the poor performance of
tf-idf for this particular application are similar to the reasons
why many of our aforementioned algorithms failed. More
speciﬁcally, we were using very few training examples relative
to the size of the feature space we were attempting to learn.
Additionally, since each training example consists of the lyrics
to a song, rather than a much larger body of work, say an essay
or a book, the examples will not be as informative as would
be the case for classiﬁcation of larger documents.

V. REDUCING THE FEATURE SPACE DIMENSIONALITY
Lastly, with most of the algorithms unable to achieve any
signiﬁcant accuracy, we explored the possibility of reducing
the dimensionality of the data. Liang et. al. remarks, “low-
content ‘stop words’ have been shown to be very useful
statistical cues of sentiment and psychology” which led us

CS229 FALL 2014

1

A Binary Classiﬁcation of Beatles Song Authorship

Miles Bennett, Casey Haaland, and Atsu Kobashi

I. INTRODUCTION

M ACHINE learning algorithms have a long history with

the problem of text classiﬁcation which has been used
extensively for a variety of applications including spam classi-
ﬁcation as well as the identiﬁcation of the disputed authorship
of several Federalist Papers [1][2]. In our project, we attempt
to use the tools of machine learning to identify the primary
author of a given Beatles’ song using the lyrics as features.
Since a large majority of Beatles songs were written by either
John Lennon or Paul McCartney, we restricted the scope of
our project to a simple binary classiﬁcation problem. Though
text classiﬁcation is a well known and widely used application
of machine learning, the aim of this project is to analyze the
performance of various classiﬁcation algorithms and quantify
their suitability for this problem.

II. METHODOLOGY

A. Data Collection and Preprocessing

After converting a PDF of Beatles lyrics to plain text format
we ﬁrst separated the set of lyrics into separate text ﬁles for
each song [3]. In the ﬁrst iteration of testing, we also removed
words we believed were content free, in an attempt to improve
the accuracy of our classiﬁer. Since some basic words were
likely to appear in a vast majority of the songs, we felt it was
likely that these words would not be informative features in
determining authorship so we would be justiﬁed in removing
them. As such, we deﬁned the following words which would
not be included in the feature space:

1) “the”
2) “a”
3) “and”

4) “or”
5) “but”
6) “if”

In addition to removing these so called “stop words”, we
also lemmatized the song lyrics using a library provided by the
Natural Language Toolkit (NLTK), an open source module for
python [4]. The lemmatization process converts a noun, verb,
or adjective to the canonical form of the word. For example,
the plural noun “dogs” would be converted to the singular
noun “dog” and the word “went” would be converted to “go.”
By lemmatizing the aggregate list of all words that appear in
the corpus of Beatles songs, the size of the vocabulary was
reduced from 1435 to 1142. This is a similar preprocessing
technique that was used in Problem Set #2.

Lastly, because there are only 93 songs written by either
Lennon or McCartney, our data set was relatively small. For
this reason, in the algorithms to follow, all accuracies are
reported under leave one out cross validation. Speciﬁcally, we

evaluated our experimental accuracy as:

n(cid:88)

i=1

(cid:16)

x(i)(cid:17) (cid:54)= y(i)(cid:111)

(cid:110)

1

hθ

ˆτLOOCV = 1 − 1
n

Since 43 of the songs were by McCartney and 50 by
Lennon, there was very little class imbalance and this seemed
to be an appropriate metric for the classiﬁer.

B. Algorithms

There are a variety of well known algorithms in the ﬁeld of
machine learning for the purpose of binary text classiﬁcation.
One possibility that our group hypothesized was that songs
written by Lennon and songs written by McCartney would
lie close together in a cluster. For this reason we wanted to
explore the use of non-parametric clustering algorithms such
as k-means or k-Nearest Neighbors as well as investigate the
effect of using different distance metrics (e.g. (cid:96)1 and (cid:96)2) on
our classiﬁcation accuracy.

Another well known algorithm considered to be both simple
and effective for text classiﬁcation problems is Naive Bayes.
The large volume of literature concerning the use of Naive
Bayes for textual analysis seems to suggest both the Bernoulli
and Multinomial event model would be well suited for our
classiﬁcation problem.

The last

two algorithms we seek to investigate in this
project are Support Vector Machines and regularized logistic
regression. SVMs have many desirable properties such as
large margin, the ability to be kernelized, and a relatively low
tendency to overﬁt, all of which would allow it to perform well
on this problem. Logistic regression is generally accepted as an
algorithm that performs well on binary classiﬁcation. With the
addition of a regularization term we can obviate the problem
of overﬁtting that logistic regression tends to suffer from.

A. Initial Classiﬁcation Approach

III. RESULTS

Before attempting the algorithms listed above, we ﬁrst
attempted to see if there was a simple feature space in which
we could visualize and perhaps ﬁnd structure in the data. One
feature that was often cited as being useful for the purpose
of text classiﬁcation was the number of unique words per
document. In accordance with this, we created a scatter plot
of the number of unique words vs. song length.

As evidenced by Figure 1, this two dimensional feature
space was very densely populated with little discernible pattern
that would enable us to distinguish Lennon from McCartney.
Therefore, we found that lack of linear separability or distinct
clustering meant this feature space was not rich enough to
accurately perform the classiﬁcation. This observation was

CS229 FALL 2014

2

Fig. 1. The circles denote songs composed by John Lennon while the x’s
denote ones written by Paul McCartney

Fig. 2. Graphical depiction of the k-nearest neighbors algorithm. The dashed
circle corresponds to the chosen distance metric

veriﬁed by our initial attempts using k-NN which yielded an
accuracy of 45.16%.

In hopes of better capturing more information that could be
used to discriminate between the two artists, we chose to create
a design matrix X ∈ R93×1435 and a classiﬁcation vector
y ∈ R93

 − x(1) −

− x(93) −

...





 y(1)

...
y(93)

X =

y =

where x(i)
gives the number of times the jth word in our
j
vocabulary V appears in the ith Beatles’ song in our training
set where y(i) = 1 if McCartney was the artist of song i and
y(i) = 0 if Lennon was the artist.1

B. k-means & k-Nearest Neighbors

The k-means algorithm is an unsupervised learning algo-
rithm that clusters data based on a chosen distance metric.
This algorithm is ideal in the case that the convex hulls of
the points from each class are mutually disjoint.2 The k-
Nearest Neighbors (k-NN) algorithm is another non-parametric
algorithm in which the class label of a new example is
predicted by a majority vote of the k nearest neighbors (where
nearest was measured by either the (cid:96)1 or (cid:96)2 norm). We chose
to break ties by using the class label of the closest example to
the test point. It is also worth noting that k-NN exploits local
clustering of data and as a result tends to perform well on data
in which classes lie in a number of dense clusters [5].

1For the case of the SVM, the class designations are swtiched so that
2The convex hull of a set of points S = {x1, · · · , xn} is simply the set

y(i) = −1 identiﬁes a song composed by Lennon rather than 0
of all convex combinations of points in the set (i.e. conv(S) = {λT x|
0 ≺ λ, λT 1 = 1}

Initially, we supposed that the data for the Beatles’ songs
written by each author might lie in two distinct clusters of
our feature space. This situation might occur if, for example,
McCartney used one subset of the vocabulary much more
frequently than Lennon did. Another hypothesis for the struc-
ture of our data that we thought was reasonable was that the
classiﬁcations might lie close together in a number of smaller
clusters in the feature space. Since both of these seemed to be
plausible assumptions about the data, we used the k-means and
k-NN MATLAB implementations, iterating over the number of
neighbors and utilizing the 1-norm and 2-norm, the results of
which are shown in the table.

Algorithm k Accuracy Metric
k-means
k-NN
k-NN
k-NN
k-NN
k-NN
k-NN

53.76%
54.84%
53.76%
37.63%
25.81%
49.46%
50.54%

(cid:96)2
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1

2
1
1
2
2
5
5

As evidenced by the poor performance of k-means, it was
clear the data did not lie in two distinct clusters of our chosen
feature space. This result might be explained by Lennon and
McCartney having similar writing styles and therefore the
different classes were highly interspersed amongst each other.
The negative results of both algorithms could also be attributed
to the curse of dimensionality, as the feature space is over 1400
dimensional while the number of test examples was just 93.

C. Naive Bayes

Naive Bayes is a simple generative learning algorithm that
is easily applied to binary classiﬁcation problems. The key
assumption of the algorithm is the conditional independence
of each of the words given the document’s classiﬁcation (i.e.
p(xi, xj|y) = p(xi|y)p(xj|y). The algorithm then seeks to
learn a model by computing

max

θ

p(x, y) = max

θ

p(y)

where θ is the vector of parameters p(y = 1), p(xi|y = 1), and
p(xi|y = 0) which are to be estimated. In our initial attempts

n(cid:89)

i=1

p(xi|y)

CS229 FALL 2014

3

to apply Naive Bayes we implemented both the Bernoulli
and Multinomial event models with Laplace smoothing. The
results of which are summarized below.

Event Model Accuracy
62.37%
53.76%

Bernoulli
Multinomial

As was the case with the previous learning algorithms, Naive
Bayes only slightly outperformed random guessing. The rea-
sons for the failure of this particular learning algorithm is most
likely due to the small training set as well as the conditional
independence assumption being violated.

D. SVMs & Regularized Logistic Regression

Algorithm
Logistic

Loss

log-loss

SVM
SVM
SVM

(cid:96)2
(cid:96)1
(cid:96)2

ˆλ

Penalty Accuracy
55.69%
0.0015
58.06% 6.9 ×10−4
61.29%
63.44%

0.002
0.02

(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)1

IV. AN ALTERNATIVE FEATURE SPACE

In light of the poor performance of many of the algorithms
we used to perform the classiﬁcation, we sought alternative
feature mappings for the lyrical data. In the course of our
research, we discovered term frequency - inverse document
frequency (tf-idf ) is a standard feature mapping that performs
well in text classiﬁcation. To perform the feature mapping we
deﬁne the following quantities [7]

The last two algorithms that we attempted were Support
Vector Machines and regularized logistic regression, both of
which are supported by the LIBLINEAR package [6]. In order
to fully investigate the range of classiﬁers available, we used
a variety of objectives for the SVM algorithm such as (cid:96)2 loss
with (cid:96)2 penalty:

J1 = 1/2(cid:107)w(cid:107)2
as well as (cid:96)2 loss with (cid:96)1 penalty:
J2 = 1/2(cid:107)w(cid:107)2

2 + λ(cid:107)ξ(cid:107)2

2

2 + λ(cid:107)ξ(cid:107)1

among other options supported by the software package. For
the logistic regression learning algorithm we also experi-
mented with values of the regularization term for the objective

(cid:16)

m(cid:88)

i=1

JLR (θ) =

log

1 + exp

(cid:16)−y(i)θT ˜x(i)(cid:17)(cid:17)
(cid:105)T
(cid:104)(cid:0)x(i)(cid:1)T

1

+ λ(cid:107)θ(cid:107)2

2

with θ, ˜x(i) ∈ R94 where ˜x(i) =
. Assuming we
have a cost function J for the unregularized algorithm, our
objective for the regularized learning algorithms take the form

J + λ · fp ((cid:107)x(cid:107)p)

where (cid:107)x(cid:107)p is deﬁned for p ≥ 1 as

(cid:33)1/p

(cid:107)x(cid:107)p (cid:44)

|xi|p

and fp : R+ → R+ is the mapping

(cid:32) n(cid:88)
(cid:40)

i=1

z
z2

f (z) (cid:44)

if p = 1
if p = 2

The value λ ∈ R is the regularization parameter which helps
stabilize the objective J [7]. To choose the value of the reg-
ularization parameter, we iterated over a logarithmic spacing
of values from 0.0001 to 5000 and computed the resulting
accuracies for each value of λ. The accuracies reported in
table below are for

ˆλ ≈ arg max

λ

ˆτLOOCV

Speciﬁcally, we selected the value of λ that produced the
maximum accuracy on our set of examples under Leave One
Out Cross Validation.

nw(x) = number of times word w appears in doc x

f (w|x) =

= term frequency

(cid:80)(cid:48)

nw(x)
w nw(cid:48)(x)

(cid:20)

φw(x) = f (w|x) log

# of docs + 1

# of docs with word w + 1

(cid:21)

The “term frequency” portion of the mapping gives a weight-
ing corresponding to how often word w appears in document
x while the “inverse document frequency” portion serves to
deemphasize words which appear in a large fraction of the
documents. The addition of one in both the numerator and
denominator of the idf term acts as a form of smoothing and
prevents division by zero in the case that a word does not
appear [8].

Using this new feature mapping we reran a selection of our
previous algorithms to see if the new feature mapping φw(x)
garnered any improvement in classiﬁcation accuracy and per-
haps be worth pursuing. The following table summarizes the
results of implementing the tf-idf frequency feature mapping.

Algorithm
Logistic

SVM

k-means

Loss

log-loss

(cid:96)1
-

Penalty Accuracy

ˆλ
60.22% 2.5
10
58.06%
44.09 %
-

(cid:96)2
(cid:96)2
-

We speculate that the reasons for the poor performance of
tf-idf for this particular application are similar to the reasons
why many of our aforementioned algorithms failed. More
speciﬁcally, we were using very few training examples relative
to the size of the feature space we were attempting to learn.
Additionally, since each training example consists of the lyrics
to a song, rather than a much larger body of work, say an essay
or a book, the examples will not be as informative as would
be the case for classiﬁcation of larger documents.

V. REDUCING THE FEATURE SPACE DIMENSIONALITY
Lastly, with most of the algorithms unable to achieve any
signiﬁcant accuracy, we explored the possibility of reducing
the dimensionality of the data. Liang et. al. remarks, “low-
content ‘stop words’ have been shown to be very useful
statistical cues of sentiment and psychology” which led us

CS229 FALL 2014

4

to believe that perhaps we were not justiﬁed in removing the
stop words from our vocabulary [9].

Futhermore, Fung discusses in his paper “The Disputed
Federalist Papers: SVM Feature Selection via Concave Mini-
mization” the use of a smaller set of 70 “function words” rather
than a complete vocabulary for improving text classiﬁcation
[2]. In fact, Fung demonstrates the ability to classify 12
disputed Federalist Papers using only a three dimensional
feature space consisting of the words ‘upon’ , ‘would’, and
‘to’.

In order to choose the subset of say, 10 words, that provide
the best accuracy to use as our feature space, we would need

(cid:1)10 ≈ 4 × 1021 different combinations

to try (cid:0)1435

(cid:1) ≥ (cid:0) 1435

10

10

of words, a clearly intractable problem already, let alone for
a vocabulary of size 20. In light of this, we decided that
implementing a simple forward search would be the best way
to generate of low dimensional representation of our data.

Starting with an empty vocabulary, we built up a lexicon
by iterating over the inclusion of every additional word that
was not already in the set and subsequently included the word
that achieved the highest value of ˆτLOOCV . The number of
words we ultimately included in our vocabulary was selected
as the number of words after which the addition of several new
features no longer improved training accuracy. Figure 3 shows
how the training accuracy increase as a function of the vocab-
ulary size under forward search. For the SVM algorithms, the
ﬁrst item in the legend denotes the regularization norm and
the second speciﬁes the loss function for the objective.

Figure 3 that are worth drawing attention to. First, the (cid:96)2
regularized (cid:96)2 loss SVM learning algorithm (shown in light
blue) learned far better with a small feature space than any
of the others, initially gaining nearly 10% accuracy for each
word added to the vocabulary (after the ﬁrst word). However,
it ended up performing the worst by the completion of the
forward search, with the learning curve tapering off to ≈ 90%
after only 9 words. On the other hand, the (cid:96)2 regularized (cid:96)1
loss SVM, which ended up performing the best, experienced
one of the slower learning rates, coming in with consistently
lesser or equal accuracies to the other three learning objectives.
In addition to examining the Accuracy vs. Number of
Features curve, we also looked at the vocabularies generated.
The words obtained from the forward search using the (cid:96)2
regularized, (cid:96)1 penalty SVM is displayed below

1) it’s
2) would
3) can
4) alone
5) stop
6) do

7) bag
8) everybody’s
9) ﬁll
10) ﬂoat
11) mr
12) after

13) answer
14) remember
15) sky
16) about
17) above
18) accidents

Though using forward search with the other three objectives
did result in different vocabularies for each algorithm, the ﬁrst
several words generally tended to be the same across all the
algorithms indicating that there are certain words which are
particularly important in distinguishing the two composers.

Seeing that the forward search provided such dramatic im-
provements in classiﬁcation accuracy for the SVM and logistic
regression based algorithms, we also implemented a forward
search using Naive Bayes.3 As we had come to expect from
our research as well as our success using SVMs, the Naive
Bayes approach saw a remarkable increase in classiﬁcation
accuracy when used in conjunction with feature selection.
While the Naive Bayes algorithm using the original 1435
dimensional feature space only achieved ˆτLOOCV = 53.76%,
using feature selection to build up a 12 word vocabulary we
were able to increase this number to nearly 94%.

Fig. 3. Training accuracy as a function of vocabulary size for various
optimization objectives. All algorithms were trained using regularization
parameter C = 1

It can clearly be seen from the graph above that using this
greedy approach to choose a subset of our feature space had
a profound effect on classiﬁcation accuracy. As can be seen
from the plot, the accuracy grows quickly as new words are
added to the vocabulary (at least 5% with each added word
across all algorithms) and then almost all of the algorithms’
training accuracies begin to plateau after 13 words, with the
(cid:96)2 regularized (cid:96)1 penalty SVM performing the best, attaining
a classiﬁcation accuracy of 97.85%

Before moving on, there are some interesting patterns in

Fig. 4. The learning curve for Naive Bayes plateaus after a 12 dimensional
feature set is achieved, attaining an accuracy of 94% at saturation

3Multinomial Event model

CS229 FALL 2014

1

A Binary Classiﬁcation of Beatles Song Authorship

Miles Bennett, Casey Haaland, and Atsu Kobashi

I. INTRODUCTION

M ACHINE learning algorithms have a long history with

the problem of text classiﬁcation which has been used
extensively for a variety of applications including spam classi-
ﬁcation as well as the identiﬁcation of the disputed authorship
of several Federalist Papers [1][2]. In our project, we attempt
to use the tools of machine learning to identify the primary
author of a given Beatles’ song using the lyrics as features.
Since a large majority of Beatles songs were written by either
John Lennon or Paul McCartney, we restricted the scope of
our project to a simple binary classiﬁcation problem. Though
text classiﬁcation is a well known and widely used application
of machine learning, the aim of this project is to analyze the
performance of various classiﬁcation algorithms and quantify
their suitability for this problem.

II. METHODOLOGY

A. Data Collection and Preprocessing

After converting a PDF of Beatles lyrics to plain text format
we ﬁrst separated the set of lyrics into separate text ﬁles for
each song [3]. In the ﬁrst iteration of testing, we also removed
words we believed were content free, in an attempt to improve
the accuracy of our classiﬁer. Since some basic words were
likely to appear in a vast majority of the songs, we felt it was
likely that these words would not be informative features in
determining authorship so we would be justiﬁed in removing
them. As such, we deﬁned the following words which would
not be included in the feature space:

1) “the”
2) “a”
3) “and”

4) “or”
5) “but”
6) “if”

In addition to removing these so called “stop words”, we
also lemmatized the song lyrics using a library provided by the
Natural Language Toolkit (NLTK), an open source module for
python [4]. The lemmatization process converts a noun, verb,
or adjective to the canonical form of the word. For example,
the plural noun “dogs” would be converted to the singular
noun “dog” and the word “went” would be converted to “go.”
By lemmatizing the aggregate list of all words that appear in
the corpus of Beatles songs, the size of the vocabulary was
reduced from 1435 to 1142. This is a similar preprocessing
technique that was used in Problem Set #2.

Lastly, because there are only 93 songs written by either
Lennon or McCartney, our data set was relatively small. For
this reason, in the algorithms to follow, all accuracies are
reported under leave one out cross validation. Speciﬁcally, we

evaluated our experimental accuracy as:

n(cid:88)

i=1

(cid:16)

x(i)(cid:17) (cid:54)= y(i)(cid:111)

(cid:110)

1

hθ

ˆτLOOCV = 1 − 1
n

Since 43 of the songs were by McCartney and 50 by
Lennon, there was very little class imbalance and this seemed
to be an appropriate metric for the classiﬁer.

B. Algorithms

There are a variety of well known algorithms in the ﬁeld of
machine learning for the purpose of binary text classiﬁcation.
One possibility that our group hypothesized was that songs
written by Lennon and songs written by McCartney would
lie close together in a cluster. For this reason we wanted to
explore the use of non-parametric clustering algorithms such
as k-means or k-Nearest Neighbors as well as investigate the
effect of using different distance metrics (e.g. (cid:96)1 and (cid:96)2) on
our classiﬁcation accuracy.

Another well known algorithm considered to be both simple
and effective for text classiﬁcation problems is Naive Bayes.
The large volume of literature concerning the use of Naive
Bayes for textual analysis seems to suggest both the Bernoulli
and Multinomial event model would be well suited for our
classiﬁcation problem.

The last

two algorithms we seek to investigate in this
project are Support Vector Machines and regularized logistic
regression. SVMs have many desirable properties such as
large margin, the ability to be kernelized, and a relatively low
tendency to overﬁt, all of which would allow it to perform well
on this problem. Logistic regression is generally accepted as an
algorithm that performs well on binary classiﬁcation. With the
addition of a regularization term we can obviate the problem
of overﬁtting that logistic regression tends to suffer from.

A. Initial Classiﬁcation Approach

III. RESULTS

Before attempting the algorithms listed above, we ﬁrst
attempted to see if there was a simple feature space in which
we could visualize and perhaps ﬁnd structure in the data. One
feature that was often cited as being useful for the purpose
of text classiﬁcation was the number of unique words per
document. In accordance with this, we created a scatter plot
of the number of unique words vs. song length.

As evidenced by Figure 1, this two dimensional feature
space was very densely populated with little discernible pattern
that would enable us to distinguish Lennon from McCartney.
Therefore, we found that lack of linear separability or distinct
clustering meant this feature space was not rich enough to
accurately perform the classiﬁcation. This observation was

CS229 FALL 2014

2

Fig. 1. The circles denote songs composed by John Lennon while the x’s
denote ones written by Paul McCartney

Fig. 2. Graphical depiction of the k-nearest neighbors algorithm. The dashed
circle corresponds to the chosen distance metric

veriﬁed by our initial attempts using k-NN which yielded an
accuracy of 45.16%.

In hopes of better capturing more information that could be
used to discriminate between the two artists, we chose to create
a design matrix X ∈ R93×1435 and a classiﬁcation vector
y ∈ R93

 − x(1) −

− x(93) −

...





 y(1)

...
y(93)

X =

y =

where x(i)
gives the number of times the jth word in our
j
vocabulary V appears in the ith Beatles’ song in our training
set where y(i) = 1 if McCartney was the artist of song i and
y(i) = 0 if Lennon was the artist.1

B. k-means & k-Nearest Neighbors

The k-means algorithm is an unsupervised learning algo-
rithm that clusters data based on a chosen distance metric.
This algorithm is ideal in the case that the convex hulls of
the points from each class are mutually disjoint.2 The k-
Nearest Neighbors (k-NN) algorithm is another non-parametric
algorithm in which the class label of a new example is
predicted by a majority vote of the k nearest neighbors (where
nearest was measured by either the (cid:96)1 or (cid:96)2 norm). We chose
to break ties by using the class label of the closest example to
the test point. It is also worth noting that k-NN exploits local
clustering of data and as a result tends to perform well on data
in which classes lie in a number of dense clusters [5].

1For the case of the SVM, the class designations are swtiched so that
2The convex hull of a set of points S = {x1, · · · , xn} is simply the set

y(i) = −1 identiﬁes a song composed by Lennon rather than 0
of all convex combinations of points in the set (i.e. conv(S) = {λT x|
0 ≺ λ, λT 1 = 1}

Initially, we supposed that the data for the Beatles’ songs
written by each author might lie in two distinct clusters of
our feature space. This situation might occur if, for example,
McCartney used one subset of the vocabulary much more
frequently than Lennon did. Another hypothesis for the struc-
ture of our data that we thought was reasonable was that the
classiﬁcations might lie close together in a number of smaller
clusters in the feature space. Since both of these seemed to be
plausible assumptions about the data, we used the k-means and
k-NN MATLAB implementations, iterating over the number of
neighbors and utilizing the 1-norm and 2-norm, the results of
which are shown in the table.

Algorithm k Accuracy Metric
k-means
k-NN
k-NN
k-NN
k-NN
k-NN
k-NN

53.76%
54.84%
53.76%
37.63%
25.81%
49.46%
50.54%

(cid:96)2
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1

2
1
1
2
2
5
5

As evidenced by the poor performance of k-means, it was
clear the data did not lie in two distinct clusters of our chosen
feature space. This result might be explained by Lennon and
McCartney having similar writing styles and therefore the
different classes were highly interspersed amongst each other.
The negative results of both algorithms could also be attributed
to the curse of dimensionality, as the feature space is over 1400
dimensional while the number of test examples was just 93.

C. Naive Bayes

Naive Bayes is a simple generative learning algorithm that
is easily applied to binary classiﬁcation problems. The key
assumption of the algorithm is the conditional independence
of each of the words given the document’s classiﬁcation (i.e.
p(xi, xj|y) = p(xi|y)p(xj|y). The algorithm then seeks to
learn a model by computing

max

θ

p(x, y) = max

θ

p(y)

where θ is the vector of parameters p(y = 1), p(xi|y = 1), and
p(xi|y = 0) which are to be estimated. In our initial attempts

n(cid:89)

i=1

p(xi|y)

CS229 FALL 2014

3

to apply Naive Bayes we implemented both the Bernoulli
and Multinomial event models with Laplace smoothing. The
results of which are summarized below.

Event Model Accuracy
62.37%
53.76%

Bernoulli
Multinomial

As was the case with the previous learning algorithms, Naive
Bayes only slightly outperformed random guessing. The rea-
sons for the failure of this particular learning algorithm is most
likely due to the small training set as well as the conditional
independence assumption being violated.

D. SVMs & Regularized Logistic Regression

Algorithm
Logistic

Loss

log-loss

SVM
SVM
SVM

(cid:96)2
(cid:96)1
(cid:96)2

ˆλ

Penalty Accuracy
55.69%
0.0015
58.06% 6.9 ×10−4
61.29%
63.44%

0.002
0.02

(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)1

IV. AN ALTERNATIVE FEATURE SPACE

In light of the poor performance of many of the algorithms
we used to perform the classiﬁcation, we sought alternative
feature mappings for the lyrical data. In the course of our
research, we discovered term frequency - inverse document
frequency (tf-idf ) is a standard feature mapping that performs
well in text classiﬁcation. To perform the feature mapping we
deﬁne the following quantities [7]

The last two algorithms that we attempted were Support
Vector Machines and regularized logistic regression, both of
which are supported by the LIBLINEAR package [6]. In order
to fully investigate the range of classiﬁers available, we used
a variety of objectives for the SVM algorithm such as (cid:96)2 loss
with (cid:96)2 penalty:

J1 = 1/2(cid:107)w(cid:107)2
as well as (cid:96)2 loss with (cid:96)1 penalty:
J2 = 1/2(cid:107)w(cid:107)2

2 + λ(cid:107)ξ(cid:107)2

2

2 + λ(cid:107)ξ(cid:107)1

among other options supported by the software package. For
the logistic regression learning algorithm we also experi-
mented with values of the regularization term for the objective

(cid:16)

m(cid:88)

i=1

JLR (θ) =

log

1 + exp

(cid:16)−y(i)θT ˜x(i)(cid:17)(cid:17)
(cid:105)T
(cid:104)(cid:0)x(i)(cid:1)T

1

+ λ(cid:107)θ(cid:107)2

2

with θ, ˜x(i) ∈ R94 where ˜x(i) =
. Assuming we
have a cost function J for the unregularized algorithm, our
objective for the regularized learning algorithms take the form

J + λ · fp ((cid:107)x(cid:107)p)

where (cid:107)x(cid:107)p is deﬁned for p ≥ 1 as

(cid:33)1/p

(cid:107)x(cid:107)p (cid:44)

|xi|p

and fp : R+ → R+ is the mapping

(cid:32) n(cid:88)
(cid:40)

i=1

z
z2

f (z) (cid:44)

if p = 1
if p = 2

The value λ ∈ R is the regularization parameter which helps
stabilize the objective J [7]. To choose the value of the reg-
ularization parameter, we iterated over a logarithmic spacing
of values from 0.0001 to 5000 and computed the resulting
accuracies for each value of λ. The accuracies reported in
table below are for

ˆλ ≈ arg max

λ

ˆτLOOCV

Speciﬁcally, we selected the value of λ that produced the
maximum accuracy on our set of examples under Leave One
Out Cross Validation.

nw(x) = number of times word w appears in doc x

f (w|x) =

= term frequency

(cid:80)(cid:48)

nw(x)
w nw(cid:48)(x)

(cid:20)

φw(x) = f (w|x) log

# of docs + 1

# of docs with word w + 1

(cid:21)

The “term frequency” portion of the mapping gives a weight-
ing corresponding to how often word w appears in document
x while the “inverse document frequency” portion serves to
deemphasize words which appear in a large fraction of the
documents. The addition of one in both the numerator and
denominator of the idf term acts as a form of smoothing and
prevents division by zero in the case that a word does not
appear [8].

Using this new feature mapping we reran a selection of our
previous algorithms to see if the new feature mapping φw(x)
garnered any improvement in classiﬁcation accuracy and per-
haps be worth pursuing. The following table summarizes the
results of implementing the tf-idf frequency feature mapping.

Algorithm
Logistic

SVM

k-means

Loss

log-loss

(cid:96)1
-

Penalty Accuracy

ˆλ
60.22% 2.5
10
58.06%
44.09 %
-

(cid:96)2
(cid:96)2
-

We speculate that the reasons for the poor performance of
tf-idf for this particular application are similar to the reasons
why many of our aforementioned algorithms failed. More
speciﬁcally, we were using very few training examples relative
to the size of the feature space we were attempting to learn.
Additionally, since each training example consists of the lyrics
to a song, rather than a much larger body of work, say an essay
or a book, the examples will not be as informative as would
be the case for classiﬁcation of larger documents.

V. REDUCING THE FEATURE SPACE DIMENSIONALITY
Lastly, with most of the algorithms unable to achieve any
signiﬁcant accuracy, we explored the possibility of reducing
the dimensionality of the data. Liang et. al. remarks, “low-
content ‘stop words’ have been shown to be very useful
statistical cues of sentiment and psychology” which led us

CS229 FALL 2014

4

to believe that perhaps we were not justiﬁed in removing the
stop words from our vocabulary [9].

Futhermore, Fung discusses in his paper “The Disputed
Federalist Papers: SVM Feature Selection via Concave Mini-
mization” the use of a smaller set of 70 “function words” rather
than a complete vocabulary for improving text classiﬁcation
[2]. In fact, Fung demonstrates the ability to classify 12
disputed Federalist Papers using only a three dimensional
feature space consisting of the words ‘upon’ , ‘would’, and
‘to’.

In order to choose the subset of say, 10 words, that provide
the best accuracy to use as our feature space, we would need

(cid:1)10 ≈ 4 × 1021 different combinations

to try (cid:0)1435

(cid:1) ≥ (cid:0) 1435

10

10

of words, a clearly intractable problem already, let alone for
a vocabulary of size 20. In light of this, we decided that
implementing a simple forward search would be the best way
to generate of low dimensional representation of our data.

Starting with an empty vocabulary, we built up a lexicon
by iterating over the inclusion of every additional word that
was not already in the set and subsequently included the word
that achieved the highest value of ˆτLOOCV . The number of
words we ultimately included in our vocabulary was selected
as the number of words after which the addition of several new
features no longer improved training accuracy. Figure 3 shows
how the training accuracy increase as a function of the vocab-
ulary size under forward search. For the SVM algorithms, the
ﬁrst item in the legend denotes the regularization norm and
the second speciﬁes the loss function for the objective.

Figure 3 that are worth drawing attention to. First, the (cid:96)2
regularized (cid:96)2 loss SVM learning algorithm (shown in light
blue) learned far better with a small feature space than any
of the others, initially gaining nearly 10% accuracy for each
word added to the vocabulary (after the ﬁrst word). However,
it ended up performing the worst by the completion of the
forward search, with the learning curve tapering off to ≈ 90%
after only 9 words. On the other hand, the (cid:96)2 regularized (cid:96)1
loss SVM, which ended up performing the best, experienced
one of the slower learning rates, coming in with consistently
lesser or equal accuracies to the other three learning objectives.
In addition to examining the Accuracy vs. Number of
Features curve, we also looked at the vocabularies generated.
The words obtained from the forward search using the (cid:96)2
regularized, (cid:96)1 penalty SVM is displayed below

1) it’s
2) would
3) can
4) alone
5) stop
6) do

7) bag
8) everybody’s
9) ﬁll
10) ﬂoat
11) mr
12) after

13) answer
14) remember
15) sky
16) about
17) above
18) accidents

Though using forward search with the other three objectives
did result in different vocabularies for each algorithm, the ﬁrst
several words generally tended to be the same across all the
algorithms indicating that there are certain words which are
particularly important in distinguishing the two composers.

Seeing that the forward search provided such dramatic im-
provements in classiﬁcation accuracy for the SVM and logistic
regression based algorithms, we also implemented a forward
search using Naive Bayes.3 As we had come to expect from
our research as well as our success using SVMs, the Naive
Bayes approach saw a remarkable increase in classiﬁcation
accuracy when used in conjunction with feature selection.
While the Naive Bayes algorithm using the original 1435
dimensional feature space only achieved ˆτLOOCV = 53.76%,
using feature selection to build up a 12 word vocabulary we
were able to increase this number to nearly 94%.

Fig. 3. Training accuracy as a function of vocabulary size for various
optimization objectives. All algorithms were trained using regularization
parameter C = 1

It can clearly be seen from the graph above that using this
greedy approach to choose a subset of our feature space had
a profound effect on classiﬁcation accuracy. As can be seen
from the plot, the accuracy grows quickly as new words are
added to the vocabulary (at least 5% with each added word
across all algorithms) and then almost all of the algorithms’
training accuracies begin to plateau after 13 words, with the
(cid:96)2 regularized (cid:96)1 penalty SVM performing the best, attaining
a classiﬁcation accuracy of 97.85%

Before moving on, there are some interesting patterns in

Fig. 4. The learning curve for Naive Bayes plateaus after a 12 dimensional
feature set is achieved, attaining an accuracy of 94% at saturation

3Multinomial Event model

CS229 FALL 2014

5

Having markedly improved our accuracy for SVM, logistic
regression, and Naive Bayes we revisited the tf-idf feature
mapping to see if it similarly improved with the use of feature
selection. Interestingly, retraining on all the algorithms (not
including Naive Bayes) achieved the accuracy proﬁle shown
below.

potential application of this classiﬁer is the identiﬁcation of
the principal author of songs written by multiple individuals
and for songs with disputed authorship. A further extension
to the project would be to include audio data in the feature
space or to examine if audio data alone (without lyrics) could
be used to determine the author of a song.

REFERENCES

[1] Mehran Sahami, Susan Damais, David Heckerman, Eric Horvitz. A

Bayesian Approach to Filtering Junk E-Mail. 2014 Dec 3.

[2] Glen

Fung,

Selection

via

Feature
http://pages.cs.wisc.edu/ gfung/federalist.pdf
2014 Dec 3.

Concave MinimizationAvailable

The

Disputed

Federalist

Papers:

SVM
at

[3] Fred. THE BEATLES: Complete Lyrics of all Songs Available at

http://www.gratuit-cours.com 2014 Dec 3.

[4] Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language

Processing with Python. OReilly Media Inc. 2014 Dec 3.

[5] John Cast, Chris Schulze, Ali Fauci Music Genre Classiﬁcation.
Available at http://cs229.stanford.edu/proj2013. 2014
Dec 3.

[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin.
LIBLINEAR: A Library for Large Linear Classiﬁcation, Journal
of Machine Learning Research 9(2008), 1871-1874. Available at
http://www.csie.ntu.edu.tw/ cjlin/liblinear

[7] Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall
2006. MIT OpenCourseWare, Massachusetts Institute of Technology.
Available at http://ocw.mit.edu/ 2014 Dec 3.

[8] George Forman (2007), Feature Selection for Text Classiﬁcation

Available at http://www.hpl.hp.com/techreports/2007

[9] Dawen Liang, Haijie Gu, and Brendan O’Connor (2011)Music Genre

Classiﬁcation with the Million Song Dataset 2014 Dec 3.

Fig. 5. Shows how the tf-idf feature mapping affects training accuracy.
Interestingly,
the curve was identical for the three SVM algorithms and
regularized logistic regression

Overall the tf-idf feature mapping exhibited a similar in-
crease in accuracy from the forward search but overall was
not noticeably better than our initial feature mapping consist-
ing of raw word counts. The general shape of the learning
curve shows the same general trend as the SVM and logistic
regression accuracies, ﬂattening out after a little over a dozen
words.

VI. CONCLUSIONS

The results of our project indicate that it is indeed possible
to distinguish between songs composed by Paul McCartney
and songs by John Lennon based only on song’s lyrics. From
our results we have concluded that Support Vector Machines
yield the best performance for this binary classiﬁcation prob-
lem. More important than the particular algorithm however,
is choice of features. Through the course of this project we
discovered that many of these algorithms will only perform
well provided you have chosen a ‘good’ feature space in which
to represent the data.

Additionally, much of the time spent preprocessing the data
(i.e. lemmatization and removing ‘stop’ word) provided no
noticeable performance increase as compared with selecting
the proper features. The results of our project seem to suggest
that much of the intuition that underlies the removal of so-
called ‘stop’ words is not applicable in this setting as many of
the words we suspected of being content free were in fact the
highest precedence features with regard to the forward search
feature selection.

Regarding future work, our binary classiﬁer could in prin-
ciple be expanded to a multi-class classiﬁer. This multi-
classiﬁer could include all of the members of the Beatles: Paul
McCartney, John Lennon, George Harrison and Ringo Star. A

