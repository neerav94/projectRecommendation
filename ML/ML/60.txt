Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JHWANG89@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial distri-
bution of certain visual elements within an
image correlates with its aesthetic quality.
To this end, we present a novel approach
wherein we model each photograph as a set
of image tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 85.03%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence, arrange-
ment, and combination of certain visual characteris-
tics does indeed make an image more aesthetically-
pleasing in general. To understand why this may be
true, consider the two images shown in Figure 1. Al-
though both are of ﬂowers, the left photograph has
a signiﬁcantly higher aesthetic rating than the right
photograph. One might reason that this is so sim-
ply because the right photograph is blurry. This con-

Figure 1. A photograph with a high aesthetic rating (left)
and a photograph with a low aesthetic rating (right).

jecture, however, is imprecise because the left photo-
graph is, on average, blurry as well. In fact, the major-
ity of the image is even blurrier than the right photo-
graph. Here, it seems that the juxtaposition of sharp
salient regions with blurry regions and their locations
in the frame positively inﬂuence the perceived aes-
thetic quality of the photograph.
Accordingly, we begin by identifying generic visual
features that we believe may affect the aesthetic qual-
ity of a photograph, such as blur and hue. We then
build a learning pipeline that extracts these features
from images on a per-image-tile basis and uses them
along with the images’ aesthetics ratings to train a
classiﬁer. In this manner, we endow the classiﬁer with
the ability to infer spatial relationships amongst fea-
tures that correlate with an image’s aesthetics.
The potential impact of building a system to solve
this problem is broad. For example, by implement-
ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JHWANG89@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial distri-
bution of certain visual elements within an
image correlates with its aesthetic quality.
To this end, we present a novel approach
wherein we model each photograph as a set
of image tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 85.03%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence, arrange-
ment, and combination of certain visual characteris-
tics does indeed make an image more aesthetically-
pleasing in general. To understand why this may be
true, consider the two images shown in Figure 1. Al-
though both are of ﬂowers, the left photograph has
a signiﬁcantly higher aesthetic rating than the right
photograph. One might reason that this is so sim-
ply because the right photograph is blurry. This con-

Figure 1. A photograph with a high aesthetic rating (left)
and a photograph with a low aesthetic rating (right).

jecture, however, is imprecise because the left photo-
graph is, on average, blurry as well. In fact, the major-
ity of the image is even blurrier than the right photo-
graph. Here, it seems that the juxtaposition of sharp
salient regions with blurry regions and their locations
in the frame positively inﬂuence the perceived aes-
thetic quality of the photograph.
Accordingly, we begin by identifying generic visual
features that we believe may affect the aesthetic qual-
ity of a photograph, such as blur and hue. We then
build a learning pipeline that extracts these features
from images on a per-image-tile basis and uses them
along with the images’ aesthetics ratings to train a
classiﬁer. In this manner, we endow the classiﬁer with
the ability to infer spatial relationships amongst fea-
tures that correlate with an image’s aesthetics.
The potential impact of building a system to solve
this problem is broad. For example, by implement-
ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.

2. Related work
There have been several efforts to tackle this prob-
lem from different angles within the past decade.
Pogaˇcnik et al [1] believed that the features depended
heavily on identiﬁcation of the subject of the pho-
tograph. Datta et al [2] evaluated the performance
of different machine learning models (support vector
machines, decision trees) on the problem. Ke et al [3]
focused on extracting perceptual factors important to
professional photographers, such as color, noise, blur,
and spatial distribution of edges.
Also, in contrast to our approach, it is interesting
to note that these studies have focused on extract-
ing global features that attempt to capture prior be-
liefs on the spatiality of visual elements within high-
quality images. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle of the image, and Pogaˇcnik et al de-
ﬁned features that assessed adherence to a multitude
of compositional rules as well as the positioning of
the subject relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website that allows
members to rate community-uploaded images on a
scale of 1 to 10. The index ﬁle we used to locate im-
ages was generated by Murray et al [4]. Following
guidelines from prior work, we choose to use pho-
tographs with ratings above 7.2 or below 3.4, result-
ing in a dataset containing 3000 images split evenly
between positive and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into equally-sized tiles (Figure 2). By extracting fea-

Figure 2. Tiling scheme applied to image by learning
pipeline.

tures on a per-tile basis, the learning algorithm can
identify regions of interest and infer relationships be-
tween feature-tile pairs that indicate aesthetic qual-
ity. For example, in the case of the image depicted
in Figure 2, we surmise that the learning algorithm
would be able to discern the well-composed framing
of the pier from the features extracted from its con-
taining tiles with respect to those extracted from the
surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the im-
age’s subject from its background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

(cid:80)

fsd =

(x,y)∈Tile 1{I(x, y) = Thresholded edge}

|{(x, y)|(x, y) ∈ Tile}|

Color: A photograph’s color composition can dra-
matically inﬂuence how a person perceives a photo-
graph. We capture the color diversity within an image
tile using a color histogram that subdivides the three
dimensional RGB color space into 64 equally sized
bins. Since each pixel can take on one of 256 discrete
values in each color channel, this results in each bin
being a cube with 16 possible values in each dimen-
sion. We normalize each bin’s count by the total pixel
count so that it is invariant to image dimensions.
We also measure the average saturation and lumi-
nance of each tile’s pixels. Finally, for the entire im-
age, we compute the proportion of pixels that corre-
spond to a particular hue (red, yellow, green, blue,
and purple).
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of the

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JHWANG89@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial distri-
bution of certain visual elements within an
image correlates with its aesthetic quality.
To this end, we present a novel approach
wherein we model each photograph as a set
of image tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 85.03%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence, arrange-
ment, and combination of certain visual characteris-
tics does indeed make an image more aesthetically-
pleasing in general. To understand why this may be
true, consider the two images shown in Figure 1. Al-
though both are of ﬂowers, the left photograph has
a signiﬁcantly higher aesthetic rating than the right
photograph. One might reason that this is so sim-
ply because the right photograph is blurry. This con-

Figure 1. A photograph with a high aesthetic rating (left)
and a photograph with a low aesthetic rating (right).

jecture, however, is imprecise because the left photo-
graph is, on average, blurry as well. In fact, the major-
ity of the image is even blurrier than the right photo-
graph. Here, it seems that the juxtaposition of sharp
salient regions with blurry regions and their locations
in the frame positively inﬂuence the perceived aes-
thetic quality of the photograph.
Accordingly, we begin by identifying generic visual
features that we believe may affect the aesthetic qual-
ity of a photograph, such as blur and hue. We then
build a learning pipeline that extracts these features
from images on a per-image-tile basis and uses them
along with the images’ aesthetics ratings to train a
classiﬁer. In this manner, we endow the classiﬁer with
the ability to infer spatial relationships amongst fea-
tures that correlate with an image’s aesthetics.
The potential impact of building a system to solve
this problem is broad. For example, by implement-
ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.

2. Related work
There have been several efforts to tackle this prob-
lem from different angles within the past decade.
Pogaˇcnik et al [1] believed that the features depended
heavily on identiﬁcation of the subject of the pho-
tograph. Datta et al [2] evaluated the performance
of different machine learning models (support vector
machines, decision trees) on the problem. Ke et al [3]
focused on extracting perceptual factors important to
professional photographers, such as color, noise, blur,
and spatial distribution of edges.
Also, in contrast to our approach, it is interesting
to note that these studies have focused on extract-
ing global features that attempt to capture prior be-
liefs on the spatiality of visual elements within high-
quality images. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle of the image, and Pogaˇcnik et al de-
ﬁned features that assessed adherence to a multitude
of compositional rules as well as the positioning of
the subject relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website that allows
members to rate community-uploaded images on a
scale of 1 to 10. The index ﬁle we used to locate im-
ages was generated by Murray et al [4]. Following
guidelines from prior work, we choose to use pho-
tographs with ratings above 7.2 or below 3.4, result-
ing in a dataset containing 3000 images split evenly
between positive and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into equally-sized tiles (Figure 2). By extracting fea-

Figure 2. Tiling scheme applied to image by learning
pipeline.

tures on a per-tile basis, the learning algorithm can
identify regions of interest and infer relationships be-
tween feature-tile pairs that indicate aesthetic qual-
ity. For example, in the case of the image depicted
in Figure 2, we surmise that the learning algorithm
would be able to discern the well-composed framing
of the pier from the features extracted from its con-
taining tiles with respect to those extracted from the
surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the im-
age’s subject from its background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

(cid:80)

fsd =

(x,y)∈Tile 1{I(x, y) = Thresholded edge}

|{(x, y)|(x, y) ∈ Tile}|

Color: A photograph’s color composition can dra-
matically inﬂuence how a person perceives a photo-
graph. We capture the color diversity within an image
tile using a color histogram that subdivides the three
dimensional RGB color space into 64 equally sized
bins. Since each pixel can take on one of 256 discrete
values in each color channel, this results in each bin
being a cube with 16 possible values in each dimen-
sion. We normalize each bin’s count by the total pixel
count so that it is invariant to image dimensions.
We also measure the average saturation and lumi-
nance of each tile’s pixels. Finally, for the entire im-
age, we compute the proportion of pixels that corre-
spond to a particular hue (red, yellow, green, blue,
and purple).
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of the

image tile to the number of edge pixels in the original
image tile, i.e.

(cid:80)

fd =

(cid:80)
(x,y)∈Tile 1{If iltered(x, y) = Edge}

(x,y)∈Tile 1{I(x, y) = Edge}

For an image tile that is exceptionally detailed, many
of the higher-frequency edges in the region would be
removed by the Gaussian ﬁlter. Consequently, we
would expect fd to be closer to 0. Conversely, for a
tile that lacks detail, since few edges exist in the re-
gion, applying the Gaussian ﬁlter would impart little
change to the number of edges. In this case, we would
expect fd to be closer to 1.
Contrast: Contrast is the difference in color or bright-
ness amongst regions in an image. Generally, the
higher the contrast, the more distinguishable objects
are from one another. We approximate the contrast
within each image tile by calculating the standard de-
viation of the grayscale intensities.
Blur: Depending on the image region, blurriness may
or may not be desirable. Poor technique or camera
shake tends to yield images that are blurry across the
entire frame, which is generally undesirable. On the
other hand, low depth-of-ﬁeld images with blurred
out-of-focus highlights (“bokeh”) that complement
sharp subjects are often regarded as being pleasing.
To efﬁciently estimate the amount of blur within an
image, we calculate the variance of the Laplacian of
the image. Low variance corresponds to blurrier im-
ages, and high variance to sharper images.
Noise: The desirability of visual noise is contextual.
For most modern images and for images that con-
vey positive emotions, noise is generally undesirable.
For images that convey negative semantics, however,
noise may be desirable to accentuate their visual im-
pact. We measure noise by calculating the image’s en-
tropy.
Saliency: The saliency of the subject within a photo-
graph can have a signiﬁcant impact on the perceived
aesthetic quality of the photograph. We post-process
each image to separate the salient region from the
background using a center-vs-surround approach de-
scribed in Achanta et al [5]. We then sum the number
of salient pixels per image tile and normalize by the
tile size.

5. Methods
Figure 3 depicts a high-level block diagram of the
learning pipeline we built. The pipeline comprises
three main components: an image scraper, a bank of
feature extractors, and a learning algorithm.

Figure 3. Block diagram of learning pipeline.

For each of the features we identiﬁed, there exists a
feature extractor function that accepts an image as
an input, calculates the feature value, and inserts the
feature-value mapping into a sparse feature vector al-
located for the image. We rely on image processing
algorithms implemented in the scikit-image and
opencv libraries for many of these functions [6, 7].
After the pipeline generates feature vectors for all
images in the training set, it uses them to train a
classiﬁer.
For the learning algorithm, we exper-
imented with scikit-learn’s implementations of
support vector machines (SVM), random forests (RF),
and gradient tree boosting (GBRT) [8]. We focus our
attention on these algorithms because they can ac-
count for non-linear relationships amongst features.
SVM: The SVM learning algorithm with (cid:96)1 regu-
larization involves solving the primal optimization
problem

min
γ,w,b

subject to

||w||2 + C

1
2
y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m

i=1

ξi

m(cid:88)

, the dual of which is

m(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

max

α

subject to

αi − 1
2

i,j=1

0 ≤ αi ≤ C, i = 1, ..., m

αiy(i) = 0

m(cid:88)
m(cid:88)

i=1

i=1

Accordingly, provided that we ﬁnd the values of α
that maximize the dual optimization problem, the hy-
pothesis can be formulated as

if (cid:80)m

(cid:26) 1

−1

h(x) =

i=1 αiy(i)(cid:104)x(i), x(cid:105) + b ≥ 0

otherwise

Note that since the dual optimization problem and
hypothesis can be expressed as inner products be-
tween input feature vectors, we can replace each in-
ner product with a kernel applied to the two input

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JHWANG89@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial distri-
bution of certain visual elements within an
image correlates with its aesthetic quality.
To this end, we present a novel approach
wherein we model each photograph as a set
of image tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 85.03%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence, arrange-
ment, and combination of certain visual characteris-
tics does indeed make an image more aesthetically-
pleasing in general. To understand why this may be
true, consider the two images shown in Figure 1. Al-
though both are of ﬂowers, the left photograph has
a signiﬁcantly higher aesthetic rating than the right
photograph. One might reason that this is so sim-
ply because the right photograph is blurry. This con-

Figure 1. A photograph with a high aesthetic rating (left)
and a photograph with a low aesthetic rating (right).

jecture, however, is imprecise because the left photo-
graph is, on average, blurry as well. In fact, the major-
ity of the image is even blurrier than the right photo-
graph. Here, it seems that the juxtaposition of sharp
salient regions with blurry regions and their locations
in the frame positively inﬂuence the perceived aes-
thetic quality of the photograph.
Accordingly, we begin by identifying generic visual
features that we believe may affect the aesthetic qual-
ity of a photograph, such as blur and hue. We then
build a learning pipeline that extracts these features
from images on a per-image-tile basis and uses them
along with the images’ aesthetics ratings to train a
classiﬁer. In this manner, we endow the classiﬁer with
the ability to infer spatial relationships amongst fea-
tures that correlate with an image’s aesthetics.
The potential impact of building a system to solve
this problem is broad. For example, by implement-
ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.

2. Related work
There have been several efforts to tackle this prob-
lem from different angles within the past decade.
Pogaˇcnik et al [1] believed that the features depended
heavily on identiﬁcation of the subject of the pho-
tograph. Datta et al [2] evaluated the performance
of different machine learning models (support vector
machines, decision trees) on the problem. Ke et al [3]
focused on extracting perceptual factors important to
professional photographers, such as color, noise, blur,
and spatial distribution of edges.
Also, in contrast to our approach, it is interesting
to note that these studies have focused on extract-
ing global features that attempt to capture prior be-
liefs on the spatiality of visual elements within high-
quality images. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle of the image, and Pogaˇcnik et al de-
ﬁned features that assessed adherence to a multitude
of compositional rules as well as the positioning of
the subject relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website that allows
members to rate community-uploaded images on a
scale of 1 to 10. The index ﬁle we used to locate im-
ages was generated by Murray et al [4]. Following
guidelines from prior work, we choose to use pho-
tographs with ratings above 7.2 or below 3.4, result-
ing in a dataset containing 3000 images split evenly
between positive and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into equally-sized tiles (Figure 2). By extracting fea-

Figure 2. Tiling scheme applied to image by learning
pipeline.

tures on a per-tile basis, the learning algorithm can
identify regions of interest and infer relationships be-
tween feature-tile pairs that indicate aesthetic qual-
ity. For example, in the case of the image depicted
in Figure 2, we surmise that the learning algorithm
would be able to discern the well-composed framing
of the pier from the features extracted from its con-
taining tiles with respect to those extracted from the
surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the im-
age’s subject from its background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

(cid:80)

fsd =

(x,y)∈Tile 1{I(x, y) = Thresholded edge}

|{(x, y)|(x, y) ∈ Tile}|

Color: A photograph’s color composition can dra-
matically inﬂuence how a person perceives a photo-
graph. We capture the color diversity within an image
tile using a color histogram that subdivides the three
dimensional RGB color space into 64 equally sized
bins. Since each pixel can take on one of 256 discrete
values in each color channel, this results in each bin
being a cube with 16 possible values in each dimen-
sion. We normalize each bin’s count by the total pixel
count so that it is invariant to image dimensions.
We also measure the average saturation and lumi-
nance of each tile’s pixels. Finally, for the entire im-
age, we compute the proportion of pixels that corre-
spond to a particular hue (red, yellow, green, blue,
and purple).
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of the

image tile to the number of edge pixels in the original
image tile, i.e.

(cid:80)

fd =

(cid:80)
(x,y)∈Tile 1{If iltered(x, y) = Edge}

(x,y)∈Tile 1{I(x, y) = Edge}

For an image tile that is exceptionally detailed, many
of the higher-frequency edges in the region would be
removed by the Gaussian ﬁlter. Consequently, we
would expect fd to be closer to 0. Conversely, for a
tile that lacks detail, since few edges exist in the re-
gion, applying the Gaussian ﬁlter would impart little
change to the number of edges. In this case, we would
expect fd to be closer to 1.
Contrast: Contrast is the difference in color or bright-
ness amongst regions in an image. Generally, the
higher the contrast, the more distinguishable objects
are from one another. We approximate the contrast
within each image tile by calculating the standard de-
viation of the grayscale intensities.
Blur: Depending on the image region, blurriness may
or may not be desirable. Poor technique or camera
shake tends to yield images that are blurry across the
entire frame, which is generally undesirable. On the
other hand, low depth-of-ﬁeld images with blurred
out-of-focus highlights (“bokeh”) that complement
sharp subjects are often regarded as being pleasing.
To efﬁciently estimate the amount of blur within an
image, we calculate the variance of the Laplacian of
the image. Low variance corresponds to blurrier im-
ages, and high variance to sharper images.
Noise: The desirability of visual noise is contextual.
For most modern images and for images that con-
vey positive emotions, noise is generally undesirable.
For images that convey negative semantics, however,
noise may be desirable to accentuate their visual im-
pact. We measure noise by calculating the image’s en-
tropy.
Saliency: The saliency of the subject within a photo-
graph can have a signiﬁcant impact on the perceived
aesthetic quality of the photograph. We post-process
each image to separate the salient region from the
background using a center-vs-surround approach de-
scribed in Achanta et al [5]. We then sum the number
of salient pixels per image tile and normalize by the
tile size.

5. Methods
Figure 3 depicts a high-level block diagram of the
learning pipeline we built. The pipeline comprises
three main components: an image scraper, a bank of
feature extractors, and a learning algorithm.

Figure 3. Block diagram of learning pipeline.

For each of the features we identiﬁed, there exists a
feature extractor function that accepts an image as
an input, calculates the feature value, and inserts the
feature-value mapping into a sparse feature vector al-
located for the image. We rely on image processing
algorithms implemented in the scikit-image and
opencv libraries for many of these functions [6, 7].
After the pipeline generates feature vectors for all
images in the training set, it uses them to train a
classiﬁer.
For the learning algorithm, we exper-
imented with scikit-learn’s implementations of
support vector machines (SVM), random forests (RF),
and gradient tree boosting (GBRT) [8]. We focus our
attention on these algorithms because they can ac-
count for non-linear relationships amongst features.
SVM: The SVM learning algorithm with (cid:96)1 regu-
larization involves solving the primal optimization
problem

min
γ,w,b

subject to

||w||2 + C

1
2
y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m

i=1

ξi

m(cid:88)

, the dual of which is

m(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

max

α

subject to

αi − 1
2

i,j=1

0 ≤ αi ≤ C, i = 1, ..., m

αiy(i) = 0

m(cid:88)
m(cid:88)

i=1

i=1

Accordingly, provided that we ﬁnd the values of α
that maximize the dual optimization problem, the hy-
pothesis can be formulated as

if (cid:80)m

(cid:26) 1

−1

h(x) =

i=1 αiy(i)(cid:104)x(i), x(cid:105) + b ≥ 0

otherwise

Note that since the dual optimization problem and
hypothesis can be expressed as inner products be-
tween input feature vectors, we can replace each in-
ner product with a kernel applied to the two input

vectors, which allows us to train our classiﬁer and
perform classiﬁcation in a higher-dimensional feature
space. This characteristic of SVMs makes them well-
suited for our problem since we speculate that non-
linear relationships amongst features inﬂuence image
aesthetic quality. For our system, we choose to use the

Gaussian kernel K(x, y) = exp(cid:0)γ||x − y||2

(cid:1), which

corresponds to an inﬁnite-dimensional feature map-
ping.
Random forest: Random forests comprise collections
of decision trees. Each decision tree is grown by
selecting a random subset of input variables to use
for splitting at a particular node. Prediction then in-
volves taking the average of the predictions of all the
constituent trees:

2

(cid:33)

(cid:32)

m(cid:88)

i=1

1
m

h(x) = sign

Ti(x)

Because of the way each decision tree is constructed,
the variance of the average prediction is less than that
of any individual prediction.
It is this characteris-
tic that makes random forests more resistant to over-
ﬁtting than decision trees, and, thus, generally have
much higher performance.
Gradient tree boosting: Boosting is a powerful learn-
ing method that sequentially applies weak classiﬁca-
tion algorithms to reweighted versions of the training
data, with the reweighting done in such a way that,
between every pair of classiﬁers in the sequence, the
examples that were misclassiﬁed by the previous clas-
siﬁer are weighted higher for the next classiﬁer.
In
this manner, each subsequent classiﬁer in the ensem-
ble is forced to concentrate on correctly classifying the
examples that were previously misclassiﬁed.
In gradient tree boosting, or gradient-boosted regres-
sion trees (GBRT), the weak classiﬁers are decision
trees. After ﬁtting the trees, the predictions from all
the decision trees are weighted and combined to form
the ﬁnal prediction:

(cid:32) m(cid:88)

(cid:33)

h(x) = sign

αiTi(x)

In literature, tree boosting has been identiﬁed as be-
ing one of the best learning algorithms available [9].

i=1

6. Experimental results and analysis
For each learning algorithm, we measure the per-
formance of our classiﬁer using 10-fold cross valida-
tion on the photo.net dataset and the DPChallenge
dataset. We run backward feature selection to elimi-
nate ineffective features to improve classiﬁcation per-
formance. For the ﬁnal set of features, we remove

Figure 4. Classiﬁer 10-fold cross-validation accuracy versus
tiling dimension (SVM with C = 1 and γ = 0.1, DPChal-
lenge dataset).

each feature from the set and run cross-validation on
the resulting set to verify that the feature contributes
positively to the classiﬁer’s performance.
In experimenting with different tiling dimensions, we
found that dividing each image into ﬁve-by-ﬁve tiles
gave the best performance (Figure 4). A tiling di-
mension of 1 corresponds to extracting each feature
across the entire frame of the image. As anticipated,
this yields signiﬁcantly worse performance. Larger
tiling dimensions should theoretically work better
provided that we have enough data to support the as-
sociated increase in the number of features. Unfortu-
nately, given the limited sizes of our datasets, the ad-
dition of more features causes our classiﬁer to overﬁt
and thus degrades its accuracy for dimensions larger
than 5.
For SVM, we tuned our parameters using grid search,
which ultimately led us to use C = 1 and γ = 0.1. For
random forest, we used 300 decision trees. We deter-
mined this value by empirically ﬁnding the asymp-
totic limit to the generalization error with respect
to the number of decision trees used. For gradient
tree boosting, we used 500 decision trees and a sub-
sampling coefﬁcient of 0.9. Using a sub-sampling co-
efﬁcient smaller than 1 allows us to trade off vari-
ance for bias, which thereby mitigates overﬁtting and
hence improves generalization performance.
Table 1 shows our 10-fold cross-validation accuracy
for each learning algorithm. For both datasets, we got
the highest performance with GBRT, with accuracies
of 80.88% and 85.03%. The difference in performance
may have resulted from the DPChallenge dataset’s
having higher-resolution images than the photo.net
dataset, which makes certain visual features more
distinct in the former than the latter. Nonetheless,
the similarity in results suggests that our methodol-
ogy generalizes well to different datasets.
Figure 5 shows the confusion matrix for 10-fold cross-

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JHWANG89@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial distri-
bution of certain visual elements within an
image correlates with its aesthetic quality.
To this end, we present a novel approach
wherein we model each photograph as a set
of image tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 85.03%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence, arrange-
ment, and combination of certain visual characteris-
tics does indeed make an image more aesthetically-
pleasing in general. To understand why this may be
true, consider the two images shown in Figure 1. Al-
though both are of ﬂowers, the left photograph has
a signiﬁcantly higher aesthetic rating than the right
photograph. One might reason that this is so sim-
ply because the right photograph is blurry. This con-

Figure 1. A photograph with a high aesthetic rating (left)
and a photograph with a low aesthetic rating (right).

jecture, however, is imprecise because the left photo-
graph is, on average, blurry as well. In fact, the major-
ity of the image is even blurrier than the right photo-
graph. Here, it seems that the juxtaposition of sharp
salient regions with blurry regions and their locations
in the frame positively inﬂuence the perceived aes-
thetic quality of the photograph.
Accordingly, we begin by identifying generic visual
features that we believe may affect the aesthetic qual-
ity of a photograph, such as blur and hue. We then
build a learning pipeline that extracts these features
from images on a per-image-tile basis and uses them
along with the images’ aesthetics ratings to train a
classiﬁer. In this manner, we endow the classiﬁer with
the ability to infer spatial relationships amongst fea-
tures that correlate with an image’s aesthetics.
The potential impact of building a system to solve
this problem is broad. For example, by implement-
ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.

2. Related work
There have been several efforts to tackle this prob-
lem from different angles within the past decade.
Pogaˇcnik et al [1] believed that the features depended
heavily on identiﬁcation of the subject of the pho-
tograph. Datta et al [2] evaluated the performance
of different machine learning models (support vector
machines, decision trees) on the problem. Ke et al [3]
focused on extracting perceptual factors important to
professional photographers, such as color, noise, blur,
and spatial distribution of edges.
Also, in contrast to our approach, it is interesting
to note that these studies have focused on extract-
ing global features that attempt to capture prior be-
liefs on the spatiality of visual elements within high-
quality images. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle of the image, and Pogaˇcnik et al de-
ﬁned features that assessed adherence to a multitude
of compositional rules as well as the positioning of
the subject relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website that allows
members to rate community-uploaded images on a
scale of 1 to 10. The index ﬁle we used to locate im-
ages was generated by Murray et al [4]. Following
guidelines from prior work, we choose to use pho-
tographs with ratings above 7.2 or below 3.4, result-
ing in a dataset containing 3000 images split evenly
between positive and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into equally-sized tiles (Figure 2). By extracting fea-

Figure 2. Tiling scheme applied to image by learning
pipeline.

tures on a per-tile basis, the learning algorithm can
identify regions of interest and infer relationships be-
tween feature-tile pairs that indicate aesthetic qual-
ity. For example, in the case of the image depicted
in Figure 2, we surmise that the learning algorithm
would be able to discern the well-composed framing
of the pier from the features extracted from its con-
taining tiles with respect to those extracted from the
surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the im-
age’s subject from its background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

(cid:80)

fsd =

(x,y)∈Tile 1{I(x, y) = Thresholded edge}

|{(x, y)|(x, y) ∈ Tile}|

Color: A photograph’s color composition can dra-
matically inﬂuence how a person perceives a photo-
graph. We capture the color diversity within an image
tile using a color histogram that subdivides the three
dimensional RGB color space into 64 equally sized
bins. Since each pixel can take on one of 256 discrete
values in each color channel, this results in each bin
being a cube with 16 possible values in each dimen-
sion. We normalize each bin’s count by the total pixel
count so that it is invariant to image dimensions.
We also measure the average saturation and lumi-
nance of each tile’s pixels. Finally, for the entire im-
age, we compute the proportion of pixels that corre-
spond to a particular hue (red, yellow, green, blue,
and purple).
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of the

image tile to the number of edge pixels in the original
image tile, i.e.

(cid:80)

fd =

(cid:80)
(x,y)∈Tile 1{If iltered(x, y) = Edge}

(x,y)∈Tile 1{I(x, y) = Edge}

For an image tile that is exceptionally detailed, many
of the higher-frequency edges in the region would be
removed by the Gaussian ﬁlter. Consequently, we
would expect fd to be closer to 0. Conversely, for a
tile that lacks detail, since few edges exist in the re-
gion, applying the Gaussian ﬁlter would impart little
change to the number of edges. In this case, we would
expect fd to be closer to 1.
Contrast: Contrast is the difference in color or bright-
ness amongst regions in an image. Generally, the
higher the contrast, the more distinguishable objects
are from one another. We approximate the contrast
within each image tile by calculating the standard de-
viation of the grayscale intensities.
Blur: Depending on the image region, blurriness may
or may not be desirable. Poor technique or camera
shake tends to yield images that are blurry across the
entire frame, which is generally undesirable. On the
other hand, low depth-of-ﬁeld images with blurred
out-of-focus highlights (“bokeh”) that complement
sharp subjects are often regarded as being pleasing.
To efﬁciently estimate the amount of blur within an
image, we calculate the variance of the Laplacian of
the image. Low variance corresponds to blurrier im-
ages, and high variance to sharper images.
Noise: The desirability of visual noise is contextual.
For most modern images and for images that con-
vey positive emotions, noise is generally undesirable.
For images that convey negative semantics, however,
noise may be desirable to accentuate their visual im-
pact. We measure noise by calculating the image’s en-
tropy.
Saliency: The saliency of the subject within a photo-
graph can have a signiﬁcant impact on the perceived
aesthetic quality of the photograph. We post-process
each image to separate the salient region from the
background using a center-vs-surround approach de-
scribed in Achanta et al [5]. We then sum the number
of salient pixels per image tile and normalize by the
tile size.

5. Methods
Figure 3 depicts a high-level block diagram of the
learning pipeline we built. The pipeline comprises
three main components: an image scraper, a bank of
feature extractors, and a learning algorithm.

Figure 3. Block diagram of learning pipeline.

For each of the features we identiﬁed, there exists a
feature extractor function that accepts an image as
an input, calculates the feature value, and inserts the
feature-value mapping into a sparse feature vector al-
located for the image. We rely on image processing
algorithms implemented in the scikit-image and
opencv libraries for many of these functions [6, 7].
After the pipeline generates feature vectors for all
images in the training set, it uses them to train a
classiﬁer.
For the learning algorithm, we exper-
imented with scikit-learn’s implementations of
support vector machines (SVM), random forests (RF),
and gradient tree boosting (GBRT) [8]. We focus our
attention on these algorithms because they can ac-
count for non-linear relationships amongst features.
SVM: The SVM learning algorithm with (cid:96)1 regu-
larization involves solving the primal optimization
problem

min
γ,w,b

subject to

||w||2 + C

1
2
y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m

i=1

ξi

m(cid:88)

, the dual of which is

m(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

max

α

subject to

αi − 1
2

i,j=1

0 ≤ αi ≤ C, i = 1, ..., m

αiy(i) = 0

m(cid:88)
m(cid:88)

i=1

i=1

Accordingly, provided that we ﬁnd the values of α
that maximize the dual optimization problem, the hy-
pothesis can be formulated as

if (cid:80)m

(cid:26) 1

−1

h(x) =

i=1 αiy(i)(cid:104)x(i), x(cid:105) + b ≥ 0

otherwise

Note that since the dual optimization problem and
hypothesis can be expressed as inner products be-
tween input feature vectors, we can replace each in-
ner product with a kernel applied to the two input

vectors, which allows us to train our classiﬁer and
perform classiﬁcation in a higher-dimensional feature
space. This characteristic of SVMs makes them well-
suited for our problem since we speculate that non-
linear relationships amongst features inﬂuence image
aesthetic quality. For our system, we choose to use the

Gaussian kernel K(x, y) = exp(cid:0)γ||x − y||2

(cid:1), which

corresponds to an inﬁnite-dimensional feature map-
ping.
Random forest: Random forests comprise collections
of decision trees. Each decision tree is grown by
selecting a random subset of input variables to use
for splitting at a particular node. Prediction then in-
volves taking the average of the predictions of all the
constituent trees:

2

(cid:33)

(cid:32)

m(cid:88)

i=1

1
m

h(x) = sign

Ti(x)

Because of the way each decision tree is constructed,
the variance of the average prediction is less than that
of any individual prediction.
It is this characteris-
tic that makes random forests more resistant to over-
ﬁtting than decision trees, and, thus, generally have
much higher performance.
Gradient tree boosting: Boosting is a powerful learn-
ing method that sequentially applies weak classiﬁca-
tion algorithms to reweighted versions of the training
data, with the reweighting done in such a way that,
between every pair of classiﬁers in the sequence, the
examples that were misclassiﬁed by the previous clas-
siﬁer are weighted higher for the next classiﬁer.
In
this manner, each subsequent classiﬁer in the ensem-
ble is forced to concentrate on correctly classifying the
examples that were previously misclassiﬁed.
In gradient tree boosting, or gradient-boosted regres-
sion trees (GBRT), the weak classiﬁers are decision
trees. After ﬁtting the trees, the predictions from all
the decision trees are weighted and combined to form
the ﬁnal prediction:

(cid:32) m(cid:88)

(cid:33)

h(x) = sign

αiTi(x)

In literature, tree boosting has been identiﬁed as be-
ing one of the best learning algorithms available [9].

i=1

6. Experimental results and analysis
For each learning algorithm, we measure the per-
formance of our classiﬁer using 10-fold cross valida-
tion on the photo.net dataset and the DPChallenge
dataset. We run backward feature selection to elimi-
nate ineffective features to improve classiﬁcation per-
formance. For the ﬁnal set of features, we remove

Figure 4. Classiﬁer 10-fold cross-validation accuracy versus
tiling dimension (SVM with C = 1 and γ = 0.1, DPChal-
lenge dataset).

each feature from the set and run cross-validation on
the resulting set to verify that the feature contributes
positively to the classiﬁer’s performance.
In experimenting with different tiling dimensions, we
found that dividing each image into ﬁve-by-ﬁve tiles
gave the best performance (Figure 4). A tiling di-
mension of 1 corresponds to extracting each feature
across the entire frame of the image. As anticipated,
this yields signiﬁcantly worse performance. Larger
tiling dimensions should theoretically work better
provided that we have enough data to support the as-
sociated increase in the number of features. Unfortu-
nately, given the limited sizes of our datasets, the ad-
dition of more features causes our classiﬁer to overﬁt
and thus degrades its accuracy for dimensions larger
than 5.
For SVM, we tuned our parameters using grid search,
which ultimately led us to use C = 1 and γ = 0.1. For
random forest, we used 300 decision trees. We deter-
mined this value by empirically ﬁnding the asymp-
totic limit to the generalization error with respect
to the number of decision trees used. For gradient
tree boosting, we used 500 decision trees and a sub-
sampling coefﬁcient of 0.9. Using a sub-sampling co-
efﬁcient smaller than 1 allows us to trade off vari-
ance for bias, which thereby mitigates overﬁtting and
hence improves generalization performance.
Table 1 shows our 10-fold cross-validation accuracy
for each learning algorithm. For both datasets, we got
the highest performance with GBRT, with accuracies
of 80.88% and 85.03%. The difference in performance
may have resulted from the DPChallenge dataset’s
having higher-resolution images than the photo.net
dataset, which makes certain visual features more
distinct in the former than the latter. Nonetheless,
the similarity in results suggests that our methodol-
ogy generalizes well to different datasets.
Figure 5 shows the confusion matrix for 10-fold cross-

photo.net
DPChallenge

RF

GBRT
SVM
78.71% 78.58% 80.88%
84.00% 83.15% 85.03%

Table 1. 10-fold cross-validation accuracy

1
1 TP

Actual
label

85.80%

0 FP

15.73%

Predicted label

0

FN
14.20%

TN
84.27%

Figure 5. Confusion matrix for 10-fold cross validation with
GBRT on DPChallenge dataset.

validation using GBRT on the DPChallenge dataset.
The true positive and false negative rates are approx-
imately symmetric with the true negative and false
positive rates, respectively, which signiﬁes that our
classiﬁer is not biased towards predicting a certain
class. This also holds true for the photo.net dataset.
To analyze the shortcomings of our approach, we ex-
amine images that our classiﬁer misclassiﬁed.
Figure 6 shows an example of a negative image from
the photo.net dataset that the classiﬁer mispredicted
as being positive. Note that the image is compo-
sitionally sound – the subject is clearly distinguish-
able from the background, ﬁlls most of the frame, is
well-balanced in the frame, and has components that
lie along the rule-of-thirds axes. The hot-pink back-
ground, however, is jarring, and the subject matter is
mundane and lacks signiﬁcance. Unfortunately, be-
cause it discretizes color features so coarsely, the clas-
siﬁer is likely not able to effectively differentiate be-
tween the artiﬁcial pink shade of the image’s back-
ground and the warm red shade of a sunset, for in-
stance. Moreover, it has no way of gleaning meaning
from images. We therefore believe that it is primarily
due to these shortcomings that our classiﬁer misclas-
siﬁed this particular image.

Figure 6. Negative image classiﬁed as positive by the
model.

Figure 7. Positive images classiﬁed as negative by the
model.

Figure 7 shows two photographs from the DPChal-
lenge dataset that our classiﬁer misclassiﬁed as be-
ing negative. While the left photograph follows good
composition techniques, the subject has few high fre-
quency edges, so the classiﬁer would likely need to
rely more on saliency detection to pinpoint the sub-
ject. Unfortunately, the current method of detecting
the salient region is not consistently reliable, so de-
spite this photograph’s having a distinct salient re-
gion, the classiﬁer may deemphasize the contribu-
tions of this feature. We believe that improving our
salient region detection accuracy across all images
may enable the classiﬁer to utilize the saliency feature
more effectively, and thus correctly classify this pho-
tograph. In the right image in Figure 7, the key visual
element is the strong leading lines that draw atten-
tion to the hiker – the subject of the image. Leading
lines, however, are global features that are not well-
captured by our tiling methodology, and, thus, are
likely not considered by the classiﬁer.
In sum, although our system performs respectably,
examining the images it mispredicts reveals many po-
tential areas of improvement.

7. Future work and conclusions
We have demonstrated that modeling an image as a
set of tiles, extracting certain visual features from each
tile, and training a learning algorithm to infer rela-
tionships between tiles yields a high-performing pho-
tographic aesthetics classiﬁcation system that adapts
well to different image datasets. Thus, our work lays
a sound foundation for future development. In par-
ticular, we believe we can further improve the accu-
racy of our system by deriving global visual features
and parsing semantics from photographs. Our model
should also apply to regression for use cases where
numerical ratings are desired. Finally, augmenting
the system with the ability to choose a classiﬁer de-
pending on the identiﬁed mode of a photograph, e.g.
portrait or landscape, may lead to more accurate clas-
siﬁcation of aesthetic quality.

Classiﬁcation of photographic images based on

perceived aesthetic quality

Jeff Hwang
Department of Electrical Engineering, Stanford University

Sean Shi
Department of Electrical Engineering, Stanford University

JHWANG89@STANFORD.EDU

SSHI11@STANFORD.EDU

Abstract

In this paper, we explore automated aes-
thetic evaluation of photographs using ma-
chine learning and image processing tech-
niques. We theorize that the spatial distri-
bution of certain visual elements within an
image correlates with its aesthetic quality.
To this end, we present a novel approach
wherein we model each photograph as a set
of image tiles, extract visual features from
each tile, and train a classiﬁer on the re-
sulting features along with the images’ aes-
thetics ratings. Our model achieves a 10-
fold cross-validation classiﬁcation success
rate of 85.03%, corroborating the efﬁcacy
of our methodology and therefore showing
promise for future development.

1. Introduction
Aesthetics in photography are highly subjective. The
average individual may judge the quality of a photo-
graph simply by gut feeling; in contrast, a photogra-
pher might evaluate a photograph he or she captures
vis-a-vis technical criteria such as composition, con-
trast, and sharpness. Towards fulﬁlling these crite-
ria, photographers follow many rules of thumb. The
actual and relative visual impact of doing so for the
general public, however, remains unclear.
In our project, we show that the existence, arrange-
ment, and combination of certain visual characteris-
tics does indeed make an image more aesthetically-
pleasing in general. To understand why this may be
true, consider the two images shown in Figure 1. Al-
though both are of ﬂowers, the left photograph has
a signiﬁcantly higher aesthetic rating than the right
photograph. One might reason that this is so sim-
ply because the right photograph is blurry. This con-

Figure 1. A photograph with a high aesthetic rating (left)
and a photograph with a low aesthetic rating (right).

jecture, however, is imprecise because the left photo-
graph is, on average, blurry as well. In fact, the major-
ity of the image is even blurrier than the right photo-
graph. Here, it seems that the juxtaposition of sharp
salient regions with blurry regions and their locations
in the frame positively inﬂuence the perceived aes-
thetic quality of the photograph.
Accordingly, we begin by identifying generic visual
features that we believe may affect the aesthetic qual-
ity of a photograph, such as blur and hue. We then
build a learning pipeline that extracts these features
from images on a per-image-tile basis and uses them
along with the images’ aesthetics ratings to train a
classiﬁer. In this manner, we endow the classiﬁer with
the ability to infer spatial relationships amongst fea-
tures that correlate with an image’s aesthetics.
The potential impact of building a system to solve
this problem is broad. For example, by implement-
ing such a system, websites with community-sourced
images can programmatically ﬁlter out bad images
to maintain the desired quality of content. Cam-
eras can provide real-time visual feedback to help
users improve their photographic skills. Moreover,
from a cognitive standpoint, solving this problem
may lend interesting insight towards how humans
perceive beauty.

2. Related work
There have been several efforts to tackle this prob-
lem from different angles within the past decade.
Pogaˇcnik et al [1] believed that the features depended
heavily on identiﬁcation of the subject of the pho-
tograph. Datta et al [2] evaluated the performance
of different machine learning models (support vector
machines, decision trees) on the problem. Ke et al [3]
focused on extracting perceptual factors important to
professional photographers, such as color, noise, blur,
and spatial distribution of edges.
Also, in contrast to our approach, it is interesting
to note that these studies have focused on extract-
ing global features that attempt to capture prior be-
liefs on the spatiality of visual elements within high-
quality images. For example, Datta et al attempted to
model rule-of-thirds composition by computing the
average hue, saturation, and luminance of the inner
thirds rectangle of the image, and Pogaˇcnik et al de-
ﬁned features that assessed adherence to a multitude
of compositional rules as well as the positioning of
the subject relative to the image’s frame.

3. Dataset
Our learning pipeline downloads images and their
average aesthetic ratings from two separate datasets.
The ﬁrst is an image database hosted by photo.net, a
photo sharing website for photographers. The index
ﬁle we use to locate images was generated by Datta
et al. Members of photo.net can upload and critique
each others photographs and rate each photograph
with a number between 1 and 7, with 7 being the best
possible rating. Due to the wide range and subjectiv-
ity of ratings, we choose to only use photographs with
ratings above 6 or below 4.2, which yields a dataset
containing 1700 images split evenly between positive
labels and negative labels.
The second comprises images scraped from DPChal-
lenge, another photo sharing website that allows
members to rate community-uploaded images on a
scale of 1 to 10. The index ﬁle we used to locate im-
ages was generated by Murray et al [4]. Following
guidelines from prior work, we choose to use pho-
tographs with ratings above 7.2 or below 3.4, result-
ing in a dataset containing 3000 images split evenly
between positive and negative labels.

4. Feature extraction
Prior to extracting features, we partition each image
into equally-sized tiles (Figure 2). By extracting fea-

Figure 2. Tiling scheme applied to image by learning
pipeline.

tures on a per-tile basis, the learning algorithm can
identify regions of interest and infer relationships be-
tween feature-tile pairs that indicate aesthetic qual-
ity. For example, in the case of the image depicted
in Figure 2, we surmise that the learning algorithm
would be able to discern the well-composed framing
of the pier from the features extracted from its con-
taining tiles with respect to those extracted from the
surrounding tiles.
Below, we describe the features we extract from each
image tile.
Subject detection: Strong edges distinguish the im-
age’s subject from its background. To quantify the
degree of subject-background separation, we apply a
Sobel ﬁlter to each image tile, binarize the result via
Otsu’s method, and compute the proportion of pixels
in the tile that are edge pixels:

(cid:80)

fsd =

(x,y)∈Tile 1{I(x, y) = Thresholded edge}

|{(x, y)|(x, y) ∈ Tile}|

Color: A photograph’s color composition can dra-
matically inﬂuence how a person perceives a photo-
graph. We capture the color diversity within an image
tile using a color histogram that subdivides the three
dimensional RGB color space into 64 equally sized
bins. Since each pixel can take on one of 256 discrete
values in each color channel, this results in each bin
being a cube with 16 possible values in each dimen-
sion. We normalize each bin’s count by the total pixel
count so that it is invariant to image dimensions.
We also measure the average saturation and lumi-
nance of each tile’s pixels. Finally, for the entire im-
age, we compute the proportion of pixels that corre-
spond to a particular hue (red, yellow, green, blue,
and purple).
Detail: Higher levels of detail are generally desirable
for photographs, particularly for its subject. To ap-
proximate the amount of detail, we compare the num-
ber of edge pixels of a Gaussian ﬁltered version of the

image tile to the number of edge pixels in the original
image tile, i.e.

(cid:80)

fd =

(cid:80)
(x,y)∈Tile 1{If iltered(x, y) = Edge}

(x,y)∈Tile 1{I(x, y) = Edge}

For an image tile that is exceptionally detailed, many
of the higher-frequency edges in the region would be
removed by the Gaussian ﬁlter. Consequently, we
would expect fd to be closer to 0. Conversely, for a
tile that lacks detail, since few edges exist in the re-
gion, applying the Gaussian ﬁlter would impart little
change to the number of edges. In this case, we would
expect fd to be closer to 1.
Contrast: Contrast is the difference in color or bright-
ness amongst regions in an image. Generally, the
higher the contrast, the more distinguishable objects
are from one another. We approximate the contrast
within each image tile by calculating the standard de-
viation of the grayscale intensities.
Blur: Depending on the image region, blurriness may
or may not be desirable. Poor technique or camera
shake tends to yield images that are blurry across the
entire frame, which is generally undesirable. On the
other hand, low depth-of-ﬁeld images with blurred
out-of-focus highlights (“bokeh”) that complement
sharp subjects are often regarded as being pleasing.
To efﬁciently estimate the amount of blur within an
image, we calculate the variance of the Laplacian of
the image. Low variance corresponds to blurrier im-
ages, and high variance to sharper images.
Noise: The desirability of visual noise is contextual.
For most modern images and for images that con-
vey positive emotions, noise is generally undesirable.
For images that convey negative semantics, however,
noise may be desirable to accentuate their visual im-
pact. We measure noise by calculating the image’s en-
tropy.
Saliency: The saliency of the subject within a photo-
graph can have a signiﬁcant impact on the perceived
aesthetic quality of the photograph. We post-process
each image to separate the salient region from the
background using a center-vs-surround approach de-
scribed in Achanta et al [5]. We then sum the number
of salient pixels per image tile and normalize by the
tile size.

5. Methods
Figure 3 depicts a high-level block diagram of the
learning pipeline we built. The pipeline comprises
three main components: an image scraper, a bank of
feature extractors, and a learning algorithm.

Figure 3. Block diagram of learning pipeline.

For each of the features we identiﬁed, there exists a
feature extractor function that accepts an image as
an input, calculates the feature value, and inserts the
feature-value mapping into a sparse feature vector al-
located for the image. We rely on image processing
algorithms implemented in the scikit-image and
opencv libraries for many of these functions [6, 7].
After the pipeline generates feature vectors for all
images in the training set, it uses them to train a
classiﬁer.
For the learning algorithm, we exper-
imented with scikit-learn’s implementations of
support vector machines (SVM), random forests (RF),
and gradient tree boosting (GBRT) [8]. We focus our
attention on these algorithms because they can ac-
count for non-linear relationships amongst features.
SVM: The SVM learning algorithm with (cid:96)1 regu-
larization involves solving the primal optimization
problem

min
γ,w,b

subject to

||w||2 + C

1
2
y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, ..., m

i=1

ξi

m(cid:88)

, the dual of which is

m(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

max

α

subject to

αi − 1
2

i,j=1

0 ≤ αi ≤ C, i = 1, ..., m

αiy(i) = 0

m(cid:88)
m(cid:88)

i=1

i=1

Accordingly, provided that we ﬁnd the values of α
that maximize the dual optimization problem, the hy-
pothesis can be formulated as

if (cid:80)m

(cid:26) 1

−1

h(x) =

i=1 αiy(i)(cid:104)x(i), x(cid:105) + b ≥ 0

otherwise

Note that since the dual optimization problem and
hypothesis can be expressed as inner products be-
tween input feature vectors, we can replace each in-
ner product with a kernel applied to the two input

vectors, which allows us to train our classiﬁer and
perform classiﬁcation in a higher-dimensional feature
space. This characteristic of SVMs makes them well-
suited for our problem since we speculate that non-
linear relationships amongst features inﬂuence image
aesthetic quality. For our system, we choose to use the

Gaussian kernel K(x, y) = exp(cid:0)γ||x − y||2

(cid:1), which

corresponds to an inﬁnite-dimensional feature map-
ping.
Random forest: Random forests comprise collections
of decision trees. Each decision tree is grown by
selecting a random subset of input variables to use
for splitting at a particular node. Prediction then in-
volves taking the average of the predictions of all the
constituent trees:

2

(cid:33)

(cid:32)

m(cid:88)

i=1

1
m

h(x) = sign

Ti(x)

Because of the way each decision tree is constructed,
the variance of the average prediction is less than that
of any individual prediction.
It is this characteris-
tic that makes random forests more resistant to over-
ﬁtting than decision trees, and, thus, generally have
much higher performance.
Gradient tree boosting: Boosting is a powerful learn-
ing method that sequentially applies weak classiﬁca-
tion algorithms to reweighted versions of the training
data, with the reweighting done in such a way that,
between every pair of classiﬁers in the sequence, the
examples that were misclassiﬁed by the previous clas-
siﬁer are weighted higher for the next classiﬁer.
In
this manner, each subsequent classiﬁer in the ensem-
ble is forced to concentrate on correctly classifying the
examples that were previously misclassiﬁed.
In gradient tree boosting, or gradient-boosted regres-
sion trees (GBRT), the weak classiﬁers are decision
trees. After ﬁtting the trees, the predictions from all
the decision trees are weighted and combined to form
the ﬁnal prediction:

(cid:32) m(cid:88)

(cid:33)

h(x) = sign

αiTi(x)

In literature, tree boosting has been identiﬁed as be-
ing one of the best learning algorithms available [9].

i=1

6. Experimental results and analysis
For each learning algorithm, we measure the per-
formance of our classiﬁer using 10-fold cross valida-
tion on the photo.net dataset and the DPChallenge
dataset. We run backward feature selection to elimi-
nate ineffective features to improve classiﬁcation per-
formance. For the ﬁnal set of features, we remove

Figure 4. Classiﬁer 10-fold cross-validation accuracy versus
tiling dimension (SVM with C = 1 and γ = 0.1, DPChal-
lenge dataset).

each feature from the set and run cross-validation on
the resulting set to verify that the feature contributes
positively to the classiﬁer’s performance.
In experimenting with different tiling dimensions, we
found that dividing each image into ﬁve-by-ﬁve tiles
gave the best performance (Figure 4). A tiling di-
mension of 1 corresponds to extracting each feature
across the entire frame of the image. As anticipated,
this yields signiﬁcantly worse performance. Larger
tiling dimensions should theoretically work better
provided that we have enough data to support the as-
sociated increase in the number of features. Unfortu-
nately, given the limited sizes of our datasets, the ad-
dition of more features causes our classiﬁer to overﬁt
and thus degrades its accuracy for dimensions larger
than 5.
For SVM, we tuned our parameters using grid search,
which ultimately led us to use C = 1 and γ = 0.1. For
random forest, we used 300 decision trees. We deter-
mined this value by empirically ﬁnding the asymp-
totic limit to the generalization error with respect
to the number of decision trees used. For gradient
tree boosting, we used 500 decision trees and a sub-
sampling coefﬁcient of 0.9. Using a sub-sampling co-
efﬁcient smaller than 1 allows us to trade off vari-
ance for bias, which thereby mitigates overﬁtting and
hence improves generalization performance.
Table 1 shows our 10-fold cross-validation accuracy
for each learning algorithm. For both datasets, we got
the highest performance with GBRT, with accuracies
of 80.88% and 85.03%. The difference in performance
may have resulted from the DPChallenge dataset’s
having higher-resolution images than the photo.net
dataset, which makes certain visual features more
distinct in the former than the latter. Nonetheless,
the similarity in results suggests that our methodol-
ogy generalizes well to different datasets.
Figure 5 shows the confusion matrix for 10-fold cross-

photo.net
DPChallenge

RF

GBRT
SVM
78.71% 78.58% 80.88%
84.00% 83.15% 85.03%

Table 1. 10-fold cross-validation accuracy

1
1 TP

Actual
label

85.80%

0 FP

15.73%

Predicted label

0

FN
14.20%

TN
84.27%

Figure 5. Confusion matrix for 10-fold cross validation with
GBRT on DPChallenge dataset.

validation using GBRT on the DPChallenge dataset.
The true positive and false negative rates are approx-
imately symmetric with the true negative and false
positive rates, respectively, which signiﬁes that our
classiﬁer is not biased towards predicting a certain
class. This also holds true for the photo.net dataset.
To analyze the shortcomings of our approach, we ex-
amine images that our classiﬁer misclassiﬁed.
Figure 6 shows an example of a negative image from
the photo.net dataset that the classiﬁer mispredicted
as being positive. Note that the image is compo-
sitionally sound – the subject is clearly distinguish-
able from the background, ﬁlls most of the frame, is
well-balanced in the frame, and has components that
lie along the rule-of-thirds axes. The hot-pink back-
ground, however, is jarring, and the subject matter is
mundane and lacks signiﬁcance. Unfortunately, be-
cause it discretizes color features so coarsely, the clas-
siﬁer is likely not able to effectively differentiate be-
tween the artiﬁcial pink shade of the image’s back-
ground and the warm red shade of a sunset, for in-
stance. Moreover, it has no way of gleaning meaning
from images. We therefore believe that it is primarily
due to these shortcomings that our classiﬁer misclas-
siﬁed this particular image.

Figure 6. Negative image classiﬁed as positive by the
model.

Figure 7. Positive images classiﬁed as negative by the
model.

Figure 7 shows two photographs from the DPChal-
lenge dataset that our classiﬁer misclassiﬁed as be-
ing negative. While the left photograph follows good
composition techniques, the subject has few high fre-
quency edges, so the classiﬁer would likely need to
rely more on saliency detection to pinpoint the sub-
ject. Unfortunately, the current method of detecting
the salient region is not consistently reliable, so de-
spite this photograph’s having a distinct salient re-
gion, the classiﬁer may deemphasize the contribu-
tions of this feature. We believe that improving our
salient region detection accuracy across all images
may enable the classiﬁer to utilize the saliency feature
more effectively, and thus correctly classify this pho-
tograph. In the right image in Figure 7, the key visual
element is the strong leading lines that draw atten-
tion to the hiker – the subject of the image. Leading
lines, however, are global features that are not well-
captured by our tiling methodology, and, thus, are
likely not considered by the classiﬁer.
In sum, although our system performs respectably,
examining the images it mispredicts reveals many po-
tential areas of improvement.

7. Future work and conclusions
We have demonstrated that modeling an image as a
set of tiles, extracting certain visual features from each
tile, and training a learning algorithm to infer rela-
tionships between tiles yields a high-performing pho-
tographic aesthetics classiﬁcation system that adapts
well to different image datasets. Thus, our work lays
a sound foundation for future development. In par-
ticular, we believe we can further improve the accu-
racy of our system by deriving global visual features
and parsing semantics from photographs. Our model
should also apply to regression for use cases where
numerical ratings are desired. Finally, augmenting
the system with the ability to choose a classiﬁer de-
pending on the identiﬁed mode of a photograph, e.g.
portrait or landscape, may lead to more accurate clas-
siﬁcation of aesthetic quality.

References
[1] Pogaˇcnik, D., Ravnik, R., Bovcon, N., & Solina,
F. (2012). Evaluating photo aesthetics using ma-
chine learning.

[2] Datta, R., Joshi, D., Li, J., & Wang, J. Z. (2006).
Studying aesthetics in photographic images us-
In Computer
ing a computational approach.
Vision–ECCV 2006 (pp. 288-301). Springer Berlin
Heidelberg.

[3] Ke, Y., Tang, X., & Jing, F. (2006, June). The de-
sign of high-level features for photo quality as-
sessment. In Computer Vision and Pattern Recog-
nition, 2006 IEEE Computer Society Conference on
(Vol. 1, pp. 419-426). IEEE.

[4] Murray, N., Marchesotti, L., & Perronnin, F.
(2012, June). AVA: A large-scale database for aes-
thetic visual analysis. In Computer Vision and Pat-
tern Recognition (CVPR), 2012 IEEE Conference on
(pp. 2408-2415). IEEE.

[5] Achanta, R., Hemami, S., Estrada, F., &
Susstrunk, S. (2009,
Frequency-tuned
salient region detection. In Computer vision and
pattern recognition, 2009. cvpr 2009. ieee conference
on (pp. 1597-1604). IEEE.

June).

[6] van der Walt, S., Sch¨onberger, J. L., Nunez-
Iglesias, J., Boulogne, F., Warner, J. D., Yager, N.,
... & Yu, T. the scikit-image contributors,(2014)
Scikit-image: image processing in Python. Peer
J, 2(6).

[7] Bradski, G. (2000). The opencv library. Doctor

Dobbs Journal, 25(11), 120-126.

[8] Pedregosa, F., Varoquaux, G., Gramfort, A.,
Michel, V., Thirion, B., Grisel, O., ... & Vander-
plas, J. (2011). Scikit-learn: Machine learning in
Python. The Journal of Machine Learning Research,
12, 2825-2830.

[9] Friedman, J., Hastie, T., & Tibshirani, R. (2001).
1).

The elements of statistical
Springer, Berlin: Springer series in statistics.

learning (Vol.

