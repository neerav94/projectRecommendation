Thyroid Dysfunction: Prediction and Diagnostics

Albert Y. Lui & Alexandra M. Pappas

December 11, 2015

I. Introduction

II. Data Preprocessing

The thyroid gland is responsible for regulating hu-
man metabolism by controlling the production of
thyroid hormones. These compounds are known to
have widespread eﬀects on the human body, and their
misregulation often leads to signiﬁcant symptoms.
Patients usually present with heart problems, rapid
weight loss/gain, fatigue, and anxiety.[1] Diagnosti-
cally, thyroid dysfunction is grouped into two cate-
gories: hyperthyroidism and hypothyroidism.

The American Thyroid Association (ATA) re-
ports that more than 1 in 10 Americans develop thy-
roid disease during their lifetime. Currently, an es-
timated 20 million individuals in the US suﬀer from
some form of thyroid disease, and up to 60% are un-
aware of their condition.[2] Although hyperthyroidism
and hypothyroidism are rarely life-threatening, be-
cause of their widespread but subtle symptoms, these
conditions can often signiﬁcantly lower the patient’s
quality of life.

The main focus of this project was to use various
learning methods to model the presence of hyperthy-
roidism or hypothyroidism in previously undiagnosed
patients who were otherwise healthy. Our approach
was exploratory: to compare diﬀerent models and
elucidate any structure within the data. Ultimately,
we hope that these results could help develop more
sensitive guidelines in thyroid disease diagnostics.

The data used for this project was obtained
from the Garavan Institute, an Australian research
facility. All relevant patient information was col-
lected between 1984 and 1987 and subsequently de-
identiﬁed.[7] More details can be found in Table 1.

The focus of this project was to predict thyroid dys-
function in healthy and undiagnosed patients. Of
the 21 outcome categories in the data, 12 did not
meet this criterion∗ and were removed. The set of
7679 eligible patients (of the original 9172) were then
grouped into three categories based on the remain-
ing 9 outcomes: Euthyroid (normal), Hyperthyroid†,
and Hypothyroid‡.

There were many observations with missing
data, which occurred mostly among measurements
of biomarker concentrations. We decided against im-
putation as it assumes that the data is missing at
random. For our data set, this is unlikely. Doctors
are more likely to order for lab tests when patients ex-
hibit symptoms consistent with thyroid dysfunction.
Thus, missing data may be an indicator of patient
health, and imputation would mask this eﬀect. In-
stead, we opted to use methods that could appropri-
ately deal with missing data, or we otherwise removed
troublesome observations for methods that could not.

Prior to removing observations in an eﬀort to
deal with missing data, the TBGm and TBG vari-
ables were dropped, as 7420 of the 7679 patients
had no data here. Afterward, only complete cases
were kept, leaving 4781 observations (37.7% were re-
moved) and 21 predictors. A summary of the ﬁnal
data sets after preprocessing is shown in Table 1.

placement therapy, and undergoing anti-thyroid treatments

∗Concurrent illnesses, undergoing thyroid hormone re-
†Including hyperthyroid, T3 toxic hyperthyroid, toxic goi-
‡Including hypothyroid, primary hypothyroid, compen-

ter, and secondary toxic hyperthyroid.

sated hypothyroid, and secondary hypothyroid.

1

Thyroid Dysfunction: Prediction and Diagnostics

Albert Y. Lui & Alexandra M. Pappas

December 11, 2015

I. Introduction

II. Data Preprocessing

The thyroid gland is responsible for regulating hu-
man metabolism by controlling the production of
thyroid hormones. These compounds are known to
have widespread eﬀects on the human body, and their
misregulation often leads to signiﬁcant symptoms.
Patients usually present with heart problems, rapid
weight loss/gain, fatigue, and anxiety.[1] Diagnosti-
cally, thyroid dysfunction is grouped into two cate-
gories: hyperthyroidism and hypothyroidism.

The American Thyroid Association (ATA) re-
ports that more than 1 in 10 Americans develop thy-
roid disease during their lifetime. Currently, an es-
timated 20 million individuals in the US suﬀer from
some form of thyroid disease, and up to 60% are un-
aware of their condition.[2] Although hyperthyroidism
and hypothyroidism are rarely life-threatening, be-
cause of their widespread but subtle symptoms, these
conditions can often signiﬁcantly lower the patient’s
quality of life.

The main focus of this project was to use various
learning methods to model the presence of hyperthy-
roidism or hypothyroidism in previously undiagnosed
patients who were otherwise healthy. Our approach
was exploratory: to compare diﬀerent models and
elucidate any structure within the data. Ultimately,
we hope that these results could help develop more
sensitive guidelines in thyroid disease diagnostics.

The data used for this project was obtained
from the Garavan Institute, an Australian research
facility. All relevant patient information was col-
lected between 1984 and 1987 and subsequently de-
identiﬁed.[7] More details can be found in Table 1.

The focus of this project was to predict thyroid dys-
function in healthy and undiagnosed patients. Of
the 21 outcome categories in the data, 12 did not
meet this criterion∗ and were removed. The set of
7679 eligible patients (of the original 9172) were then
grouped into three categories based on the remain-
ing 9 outcomes: Euthyroid (normal), Hyperthyroid†,
and Hypothyroid‡.

There were many observations with missing
data, which occurred mostly among measurements
of biomarker concentrations. We decided against im-
putation as it assumes that the data is missing at
random. For our data set, this is unlikely. Doctors
are more likely to order for lab tests when patients ex-
hibit symptoms consistent with thyroid dysfunction.
Thus, missing data may be an indicator of patient
health, and imputation would mask this eﬀect. In-
stead, we opted to use methods that could appropri-
ately deal with missing data, or we otherwise removed
troublesome observations for methods that could not.

Prior to removing observations in an eﬀort to
deal with missing data, the TBGm and TBG vari-
ables were dropped, as 7420 of the 7679 patients
had no data here. Afterward, only complete cases
were kept, leaving 4781 observations (37.7% were re-
moved) and 21 predictors. A summary of the ﬁnal
data sets after preprocessing is shown in Table 1.

placement therapy, and undergoing anti-thyroid treatments

∗Concurrent illnesses, undergoing thyroid hormone re-
†Including hyperthyroid, T3 toxic hyperthyroid, toxic goi-
‡Including hypothyroid, primary hypothyroid, compen-

ter, and secondary toxic hyperthyroid.

sated hypothyroid, and secondary hypothyroid.

1

# of Observations

# of Features

# of Outcome Observations

Data Set

Raw

Full

Reduced

Training Hold-Out

(%)

—
(—)

5679
(74.0)

3522
(73.7)

(%)

—
(—)

2000
(26.0)

1259
(26.3)

Total Bin. Cont. Total

9172

21

7679

21

4781

15

7

7

6

28

28

21

Eu
(%)

6771
(73.8)

6771
(88.2)

4168
(87.2)

Hypo
(%)

241
(2.6)

241
(3.1)

169
(3.5)

Hyper

(%)

667
(7.3)

667
(8.7)

444
(9.3)

Table 1: Summary of data set characteristics (sizes, types of features, and outcome distributions).

III. Methods

The full and reduced data sets were split into two
partitions: the training set for model construction;
and the hold-out set for model evaluation. Where
relevant, forward stepwise feature selection and pa-
rameter tuning were performed on the training set via
10-fold cross-validation. In both data sets, approxi-
mately 13% of patients were diagnosed with thyroid
dysfunction. A trivial predictor (i.e., one that clas-
siﬁes all patients as euthyroid) would give 13% mis-
classiﬁcation error. As such, this was our starting
baseline for improvement. Details can be found in
Table 1.

IV. Logistic Regression &

k-Nearest Neighbors Classification

Given the multi-class classiﬁcation problem at hand,
we began our analysis with multinomial logistic re-
gression, which determines a linear boundary in the
feature space. To assess the linearity of class bound-
aries, we also decided to use k-nearest neighbors
(KNN) classiﬁcation, which can be adjusted to form
highly non-linear boundaries. As neither can cope
with missing values without imputation, both were
trained on the reduced training set.

Multinomial logistic regression was directly ﬁt-
ted and evaluated, as no parameters required tuning.
The optimal value of k = 3 for KNN was determined

through 10-fold cross-validation. The features were
not scaled in the procedure for KNN. Testing these
ﬁtted models on the reduced hold-out set gave gener-
alization errors of 0.03415 and 0.05004, respectively
(Figure 2, bottom right).

The learning curves for both models were then
plotted (Figure 2, top row). These revealed that the
KNN model was overﬁtting the training data and suf-
fered from high variance. The multinomial logistic
model, on the other hand, did not have this issue.
We speculate that the poor performance of KNN is
consistent with the curse of dimensionality, since our
feature space has 21 dimensions but only about 3500
points.

In an eﬀort to mitigate the eﬀects of overﬁt-
ting in KNN, we reduced the feature space using
forward stepwise feature selection with 10-fold cross-
validation error. The shrunken KNN(s) model con-
tains 10 features∗ and gave a generalization error of
0.02939 (Figure 2, bottom right). The learning curve
also indicates far lower variance in the ﬁtted model
along with a lower misclassiﬁcation rate.

Note that the linearity of the class boundaries
are inversely related to the magnitude of k in KNN.
Thus, with the optimal value of k = 3 for our reduced
data set, we speculate that the boundaries separat-
ing euthyroid, hyperthyroid, and hypothyroid obser-
vations may be highly non-linear.

∗Thyroxine, qthyroxine, antithyroid, sick, lithium, goiter,

tumor, hypopituitary, TSH, and T3.

2

Thyroid Dysfunction: Prediction and Diagnostics

Albert Y. Lui & Alexandra M. Pappas

December 11, 2015

I. Introduction

II. Data Preprocessing

The thyroid gland is responsible for regulating hu-
man metabolism by controlling the production of
thyroid hormones. These compounds are known to
have widespread eﬀects on the human body, and their
misregulation often leads to signiﬁcant symptoms.
Patients usually present with heart problems, rapid
weight loss/gain, fatigue, and anxiety.[1] Diagnosti-
cally, thyroid dysfunction is grouped into two cate-
gories: hyperthyroidism and hypothyroidism.

The American Thyroid Association (ATA) re-
ports that more than 1 in 10 Americans develop thy-
roid disease during their lifetime. Currently, an es-
timated 20 million individuals in the US suﬀer from
some form of thyroid disease, and up to 60% are un-
aware of their condition.[2] Although hyperthyroidism
and hypothyroidism are rarely life-threatening, be-
cause of their widespread but subtle symptoms, these
conditions can often signiﬁcantly lower the patient’s
quality of life.

The main focus of this project was to use various
learning methods to model the presence of hyperthy-
roidism or hypothyroidism in previously undiagnosed
patients who were otherwise healthy. Our approach
was exploratory: to compare diﬀerent models and
elucidate any structure within the data. Ultimately,
we hope that these results could help develop more
sensitive guidelines in thyroid disease diagnostics.

The data used for this project was obtained
from the Garavan Institute, an Australian research
facility. All relevant patient information was col-
lected between 1984 and 1987 and subsequently de-
identiﬁed.[7] More details can be found in Table 1.

The focus of this project was to predict thyroid dys-
function in healthy and undiagnosed patients. Of
the 21 outcome categories in the data, 12 did not
meet this criterion∗ and were removed. The set of
7679 eligible patients (of the original 9172) were then
grouped into three categories based on the remain-
ing 9 outcomes: Euthyroid (normal), Hyperthyroid†,
and Hypothyroid‡.

There were many observations with missing
data, which occurred mostly among measurements
of biomarker concentrations. We decided against im-
putation as it assumes that the data is missing at
random. For our data set, this is unlikely. Doctors
are more likely to order for lab tests when patients ex-
hibit symptoms consistent with thyroid dysfunction.
Thus, missing data may be an indicator of patient
health, and imputation would mask this eﬀect. In-
stead, we opted to use methods that could appropri-
ately deal with missing data, or we otherwise removed
troublesome observations for methods that could not.

Prior to removing observations in an eﬀort to
deal with missing data, the TBGm and TBG vari-
ables were dropped, as 7420 of the 7679 patients
had no data here. Afterward, only complete cases
were kept, leaving 4781 observations (37.7% were re-
moved) and 21 predictors. A summary of the ﬁnal
data sets after preprocessing is shown in Table 1.

placement therapy, and undergoing anti-thyroid treatments

∗Concurrent illnesses, undergoing thyroid hormone re-
†Including hyperthyroid, T3 toxic hyperthyroid, toxic goi-
‡Including hypothyroid, primary hypothyroid, compen-

ter, and secondary toxic hyperthyroid.

sated hypothyroid, and secondary hypothyroid.

1

# of Observations

# of Features

# of Outcome Observations

Data Set

Raw

Full

Reduced

Training Hold-Out

(%)

—
(—)

5679
(74.0)

3522
(73.7)

(%)

—
(—)

2000
(26.0)

1259
(26.3)

Total Bin. Cont. Total

9172

21

7679

21

4781

15

7

7

6

28

28

21

Eu
(%)

6771
(73.8)

6771
(88.2)

4168
(87.2)

Hypo
(%)

241
(2.6)

241
(3.1)

169
(3.5)

Hyper

(%)

667
(7.3)

667
(8.7)

444
(9.3)

Table 1: Summary of data set characteristics (sizes, types of features, and outcome distributions).

III. Methods

The full and reduced data sets were split into two
partitions: the training set for model construction;
and the hold-out set for model evaluation. Where
relevant, forward stepwise feature selection and pa-
rameter tuning were performed on the training set via
10-fold cross-validation. In both data sets, approxi-
mately 13% of patients were diagnosed with thyroid
dysfunction. A trivial predictor (i.e., one that clas-
siﬁes all patients as euthyroid) would give 13% mis-
classiﬁcation error. As such, this was our starting
baseline for improvement. Details can be found in
Table 1.

IV. Logistic Regression &

k-Nearest Neighbors Classification

Given the multi-class classiﬁcation problem at hand,
we began our analysis with multinomial logistic re-
gression, which determines a linear boundary in the
feature space. To assess the linearity of class bound-
aries, we also decided to use k-nearest neighbors
(KNN) classiﬁcation, which can be adjusted to form
highly non-linear boundaries. As neither can cope
with missing values without imputation, both were
trained on the reduced training set.

Multinomial logistic regression was directly ﬁt-
ted and evaluated, as no parameters required tuning.
The optimal value of k = 3 for KNN was determined

through 10-fold cross-validation. The features were
not scaled in the procedure for KNN. Testing these
ﬁtted models on the reduced hold-out set gave gener-
alization errors of 0.03415 and 0.05004, respectively
(Figure 2, bottom right).

The learning curves for both models were then
plotted (Figure 2, top row). These revealed that the
KNN model was overﬁtting the training data and suf-
fered from high variance. The multinomial logistic
model, on the other hand, did not have this issue.
We speculate that the poor performance of KNN is
consistent with the curse of dimensionality, since our
feature space has 21 dimensions but only about 3500
points.

In an eﬀort to mitigate the eﬀects of overﬁt-
ting in KNN, we reduced the feature space using
forward stepwise feature selection with 10-fold cross-
validation error. The shrunken KNN(s) model con-
tains 10 features∗ and gave a generalization error of
0.02939 (Figure 2, bottom right). The learning curve
also indicates far lower variance in the ﬁtted model
along with a lower misclassiﬁcation rate.

Note that the linearity of the class boundaries
are inversely related to the magnitude of k in KNN.
Thus, with the optimal value of k = 3 for our reduced
data set, we speculate that the boundaries separat-
ing euthyroid, hyperthyroid, and hypothyroid obser-
vations may be highly non-linear.

∗Thyroxine, qthyroxine, antithyroid, sick, lithium, goiter,

tumor, hypopituitary, TSH, and T3.

2

Model

Param.

Feat. Gen. Error

Logistic

KNN
KNN(s)

SVM

SVM(s)

—

k = 3

k = 3

C = 70
γ = 0.08

C = 70
γ = 0.08

21

21

10

21

10

0.03415

0.05004

0.02939

0.02701

0.02621

Figure 2: Top Row, Bottom Left: Learning curves of misclassiﬁcation error over training set size. Generalization error
(solid) and training error (dashed) curves are also shown. Full model curves are in blue, and smaller model curves
(with forward stepwise feature selection) are in red. Bottom Right: Model summaries. The (s) superscript designates
forward stepwise feature selection.

V. Support Vector Machines

To further explore the boundary non-linearities, we
opted to use support vector machines (SVM) with ra-
dial kernels∗. Unlike multinomial logistic regression
and KNN classiﬁcation, SVMs are inherently binary
classiﬁers. To extend this into the multi-class setting,
we used the one-versus-one classiﬁcation strategy.[6]

As with before, the parameters C and γ were
selected through 10-fold cross-validation, which gave
the optimal values of C = 70 and γ = 0.08. The
learning curves for this model (Figure 2, bottom left,
red) reveals severe overﬁtting, with training error
30 times lower than the generalization error (0.0075

∗K(u, v) = exp{−γ(cid:107)u − v(cid:107)2}

versus 0.02701). Despite this, the SVM still out-
performs the multinomial logistic model, KNN, and
KNN(s).

Again, we addressed this problem of high vari-
ance by reducing the feature space through for-
ward stepwise feature selection. The shrunken model
SVM(s) contains 10 features† with a generalization
error of 0.02621. While this is not a signiﬁcant im-
provement over the full model (with all 21 features),
the learning curve indicates that SVM(s) does not
overﬁt nor does it suﬀer from high variance.

The two shrunken models KNN(s) and SVM(s)
both selected for 10 features from the original set

†Thyroxine, qthyroxine, pregnant, surgery, I131, goiter, tu-

mor, TSH, TT4, and FTI.

3

Thyroid Dysfunction: Prediction and Diagnostics

Albert Y. Lui & Alexandra M. Pappas

December 11, 2015

I. Introduction

II. Data Preprocessing

The thyroid gland is responsible for regulating hu-
man metabolism by controlling the production of
thyroid hormones. These compounds are known to
have widespread eﬀects on the human body, and their
misregulation often leads to signiﬁcant symptoms.
Patients usually present with heart problems, rapid
weight loss/gain, fatigue, and anxiety.[1] Diagnosti-
cally, thyroid dysfunction is grouped into two cate-
gories: hyperthyroidism and hypothyroidism.

The American Thyroid Association (ATA) re-
ports that more than 1 in 10 Americans develop thy-
roid disease during their lifetime. Currently, an es-
timated 20 million individuals in the US suﬀer from
some form of thyroid disease, and up to 60% are un-
aware of their condition.[2] Although hyperthyroidism
and hypothyroidism are rarely life-threatening, be-
cause of their widespread but subtle symptoms, these
conditions can often signiﬁcantly lower the patient’s
quality of life.

The main focus of this project was to use various
learning methods to model the presence of hyperthy-
roidism or hypothyroidism in previously undiagnosed
patients who were otherwise healthy. Our approach
was exploratory: to compare diﬀerent models and
elucidate any structure within the data. Ultimately,
we hope that these results could help develop more
sensitive guidelines in thyroid disease diagnostics.

The data used for this project was obtained
from the Garavan Institute, an Australian research
facility. All relevant patient information was col-
lected between 1984 and 1987 and subsequently de-
identiﬁed.[7] More details can be found in Table 1.

The focus of this project was to predict thyroid dys-
function in healthy and undiagnosed patients. Of
the 21 outcome categories in the data, 12 did not
meet this criterion∗ and were removed. The set of
7679 eligible patients (of the original 9172) were then
grouped into three categories based on the remain-
ing 9 outcomes: Euthyroid (normal), Hyperthyroid†,
and Hypothyroid‡.

There were many observations with missing
data, which occurred mostly among measurements
of biomarker concentrations. We decided against im-
putation as it assumes that the data is missing at
random. For our data set, this is unlikely. Doctors
are more likely to order for lab tests when patients ex-
hibit symptoms consistent with thyroid dysfunction.
Thus, missing data may be an indicator of patient
health, and imputation would mask this eﬀect. In-
stead, we opted to use methods that could appropri-
ately deal with missing data, or we otherwise removed
troublesome observations for methods that could not.

Prior to removing observations in an eﬀort to
deal with missing data, the TBGm and TBG vari-
ables were dropped, as 7420 of the 7679 patients
had no data here. Afterward, only complete cases
were kept, leaving 4781 observations (37.7% were re-
moved) and 21 predictors. A summary of the ﬁnal
data sets after preprocessing is shown in Table 1.

placement therapy, and undergoing anti-thyroid treatments

∗Concurrent illnesses, undergoing thyroid hormone re-
†Including hyperthyroid, T3 toxic hyperthyroid, toxic goi-
‡Including hypothyroid, primary hypothyroid, compen-

ter, and secondary toxic hyperthyroid.

sated hypothyroid, and secondary hypothyroid.

1

# of Observations

# of Features

# of Outcome Observations

Data Set

Raw

Full

Reduced

Training Hold-Out

(%)

—
(—)

5679
(74.0)

3522
(73.7)

(%)

—
(—)

2000
(26.0)

1259
(26.3)

Total Bin. Cont. Total

9172

21

7679

21

4781

15

7

7

6

28

28

21

Eu
(%)

6771
(73.8)

6771
(88.2)

4168
(87.2)

Hypo
(%)

241
(2.6)

241
(3.1)

169
(3.5)

Hyper

(%)

667
(7.3)

667
(8.7)

444
(9.3)

Table 1: Summary of data set characteristics (sizes, types of features, and outcome distributions).

III. Methods

The full and reduced data sets were split into two
partitions: the training set for model construction;
and the hold-out set for model evaluation. Where
relevant, forward stepwise feature selection and pa-
rameter tuning were performed on the training set via
10-fold cross-validation. In both data sets, approxi-
mately 13% of patients were diagnosed with thyroid
dysfunction. A trivial predictor (i.e., one that clas-
siﬁes all patients as euthyroid) would give 13% mis-
classiﬁcation error. As such, this was our starting
baseline for improvement. Details can be found in
Table 1.

IV. Logistic Regression &

k-Nearest Neighbors Classification

Given the multi-class classiﬁcation problem at hand,
we began our analysis with multinomial logistic re-
gression, which determines a linear boundary in the
feature space. To assess the linearity of class bound-
aries, we also decided to use k-nearest neighbors
(KNN) classiﬁcation, which can be adjusted to form
highly non-linear boundaries. As neither can cope
with missing values without imputation, both were
trained on the reduced training set.

Multinomial logistic regression was directly ﬁt-
ted and evaluated, as no parameters required tuning.
The optimal value of k = 3 for KNN was determined

through 10-fold cross-validation. The features were
not scaled in the procedure for KNN. Testing these
ﬁtted models on the reduced hold-out set gave gener-
alization errors of 0.03415 and 0.05004, respectively
(Figure 2, bottom right).

The learning curves for both models were then
plotted (Figure 2, top row). These revealed that the
KNN model was overﬁtting the training data and suf-
fered from high variance. The multinomial logistic
model, on the other hand, did not have this issue.
We speculate that the poor performance of KNN is
consistent with the curse of dimensionality, since our
feature space has 21 dimensions but only about 3500
points.

In an eﬀort to mitigate the eﬀects of overﬁt-
ting in KNN, we reduced the feature space using
forward stepwise feature selection with 10-fold cross-
validation error. The shrunken KNN(s) model con-
tains 10 features∗ and gave a generalization error of
0.02939 (Figure 2, bottom right). The learning curve
also indicates far lower variance in the ﬁtted model
along with a lower misclassiﬁcation rate.

Note that the linearity of the class boundaries
are inversely related to the magnitude of k in KNN.
Thus, with the optimal value of k = 3 for our reduced
data set, we speculate that the boundaries separat-
ing euthyroid, hyperthyroid, and hypothyroid obser-
vations may be highly non-linear.

∗Thyroxine, qthyroxine, antithyroid, sick, lithium, goiter,

tumor, hypopituitary, TSH, and T3.

2

Model

Param.

Feat. Gen. Error

Logistic

KNN
KNN(s)

SVM

SVM(s)

—

k = 3

k = 3

C = 70
γ = 0.08

C = 70
γ = 0.08

21

21

10

21

10

0.03415

0.05004

0.02939

0.02701

0.02621

Figure 2: Top Row, Bottom Left: Learning curves of misclassiﬁcation error over training set size. Generalization error
(solid) and training error (dashed) curves are also shown. Full model curves are in blue, and smaller model curves
(with forward stepwise feature selection) are in red. Bottom Right: Model summaries. The (s) superscript designates
forward stepwise feature selection.

V. Support Vector Machines

To further explore the boundary non-linearities, we
opted to use support vector machines (SVM) with ra-
dial kernels∗. Unlike multinomial logistic regression
and KNN classiﬁcation, SVMs are inherently binary
classiﬁers. To extend this into the multi-class setting,
we used the one-versus-one classiﬁcation strategy.[6]

As with before, the parameters C and γ were
selected through 10-fold cross-validation, which gave
the optimal values of C = 70 and γ = 0.08. The
learning curves for this model (Figure 2, bottom left,
red) reveals severe overﬁtting, with training error
30 times lower than the generalization error (0.0075

∗K(u, v) = exp{−γ(cid:107)u − v(cid:107)2}

versus 0.02701). Despite this, the SVM still out-
performs the multinomial logistic model, KNN, and
KNN(s).

Again, we addressed this problem of high vari-
ance by reducing the feature space through for-
ward stepwise feature selection. The shrunken model
SVM(s) contains 10 features† with a generalization
error of 0.02621. While this is not a signiﬁcant im-
provement over the full model (with all 21 features),
the learning curve indicates that SVM(s) does not
overﬁt nor does it suﬀer from high variance.

The two shrunken models KNN(s) and SVM(s)
both selected for 10 features from the original set

†Thyroxine, qthyroxine, pregnant, surgery, I131, goiter, tu-

mor, TSH, TT4, and FTI.

3

of 21. These selections were not identical, though
ﬁve made it into both models: thyroxine, qthyroxine,
goiter, tumor, and TSH.

VI. Classification Trees &

Bootstrap Aggregation

The appeal of using decision trees (CART) for this
project is three-fold: (i) it lends itself very well to
easy interpretation; (ii) it inherently performs feature
selection; and (iii) it has simple methods of dealing
with missing data.[8] In order to compare CART to
the previous methods, we ﬁtted two models, one over
the reduced set and another over the full set of data.

The decision trees were grown and subsequently
pruned using the optimal cost complexity parameter
α. This value was determined using 10-fold cross-
validation error, and to further ease model inter-
pretability, we opted to follow the one-standard-error
heuristic to generate a simpler tree.[6] The values were
α = 0.1 for the reduced data set and α = 0.035 for the
full data set. For missing values, a surrogate splitting
strategy was used.

The CART model ﬁtted on the reduced train-
ing set out-performs all previously discussed models.
This fact was particularly surprising in the context of
the tree’s simplicity (Figure 3, left). With only two
splits (on TSH and FTI), this model achieves a gener-
alization error of 0.02383. This is in contrast with the
next best model SVM(s), at 0.02621 misclassiﬁcation
rate with ten features.

On the other hand, the CART model trained on
the full data set gives even higher performance, with a
generalization error of 0.02150. This, however, is not
surprising for two reasons. Firstly, there are many
more observations in the full training set (5679 versus
3522). Secondly, as we had mentioned previously,
the missing values may contain information regarding
patient health.

In an eﬀort to further lower the error rate at the
cost of low interpretability, we bootstrap aggregated
(bagged) the decision trees. This paradigm of ensem-

Figure 3: Decision Trees (CART) trained on the re-
duced and full sets of data, respectively. Both trees
were pruned using the cost complexity parameter de-
termined using the one-standard-error heuristic on 10-
fold cross-validation errors.

ble learning allows lower bias by using more ﬂexible
ﬁts, while tempering high variance by averaging the
predictions over multiple base learners. Using an en-
semble of n = 2000 decision trees (no pruning), we
achieved generalization errors of 0.01589 over the re-
duced set and 0.1500 over the full set of data.

4

Thyroid Dysfunction: Prediction and Diagnostics

Albert Y. Lui & Alexandra M. Pappas

December 11, 2015

I. Introduction

II. Data Preprocessing

The thyroid gland is responsible for regulating hu-
man metabolism by controlling the production of
thyroid hormones. These compounds are known to
have widespread eﬀects on the human body, and their
misregulation often leads to signiﬁcant symptoms.
Patients usually present with heart problems, rapid
weight loss/gain, fatigue, and anxiety.[1] Diagnosti-
cally, thyroid dysfunction is grouped into two cate-
gories: hyperthyroidism and hypothyroidism.

The American Thyroid Association (ATA) re-
ports that more than 1 in 10 Americans develop thy-
roid disease during their lifetime. Currently, an es-
timated 20 million individuals in the US suﬀer from
some form of thyroid disease, and up to 60% are un-
aware of their condition.[2] Although hyperthyroidism
and hypothyroidism are rarely life-threatening, be-
cause of their widespread but subtle symptoms, these
conditions can often signiﬁcantly lower the patient’s
quality of life.

The main focus of this project was to use various
learning methods to model the presence of hyperthy-
roidism or hypothyroidism in previously undiagnosed
patients who were otherwise healthy. Our approach
was exploratory: to compare diﬀerent models and
elucidate any structure within the data. Ultimately,
we hope that these results could help develop more
sensitive guidelines in thyroid disease diagnostics.

The data used for this project was obtained
from the Garavan Institute, an Australian research
facility. All relevant patient information was col-
lected between 1984 and 1987 and subsequently de-
identiﬁed.[7] More details can be found in Table 1.

The focus of this project was to predict thyroid dys-
function in healthy and undiagnosed patients. Of
the 21 outcome categories in the data, 12 did not
meet this criterion∗ and were removed. The set of
7679 eligible patients (of the original 9172) were then
grouped into three categories based on the remain-
ing 9 outcomes: Euthyroid (normal), Hyperthyroid†,
and Hypothyroid‡.

There were many observations with missing
data, which occurred mostly among measurements
of biomarker concentrations. We decided against im-
putation as it assumes that the data is missing at
random. For our data set, this is unlikely. Doctors
are more likely to order for lab tests when patients ex-
hibit symptoms consistent with thyroid dysfunction.
Thus, missing data may be an indicator of patient
health, and imputation would mask this eﬀect. In-
stead, we opted to use methods that could appropri-
ately deal with missing data, or we otherwise removed
troublesome observations for methods that could not.

Prior to removing observations in an eﬀort to
deal with missing data, the TBGm and TBG vari-
ables were dropped, as 7420 of the 7679 patients
had no data here. Afterward, only complete cases
were kept, leaving 4781 observations (37.7% were re-
moved) and 21 predictors. A summary of the ﬁnal
data sets after preprocessing is shown in Table 1.

placement therapy, and undergoing anti-thyroid treatments

∗Concurrent illnesses, undergoing thyroid hormone re-
†Including hyperthyroid, T3 toxic hyperthyroid, toxic goi-
‡Including hypothyroid, primary hypothyroid, compen-

ter, and secondary toxic hyperthyroid.

sated hypothyroid, and secondary hypothyroid.

1

# of Observations

# of Features

# of Outcome Observations

Data Set

Raw

Full

Reduced

Training Hold-Out

(%)

—
(—)

5679
(74.0)

3522
(73.7)

(%)

—
(—)

2000
(26.0)

1259
(26.3)

Total Bin. Cont. Total

9172

21

7679

21

4781

15

7

7

6

28

28

21

Eu
(%)

6771
(73.8)

6771
(88.2)

4168
(87.2)

Hypo
(%)

241
(2.6)

241
(3.1)

169
(3.5)

Hyper

(%)

667
(7.3)

667
(8.7)

444
(9.3)

Table 1: Summary of data set characteristics (sizes, types of features, and outcome distributions).

III. Methods

The full and reduced data sets were split into two
partitions: the training set for model construction;
and the hold-out set for model evaluation. Where
relevant, forward stepwise feature selection and pa-
rameter tuning were performed on the training set via
10-fold cross-validation. In both data sets, approxi-
mately 13% of patients were diagnosed with thyroid
dysfunction. A trivial predictor (i.e., one that clas-
siﬁes all patients as euthyroid) would give 13% mis-
classiﬁcation error. As such, this was our starting
baseline for improvement. Details can be found in
Table 1.

IV. Logistic Regression &

k-Nearest Neighbors Classification

Given the multi-class classiﬁcation problem at hand,
we began our analysis with multinomial logistic re-
gression, which determines a linear boundary in the
feature space. To assess the linearity of class bound-
aries, we also decided to use k-nearest neighbors
(KNN) classiﬁcation, which can be adjusted to form
highly non-linear boundaries. As neither can cope
with missing values without imputation, both were
trained on the reduced training set.

Multinomial logistic regression was directly ﬁt-
ted and evaluated, as no parameters required tuning.
The optimal value of k = 3 for KNN was determined

through 10-fold cross-validation. The features were
not scaled in the procedure for KNN. Testing these
ﬁtted models on the reduced hold-out set gave gener-
alization errors of 0.03415 and 0.05004, respectively
(Figure 2, bottom right).

The learning curves for both models were then
plotted (Figure 2, top row). These revealed that the
KNN model was overﬁtting the training data and suf-
fered from high variance. The multinomial logistic
model, on the other hand, did not have this issue.
We speculate that the poor performance of KNN is
consistent with the curse of dimensionality, since our
feature space has 21 dimensions but only about 3500
points.

In an eﬀort to mitigate the eﬀects of overﬁt-
ting in KNN, we reduced the feature space using
forward stepwise feature selection with 10-fold cross-
validation error. The shrunken KNN(s) model con-
tains 10 features∗ and gave a generalization error of
0.02939 (Figure 2, bottom right). The learning curve
also indicates far lower variance in the ﬁtted model
along with a lower misclassiﬁcation rate.

Note that the linearity of the class boundaries
are inversely related to the magnitude of k in KNN.
Thus, with the optimal value of k = 3 for our reduced
data set, we speculate that the boundaries separat-
ing euthyroid, hyperthyroid, and hypothyroid obser-
vations may be highly non-linear.

∗Thyroxine, qthyroxine, antithyroid, sick, lithium, goiter,

tumor, hypopituitary, TSH, and T3.

2

Model

Param.

Feat. Gen. Error

Logistic

KNN
KNN(s)

SVM

SVM(s)

—

k = 3

k = 3

C = 70
γ = 0.08

C = 70
γ = 0.08

21

21

10

21

10

0.03415

0.05004

0.02939

0.02701

0.02621

Figure 2: Top Row, Bottom Left: Learning curves of misclassiﬁcation error over training set size. Generalization error
(solid) and training error (dashed) curves are also shown. Full model curves are in blue, and smaller model curves
(with forward stepwise feature selection) are in red. Bottom Right: Model summaries. The (s) superscript designates
forward stepwise feature selection.

V. Support Vector Machines

To further explore the boundary non-linearities, we
opted to use support vector machines (SVM) with ra-
dial kernels∗. Unlike multinomial logistic regression
and KNN classiﬁcation, SVMs are inherently binary
classiﬁers. To extend this into the multi-class setting,
we used the one-versus-one classiﬁcation strategy.[6]

As with before, the parameters C and γ were
selected through 10-fold cross-validation, which gave
the optimal values of C = 70 and γ = 0.08. The
learning curves for this model (Figure 2, bottom left,
red) reveals severe overﬁtting, with training error
30 times lower than the generalization error (0.0075

∗K(u, v) = exp{−γ(cid:107)u − v(cid:107)2}

versus 0.02701). Despite this, the SVM still out-
performs the multinomial logistic model, KNN, and
KNN(s).

Again, we addressed this problem of high vari-
ance by reducing the feature space through for-
ward stepwise feature selection. The shrunken model
SVM(s) contains 10 features† with a generalization
error of 0.02621. While this is not a signiﬁcant im-
provement over the full model (with all 21 features),
the learning curve indicates that SVM(s) does not
overﬁt nor does it suﬀer from high variance.

The two shrunken models KNN(s) and SVM(s)
both selected for 10 features from the original set

†Thyroxine, qthyroxine, pregnant, surgery, I131, goiter, tu-

mor, TSH, TT4, and FTI.

3

of 21. These selections were not identical, though
ﬁve made it into both models: thyroxine, qthyroxine,
goiter, tumor, and TSH.

VI. Classification Trees &

Bootstrap Aggregation

The appeal of using decision trees (CART) for this
project is three-fold: (i) it lends itself very well to
easy interpretation; (ii) it inherently performs feature
selection; and (iii) it has simple methods of dealing
with missing data.[8] In order to compare CART to
the previous methods, we ﬁtted two models, one over
the reduced set and another over the full set of data.

The decision trees were grown and subsequently
pruned using the optimal cost complexity parameter
α. This value was determined using 10-fold cross-
validation error, and to further ease model inter-
pretability, we opted to follow the one-standard-error
heuristic to generate a simpler tree.[6] The values were
α = 0.1 for the reduced data set and α = 0.035 for the
full data set. For missing values, a surrogate splitting
strategy was used.

The CART model ﬁtted on the reduced train-
ing set out-performs all previously discussed models.
This fact was particularly surprising in the context of
the tree’s simplicity (Figure 3, left). With only two
splits (on TSH and FTI), this model achieves a gener-
alization error of 0.02383. This is in contrast with the
next best model SVM(s), at 0.02621 misclassiﬁcation
rate with ten features.

On the other hand, the CART model trained on
the full data set gives even higher performance, with a
generalization error of 0.02150. This, however, is not
surprising for two reasons. Firstly, there are many
more observations in the full training set (5679 versus
3522). Secondly, as we had mentioned previously,
the missing values may contain information regarding
patient health.

In an eﬀort to further lower the error rate at the
cost of low interpretability, we bootstrap aggregated
(bagged) the decision trees. This paradigm of ensem-

Figure 3: Decision Trees (CART) trained on the re-
duced and full sets of data, respectively. Both trees
were pruned using the cost complexity parameter de-
termined using the one-standard-error heuristic on 10-
fold cross-validation errors.

ble learning allows lower bias by using more ﬂexible
ﬁts, while tempering high variance by averaging the
predictions over multiple base learners. Using an en-
semble of n = 2000 decision trees (no pruning), we
achieved generalization errors of 0.01589 over the re-
duced set and 0.1500 over the full set of data.

4

VII. Conclusions

From our analysis, it appears that thyroid dysfunc-
tion can be predicted to good accuracy using TSH,
FTI, and TT4 in a simple decision tree (CART). In
the context of human physiology, these results are
not surprising. Speciﬁcally, FTI∗ and TT4† are mea-
surements of thyroid hormone concentration, so one
would expect hyperthyroid patients to have elevated
levels. The inverse would also be true of hypothyroid
patients.[3] TSH‡ concentration is inversely related to
that of thyroid hormones. Thus, higher TSH would
be indicative of lower thyroid hormone levels.[4]

In the practice of medicine, thyroid disease diag-
nosis suﬀers from high inconsistency, especially with
respect to TSH assays. Some have argued that na-
tional standardization of techniques may be helpful
in improving patient care.[5] Despite this, our results
suggests that a level of less than 6 µUI/mL may be a
good global cut-oﬀ (regardless of assay).

One possible avenue of future research would be

∗Free Thyroxine Index
†Total T4 (Thyroxine)
‡Thyroid Stimulating Hormone

Data Set

Model

Gen. Error

Logistic
KNN(s)
SVM(s)
CART
CART(b)

CART
CART(b)

0.03415
0.02939
0.02621
0.02383
0.01589

0.02150
0.01500

Reduced

Full

Table 2: Generalization errors for the various methods
used and over the two data sets. The superscripts (s)
and (b) respectively designate forward stepwise feature
selection and bagging.

to determine the validity of the above assertion. How-
ever, to do so, it would be preferable to collect a bet-
ter set of data. Speciﬁcally: (i) more recent data, as
this set was from the mid-1980s; (ii) more complete
data, to see if other classiﬁcation methods can per-
form better than decision trees; and (iii) a larger set
of data from a larger variety of sources.

VIII. References

[1] “Thyroid Diseases: MedlinePlus.” U.S National Library of
Medicine. U.S. National Library of Medicine, 12 Nov. 2015.
Web. 12 Dec. 2015.

[2] American

tion/Press Room.
2015,
hypothyroidism/

(n.d.).

Thyroid Association,

Informa-
Retrieved December 7,
from http://www.thyroid.org/media-main/about-

General

[3] “Free T4.” : The Test. American Association for Clinical

Chemistry, 29 Oct. 2015. Web. 12 Dec. 2015.

[4] Faix, James D., MD, and Linda M. Thienpont, PhD.
“Thyroid-Stimulating Hormone.” - AACC.org. American
Association for Clinical Chemistry, 1 May 2013. Web. 12
Dec. 2015.

[5] “TSH.” : The Test. American Association for Clinical Chem-

istry, 29 Oct. 2015. Web. 12 Dec. 2015.

[6] James, Gareth, Daniela Witten, Trevor Hastie, and Robert
Tibshirani. An Introduction to Statistical Learning: With
Applications in R. New York: Springer Science+Business
Media, 2013. Electronic.

[7] Lichman, M. (2013). UCI Machine Learning Repository
Irvine, CA: University of

[http://archive.ics.uci.edu/ml].
California, School of Information and Computer Science.

[8] Hastie, Trevor, Robert Tibshirani, and J. H. Friedman. The
Elements of Statistical Learning: Data Mining, Inference,
and Prediction. 2nd ed. New York: Springer, 2009. Print.

Figure 4: Variable importance of the bagged decision
trees (CART). Only the ten highest are shown, as all
others had values less than 0.0025 and were omitted
for clarity.

5

