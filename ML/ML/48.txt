Predicting Song Popularity

James Pham

jqpham@stanford.edu

Edric Kyauk

ekyauk@stanford.edu

Edwin Park

edpark@stanford.edu

Department of Computer Science

Department of Computer Science

Department of Computer Science

Stanford University

Stanford University

Stanford University

Abstract—Predicting song popularity is particularly important
in keeping businesses competitive within a growing music in-
dustry. But what exactly makes a song popular? Starting with
the Million Song Dataset, a collection of audio features and
metadata for approximately one million songs, we evaluated
different classiﬁcation and regression algorithms on their ability
to predict popularity and determined the types of features that
hold the most predictive power.

I. INTRODUCTION

Music has been an integral part of our culture all through-
out human history. In 2012 alone, the U.S. music industry
generated $15 billion. Of this $15 billion, the majority of the
revenue is generated by popular, mainstream songs. Having a
fundamental understanding of what makes a song popular has
major implications to businesses that thrive on popular music,
namely radio stations, record labels, and digital and physical
music marketplaces.

The ability to make accurate predictions of song popular-
ity also has implications for customized music suggestions.
Predicting popular songs can be applied to the problem of
predicting preferred songs for a given population.

Making predictions of song popularity based on machine
learning, often referred to as Hit Song Science, is a problem
that has gained a lot of traction within the past 10 years. Many
private companies in the music industry are working on this
problem, but details about the success of these companies are
held private for competitive reasons.

Our project focuses on predicting whether or not a song is
popular based on a number of characteristics of the song and
its artist. The input for our algorithm is a list of song charac-
teristics consisting of both acoustic features and metadata. We
then use a number of machine learning algorithms (SVMs,
neural networks,
logistic regression, Gaussian discriminant
analysis, and linear regression) to output whether or not the
song is popular (or in the case of linear regression the score
of the song popularity).

II. RELATED WORK

The problem of predicting popularity is one that has been
heavily researched. Salganik, Dodds, and Watts conducted an
experimental study on popularity that focused heavily on the
social inﬂuence of popularity. They found that the quality of a
song only partially inﬂuences whether or not a song becomes
popular, and that social inﬂuence plays an extremely large role
[7]. Therefore, our project aims to use both acoustic features

and metadata features to create a more accurate prediction
model.

The work by Koenignstein, Shavitt, and Zilberman, which
predicts billboard success based on peer-to-peer networks,
potentially captures this social inﬂuence on song popularity.
This group was extremely thorough with their work and
used multiple regression and classiﬁcation algorithms for their
predictions [4].

Bertin-Mahieux et al. found that machine learning tech-
niques can be used to apply labels to songs based on acoustic
features. They created a model for predicting social tags from
acoustic features on a large music database by using AdaBoost
and FilterBoost [1]. While this group was extremely thorough
with considering the possible models to use and sanitizing their
features, using SVMs instead of AdaBoost with FilterBoost
may have been a better option.

However, Pachet and Roy investigated the problem of
making predictions of song popularity and made the blunt
claim that the popularity of a song cannot be learnt by using
state-of-the-art machine learning [6]. In order to test the effec-
tiveness of current machine learning algorithms, they test the
improvement of their classiﬁcation models to a generic random
classiﬁer. Similarly to our work, Pachet and Roy consider both
acoustic features and metadata; however, the study deals with
an extremely large number of features (over 600) but does
not mention any type of feature selection algorithm. As a
result it is extremely likely that their model was subjected to
overﬁtting. Pacet and Roy also considered features commonly
used for music analysis which potentially could have affected
the success of their results.

However, Ni et al have responded to the above deﬁnitive
claim with more optimistic results on music popularity pre-
diction, using a Shifting Perceptron algorithm to classify the
top 5 hits from the top 30-40 hits (a slightly different problem
from the aforementioned study) [5]. However, this study also
uses more novel audio features which is a likely factor in their
improved results.

III. DATASET AND FEATURES

A. Data

We used music data from The Million Song Dataset [8].
is an exhaustive collection of
The Million Song Dataset
audio features and metadata for one million songs dating to
2011. The audio features include attributes about the music
track itself, such as duration, key, year. The metadata uses

Predicting Song Popularity

James Pham

jqpham@stanford.edu

Edric Kyauk

ekyauk@stanford.edu

Edwin Park

edpark@stanford.edu

Department of Computer Science

Department of Computer Science

Department of Computer Science

Stanford University

Stanford University

Stanford University

Abstract—Predicting song popularity is particularly important
in keeping businesses competitive within a growing music in-
dustry. But what exactly makes a song popular? Starting with
the Million Song Dataset, a collection of audio features and
metadata for approximately one million songs, we evaluated
different classiﬁcation and regression algorithms on their ability
to predict popularity and determined the types of features that
hold the most predictive power.

I. INTRODUCTION

Music has been an integral part of our culture all through-
out human history. In 2012 alone, the U.S. music industry
generated $15 billion. Of this $15 billion, the majority of the
revenue is generated by popular, mainstream songs. Having a
fundamental understanding of what makes a song popular has
major implications to businesses that thrive on popular music,
namely radio stations, record labels, and digital and physical
music marketplaces.

The ability to make accurate predictions of song popular-
ity also has implications for customized music suggestions.
Predicting popular songs can be applied to the problem of
predicting preferred songs for a given population.

Making predictions of song popularity based on machine
learning, often referred to as Hit Song Science, is a problem
that has gained a lot of traction within the past 10 years. Many
private companies in the music industry are working on this
problem, but details about the success of these companies are
held private for competitive reasons.

Our project focuses on predicting whether or not a song is
popular based on a number of characteristics of the song and
its artist. The input for our algorithm is a list of song charac-
teristics consisting of both acoustic features and metadata. We
then use a number of machine learning algorithms (SVMs,
neural networks,
logistic regression, Gaussian discriminant
analysis, and linear regression) to output whether or not the
song is popular (or in the case of linear regression the score
of the song popularity).

II. RELATED WORK

The problem of predicting popularity is one that has been
heavily researched. Salganik, Dodds, and Watts conducted an
experimental study on popularity that focused heavily on the
social inﬂuence of popularity. They found that the quality of a
song only partially inﬂuences whether or not a song becomes
popular, and that social inﬂuence plays an extremely large role
[7]. Therefore, our project aims to use both acoustic features

and metadata features to create a more accurate prediction
model.

The work by Koenignstein, Shavitt, and Zilberman, which
predicts billboard success based on peer-to-peer networks,
potentially captures this social inﬂuence on song popularity.
This group was extremely thorough with their work and
used multiple regression and classiﬁcation algorithms for their
predictions [4].

Bertin-Mahieux et al. found that machine learning tech-
niques can be used to apply labels to songs based on acoustic
features. They created a model for predicting social tags from
acoustic features on a large music database by using AdaBoost
and FilterBoost [1]. While this group was extremely thorough
with considering the possible models to use and sanitizing their
features, using SVMs instead of AdaBoost with FilterBoost
may have been a better option.

However, Pachet and Roy investigated the problem of
making predictions of song popularity and made the blunt
claim that the popularity of a song cannot be learnt by using
state-of-the-art machine learning [6]. In order to test the effec-
tiveness of current machine learning algorithms, they test the
improvement of their classiﬁcation models to a generic random
classiﬁer. Similarly to our work, Pachet and Roy consider both
acoustic features and metadata; however, the study deals with
an extremely large number of features (over 600) but does
not mention any type of feature selection algorithm. As a
result it is extremely likely that their model was subjected to
overﬁtting. Pacet and Roy also considered features commonly
used for music analysis which potentially could have affected
the success of their results.

However, Ni et al have responded to the above deﬁnitive
claim with more optimistic results on music popularity pre-
diction, using a Shifting Perceptron algorithm to classify the
top 5 hits from the top 30-40 hits (a slightly different problem
from the aforementioned study) [5]. However, this study also
uses more novel audio features which is a likely factor in their
improved results.

III. DATASET AND FEATURES

A. Data

We used music data from The Million Song Dataset [8].
is an exhaustive collection of
The Million Song Dataset
audio features and metadata for one million songs dating to
2011. The audio features include attributes about the music
track itself, such as duration, key, year. The metadata uses

more abstract features, such as danceability, energy, or song
hotttnesss, generated from The Echo Nest, a music intelligence
platform. Our project uses a subset of this data. We extracted
10,000 tracks from the database and of those 10,000 tracks,
we removed all tracks that were missing any of features we
were considering. This left us with 2,717 tracks. We divided
these tracks so that 90% of the tracks was used for training
and 10% was used for testing. Below is a subset of the ﬁelds
for a song in the datset.

Feature
key
loudness
mode
mode conﬁdence
release

Type
int
ﬂoat
int
ﬂoat
string

Description
key the song is in
overall loudness in dB
major or minor
conﬁdence measure
album name

B. Feature Extraction

1) Baseline Features: The Million Song Dataset contains a
plethora of feature types, ranging from number-type features
such as those that measure the general loudness of a song,
string-type features such as names of artists and albums, and
array-type features such as those that contain pitches across
the length of the song. All array-type acoustic features contain
measures for segments across the song. We included the mean
and variance throughout all of the segments as features.

2) Additional Features: By looking at plots of different
features vs. song popularity, we saw that there were some non-
linear relationships. By squaring the values, we were able to
capture polynomial relationships between some of our features
and the popularity. Additionally, to avoid constraining our
model under an additive assumption, we also incorporated
some interaction terms (e.g. tempo × major/minor) to capture
multiplicative effects.

3) Bag of Words: Our dataset includes many string features,
including song name, artist id, and terms that the artist is
frequently associated with (genre). In order to capture these in
our model, we used a bag of words approach and added the
top 900 frequently occurring words as term frequency features
in our model.

C. Popularity

The Echo Nest provides social ﬁeld for a song called song
hotttnesss which we will use as our metric of popularity. While
the exact calculation of this ﬁeld is not released, the metric is
generally based upon total activity they see for the songs on
the thousands of websites that Echo Nest uses. Songs with a
hotttnesss value above a threshold were classiﬁed as popular;
in our case, we deﬁned songs as popular if they were in the
top 25% of song hotttnesss. We set this threshold to be 0.623,
since this was the 75th percentile for our dataset.

IV. METHODS

A. Feature Selection

In total, our ﬁnal dataset consists of 977 features, many
of which were not relevant to predicting popularity. As a

p(x; µ, Σ) =

1

(2π)n/2|Σ|1/2

exp

(x − µ)T Σ−1(x − µ)

(cid:16) − 1

2

result, using all features suffered from overﬁtting. In order
narrow down the number of features and ﬁnd the most relevant
features, we conducted several feature selection algorithms.

1) Forward Stepwise Selection: Forward selection greedily
chooses the best combination of features by starting with an
empty subset of features, then incrementally adding a feature
to the model that was selected through evaluation of the feature
subset through cross-validation. This step is repeated until
the generalization error is minimized and the best subset of
features is reported.

2) Backward Stepwise Selection: Backward stepwise selec-
tion works similarly to forward stepwise selection; however,
instead of starting with an empty subset of features, it begins
by evaluating the use of all features and incrementally removes
features until the model is optimized.

3) l1 Regularization:

l1 Regularization is a shrinkage
method that regularizes the coefﬁcient estimates by shrinking
the coefﬁcients towards zero. Regularization often improves
the ﬁt because reducing coefﬁcient estimates can signiﬁcantly
reduce their variance, thus decreasing the effect of overﬁtting.

m(cid:88)

min

θ

(y(i) − θT x(i))2 + λ

|θj|

i=1

j=1

n(cid:88)

By increasing the tuning parameter λ, the shrinkage penalty
term effectively serves to force some coefﬁcient estimates
to become exactly equal to zero. l1 regularization has an
advantage over l2 regularization in that the models obtained
by l1 regularization are easily interpretable: setting some
coefﬁcient estimates to zero is essentially performing feature
selection. Tuning λ has a bias-variance trade-off: increasing λ
decreases the ﬂexibility of the ﬁt but also increases bias.

B. Classiﬁcation

1) Logistic Regression: We used logistic regression with l1
regularization. Logistic regression selects the parameter θ that
maximizes the likelihood function:

m(cid:89)

m(cid:89)

L(θ) =

p(y(i)|x(i); θ) =

(hθ(x(i)))y(i)

(1−hθ(x(i))1−y(i)

i=1

i=1
where

hθ(x) = g(θT x) =

1

1 + e−θT x

2) Linear Discriminant Analysis (LDA): We used LDA,
which is a speciﬁc method of Gaussian Discriminant Analysis,
to build our classiﬁer. LDA assumes that samples have come
from a multivariate Gaussian distribution with a speciﬁc mean
vector and a covariance matrix that applies to all classes. LDA
is similar to logistic regression in that they both produce linear
boundaries and thus similar results, but LDA tends to perform
better when the Gaussian assumptions are met.

(cid:17)

Predicting Song Popularity

James Pham

jqpham@stanford.edu

Edric Kyauk

ekyauk@stanford.edu

Edwin Park

edpark@stanford.edu

Department of Computer Science

Department of Computer Science

Department of Computer Science

Stanford University

Stanford University

Stanford University

Abstract—Predicting song popularity is particularly important
in keeping businesses competitive within a growing music in-
dustry. But what exactly makes a song popular? Starting with
the Million Song Dataset, a collection of audio features and
metadata for approximately one million songs, we evaluated
different classiﬁcation and regression algorithms on their ability
to predict popularity and determined the types of features that
hold the most predictive power.

I. INTRODUCTION

Music has been an integral part of our culture all through-
out human history. In 2012 alone, the U.S. music industry
generated $15 billion. Of this $15 billion, the majority of the
revenue is generated by popular, mainstream songs. Having a
fundamental understanding of what makes a song popular has
major implications to businesses that thrive on popular music,
namely radio stations, record labels, and digital and physical
music marketplaces.

The ability to make accurate predictions of song popular-
ity also has implications for customized music suggestions.
Predicting popular songs can be applied to the problem of
predicting preferred songs for a given population.

Making predictions of song popularity based on machine
learning, often referred to as Hit Song Science, is a problem
that has gained a lot of traction within the past 10 years. Many
private companies in the music industry are working on this
problem, but details about the success of these companies are
held private for competitive reasons.

Our project focuses on predicting whether or not a song is
popular based on a number of characteristics of the song and
its artist. The input for our algorithm is a list of song charac-
teristics consisting of both acoustic features and metadata. We
then use a number of machine learning algorithms (SVMs,
neural networks,
logistic regression, Gaussian discriminant
analysis, and linear regression) to output whether or not the
song is popular (or in the case of linear regression the score
of the song popularity).

II. RELATED WORK

The problem of predicting popularity is one that has been
heavily researched. Salganik, Dodds, and Watts conducted an
experimental study on popularity that focused heavily on the
social inﬂuence of popularity. They found that the quality of a
song only partially inﬂuences whether or not a song becomes
popular, and that social inﬂuence plays an extremely large role
[7]. Therefore, our project aims to use both acoustic features

and metadata features to create a more accurate prediction
model.

The work by Koenignstein, Shavitt, and Zilberman, which
predicts billboard success based on peer-to-peer networks,
potentially captures this social inﬂuence on song popularity.
This group was extremely thorough with their work and
used multiple regression and classiﬁcation algorithms for their
predictions [4].

Bertin-Mahieux et al. found that machine learning tech-
niques can be used to apply labels to songs based on acoustic
features. They created a model for predicting social tags from
acoustic features on a large music database by using AdaBoost
and FilterBoost [1]. While this group was extremely thorough
with considering the possible models to use and sanitizing their
features, using SVMs instead of AdaBoost with FilterBoost
may have been a better option.

However, Pachet and Roy investigated the problem of
making predictions of song popularity and made the blunt
claim that the popularity of a song cannot be learnt by using
state-of-the-art machine learning [6]. In order to test the effec-
tiveness of current machine learning algorithms, they test the
improvement of their classiﬁcation models to a generic random
classiﬁer. Similarly to our work, Pachet and Roy consider both
acoustic features and metadata; however, the study deals with
an extremely large number of features (over 600) but does
not mention any type of feature selection algorithm. As a
result it is extremely likely that their model was subjected to
overﬁtting. Pacet and Roy also considered features commonly
used for music analysis which potentially could have affected
the success of their results.

However, Ni et al have responded to the above deﬁnitive
claim with more optimistic results on music popularity pre-
diction, using a Shifting Perceptron algorithm to classify the
top 5 hits from the top 30-40 hits (a slightly different problem
from the aforementioned study) [5]. However, this study also
uses more novel audio features which is a likely factor in their
improved results.

III. DATASET AND FEATURES

A. Data

We used music data from The Million Song Dataset [8].
is an exhaustive collection of
The Million Song Dataset
audio features and metadata for one million songs dating to
2011. The audio features include attributes about the music
track itself, such as duration, key, year. The metadata uses

more abstract features, such as danceability, energy, or song
hotttnesss, generated from The Echo Nest, a music intelligence
platform. Our project uses a subset of this data. We extracted
10,000 tracks from the database and of those 10,000 tracks,
we removed all tracks that were missing any of features we
were considering. This left us with 2,717 tracks. We divided
these tracks so that 90% of the tracks was used for training
and 10% was used for testing. Below is a subset of the ﬁelds
for a song in the datset.

Feature
key
loudness
mode
mode conﬁdence
release

Type
int
ﬂoat
int
ﬂoat
string

Description
key the song is in
overall loudness in dB
major or minor
conﬁdence measure
album name

B. Feature Extraction

1) Baseline Features: The Million Song Dataset contains a
plethora of feature types, ranging from number-type features
such as those that measure the general loudness of a song,
string-type features such as names of artists and albums, and
array-type features such as those that contain pitches across
the length of the song. All array-type acoustic features contain
measures for segments across the song. We included the mean
and variance throughout all of the segments as features.

2) Additional Features: By looking at plots of different
features vs. song popularity, we saw that there were some non-
linear relationships. By squaring the values, we were able to
capture polynomial relationships between some of our features
and the popularity. Additionally, to avoid constraining our
model under an additive assumption, we also incorporated
some interaction terms (e.g. tempo × major/minor) to capture
multiplicative effects.

3) Bag of Words: Our dataset includes many string features,
including song name, artist id, and terms that the artist is
frequently associated with (genre). In order to capture these in
our model, we used a bag of words approach and added the
top 900 frequently occurring words as term frequency features
in our model.

C. Popularity

The Echo Nest provides social ﬁeld for a song called song
hotttnesss which we will use as our metric of popularity. While
the exact calculation of this ﬁeld is not released, the metric is
generally based upon total activity they see for the songs on
the thousands of websites that Echo Nest uses. Songs with a
hotttnesss value above a threshold were classiﬁed as popular;
in our case, we deﬁned songs as popular if they were in the
top 25% of song hotttnesss. We set this threshold to be 0.623,
since this was the 75th percentile for our dataset.

IV. METHODS

A. Feature Selection

In total, our ﬁnal dataset consists of 977 features, many
of which were not relevant to predicting popularity. As a

p(x; µ, Σ) =

1

(2π)n/2|Σ|1/2

exp

(x − µ)T Σ−1(x − µ)

(cid:16) − 1

2

result, using all features suffered from overﬁtting. In order
narrow down the number of features and ﬁnd the most relevant
features, we conducted several feature selection algorithms.

1) Forward Stepwise Selection: Forward selection greedily
chooses the best combination of features by starting with an
empty subset of features, then incrementally adding a feature
to the model that was selected through evaluation of the feature
subset through cross-validation. This step is repeated until
the generalization error is minimized and the best subset of
features is reported.

2) Backward Stepwise Selection: Backward stepwise selec-
tion works similarly to forward stepwise selection; however,
instead of starting with an empty subset of features, it begins
by evaluating the use of all features and incrementally removes
features until the model is optimized.

3) l1 Regularization:

l1 Regularization is a shrinkage
method that regularizes the coefﬁcient estimates by shrinking
the coefﬁcients towards zero. Regularization often improves
the ﬁt because reducing coefﬁcient estimates can signiﬁcantly
reduce their variance, thus decreasing the effect of overﬁtting.

m(cid:88)

min

θ

(y(i) − θT x(i))2 + λ

|θj|

i=1

j=1

n(cid:88)

By increasing the tuning parameter λ, the shrinkage penalty
term effectively serves to force some coefﬁcient estimates
to become exactly equal to zero. l1 regularization has an
advantage over l2 regularization in that the models obtained
by l1 regularization are easily interpretable: setting some
coefﬁcient estimates to zero is essentially performing feature
selection. Tuning λ has a bias-variance trade-off: increasing λ
decreases the ﬂexibility of the ﬁt but also increases bias.

B. Classiﬁcation

1) Logistic Regression: We used logistic regression with l1
regularization. Logistic regression selects the parameter θ that
maximizes the likelihood function:

m(cid:89)

m(cid:89)

L(θ) =

p(y(i)|x(i); θ) =

(hθ(x(i)))y(i)

(1−hθ(x(i))1−y(i)

i=1

i=1
where

hθ(x) = g(θT x) =

1

1 + e−θT x

2) Linear Discriminant Analysis (LDA): We used LDA,
which is a speciﬁc method of Gaussian Discriminant Analysis,
to build our classiﬁer. LDA assumes that samples have come
from a multivariate Gaussian distribution with a speciﬁc mean
vector and a covariance matrix that applies to all classes. LDA
is similar to logistic regression in that they both produce linear
boundaries and thus similar results, but LDA tends to perform
better when the Gaussian assumptions are met.

(cid:17)

3) Quadratic Discriminant Analysis (QDA): Similar to
LDA, QDA is another speciﬁc method of Gaussian Dis-
criminant Analysis that assumes that samples come from a
multivariate Gaussian distribution with a speciﬁc mean vector.
However, unlike LDA,
it assumes that each class has its
own covariance matrix. By estimating multiple covariance
matrices, QDA allows for more ﬂexible ﬁt by allowing non-
linear boundaries.

1

exp

p(x; µ, Σk) =

(x−µ)T Σ−1

(2π)n/2|Σk|1/2

k (x−µ)
4) Support Vector Machines (SVM): We also leveraged
support vector machines to classify our data. To create an
optimal margin classiﬁer, a decision boundary or separating
hyperplane can be calculated that maximizes the distance
of the nearest points of any class to the decision boundary.
For SVMs to perform efﬁciently in high-dimensional spaces,
SVMs leverage the kernel trick, which allows for computation
without having to explicitly represent the high-dimensional
feature vectors.

(cid:16)− 1

2

(cid:17)

max

α

W (α) =

α(i) − 1
2

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

m(cid:88)

i=1

m(cid:88)

i,j=1

s.t. 0 ≤ αi ≤ C, i = 1, ..., m

m(cid:88)

i=1

αiy(i) = 0

We used a Gaussian radial basis function (rbf) kernel to
create a non-linear classiﬁer by transforming the feature space.

(cid:16)||x − z||2

(cid:17)

2σ2

K(x, z) = exp

5) Multilayer Perceptron: We also used a multilayer per-
ceptron classiﬁcation algorithm, an example of a neural net-
work model. The MLP consists of a directed graph, where
possibly different dimensional layers of nodes are all fully
connected. The ﬁrst layer of nodes embody the original set
of features, and the ﬁnal layer of nodes represents higher
level features inﬂuenced by multiple original features. The
weights of these nodes are learned through backpropagation.
To minimize the error of the output, we use stochastic gradient
descent to ﬁnd the change of each nodes weight and the
activation function y(vi) = tanh(vi) to map the weighted
inputs to the output of each neuron.

(cid:88)

j

E(n) =

1
2

Fig. 1. Forward Stepwise Feature Selection

Fig. 2. Backward Stepwise Feature Selection

Fig. 3. Most Important Variables

multiple linear regression, and applied feature selection meth-
ods to achieve the best coefﬁcient estimates for regression.
For feature selection, we evaluated each model of size 1 ... m
features using cross-validated mean squared errors.

V. EXPERIMENTAL RESULTS

e2
j (n)

A. Feature Selection

∆wji(n) = −η

δE(n)
δvJ (n)

yi(n)

C. Regression

Seeing our classiﬁcation results, we noticed that classiﬁ-
cation approaches lose valuable information about the value
of the song popularity itself due to the binary conversion.
As a result, we also use regression to predict the values of
popularity. For regression, we ﬁtted models using a standard

For both forward stepwise selection and backward stepwise
selection methods, we determined the optimal number of
features in our ﬁtted model by seeing which set of features
gives the smallest cross-validation error. From a model with
976 features, forward stepwise selection chose a model of
45 features (see Fig. 1), whereas backward stepwise selection
chose a model of 81 features (see Fig. 2). In both feature se-
lection methods, signiﬁcant features included artist familiarity,
loudness, year, and tag words such as ”alternative”, ”guitar”,
and ”jazz” (see Fig. 3).

Predicting Song Popularity

James Pham

jqpham@stanford.edu

Edric Kyauk

ekyauk@stanford.edu

Edwin Park

edpark@stanford.edu

Department of Computer Science

Department of Computer Science

Department of Computer Science

Stanford University

Stanford University

Stanford University

Abstract—Predicting song popularity is particularly important
in keeping businesses competitive within a growing music in-
dustry. But what exactly makes a song popular? Starting with
the Million Song Dataset, a collection of audio features and
metadata for approximately one million songs, we evaluated
different classiﬁcation and regression algorithms on their ability
to predict popularity and determined the types of features that
hold the most predictive power.

I. INTRODUCTION

Music has been an integral part of our culture all through-
out human history. In 2012 alone, the U.S. music industry
generated $15 billion. Of this $15 billion, the majority of the
revenue is generated by popular, mainstream songs. Having a
fundamental understanding of what makes a song popular has
major implications to businesses that thrive on popular music,
namely radio stations, record labels, and digital and physical
music marketplaces.

The ability to make accurate predictions of song popular-
ity also has implications for customized music suggestions.
Predicting popular songs can be applied to the problem of
predicting preferred songs for a given population.

Making predictions of song popularity based on machine
learning, often referred to as Hit Song Science, is a problem
that has gained a lot of traction within the past 10 years. Many
private companies in the music industry are working on this
problem, but details about the success of these companies are
held private for competitive reasons.

Our project focuses on predicting whether or not a song is
popular based on a number of characteristics of the song and
its artist. The input for our algorithm is a list of song charac-
teristics consisting of both acoustic features and metadata. We
then use a number of machine learning algorithms (SVMs,
neural networks,
logistic regression, Gaussian discriminant
analysis, and linear regression) to output whether or not the
song is popular (or in the case of linear regression the score
of the song popularity).

II. RELATED WORK

The problem of predicting popularity is one that has been
heavily researched. Salganik, Dodds, and Watts conducted an
experimental study on popularity that focused heavily on the
social inﬂuence of popularity. They found that the quality of a
song only partially inﬂuences whether or not a song becomes
popular, and that social inﬂuence plays an extremely large role
[7]. Therefore, our project aims to use both acoustic features

and metadata features to create a more accurate prediction
model.

The work by Koenignstein, Shavitt, and Zilberman, which
predicts billboard success based on peer-to-peer networks,
potentially captures this social inﬂuence on song popularity.
This group was extremely thorough with their work and
used multiple regression and classiﬁcation algorithms for their
predictions [4].

Bertin-Mahieux et al. found that machine learning tech-
niques can be used to apply labels to songs based on acoustic
features. They created a model for predicting social tags from
acoustic features on a large music database by using AdaBoost
and FilterBoost [1]. While this group was extremely thorough
with considering the possible models to use and sanitizing their
features, using SVMs instead of AdaBoost with FilterBoost
may have been a better option.

However, Pachet and Roy investigated the problem of
making predictions of song popularity and made the blunt
claim that the popularity of a song cannot be learnt by using
state-of-the-art machine learning [6]. In order to test the effec-
tiveness of current machine learning algorithms, they test the
improvement of their classiﬁcation models to a generic random
classiﬁer. Similarly to our work, Pachet and Roy consider both
acoustic features and metadata; however, the study deals with
an extremely large number of features (over 600) but does
not mention any type of feature selection algorithm. As a
result it is extremely likely that their model was subjected to
overﬁtting. Pacet and Roy also considered features commonly
used for music analysis which potentially could have affected
the success of their results.

However, Ni et al have responded to the above deﬁnitive
claim with more optimistic results on music popularity pre-
diction, using a Shifting Perceptron algorithm to classify the
top 5 hits from the top 30-40 hits (a slightly different problem
from the aforementioned study) [5]. However, this study also
uses more novel audio features which is a likely factor in their
improved results.

III. DATASET AND FEATURES

A. Data

We used music data from The Million Song Dataset [8].
is an exhaustive collection of
The Million Song Dataset
audio features and metadata for one million songs dating to
2011. The audio features include attributes about the music
track itself, such as duration, key, year. The metadata uses

more abstract features, such as danceability, energy, or song
hotttnesss, generated from The Echo Nest, a music intelligence
platform. Our project uses a subset of this data. We extracted
10,000 tracks from the database and of those 10,000 tracks,
we removed all tracks that were missing any of features we
were considering. This left us with 2,717 tracks. We divided
these tracks so that 90% of the tracks was used for training
and 10% was used for testing. Below is a subset of the ﬁelds
for a song in the datset.

Feature
key
loudness
mode
mode conﬁdence
release

Type
int
ﬂoat
int
ﬂoat
string

Description
key the song is in
overall loudness in dB
major or minor
conﬁdence measure
album name

B. Feature Extraction

1) Baseline Features: The Million Song Dataset contains a
plethora of feature types, ranging from number-type features
such as those that measure the general loudness of a song,
string-type features such as names of artists and albums, and
array-type features such as those that contain pitches across
the length of the song. All array-type acoustic features contain
measures for segments across the song. We included the mean
and variance throughout all of the segments as features.

2) Additional Features: By looking at plots of different
features vs. song popularity, we saw that there were some non-
linear relationships. By squaring the values, we were able to
capture polynomial relationships between some of our features
and the popularity. Additionally, to avoid constraining our
model under an additive assumption, we also incorporated
some interaction terms (e.g. tempo × major/minor) to capture
multiplicative effects.

3) Bag of Words: Our dataset includes many string features,
including song name, artist id, and terms that the artist is
frequently associated with (genre). In order to capture these in
our model, we used a bag of words approach and added the
top 900 frequently occurring words as term frequency features
in our model.

C. Popularity

The Echo Nest provides social ﬁeld for a song called song
hotttnesss which we will use as our metric of popularity. While
the exact calculation of this ﬁeld is not released, the metric is
generally based upon total activity they see for the songs on
the thousands of websites that Echo Nest uses. Songs with a
hotttnesss value above a threshold were classiﬁed as popular;
in our case, we deﬁned songs as popular if they were in the
top 25% of song hotttnesss. We set this threshold to be 0.623,
since this was the 75th percentile for our dataset.

IV. METHODS

A. Feature Selection

In total, our ﬁnal dataset consists of 977 features, many
of which were not relevant to predicting popularity. As a

p(x; µ, Σ) =

1

(2π)n/2|Σ|1/2

exp

(x − µ)T Σ−1(x − µ)

(cid:16) − 1

2

result, using all features suffered from overﬁtting. In order
narrow down the number of features and ﬁnd the most relevant
features, we conducted several feature selection algorithms.

1) Forward Stepwise Selection: Forward selection greedily
chooses the best combination of features by starting with an
empty subset of features, then incrementally adding a feature
to the model that was selected through evaluation of the feature
subset through cross-validation. This step is repeated until
the generalization error is minimized and the best subset of
features is reported.

2) Backward Stepwise Selection: Backward stepwise selec-
tion works similarly to forward stepwise selection; however,
instead of starting with an empty subset of features, it begins
by evaluating the use of all features and incrementally removes
features until the model is optimized.

3) l1 Regularization:

l1 Regularization is a shrinkage
method that regularizes the coefﬁcient estimates by shrinking
the coefﬁcients towards zero. Regularization often improves
the ﬁt because reducing coefﬁcient estimates can signiﬁcantly
reduce their variance, thus decreasing the effect of overﬁtting.

m(cid:88)

min

θ

(y(i) − θT x(i))2 + λ

|θj|

i=1

j=1

n(cid:88)

By increasing the tuning parameter λ, the shrinkage penalty
term effectively serves to force some coefﬁcient estimates
to become exactly equal to zero. l1 regularization has an
advantage over l2 regularization in that the models obtained
by l1 regularization are easily interpretable: setting some
coefﬁcient estimates to zero is essentially performing feature
selection. Tuning λ has a bias-variance trade-off: increasing λ
decreases the ﬂexibility of the ﬁt but also increases bias.

B. Classiﬁcation

1) Logistic Regression: We used logistic regression with l1
regularization. Logistic regression selects the parameter θ that
maximizes the likelihood function:

m(cid:89)

m(cid:89)

L(θ) =

p(y(i)|x(i); θ) =

(hθ(x(i)))y(i)

(1−hθ(x(i))1−y(i)

i=1

i=1
where

hθ(x) = g(θT x) =

1

1 + e−θT x

2) Linear Discriminant Analysis (LDA): We used LDA,
which is a speciﬁc method of Gaussian Discriminant Analysis,
to build our classiﬁer. LDA assumes that samples have come
from a multivariate Gaussian distribution with a speciﬁc mean
vector and a covariance matrix that applies to all classes. LDA
is similar to logistic regression in that they both produce linear
boundaries and thus similar results, but LDA tends to perform
better when the Gaussian assumptions are met.

(cid:17)

3) Quadratic Discriminant Analysis (QDA): Similar to
LDA, QDA is another speciﬁc method of Gaussian Dis-
criminant Analysis that assumes that samples come from a
multivariate Gaussian distribution with a speciﬁc mean vector.
However, unlike LDA,
it assumes that each class has its
own covariance matrix. By estimating multiple covariance
matrices, QDA allows for more ﬂexible ﬁt by allowing non-
linear boundaries.

1

exp

p(x; µ, Σk) =

(x−µ)T Σ−1

(2π)n/2|Σk|1/2

k (x−µ)
4) Support Vector Machines (SVM): We also leveraged
support vector machines to classify our data. To create an
optimal margin classiﬁer, a decision boundary or separating
hyperplane can be calculated that maximizes the distance
of the nearest points of any class to the decision boundary.
For SVMs to perform efﬁciently in high-dimensional spaces,
SVMs leverage the kernel trick, which allows for computation
without having to explicitly represent the high-dimensional
feature vectors.

(cid:16)− 1

2

(cid:17)

max

α

W (α) =

α(i) − 1
2

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

m(cid:88)

i=1

m(cid:88)

i,j=1

s.t. 0 ≤ αi ≤ C, i = 1, ..., m

m(cid:88)

i=1

αiy(i) = 0

We used a Gaussian radial basis function (rbf) kernel to
create a non-linear classiﬁer by transforming the feature space.

(cid:16)||x − z||2

(cid:17)

2σ2

K(x, z) = exp

5) Multilayer Perceptron: We also used a multilayer per-
ceptron classiﬁcation algorithm, an example of a neural net-
work model. The MLP consists of a directed graph, where
possibly different dimensional layers of nodes are all fully
connected. The ﬁrst layer of nodes embody the original set
of features, and the ﬁnal layer of nodes represents higher
level features inﬂuenced by multiple original features. The
weights of these nodes are learned through backpropagation.
To minimize the error of the output, we use stochastic gradient
descent to ﬁnd the change of each nodes weight and the
activation function y(vi) = tanh(vi) to map the weighted
inputs to the output of each neuron.

(cid:88)

j

E(n) =

1
2

Fig. 1. Forward Stepwise Feature Selection

Fig. 2. Backward Stepwise Feature Selection

Fig. 3. Most Important Variables

multiple linear regression, and applied feature selection meth-
ods to achieve the best coefﬁcient estimates for regression.
For feature selection, we evaluated each model of size 1 ... m
features using cross-validated mean squared errors.

V. EXPERIMENTAL RESULTS

e2
j (n)

A. Feature Selection

∆wji(n) = −η

δE(n)
δvJ (n)

yi(n)

C. Regression

Seeing our classiﬁcation results, we noticed that classiﬁ-
cation approaches lose valuable information about the value
of the song popularity itself due to the binary conversion.
As a result, we also use regression to predict the values of
popularity. For regression, we ﬁtted models using a standard

For both forward stepwise selection and backward stepwise
selection methods, we determined the optimal number of
features in our ﬁtted model by seeing which set of features
gives the smallest cross-validation error. From a model with
976 features, forward stepwise selection chose a model of
45 features (see Fig. 1), whereas backward stepwise selection
chose a model of 81 features (see Fig. 2). In both feature se-
lection methods, signiﬁcant features included artist familiarity,
loudness, year, and tag words such as ”alternative”, ”guitar”,
and ”jazz” (see Fig. 3).

simplicity of the decision surface. γ determines the weight of
a single training example. We used 10-fold CV to tune the C
and γ parameters. For the linear kernel, C and γ were chosen
n respectively, and for the RBF kernel, C and γ
to be 1 and 1
were chosen to be 44 and .0001 respectively.

Additionally, we used 10-fold CV to tune two parameters for
the MLP; we chose tanh(x) to be the activation function for
the hidden layers of nodes, and we chose stochastic gradient
descent to be the algorithm for node weight optimization.

3) Results: See Fig. 4, 5, and 6.
4) Discussion: The algorithm with the highest F1 score
was the SVM using a Gaussian kernel. Considering the fact
that the SVM with a linear kernel had a lower F1 score than
the Gaussian kernel, our data is likely non-linear. As a result,
SVMs using a Gaussian kernel would naturally perform better
than the other models that assume linearity. Additionally,
higher F1 scores were achieved using the rbf kernel over the
linear kernel suggesting nonlinear relationships between the
audio/metadata features and popularity.

However, there were no signiﬁcant differences in the perfor-
mance among all our our tested models; F1 scores all ranged
between 0.5 and 0.6 and the accuracies all ranged between
0.75 and 0.80. This could be due to the fact that all of our
classiﬁcation algorithms assume the data is linearly separa-
ble, and the data was mostly likely not linearly separable.
The differences in the performance of these models can be
attributed to the tradeoffs made in the algorithms. For example,
SVMs are primarily inﬂuenced by the data points closest to
the margin, while logistic regression and other models are
inﬂuenced by all data points. SVMs also perform better in
problems with a high number of dimensions.

We chose our hotttnesss threshold to consider the top 25%
of songs to be popular (songs with a hotttnesss larger than
0.623). We initially chose a threshold that considered the
top 50% of songs to be popular and were receiving fairly
low metrics across the board,
including precision, recall,
F1 score, and accuracy. We hypothesize that this happened
because popular songs (which we predict to have deﬁning
characteristics) in reality are only a small subset of all songs.
Therefore, the top percentile of popular songs that had these
deﬁning characteristics were confounded by the more songs
we labeled as popular. As a result we changed our threshold
to classify the top 5% as popular. This increased the accuracy,
but it was the result of our algorithms predicting that all songs
were not popular, a fact that was reﬂected in recalls of 0.
A likely cause of this result is a lack of popular training
examples. By using the top 25% of songs as popular, we are
able to have songs with deﬁning characteristics in addition to
enough popular examples to properly train our models.

C. Regression

1) Metrics: In the regression setting, we use mean squared
error (MSE) to evaluate how well our model predicts pop-
ularity. The average error is obtained by taking the square
root of the MSE, which approximates the standard error of

Fig. 4. ROC Curve for SVM with Gaussian Kernel

Model
SVM (Linear Kernel)
SVM (RBF Kernel)
Logistic Regression (LR)
LDA
QDA
Multilayer Perceptron (MLP)

AUC
0.79
0.81
0.69
0.71
0.64
0.79

Fig. 5. AUC for Classiﬁcation Models

Model
SVM (Linear)
SVM (RBF)
Logistic Reg
LDA
QDA
MLP

Train
0.830
0.827
0.861
0.858
0.756
0.889
Fig. 6. Classiﬁcation Results: {train, test} accuracy

F1
0.585
0.597
0.531
0.567
0.454
0.504

R

0.706
0.750
0.500
0.529
0.618
0.441

P

0.500
0.495
0.567
0.610
0.359
0.588

Test
0.762
0.759
0.790
0.807
0.647
0.793

B. Classiﬁcation

1) Metrics: Since we set our threshold such that 75% of
the songs in our dataset are classiﬁed as not popular and
25% are classiﬁed as popular, 75% accuracy can be achieved
by predicting all 0s. Therefore, we concluded that accuracy
alone would not be a good measure of how well our model
can classify. In order to capture this, we also considered the
precision, recall, F1, and AUC scores of our models.

Precision, recall, and F1 score are used to capture how well
our model does in the task of classiﬁcation. Precision measures
the portion of examples that were classiﬁed as popular that are
truly popular while recall measures the portion of examples
that are truly popular that our model classiﬁed as popular. F1
score acts as the weighted average between these two values.
The area under the receiver operator characteristic curve
is a metric used to evaluate the performance of a binary
classiﬁer by taking the area under a curve created by plotting
TPR vs. FPR at different probability thresholds. The AUC
represents the probability that the classiﬁer ranks a random
positive example higher than a random negative one.

2) Tuning Parameters: With respect to the kernels used
by our SVM models, we optimized two parameters: C and
γ. C determines the trade-offs between misclassiﬁcation and

Predicting Song Popularity

James Pham

jqpham@stanford.edu

Edric Kyauk

ekyauk@stanford.edu

Edwin Park

edpark@stanford.edu

Department of Computer Science

Department of Computer Science

Department of Computer Science

Stanford University

Stanford University

Stanford University

Abstract—Predicting song popularity is particularly important
in keeping businesses competitive within a growing music in-
dustry. But what exactly makes a song popular? Starting with
the Million Song Dataset, a collection of audio features and
metadata for approximately one million songs, we evaluated
different classiﬁcation and regression algorithms on their ability
to predict popularity and determined the types of features that
hold the most predictive power.

I. INTRODUCTION

Music has been an integral part of our culture all through-
out human history. In 2012 alone, the U.S. music industry
generated $15 billion. Of this $15 billion, the majority of the
revenue is generated by popular, mainstream songs. Having a
fundamental understanding of what makes a song popular has
major implications to businesses that thrive on popular music,
namely radio stations, record labels, and digital and physical
music marketplaces.

The ability to make accurate predictions of song popular-
ity also has implications for customized music suggestions.
Predicting popular songs can be applied to the problem of
predicting preferred songs for a given population.

Making predictions of song popularity based on machine
learning, often referred to as Hit Song Science, is a problem
that has gained a lot of traction within the past 10 years. Many
private companies in the music industry are working on this
problem, but details about the success of these companies are
held private for competitive reasons.

Our project focuses on predicting whether or not a song is
popular based on a number of characteristics of the song and
its artist. The input for our algorithm is a list of song charac-
teristics consisting of both acoustic features and metadata. We
then use a number of machine learning algorithms (SVMs,
neural networks,
logistic regression, Gaussian discriminant
analysis, and linear regression) to output whether or not the
song is popular (or in the case of linear regression the score
of the song popularity).

II. RELATED WORK

The problem of predicting popularity is one that has been
heavily researched. Salganik, Dodds, and Watts conducted an
experimental study on popularity that focused heavily on the
social inﬂuence of popularity. They found that the quality of a
song only partially inﬂuences whether or not a song becomes
popular, and that social inﬂuence plays an extremely large role
[7]. Therefore, our project aims to use both acoustic features

and metadata features to create a more accurate prediction
model.

The work by Koenignstein, Shavitt, and Zilberman, which
predicts billboard success based on peer-to-peer networks,
potentially captures this social inﬂuence on song popularity.
This group was extremely thorough with their work and
used multiple regression and classiﬁcation algorithms for their
predictions [4].

Bertin-Mahieux et al. found that machine learning tech-
niques can be used to apply labels to songs based on acoustic
features. They created a model for predicting social tags from
acoustic features on a large music database by using AdaBoost
and FilterBoost [1]. While this group was extremely thorough
with considering the possible models to use and sanitizing their
features, using SVMs instead of AdaBoost with FilterBoost
may have been a better option.

However, Pachet and Roy investigated the problem of
making predictions of song popularity and made the blunt
claim that the popularity of a song cannot be learnt by using
state-of-the-art machine learning [6]. In order to test the effec-
tiveness of current machine learning algorithms, they test the
improvement of their classiﬁcation models to a generic random
classiﬁer. Similarly to our work, Pachet and Roy consider both
acoustic features and metadata; however, the study deals with
an extremely large number of features (over 600) but does
not mention any type of feature selection algorithm. As a
result it is extremely likely that their model was subjected to
overﬁtting. Pacet and Roy also considered features commonly
used for music analysis which potentially could have affected
the success of their results.

However, Ni et al have responded to the above deﬁnitive
claim with more optimistic results on music popularity pre-
diction, using a Shifting Perceptron algorithm to classify the
top 5 hits from the top 30-40 hits (a slightly different problem
from the aforementioned study) [5]. However, this study also
uses more novel audio features which is a likely factor in their
improved results.

III. DATASET AND FEATURES

A. Data

We used music data from The Million Song Dataset [8].
is an exhaustive collection of
The Million Song Dataset
audio features and metadata for one million songs dating to
2011. The audio features include attributes about the music
track itself, such as duration, key, year. The metadata uses

more abstract features, such as danceability, energy, or song
hotttnesss, generated from The Echo Nest, a music intelligence
platform. Our project uses a subset of this data. We extracted
10,000 tracks from the database and of those 10,000 tracks,
we removed all tracks that were missing any of features we
were considering. This left us with 2,717 tracks. We divided
these tracks so that 90% of the tracks was used for training
and 10% was used for testing. Below is a subset of the ﬁelds
for a song in the datset.

Feature
key
loudness
mode
mode conﬁdence
release

Type
int
ﬂoat
int
ﬂoat
string

Description
key the song is in
overall loudness in dB
major or minor
conﬁdence measure
album name

B. Feature Extraction

1) Baseline Features: The Million Song Dataset contains a
plethora of feature types, ranging from number-type features
such as those that measure the general loudness of a song,
string-type features such as names of artists and albums, and
array-type features such as those that contain pitches across
the length of the song. All array-type acoustic features contain
measures for segments across the song. We included the mean
and variance throughout all of the segments as features.

2) Additional Features: By looking at plots of different
features vs. song popularity, we saw that there were some non-
linear relationships. By squaring the values, we were able to
capture polynomial relationships between some of our features
and the popularity. Additionally, to avoid constraining our
model under an additive assumption, we also incorporated
some interaction terms (e.g. tempo × major/minor) to capture
multiplicative effects.

3) Bag of Words: Our dataset includes many string features,
including song name, artist id, and terms that the artist is
frequently associated with (genre). In order to capture these in
our model, we used a bag of words approach and added the
top 900 frequently occurring words as term frequency features
in our model.

C. Popularity

The Echo Nest provides social ﬁeld for a song called song
hotttnesss which we will use as our metric of popularity. While
the exact calculation of this ﬁeld is not released, the metric is
generally based upon total activity they see for the songs on
the thousands of websites that Echo Nest uses. Songs with a
hotttnesss value above a threshold were classiﬁed as popular;
in our case, we deﬁned songs as popular if they were in the
top 25% of song hotttnesss. We set this threshold to be 0.623,
since this was the 75th percentile for our dataset.

IV. METHODS

A. Feature Selection

In total, our ﬁnal dataset consists of 977 features, many
of which were not relevant to predicting popularity. As a

p(x; µ, Σ) =

1

(2π)n/2|Σ|1/2

exp

(x − µ)T Σ−1(x − µ)

(cid:16) − 1

2

result, using all features suffered from overﬁtting. In order
narrow down the number of features and ﬁnd the most relevant
features, we conducted several feature selection algorithms.

1) Forward Stepwise Selection: Forward selection greedily
chooses the best combination of features by starting with an
empty subset of features, then incrementally adding a feature
to the model that was selected through evaluation of the feature
subset through cross-validation. This step is repeated until
the generalization error is minimized and the best subset of
features is reported.

2) Backward Stepwise Selection: Backward stepwise selec-
tion works similarly to forward stepwise selection; however,
instead of starting with an empty subset of features, it begins
by evaluating the use of all features and incrementally removes
features until the model is optimized.

3) l1 Regularization:

l1 Regularization is a shrinkage
method that regularizes the coefﬁcient estimates by shrinking
the coefﬁcients towards zero. Regularization often improves
the ﬁt because reducing coefﬁcient estimates can signiﬁcantly
reduce their variance, thus decreasing the effect of overﬁtting.

m(cid:88)

min

θ

(y(i) − θT x(i))2 + λ

|θj|

i=1

j=1

n(cid:88)

By increasing the tuning parameter λ, the shrinkage penalty
term effectively serves to force some coefﬁcient estimates
to become exactly equal to zero. l1 regularization has an
advantage over l2 regularization in that the models obtained
by l1 regularization are easily interpretable: setting some
coefﬁcient estimates to zero is essentially performing feature
selection. Tuning λ has a bias-variance trade-off: increasing λ
decreases the ﬂexibility of the ﬁt but also increases bias.

B. Classiﬁcation

1) Logistic Regression: We used logistic regression with l1
regularization. Logistic regression selects the parameter θ that
maximizes the likelihood function:

m(cid:89)

m(cid:89)

L(θ) =

p(y(i)|x(i); θ) =

(hθ(x(i)))y(i)

(1−hθ(x(i))1−y(i)

i=1

i=1
where

hθ(x) = g(θT x) =

1

1 + e−θT x

2) Linear Discriminant Analysis (LDA): We used LDA,
which is a speciﬁc method of Gaussian Discriminant Analysis,
to build our classiﬁer. LDA assumes that samples have come
from a multivariate Gaussian distribution with a speciﬁc mean
vector and a covariance matrix that applies to all classes. LDA
is similar to logistic regression in that they both produce linear
boundaries and thus similar results, but LDA tends to perform
better when the Gaussian assumptions are met.

(cid:17)

3) Quadratic Discriminant Analysis (QDA): Similar to
LDA, QDA is another speciﬁc method of Gaussian Dis-
criminant Analysis that assumes that samples come from a
multivariate Gaussian distribution with a speciﬁc mean vector.
However, unlike LDA,
it assumes that each class has its
own covariance matrix. By estimating multiple covariance
matrices, QDA allows for more ﬂexible ﬁt by allowing non-
linear boundaries.

1

exp

p(x; µ, Σk) =

(x−µ)T Σ−1

(2π)n/2|Σk|1/2

k (x−µ)
4) Support Vector Machines (SVM): We also leveraged
support vector machines to classify our data. To create an
optimal margin classiﬁer, a decision boundary or separating
hyperplane can be calculated that maximizes the distance
of the nearest points of any class to the decision boundary.
For SVMs to perform efﬁciently in high-dimensional spaces,
SVMs leverage the kernel trick, which allows for computation
without having to explicitly represent the high-dimensional
feature vectors.

(cid:16)− 1

2

(cid:17)

max

α

W (α) =

α(i) − 1
2

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

m(cid:88)

i=1

m(cid:88)

i,j=1

s.t. 0 ≤ αi ≤ C, i = 1, ..., m

m(cid:88)

i=1

αiy(i) = 0

We used a Gaussian radial basis function (rbf) kernel to
create a non-linear classiﬁer by transforming the feature space.

(cid:16)||x − z||2

(cid:17)

2σ2

K(x, z) = exp

5) Multilayer Perceptron: We also used a multilayer per-
ceptron classiﬁcation algorithm, an example of a neural net-
work model. The MLP consists of a directed graph, where
possibly different dimensional layers of nodes are all fully
connected. The ﬁrst layer of nodes embody the original set
of features, and the ﬁnal layer of nodes represents higher
level features inﬂuenced by multiple original features. The
weights of these nodes are learned through backpropagation.
To minimize the error of the output, we use stochastic gradient
descent to ﬁnd the change of each nodes weight and the
activation function y(vi) = tanh(vi) to map the weighted
inputs to the output of each neuron.

(cid:88)

j

E(n) =

1
2

Fig. 1. Forward Stepwise Feature Selection

Fig. 2. Backward Stepwise Feature Selection

Fig. 3. Most Important Variables

multiple linear regression, and applied feature selection meth-
ods to achieve the best coefﬁcient estimates for regression.
For feature selection, we evaluated each model of size 1 ... m
features using cross-validated mean squared errors.

V. EXPERIMENTAL RESULTS

e2
j (n)

A. Feature Selection

∆wji(n) = −η

δE(n)
δvJ (n)

yi(n)

C. Regression

Seeing our classiﬁcation results, we noticed that classiﬁ-
cation approaches lose valuable information about the value
of the song popularity itself due to the binary conversion.
As a result, we also use regression to predict the values of
popularity. For regression, we ﬁtted models using a standard

For both forward stepwise selection and backward stepwise
selection methods, we determined the optimal number of
features in our ﬁtted model by seeing which set of features
gives the smallest cross-validation error. From a model with
976 features, forward stepwise selection chose a model of
45 features (see Fig. 1), whereas backward stepwise selection
chose a model of 81 features (see Fig. 2). In both feature se-
lection methods, signiﬁcant features included artist familiarity,
loudness, year, and tag words such as ”alternative”, ”guitar”,
and ”jazz” (see Fig. 3).

simplicity of the decision surface. γ determines the weight of
a single training example. We used 10-fold CV to tune the C
and γ parameters. For the linear kernel, C and γ were chosen
n respectively, and for the RBF kernel, C and γ
to be 1 and 1
were chosen to be 44 and .0001 respectively.

Additionally, we used 10-fold CV to tune two parameters for
the MLP; we chose tanh(x) to be the activation function for
the hidden layers of nodes, and we chose stochastic gradient
descent to be the algorithm for node weight optimization.

3) Results: See Fig. 4, 5, and 6.
4) Discussion: The algorithm with the highest F1 score
was the SVM using a Gaussian kernel. Considering the fact
that the SVM with a linear kernel had a lower F1 score than
the Gaussian kernel, our data is likely non-linear. As a result,
SVMs using a Gaussian kernel would naturally perform better
than the other models that assume linearity. Additionally,
higher F1 scores were achieved using the rbf kernel over the
linear kernel suggesting nonlinear relationships between the
audio/metadata features and popularity.

However, there were no signiﬁcant differences in the perfor-
mance among all our our tested models; F1 scores all ranged
between 0.5 and 0.6 and the accuracies all ranged between
0.75 and 0.80. This could be due to the fact that all of our
classiﬁcation algorithms assume the data is linearly separa-
ble, and the data was mostly likely not linearly separable.
The differences in the performance of these models can be
attributed to the tradeoffs made in the algorithms. For example,
SVMs are primarily inﬂuenced by the data points closest to
the margin, while logistic regression and other models are
inﬂuenced by all data points. SVMs also perform better in
problems with a high number of dimensions.

We chose our hotttnesss threshold to consider the top 25%
of songs to be popular (songs with a hotttnesss larger than
0.623). We initially chose a threshold that considered the
top 50% of songs to be popular and were receiving fairly
low metrics across the board,
including precision, recall,
F1 score, and accuracy. We hypothesize that this happened
because popular songs (which we predict to have deﬁning
characteristics) in reality are only a small subset of all songs.
Therefore, the top percentile of popular songs that had these
deﬁning characteristics were confounded by the more songs
we labeled as popular. As a result we changed our threshold
to classify the top 5% as popular. This increased the accuracy,
but it was the result of our algorithms predicting that all songs
were not popular, a fact that was reﬂected in recalls of 0.
A likely cause of this result is a lack of popular training
examples. By using the top 25% of songs as popular, we are
able to have songs with deﬁning characteristics in addition to
enough popular examples to properly train our models.

C. Regression

1) Metrics: In the regression setting, we use mean squared
error (MSE) to evaluate how well our model predicts pop-
ularity. The average error is obtained by taking the square
root of the MSE, which approximates the standard error of

Fig. 4. ROC Curve for SVM with Gaussian Kernel

Model
SVM (Linear Kernel)
SVM (RBF Kernel)
Logistic Regression (LR)
LDA
QDA
Multilayer Perceptron (MLP)

AUC
0.79
0.81
0.69
0.71
0.64
0.79

Fig. 5. AUC for Classiﬁcation Models

Model
SVM (Linear)
SVM (RBF)
Logistic Reg
LDA
QDA
MLP

Train
0.830
0.827
0.861
0.858
0.756
0.889
Fig. 6. Classiﬁcation Results: {train, test} accuracy

F1
0.585
0.597
0.531
0.567
0.454
0.504

R

0.706
0.750
0.500
0.529
0.618
0.441

P

0.500
0.495
0.567
0.610
0.359
0.588

Test
0.762
0.759
0.790
0.807
0.647
0.793

B. Classiﬁcation

1) Metrics: Since we set our threshold such that 75% of
the songs in our dataset are classiﬁed as not popular and
25% are classiﬁed as popular, 75% accuracy can be achieved
by predicting all 0s. Therefore, we concluded that accuracy
alone would not be a good measure of how well our model
can classify. In order to capture this, we also considered the
precision, recall, F1, and AUC scores of our models.

Precision, recall, and F1 score are used to capture how well
our model does in the task of classiﬁcation. Precision measures
the portion of examples that were classiﬁed as popular that are
truly popular while recall measures the portion of examples
that are truly popular that our model classiﬁed as popular. F1
score acts as the weighted average between these two values.
The area under the receiver operator characteristic curve
is a metric used to evaluate the performance of a binary
classiﬁer by taking the area under a curve created by plotting
TPR vs. FPR at different probability thresholds. The AUC
represents the probability that the classiﬁer ranks a random
positive example higher than a random negative one.

2) Tuning Parameters: With respect to the kernels used
by our SVM models, we optimized two parameters: C and
γ. C determines the trade-offs between misclassiﬁcation and

Fig. 7. Cross-validation for optimal λ for Lasso

Fig. 9. Features names scaled by the log of coefﬁcients for linear regression

Model
Baseline
Full Model (n = 976)
Selected Model (n = 45)
Lasso (λ = 0.00238)

MSE
0.02529
0.03010
0.01842
0.01802
Fig. 8. Regression Results

Avg Error

0.1590
0.1735
0.1357
0.1342

our predictions. Therefore, the smaller the MSE, the higher
conﬁdence we have about our predictions.

The λ parameter

2) Tuning Parameters:

l1-
regularization was
tuned using 10-fold cross-validation.
In addition, the number of features to use for determining
forward and backward stepwise feature selection was obtained
using 10-fold cross-validation.

for

3) Results: See Fig. 8.
4) Discussion: According to the results in Fig. 8, seeing
that
the test error increases from the baseline model (58
features) to the full model (976 features), we can conclude
that adding a lot of additional Bag of Words features leads
to overﬁtting to noise in our training set. However, by signif-
icantly reducing the number of features to 45 using forward
feature selection, we were able to signiﬁcantly decrease the
variance of our ﬁt and consequently reduce overﬁtting.

Overall,

the smallest

test error among our models is
achieved by using Lasso regression. 10-fold cross-validation
is used to choose the optimal value of λ, which turns out to
be 0.00238, over a grid of log λ values, (see Fig. 7). Lasso
regression gave an output of 21 different features with non-
zero coefﬁcient estimates. The features chosen by the Lasso
are very similar to the ones produced by forward and backward
feature selection.

Our feature vector is generally very sparse due to the large
number of BoW binary features, so stepwise selection’s greedy
approach may not work as well as Lasso’s shrinkage method.

VI. CONCLUSION

Through several different feature selection algorithms, we
were able to identify the most
inﬂuential features in our
dataset by taking the intersection among the feature selection
algorithms, namely artist familiarity, loudness, year, and a
number of genre tags. All of the features can be seen in Fig. 9.
We found that the acoustic features arent nearly as predicative

as the metadata features. A likely reason for this is that there
is a lot of variation in acoustic features within a single song
that make it difﬁcult to extract metrics that represent an entire
song. Metadata such as genre tags or year of release are much
better at accurately reﬂecting a trait of a song.

Currently, the features we used divided the range of pitch
and timbre into buckets. However, because the original data
consists of time series data points with respect
to these
values, we could add features that represent the transition
between certain pitch values. An n-gram type model using
sequences of pitches/loudness as features would allow us to
investigate if particular pitch intervals have any inﬂuence on
song popularity.

In addition to using the song hotttnesss metric, we can also
create our own metric of popularity, which we can deﬁne as
the number of downloads on iTunes or the number of plays
on Spotify. This would allow us to more accurately capture
what we deﬁne to be popular and allow us to generalize our
ﬁndings to commonly understood metrics. Furthermore, we
can potentially extend the application of this project to a song
recommendation system: training a model on examples with
the most number of plays labeled as popular could lead to
personal playlist or song recommendations.

REFERENCES

[1] Bertin-Mahieux, Thierry, et al. ”Autotagger: A model for predicting social
tags from acoustic features on large music databases.” Journal of New
Music Research 37.2 (2008): 115-135.

[2] Hastie, Trevor, and Hui Zou. ”Regularization and variable selection via

the elastic net”. J.R. Statist. Soc. B, 2005.

[3] James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An

Introduction to Statistical Learning. Springer Texts, 2014.

[4] Koenigstein, Noam, Yuval Shavitt, and Noa Zilberman. ”Predicting bill-
board success using data-mining in p2p networks.” Multimedia, 2009.
ISM’09. 11th IEEE International Symposium on. IEEE, 2009.

[5] Ni, Yizhao, et al. ”Hit song science once again a science.” 4th Interna-

tional Workshop on Machine Learning and Music, Spain. 2011.

[6] Pachet, Franois, and Pierre Roy. ”Hit Song Science Is Not Yet a Science.”

ISMIR. 2008.

[7] Salganik, Matthew J., Peter Sheridan Dodds, and Duncan J. Watts.
”Experimental study of inequality and unpredictability in an artiﬁcial
cultural market.” science 311.5762 (2006): 854-856.

[8] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul
Lamere. The Million Song Dataset. In Proceedings of the 12th Interna-
tional Society for Music Information Retrieval Conference (ISMIR 2011),
2011.

