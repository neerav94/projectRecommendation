Biomarker Identiﬁcation for Early-stage
Diabetes Diagnosis in Mice Liver Cells

Andrea Agazzi(1), Vincent Deo(2)

(1) Theoretical Physics Department, Université de Genève, Switzerland
(2) Electrical Engineering Department, Stanford University, CA, U.S.A.

agazzian, vdeo@stanford.edu

December 11th, 2015

ABSTRACT

This projects investigates automated diagnosis possibilities from metabolite micro-array measurements in liver
cells, at an incipient stage of the disease. L1-regularized logistic regression, Fisher Discriminant Analysis and Ran-
dom Forests classiﬁers are studied, yielding promising results and hindsight on biomarker signiﬁcance.

1. INTRODUCTION

Diabetes Mellitus is one of today’s most common chronic diseases, affecting approximately 382M of people world-
wide in 2013 [1]. The disease, resulting in serious cardiovascular complications, is divided into two categories.
Type 2 diabetes, accounting for approximately 90% of the cases, is a gradually developing form of the disease, and
can be prevented if diagnosed at an early stage. Motivated by the inherently metabolic character of the disease, the
Maechler Lab at the University Hospital of Geneva is investigating a novel form of early stage diagnosis, searching
for quantitative biological markers in a patient’s liver cells. The liver is responsible for carbohydrate metabolism
and is therefore expected to be affected ﬁrst by incipient diabetes.

This project aims to analyze the data conveyed from the Maechler Lab and tackle the issues of early-stage di-
agnosis and biomarker identiﬁcation. Speciﬁcally, the algorithm input is the result of micro-array experiments
of metabolite concentrations in both sick and healthy mice liver cells. We apply different combinations of pre-
processing and supervised classiﬁcation algorithms, for the purpose of automated diagnosis of new data points.
Furthermore, we aim to extract the most signiﬁcant features indicating the presence of the disease through inspec-
tion of the best performing classiﬁers.

2. RELATED WORK

Micro-array and metabolite proﬁling experiments have been an object of interest in the machine learning liter-
ature ever since they appeared in cell biology. The data is high-dimensional, and therefore heavily regularized
approaches often become the methods of choice for these problems [2]. For feature extraction and preprocess-
ing of micro-array data, the most recurring algorithms in the literature are PCA [3], information gain, correlation
analysis, and False Discovery Rate thresholding [4].

For the classiﬁcation, widely applied methods include Support Vector Machines (SVM) with different kernels,
Fisher Discriminant Analysis (FDA), Logistic Regression (LogReg), k-Nearest-Neighbors classiﬁers, Decision Trees
and Random Forests [5]. In most cases, these can be complemented with a regularization procedure [2], encour-
aging model sparsity and providing helpful preprocessing to high dimensional and correlated data.

Given the wide range of high dimensional problems in the computational biology and high-throughput data
analysis framework, there is no unique state-of-the-art pipeline for feature selection and data classiﬁcation clearly
outperforming others [5]. Therefore, the approach of this project will be to test a wide range of combinations of
standard approaches and select the best ones for a more thorough analysis.

1

Biomarker Identiﬁcation for Early-stage
Diabetes Diagnosis in Mice Liver Cells

Andrea Agazzi(1), Vincent Deo(2)

(1) Theoretical Physics Department, Université de Genève, Switzerland
(2) Electrical Engineering Department, Stanford University, CA, U.S.A.

agazzian, vdeo@stanford.edu

December 11th, 2015

ABSTRACT

This projects investigates automated diagnosis possibilities from metabolite micro-array measurements in liver
cells, at an incipient stage of the disease. L1-regularized logistic regression, Fisher Discriminant Analysis and Ran-
dom Forests classiﬁers are studied, yielding promising results and hindsight on biomarker signiﬁcance.

1. INTRODUCTION

Diabetes Mellitus is one of today’s most common chronic diseases, affecting approximately 382M of people world-
wide in 2013 [1]. The disease, resulting in serious cardiovascular complications, is divided into two categories.
Type 2 diabetes, accounting for approximately 90% of the cases, is a gradually developing form of the disease, and
can be prevented if diagnosed at an early stage. Motivated by the inherently metabolic character of the disease, the
Maechler Lab at the University Hospital of Geneva is investigating a novel form of early stage diagnosis, searching
for quantitative biological markers in a patient’s liver cells. The liver is responsible for carbohydrate metabolism
and is therefore expected to be affected ﬁrst by incipient diabetes.

This project aims to analyze the data conveyed from the Maechler Lab and tackle the issues of early-stage di-
agnosis and biomarker identiﬁcation. Speciﬁcally, the algorithm input is the result of micro-array experiments
of metabolite concentrations in both sick and healthy mice liver cells. We apply different combinations of pre-
processing and supervised classiﬁcation algorithms, for the purpose of automated diagnosis of new data points.
Furthermore, we aim to extract the most signiﬁcant features indicating the presence of the disease through inspec-
tion of the best performing classiﬁers.

2. RELATED WORK

Micro-array and metabolite proﬁling experiments have been an object of interest in the machine learning liter-
ature ever since they appeared in cell biology. The data is high-dimensional, and therefore heavily regularized
approaches often become the methods of choice for these problems [2]. For feature extraction and preprocess-
ing of micro-array data, the most recurring algorithms in the literature are PCA [3], information gain, correlation
analysis, and False Discovery Rate thresholding [4].

For the classiﬁcation, widely applied methods include Support Vector Machines (SVM) with different kernels,
Fisher Discriminant Analysis (FDA), Logistic Regression (LogReg), k-Nearest-Neighbors classiﬁers, Decision Trees
and Random Forests [5]. In most cases, these can be complemented with a regularization procedure [2], encour-
aging model sparsity and providing helpful preprocessing to high dimensional and correlated data.

Given the wide range of high dimensional problems in the computational biology and high-throughput data
analysis framework, there is no unique state-of-the-art pipeline for feature selection and data classiﬁcation clearly
outperforming others [5]. Therefore, the approach of this project will be to test a wide range of combinations of
standard approaches and select the best ones for a more thorough analysis.

1

3. DATASET DESCRIPTION

The database this project uses was jointly created by the Maechler Lab at UNIGE and the Zamboni Lab at ETHZ,
and was transmitted to the authors under a conﬁdentiality agreement. A colony of 56 mice was divided into two
subgroups of 24 and 32 elements. The ﬁrst group mice were genetically modiﬁed (knockout, KO) to induce dia-
betes, and the second group mice were kept as control (CTRL). Out of each group, samples of 6 to 8 mice were
taken (without replacement) at weeks 4, 5, 6 and 10 of life (Table 1). For each selected mice, 2 samples of liver cells
are processed through mass spectrometry [6] and the concentrations of metabolites are estimated (not uniquely
but up to mass equivalence), for a total of 756 features per sample.

Type

CTRL
KO

Week 4

Week 5

Week 6

Week 10

(post-symptoms)

16
14

14
10

18
12

16
12

Total

64
48

Table 1: Dataset distribution, week of sampling and sick (KO) or healthy (CTRL) labels.

Our training set is the array of spectrometric measurements (x(i ))1≤i≤112 ∈ (cid:82)756, with labels giving the week
of sampling w (i ) ∈ {4,5,6,10} and the knockout state of the mice y (i ) ∈ {0,1} for each of the p = 112 sampled liver
samples. We note X the design matrix and Y the binary label vector.

Since the number of features is larger than the number of samples, data preprocessing is an important step to

avoid critical overﬁts. Dimensionality reduction algorithms were applied prior to data classiﬁcation:
PCA We compute the most signiﬁcant n eigenvectors of the empirical covariance matrix C = X T X , where n is
selected by cross validation (Figure 1, left), and project the data in the span of these principal component vectors
e1,...,en: XPC = X × [e1 ... en].

CORRELATION THRESHOLDING We estimate the Pearson Correlation Coefﬁcient ρ(Y , X·,j ) = (cid:100)Cov(Y , X·,j )

. The n

ˆσY ˆσX·,j ,

features with highest absolute value of the correlation coefﬁcient are selected, with n chosen by cross-validation.
This method has been preferred over Mutual Information Thresholding, for the predictors being real-valued.

L1 REGULARIZED LOGISTIC REGRESSION This classiﬁer is applied, and the induced model parameter sparsity [7] is
used as feature selector for a further classiﬁcation by another algorithm.

4. METHODS

METHODOLOGY The dataset was used to train different supervised classiﬁcation algorithms, whose quality was
quantiﬁed through their test scores in either leave-20-out or .632-bootstrap cross-validations. The best performing
classiﬁcation algorithms were used thereafter for quantitative and qualitative biomarker signiﬁcance assessment.

FISHER DISCRIMINANT ANALYSIS A Fisher Linear Discriminant (FDA) [8] was implemented hands-on as a starting
point to assess the potential of the dataset, and then compared to the built-in matlab LDA and SVM with default
conﬁgurations. The data is preprocessed using PCA (section 3), after which the ﬁrst Fisher vector, normal the
maximally separating hyperplane, is computed. The offset of the decision boundary between the two classes may
be selected a posteriori, in order to adjust the false positive-false negative ratio to meet requirements.

SUPPORT VECTOR MACHINES classiﬁers were attempted with a variety of kernels over PCA-preprocessed data.
The linear kernel yields preliminary results of the order of the FDA, although less stable over extensive test runs.
Polynomial, Gaussian and Sigmoid kernels did not seem adapted to the dataset, leading to less satisfactory results.

2

Biomarker Identiﬁcation for Early-stage
Diabetes Diagnosis in Mice Liver Cells

Andrea Agazzi(1), Vincent Deo(2)

(1) Theoretical Physics Department, Université de Genève, Switzerland
(2) Electrical Engineering Department, Stanford University, CA, U.S.A.

agazzian, vdeo@stanford.edu

December 11th, 2015

ABSTRACT

This projects investigates automated diagnosis possibilities from metabolite micro-array measurements in liver
cells, at an incipient stage of the disease. L1-regularized logistic regression, Fisher Discriminant Analysis and Ran-
dom Forests classiﬁers are studied, yielding promising results and hindsight on biomarker signiﬁcance.

1. INTRODUCTION

Diabetes Mellitus is one of today’s most common chronic diseases, affecting approximately 382M of people world-
wide in 2013 [1]. The disease, resulting in serious cardiovascular complications, is divided into two categories.
Type 2 diabetes, accounting for approximately 90% of the cases, is a gradually developing form of the disease, and
can be prevented if diagnosed at an early stage. Motivated by the inherently metabolic character of the disease, the
Maechler Lab at the University Hospital of Geneva is investigating a novel form of early stage diagnosis, searching
for quantitative biological markers in a patient’s liver cells. The liver is responsible for carbohydrate metabolism
and is therefore expected to be affected ﬁrst by incipient diabetes.

This project aims to analyze the data conveyed from the Maechler Lab and tackle the issues of early-stage di-
agnosis and biomarker identiﬁcation. Speciﬁcally, the algorithm input is the result of micro-array experiments
of metabolite concentrations in both sick and healthy mice liver cells. We apply different combinations of pre-
processing and supervised classiﬁcation algorithms, for the purpose of automated diagnosis of new data points.
Furthermore, we aim to extract the most signiﬁcant features indicating the presence of the disease through inspec-
tion of the best performing classiﬁers.

2. RELATED WORK

Micro-array and metabolite proﬁling experiments have been an object of interest in the machine learning liter-
ature ever since they appeared in cell biology. The data is high-dimensional, and therefore heavily regularized
approaches often become the methods of choice for these problems [2]. For feature extraction and preprocess-
ing of micro-array data, the most recurring algorithms in the literature are PCA [3], information gain, correlation
analysis, and False Discovery Rate thresholding [4].

For the classiﬁcation, widely applied methods include Support Vector Machines (SVM) with different kernels,
Fisher Discriminant Analysis (FDA), Logistic Regression (LogReg), k-Nearest-Neighbors classiﬁers, Decision Trees
and Random Forests [5]. In most cases, these can be complemented with a regularization procedure [2], encour-
aging model sparsity and providing helpful preprocessing to high dimensional and correlated data.

Given the wide range of high dimensional problems in the computational biology and high-throughput data
analysis framework, there is no unique state-of-the-art pipeline for feature selection and data classiﬁcation clearly
outperforming others [5]. Therefore, the approach of this project will be to test a wide range of combinations of
standard approaches and select the best ones for a more thorough analysis.

1

3. DATASET DESCRIPTION

The database this project uses was jointly created by the Maechler Lab at UNIGE and the Zamboni Lab at ETHZ,
and was transmitted to the authors under a conﬁdentiality agreement. A colony of 56 mice was divided into two
subgroups of 24 and 32 elements. The ﬁrst group mice were genetically modiﬁed (knockout, KO) to induce dia-
betes, and the second group mice were kept as control (CTRL). Out of each group, samples of 6 to 8 mice were
taken (without replacement) at weeks 4, 5, 6 and 10 of life (Table 1). For each selected mice, 2 samples of liver cells
are processed through mass spectrometry [6] and the concentrations of metabolites are estimated (not uniquely
but up to mass equivalence), for a total of 756 features per sample.

Type

CTRL
KO

Week 4

Week 5

Week 6

Week 10

(post-symptoms)

16
14

14
10

18
12

16
12

Total

64
48

Table 1: Dataset distribution, week of sampling and sick (KO) or healthy (CTRL) labels.

Our training set is the array of spectrometric measurements (x(i ))1≤i≤112 ∈ (cid:82)756, with labels giving the week
of sampling w (i ) ∈ {4,5,6,10} and the knockout state of the mice y (i ) ∈ {0,1} for each of the p = 112 sampled liver
samples. We note X the design matrix and Y the binary label vector.

Since the number of features is larger than the number of samples, data preprocessing is an important step to

avoid critical overﬁts. Dimensionality reduction algorithms were applied prior to data classiﬁcation:
PCA We compute the most signiﬁcant n eigenvectors of the empirical covariance matrix C = X T X , where n is
selected by cross validation (Figure 1, left), and project the data in the span of these principal component vectors
e1,...,en: XPC = X × [e1 ... en].

CORRELATION THRESHOLDING We estimate the Pearson Correlation Coefﬁcient ρ(Y , X·,j ) = (cid:100)Cov(Y , X·,j )

. The n

ˆσY ˆσX·,j ,

features with highest absolute value of the correlation coefﬁcient are selected, with n chosen by cross-validation.
This method has been preferred over Mutual Information Thresholding, for the predictors being real-valued.

L1 REGULARIZED LOGISTIC REGRESSION This classiﬁer is applied, and the induced model parameter sparsity [7] is
used as feature selector for a further classiﬁcation by another algorithm.

4. METHODS

METHODOLOGY The dataset was used to train different supervised classiﬁcation algorithms, whose quality was
quantiﬁed through their test scores in either leave-20-out or .632-bootstrap cross-validations. The best performing
classiﬁcation algorithms were used thereafter for quantitative and qualitative biomarker signiﬁcance assessment.

FISHER DISCRIMINANT ANALYSIS A Fisher Linear Discriminant (FDA) [8] was implemented hands-on as a starting
point to assess the potential of the dataset, and then compared to the built-in matlab LDA and SVM with default
conﬁgurations. The data is preprocessed using PCA (section 3), after which the ﬁrst Fisher vector, normal the
maximally separating hyperplane, is computed. The offset of the decision boundary between the two classes may
be selected a posteriori, in order to adjust the false positive-false negative ratio to meet requirements.

SUPPORT VECTOR MACHINES classiﬁers were attempted with a variety of kernels over PCA-preprocessed data.
The linear kernel yields preliminary results of the order of the FDA, although less stable over extensive test runs.
Polynomial, Gaussian and Sigmoid kernels did not seem adapted to the dataset, leading to less satisfactory results.

2

REGULARIZED LOGISTIC REGRESSION Logistic regression -the generalized linear model with conditional proba-
bility p(y|x) Bernoulli-distributed- is a natural choice for a 2-class linear classiﬁer. The high dimensionality of the
data at hand suggests the application of regularization procedures. This is equivalent to setting a Bayesian prior
pα(θ) on the parameters of the model, and results in a modiﬁed log likelihood function:

l(θ, λ) =(cid:88)

log p(y (i )|x(i ), θ)+ λlog p(θ) ,

i

where the regularization parameter λ is chosen by cross-validation. Most popular choices of prior distribution
are Exp(α) and N (0, α2I ), resulting respectively in so-called L1 and L2 regularizations. L1 regularization is attrac-
tive in this framework as it induces sparsity in the model parameters [7]. A weighted choice of L1 and L2, called
Elastic Net [9], has also been implemented. This learning algorithm was implemented using the python library
scikitlearn [10].

RANDOM FORESTS Random forest (RF) classiﬁer [11] is an ensemble learning approach based on decision trees.
The latter sequentially divide the training data into subsets on single-decision rules that maximize class separation.
As for Tree Bagging, RF classiﬁers bootstrap the training set and construct a decision tree for every bootstrapped
(cid:112)
sample, reducing the high variance of the decision tree classiﬁer approach. RF differentiates form Tree Bagging by
performing each split over a randomly chosen subset of features, of typical size
#features. This randomized boot-
strapping prevents correlated trees in the forest and improves accuracy. Interestingly, RF are invariant to rescaling
of data, but tend to extract only a small subset of discriminative variables when features are highly correlated. This
learning algorithm was implemented using the python library scikitlearn [10].

5. RESULTS AND DISCUSSION

PARAMETER SELECTION For feature extraction and regularized classiﬁcations, some hyperparameters must be
picked to optimize the expected score of the algorithm. This is done by maximizing the CV-score over the full
range of possible parameter combinations on the training set with a repeated leave-20-out procedure, which has
been chosen because of the relatively small dataset size. This includes the regularization parameter for L1-LogReg
and the dimension of the PCA subspace for FDA. The resulting validation curves are displayed in Figure 1.

GENERAL RESULTS The scores for some of the preprocessor-classiﬁer algorithms that have been applied in this
analysis are reported in table 2. Out of the considered methods, those with best performance were studied in
more detail, with a preference for methods allowing a direct ranking of discriminative features in order to facilitate
biomarker extraction.

Method
LDA
Linear SVM
Quadratic SVM
L1-penalized Logistic Reg.
L2-penalized Logistic Reg.
ElNet-penalized Linear Reg.
Random Forests

Preproc.

PCA
PCA
PCA

CorrThresh
CorrThresh
CorrThresh
L1-LogReg

CV score
97.4%
96.1%
92.7%
97.1%
96.1%
83.1%
93.0%

Sparsity

0%
0%
0%

90.6%
80.4%
54.4%
86.4 %

Table 2: Leave-20-out cross-validation scores for preprocessor-classiﬁer pairs tried on the dataset.

MODEL EVALUATION The learning procedures listed in Section 4 were evaluated with different cross validation
procedures: 0.632-bootstrap, 10-fold, leave-one-out and leave-20-out cross-validations. Detailed results are shown

3

Biomarker Identiﬁcation for Early-stage
Diabetes Diagnosis in Mice Liver Cells

Andrea Agazzi(1), Vincent Deo(2)

(1) Theoretical Physics Department, Université de Genève, Switzerland
(2) Electrical Engineering Department, Stanford University, CA, U.S.A.

agazzian, vdeo@stanford.edu

December 11th, 2015

ABSTRACT

This projects investigates automated diagnosis possibilities from metabolite micro-array measurements in liver
cells, at an incipient stage of the disease. L1-regularized logistic regression, Fisher Discriminant Analysis and Ran-
dom Forests classiﬁers are studied, yielding promising results and hindsight on biomarker signiﬁcance.

1. INTRODUCTION

Diabetes Mellitus is one of today’s most common chronic diseases, affecting approximately 382M of people world-
wide in 2013 [1]. The disease, resulting in serious cardiovascular complications, is divided into two categories.
Type 2 diabetes, accounting for approximately 90% of the cases, is a gradually developing form of the disease, and
can be prevented if diagnosed at an early stage. Motivated by the inherently metabolic character of the disease, the
Maechler Lab at the University Hospital of Geneva is investigating a novel form of early stage diagnosis, searching
for quantitative biological markers in a patient’s liver cells. The liver is responsible for carbohydrate metabolism
and is therefore expected to be affected ﬁrst by incipient diabetes.

This project aims to analyze the data conveyed from the Maechler Lab and tackle the issues of early-stage di-
agnosis and biomarker identiﬁcation. Speciﬁcally, the algorithm input is the result of micro-array experiments
of metabolite concentrations in both sick and healthy mice liver cells. We apply different combinations of pre-
processing and supervised classiﬁcation algorithms, for the purpose of automated diagnosis of new data points.
Furthermore, we aim to extract the most signiﬁcant features indicating the presence of the disease through inspec-
tion of the best performing classiﬁers.

2. RELATED WORK

Micro-array and metabolite proﬁling experiments have been an object of interest in the machine learning liter-
ature ever since they appeared in cell biology. The data is high-dimensional, and therefore heavily regularized
approaches often become the methods of choice for these problems [2]. For feature extraction and preprocess-
ing of micro-array data, the most recurring algorithms in the literature are PCA [3], information gain, correlation
analysis, and False Discovery Rate thresholding [4].

For the classiﬁcation, widely applied methods include Support Vector Machines (SVM) with different kernels,
Fisher Discriminant Analysis (FDA), Logistic Regression (LogReg), k-Nearest-Neighbors classiﬁers, Decision Trees
and Random Forests [5]. In most cases, these can be complemented with a regularization procedure [2], encour-
aging model sparsity and providing helpful preprocessing to high dimensional and correlated data.

Given the wide range of high dimensional problems in the computational biology and high-throughput data
analysis framework, there is no unique state-of-the-art pipeline for feature selection and data classiﬁcation clearly
outperforming others [5]. Therefore, the approach of this project will be to test a wide range of combinations of
standard approaches and select the best ones for a more thorough analysis.

1

3. DATASET DESCRIPTION

The database this project uses was jointly created by the Maechler Lab at UNIGE and the Zamboni Lab at ETHZ,
and was transmitted to the authors under a conﬁdentiality agreement. A colony of 56 mice was divided into two
subgroups of 24 and 32 elements. The ﬁrst group mice were genetically modiﬁed (knockout, KO) to induce dia-
betes, and the second group mice were kept as control (CTRL). Out of each group, samples of 6 to 8 mice were
taken (without replacement) at weeks 4, 5, 6 and 10 of life (Table 1). For each selected mice, 2 samples of liver cells
are processed through mass spectrometry [6] and the concentrations of metabolites are estimated (not uniquely
but up to mass equivalence), for a total of 756 features per sample.

Type

CTRL
KO

Week 4

Week 5

Week 6

Week 10

(post-symptoms)

16
14

14
10

18
12

16
12

Total

64
48

Table 1: Dataset distribution, week of sampling and sick (KO) or healthy (CTRL) labels.

Our training set is the array of spectrometric measurements (x(i ))1≤i≤112 ∈ (cid:82)756, with labels giving the week
of sampling w (i ) ∈ {4,5,6,10} and the knockout state of the mice y (i ) ∈ {0,1} for each of the p = 112 sampled liver
samples. We note X the design matrix and Y the binary label vector.

Since the number of features is larger than the number of samples, data preprocessing is an important step to

avoid critical overﬁts. Dimensionality reduction algorithms were applied prior to data classiﬁcation:
PCA We compute the most signiﬁcant n eigenvectors of the empirical covariance matrix C = X T X , where n is
selected by cross validation (Figure 1, left), and project the data in the span of these principal component vectors
e1,...,en: XPC = X × [e1 ... en].

CORRELATION THRESHOLDING We estimate the Pearson Correlation Coefﬁcient ρ(Y , X·,j ) = (cid:100)Cov(Y , X·,j )

. The n

ˆσY ˆσX·,j ,

features with highest absolute value of the correlation coefﬁcient are selected, with n chosen by cross-validation.
This method has been preferred over Mutual Information Thresholding, for the predictors being real-valued.

L1 REGULARIZED LOGISTIC REGRESSION This classiﬁer is applied, and the induced model parameter sparsity [7] is
used as feature selector for a further classiﬁcation by another algorithm.

4. METHODS

METHODOLOGY The dataset was used to train different supervised classiﬁcation algorithms, whose quality was
quantiﬁed through their test scores in either leave-20-out or .632-bootstrap cross-validations. The best performing
classiﬁcation algorithms were used thereafter for quantitative and qualitative biomarker signiﬁcance assessment.

FISHER DISCRIMINANT ANALYSIS A Fisher Linear Discriminant (FDA) [8] was implemented hands-on as a starting
point to assess the potential of the dataset, and then compared to the built-in matlab LDA and SVM with default
conﬁgurations. The data is preprocessed using PCA (section 3), after which the ﬁrst Fisher vector, normal the
maximally separating hyperplane, is computed. The offset of the decision boundary between the two classes may
be selected a posteriori, in order to adjust the false positive-false negative ratio to meet requirements.

SUPPORT VECTOR MACHINES classiﬁers were attempted with a variety of kernels over PCA-preprocessed data.
The linear kernel yields preliminary results of the order of the FDA, although less stable over extensive test runs.
Polynomial, Gaussian and Sigmoid kernels did not seem adapted to the dataset, leading to less satisfactory results.

2

REGULARIZED LOGISTIC REGRESSION Logistic regression -the generalized linear model with conditional proba-
bility p(y|x) Bernoulli-distributed- is a natural choice for a 2-class linear classiﬁer. The high dimensionality of the
data at hand suggests the application of regularization procedures. This is equivalent to setting a Bayesian prior
pα(θ) on the parameters of the model, and results in a modiﬁed log likelihood function:

l(θ, λ) =(cid:88)

log p(y (i )|x(i ), θ)+ λlog p(θ) ,

i

where the regularization parameter λ is chosen by cross-validation. Most popular choices of prior distribution
are Exp(α) and N (0, α2I ), resulting respectively in so-called L1 and L2 regularizations. L1 regularization is attrac-
tive in this framework as it induces sparsity in the model parameters [7]. A weighted choice of L1 and L2, called
Elastic Net [9], has also been implemented. This learning algorithm was implemented using the python library
scikitlearn [10].

RANDOM FORESTS Random forest (RF) classiﬁer [11] is an ensemble learning approach based on decision trees.
The latter sequentially divide the training data into subsets on single-decision rules that maximize class separation.
As for Tree Bagging, RF classiﬁers bootstrap the training set and construct a decision tree for every bootstrapped
(cid:112)
sample, reducing the high variance of the decision tree classiﬁer approach. RF differentiates form Tree Bagging by
performing each split over a randomly chosen subset of features, of typical size
#features. This randomized boot-
strapping prevents correlated trees in the forest and improves accuracy. Interestingly, RF are invariant to rescaling
of data, but tend to extract only a small subset of discriminative variables when features are highly correlated. This
learning algorithm was implemented using the python library scikitlearn [10].

5. RESULTS AND DISCUSSION

PARAMETER SELECTION For feature extraction and regularized classiﬁcations, some hyperparameters must be
picked to optimize the expected score of the algorithm. This is done by maximizing the CV-score over the full
range of possible parameter combinations on the training set with a repeated leave-20-out procedure, which has
been chosen because of the relatively small dataset size. This includes the regularization parameter for L1-LogReg
and the dimension of the PCA subspace for FDA. The resulting validation curves are displayed in Figure 1.

GENERAL RESULTS The scores for some of the preprocessor-classiﬁer algorithms that have been applied in this
analysis are reported in table 2. Out of the considered methods, those with best performance were studied in
more detail, with a preference for methods allowing a direct ranking of discriminative features in order to facilitate
biomarker extraction.

Method
LDA
Linear SVM
Quadratic SVM
L1-penalized Logistic Reg.
L2-penalized Logistic Reg.
ElNet-penalized Linear Reg.
Random Forests

Preproc.

PCA
PCA
PCA

CorrThresh
CorrThresh
CorrThresh
L1-LogReg

CV score
97.4%
96.1%
92.7%
97.1%
96.1%
83.1%
93.0%

Sparsity

0%
0%
0%

90.6%
80.4%
54.4%
86.4 %

Table 2: Leave-20-out cross-validation scores for preprocessor-classiﬁer pairs tried on the dataset.

MODEL EVALUATION The learning procedures listed in Section 4 were evaluated with different cross validation
procedures: 0.632-bootstrap, 10-fold, leave-one-out and leave-20-out cross-validations. Detailed results are shown

3

Figure 1: Left: Cross-validation of the number of PC dimensions in the FDA algorithm. The training score
ceils at 1, whereas the testing score drops to 0.5 when dimensionality is higher than (number_of_points
- 1) for separability reasons (red vertical line). Right: Cross-validation of the regularization parameter
C in the L1-LogReg preprocesser/RandomForest classiﬁer algorithm. Colored areas correspond to the
standard deviation on the errors. The black solid line represents the sparsity of model parameters as
a function of the regularization parameter. Both cross-validations are realized in a repeated 300 times

leave-20-out fashion.

for PCA preprocessing with FDA classiﬁer in table 3, and for the two other algorithms in table 4. Cross-validations
have been implemented manually in python and Matlab.

Total tests

6000

Predicted

CTRL

Predicted

KO

Accuracy
97.19%

Actual
CTRL

3342

79

Actual

KO

88

2491

Prevalence

57.04%

Positive predictive

False discovery

value: 97.41%
False omission

rate: 3.10%

rate: 2.59%

Negative Predictive

value: 96.90%

True positive
rate: 97.67%
False negative

rate: 2.33%

False positive
rate: 3.44%
True negative
rate: 96.56%

Positive likelihood

ratio: 28.39

Negative likelihood

ratio: 0.024

Diagnostic odds

ratio: 1,176

Table 3: Detailed confusion matrix for the Linear Discriminant classiﬁer

Algorithm

True Positive

False Positive

True Negative

False Negative

Accuracy

L1-reg Logistic Regression +

Random Forest

Correlation thresholding +
L1-reg Logistic Regression

3346

3363

359

97

60

75

2235

2467

93.02%

97.14%

Table 4: Reduced confusion matrix for the L1-LogReg and Random Forest Classiﬁers

4

Principal Component Dimension n102030405060708090100110120Score0.40.50.60.70.80.911.1Cross validation of dimensionality in PCA preprocessingTesting ScoreTraining ScorefontsizeBiomarker Identiﬁcation for Early-stage
Diabetes Diagnosis in Mice Liver Cells

Andrea Agazzi(1), Vincent Deo(2)

(1) Theoretical Physics Department, Université de Genève, Switzerland
(2) Electrical Engineering Department, Stanford University, CA, U.S.A.

agazzian, vdeo@stanford.edu

December 11th, 2015

ABSTRACT

This projects investigates automated diagnosis possibilities from metabolite micro-array measurements in liver
cells, at an incipient stage of the disease. L1-regularized logistic regression, Fisher Discriminant Analysis and Ran-
dom Forests classiﬁers are studied, yielding promising results and hindsight on biomarker signiﬁcance.

1. INTRODUCTION

Diabetes Mellitus is one of today’s most common chronic diseases, affecting approximately 382M of people world-
wide in 2013 [1]. The disease, resulting in serious cardiovascular complications, is divided into two categories.
Type 2 diabetes, accounting for approximately 90% of the cases, is a gradually developing form of the disease, and
can be prevented if diagnosed at an early stage. Motivated by the inherently metabolic character of the disease, the
Maechler Lab at the University Hospital of Geneva is investigating a novel form of early stage diagnosis, searching
for quantitative biological markers in a patient’s liver cells. The liver is responsible for carbohydrate metabolism
and is therefore expected to be affected ﬁrst by incipient diabetes.

This project aims to analyze the data conveyed from the Maechler Lab and tackle the issues of early-stage di-
agnosis and biomarker identiﬁcation. Speciﬁcally, the algorithm input is the result of micro-array experiments
of metabolite concentrations in both sick and healthy mice liver cells. We apply different combinations of pre-
processing and supervised classiﬁcation algorithms, for the purpose of automated diagnosis of new data points.
Furthermore, we aim to extract the most signiﬁcant features indicating the presence of the disease through inspec-
tion of the best performing classiﬁers.

2. RELATED WORK

Micro-array and metabolite proﬁling experiments have been an object of interest in the machine learning liter-
ature ever since they appeared in cell biology. The data is high-dimensional, and therefore heavily regularized
approaches often become the methods of choice for these problems [2]. For feature extraction and preprocess-
ing of micro-array data, the most recurring algorithms in the literature are PCA [3], information gain, correlation
analysis, and False Discovery Rate thresholding [4].

For the classiﬁcation, widely applied methods include Support Vector Machines (SVM) with different kernels,
Fisher Discriminant Analysis (FDA), Logistic Regression (LogReg), k-Nearest-Neighbors classiﬁers, Decision Trees
and Random Forests [5]. In most cases, these can be complemented with a regularization procedure [2], encour-
aging model sparsity and providing helpful preprocessing to high dimensional and correlated data.

Given the wide range of high dimensional problems in the computational biology and high-throughput data
analysis framework, there is no unique state-of-the-art pipeline for feature selection and data classiﬁcation clearly
outperforming others [5]. Therefore, the approach of this project will be to test a wide range of combinations of
standard approaches and select the best ones for a more thorough analysis.

1

3. DATASET DESCRIPTION

The database this project uses was jointly created by the Maechler Lab at UNIGE and the Zamboni Lab at ETHZ,
and was transmitted to the authors under a conﬁdentiality agreement. A colony of 56 mice was divided into two
subgroups of 24 and 32 elements. The ﬁrst group mice were genetically modiﬁed (knockout, KO) to induce dia-
betes, and the second group mice were kept as control (CTRL). Out of each group, samples of 6 to 8 mice were
taken (without replacement) at weeks 4, 5, 6 and 10 of life (Table 1). For each selected mice, 2 samples of liver cells
are processed through mass spectrometry [6] and the concentrations of metabolites are estimated (not uniquely
but up to mass equivalence), for a total of 756 features per sample.

Type

CTRL
KO

Week 4

Week 5

Week 6

Week 10

(post-symptoms)

16
14

14
10

18
12

16
12

Total

64
48

Table 1: Dataset distribution, week of sampling and sick (KO) or healthy (CTRL) labels.

Our training set is the array of spectrometric measurements (x(i ))1≤i≤112 ∈ (cid:82)756, with labels giving the week
of sampling w (i ) ∈ {4,5,6,10} and the knockout state of the mice y (i ) ∈ {0,1} for each of the p = 112 sampled liver
samples. We note X the design matrix and Y the binary label vector.

Since the number of features is larger than the number of samples, data preprocessing is an important step to

avoid critical overﬁts. Dimensionality reduction algorithms were applied prior to data classiﬁcation:
PCA We compute the most signiﬁcant n eigenvectors of the empirical covariance matrix C = X T X , where n is
selected by cross validation (Figure 1, left), and project the data in the span of these principal component vectors
e1,...,en: XPC = X × [e1 ... en].

CORRELATION THRESHOLDING We estimate the Pearson Correlation Coefﬁcient ρ(Y , X·,j ) = (cid:100)Cov(Y , X·,j )

. The n

ˆσY ˆσX·,j ,

features with highest absolute value of the correlation coefﬁcient are selected, with n chosen by cross-validation.
This method has been preferred over Mutual Information Thresholding, for the predictors being real-valued.

L1 REGULARIZED LOGISTIC REGRESSION This classiﬁer is applied, and the induced model parameter sparsity [7] is
used as feature selector for a further classiﬁcation by another algorithm.

4. METHODS

METHODOLOGY The dataset was used to train different supervised classiﬁcation algorithms, whose quality was
quantiﬁed through their test scores in either leave-20-out or .632-bootstrap cross-validations. The best performing
classiﬁcation algorithms were used thereafter for quantitative and qualitative biomarker signiﬁcance assessment.

FISHER DISCRIMINANT ANALYSIS A Fisher Linear Discriminant (FDA) [8] was implemented hands-on as a starting
point to assess the potential of the dataset, and then compared to the built-in matlab LDA and SVM with default
conﬁgurations. The data is preprocessed using PCA (section 3), after which the ﬁrst Fisher vector, normal the
maximally separating hyperplane, is computed. The offset of the decision boundary between the two classes may
be selected a posteriori, in order to adjust the false positive-false negative ratio to meet requirements.

SUPPORT VECTOR MACHINES classiﬁers were attempted with a variety of kernels over PCA-preprocessed data.
The linear kernel yields preliminary results of the order of the FDA, although less stable over extensive test runs.
Polynomial, Gaussian and Sigmoid kernels did not seem adapted to the dataset, leading to less satisfactory results.

2

REGULARIZED LOGISTIC REGRESSION Logistic regression -the generalized linear model with conditional proba-
bility p(y|x) Bernoulli-distributed- is a natural choice for a 2-class linear classiﬁer. The high dimensionality of the
data at hand suggests the application of regularization procedures. This is equivalent to setting a Bayesian prior
pα(θ) on the parameters of the model, and results in a modiﬁed log likelihood function:

l(θ, λ) =(cid:88)

log p(y (i )|x(i ), θ)+ λlog p(θ) ,

i

where the regularization parameter λ is chosen by cross-validation. Most popular choices of prior distribution
are Exp(α) and N (0, α2I ), resulting respectively in so-called L1 and L2 regularizations. L1 regularization is attrac-
tive in this framework as it induces sparsity in the model parameters [7]. A weighted choice of L1 and L2, called
Elastic Net [9], has also been implemented. This learning algorithm was implemented using the python library
scikitlearn [10].

RANDOM FORESTS Random forest (RF) classiﬁer [11] is an ensemble learning approach based on decision trees.
The latter sequentially divide the training data into subsets on single-decision rules that maximize class separation.
As for Tree Bagging, RF classiﬁers bootstrap the training set and construct a decision tree for every bootstrapped
(cid:112)
sample, reducing the high variance of the decision tree classiﬁer approach. RF differentiates form Tree Bagging by
performing each split over a randomly chosen subset of features, of typical size
#features. This randomized boot-
strapping prevents correlated trees in the forest and improves accuracy. Interestingly, RF are invariant to rescaling
of data, but tend to extract only a small subset of discriminative variables when features are highly correlated. This
learning algorithm was implemented using the python library scikitlearn [10].

5. RESULTS AND DISCUSSION

PARAMETER SELECTION For feature extraction and regularized classiﬁcations, some hyperparameters must be
picked to optimize the expected score of the algorithm. This is done by maximizing the CV-score over the full
range of possible parameter combinations on the training set with a repeated leave-20-out procedure, which has
been chosen because of the relatively small dataset size. This includes the regularization parameter for L1-LogReg
and the dimension of the PCA subspace for FDA. The resulting validation curves are displayed in Figure 1.

GENERAL RESULTS The scores for some of the preprocessor-classiﬁer algorithms that have been applied in this
analysis are reported in table 2. Out of the considered methods, those with best performance were studied in
more detail, with a preference for methods allowing a direct ranking of discriminative features in order to facilitate
biomarker extraction.

Method
LDA
Linear SVM
Quadratic SVM
L1-penalized Logistic Reg.
L2-penalized Logistic Reg.
ElNet-penalized Linear Reg.
Random Forests

Preproc.

PCA
PCA
PCA

CorrThresh
CorrThresh
CorrThresh
L1-LogReg

CV score
97.4%
96.1%
92.7%
97.1%
96.1%
83.1%
93.0%

Sparsity

0%
0%
0%

90.6%
80.4%
54.4%
86.4 %

Table 2: Leave-20-out cross-validation scores for preprocessor-classiﬁer pairs tried on the dataset.

MODEL EVALUATION The learning procedures listed in Section 4 were evaluated with different cross validation
procedures: 0.632-bootstrap, 10-fold, leave-one-out and leave-20-out cross-validations. Detailed results are shown

3

Figure 1: Left: Cross-validation of the number of PC dimensions in the FDA algorithm. The training score
ceils at 1, whereas the testing score drops to 0.5 when dimensionality is higher than (number_of_points
- 1) for separability reasons (red vertical line). Right: Cross-validation of the regularization parameter
C in the L1-LogReg preprocesser/RandomForest classiﬁer algorithm. Colored areas correspond to the
standard deviation on the errors. The black solid line represents the sparsity of model parameters as
a function of the regularization parameter. Both cross-validations are realized in a repeated 300 times

leave-20-out fashion.

for PCA preprocessing with FDA classiﬁer in table 3, and for the two other algorithms in table 4. Cross-validations
have been implemented manually in python and Matlab.

Total tests

6000

Predicted

CTRL

Predicted

KO

Accuracy
97.19%

Actual
CTRL

3342

79

Actual

KO

88

2491

Prevalence

57.04%

Positive predictive

False discovery

value: 97.41%
False omission

rate: 3.10%

rate: 2.59%

Negative Predictive

value: 96.90%

True positive
rate: 97.67%
False negative

rate: 2.33%

False positive
rate: 3.44%
True negative
rate: 96.56%

Positive likelihood

ratio: 28.39

Negative likelihood

ratio: 0.024

Diagnostic odds

ratio: 1,176

Table 3: Detailed confusion matrix for the Linear Discriminant classiﬁer

Algorithm

True Positive

False Positive

True Negative

False Negative

Accuracy

L1-reg Logistic Regression +

Random Forest

Correlation thresholding +
L1-reg Logistic Regression

3346

3363

359

97

60

75

2235

2467

93.02%

97.14%

Table 4: Reduced confusion matrix for the L1-LogReg and Random Forest Classiﬁers

4

Principal Component Dimension n102030405060708090100110120Score0.40.50.60.70.80.911.1Cross validation of dimensionality in PCA preprocessingTesting ScoreTraining ScorefontsizeFEATURE EXTRACTION AND BIOMARKER IDENTIFICATION The selected algorithms FDA, L1-LogReg and Random
Forests provided us with the ability to extract the importance of each feature in the discrimination between the two
classes. We validate the extraction of these biomarkers by measuring the stability of their prevalence coefﬁcients
across algorithms. Figure 2 and the associated table show the stability of biomarker prevalence. The comparison
between L1-LogReg and FDA is most natural because the weight of the features are quantiﬁed by the corresponding
component of the vector normal to the maximally separating hyperplane.

It is also a key point to verify our best found biomarkers against the medical knowledge as reﬂected by the
current scientiﬁc literature. Preliminary research on the matter shows a consistent correlation between biomarker
rankings and the amount of available scientiﬁc literature about diabetes mentioning the biomarker.

Compound \ Rank
Ajugalactone
UDP-glucosamine
Cholic Acid
UDP-glucose
Uridine

L1-LogReg

1
2
3
4
5

RF
39
1
21
12
18

LDA

1
2
48
36
3

Figure 2: Left: Prevalence coefﬁcients of the 17 most signiﬁcant biomarkers in the L1-LogReg algorithm
versus their FDA prevalence. The dashed black line is provided as a guide to the eye. Right: rankings by

prevalence of a selected subset of biomarkers across all three classiﬁers.

6. CONCLUSION AND FUTURE WORK

This work has addressed the problem of supervised classiﬁcation and biomarker identiﬁcation in metabolic pro-
ﬁling data of mice liver cells depending on the presence of Type II diabetes. The high dimensionality of data
requires testing different preprocessors and learning algorithms for supervised classiﬁcation. The preproces-
sor/classiﬁer combinations that lead to the highest cross-validation scores are PCA/FDA, L1-regularized Logistic
Regression/Random Forest Classiﬁer and Correlation thresholding/L1-regularized Logistic Regression. For each
of these three most successful methods, the features with the highest inﬂuence on the classiﬁer have been ranked
and compare favorably.

Possible directions for future work include the application of methods integrating prior biological knowledge
-such as the known topology of the metabolic graph- for example through overlapping sparse group lasso or pre-
conditioned PCA techniques.

The authors hope that this work will drive interest for further analytics in the biomedical scientiﬁc community,
in validation, continuance, and in building denser datasets that will help shed light on the statistical signiﬁcance
of this work and future ones.

5

Biomarker weights - FDA#10-400.10.20.30.40.50.60.70.80.91Biomarker weights - L1-LogReg00.050.10.150.20.250.30.350.4Prevalence coefficents of BiomarkersBiomarker Identiﬁcation for Early-stage
Diabetes Diagnosis in Mice Liver Cells

Andrea Agazzi(1), Vincent Deo(2)

(1) Theoretical Physics Department, Université de Genève, Switzerland
(2) Electrical Engineering Department, Stanford University, CA, U.S.A.

agazzian, vdeo@stanford.edu

December 11th, 2015

ABSTRACT

This projects investigates automated diagnosis possibilities from metabolite micro-array measurements in liver
cells, at an incipient stage of the disease. L1-regularized logistic regression, Fisher Discriminant Analysis and Ran-
dom Forests classiﬁers are studied, yielding promising results and hindsight on biomarker signiﬁcance.

1. INTRODUCTION

Diabetes Mellitus is one of today’s most common chronic diseases, affecting approximately 382M of people world-
wide in 2013 [1]. The disease, resulting in serious cardiovascular complications, is divided into two categories.
Type 2 diabetes, accounting for approximately 90% of the cases, is a gradually developing form of the disease, and
can be prevented if diagnosed at an early stage. Motivated by the inherently metabolic character of the disease, the
Maechler Lab at the University Hospital of Geneva is investigating a novel form of early stage diagnosis, searching
for quantitative biological markers in a patient’s liver cells. The liver is responsible for carbohydrate metabolism
and is therefore expected to be affected ﬁrst by incipient diabetes.

This project aims to analyze the data conveyed from the Maechler Lab and tackle the issues of early-stage di-
agnosis and biomarker identiﬁcation. Speciﬁcally, the algorithm input is the result of micro-array experiments
of metabolite concentrations in both sick and healthy mice liver cells. We apply different combinations of pre-
processing and supervised classiﬁcation algorithms, for the purpose of automated diagnosis of new data points.
Furthermore, we aim to extract the most signiﬁcant features indicating the presence of the disease through inspec-
tion of the best performing classiﬁers.

2. RELATED WORK

Micro-array and metabolite proﬁling experiments have been an object of interest in the machine learning liter-
ature ever since they appeared in cell biology. The data is high-dimensional, and therefore heavily regularized
approaches often become the methods of choice for these problems [2]. For feature extraction and preprocess-
ing of micro-array data, the most recurring algorithms in the literature are PCA [3], information gain, correlation
analysis, and False Discovery Rate thresholding [4].

For the classiﬁcation, widely applied methods include Support Vector Machines (SVM) with different kernels,
Fisher Discriminant Analysis (FDA), Logistic Regression (LogReg), k-Nearest-Neighbors classiﬁers, Decision Trees
and Random Forests [5]. In most cases, these can be complemented with a regularization procedure [2], encour-
aging model sparsity and providing helpful preprocessing to high dimensional and correlated data.

Given the wide range of high dimensional problems in the computational biology and high-throughput data
analysis framework, there is no unique state-of-the-art pipeline for feature selection and data classiﬁcation clearly
outperforming others [5]. Therefore, the approach of this project will be to test a wide range of combinations of
standard approaches and select the best ones for a more thorough analysis.

1

3. DATASET DESCRIPTION

The database this project uses was jointly created by the Maechler Lab at UNIGE and the Zamboni Lab at ETHZ,
and was transmitted to the authors under a conﬁdentiality agreement. A colony of 56 mice was divided into two
subgroups of 24 and 32 elements. The ﬁrst group mice were genetically modiﬁed (knockout, KO) to induce dia-
betes, and the second group mice were kept as control (CTRL). Out of each group, samples of 6 to 8 mice were
taken (without replacement) at weeks 4, 5, 6 and 10 of life (Table 1). For each selected mice, 2 samples of liver cells
are processed through mass spectrometry [6] and the concentrations of metabolites are estimated (not uniquely
but up to mass equivalence), for a total of 756 features per sample.

Type

CTRL
KO

Week 4

Week 5

Week 6

Week 10

(post-symptoms)

16
14

14
10

18
12

16
12

Total

64
48

Table 1: Dataset distribution, week of sampling and sick (KO) or healthy (CTRL) labels.

Our training set is the array of spectrometric measurements (x(i ))1≤i≤112 ∈ (cid:82)756, with labels giving the week
of sampling w (i ) ∈ {4,5,6,10} and the knockout state of the mice y (i ) ∈ {0,1} for each of the p = 112 sampled liver
samples. We note X the design matrix and Y the binary label vector.

Since the number of features is larger than the number of samples, data preprocessing is an important step to

avoid critical overﬁts. Dimensionality reduction algorithms were applied prior to data classiﬁcation:
PCA We compute the most signiﬁcant n eigenvectors of the empirical covariance matrix C = X T X , where n is
selected by cross validation (Figure 1, left), and project the data in the span of these principal component vectors
e1,...,en: XPC = X × [e1 ... en].

CORRELATION THRESHOLDING We estimate the Pearson Correlation Coefﬁcient ρ(Y , X·,j ) = (cid:100)Cov(Y , X·,j )

. The n

ˆσY ˆσX·,j ,

features with highest absolute value of the correlation coefﬁcient are selected, with n chosen by cross-validation.
This method has been preferred over Mutual Information Thresholding, for the predictors being real-valued.

L1 REGULARIZED LOGISTIC REGRESSION This classiﬁer is applied, and the induced model parameter sparsity [7] is
used as feature selector for a further classiﬁcation by another algorithm.

4. METHODS

METHODOLOGY The dataset was used to train different supervised classiﬁcation algorithms, whose quality was
quantiﬁed through their test scores in either leave-20-out or .632-bootstrap cross-validations. The best performing
classiﬁcation algorithms were used thereafter for quantitative and qualitative biomarker signiﬁcance assessment.

FISHER DISCRIMINANT ANALYSIS A Fisher Linear Discriminant (FDA) [8] was implemented hands-on as a starting
point to assess the potential of the dataset, and then compared to the built-in matlab LDA and SVM with default
conﬁgurations. The data is preprocessed using PCA (section 3), after which the ﬁrst Fisher vector, normal the
maximally separating hyperplane, is computed. The offset of the decision boundary between the two classes may
be selected a posteriori, in order to adjust the false positive-false negative ratio to meet requirements.

SUPPORT VECTOR MACHINES classiﬁers were attempted with a variety of kernels over PCA-preprocessed data.
The linear kernel yields preliminary results of the order of the FDA, although less stable over extensive test runs.
Polynomial, Gaussian and Sigmoid kernels did not seem adapted to the dataset, leading to less satisfactory results.

2

REGULARIZED LOGISTIC REGRESSION Logistic regression -the generalized linear model with conditional proba-
bility p(y|x) Bernoulli-distributed- is a natural choice for a 2-class linear classiﬁer. The high dimensionality of the
data at hand suggests the application of regularization procedures. This is equivalent to setting a Bayesian prior
pα(θ) on the parameters of the model, and results in a modiﬁed log likelihood function:

l(θ, λ) =(cid:88)

log p(y (i )|x(i ), θ)+ λlog p(θ) ,

i

where the regularization parameter λ is chosen by cross-validation. Most popular choices of prior distribution
are Exp(α) and N (0, α2I ), resulting respectively in so-called L1 and L2 regularizations. L1 regularization is attrac-
tive in this framework as it induces sparsity in the model parameters [7]. A weighted choice of L1 and L2, called
Elastic Net [9], has also been implemented. This learning algorithm was implemented using the python library
scikitlearn [10].

RANDOM FORESTS Random forest (RF) classiﬁer [11] is an ensemble learning approach based on decision trees.
The latter sequentially divide the training data into subsets on single-decision rules that maximize class separation.
As for Tree Bagging, RF classiﬁers bootstrap the training set and construct a decision tree for every bootstrapped
(cid:112)
sample, reducing the high variance of the decision tree classiﬁer approach. RF differentiates form Tree Bagging by
performing each split over a randomly chosen subset of features, of typical size
#features. This randomized boot-
strapping prevents correlated trees in the forest and improves accuracy. Interestingly, RF are invariant to rescaling
of data, but tend to extract only a small subset of discriminative variables when features are highly correlated. This
learning algorithm was implemented using the python library scikitlearn [10].

5. RESULTS AND DISCUSSION

PARAMETER SELECTION For feature extraction and regularized classiﬁcations, some hyperparameters must be
picked to optimize the expected score of the algorithm. This is done by maximizing the CV-score over the full
range of possible parameter combinations on the training set with a repeated leave-20-out procedure, which has
been chosen because of the relatively small dataset size. This includes the regularization parameter for L1-LogReg
and the dimension of the PCA subspace for FDA. The resulting validation curves are displayed in Figure 1.

GENERAL RESULTS The scores for some of the preprocessor-classiﬁer algorithms that have been applied in this
analysis are reported in table 2. Out of the considered methods, those with best performance were studied in
more detail, with a preference for methods allowing a direct ranking of discriminative features in order to facilitate
biomarker extraction.

Method
LDA
Linear SVM
Quadratic SVM
L1-penalized Logistic Reg.
L2-penalized Logistic Reg.
ElNet-penalized Linear Reg.
Random Forests

Preproc.

PCA
PCA
PCA

CorrThresh
CorrThresh
CorrThresh
L1-LogReg

CV score
97.4%
96.1%
92.7%
97.1%
96.1%
83.1%
93.0%

Sparsity

0%
0%
0%

90.6%
80.4%
54.4%
86.4 %

Table 2: Leave-20-out cross-validation scores for preprocessor-classiﬁer pairs tried on the dataset.

MODEL EVALUATION The learning procedures listed in Section 4 were evaluated with different cross validation
procedures: 0.632-bootstrap, 10-fold, leave-one-out and leave-20-out cross-validations. Detailed results are shown

3

Figure 1: Left: Cross-validation of the number of PC dimensions in the FDA algorithm. The training score
ceils at 1, whereas the testing score drops to 0.5 when dimensionality is higher than (number_of_points
- 1) for separability reasons (red vertical line). Right: Cross-validation of the regularization parameter
C in the L1-LogReg preprocesser/RandomForest classiﬁer algorithm. Colored areas correspond to the
standard deviation on the errors. The black solid line represents the sparsity of model parameters as
a function of the regularization parameter. Both cross-validations are realized in a repeated 300 times

leave-20-out fashion.

for PCA preprocessing with FDA classiﬁer in table 3, and for the two other algorithms in table 4. Cross-validations
have been implemented manually in python and Matlab.

Total tests

6000

Predicted

CTRL

Predicted

KO

Accuracy
97.19%

Actual
CTRL

3342

79

Actual

KO

88

2491

Prevalence

57.04%

Positive predictive

False discovery

value: 97.41%
False omission

rate: 3.10%

rate: 2.59%

Negative Predictive

value: 96.90%

True positive
rate: 97.67%
False negative

rate: 2.33%

False positive
rate: 3.44%
True negative
rate: 96.56%

Positive likelihood

ratio: 28.39

Negative likelihood

ratio: 0.024

Diagnostic odds

ratio: 1,176

Table 3: Detailed confusion matrix for the Linear Discriminant classiﬁer

Algorithm

True Positive

False Positive

True Negative

False Negative

Accuracy

L1-reg Logistic Regression +

Random Forest

Correlation thresholding +
L1-reg Logistic Regression

3346

3363

359

97

60

75

2235

2467

93.02%

97.14%

Table 4: Reduced confusion matrix for the L1-LogReg and Random Forest Classiﬁers

4

Principal Component Dimension n102030405060708090100110120Score0.40.50.60.70.80.911.1Cross validation of dimensionality in PCA preprocessingTesting ScoreTraining ScorefontsizeFEATURE EXTRACTION AND BIOMARKER IDENTIFICATION The selected algorithms FDA, L1-LogReg and Random
Forests provided us with the ability to extract the importance of each feature in the discrimination between the two
classes. We validate the extraction of these biomarkers by measuring the stability of their prevalence coefﬁcients
across algorithms. Figure 2 and the associated table show the stability of biomarker prevalence. The comparison
between L1-LogReg and FDA is most natural because the weight of the features are quantiﬁed by the corresponding
component of the vector normal to the maximally separating hyperplane.

It is also a key point to verify our best found biomarkers against the medical knowledge as reﬂected by the
current scientiﬁc literature. Preliminary research on the matter shows a consistent correlation between biomarker
rankings and the amount of available scientiﬁc literature about diabetes mentioning the biomarker.

Compound \ Rank
Ajugalactone
UDP-glucosamine
Cholic Acid
UDP-glucose
Uridine

L1-LogReg

1
2
3
4
5

RF
39
1
21
12
18

LDA

1
2
48
36
3

Figure 2: Left: Prevalence coefﬁcients of the 17 most signiﬁcant biomarkers in the L1-LogReg algorithm
versus their FDA prevalence. The dashed black line is provided as a guide to the eye. Right: rankings by

prevalence of a selected subset of biomarkers across all three classiﬁers.

6. CONCLUSION AND FUTURE WORK

This work has addressed the problem of supervised classiﬁcation and biomarker identiﬁcation in metabolic pro-
ﬁling data of mice liver cells depending on the presence of Type II diabetes. The high dimensionality of data
requires testing different preprocessors and learning algorithms for supervised classiﬁcation. The preproces-
sor/classiﬁer combinations that lead to the highest cross-validation scores are PCA/FDA, L1-regularized Logistic
Regression/Random Forest Classiﬁer and Correlation thresholding/L1-regularized Logistic Regression. For each
of these three most successful methods, the features with the highest inﬂuence on the classiﬁer have been ranked
and compare favorably.

Possible directions for future work include the application of methods integrating prior biological knowledge
-such as the known topology of the metabolic graph- for example through overlapping sparse group lasso or pre-
conditioned PCA techniques.

The authors hope that this work will drive interest for further analytics in the biomedical scientiﬁc community,
in validation, continuance, and in building denser datasets that will help shed light on the statistical signiﬁcance
of this work and future ones.

5

Biomarker weights - FDA#10-400.10.20.30.40.50.60.70.80.91Biomarker weights - L1-LogReg00.050.10.150.20.250.30.350.4Prevalence coefficents of BiomarkersREFERENCES

[1] Y. Shi and F. B. Hu, “The global implications of diabetes and cancer,” The Lancet, vol. 383, no. 9933, pp. 1947 –

1948, 2014.

[2] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Springer Series in Statistics,

New York, NY, USA: Springer New York Inc., 2001.

[3] K. Y. Yeung and W. L. Ruzzo, “Principal component analysis for clustering gene expression data,” Bioinformat-

ics, vol. 17, no. 9, pp. 763–774, 2001.

[4] J. Quackenbush, “Computational analysis of microarray data,” Nature reviews genetics, vol. 2, no. 6, pp. 418–

427, 2001.

[5] T. Li, C. Zhang, and M. Ogihara, “A comparative study of feature selection and multiclass classiﬁcation meth-

ods for tissue classiﬁcation based on gene expression,” vol. 20, no. 15, pp. 2429–2437, 2004.

[6] T. Fuhrer, D. Heer, B. Begemann, and N. Zamboni, “High-throughput, accurate mass metabolome proﬁling
of cellular extracts by ﬂow injection–time-of-ﬂight mass spectrometry,” Analytical Chemistry, vol. 83, no. 18,
pp. 7074–7080, 2011. PMID: 21830798.

[7] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical Society. Series

B (Methodological), pp. 267–288, 1996.

[8] R. A. Fisher, “The use of multiple measurements in taxonomic problems,” Annals of Eugenics, vol. 7, no. 2,

pp. 179–188, 1936.

[9] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,” Journal of the Royal Statistical

Society: Series B (Statistical Methodology), vol. 67, no. 2, pp. 301–320, 2005.

[10] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn:
Machine learning in Python,” Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.

[11] T. K. Ho, “The random subspace method for constructing decision forests,” Pattern Analysis and Machine

Intelligence, IEEE Transactions on, vol. 20, no. 8, pp. 832–844, 1998.

6

