HEP Computing in a Context-Aware Cloud Environment

Frank Berghaus, Ron Desmarais, Ian
Gable,Colin Leavett-Brown, Michael

Paterson, Randall J. Sobie, Ryan Taylor

Department of Physics and Astronomy,
University of Victoria, Victoria, Canada
Institute for Particle Physics, University of

Victoria, Victoria, Canada

frank@uvic,ca, rd@uvic.ca, igable@uvic.ca,
crlb@uvic.ca, mhp@uvic.ca, rsobie@uvic.ca,

rptaylor@uvic.ca

ABSTRACT
This paper describes the use of a distributed cloud comput-
ing system for high energy physics (HEP) applications. The
system is composed of IaaS clouds integrated into a uniﬁed
infrastructure that has been in production for over two years.
It continues to expand in scale and sites, encompassing more
than twenty clouds on three continents. We are prototyping
a new context-aware architecture that enables the virtual
machines to make connections to both software and data
repositories based on geolocation information. The new de-
sign will signiﬁcantly enhance the ability of the system to
scale to higher workloads and run data-intensive applica-
tions. We review the operation of the production system
and describe our work towards a context-aware cloud sys-
tem.

Categories and Subject Descriptors
C.2.4 [Distributed Systems]: Distributed Applications

Keywords
cloud computing, grid of clouds, scientiﬁc applications

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ScienceCloud’14, June xx, 2014, Vancouver, BC, Canada.
Copyright 2014 ACM 978-1-4503-1979-9/13/06 ...$15.00.

Andre Charbonneau

Research Computing Support

Shared Services Canada, Ottawa, Canada

Andre.Charbonneau@SSC-SPC.gc.ca

1.

INTRODUCTION

In a previous paper, we showed that a distributed cloud
computing system can be eﬀectively used for high-throughput
computing workloads for high energy physics (HEP) appli-
cations [12]. Since 2012, we have operated a distributed
cloud in a production mode, running over two million jobs,
with peak workloads of 1500 concurrent jobs. This system
is composed of a varying number of IaaS clouds, owned and
administered by academic institutions and national labora-
tories, and distributed across three continents. The exact
size of this uniﬁed infrastructure varies according to avail-
ability, maintenance and development cycles, but as many
as twenty ﬁve clouds have been employed at one time.

We have focused on running applications with no input
data requirements such as the simulation of the particle col-
lisions that are required for the development of analysis al-
gorithms and understanding the interaction of particles with
the detectors. The output from these applications is mod-
est and is written to a single site on each continent. The
applications running in the virtual machine (VM) instances
retrieve the HEP-speciﬁc software and calibration databases
from a few locations. The distributed cloud has been very
successful, as highlighted in our previous work [12], without
any scalability limits observed.

In order to make the system more versatile and to run
applications requiring large input data sets, we need to ad-
dress a number of issues. First, we want the VM instance
to access the closest software cache to reduce the demand
on the single site (eliminating a single point of failure), and
minimize the network latency.

Second, we wish to run applications that require moder-
ately large input data sets (20-100 GB for a single job over
a 12-hour period). The input data needs to be read in at a
high rate by the application. Hence we have selected pro-
tocols that cache or copy the data to the local disk rather
than streaming the data directly over the network. In addi-
tion, the job will query a federation storage system for the
nearest location of the input data and stage the data, using
high-speed transfer protocols, to the local disk.

We believe that these issues can be addressed if the dis-
tributed cloud computing system becomes context-aware.
Context-aware systems, that can sense and react to their
environment, are not new and are used extensively in the

HEP Computing in a Context-Aware Cloud Environment

Frank Berghaus, Ron Desmarais, Ian
Gable,Colin Leavett-Brown, Michael

Paterson, Randall J. Sobie, Ryan Taylor

Department of Physics and Astronomy,
University of Victoria, Victoria, Canada
Institute for Particle Physics, University of

Victoria, Victoria, Canada

frank@uvic,ca, rd@uvic.ca, igable@uvic.ca,
crlb@uvic.ca, mhp@uvic.ca, rsobie@uvic.ca,

rptaylor@uvic.ca

ABSTRACT
This paper describes the use of a distributed cloud comput-
ing system for high energy physics (HEP) applications. The
system is composed of IaaS clouds integrated into a uniﬁed
infrastructure that has been in production for over two years.
It continues to expand in scale and sites, encompassing more
than twenty clouds on three continents. We are prototyping
a new context-aware architecture that enables the virtual
machines to make connections to both software and data
repositories based on geolocation information. The new de-
sign will signiﬁcantly enhance the ability of the system to
scale to higher workloads and run data-intensive applica-
tions. We review the operation of the production system
and describe our work towards a context-aware cloud sys-
tem.

Categories and Subject Descriptors
C.2.4 [Distributed Systems]: Distributed Applications

Keywords
cloud computing, grid of clouds, scientiﬁc applications

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ScienceCloud’14, June xx, 2014, Vancouver, BC, Canada.
Copyright 2014 ACM 978-1-4503-1979-9/13/06 ...$15.00.

Andre Charbonneau

Research Computing Support

Shared Services Canada, Ottawa, Canada

Andre.Charbonneau@SSC-SPC.gc.ca

1.

INTRODUCTION

In a previous paper, we showed that a distributed cloud
computing system can be eﬀectively used for high-throughput
computing workloads for high energy physics (HEP) appli-
cations [12]. Since 2012, we have operated a distributed
cloud in a production mode, running over two million jobs,
with peak workloads of 1500 concurrent jobs. This system
is composed of a varying number of IaaS clouds, owned and
administered by academic institutions and national labora-
tories, and distributed across three continents. The exact
size of this uniﬁed infrastructure varies according to avail-
ability, maintenance and development cycles, but as many
as twenty ﬁve clouds have been employed at one time.

We have focused on running applications with no input
data requirements such as the simulation of the particle col-
lisions that are required for the development of analysis al-
gorithms and understanding the interaction of particles with
the detectors. The output from these applications is mod-
est and is written to a single site on each continent. The
applications running in the virtual machine (VM) instances
retrieve the HEP-speciﬁc software and calibration databases
from a few locations. The distributed cloud has been very
successful, as highlighted in our previous work [12], without
any scalability limits observed.

In order to make the system more versatile and to run
applications requiring large input data sets, we need to ad-
dress a number of issues. First, we want the VM instance
to access the closest software cache to reduce the demand
on the single site (eliminating a single point of failure), and
minimize the network latency.

Second, we wish to run applications that require moder-
ately large input data sets (20-100 GB for a single job over
a 12-hour period). The input data needs to be read in at a
high rate by the application. Hence we have selected pro-
tocols that cache or copy the data to the local disk rather
than streaming the data directly over the network. In addi-
tion, the job will query a federation storage system for the
nearest location of the input data and stage the data, using
high-speed transfer protocols, to the local disk.

We believe that these issues can be addressed if the dis-
tributed cloud computing system becomes context-aware.
Context-aware systems, that can sense and react to their
environment, are not new and are used extensively in the

Figure 1: A high-level view of the distributed cloud computing system as used by the ATLAS experiment
since 2012. The cloud batch system is contained in the outlined box where jobs are submitted by the ATLAS
user to HTCondor. CloudScheduler uses the list of jobs to request the start of VM instances on the available
IaaS clouds. The VM instances connect to the software repository, the application database and the data
repositories. The application database contains calibration constants associated with experimental apparatus
whereas the software repository has the application source code.

mobile phone industry. For example, context-aware appli-
cations use location to adapt interfaces, locate data, and
discover services for the user. In our case, a context-aware
cloud would dynamically locate the optimal locations for
retrieving the software and data. More speculatively, we
imagine the job or VM instance would establish a network
connection to those repositories using the emerging Software
Deﬁned Network technologies.

Section 2 of this paper describes the distributed cloud
computing system as it exists today, and highlights the ma-
jor components and operation. Section 3 describes the changes
needed to transition to a context-aware distributed cloud.
Section 4 provides a summary of this work.

2. DISTRIBUTED CLOUD SYSTEM

A full description of the distributed cloud computing sys-
tem can be found in [12]. The design is based on the original
concept of a ‘grid of clouds’ or ‘sky computing’ as a solution
for combining separate IaaS clouds into a uniﬁed infrastruc-
ture [6].

The distributed cloud is used for the ATLAS experiment
at the CERN Laboratory in Geneva by utilizing IaaS aca-
demic clouds in Canada, the United States, Europe and Aus-
tralia (see [12]). The majority of participating clouds are
conﬁgured with the OpenStack [9] software suite, though
some are still running Nimbus [7]. The use of the Nimbus

software on the IaaS clouds is expected to end in 2014 with
most clouds migrating to OpenStack.

At the heart of the distributed cloud computing system is
an HTCondor and CloudScheduler instance operating at the
University of Victoria. We have recently commissioned a
new, independent instance at CERN with its purpose being
to shift operations to the central location so that we can
take advantage of round-the-clock production support staﬀ.
The applicability of the distributed cloud computing model
is not limited to the ATLAS experiment. The Canadian
CANFAR project has created an independent system using
diﬀerent resources for the processing and analysis of astro-
nomical data [1].
In addition, we recently created a new
system for the Belle-II HEP experiment at the KEK Lab-
oratory in Japan. Each of the systems has their own HT-
Condor/CloudScheduler instance and utilize diﬀerent IaaS
clouds (though there are some shared clouds).

The goal is to run embarrassingly-parallel HEP applica-
tions on IaaS clouds. The application environment is com-
pletely provided by the VM image and distributed cloud
supporting services. Typically, jobs run on a single core for
approximately 6 − 12 hours.

Very recently the ATLAS experiment has developed an
application that uses 8-cores for a single job. Running multi-
core jobs in an 8-core VM is expected to have many beneﬁts
(reduced memory requirements, shared caches, reduced in-

ATLAS	  VM	  VM	  VM	  VM	  VM	  VM	  HTCondor	  Cloud	  Scheduler	  Run	  job	  Request	  VM	  So9ware	  repository	  Applica?on	  database	  Data	  repository	  HEP Computing in a Context-Aware Cloud Environment

Frank Berghaus, Ron Desmarais, Ian
Gable,Colin Leavett-Brown, Michael

Paterson, Randall J. Sobie, Ryan Taylor

Department of Physics and Astronomy,
University of Victoria, Victoria, Canada
Institute for Particle Physics, University of

Victoria, Victoria, Canada

frank@uvic,ca, rd@uvic.ca, igable@uvic.ca,
crlb@uvic.ca, mhp@uvic.ca, rsobie@uvic.ca,

rptaylor@uvic.ca

ABSTRACT
This paper describes the use of a distributed cloud comput-
ing system for high energy physics (HEP) applications. The
system is composed of IaaS clouds integrated into a uniﬁed
infrastructure that has been in production for over two years.
It continues to expand in scale and sites, encompassing more
than twenty clouds on three continents. We are prototyping
a new context-aware architecture that enables the virtual
machines to make connections to both software and data
repositories based on geolocation information. The new de-
sign will signiﬁcantly enhance the ability of the system to
scale to higher workloads and run data-intensive applica-
tions. We review the operation of the production system
and describe our work towards a context-aware cloud sys-
tem.

Categories and Subject Descriptors
C.2.4 [Distributed Systems]: Distributed Applications

Keywords
cloud computing, grid of clouds, scientiﬁc applications

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ScienceCloud’14, June xx, 2014, Vancouver, BC, Canada.
Copyright 2014 ACM 978-1-4503-1979-9/13/06 ...$15.00.

Andre Charbonneau

Research Computing Support

Shared Services Canada, Ottawa, Canada

Andre.Charbonneau@SSC-SPC.gc.ca

1.

INTRODUCTION

In a previous paper, we showed that a distributed cloud
computing system can be eﬀectively used for high-throughput
computing workloads for high energy physics (HEP) appli-
cations [12]. Since 2012, we have operated a distributed
cloud in a production mode, running over two million jobs,
with peak workloads of 1500 concurrent jobs. This system
is composed of a varying number of IaaS clouds, owned and
administered by academic institutions and national labora-
tories, and distributed across three continents. The exact
size of this uniﬁed infrastructure varies according to avail-
ability, maintenance and development cycles, but as many
as twenty ﬁve clouds have been employed at one time.

We have focused on running applications with no input
data requirements such as the simulation of the particle col-
lisions that are required for the development of analysis al-
gorithms and understanding the interaction of particles with
the detectors. The output from these applications is mod-
est and is written to a single site on each continent. The
applications running in the virtual machine (VM) instances
retrieve the HEP-speciﬁc software and calibration databases
from a few locations. The distributed cloud has been very
successful, as highlighted in our previous work [12], without
any scalability limits observed.

In order to make the system more versatile and to run
applications requiring large input data sets, we need to ad-
dress a number of issues. First, we want the VM instance
to access the closest software cache to reduce the demand
on the single site (eliminating a single point of failure), and
minimize the network latency.

Second, we wish to run applications that require moder-
ately large input data sets (20-100 GB for a single job over
a 12-hour period). The input data needs to be read in at a
high rate by the application. Hence we have selected pro-
tocols that cache or copy the data to the local disk rather
than streaming the data directly over the network. In addi-
tion, the job will query a federation storage system for the
nearest location of the input data and stage the data, using
high-speed transfer protocols, to the local disk.

We believe that these issues can be addressed if the dis-
tributed cloud computing system becomes context-aware.
Context-aware systems, that can sense and react to their
environment, are not new and are used extensively in the

Figure 1: A high-level view of the distributed cloud computing system as used by the ATLAS experiment
since 2012. The cloud batch system is contained in the outlined box where jobs are submitted by the ATLAS
user to HTCondor. CloudScheduler uses the list of jobs to request the start of VM instances on the available
IaaS clouds. The VM instances connect to the software repository, the application database and the data
repositories. The application database contains calibration constants associated with experimental apparatus
whereas the software repository has the application source code.

mobile phone industry. For example, context-aware appli-
cations use location to adapt interfaces, locate data, and
discover services for the user. In our case, a context-aware
cloud would dynamically locate the optimal locations for
retrieving the software and data. More speculatively, we
imagine the job or VM instance would establish a network
connection to those repositories using the emerging Software
Deﬁned Network technologies.

Section 2 of this paper describes the distributed cloud
computing system as it exists today, and highlights the ma-
jor components and operation. Section 3 describes the changes
needed to transition to a context-aware distributed cloud.
Section 4 provides a summary of this work.

2. DISTRIBUTED CLOUD SYSTEM

A full description of the distributed cloud computing sys-
tem can be found in [12]. The design is based on the original
concept of a ‘grid of clouds’ or ‘sky computing’ as a solution
for combining separate IaaS clouds into a uniﬁed infrastruc-
ture [6].

The distributed cloud is used for the ATLAS experiment
at the CERN Laboratory in Geneva by utilizing IaaS aca-
demic clouds in Canada, the United States, Europe and Aus-
tralia (see [12]). The majority of participating clouds are
conﬁgured with the OpenStack [9] software suite, though
some are still running Nimbus [7]. The use of the Nimbus

software on the IaaS clouds is expected to end in 2014 with
most clouds migrating to OpenStack.

At the heart of the distributed cloud computing system is
an HTCondor and CloudScheduler instance operating at the
University of Victoria. We have recently commissioned a
new, independent instance at CERN with its purpose being
to shift operations to the central location so that we can
take advantage of round-the-clock production support staﬀ.
The applicability of the distributed cloud computing model
is not limited to the ATLAS experiment. The Canadian
CANFAR project has created an independent system using
diﬀerent resources for the processing and analysis of astro-
nomical data [1].
In addition, we recently created a new
system for the Belle-II HEP experiment at the KEK Lab-
oratory in Japan. Each of the systems has their own HT-
Condor/CloudScheduler instance and utilize diﬀerent IaaS
clouds (though there are some shared clouds).

The goal is to run embarrassingly-parallel HEP applica-
tions on IaaS clouds. The application environment is com-
pletely provided by the VM image and distributed cloud
supporting services. Typically, jobs run on a single core for
approximately 6 − 12 hours.

Very recently the ATLAS experiment has developed an
application that uses 8-cores for a single job. Running multi-
core jobs in an 8-core VM is expected to have many beneﬁts
(reduced memory requirements, shared caches, reduced in-

ATLAS	  VM	  VM	  VM	  VM	  VM	  VM	  HTCondor	  Cloud	  Scheduler	  Run	  job	  Request	  VM	  So9ware	  repository	  Applica?on	  database	  Data	  repository	  put ﬁle staging space, and fewer jobs) and we expect to run
these jobs in the coming months.

In ﬁg. 1, we present a high-level view of the distributed
cloud. The scheduling of batch jobs is performed by HTCon-
dor [13] and the deployment of VM images is done by Cloud
Scheduler [2]. HTCondor was designed as a cycle scavenger,
making it an ideal job scheduler for a dynamic cloud envi-
ronment where VM instances appear and disappear based on
demand. Cloud Scheduler periodically reviews the require-
ments of the jobs in the HTCondor job queue and makes
requests to boot user-speciﬁc VM images on one of the IaaS
clouds. Once the VM image is booted, it attaches itself to
the HTCondor resource pool and is ready to accept jobs.
The instances are shut down when there are no jobs in the
HTCondor queue.

The VM images are manually uploaded and stored in each
cloud, as in the case of OpenStack clouds, or retrieved from a
central image repository for Nimbus clouds. A collaboration
involving researchers on ATLAS at CERN and members of
this group have developed a set of Puppet scripts [10] that
provide site-speciﬁc contextualization [8]. In addition, the
Puppet scripts can conﬁgure a VM instance with the desired
application software suite, starting from a standard release
of Scientiﬁc or RedHat Linux. This is required for the clouds
that do not allow users to upload their own customized im-
ages. In this situation, an image provided by the cloud site
is booted and then conﬁgured at run-time for the task at
hand using Puppet.

The application software is stored in an HTTP read-only
ﬁle system called the CERN Virtual Machine File System
(CVMFS)[3]. CVMFS presents a remote directory as a local
ﬁle system, in which the client has read access to the project
application ﬁles. On the client’s ﬁrst access request, a ﬁle is
downloaded and cached locally, and all subsequent requests
for that ﬁle will use the cached copy. The ATLAS project
uses Squid [17] as an HTTP web cache for CVMFS; we will
describe the use of Squid caches in more detail in the next
section.

In addition, a Frontier database, developed for the HEP
community [4], is used for calibration data required during
the running of the application software by the ATLAS ex-
periment. All output data is written to the Victoria Storage
Element (SE) for both the ATLAS and Belle-II experiments.

3. CONTEXT-AWARE CLOUD DESIGN

The distributed cloud is performing well with the current
type of HEP applications and workload conditions. The
new design will include a system (currently under devel-
opment) to manage and distribute VM images to the dis-
tributed clouds. Further, it will include a more eﬃcient sys-
tem for distributing software that is starting to be used in a
production environment. We plan to use a federated storage
repository for input data sets and currently evaluating it in
a test bed. In the following sections, we describe our eﬀorts
in these key areas.
3.1

Image distribution system

In working with IaaS clouds that can instantiate images
stored remotely (such as Nimbus clouds), we have developed
a VM image repository, called Repoman [18]. Repoman op-
erates as a standalone system that exposes a RESTful web
service allowing users to upload their VM images to the
repository and conﬁgure various image properties such as

Figure 2: The upper plot shows the new software
distribution system. The VM periodically retrieves
a list of Squid caches from Shoal that is ordered by
location and load of the Squid. Each Squid cache
connects to the CVMFS repository at CERN. The
lower plot shows the federated storage system that
will be used for the input data samples. The VM
queries the Dynamic Storage Federator for the phys-
ical path to the nearest Storage Element (SE) with
the requested ﬁle. The Dynamic Storage Federa-
tor has a complete metadata catalog of the Storage
Elements (SE).

image metadata and access controls (the software can be
obtained from https://github.com/hep-gc/repoman). Us-
ing Repoman client tools, image snapshots can be saved to
the Repoman server from within the running image, which
greatly simpliﬁes the workﬂow for the end users. In addition
to being accessible via the Repoman client tools, each VM
image stored in Repoman is also accessible via HTTP(S) and
is easily retrieved using command-line tools such as wget and
curl. Repoman implements the concept of dual-hypervisor
VM images, allowing images containing older Linux kernels
to boot under both the KVM and Xen hypervisors. We are
unaware of another VM image repository that operates inde-
pendently of an IaaS cloud and also provides dual-hypervisor
capability.

With the emergence of clouds that require images to be
stored locally before instantiation (such as OpenStack), an
image propagation component is needed to automatically
deploy images to remote clouds. We are developing a ser-
vice, called glint, for managing images stored on multiple
clouds, concentrating on OpenStack clouds and the Glance
repository, but with a plugable architecture to allow support
for diﬀerent cloud types. Through a web browser interface,

Squid	  cache	  Squid	  cache	  Squid	  cache	  Squid	  cache	  	  	  CVMFS	  	  VM	  Shoal	  Get	  list	  of	  nearest	  Squids	  Squid	  cache	  	  VM	  SE	  SE	  SE	  SE	  Dynamic	  	  Storage	  Federator	  SE	  meta	  data	  Get	  nearest	  SE	  with	  the	  ﬁle	  HEP Computing in a Context-Aware Cloud Environment

Frank Berghaus, Ron Desmarais, Ian
Gable,Colin Leavett-Brown, Michael

Paterson, Randall J. Sobie, Ryan Taylor

Department of Physics and Astronomy,
University of Victoria, Victoria, Canada
Institute for Particle Physics, University of

Victoria, Victoria, Canada

frank@uvic,ca, rd@uvic.ca, igable@uvic.ca,
crlb@uvic.ca, mhp@uvic.ca, rsobie@uvic.ca,

rptaylor@uvic.ca

ABSTRACT
This paper describes the use of a distributed cloud comput-
ing system for high energy physics (HEP) applications. The
system is composed of IaaS clouds integrated into a uniﬁed
infrastructure that has been in production for over two years.
It continues to expand in scale and sites, encompassing more
than twenty clouds on three continents. We are prototyping
a new context-aware architecture that enables the virtual
machines to make connections to both software and data
repositories based on geolocation information. The new de-
sign will signiﬁcantly enhance the ability of the system to
scale to higher workloads and run data-intensive applica-
tions. We review the operation of the production system
and describe our work towards a context-aware cloud sys-
tem.

Categories and Subject Descriptors
C.2.4 [Distributed Systems]: Distributed Applications

Keywords
cloud computing, grid of clouds, scientiﬁc applications

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ScienceCloud’14, June xx, 2014, Vancouver, BC, Canada.
Copyright 2014 ACM 978-1-4503-1979-9/13/06 ...$15.00.

Andre Charbonneau

Research Computing Support

Shared Services Canada, Ottawa, Canada

Andre.Charbonneau@SSC-SPC.gc.ca

1.

INTRODUCTION

In a previous paper, we showed that a distributed cloud
computing system can be eﬀectively used for high-throughput
computing workloads for high energy physics (HEP) appli-
cations [12]. Since 2012, we have operated a distributed
cloud in a production mode, running over two million jobs,
with peak workloads of 1500 concurrent jobs. This system
is composed of a varying number of IaaS clouds, owned and
administered by academic institutions and national labora-
tories, and distributed across three continents. The exact
size of this uniﬁed infrastructure varies according to avail-
ability, maintenance and development cycles, but as many
as twenty ﬁve clouds have been employed at one time.

We have focused on running applications with no input
data requirements such as the simulation of the particle col-
lisions that are required for the development of analysis al-
gorithms and understanding the interaction of particles with
the detectors. The output from these applications is mod-
est and is written to a single site on each continent. The
applications running in the virtual machine (VM) instances
retrieve the HEP-speciﬁc software and calibration databases
from a few locations. The distributed cloud has been very
successful, as highlighted in our previous work [12], without
any scalability limits observed.

In order to make the system more versatile and to run
applications requiring large input data sets, we need to ad-
dress a number of issues. First, we want the VM instance
to access the closest software cache to reduce the demand
on the single site (eliminating a single point of failure), and
minimize the network latency.

Second, we wish to run applications that require moder-
ately large input data sets (20-100 GB for a single job over
a 12-hour period). The input data needs to be read in at a
high rate by the application. Hence we have selected pro-
tocols that cache or copy the data to the local disk rather
than streaming the data directly over the network. In addi-
tion, the job will query a federation storage system for the
nearest location of the input data and stage the data, using
high-speed transfer protocols, to the local disk.

We believe that these issues can be addressed if the dis-
tributed cloud computing system becomes context-aware.
Context-aware systems, that can sense and react to their
environment, are not new and are used extensively in the

Figure 1: A high-level view of the distributed cloud computing system as used by the ATLAS experiment
since 2012. The cloud batch system is contained in the outlined box where jobs are submitted by the ATLAS
user to HTCondor. CloudScheduler uses the list of jobs to request the start of VM instances on the available
IaaS clouds. The VM instances connect to the software repository, the application database and the data
repositories. The application database contains calibration constants associated with experimental apparatus
whereas the software repository has the application source code.

mobile phone industry. For example, context-aware appli-
cations use location to adapt interfaces, locate data, and
discover services for the user. In our case, a context-aware
cloud would dynamically locate the optimal locations for
retrieving the software and data. More speculatively, we
imagine the job or VM instance would establish a network
connection to those repositories using the emerging Software
Deﬁned Network technologies.

Section 2 of this paper describes the distributed cloud
computing system as it exists today, and highlights the ma-
jor components and operation. Section 3 describes the changes
needed to transition to a context-aware distributed cloud.
Section 4 provides a summary of this work.

2. DISTRIBUTED CLOUD SYSTEM

A full description of the distributed cloud computing sys-
tem can be found in [12]. The design is based on the original
concept of a ‘grid of clouds’ or ‘sky computing’ as a solution
for combining separate IaaS clouds into a uniﬁed infrastruc-
ture [6].

The distributed cloud is used for the ATLAS experiment
at the CERN Laboratory in Geneva by utilizing IaaS aca-
demic clouds in Canada, the United States, Europe and Aus-
tralia (see [12]). The majority of participating clouds are
conﬁgured with the OpenStack [9] software suite, though
some are still running Nimbus [7]. The use of the Nimbus

software on the IaaS clouds is expected to end in 2014 with
most clouds migrating to OpenStack.

At the heart of the distributed cloud computing system is
an HTCondor and CloudScheduler instance operating at the
University of Victoria. We have recently commissioned a
new, independent instance at CERN with its purpose being
to shift operations to the central location so that we can
take advantage of round-the-clock production support staﬀ.
The applicability of the distributed cloud computing model
is not limited to the ATLAS experiment. The Canadian
CANFAR project has created an independent system using
diﬀerent resources for the processing and analysis of astro-
nomical data [1].
In addition, we recently created a new
system for the Belle-II HEP experiment at the KEK Lab-
oratory in Japan. Each of the systems has their own HT-
Condor/CloudScheduler instance and utilize diﬀerent IaaS
clouds (though there are some shared clouds).

The goal is to run embarrassingly-parallel HEP applica-
tions on IaaS clouds. The application environment is com-
pletely provided by the VM image and distributed cloud
supporting services. Typically, jobs run on a single core for
approximately 6 − 12 hours.

Very recently the ATLAS experiment has developed an
application that uses 8-cores for a single job. Running multi-
core jobs in an 8-core VM is expected to have many beneﬁts
(reduced memory requirements, shared caches, reduced in-

ATLAS	  VM	  VM	  VM	  VM	  VM	  VM	  HTCondor	  Cloud	  Scheduler	  Run	  job	  Request	  VM	  So9ware	  repository	  Applica?on	  database	  Data	  repository	  put ﬁle staging space, and fewer jobs) and we expect to run
these jobs in the coming months.

In ﬁg. 1, we present a high-level view of the distributed
cloud. The scheduling of batch jobs is performed by HTCon-
dor [13] and the deployment of VM images is done by Cloud
Scheduler [2]. HTCondor was designed as a cycle scavenger,
making it an ideal job scheduler for a dynamic cloud envi-
ronment where VM instances appear and disappear based on
demand. Cloud Scheduler periodically reviews the require-
ments of the jobs in the HTCondor job queue and makes
requests to boot user-speciﬁc VM images on one of the IaaS
clouds. Once the VM image is booted, it attaches itself to
the HTCondor resource pool and is ready to accept jobs.
The instances are shut down when there are no jobs in the
HTCondor queue.

The VM images are manually uploaded and stored in each
cloud, as in the case of OpenStack clouds, or retrieved from a
central image repository for Nimbus clouds. A collaboration
involving researchers on ATLAS at CERN and members of
this group have developed a set of Puppet scripts [10] that
provide site-speciﬁc contextualization [8]. In addition, the
Puppet scripts can conﬁgure a VM instance with the desired
application software suite, starting from a standard release
of Scientiﬁc or RedHat Linux. This is required for the clouds
that do not allow users to upload their own customized im-
ages. In this situation, an image provided by the cloud site
is booted and then conﬁgured at run-time for the task at
hand using Puppet.

The application software is stored in an HTTP read-only
ﬁle system called the CERN Virtual Machine File System
(CVMFS)[3]. CVMFS presents a remote directory as a local
ﬁle system, in which the client has read access to the project
application ﬁles. On the client’s ﬁrst access request, a ﬁle is
downloaded and cached locally, and all subsequent requests
for that ﬁle will use the cached copy. The ATLAS project
uses Squid [17] as an HTTP web cache for CVMFS; we will
describe the use of Squid caches in more detail in the next
section.

In addition, a Frontier database, developed for the HEP
community [4], is used for calibration data required during
the running of the application software by the ATLAS ex-
periment. All output data is written to the Victoria Storage
Element (SE) for both the ATLAS and Belle-II experiments.

3. CONTEXT-AWARE CLOUD DESIGN

The distributed cloud is performing well with the current
type of HEP applications and workload conditions. The
new design will include a system (currently under devel-
opment) to manage and distribute VM images to the dis-
tributed clouds. Further, it will include a more eﬃcient sys-
tem for distributing software that is starting to be used in a
production environment. We plan to use a federated storage
repository for input data sets and currently evaluating it in
a test bed. In the following sections, we describe our eﬀorts
in these key areas.
3.1

Image distribution system

In working with IaaS clouds that can instantiate images
stored remotely (such as Nimbus clouds), we have developed
a VM image repository, called Repoman [18]. Repoman op-
erates as a standalone system that exposes a RESTful web
service allowing users to upload their VM images to the
repository and conﬁgure various image properties such as

Figure 2: The upper plot shows the new software
distribution system. The VM periodically retrieves
a list of Squid caches from Shoal that is ordered by
location and load of the Squid. Each Squid cache
connects to the CVMFS repository at CERN. The
lower plot shows the federated storage system that
will be used for the input data samples. The VM
queries the Dynamic Storage Federator for the phys-
ical path to the nearest Storage Element (SE) with
the requested ﬁle. The Dynamic Storage Federa-
tor has a complete metadata catalog of the Storage
Elements (SE).

image metadata and access controls (the software can be
obtained from https://github.com/hep-gc/repoman). Us-
ing Repoman client tools, image snapshots can be saved to
the Repoman server from within the running image, which
greatly simpliﬁes the workﬂow for the end users. In addition
to being accessible via the Repoman client tools, each VM
image stored in Repoman is also accessible via HTTP(S) and
is easily retrieved using command-line tools such as wget and
curl. Repoman implements the concept of dual-hypervisor
VM images, allowing images containing older Linux kernels
to boot under both the KVM and Xen hypervisors. We are
unaware of another VM image repository that operates inde-
pendently of an IaaS cloud and also provides dual-hypervisor
capability.

With the emergence of clouds that require images to be
stored locally before instantiation (such as OpenStack), an
image propagation component is needed to automatically
deploy images to remote clouds. We are developing a ser-
vice, called glint, for managing images stored on multiple
clouds, concentrating on OpenStack clouds and the Glance
repository, but with a plugable architecture to allow support
for diﬀerent cloud types. Through a web browser interface,

Squid	  cache	  Squid	  cache	  Squid	  cache	  Squid	  cache	  	  	  CVMFS	  	  VM	  Shoal	  Get	  list	  of	  nearest	  Squids	  Squid	  cache	  	  VM	  SE	  SE	  SE	  SE	  Dynamic	  	  Storage	  Federator	  SE	  meta	  data	  Get	  nearest	  SE	  with	  the	  ﬁle	  the user will identify clouds and be able to control the dis-
tribution of images.

Glint provides site management, image registration, cre-

currently have it set to once every 30 minutes. If a Squid
cache is no longer available, then the client fails over to the
next one in the list.

dential management and image deployment (see https://github.com/hep-
gc/glint). By design, glint has no pre-established list of
clouds. Instead the user must specify and provide their cre-
dentials for each cloud. The images to be distributed are
uploaded by the user to glint who then selects the target
clouds for each image. Image transfers are multithreaded to
reduce upload times and the user is notiﬁed once the trans-
fers are completed.

Recently, we added Squid caches in Geneva and the TRI-
UMF Laboratory in Vancouver to shoal and initial operation
has been successful. We are planning to expand the pool of
Squid caches as we gain further operational experience.

3.3 Data federation

The software is modular, making it simple to integrate
new cloud types such as Amazon EC2 or Google Compute
Engine. Extra functionality such as web-based image consis-
tency checks and improved propagation monitoring is planned
for future releases of glint.

3.2 Software repository

The CernVM ﬁle system (CVMFS) [3], supported with
Squid HTTP web caches, is an ideal solution for distributing
application software to VM instances. The software suite
for HEP projects, such as ATLAS or Belle-II, can be many
gigabytes in size but often only a small subset of the software
is used in a particular job. CVMFS signiﬁcantly reduces the
size of the HEP VM images by eliminating the need to store
the application ﬁle tree in the image. Our use of 8-core VM
instances, also allows the applications to share the CVMFS
cache within the VM instance further reducing the memory
requirements.

The master copy of the ATLAS and Belle-II software is
stored in the CVMFS repository at CERN. The HEP com-
munity uses Squid caches to distribute the workload of de-
livering the application software, minimizing the demand on
single, distant repositories.

For most of the recent operation, we have used the Squid
cache in Victoria for the cloud sites. The Victoria Squid
cache supports approximately 1000 cores of cloud and tradi-
tional computing resources. The number of client requests
to this Squid ranges from 6000 requests per minute on av-
erage to a peak of 130,000 requests per minute with the
rate of data transfer ranging from 2 MB/s to 16 MB/s, re-
spectively. The Squid server fetches information from the
CVMFS server at CERN at between 100 (average) and 1000
(peak) times per minute with the rate of 100 (average) to
4000 (peak) KB/s.

We view our reliance on a single Squid cache for the cloud
resources as a potential bottleneck as the system expands
to more sites. There are many Squid caches for the HEP
community distributed around the globe. We can distribute
the load and minimize the long distance transfers by having
the VM instances use the nearest Squid cache. There are no
services that provide this functionality, and, as a result, our
group has developed Shoal, which is a dynamic web cache
publishing service [16].

Shoal provides a reliable, scalable solution for a distributed
cloud environment and can manage large numbers of re-
quests (see https://github.com/hep-gc/shoal). Shoal builds
and maintains a list of Squid caches that advertise their ex-
istence to the central shoal-server using a shoal-agent. The
clients (VM instances) contact the server for a list of Squids
that is ordered on the relative location to the client and
the load of the Squid server (see Fig. 2). The frequency at
which the clients contact the server is conﬁgurable and we

Currently, the distributed cloud is used for ATLAS pro-
duction jobs that use little or no input data. The jobs write
their modestly-sized output to the ephemeral storage on the
VM, which is then transferred to the Victoria SE after the
job is completed.

We are evaluating various options for running jobs with
large input data sets. We observe that streaming the input
data from a remote location results in a large number of
failed jobs because of latency issues. Hence, we are only
considering solutions where the input data is staged to the
ephemeral disk of the VM instance.

In general, the input data used by each HEP job is inde-
pendent (typically 20-100 GB) and not shared between jobs.
The data staged by the VM is stored on the ephemeral disk
of the instance, which is physically attached to the node.
For completeness, we mention that the input/output data
are written in a HEP-speciﬁc format, called Root [11] and
transferred over the network with protocols such as XRootD
[19] or gridftp. However, the operation of the distributed
cloud does not have any dependence on the ﬁle format or
network protocol.

We have adopted WebDAV [14], a widely-used open stan-
dard using HTTP protocols, to present a ﬁle system to the
VM instances. DAVfs, a WebDAV client, presents a view of
the entire ﬁle system tree but only stages the ﬁle content as
it is accessed. Once the ﬁle is local, there are no longer any
latency issues. Like CVMFS, the WebDAV cache is shared
by all applications within the VM instance.

Currently, the VM instances retrieve the input data from
the Victoria SE using a WebDAV client. Using a single SE
for the entire system is a limiting element, especially for dis-
tant clouds. Further, the Victoria SE only hosts a subset
of the ATLAS data. Other SEs in Canada, for example, al-
low access to their storage by WebDAV and using these SEs
would increase the pool of available data as well as distribute
the ﬁle transfer workload.

A federated storage system is an ideal solution for our dis-
tributed cloud. The federated storage system would provide,
like CVMFS, a common view of the data ﬁle tree to the ap-
plication (see Fig. 2). The federated storage system would
direct the job to the nearest SE with the desired input data.
If multiple copies of the data reside on diﬀerent SEs, then
the selected SE would be determined by the geographical
location of the client with respect to the storage servers.

We are testing a new federated WebDAV system that
meets our design requirements [5]. There is an alternative
solution being developed internally by the ATLAS project
called FAX [15]. We favour the federated WebDAV solu-
tion as FAX is designed for a HEP environment. The use
of federated storage systems within HEP is relatively new
and both WebDAV and FAX are under evaluation. FAX is
being used in a pre-production deployment with a limited
number of sites [15]; currently there are no HEP federated
WebDAV deployments.

HEP Computing in a Context-Aware Cloud Environment

Frank Berghaus, Ron Desmarais, Ian
Gable,Colin Leavett-Brown, Michael

Paterson, Randall J. Sobie, Ryan Taylor

Department of Physics and Astronomy,
University of Victoria, Victoria, Canada
Institute for Particle Physics, University of

Victoria, Victoria, Canada

frank@uvic,ca, rd@uvic.ca, igable@uvic.ca,
crlb@uvic.ca, mhp@uvic.ca, rsobie@uvic.ca,

rptaylor@uvic.ca

ABSTRACT
This paper describes the use of a distributed cloud comput-
ing system for high energy physics (HEP) applications. The
system is composed of IaaS clouds integrated into a uniﬁed
infrastructure that has been in production for over two years.
It continues to expand in scale and sites, encompassing more
than twenty clouds on three continents. We are prototyping
a new context-aware architecture that enables the virtual
machines to make connections to both software and data
repositories based on geolocation information. The new de-
sign will signiﬁcantly enhance the ability of the system to
scale to higher workloads and run data-intensive applica-
tions. We review the operation of the production system
and describe our work towards a context-aware cloud sys-
tem.

Categories and Subject Descriptors
C.2.4 [Distributed Systems]: Distributed Applications

Keywords
cloud computing, grid of clouds, scientiﬁc applications

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ScienceCloud’14, June xx, 2014, Vancouver, BC, Canada.
Copyright 2014 ACM 978-1-4503-1979-9/13/06 ...$15.00.

Andre Charbonneau

Research Computing Support

Shared Services Canada, Ottawa, Canada

Andre.Charbonneau@SSC-SPC.gc.ca

1.

INTRODUCTION

In a previous paper, we showed that a distributed cloud
computing system can be eﬀectively used for high-throughput
computing workloads for high energy physics (HEP) appli-
cations [12]. Since 2012, we have operated a distributed
cloud in a production mode, running over two million jobs,
with peak workloads of 1500 concurrent jobs. This system
is composed of a varying number of IaaS clouds, owned and
administered by academic institutions and national labora-
tories, and distributed across three continents. The exact
size of this uniﬁed infrastructure varies according to avail-
ability, maintenance and development cycles, but as many
as twenty ﬁve clouds have been employed at one time.

We have focused on running applications with no input
data requirements such as the simulation of the particle col-
lisions that are required for the development of analysis al-
gorithms and understanding the interaction of particles with
the detectors. The output from these applications is mod-
est and is written to a single site on each continent. The
applications running in the virtual machine (VM) instances
retrieve the HEP-speciﬁc software and calibration databases
from a few locations. The distributed cloud has been very
successful, as highlighted in our previous work [12], without
any scalability limits observed.

In order to make the system more versatile and to run
applications requiring large input data sets, we need to ad-
dress a number of issues. First, we want the VM instance
to access the closest software cache to reduce the demand
on the single site (eliminating a single point of failure), and
minimize the network latency.

Second, we wish to run applications that require moder-
ately large input data sets (20-100 GB for a single job over
a 12-hour period). The input data needs to be read in at a
high rate by the application. Hence we have selected pro-
tocols that cache or copy the data to the local disk rather
than streaming the data directly over the network. In addi-
tion, the job will query a federation storage system for the
nearest location of the input data and stage the data, using
high-speed transfer protocols, to the local disk.

We believe that these issues can be addressed if the dis-
tributed cloud computing system becomes context-aware.
Context-aware systems, that can sense and react to their
environment, are not new and are used extensively in the

Figure 1: A high-level view of the distributed cloud computing system as used by the ATLAS experiment
since 2012. The cloud batch system is contained in the outlined box where jobs are submitted by the ATLAS
user to HTCondor. CloudScheduler uses the list of jobs to request the start of VM instances on the available
IaaS clouds. The VM instances connect to the software repository, the application database and the data
repositories. The application database contains calibration constants associated with experimental apparatus
whereas the software repository has the application source code.

mobile phone industry. For example, context-aware appli-
cations use location to adapt interfaces, locate data, and
discover services for the user. In our case, a context-aware
cloud would dynamically locate the optimal locations for
retrieving the software and data. More speculatively, we
imagine the job or VM instance would establish a network
connection to those repositories using the emerging Software
Deﬁned Network technologies.

Section 2 of this paper describes the distributed cloud
computing system as it exists today, and highlights the ma-
jor components and operation. Section 3 describes the changes
needed to transition to a context-aware distributed cloud.
Section 4 provides a summary of this work.

2. DISTRIBUTED CLOUD SYSTEM

A full description of the distributed cloud computing sys-
tem can be found in [12]. The design is based on the original
concept of a ‘grid of clouds’ or ‘sky computing’ as a solution
for combining separate IaaS clouds into a uniﬁed infrastruc-
ture [6].

The distributed cloud is used for the ATLAS experiment
at the CERN Laboratory in Geneva by utilizing IaaS aca-
demic clouds in Canada, the United States, Europe and Aus-
tralia (see [12]). The majority of participating clouds are
conﬁgured with the OpenStack [9] software suite, though
some are still running Nimbus [7]. The use of the Nimbus

software on the IaaS clouds is expected to end in 2014 with
most clouds migrating to OpenStack.

At the heart of the distributed cloud computing system is
an HTCondor and CloudScheduler instance operating at the
University of Victoria. We have recently commissioned a
new, independent instance at CERN with its purpose being
to shift operations to the central location so that we can
take advantage of round-the-clock production support staﬀ.
The applicability of the distributed cloud computing model
is not limited to the ATLAS experiment. The Canadian
CANFAR project has created an independent system using
diﬀerent resources for the processing and analysis of astro-
nomical data [1].
In addition, we recently created a new
system for the Belle-II HEP experiment at the KEK Lab-
oratory in Japan. Each of the systems has their own HT-
Condor/CloudScheduler instance and utilize diﬀerent IaaS
clouds (though there are some shared clouds).

The goal is to run embarrassingly-parallel HEP applica-
tions on IaaS clouds. The application environment is com-
pletely provided by the VM image and distributed cloud
supporting services. Typically, jobs run on a single core for
approximately 6 − 12 hours.

Very recently the ATLAS experiment has developed an
application that uses 8-cores for a single job. Running multi-
core jobs in an 8-core VM is expected to have many beneﬁts
(reduced memory requirements, shared caches, reduced in-

ATLAS	  VM	  VM	  VM	  VM	  VM	  VM	  HTCondor	  Cloud	  Scheduler	  Run	  job	  Request	  VM	  So9ware	  repository	  Applica?on	  database	  Data	  repository	  put ﬁle staging space, and fewer jobs) and we expect to run
these jobs in the coming months.

In ﬁg. 1, we present a high-level view of the distributed
cloud. The scheduling of batch jobs is performed by HTCon-
dor [13] and the deployment of VM images is done by Cloud
Scheduler [2]. HTCondor was designed as a cycle scavenger,
making it an ideal job scheduler for a dynamic cloud envi-
ronment where VM instances appear and disappear based on
demand. Cloud Scheduler periodically reviews the require-
ments of the jobs in the HTCondor job queue and makes
requests to boot user-speciﬁc VM images on one of the IaaS
clouds. Once the VM image is booted, it attaches itself to
the HTCondor resource pool and is ready to accept jobs.
The instances are shut down when there are no jobs in the
HTCondor queue.

The VM images are manually uploaded and stored in each
cloud, as in the case of OpenStack clouds, or retrieved from a
central image repository for Nimbus clouds. A collaboration
involving researchers on ATLAS at CERN and members of
this group have developed a set of Puppet scripts [10] that
provide site-speciﬁc contextualization [8]. In addition, the
Puppet scripts can conﬁgure a VM instance with the desired
application software suite, starting from a standard release
of Scientiﬁc or RedHat Linux. This is required for the clouds
that do not allow users to upload their own customized im-
ages. In this situation, an image provided by the cloud site
is booted and then conﬁgured at run-time for the task at
hand using Puppet.

The application software is stored in an HTTP read-only
ﬁle system called the CERN Virtual Machine File System
(CVMFS)[3]. CVMFS presents a remote directory as a local
ﬁle system, in which the client has read access to the project
application ﬁles. On the client’s ﬁrst access request, a ﬁle is
downloaded and cached locally, and all subsequent requests
for that ﬁle will use the cached copy. The ATLAS project
uses Squid [17] as an HTTP web cache for CVMFS; we will
describe the use of Squid caches in more detail in the next
section.

In addition, a Frontier database, developed for the HEP
community [4], is used for calibration data required during
the running of the application software by the ATLAS ex-
periment. All output data is written to the Victoria Storage
Element (SE) for both the ATLAS and Belle-II experiments.

3. CONTEXT-AWARE CLOUD DESIGN

The distributed cloud is performing well with the current
type of HEP applications and workload conditions. The
new design will include a system (currently under devel-
opment) to manage and distribute VM images to the dis-
tributed clouds. Further, it will include a more eﬃcient sys-
tem for distributing software that is starting to be used in a
production environment. We plan to use a federated storage
repository for input data sets and currently evaluating it in
a test bed. In the following sections, we describe our eﬀorts
in these key areas.
3.1

Image distribution system

In working with IaaS clouds that can instantiate images
stored remotely (such as Nimbus clouds), we have developed
a VM image repository, called Repoman [18]. Repoman op-
erates as a standalone system that exposes a RESTful web
service allowing users to upload their VM images to the
repository and conﬁgure various image properties such as

Figure 2: The upper plot shows the new software
distribution system. The VM periodically retrieves
a list of Squid caches from Shoal that is ordered by
location and load of the Squid. Each Squid cache
connects to the CVMFS repository at CERN. The
lower plot shows the federated storage system that
will be used for the input data samples. The VM
queries the Dynamic Storage Federator for the phys-
ical path to the nearest Storage Element (SE) with
the requested ﬁle. The Dynamic Storage Federa-
tor has a complete metadata catalog of the Storage
Elements (SE).

image metadata and access controls (the software can be
obtained from https://github.com/hep-gc/repoman). Us-
ing Repoman client tools, image snapshots can be saved to
the Repoman server from within the running image, which
greatly simpliﬁes the workﬂow for the end users. In addition
to being accessible via the Repoman client tools, each VM
image stored in Repoman is also accessible via HTTP(S) and
is easily retrieved using command-line tools such as wget and
curl. Repoman implements the concept of dual-hypervisor
VM images, allowing images containing older Linux kernels
to boot under both the KVM and Xen hypervisors. We are
unaware of another VM image repository that operates inde-
pendently of an IaaS cloud and also provides dual-hypervisor
capability.

With the emergence of clouds that require images to be
stored locally before instantiation (such as OpenStack), an
image propagation component is needed to automatically
deploy images to remote clouds. We are developing a ser-
vice, called glint, for managing images stored on multiple
clouds, concentrating on OpenStack clouds and the Glance
repository, but with a plugable architecture to allow support
for diﬀerent cloud types. Through a web browser interface,

Squid	  cache	  Squid	  cache	  Squid	  cache	  Squid	  cache	  	  	  CVMFS	  	  VM	  Shoal	  Get	  list	  of	  nearest	  Squids	  Squid	  cache	  	  VM	  SE	  SE	  SE	  SE	  Dynamic	  	  Storage	  Federator	  SE	  meta	  data	  Get	  nearest	  SE	  with	  the	  ﬁle	  the user will identify clouds and be able to control the dis-
tribution of images.

Glint provides site management, image registration, cre-

currently have it set to once every 30 minutes. If a Squid
cache is no longer available, then the client fails over to the
next one in the list.

dential management and image deployment (see https://github.com/hep-
gc/glint). By design, glint has no pre-established list of
clouds. Instead the user must specify and provide their cre-
dentials for each cloud. The images to be distributed are
uploaded by the user to glint who then selects the target
clouds for each image. Image transfers are multithreaded to
reduce upload times and the user is notiﬁed once the trans-
fers are completed.

Recently, we added Squid caches in Geneva and the TRI-
UMF Laboratory in Vancouver to shoal and initial operation
has been successful. We are planning to expand the pool of
Squid caches as we gain further operational experience.

3.3 Data federation

The software is modular, making it simple to integrate
new cloud types such as Amazon EC2 or Google Compute
Engine. Extra functionality such as web-based image consis-
tency checks and improved propagation monitoring is planned
for future releases of glint.

3.2 Software repository

The CernVM ﬁle system (CVMFS) [3], supported with
Squid HTTP web caches, is an ideal solution for distributing
application software to VM instances. The software suite
for HEP projects, such as ATLAS or Belle-II, can be many
gigabytes in size but often only a small subset of the software
is used in a particular job. CVMFS signiﬁcantly reduces the
size of the HEP VM images by eliminating the need to store
the application ﬁle tree in the image. Our use of 8-core VM
instances, also allows the applications to share the CVMFS
cache within the VM instance further reducing the memory
requirements.

The master copy of the ATLAS and Belle-II software is
stored in the CVMFS repository at CERN. The HEP com-
munity uses Squid caches to distribute the workload of de-
livering the application software, minimizing the demand on
single, distant repositories.

For most of the recent operation, we have used the Squid
cache in Victoria for the cloud sites. The Victoria Squid
cache supports approximately 1000 cores of cloud and tradi-
tional computing resources. The number of client requests
to this Squid ranges from 6000 requests per minute on av-
erage to a peak of 130,000 requests per minute with the
rate of data transfer ranging from 2 MB/s to 16 MB/s, re-
spectively. The Squid server fetches information from the
CVMFS server at CERN at between 100 (average) and 1000
(peak) times per minute with the rate of 100 (average) to
4000 (peak) KB/s.

We view our reliance on a single Squid cache for the cloud
resources as a potential bottleneck as the system expands
to more sites. There are many Squid caches for the HEP
community distributed around the globe. We can distribute
the load and minimize the long distance transfers by having
the VM instances use the nearest Squid cache. There are no
services that provide this functionality, and, as a result, our
group has developed Shoal, which is a dynamic web cache
publishing service [16].

Shoal provides a reliable, scalable solution for a distributed
cloud environment and can manage large numbers of re-
quests (see https://github.com/hep-gc/shoal). Shoal builds
and maintains a list of Squid caches that advertise their ex-
istence to the central shoal-server using a shoal-agent. The
clients (VM instances) contact the server for a list of Squids
that is ordered on the relative location to the client and
the load of the Squid server (see Fig. 2). The frequency at
which the clients contact the server is conﬁgurable and we

Currently, the distributed cloud is used for ATLAS pro-
duction jobs that use little or no input data. The jobs write
their modestly-sized output to the ephemeral storage on the
VM, which is then transferred to the Victoria SE after the
job is completed.

We are evaluating various options for running jobs with
large input data sets. We observe that streaming the input
data from a remote location results in a large number of
failed jobs because of latency issues. Hence, we are only
considering solutions where the input data is staged to the
ephemeral disk of the VM instance.

In general, the input data used by each HEP job is inde-
pendent (typically 20-100 GB) and not shared between jobs.
The data staged by the VM is stored on the ephemeral disk
of the instance, which is physically attached to the node.
For completeness, we mention that the input/output data
are written in a HEP-speciﬁc format, called Root [11] and
transferred over the network with protocols such as XRootD
[19] or gridftp. However, the operation of the distributed
cloud does not have any dependence on the ﬁle format or
network protocol.

We have adopted WebDAV [14], a widely-used open stan-
dard using HTTP protocols, to present a ﬁle system to the
VM instances. DAVfs, a WebDAV client, presents a view of
the entire ﬁle system tree but only stages the ﬁle content as
it is accessed. Once the ﬁle is local, there are no longer any
latency issues. Like CVMFS, the WebDAV cache is shared
by all applications within the VM instance.

Currently, the VM instances retrieve the input data from
the Victoria SE using a WebDAV client. Using a single SE
for the entire system is a limiting element, especially for dis-
tant clouds. Further, the Victoria SE only hosts a subset
of the ATLAS data. Other SEs in Canada, for example, al-
low access to their storage by WebDAV and using these SEs
would increase the pool of available data as well as distribute
the ﬁle transfer workload.

A federated storage system is an ideal solution for our dis-
tributed cloud. The federated storage system would provide,
like CVMFS, a common view of the data ﬁle tree to the ap-
plication (see Fig. 2). The federated storage system would
direct the job to the nearest SE with the desired input data.
If multiple copies of the data reside on diﬀerent SEs, then
the selected SE would be determined by the geographical
location of the client with respect to the storage servers.

We are testing a new federated WebDAV system that
meets our design requirements [5]. There is an alternative
solution being developed internally by the ATLAS project
called FAX [15]. We favour the federated WebDAV solu-
tion as FAX is designed for a HEP environment. The use
of federated storage systems within HEP is relatively new
and both WebDAV and FAX are under evaluation. FAX is
being used in a pre-production deployment with a limited
number of sites [15]; currently there are no HEP federated
WebDAV deployments.

[9] OpenStack: Open source software for building clouds.

http://www.openstack.org.
[10] Puppet Open Source. http:

//puppetlabs.com/puppet/puppet-open-source.
[11] Root: a framework for the analysis of HEP data.

http://root.cern.ch.

[12] Randall Sobie, Ashok Agarwal, Ian Gable, Colin

Leavett-Brown, Michael Paterson, Ryan Taylor, Andre
Charbonneau, Roger Impey, and Wayne Podiama.
2013. HTC scientiﬁc computing in a distributed cloud
environment. In Proceedings of the 4th ACM
workshop on Scientiﬁc cloud computing (Science
Cloud ’13). ACM, New York, NY, USA, 45-52.
DOI=10.1145/2465848.2465850
http://doi.acm.org/10.1145/2465848.2465850

[13] D. Thain, T. Tannenbaum, and M. Livny. Distributed

computing in practice: the HTCondor experience.
Concurr. Comput. : Pract. Exper., 17(2-4):323–356,
Feb. 2005.

[14] Web Distributed Authoring and Versioning

(WebDAV). http://tools.ietf.org/html/rfc4918.

[15] I. Vukotic et al.. Data federation strategies for ATLAS

using XRootD. Proceedings of the CHEP 2013
Conference, Amsterdam 2013.

[16] I Gable et al.. Dynamic web cache publishing for IaaS

clouds using Shoal, Proceedings of the CHEP 2013
Conference, Amsterdam 2013. arXiv:1311.0058 [cs.DC]

[17] Squids: a caching proxy for the Web.

http://www.squid-cache.org.

[18] M Vliet et al.. Repoman: A Simple RESTful X.509

Virtual Machine Image Repository. Proceedings of the
International Symposium on Grids and Clouds ,
Taipei, March 2011

[19] XRootD: a high-performance protocol for accessing

HEP data. http://xrootd.org.

We have established a prototype federated WebDAV sys-
tem with three storage resources (including the Victoria SE)
and the basic functionality tests have been successful. Cur-
rently, we are testing its operation and performance. Our
next step is to evaluate the federated storage system in a
more realistic environment using real ATLAS applications.
We plan to establish a small distributed cloud test system
with resources at both ends of the continent. We will mea-
sure the performance and reliability of the WebDAV feder-
ator, and present the results at the workshop.
4. SUMMARY

We have described our activities and plans to evolve a pro-
duction distributed cloud computing system to a context-
aware design. The goal is for the VM instances to locate
and retrieve the software and input data based on their ge-
ographic location, network and system loads. Many of the
elements of the software distribution system are in place
and being integrated into the production system. The use
of federated data repositories will require signiﬁcant testing
and evaluation; however, the outlook for this solution is very
positive.
Acknowledgements
The support of CANARIE, the Natural Sciences and En-
gineering Research Council, and FutureGrid are acknowl-
edged.
5. REFERENCES
[1] CANFAR: Canadian Advanced Network for

Astronomical Research.
https://wiki.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/
canfar/index.php/Introduction

[2] P. Armstrong, A. Agarwal, A. Bishop,

A. Charbonneau, R. Desmarais, K. Fransham, N. Hill,
I.Gable, S.Gaudet, S.Goliath, R.Impey,
C. Leavett-Brown, J. Ouellete, M. Paterson,
C. Pritchet, D. Penfold-Brown, W. Podaima,
D. Schade, and R. Sobie. Cloud Scheduler: a resource
manager for a distributed compute cloud.
arXiv:1007.0050v1 [cs.DC].

[3] J. Blomer, P. Buncic, and T. Fuhrmann. CernVM-FS:

delivering scientiﬁc software to globally distributed
computing resources. In Proceedings of the ﬁrst
international workshop on Network-aware data
management, NDM ’11, pages 49–56, New York, NY,
USA, 2011. ACM.

[4] Frontier distributed database caching system.

http://frontier.cern.ch/.

[5] F. Furano et al.. Dynamic federations: storage

aggregation using open tools and protocols, Journal of
Physics: Conference Series 396 (2012) 032042.
doi:10.1088/1742-6596/396/3/032042

[6] Katarzyna Keahey, Mauricio Tsugawa, Andrea
Matsunaga, Jose Fortes, Sky Computing, IEEE
Internet Computing, vol. 13, no. 5, pp. 43-51, 2009.

[7] Nimbus: an open-source EC2/S3 compatible IaaS

implementation. http://www.nimbusproject.org/.

[8] H. Ohman et al.. Using Puppet to contextualize

computing resources for ATLAS analysis on Google
Compute Engine. Proceedings of the CHEP 2013
Conference, Amsterdam 2013.

