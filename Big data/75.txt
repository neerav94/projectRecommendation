epiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

epiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgepiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itepiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itFigure 5: The Programming Stack of epiC

for running the written program on the epiC’s common concurrent
runtime system.

We point out that the data processing model is problem domain
speciﬁc. For example, a MapReduce model is best suited for pro-
cessing unstructured data, a relational model is best suited for struc-
tured data and a graph model is best suited for graph data. The
common requirement is that programs written with these models
are all needed to be parallelized. Since Big Data is inherently multi-
structured, we build an Actor-like concurrent programming model
for a common runtime framework and offer epiC extensions for
users to specify domain speciﬁc data manipulation operations for
each data type. In the previous section, we have introduced the ba-
sic programming model of epiC. In this section, we focus on two
customized data processing model, the MapReduce model and re-
lational model. We will show how to implement them on top of
epiC.
3.1 The MapReduce Extension

We ﬁrst consider the MapReduce framework, and extend it to
work with epiC’s runtime framework. The MapReduce data pro-
cessing model consists of two interfaces:

(k1, v1)
(k2, list(v2)) ! list(v2)

map
reduce
Our MapReduce extension reuses Hadoop’s implementation of
these interfaces and other useful functions such as the partition.
This section only describes the auxiliary support which enables
users to run MapReduce programs on epiC and our own optimiza-
tions which are not supported in Hadoop.

! list(k2, v2)

3.1.1 General Abstractions
Running MapReduce on top of epiC is straightforward. We ﬁrst
place the map() function in a map unit and the reduce() func-
tion in a reduce unit. Then, we instantiate M map units and R
reduce units. The master network assigns a unique namespace to
each map and reduce unit. In the simplest case, the name addresses
of the units are like “x@MapUnit” and “y@ReduceUnit”, where
0 (cid:20) x < M and 0 (cid:20) y < R.

Based on the namespace, the MapUnit loads a partition of in-
put data and applies the customized map() function to process it.
The results are a set of key-value pairs. Here, a partition()
function is required to split the key-value pairs into multiple HDFS
ﬁles. Based on the application’s requirement, the partition()
can choose to sort the data by keys. By default, the partition()
simply applies the hash function to generate R ﬁles and assigns a
namespace to each ﬁle. The meta-data of the HDFS ﬁles are com-
posed into a message, which is sent to the master network. The
recipient is speciﬁed as all the ReduceUnit.

The master network then collects the messages from all MapUnits

and broadcasts them to the ReduceUnit. When a ReduceUnit
starts up, it loads the HDFS ﬁles that share the same namespace
with it. A possible merge-sort is required, if the results should be
sorted. Then, the customized reduce() function is invoked to

generate the ﬁnal results.

class Map implements Mapper {

void map() {
}

}
class Reduce implements Reducer {

void reduce() {
}

}
class MapUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new HdfsReader(s);
MapRunner map = new MapRunner(reader, Map());
map.run();
o.sendMessage("*@ReduceUnit",
map.getOutputMessage());

}

}
class ReduceUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
ReduceRunner red = new ReduceRunner(in,

Reduce());

red.run();

}

}

Here, we highlight the advantage of our design decision to de-
couple the data processing model and the concurrent programming
model. Suppose we want to extend the MapReduce programming
model to the Map-Reduce-Merge programming model [30]. All
we need to do is to add a new unit mergeUnit() and modify
the codes in the ReduceUnit to send messages to the master net-
work for declaring its output ﬁles. Compared to this non-intrusive
scheme, Hadoop needs to make dramatic changes to its runtime
system to support the same functionality [30] since Hadoop’s de-
sign bundles data processing model with concurrent programming
model.
3.1.2 Optimizations for MapReduce
In addition to the basic MapReduce implementation which is
similar to Hadoop, we add an optimization for map unit data pro-
cessing. We found that the map unit computation is CPU-bound
instead of I/O bound. The high CPU cost comes from the ﬁnal
sorting phase.

The Map unit needs to sort the intermediate key-value pairs since
MapReduce requires the reduce function to process key-value pairs
in an increasing order. Sorting in MapReduce is expensive since 1)
the sorting algorithm (i.e., quick sort) itself is CPU intensive and
2) the data de-serialization cost is not negligible. We employ two
techniques to improve the map unit sorting performance: 1) order-
preserving serialization and 2) high performance string sort (i.e.,
burst sort).

Deﬁnition 1. For a data type T , an order-preserving serialization
is an encoding scheme which serializes a variable x 2 T to a string
sx such that, for any two variables x 2 T and y 2 T , if x < y then
sx < sy in string lexicographical order.

In other words, the order-preserving serialization scheme serial-
izes keys so that the keys can be ordered by directly sorting their se-
rialized strings (in string lexicographical order) without de-serialization.
Note that the order-preserving serialization scheme exists for all
Java built-in data types.

(cid:258)(cid:258)(cid:38)(cid:82)(cid:81)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:53)(cid:88)(cid:81)(cid:87)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)epiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itFigure 5: The Programming Stack of epiC

for running the written program on the epiC’s common concurrent
runtime system.

We point out that the data processing model is problem domain
speciﬁc. For example, a MapReduce model is best suited for pro-
cessing unstructured data, a relational model is best suited for struc-
tured data and a graph model is best suited for graph data. The
common requirement is that programs written with these models
are all needed to be parallelized. Since Big Data is inherently multi-
structured, we build an Actor-like concurrent programming model
for a common runtime framework and offer epiC extensions for
users to specify domain speciﬁc data manipulation operations for
each data type. In the previous section, we have introduced the ba-
sic programming model of epiC. In this section, we focus on two
customized data processing model, the MapReduce model and re-
lational model. We will show how to implement them on top of
epiC.
3.1 The MapReduce Extension

We ﬁrst consider the MapReduce framework, and extend it to
work with epiC’s runtime framework. The MapReduce data pro-
cessing model consists of two interfaces:

(k1, v1)
(k2, list(v2)) ! list(v2)

map
reduce
Our MapReduce extension reuses Hadoop’s implementation of
these interfaces and other useful functions such as the partition.
This section only describes the auxiliary support which enables
users to run MapReduce programs on epiC and our own optimiza-
tions which are not supported in Hadoop.

! list(k2, v2)

3.1.1 General Abstractions
Running MapReduce on top of epiC is straightforward. We ﬁrst
place the map() function in a map unit and the reduce() func-
tion in a reduce unit. Then, we instantiate M map units and R
reduce units. The master network assigns a unique namespace to
each map and reduce unit. In the simplest case, the name addresses
of the units are like “x@MapUnit” and “y@ReduceUnit”, where
0 (cid:20) x < M and 0 (cid:20) y < R.

Based on the namespace, the MapUnit loads a partition of in-
put data and applies the customized map() function to process it.
The results are a set of key-value pairs. Here, a partition()
function is required to split the key-value pairs into multiple HDFS
ﬁles. Based on the application’s requirement, the partition()
can choose to sort the data by keys. By default, the partition()
simply applies the hash function to generate R ﬁles and assigns a
namespace to each ﬁle. The meta-data of the HDFS ﬁles are com-
posed into a message, which is sent to the master network. The
recipient is speciﬁed as all the ReduceUnit.

The master network then collects the messages from all MapUnits

and broadcasts them to the ReduceUnit. When a ReduceUnit
starts up, it loads the HDFS ﬁles that share the same namespace
with it. A possible merge-sort is required, if the results should be
sorted. Then, the customized reduce() function is invoked to

generate the ﬁnal results.

class Map implements Mapper {

void map() {
}

}
class Reduce implements Reducer {

void reduce() {
}

}
class MapUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new HdfsReader(s);
MapRunner map = new MapRunner(reader, Map());
map.run();
o.sendMessage("*@ReduceUnit",
map.getOutputMessage());

}

}
class ReduceUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
ReduceRunner red = new ReduceRunner(in,

Reduce());

red.run();

}

}

Here, we highlight the advantage of our design decision to de-
couple the data processing model and the concurrent programming
model. Suppose we want to extend the MapReduce programming
model to the Map-Reduce-Merge programming model [30]. All
we need to do is to add a new unit mergeUnit() and modify
the codes in the ReduceUnit to send messages to the master net-
work for declaring its output ﬁles. Compared to this non-intrusive
scheme, Hadoop needs to make dramatic changes to its runtime
system to support the same functionality [30] since Hadoop’s de-
sign bundles data processing model with concurrent programming
model.
3.1.2 Optimizations for MapReduce
In addition to the basic MapReduce implementation which is
similar to Hadoop, we add an optimization for map unit data pro-
cessing. We found that the map unit computation is CPU-bound
instead of I/O bound. The high CPU cost comes from the ﬁnal
sorting phase.

The Map unit needs to sort the intermediate key-value pairs since
MapReduce requires the reduce function to process key-value pairs
in an increasing order. Sorting in MapReduce is expensive since 1)
the sorting algorithm (i.e., quick sort) itself is CPU intensive and
2) the data de-serialization cost is not negligible. We employ two
techniques to improve the map unit sorting performance: 1) order-
preserving serialization and 2) high performance string sort (i.e.,
burst sort).

Deﬁnition 1. For a data type T , an order-preserving serialization
is an encoding scheme which serializes a variable x 2 T to a string
sx such that, for any two variables x 2 T and y 2 T , if x < y then
sx < sy in string lexicographical order.

In other words, the order-preserving serialization scheme serial-
izes keys so that the keys can be ordered by directly sorting their se-
rialized strings (in string lexicographical order) without de-serialization.
Note that the order-preserving serialization scheme exists for all
Java built-in data types.

(cid:258)(cid:258)(cid:38)(cid:82)(cid:81)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:53)(cid:88)(cid:81)(cid:87)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)}
class SingleTableUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new TableReader(s);
EmbededDBEngine

e =

new EmbededDBEngine(reader, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class JoinUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s1 = m[r.getNameAddress(LEFT\_TABLE)];
InputSplit s2 = m[r.getNameAddress(RIGHT\_TABLE)];
Reader in1 = new MapOutputReader(s1);
Reader in2 = new MapOutputReader(s2);
EmbededDBEngine e =

new EmbededDBEngine(in1, in2, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class AggregateUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
EmbededDBEngine e =

new EmbededDBEngine(in, getQuery());

e.process();

}

}

The abstractions are straightforward and we discard the detailed
discussion.
In each unit, we embed a customized query engine,
which can process single table queries, join queries and aggrega-
tions. We have not speciﬁed the recipients of each message in the
unit abstraction. This must be implemented by users for different
queries. However, as discussed later, we provide a query optimizer
to automatically ﬁll in the recipients. To show how users can adopt
the above relational model to process queries, let us consider the
following query (a variant of TPC-H Q3):

SELECT l orderkey, sum(l extendedprice*(1-l discount))

as revenue, o orderdate, o shippriority

FROM customer, orders, lineitem
WHERE c mktsegment = ’:1’ and c custkey = o custkey

and l orderkey = o orderkey and o orderdate
< date ’:2’ and l shipdate > date ’:2’

Group By o orderdate, o shippriority

Figure 6 to Figure 10 illustrate the processing of Q3 in epiC.
In step 1 (Figure 6), three types of the SingleTableUnits are
started to process the select/project operators of Lineitem, Orders
and Customer respectively. Note that those SingleTableUnits
run the same code. The only differences are their name addresses
and processed queries. The results are written back to the storage
system (either HDFS or distributed database). The meta-data of the
corresponding ﬁles are forwarded to the JoinUnits.

In step 2 and step 3 (Figures 7 and 8), we apply the hash-join
approach to process the data. In previous SingleTableUnits,
the output data are partitioned by the join keys. So the JoinUnit
can selectively load the paired partitions to perform the join. We
will discuss other possible join implementations in the next section.

Figure 6: Step 1 of Q3

We adopt burst sort algorithm to order the serialized strings. We
choose burst sort as our sorting technique since it is specially de-
signed for sorting large string collections and has been shown to
be signiﬁcantly faster than other candidates [25]. We brieﬂy out-
line the algorithm here. Interested readers should refer to [25] for
details. The burst sort technique sorts a string collection in two
passes. In the ﬁrst pass, the algorithm processes each input string
and stores the pointer of each string into a leaf node (bucket) in
a burst trie. The burst trie has a nice property that all leaf nodes
(buckets) are ordered. Thus, in the second pass, the algorithm pro-
cesses each bucket in order, applies a standard sorting technique
such as quick sort to sort strings, and produces the ﬁnal results.
The original burst sort requires a lot of additional memory to hold
the trie structure and thus does not scale well to a very large string
collection. We, thus, developed a memory efﬁcient burst sort im-
plementation which requires only 2 bits of additional space for each
key entry. We also use the multi-key quick sort algorithm [5] to sort
strings resided in the same bucket.

Combining the two techniques (i.e., order-preserving serializa-
tion and burst sort), our sorting scheme outperforms Hadoop’s quick
sort implementation by a factor of three to four.
3.2 Relational Model Extension

As pointed out earlier, for structured data, the relational data pro-
cessing model is most suited. Like the MapReduce extensions, we
can implement the relational model on top of epiC.
3.2.1 General Abstractions
Currently, three core units (SingleTableUnit, JoinUnit
and AggregateUnit) are deﬁned for the relational model. They
are capable of handling non-nested SQL queries. The SingleTableUnit
processes queries that involve only a partition of a single table.
The JoinUnit reads partitions from two tables and merge them
into one partition of the join table. Finally, the AggregateUnit
collects the partitions of different groups and computes the aggre-
gation results for each group. The abstractions of these units are
shown below. Currently, we adopt the synchronization model as
in MapReduce. Namely, we will start the next types of units, only
when all current units complete their processing. We will study
the possibility of creating a pipeline model in future work. Due to
space limitation, we only show the most important part.

class SingleTableQuery implements DBQuery {

void getQuery() {
}

}
class JoinQuery implements DBQuery {

void getQuery() {
}

}
class AggregateQuery implements DBQuery {

void getQuery() {
}

SingleTableUnitSingleTableUnitSingleTableUnitRelational Data of Customer/Orders/Lineitemselect c_custkey from Customer where c_mktsegment = ':1'select o_orderdate, o_custkey, o_orderkey, o_shippriority from Orders where o_orderdate < date ':2'select l_orderkey, l_extendedprice, l_discount from Lineitem where l_shipdate > date ':2'Master networkPartition info of CustomerPartition info of OrdersPartition info of LineitemepiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itFigure 5: The Programming Stack of epiC

for running the written program on the epiC’s common concurrent
runtime system.

We point out that the data processing model is problem domain
speciﬁc. For example, a MapReduce model is best suited for pro-
cessing unstructured data, a relational model is best suited for struc-
tured data and a graph model is best suited for graph data. The
common requirement is that programs written with these models
are all needed to be parallelized. Since Big Data is inherently multi-
structured, we build an Actor-like concurrent programming model
for a common runtime framework and offer epiC extensions for
users to specify domain speciﬁc data manipulation operations for
each data type. In the previous section, we have introduced the ba-
sic programming model of epiC. In this section, we focus on two
customized data processing model, the MapReduce model and re-
lational model. We will show how to implement them on top of
epiC.
3.1 The MapReduce Extension

We ﬁrst consider the MapReduce framework, and extend it to
work with epiC’s runtime framework. The MapReduce data pro-
cessing model consists of two interfaces:

(k1, v1)
(k2, list(v2)) ! list(v2)

map
reduce
Our MapReduce extension reuses Hadoop’s implementation of
these interfaces and other useful functions such as the partition.
This section only describes the auxiliary support which enables
users to run MapReduce programs on epiC and our own optimiza-
tions which are not supported in Hadoop.

! list(k2, v2)

3.1.1 General Abstractions
Running MapReduce on top of epiC is straightforward. We ﬁrst
place the map() function in a map unit and the reduce() func-
tion in a reduce unit. Then, we instantiate M map units and R
reduce units. The master network assigns a unique namespace to
each map and reduce unit. In the simplest case, the name addresses
of the units are like “x@MapUnit” and “y@ReduceUnit”, where
0 (cid:20) x < M and 0 (cid:20) y < R.

Based on the namespace, the MapUnit loads a partition of in-
put data and applies the customized map() function to process it.
The results are a set of key-value pairs. Here, a partition()
function is required to split the key-value pairs into multiple HDFS
ﬁles. Based on the application’s requirement, the partition()
can choose to sort the data by keys. By default, the partition()
simply applies the hash function to generate R ﬁles and assigns a
namespace to each ﬁle. The meta-data of the HDFS ﬁles are com-
posed into a message, which is sent to the master network. The
recipient is speciﬁed as all the ReduceUnit.

The master network then collects the messages from all MapUnits

and broadcasts them to the ReduceUnit. When a ReduceUnit
starts up, it loads the HDFS ﬁles that share the same namespace
with it. A possible merge-sort is required, if the results should be
sorted. Then, the customized reduce() function is invoked to

generate the ﬁnal results.

class Map implements Mapper {

void map() {
}

}
class Reduce implements Reducer {

void reduce() {
}

}
class MapUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new HdfsReader(s);
MapRunner map = new MapRunner(reader, Map());
map.run();
o.sendMessage("*@ReduceUnit",
map.getOutputMessage());

}

}
class ReduceUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
ReduceRunner red = new ReduceRunner(in,

Reduce());

red.run();

}

}

Here, we highlight the advantage of our design decision to de-
couple the data processing model and the concurrent programming
model. Suppose we want to extend the MapReduce programming
model to the Map-Reduce-Merge programming model [30]. All
we need to do is to add a new unit mergeUnit() and modify
the codes in the ReduceUnit to send messages to the master net-
work for declaring its output ﬁles. Compared to this non-intrusive
scheme, Hadoop needs to make dramatic changes to its runtime
system to support the same functionality [30] since Hadoop’s de-
sign bundles data processing model with concurrent programming
model.
3.1.2 Optimizations for MapReduce
In addition to the basic MapReduce implementation which is
similar to Hadoop, we add an optimization for map unit data pro-
cessing. We found that the map unit computation is CPU-bound
instead of I/O bound. The high CPU cost comes from the ﬁnal
sorting phase.

The Map unit needs to sort the intermediate key-value pairs since
MapReduce requires the reduce function to process key-value pairs
in an increasing order. Sorting in MapReduce is expensive since 1)
the sorting algorithm (i.e., quick sort) itself is CPU intensive and
2) the data de-serialization cost is not negligible. We employ two
techniques to improve the map unit sorting performance: 1) order-
preserving serialization and 2) high performance string sort (i.e.,
burst sort).

Deﬁnition 1. For a data type T , an order-preserving serialization
is an encoding scheme which serializes a variable x 2 T to a string
sx such that, for any two variables x 2 T and y 2 T , if x < y then
sx < sy in string lexicographical order.

In other words, the order-preserving serialization scheme serial-
izes keys so that the keys can be ordered by directly sorting their se-
rialized strings (in string lexicographical order) without de-serialization.
Note that the order-preserving serialization scheme exists for all
Java built-in data types.

(cid:258)(cid:258)(cid:38)(cid:82)(cid:81)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:53)(cid:88)(cid:81)(cid:87)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)}
class SingleTableUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new TableReader(s);
EmbededDBEngine

e =

new EmbededDBEngine(reader, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class JoinUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s1 = m[r.getNameAddress(LEFT\_TABLE)];
InputSplit s2 = m[r.getNameAddress(RIGHT\_TABLE)];
Reader in1 = new MapOutputReader(s1);
Reader in2 = new MapOutputReader(s2);
EmbededDBEngine e =

new EmbededDBEngine(in1, in2, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class AggregateUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
EmbededDBEngine e =

new EmbededDBEngine(in, getQuery());

e.process();

}

}

The abstractions are straightforward and we discard the detailed
discussion.
In each unit, we embed a customized query engine,
which can process single table queries, join queries and aggrega-
tions. We have not speciﬁed the recipients of each message in the
unit abstraction. This must be implemented by users for different
queries. However, as discussed later, we provide a query optimizer
to automatically ﬁll in the recipients. To show how users can adopt
the above relational model to process queries, let us consider the
following query (a variant of TPC-H Q3):

SELECT l orderkey, sum(l extendedprice*(1-l discount))

as revenue, o orderdate, o shippriority

FROM customer, orders, lineitem
WHERE c mktsegment = ’:1’ and c custkey = o custkey

and l orderkey = o orderkey and o orderdate
< date ’:2’ and l shipdate > date ’:2’

Group By o orderdate, o shippriority

Figure 6 to Figure 10 illustrate the processing of Q3 in epiC.
In step 1 (Figure 6), three types of the SingleTableUnits are
started to process the select/project operators of Lineitem, Orders
and Customer respectively. Note that those SingleTableUnits
run the same code. The only differences are their name addresses
and processed queries. The results are written back to the storage
system (either HDFS or distributed database). The meta-data of the
corresponding ﬁles are forwarded to the JoinUnits.

In step 2 and step 3 (Figures 7 and 8), we apply the hash-join
approach to process the data. In previous SingleTableUnits,
the output data are partitioned by the join keys. So the JoinUnit
can selectively load the paired partitions to perform the join. We
will discuss other possible join implementations in the next section.

Figure 6: Step 1 of Q3

We adopt burst sort algorithm to order the serialized strings. We
choose burst sort as our sorting technique since it is specially de-
signed for sorting large string collections and has been shown to
be signiﬁcantly faster than other candidates [25]. We brieﬂy out-
line the algorithm here. Interested readers should refer to [25] for
details. The burst sort technique sorts a string collection in two
passes. In the ﬁrst pass, the algorithm processes each input string
and stores the pointer of each string into a leaf node (bucket) in
a burst trie. The burst trie has a nice property that all leaf nodes
(buckets) are ordered. Thus, in the second pass, the algorithm pro-
cesses each bucket in order, applies a standard sorting technique
such as quick sort to sort strings, and produces the ﬁnal results.
The original burst sort requires a lot of additional memory to hold
the trie structure and thus does not scale well to a very large string
collection. We, thus, developed a memory efﬁcient burst sort im-
plementation which requires only 2 bits of additional space for each
key entry. We also use the multi-key quick sort algorithm [5] to sort
strings resided in the same bucket.

Combining the two techniques (i.e., order-preserving serializa-
tion and burst sort), our sorting scheme outperforms Hadoop’s quick
sort implementation by a factor of three to four.
3.2 Relational Model Extension

As pointed out earlier, for structured data, the relational data pro-
cessing model is most suited. Like the MapReduce extensions, we
can implement the relational model on top of epiC.
3.2.1 General Abstractions
Currently, three core units (SingleTableUnit, JoinUnit
and AggregateUnit) are deﬁned for the relational model. They
are capable of handling non-nested SQL queries. The SingleTableUnit
processes queries that involve only a partition of a single table.
The JoinUnit reads partitions from two tables and merge them
into one partition of the join table. Finally, the AggregateUnit
collects the partitions of different groups and computes the aggre-
gation results for each group. The abstractions of these units are
shown below. Currently, we adopt the synchronization model as
in MapReduce. Namely, we will start the next types of units, only
when all current units complete their processing. We will study
the possibility of creating a pipeline model in future work. Due to
space limitation, we only show the most important part.

class SingleTableQuery implements DBQuery {

void getQuery() {
}

}
class JoinQuery implements DBQuery {

void getQuery() {
}

}
class AggregateQuery implements DBQuery {

void getQuery() {
}

SingleTableUnitSingleTableUnitSingleTableUnitRelational Data of Customer/Orders/Lineitemselect c_custkey from Customer where c_mktsegment = ':1'select o_orderdate, o_custkey, o_orderkey, o_shippriority from Orders where o_orderdate < date ':2'select l_orderkey, l_extendedprice, l_discount from Lineitem where l_shipdate > date ':2'Master networkPartition info of CustomerPartition info of OrdersPartition info of LineitemFigure 7: Step 2 of Q3

Figure 8: Step 3 of Q3

Figure 9: Step 4 of Q3

Figure 10: Step 5 of Q3

Figure 11: Basic Join Operation

Figure 12: Semi-Join Operation

Finally, in step 4 (Figure 9), we perform the group operation
for two attributes. As the join results are partitioned into multiple
chunks, one SingleTableUnit can only generate the grouping
results for its own chunk. To produce the complete grouping re-
sults, we merge groups generated by different SingleTableUnits.
Therefore, in step 5 (Figure 10), one AggregateUnit needs to
load the partitions generated by all SingleTableUnits for the
same group to compute the ﬁnal aggregation results.

Our relational model simpliﬁes the query processing, as users
only need to consider how to partition the tables by the three units.
Moreover, it also provides the ﬂexibility of customized optimiza-
tion.

3.2.2 Optimizations for Relational Model
The relational model on epiC can be optimized in two layers, the

unit layer and the job layer.

In the unit layer, the user can adaptively combine the units to
implement different database operations. They can even write their
own units, such as ThetaJoinUnit, to extend the functional-
ity of our model. In this section, we use the euqi-join as an ex-
ample to illustrate the ﬂexibility of the model. Figure 11 shows
how the basic equi-join (S ◃▹ T ) is implemented in epiC. We ﬁrst
use the SingleTableUnit to scan the corresponding tables and
partition the tables by join keys. Then, the JoinUnit loads the
corresponding partitions to generate the results. In fact, the same
approach is also used in processing Q3. We partition the tables by
the keys in step 1 (Figure 6). So the following JoinUnits can
perform the join correctly.

Figure 13: Job Plan of Q3

However, if most of the tuples in S do not match tuples of T ,
semi-join is a better approach to reduce the overhead. Figure 12 il-
lustrates the idea. The ﬁrst SingleTableUnit scans table S and
only outputs the keys as the results. The keys are used in the next
SingleTableUnit to ﬁlter the tuples in T that cannot join with
S. The intermediate results are joined with S in the last JoinUnit
to produce the ﬁnal results. As shown in the example, semi-join can
be efﬁciently implemented using our relational model.

In the job layer, we offer a general query optimizer to translate
the SQL queries into an epiC job. Users can leverage the opti-
mizer to process their queries, instead of writing the codes for the
relational model by themselves. The optimizer works as a conven-
tional database optimizer. It ﬁrst generates an operator expression
tree for the SQL query and then groups the operators into differ-
ent units. The message ﬂow between units is also generated based
on the expression tree. To avoid a bad query plan, the optimizer
estimates the cost of the units based on the histograms. Currently,
we only consider the I/O costs. The optimizer will iterate over all
variants of the expression trees and select the one with the minimal
estimated cost. The corresponding epiC job is submitted to the pro-
cessing engine for execution. Figure 13 shows how the expression
tree is partitioned into units for Q3.

The query optimizer acts as the AQUA [29] for MapReduce or
PACTs compiler in Nephele [4]. But in epiC, the DAG between
units are not used for data shufﬂing as in Nephele. Instead, all rela-
tionships between units are maintained through the message pass-
ing and namespaces. All units fetch their data from the storage sys-
tem directly. This design follows the core concept of Actor model.
The advantage is three-fold: 1) we reduce the overhead of main-
taining the DAG; 2) we simplify the model as each unit runs in an

Create Partition JoinView1 as (Lineitem join Orders)Partition info of the Partial Results of Lineitem and OrdersJoinUnitPartial Results of Customer/LineitemMaster networkCreate Partition JoinView2 as (Customer join JoinView1)Partition info of the Partial Results of Customer and JoinView1JoinUnitPartial Results of Customer/JoinView1Master networkSelect * from JoinView2 Group By o_orderdate, o_shippriorityPartition info of the Partial Results of JoinView2SingleTableUnitPartial Results of JoinView2Master networkAggregateUnitPartial Results of Group ByCompute Aggregation Results for Each GroupMaster networkPartition info of GroupsSingleTableUnitTable S and TMaster networkSingleTableUnitselect * from S partitioned by S.keyselect * from T partitioned by T.foreignkeyJoinUnitTable S and TMaster networkselect * from S, T where S.key=T.foreignkeySingleTableUnitTable SMaster networkCreate Partition Tmp as (select key from S)SingleTableUnitTable T and TmpMaster networkCreate Partition Tmp2 as(select * from T where T.foreignkey in (select * from Tmp))JoinUnitTable S and Tmp2Master networkselect * from S, Tmp2 where S.key =Tmp2.foreignkeylineitemordersσl_shipdate>date ‘:2’π(l_orderkey, l_extendedprice, l_discount)σo_orderdate<date ‘:2’π(o_orderkey, o_custkey, o_orderdate, o_shippriority)customerσc_mktsegment=date ‘:1’π(c_custkey)GroupByo_orderdaye, o_shippriorirysum(l_extendedprice * (1 -l_discount)) as revenueSingleTableUnitSingleTableUnitSingleTableUnitJoinUnitJoinUnitSingleTableUnitAggregateUnitepiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itFigure 5: The Programming Stack of epiC

for running the written program on the epiC’s common concurrent
runtime system.

We point out that the data processing model is problem domain
speciﬁc. For example, a MapReduce model is best suited for pro-
cessing unstructured data, a relational model is best suited for struc-
tured data and a graph model is best suited for graph data. The
common requirement is that programs written with these models
are all needed to be parallelized. Since Big Data is inherently multi-
structured, we build an Actor-like concurrent programming model
for a common runtime framework and offer epiC extensions for
users to specify domain speciﬁc data manipulation operations for
each data type. In the previous section, we have introduced the ba-
sic programming model of epiC. In this section, we focus on two
customized data processing model, the MapReduce model and re-
lational model. We will show how to implement them on top of
epiC.
3.1 The MapReduce Extension

We ﬁrst consider the MapReduce framework, and extend it to
work with epiC’s runtime framework. The MapReduce data pro-
cessing model consists of two interfaces:

(k1, v1)
(k2, list(v2)) ! list(v2)

map
reduce
Our MapReduce extension reuses Hadoop’s implementation of
these interfaces and other useful functions such as the partition.
This section only describes the auxiliary support which enables
users to run MapReduce programs on epiC and our own optimiza-
tions which are not supported in Hadoop.

! list(k2, v2)

3.1.1 General Abstractions
Running MapReduce on top of epiC is straightforward. We ﬁrst
place the map() function in a map unit and the reduce() func-
tion in a reduce unit. Then, we instantiate M map units and R
reduce units. The master network assigns a unique namespace to
each map and reduce unit. In the simplest case, the name addresses
of the units are like “x@MapUnit” and “y@ReduceUnit”, where
0 (cid:20) x < M and 0 (cid:20) y < R.

Based on the namespace, the MapUnit loads a partition of in-
put data and applies the customized map() function to process it.
The results are a set of key-value pairs. Here, a partition()
function is required to split the key-value pairs into multiple HDFS
ﬁles. Based on the application’s requirement, the partition()
can choose to sort the data by keys. By default, the partition()
simply applies the hash function to generate R ﬁles and assigns a
namespace to each ﬁle. The meta-data of the HDFS ﬁles are com-
posed into a message, which is sent to the master network. The
recipient is speciﬁed as all the ReduceUnit.

The master network then collects the messages from all MapUnits

and broadcasts them to the ReduceUnit. When a ReduceUnit
starts up, it loads the HDFS ﬁles that share the same namespace
with it. A possible merge-sort is required, if the results should be
sorted. Then, the customized reduce() function is invoked to

generate the ﬁnal results.

class Map implements Mapper {

void map() {
}

}
class Reduce implements Reducer {

void reduce() {
}

}
class MapUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new HdfsReader(s);
MapRunner map = new MapRunner(reader, Map());
map.run();
o.sendMessage("*@ReduceUnit",
map.getOutputMessage());

}

}
class ReduceUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
ReduceRunner red = new ReduceRunner(in,

Reduce());

red.run();

}

}

Here, we highlight the advantage of our design decision to de-
couple the data processing model and the concurrent programming
model. Suppose we want to extend the MapReduce programming
model to the Map-Reduce-Merge programming model [30]. All
we need to do is to add a new unit mergeUnit() and modify
the codes in the ReduceUnit to send messages to the master net-
work for declaring its output ﬁles. Compared to this non-intrusive
scheme, Hadoop needs to make dramatic changes to its runtime
system to support the same functionality [30] since Hadoop’s de-
sign bundles data processing model with concurrent programming
model.
3.1.2 Optimizations for MapReduce
In addition to the basic MapReduce implementation which is
similar to Hadoop, we add an optimization for map unit data pro-
cessing. We found that the map unit computation is CPU-bound
instead of I/O bound. The high CPU cost comes from the ﬁnal
sorting phase.

The Map unit needs to sort the intermediate key-value pairs since
MapReduce requires the reduce function to process key-value pairs
in an increasing order. Sorting in MapReduce is expensive since 1)
the sorting algorithm (i.e., quick sort) itself is CPU intensive and
2) the data de-serialization cost is not negligible. We employ two
techniques to improve the map unit sorting performance: 1) order-
preserving serialization and 2) high performance string sort (i.e.,
burst sort).

Deﬁnition 1. For a data type T , an order-preserving serialization
is an encoding scheme which serializes a variable x 2 T to a string
sx such that, for any two variables x 2 T and y 2 T , if x < y then
sx < sy in string lexicographical order.

In other words, the order-preserving serialization scheme serial-
izes keys so that the keys can be ordered by directly sorting their se-
rialized strings (in string lexicographical order) without de-serialization.
Note that the order-preserving serialization scheme exists for all
Java built-in data types.

(cid:258)(cid:258)(cid:38)(cid:82)(cid:81)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:53)(cid:88)(cid:81)(cid:87)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)}
class SingleTableUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new TableReader(s);
EmbededDBEngine

e =

new EmbededDBEngine(reader, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class JoinUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s1 = m[r.getNameAddress(LEFT\_TABLE)];
InputSplit s2 = m[r.getNameAddress(RIGHT\_TABLE)];
Reader in1 = new MapOutputReader(s1);
Reader in2 = new MapOutputReader(s2);
EmbededDBEngine e =

new EmbededDBEngine(in1, in2, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class AggregateUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
EmbededDBEngine e =

new EmbededDBEngine(in, getQuery());

e.process();

}

}

The abstractions are straightforward and we discard the detailed
discussion.
In each unit, we embed a customized query engine,
which can process single table queries, join queries and aggrega-
tions. We have not speciﬁed the recipients of each message in the
unit abstraction. This must be implemented by users for different
queries. However, as discussed later, we provide a query optimizer
to automatically ﬁll in the recipients. To show how users can adopt
the above relational model to process queries, let us consider the
following query (a variant of TPC-H Q3):

SELECT l orderkey, sum(l extendedprice*(1-l discount))

as revenue, o orderdate, o shippriority

FROM customer, orders, lineitem
WHERE c mktsegment = ’:1’ and c custkey = o custkey

and l orderkey = o orderkey and o orderdate
< date ’:2’ and l shipdate > date ’:2’

Group By o orderdate, o shippriority

Figure 6 to Figure 10 illustrate the processing of Q3 in epiC.
In step 1 (Figure 6), three types of the SingleTableUnits are
started to process the select/project operators of Lineitem, Orders
and Customer respectively. Note that those SingleTableUnits
run the same code. The only differences are their name addresses
and processed queries. The results are written back to the storage
system (either HDFS or distributed database). The meta-data of the
corresponding ﬁles are forwarded to the JoinUnits.

In step 2 and step 3 (Figures 7 and 8), we apply the hash-join
approach to process the data. In previous SingleTableUnits,
the output data are partitioned by the join keys. So the JoinUnit
can selectively load the paired partitions to perform the join. We
will discuss other possible join implementations in the next section.

Figure 6: Step 1 of Q3

We adopt burst sort algorithm to order the serialized strings. We
choose burst sort as our sorting technique since it is specially de-
signed for sorting large string collections and has been shown to
be signiﬁcantly faster than other candidates [25]. We brieﬂy out-
line the algorithm here. Interested readers should refer to [25] for
details. The burst sort technique sorts a string collection in two
passes. In the ﬁrst pass, the algorithm processes each input string
and stores the pointer of each string into a leaf node (bucket) in
a burst trie. The burst trie has a nice property that all leaf nodes
(buckets) are ordered. Thus, in the second pass, the algorithm pro-
cesses each bucket in order, applies a standard sorting technique
such as quick sort to sort strings, and produces the ﬁnal results.
The original burst sort requires a lot of additional memory to hold
the trie structure and thus does not scale well to a very large string
collection. We, thus, developed a memory efﬁcient burst sort im-
plementation which requires only 2 bits of additional space for each
key entry. We also use the multi-key quick sort algorithm [5] to sort
strings resided in the same bucket.

Combining the two techniques (i.e., order-preserving serializa-
tion and burst sort), our sorting scheme outperforms Hadoop’s quick
sort implementation by a factor of three to four.
3.2 Relational Model Extension

As pointed out earlier, for structured data, the relational data pro-
cessing model is most suited. Like the MapReduce extensions, we
can implement the relational model on top of epiC.
3.2.1 General Abstractions
Currently, three core units (SingleTableUnit, JoinUnit
and AggregateUnit) are deﬁned for the relational model. They
are capable of handling non-nested SQL queries. The SingleTableUnit
processes queries that involve only a partition of a single table.
The JoinUnit reads partitions from two tables and merge them
into one partition of the join table. Finally, the AggregateUnit
collects the partitions of different groups and computes the aggre-
gation results for each group. The abstractions of these units are
shown below. Currently, we adopt the synchronization model as
in MapReduce. Namely, we will start the next types of units, only
when all current units complete their processing. We will study
the possibility of creating a pipeline model in future work. Due to
space limitation, we only show the most important part.

class SingleTableQuery implements DBQuery {

void getQuery() {
}

}
class JoinQuery implements DBQuery {

void getQuery() {
}

}
class AggregateQuery implements DBQuery {

void getQuery() {
}

SingleTableUnitSingleTableUnitSingleTableUnitRelational Data of Customer/Orders/Lineitemselect c_custkey from Customer where c_mktsegment = ':1'select o_orderdate, o_custkey, o_orderkey, o_shippriority from Orders where o_orderdate < date ':2'select l_orderkey, l_extendedprice, l_discount from Lineitem where l_shipdate > date ':2'Master networkPartition info of CustomerPartition info of OrdersPartition info of LineitemFigure 7: Step 2 of Q3

Figure 8: Step 3 of Q3

Figure 9: Step 4 of Q3

Figure 10: Step 5 of Q3

Figure 11: Basic Join Operation

Figure 12: Semi-Join Operation

Finally, in step 4 (Figure 9), we perform the group operation
for two attributes. As the join results are partitioned into multiple
chunks, one SingleTableUnit can only generate the grouping
results for its own chunk. To produce the complete grouping re-
sults, we merge groups generated by different SingleTableUnits.
Therefore, in step 5 (Figure 10), one AggregateUnit needs to
load the partitions generated by all SingleTableUnits for the
same group to compute the ﬁnal aggregation results.

Our relational model simpliﬁes the query processing, as users
only need to consider how to partition the tables by the three units.
Moreover, it also provides the ﬂexibility of customized optimiza-
tion.

3.2.2 Optimizations for Relational Model
The relational model on epiC can be optimized in two layers, the

unit layer and the job layer.

In the unit layer, the user can adaptively combine the units to
implement different database operations. They can even write their
own units, such as ThetaJoinUnit, to extend the functional-
ity of our model. In this section, we use the euqi-join as an ex-
ample to illustrate the ﬂexibility of the model. Figure 11 shows
how the basic equi-join (S ◃▹ T ) is implemented in epiC. We ﬁrst
use the SingleTableUnit to scan the corresponding tables and
partition the tables by join keys. Then, the JoinUnit loads the
corresponding partitions to generate the results. In fact, the same
approach is also used in processing Q3. We partition the tables by
the keys in step 1 (Figure 6). So the following JoinUnits can
perform the join correctly.

Figure 13: Job Plan of Q3

However, if most of the tuples in S do not match tuples of T ,
semi-join is a better approach to reduce the overhead. Figure 12 il-
lustrates the idea. The ﬁrst SingleTableUnit scans table S and
only outputs the keys as the results. The keys are used in the next
SingleTableUnit to ﬁlter the tuples in T that cannot join with
S. The intermediate results are joined with S in the last JoinUnit
to produce the ﬁnal results. As shown in the example, semi-join can
be efﬁciently implemented using our relational model.

In the job layer, we offer a general query optimizer to translate
the SQL queries into an epiC job. Users can leverage the opti-
mizer to process their queries, instead of writing the codes for the
relational model by themselves. The optimizer works as a conven-
tional database optimizer. It ﬁrst generates an operator expression
tree for the SQL query and then groups the operators into differ-
ent units. The message ﬂow between units is also generated based
on the expression tree. To avoid a bad query plan, the optimizer
estimates the cost of the units based on the histograms. Currently,
we only consider the I/O costs. The optimizer will iterate over all
variants of the expression trees and select the one with the minimal
estimated cost. The corresponding epiC job is submitted to the pro-
cessing engine for execution. Figure 13 shows how the expression
tree is partitioned into units for Q3.

The query optimizer acts as the AQUA [29] for MapReduce or
PACTs compiler in Nephele [4]. But in epiC, the DAG between
units are not used for data shufﬂing as in Nephele. Instead, all rela-
tionships between units are maintained through the message pass-
ing and namespaces. All units fetch their data from the storage sys-
tem directly. This design follows the core concept of Actor model.
The advantage is three-fold: 1) we reduce the overhead of main-
taining the DAG; 2) we simplify the model as each unit runs in an

Create Partition JoinView1 as (Lineitem join Orders)Partition info of the Partial Results of Lineitem and OrdersJoinUnitPartial Results of Customer/LineitemMaster networkCreate Partition JoinView2 as (Customer join JoinView1)Partition info of the Partial Results of Customer and JoinView1JoinUnitPartial Results of Customer/JoinView1Master networkSelect * from JoinView2 Group By o_orderdate, o_shippriorityPartition info of the Partial Results of JoinView2SingleTableUnitPartial Results of JoinView2Master networkAggregateUnitPartial Results of Group ByCompute Aggregation Results for Each GroupMaster networkPartition info of GroupsSingleTableUnitTable S and TMaster networkSingleTableUnitselect * from S partitioned by S.keyselect * from T partitioned by T.foreignkeyJoinUnitTable S and TMaster networkselect * from S, T where S.key=T.foreignkeySingleTableUnitTable SMaster networkCreate Partition Tmp as (select key from S)SingleTableUnitTable T and TmpMaster networkCreate Partition Tmp2 as(select * from T where T.foreignkey in (select * from Tmp))JoinUnitTable S and Tmp2Master networkselect * from S, Tmp2 where S.key =Tmp2.foreignkeylineitemordersσl_shipdate>date ‘:2’π(l_orderkey, l_extendedprice, l_discount)σo_orderdate<date ‘:2’π(o_orderkey, o_custkey, o_orderdate, o_shippriority)customerσc_mktsegment=date ‘:1’π(c_custkey)GroupByo_orderdaye, o_shippriorirysum(l_extendedprice * (1 -l_discount)) as revenueSingleTableUnitSingleTableUnitSingleTableUnitJoinUnitJoinUnitSingleTableUnitAggregateUnitto the master, reporting the completed task identity. The master will
perform the call, updating its status, and responds to the worker
tracker.

This request-reply scheme is inefﬁcient for client to continuously
query information stored at the server. Consider the example of task
assignments. To get a new task for execution, the worker tracker
must periodically make getTask() RPC calls to the master since
the master hosts all task information and the worker tracker has no
idea of whether there are pending tasks. This periodical-pulling
scheme introduces non-negligible delays to the job startup since
users may submit jobs at arbitrary time point but the task assign-
ment is only performed at the ﬁxed time points. Suppose the worker
tracker queries a new task at time t0 and the query interval is T ,
then all tasks of jobs submitted at t1 > t0 will be delayed to t0 + T
for task assignment.

Since continuously querying server-side information is a com-
mon communication pattern in epiC, we develop a new RPC scheme
to eliminate the pulling interval in successive RPC calls for low la-
tency data processing.

Our approach is called the TTL RPC which is an extension of
the standard RPC scheme by associating each RPC call with a user
speciﬁed Time To Live (TTL) parameter T . The TTL parameter
T captures the duration the RPC can live on the server if no re-
sults are returned from the server; when the TTL expires, the RPC
is considered to have been served. For example, suppose we call
getTask() with T = 10s (seconds), when there is no task to as-
sign, instead of returning a null task immediately, the master holds
the call for at most 10 seconds. During that period, if the master
ﬁnds any pending tasks (e.g., due to new job submission), the mas-
ter returns the calling worker tracker with a new task. Otherwise, if
10 seconds passed and there are still no tasks to assign, the master
returns a null task to the worker tracker. The standard request-reply
RPC can be implemented by setting T = 0, namely no live.

We use a double-evaluation scheme to process a TTL-RPC call.
When the server receives a TTL-RPC call C, it performs an ini-
tial evaluation of C by treating it as a standard RPC call. If this
initial evaluation returns nothing, the server puts C into a pending
list. The TTL-RPC call will stay in the pending list for at most
T time. The server performs a second evaluation of C if either 1)
the information that C queries changes or 2) T time has passed.
The outcome of the second evaluation is returned as the ﬁnal result
to the client. Using TTL-RPC, the client can continuously make
RPC calls to the server in a loop without pulling interval and thus
receives server-side information in real time. We found that TTL-
RPC signiﬁcantly improves the performance of small jobs and re-
duces startup costs.

Even though the TTL-RPC scheme is a simple extension to the
standard RPC scheme, the implementation of TTL-RPC poses cer-
tain challenges for the threading model that the classical Java net-
work programs adopt. A typical Java network program employs
a per-thread per-request threading model. When a network con-
nection is established, the server serves the client by ﬁrst picking
up a thread from a thread pool, then reading data from the socket,
and ﬁnally performing the appropriate computations and writing
result back to the socket. The serving thread is returned to the
thread pool after the client is served. This per-thread per-request
threading model works well with the standard RPC communica-
tion. But it is not appropriate for our TTL RPC scheme since TTL
RPC request will stay at the server for a long time (We typically set
T = 20 (cid:24) 30 seconds). When multiple worker trackers make TTL
RPC calls to the master, the per-thread per-request threading model
produces a large number of hanging threads, quickly exhausting the
thread pool, and thus makes the master unable to respond.

Figure 14: The architecture of an epiC cluster

isolated way; 3) the model is more ﬂexible to support complex data
manipulation jobs (either synchronized or asynchronized).

4.

IMPLEMENTATION DETAILS

epiC is written in Java and built from scratch although we reuse
some Hadoop codes to implement a MapReduce extension. This
section describes the internals of epiC.

Like Hadoop, epiC is expected to be deployed on a shared-nothing
cluster of commodity machines connected with switched Ethernet.
It is designed to process data stored in any data sources such as
databases or distributed ﬁle systems. The epiC software mainly
consists of three components: master, worker tracker and worker
process. The architecture of epiC is shown in Figure 14. epiC
adopts a single master (this master is different from the servers
in the master network, which are mainly responsible for routing
messages and maintaining namespaces) multi-slaves architecture.
There is only one master node in an epiC cluster, running a mas-
ter daemon. The main function of the master is to command the
worker trackers to execute jobs. The master is also responsible
for managing and monitoring the health of the cluster. The master
runs a HTTP server which hosts such status information for human
consumption. It communicates with worker trackers and worker
processes through remote procedure call (RPC).

Each slave node in an epiC cluster runs a worker tracker dae-
mon. The worker tracker manages a worker pool, a ﬁxed number
of worker processes, for running units. We run each unit in a sin-
gle worker process. We adopt this ‘pooling’ process model instead
of an on-demand process model which launches worker processes
on demand for two reasons. First, pre-launching a pool of worker
processes reduces the startup latency of job execution since launch-
ing a brand new Java process introduces non-trivial startup costs
(typically 2(cid:24)3 seconds). Second, the latest HotSpot Java Virtual
Machine (JVM) employs a Just-In-Time (JIT) compilation tech-
nique to incrementally compile the Java byte codes into native ma-
chine codes for better performance. To fully unleash the power
of HotSpot JVM, one must run a Java program for a long time so
that every hot spot (a code segment, performing expensive com-
putations) of the program can be compiled by the JIT compiler.
Therefore, a never-ending worker process is the most appropriate
one for this purpose.

Here, we will focus on two most important parts of the imple-

mentations, the TTL RPC and the failure recovery.
4.1 The TTL RPC

The standard RPC scheme adopts a client-server request-reply
scheme to process RPC calls. In this scheme, a client sends a RPC
request to the server. The server processes this request and returns
its client with results. For example, when a task completes, the
worker tracker will perform a RPC call taskComplete(taskId)

(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:39)(cid:68)(cid:72)(cid:80)(cid:82)(cid:81)(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:17)(cid:17)(cid:17)(cid:53)(cid:51)(cid:38)(cid:53)(cid:51)(cid:38)epiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itFigure 5: The Programming Stack of epiC

for running the written program on the epiC’s common concurrent
runtime system.

We point out that the data processing model is problem domain
speciﬁc. For example, a MapReduce model is best suited for pro-
cessing unstructured data, a relational model is best suited for struc-
tured data and a graph model is best suited for graph data. The
common requirement is that programs written with these models
are all needed to be parallelized. Since Big Data is inherently multi-
structured, we build an Actor-like concurrent programming model
for a common runtime framework and offer epiC extensions for
users to specify domain speciﬁc data manipulation operations for
each data type. In the previous section, we have introduced the ba-
sic programming model of epiC. In this section, we focus on two
customized data processing model, the MapReduce model and re-
lational model. We will show how to implement them on top of
epiC.
3.1 The MapReduce Extension

We ﬁrst consider the MapReduce framework, and extend it to
work with epiC’s runtime framework. The MapReduce data pro-
cessing model consists of two interfaces:

(k1, v1)
(k2, list(v2)) ! list(v2)

map
reduce
Our MapReduce extension reuses Hadoop’s implementation of
these interfaces and other useful functions such as the partition.
This section only describes the auxiliary support which enables
users to run MapReduce programs on epiC and our own optimiza-
tions which are not supported in Hadoop.

! list(k2, v2)

3.1.1 General Abstractions
Running MapReduce on top of epiC is straightforward. We ﬁrst
place the map() function in a map unit and the reduce() func-
tion in a reduce unit. Then, we instantiate M map units and R
reduce units. The master network assigns a unique namespace to
each map and reduce unit. In the simplest case, the name addresses
of the units are like “x@MapUnit” and “y@ReduceUnit”, where
0 (cid:20) x < M and 0 (cid:20) y < R.

Based on the namespace, the MapUnit loads a partition of in-
put data and applies the customized map() function to process it.
The results are a set of key-value pairs. Here, a partition()
function is required to split the key-value pairs into multiple HDFS
ﬁles. Based on the application’s requirement, the partition()
can choose to sort the data by keys. By default, the partition()
simply applies the hash function to generate R ﬁles and assigns a
namespace to each ﬁle. The meta-data of the HDFS ﬁles are com-
posed into a message, which is sent to the master network. The
recipient is speciﬁed as all the ReduceUnit.

The master network then collects the messages from all MapUnits

and broadcasts them to the ReduceUnit. When a ReduceUnit
starts up, it loads the HDFS ﬁles that share the same namespace
with it. A possible merge-sort is required, if the results should be
sorted. Then, the customized reduce() function is invoked to

generate the ﬁnal results.

class Map implements Mapper {

void map() {
}

}
class Reduce implements Reducer {

void reduce() {
}

}
class MapUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new HdfsReader(s);
MapRunner map = new MapRunner(reader, Map());
map.run();
o.sendMessage("*@ReduceUnit",
map.getOutputMessage());

}

}
class ReduceUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
ReduceRunner red = new ReduceRunner(in,

Reduce());

red.run();

}

}

Here, we highlight the advantage of our design decision to de-
couple the data processing model and the concurrent programming
model. Suppose we want to extend the MapReduce programming
model to the Map-Reduce-Merge programming model [30]. All
we need to do is to add a new unit mergeUnit() and modify
the codes in the ReduceUnit to send messages to the master net-
work for declaring its output ﬁles. Compared to this non-intrusive
scheme, Hadoop needs to make dramatic changes to its runtime
system to support the same functionality [30] since Hadoop’s de-
sign bundles data processing model with concurrent programming
model.
3.1.2 Optimizations for MapReduce
In addition to the basic MapReduce implementation which is
similar to Hadoop, we add an optimization for map unit data pro-
cessing. We found that the map unit computation is CPU-bound
instead of I/O bound. The high CPU cost comes from the ﬁnal
sorting phase.

The Map unit needs to sort the intermediate key-value pairs since
MapReduce requires the reduce function to process key-value pairs
in an increasing order. Sorting in MapReduce is expensive since 1)
the sorting algorithm (i.e., quick sort) itself is CPU intensive and
2) the data de-serialization cost is not negligible. We employ two
techniques to improve the map unit sorting performance: 1) order-
preserving serialization and 2) high performance string sort (i.e.,
burst sort).

Deﬁnition 1. For a data type T , an order-preserving serialization
is an encoding scheme which serializes a variable x 2 T to a string
sx such that, for any two variables x 2 T and y 2 T , if x < y then
sx < sy in string lexicographical order.

In other words, the order-preserving serialization scheme serial-
izes keys so that the keys can be ordered by directly sorting their se-
rialized strings (in string lexicographical order) without de-serialization.
Note that the order-preserving serialization scheme exists for all
Java built-in data types.

(cid:258)(cid:258)(cid:38)(cid:82)(cid:81)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:53)(cid:88)(cid:81)(cid:87)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)}
class SingleTableUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new TableReader(s);
EmbededDBEngine

e =

new EmbededDBEngine(reader, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class JoinUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s1 = m[r.getNameAddress(LEFT\_TABLE)];
InputSplit s2 = m[r.getNameAddress(RIGHT\_TABLE)];
Reader in1 = new MapOutputReader(s1);
Reader in2 = new MapOutputReader(s2);
EmbededDBEngine e =

new EmbededDBEngine(in1, in2, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class AggregateUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
EmbededDBEngine e =

new EmbededDBEngine(in, getQuery());

e.process();

}

}

The abstractions are straightforward and we discard the detailed
discussion.
In each unit, we embed a customized query engine,
which can process single table queries, join queries and aggrega-
tions. We have not speciﬁed the recipients of each message in the
unit abstraction. This must be implemented by users for different
queries. However, as discussed later, we provide a query optimizer
to automatically ﬁll in the recipients. To show how users can adopt
the above relational model to process queries, let us consider the
following query (a variant of TPC-H Q3):

SELECT l orderkey, sum(l extendedprice*(1-l discount))

as revenue, o orderdate, o shippriority

FROM customer, orders, lineitem
WHERE c mktsegment = ’:1’ and c custkey = o custkey

and l orderkey = o orderkey and o orderdate
< date ’:2’ and l shipdate > date ’:2’

Group By o orderdate, o shippriority

Figure 6 to Figure 10 illustrate the processing of Q3 in epiC.
In step 1 (Figure 6), three types of the SingleTableUnits are
started to process the select/project operators of Lineitem, Orders
and Customer respectively. Note that those SingleTableUnits
run the same code. The only differences are their name addresses
and processed queries. The results are written back to the storage
system (either HDFS or distributed database). The meta-data of the
corresponding ﬁles are forwarded to the JoinUnits.

In step 2 and step 3 (Figures 7 and 8), we apply the hash-join
approach to process the data. In previous SingleTableUnits,
the output data are partitioned by the join keys. So the JoinUnit
can selectively load the paired partitions to perform the join. We
will discuss other possible join implementations in the next section.

Figure 6: Step 1 of Q3

We adopt burst sort algorithm to order the serialized strings. We
choose burst sort as our sorting technique since it is specially de-
signed for sorting large string collections and has been shown to
be signiﬁcantly faster than other candidates [25]. We brieﬂy out-
line the algorithm here. Interested readers should refer to [25] for
details. The burst sort technique sorts a string collection in two
passes. In the ﬁrst pass, the algorithm processes each input string
and stores the pointer of each string into a leaf node (bucket) in
a burst trie. The burst trie has a nice property that all leaf nodes
(buckets) are ordered. Thus, in the second pass, the algorithm pro-
cesses each bucket in order, applies a standard sorting technique
such as quick sort to sort strings, and produces the ﬁnal results.
The original burst sort requires a lot of additional memory to hold
the trie structure and thus does not scale well to a very large string
collection. We, thus, developed a memory efﬁcient burst sort im-
plementation which requires only 2 bits of additional space for each
key entry. We also use the multi-key quick sort algorithm [5] to sort
strings resided in the same bucket.

Combining the two techniques (i.e., order-preserving serializa-
tion and burst sort), our sorting scheme outperforms Hadoop’s quick
sort implementation by a factor of three to four.
3.2 Relational Model Extension

As pointed out earlier, for structured data, the relational data pro-
cessing model is most suited. Like the MapReduce extensions, we
can implement the relational model on top of epiC.
3.2.1 General Abstractions
Currently, three core units (SingleTableUnit, JoinUnit
and AggregateUnit) are deﬁned for the relational model. They
are capable of handling non-nested SQL queries. The SingleTableUnit
processes queries that involve only a partition of a single table.
The JoinUnit reads partitions from two tables and merge them
into one partition of the join table. Finally, the AggregateUnit
collects the partitions of different groups and computes the aggre-
gation results for each group. The abstractions of these units are
shown below. Currently, we adopt the synchronization model as
in MapReduce. Namely, we will start the next types of units, only
when all current units complete their processing. We will study
the possibility of creating a pipeline model in future work. Due to
space limitation, we only show the most important part.

class SingleTableQuery implements DBQuery {

void getQuery() {
}

}
class JoinQuery implements DBQuery {

void getQuery() {
}

}
class AggregateQuery implements DBQuery {

void getQuery() {
}

SingleTableUnitSingleTableUnitSingleTableUnitRelational Data of Customer/Orders/Lineitemselect c_custkey from Customer where c_mktsegment = ':1'select o_orderdate, o_custkey, o_orderkey, o_shippriority from Orders where o_orderdate < date ':2'select l_orderkey, l_extendedprice, l_discount from Lineitem where l_shipdate > date ':2'Master networkPartition info of CustomerPartition info of OrdersPartition info of LineitemFigure 7: Step 2 of Q3

Figure 8: Step 3 of Q3

Figure 9: Step 4 of Q3

Figure 10: Step 5 of Q3

Figure 11: Basic Join Operation

Figure 12: Semi-Join Operation

Finally, in step 4 (Figure 9), we perform the group operation
for two attributes. As the join results are partitioned into multiple
chunks, one SingleTableUnit can only generate the grouping
results for its own chunk. To produce the complete grouping re-
sults, we merge groups generated by different SingleTableUnits.
Therefore, in step 5 (Figure 10), one AggregateUnit needs to
load the partitions generated by all SingleTableUnits for the
same group to compute the ﬁnal aggregation results.

Our relational model simpliﬁes the query processing, as users
only need to consider how to partition the tables by the three units.
Moreover, it also provides the ﬂexibility of customized optimiza-
tion.

3.2.2 Optimizations for Relational Model
The relational model on epiC can be optimized in two layers, the

unit layer and the job layer.

In the unit layer, the user can adaptively combine the units to
implement different database operations. They can even write their
own units, such as ThetaJoinUnit, to extend the functional-
ity of our model. In this section, we use the euqi-join as an ex-
ample to illustrate the ﬂexibility of the model. Figure 11 shows
how the basic equi-join (S ◃▹ T ) is implemented in epiC. We ﬁrst
use the SingleTableUnit to scan the corresponding tables and
partition the tables by join keys. Then, the JoinUnit loads the
corresponding partitions to generate the results. In fact, the same
approach is also used in processing Q3. We partition the tables by
the keys in step 1 (Figure 6). So the following JoinUnits can
perform the join correctly.

Figure 13: Job Plan of Q3

However, if most of the tuples in S do not match tuples of T ,
semi-join is a better approach to reduce the overhead. Figure 12 il-
lustrates the idea. The ﬁrst SingleTableUnit scans table S and
only outputs the keys as the results. The keys are used in the next
SingleTableUnit to ﬁlter the tuples in T that cannot join with
S. The intermediate results are joined with S in the last JoinUnit
to produce the ﬁnal results. As shown in the example, semi-join can
be efﬁciently implemented using our relational model.

In the job layer, we offer a general query optimizer to translate
the SQL queries into an epiC job. Users can leverage the opti-
mizer to process their queries, instead of writing the codes for the
relational model by themselves. The optimizer works as a conven-
tional database optimizer. It ﬁrst generates an operator expression
tree for the SQL query and then groups the operators into differ-
ent units. The message ﬂow between units is also generated based
on the expression tree. To avoid a bad query plan, the optimizer
estimates the cost of the units based on the histograms. Currently,
we only consider the I/O costs. The optimizer will iterate over all
variants of the expression trees and select the one with the minimal
estimated cost. The corresponding epiC job is submitted to the pro-
cessing engine for execution. Figure 13 shows how the expression
tree is partitioned into units for Q3.

The query optimizer acts as the AQUA [29] for MapReduce or
PACTs compiler in Nephele [4]. But in epiC, the DAG between
units are not used for data shufﬂing as in Nephele. Instead, all rela-
tionships between units are maintained through the message pass-
ing and namespaces. All units fetch their data from the storage sys-
tem directly. This design follows the core concept of Actor model.
The advantage is three-fold: 1) we reduce the overhead of main-
taining the DAG; 2) we simplify the model as each unit runs in an

Create Partition JoinView1 as (Lineitem join Orders)Partition info of the Partial Results of Lineitem and OrdersJoinUnitPartial Results of Customer/LineitemMaster networkCreate Partition JoinView2 as (Customer join JoinView1)Partition info of the Partial Results of Customer and JoinView1JoinUnitPartial Results of Customer/JoinView1Master networkSelect * from JoinView2 Group By o_orderdate, o_shippriorityPartition info of the Partial Results of JoinView2SingleTableUnitPartial Results of JoinView2Master networkAggregateUnitPartial Results of Group ByCompute Aggregation Results for Each GroupMaster networkPartition info of GroupsSingleTableUnitTable S and TMaster networkSingleTableUnitselect * from S partitioned by S.keyselect * from T partitioned by T.foreignkeyJoinUnitTable S and TMaster networkselect * from S, T where S.key=T.foreignkeySingleTableUnitTable SMaster networkCreate Partition Tmp as (select key from S)SingleTableUnitTable T and TmpMaster networkCreate Partition Tmp2 as(select * from T where T.foreignkey in (select * from Tmp))JoinUnitTable S and Tmp2Master networkselect * from S, Tmp2 where S.key =Tmp2.foreignkeylineitemordersσl_shipdate>date ‘:2’π(l_orderkey, l_extendedprice, l_discount)σo_orderdate<date ‘:2’π(o_orderkey, o_custkey, o_orderdate, o_shippriority)customerσc_mktsegment=date ‘:1’π(c_custkey)GroupByo_orderdaye, o_shippriorirysum(l_extendedprice * (1 -l_discount)) as revenueSingleTableUnitSingleTableUnitSingleTableUnitJoinUnitJoinUnitSingleTableUnitAggregateUnitto the master, reporting the completed task identity. The master will
perform the call, updating its status, and responds to the worker
tracker.

This request-reply scheme is inefﬁcient for client to continuously
query information stored at the server. Consider the example of task
assignments. To get a new task for execution, the worker tracker
must periodically make getTask() RPC calls to the master since
the master hosts all task information and the worker tracker has no
idea of whether there are pending tasks. This periodical-pulling
scheme introduces non-negligible delays to the job startup since
users may submit jobs at arbitrary time point but the task assign-
ment is only performed at the ﬁxed time points. Suppose the worker
tracker queries a new task at time t0 and the query interval is T ,
then all tasks of jobs submitted at t1 > t0 will be delayed to t0 + T
for task assignment.

Since continuously querying server-side information is a com-
mon communication pattern in epiC, we develop a new RPC scheme
to eliminate the pulling interval in successive RPC calls for low la-
tency data processing.

Our approach is called the TTL RPC which is an extension of
the standard RPC scheme by associating each RPC call with a user
speciﬁed Time To Live (TTL) parameter T . The TTL parameter
T captures the duration the RPC can live on the server if no re-
sults are returned from the server; when the TTL expires, the RPC
is considered to have been served. For example, suppose we call
getTask() with T = 10s (seconds), when there is no task to as-
sign, instead of returning a null task immediately, the master holds
the call for at most 10 seconds. During that period, if the master
ﬁnds any pending tasks (e.g., due to new job submission), the mas-
ter returns the calling worker tracker with a new task. Otherwise, if
10 seconds passed and there are still no tasks to assign, the master
returns a null task to the worker tracker. The standard request-reply
RPC can be implemented by setting T = 0, namely no live.

We use a double-evaluation scheme to process a TTL-RPC call.
When the server receives a TTL-RPC call C, it performs an ini-
tial evaluation of C by treating it as a standard RPC call. If this
initial evaluation returns nothing, the server puts C into a pending
list. The TTL-RPC call will stay in the pending list for at most
T time. The server performs a second evaluation of C if either 1)
the information that C queries changes or 2) T time has passed.
The outcome of the second evaluation is returned as the ﬁnal result
to the client. Using TTL-RPC, the client can continuously make
RPC calls to the server in a loop without pulling interval and thus
receives server-side information in real time. We found that TTL-
RPC signiﬁcantly improves the performance of small jobs and re-
duces startup costs.

Even though the TTL-RPC scheme is a simple extension to the
standard RPC scheme, the implementation of TTL-RPC poses cer-
tain challenges for the threading model that the classical Java net-
work programs adopt. A typical Java network program employs
a per-thread per-request threading model. When a network con-
nection is established, the server serves the client by ﬁrst picking
up a thread from a thread pool, then reading data from the socket,
and ﬁnally performing the appropriate computations and writing
result back to the socket. The serving thread is returned to the
thread pool after the client is served. This per-thread per-request
threading model works well with the standard RPC communica-
tion. But it is not appropriate for our TTL RPC scheme since TTL
RPC request will stay at the server for a long time (We typically set
T = 20 (cid:24) 30 seconds). When multiple worker trackers make TTL
RPC calls to the master, the per-thread per-request threading model
produces a large number of hanging threads, quickly exhausting the
thread pool, and thus makes the master unable to respond.

Figure 14: The architecture of an epiC cluster

isolated way; 3) the model is more ﬂexible to support complex data
manipulation jobs (either synchronized or asynchronized).

4.

IMPLEMENTATION DETAILS

epiC is written in Java and built from scratch although we reuse
some Hadoop codes to implement a MapReduce extension. This
section describes the internals of epiC.

Like Hadoop, epiC is expected to be deployed on a shared-nothing
cluster of commodity machines connected with switched Ethernet.
It is designed to process data stored in any data sources such as
databases or distributed ﬁle systems. The epiC software mainly
consists of three components: master, worker tracker and worker
process. The architecture of epiC is shown in Figure 14. epiC
adopts a single master (this master is different from the servers
in the master network, which are mainly responsible for routing
messages and maintaining namespaces) multi-slaves architecture.
There is only one master node in an epiC cluster, running a mas-
ter daemon. The main function of the master is to command the
worker trackers to execute jobs. The master is also responsible
for managing and monitoring the health of the cluster. The master
runs a HTTP server which hosts such status information for human
consumption. It communicates with worker trackers and worker
processes through remote procedure call (RPC).

Each slave node in an epiC cluster runs a worker tracker dae-
mon. The worker tracker manages a worker pool, a ﬁxed number
of worker processes, for running units. We run each unit in a sin-
gle worker process. We adopt this ‘pooling’ process model instead
of an on-demand process model which launches worker processes
on demand for two reasons. First, pre-launching a pool of worker
processes reduces the startup latency of job execution since launch-
ing a brand new Java process introduces non-trivial startup costs
(typically 2(cid:24)3 seconds). Second, the latest HotSpot Java Virtual
Machine (JVM) employs a Just-In-Time (JIT) compilation tech-
nique to incrementally compile the Java byte codes into native ma-
chine codes for better performance. To fully unleash the power
of HotSpot JVM, one must run a Java program for a long time so
that every hot spot (a code segment, performing expensive com-
putations) of the program can be compiled by the JIT compiler.
Therefore, a never-ending worker process is the most appropriate
one for this purpose.

Here, we will focus on two most important parts of the imple-

mentations, the TTL RPC and the failure recovery.
4.1 The TTL RPC

The standard RPC scheme adopts a client-server request-reply
scheme to process RPC calls. In this scheme, a client sends a RPC
request to the server. The server processes this request and returns
its client with results. For example, when a task completes, the
worker tracker will perform a RPC call taskComplete(taskId)

(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:39)(cid:68)(cid:72)(cid:80)(cid:82)(cid:81)(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:17)(cid:17)(cid:17)(cid:53)(cid:51)(cid:38)(cid:53)(cid:51)(cid:38)We develop a pipeline threading model to ﬁx the above prob-
lems. The pipeline threading model uses a dedicated thread to per-
form the network I/O (i.e., reading request from and writing re-
sults to the socket) and a thread pool to perform the RPC calls.
When the network I/O thread receives a TTL RPC request, it noti-
ﬁes the server and keeps the established connection to be opened.
The server then picks up a serving thread from the thread pool and
performs the initial evaluation. The serving thread will return to the
thread pool after the initial evaluation no matter whether the initial
evaluation produces the results or not. The server will re-pickup a
thread from the thread pool for the second evaluation, if necessary,
and notify the network I/O thread to complete the client request by
sending out the results of the second evaluation. Using the pipeline
threading model, no thread (serving threads or network I/O thread)
will be hanged during the processing of TTL RPC call. Thus the
threading model is scalable to thousands of concurrent TTL RPC
calls.

4.2 Fault Tolerance

Like all single master cluster architecture, epiC is designed to
be resilient to a large-scale slave machines failures. epiC treats a
slave machine failure as a network partition from that slave ma-
chine to the master. To detect such a failure, the master communi-
cates with worker trackers running on the slave machines by heart-
beat RPCs. If the master cannot receive heartbeat messages from
a worker tracker many times, it marks that worker tracker as dead
and the machine where that worker tracker runs on as “failed”.

When a worker tracker is marked as failed, the master will deter-
mine whether the tasks that the worker tracker processed need to be
recovered. We assume that users persist the output of an epiC job
into a reliable storage system like HDFS or databases. Therefore,
all completed terminal tasks (i.e., tasks hosting units in the termi-
nal group) need not to be recovered. We only recover in-progress
terminal tasks and all non-terminal tasks (no matter completed or
in-progress).

We adopt task re-execution as the main technique for task recov-
ery and employ an asynchronous output backup scheme to speedup
the recovering process. The task re-execution strategy is concep-
tually simple. However, to make it work, we need to make some
reﬁnements to the basic design. The problem is that, in some cases,
the system may not ﬁnd idle worker processes for re-running the
failed tasks.

For example, let us consider a user job that consists of three unit
groups: a map unit group M with two reduce groups R1 and R2.
The output of M is processed by R1 and the output of R1 is further
processed by R2, the terminal unit group for producing the ﬁnal
output. epiC evaluates this job by placing three unit groups M, R1
and R2, in three stages S1, S2 and S3 respectively. The system ﬁrst
launches tasks in S1 and S2. When the tasks in S1 complete, the
system will launch tasks in S3, and at the same time, shufﬂe data
from S1’s units to S2’s units.
Suppose at this time, a work tracker failure causes a task m’s
(m 2 M) output to be lost, the master will fail to ﬁnd an idle
worker process for re-executing that failed task. This is because all
worker processes are running tasks in S2 and S3 and the data lost
introduced by m causes all tasks in S2 to be stalled. Therefore, no
worker process can complete and go back to the idle state.

We introduce a preemption scheduling scheme to solve the above
deadlock problem.
If a task A fails to fetch data produced by
task B, the task A will notify the master and update its state to
in (cid:0) stick. If the master cannot ﬁnd idle worker processes for re-
covering failed tasks for a given period of time, it will kill in(cid:0)stick
tasks by sending killTask() RPCs to the corresponding worker

Algorithm 1 Generate the list of completed tasks to backup
Input: the worker tracker list W
Output: the list of tasks L to backup
1: for each worker tracker w 2 W do
T   the list of completed tasks performed by w
2:
for each completed task t 2 T do
3:
4:
5:
6:
7:
8: end for

if EB(t) < ER(t) then

L   L [ ftg

end if
end for

trackers. The worker trackers then kill the in (cid:0) stick tasks and
release the corresponding worker processes. Finally, the master
marks the killed in (cid:0) stick tasks as failed and adds them to the
failed task list for scheduling. The preemption scheduling scheme
solves the deadlock problem since epiC executes tasks based on
the stage order. The released worker processes will ﬁrst execute
predecessor failed tasks and then the killed in (cid:0) stick tasks.

Re-execution is the only approach for recovering in-progress tasks.

For completed tasks, we also adopt a task output backup strategy
for recovering. This scheme works as follows. Periodically, the
master notiﬁes the worker trackers to upload the output of com-
pleted tasks to HDFS. When the master detects a worker tracker Wi
fails, it ﬁrst commands another live worker tracker Wj to download
Wi’s completed tasks’ output and then notiﬁes all in-progress tasks
that Wj will server Wi’s completed tasks’ output.

Backing up data to HDFS consumes network bandwidth. So, the
master decides to backup a completed task’s output only if the out-
put backup can yield better performance than task re-execution re-
covery. To make such a decision, for a completed task t, the master
estimates two expected execution time ER and EB of t where ER
is the expected execution time when the task re-execution scheme
is adopted and EB is the expected execution time when the output
backup strategy is chosen. ER and EB are computed as follows

ER = Tt (cid:2) P + 2Tt (cid:2) (1 (cid:0) P )

EB = (Tt + Tu) (cid:2) P + Td (cid:2) (1 (cid:0) P )

(1)

(2)

where P is the probability that the worker track is available during
the job execution; Tt is the execution time of t; Tu is the elapsed
time for uploading output to HDFS; and Td is the elapsed time for
downloading output from HDFS. The three parameters Tt, Tu and
Td are easily collected or estimated. The parameter P is estimated
by the availability of a worker tracker in one day.

The master uses Alg. 1 for determining which completed tasks
should be backed up. The master iterates over each worker tracker
(line 1). For each worker tracker, the master retrieves its completed
task list (line 2). Then, for each task in the completed task list, the
master computes EB and ER and adds the task t into the result list
L if EB < ER (line 4 to line 5).

5. EXPERIMENTS

We evaluate the performance of epiC on different kinds of data
processing tasks, including unstructured data processing, relational
data processing and graph processing. We benchmark epiC against
Hadoop, an open source implementation of MapReduce for pro-
cessing unstructured data (i.e., text data) and relational data and
GPS [24], an open source implementation of Pregel [20] for graph
processing, respectively. For relational data, we also run additional

epiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itFigure 5: The Programming Stack of epiC

for running the written program on the epiC’s common concurrent
runtime system.

We point out that the data processing model is problem domain
speciﬁc. For example, a MapReduce model is best suited for pro-
cessing unstructured data, a relational model is best suited for struc-
tured data and a graph model is best suited for graph data. The
common requirement is that programs written with these models
are all needed to be parallelized. Since Big Data is inherently multi-
structured, we build an Actor-like concurrent programming model
for a common runtime framework and offer epiC extensions for
users to specify domain speciﬁc data manipulation operations for
each data type. In the previous section, we have introduced the ba-
sic programming model of epiC. In this section, we focus on two
customized data processing model, the MapReduce model and re-
lational model. We will show how to implement them on top of
epiC.
3.1 The MapReduce Extension

We ﬁrst consider the MapReduce framework, and extend it to
work with epiC’s runtime framework. The MapReduce data pro-
cessing model consists of two interfaces:

(k1, v1)
(k2, list(v2)) ! list(v2)

map
reduce
Our MapReduce extension reuses Hadoop’s implementation of
these interfaces and other useful functions such as the partition.
This section only describes the auxiliary support which enables
users to run MapReduce programs on epiC and our own optimiza-
tions which are not supported in Hadoop.

! list(k2, v2)

3.1.1 General Abstractions
Running MapReduce on top of epiC is straightforward. We ﬁrst
place the map() function in a map unit and the reduce() func-
tion in a reduce unit. Then, we instantiate M map units and R
reduce units. The master network assigns a unique namespace to
each map and reduce unit. In the simplest case, the name addresses
of the units are like “x@MapUnit” and “y@ReduceUnit”, where
0 (cid:20) x < M and 0 (cid:20) y < R.

Based on the namespace, the MapUnit loads a partition of in-
put data and applies the customized map() function to process it.
The results are a set of key-value pairs. Here, a partition()
function is required to split the key-value pairs into multiple HDFS
ﬁles. Based on the application’s requirement, the partition()
can choose to sort the data by keys. By default, the partition()
simply applies the hash function to generate R ﬁles and assigns a
namespace to each ﬁle. The meta-data of the HDFS ﬁles are com-
posed into a message, which is sent to the master network. The
recipient is speciﬁed as all the ReduceUnit.

The master network then collects the messages from all MapUnits

and broadcasts them to the ReduceUnit. When a ReduceUnit
starts up, it loads the HDFS ﬁles that share the same namespace
with it. A possible merge-sort is required, if the results should be
sorted. Then, the customized reduce() function is invoked to

generate the ﬁnal results.

class Map implements Mapper {

void map() {
}

}
class Reduce implements Reducer {

void reduce() {
}

}
class MapUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new HdfsReader(s);
MapRunner map = new MapRunner(reader, Map());
map.run();
o.sendMessage("*@ReduceUnit",
map.getOutputMessage());

}

}
class ReduceUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
ReduceRunner red = new ReduceRunner(in,

Reduce());

red.run();

}

}

Here, we highlight the advantage of our design decision to de-
couple the data processing model and the concurrent programming
model. Suppose we want to extend the MapReduce programming
model to the Map-Reduce-Merge programming model [30]. All
we need to do is to add a new unit mergeUnit() and modify
the codes in the ReduceUnit to send messages to the master net-
work for declaring its output ﬁles. Compared to this non-intrusive
scheme, Hadoop needs to make dramatic changes to its runtime
system to support the same functionality [30] since Hadoop’s de-
sign bundles data processing model with concurrent programming
model.
3.1.2 Optimizations for MapReduce
In addition to the basic MapReduce implementation which is
similar to Hadoop, we add an optimization for map unit data pro-
cessing. We found that the map unit computation is CPU-bound
instead of I/O bound. The high CPU cost comes from the ﬁnal
sorting phase.

The Map unit needs to sort the intermediate key-value pairs since
MapReduce requires the reduce function to process key-value pairs
in an increasing order. Sorting in MapReduce is expensive since 1)
the sorting algorithm (i.e., quick sort) itself is CPU intensive and
2) the data de-serialization cost is not negligible. We employ two
techniques to improve the map unit sorting performance: 1) order-
preserving serialization and 2) high performance string sort (i.e.,
burst sort).

Deﬁnition 1. For a data type T , an order-preserving serialization
is an encoding scheme which serializes a variable x 2 T to a string
sx such that, for any two variables x 2 T and y 2 T , if x < y then
sx < sy in string lexicographical order.

In other words, the order-preserving serialization scheme serial-
izes keys so that the keys can be ordered by directly sorting their se-
rialized strings (in string lexicographical order) without de-serialization.
Note that the order-preserving serialization scheme exists for all
Java built-in data types.

(cid:258)(cid:258)(cid:38)(cid:82)(cid:81)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:53)(cid:88)(cid:81)(cid:87)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)}
class SingleTableUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new TableReader(s);
EmbededDBEngine

e =

new EmbededDBEngine(reader, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class JoinUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s1 = m[r.getNameAddress(LEFT\_TABLE)];
InputSplit s2 = m[r.getNameAddress(RIGHT\_TABLE)];
Reader in1 = new MapOutputReader(s1);
Reader in2 = new MapOutputReader(s2);
EmbededDBEngine e =

new EmbededDBEngine(in1, in2, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class AggregateUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
EmbededDBEngine e =

new EmbededDBEngine(in, getQuery());

e.process();

}

}

The abstractions are straightforward and we discard the detailed
discussion.
In each unit, we embed a customized query engine,
which can process single table queries, join queries and aggrega-
tions. We have not speciﬁed the recipients of each message in the
unit abstraction. This must be implemented by users for different
queries. However, as discussed later, we provide a query optimizer
to automatically ﬁll in the recipients. To show how users can adopt
the above relational model to process queries, let us consider the
following query (a variant of TPC-H Q3):

SELECT l orderkey, sum(l extendedprice*(1-l discount))

as revenue, o orderdate, o shippriority

FROM customer, orders, lineitem
WHERE c mktsegment = ’:1’ and c custkey = o custkey

and l orderkey = o orderkey and o orderdate
< date ’:2’ and l shipdate > date ’:2’

Group By o orderdate, o shippriority

Figure 6 to Figure 10 illustrate the processing of Q3 in epiC.
In step 1 (Figure 6), three types of the SingleTableUnits are
started to process the select/project operators of Lineitem, Orders
and Customer respectively. Note that those SingleTableUnits
run the same code. The only differences are their name addresses
and processed queries. The results are written back to the storage
system (either HDFS or distributed database). The meta-data of the
corresponding ﬁles are forwarded to the JoinUnits.

In step 2 and step 3 (Figures 7 and 8), we apply the hash-join
approach to process the data. In previous SingleTableUnits,
the output data are partitioned by the join keys. So the JoinUnit
can selectively load the paired partitions to perform the join. We
will discuss other possible join implementations in the next section.

Figure 6: Step 1 of Q3

We adopt burst sort algorithm to order the serialized strings. We
choose burst sort as our sorting technique since it is specially de-
signed for sorting large string collections and has been shown to
be signiﬁcantly faster than other candidates [25]. We brieﬂy out-
line the algorithm here. Interested readers should refer to [25] for
details. The burst sort technique sorts a string collection in two
passes. In the ﬁrst pass, the algorithm processes each input string
and stores the pointer of each string into a leaf node (bucket) in
a burst trie. The burst trie has a nice property that all leaf nodes
(buckets) are ordered. Thus, in the second pass, the algorithm pro-
cesses each bucket in order, applies a standard sorting technique
such as quick sort to sort strings, and produces the ﬁnal results.
The original burst sort requires a lot of additional memory to hold
the trie structure and thus does not scale well to a very large string
collection. We, thus, developed a memory efﬁcient burst sort im-
plementation which requires only 2 bits of additional space for each
key entry. We also use the multi-key quick sort algorithm [5] to sort
strings resided in the same bucket.

Combining the two techniques (i.e., order-preserving serializa-
tion and burst sort), our sorting scheme outperforms Hadoop’s quick
sort implementation by a factor of three to four.
3.2 Relational Model Extension

As pointed out earlier, for structured data, the relational data pro-
cessing model is most suited. Like the MapReduce extensions, we
can implement the relational model on top of epiC.
3.2.1 General Abstractions
Currently, three core units (SingleTableUnit, JoinUnit
and AggregateUnit) are deﬁned for the relational model. They
are capable of handling non-nested SQL queries. The SingleTableUnit
processes queries that involve only a partition of a single table.
The JoinUnit reads partitions from two tables and merge them
into one partition of the join table. Finally, the AggregateUnit
collects the partitions of different groups and computes the aggre-
gation results for each group. The abstractions of these units are
shown below. Currently, we adopt the synchronization model as
in MapReduce. Namely, we will start the next types of units, only
when all current units complete their processing. We will study
the possibility of creating a pipeline model in future work. Due to
space limitation, we only show the most important part.

class SingleTableQuery implements DBQuery {

void getQuery() {
}

}
class JoinQuery implements DBQuery {

void getQuery() {
}

}
class AggregateQuery implements DBQuery {

void getQuery() {
}

SingleTableUnitSingleTableUnitSingleTableUnitRelational Data of Customer/Orders/Lineitemselect c_custkey from Customer where c_mktsegment = ':1'select o_orderdate, o_custkey, o_orderkey, o_shippriority from Orders where o_orderdate < date ':2'select l_orderkey, l_extendedprice, l_discount from Lineitem where l_shipdate > date ':2'Master networkPartition info of CustomerPartition info of OrdersPartition info of LineitemFigure 7: Step 2 of Q3

Figure 8: Step 3 of Q3

Figure 9: Step 4 of Q3

Figure 10: Step 5 of Q3

Figure 11: Basic Join Operation

Figure 12: Semi-Join Operation

Finally, in step 4 (Figure 9), we perform the group operation
for two attributes. As the join results are partitioned into multiple
chunks, one SingleTableUnit can only generate the grouping
results for its own chunk. To produce the complete grouping re-
sults, we merge groups generated by different SingleTableUnits.
Therefore, in step 5 (Figure 10), one AggregateUnit needs to
load the partitions generated by all SingleTableUnits for the
same group to compute the ﬁnal aggregation results.

Our relational model simpliﬁes the query processing, as users
only need to consider how to partition the tables by the three units.
Moreover, it also provides the ﬂexibility of customized optimiza-
tion.

3.2.2 Optimizations for Relational Model
The relational model on epiC can be optimized in two layers, the

unit layer and the job layer.

In the unit layer, the user can adaptively combine the units to
implement different database operations. They can even write their
own units, such as ThetaJoinUnit, to extend the functional-
ity of our model. In this section, we use the euqi-join as an ex-
ample to illustrate the ﬂexibility of the model. Figure 11 shows
how the basic equi-join (S ◃▹ T ) is implemented in epiC. We ﬁrst
use the SingleTableUnit to scan the corresponding tables and
partition the tables by join keys. Then, the JoinUnit loads the
corresponding partitions to generate the results. In fact, the same
approach is also used in processing Q3. We partition the tables by
the keys in step 1 (Figure 6). So the following JoinUnits can
perform the join correctly.

Figure 13: Job Plan of Q3

However, if most of the tuples in S do not match tuples of T ,
semi-join is a better approach to reduce the overhead. Figure 12 il-
lustrates the idea. The ﬁrst SingleTableUnit scans table S and
only outputs the keys as the results. The keys are used in the next
SingleTableUnit to ﬁlter the tuples in T that cannot join with
S. The intermediate results are joined with S in the last JoinUnit
to produce the ﬁnal results. As shown in the example, semi-join can
be efﬁciently implemented using our relational model.

In the job layer, we offer a general query optimizer to translate
the SQL queries into an epiC job. Users can leverage the opti-
mizer to process their queries, instead of writing the codes for the
relational model by themselves. The optimizer works as a conven-
tional database optimizer. It ﬁrst generates an operator expression
tree for the SQL query and then groups the operators into differ-
ent units. The message ﬂow between units is also generated based
on the expression tree. To avoid a bad query plan, the optimizer
estimates the cost of the units based on the histograms. Currently,
we only consider the I/O costs. The optimizer will iterate over all
variants of the expression trees and select the one with the minimal
estimated cost. The corresponding epiC job is submitted to the pro-
cessing engine for execution. Figure 13 shows how the expression
tree is partitioned into units for Q3.

The query optimizer acts as the AQUA [29] for MapReduce or
PACTs compiler in Nephele [4]. But in epiC, the DAG between
units are not used for data shufﬂing as in Nephele. Instead, all rela-
tionships between units are maintained through the message pass-
ing and namespaces. All units fetch their data from the storage sys-
tem directly. This design follows the core concept of Actor model.
The advantage is three-fold: 1) we reduce the overhead of main-
taining the DAG; 2) we simplify the model as each unit runs in an

Create Partition JoinView1 as (Lineitem join Orders)Partition info of the Partial Results of Lineitem and OrdersJoinUnitPartial Results of Customer/LineitemMaster networkCreate Partition JoinView2 as (Customer join JoinView1)Partition info of the Partial Results of Customer and JoinView1JoinUnitPartial Results of Customer/JoinView1Master networkSelect * from JoinView2 Group By o_orderdate, o_shippriorityPartition info of the Partial Results of JoinView2SingleTableUnitPartial Results of JoinView2Master networkAggregateUnitPartial Results of Group ByCompute Aggregation Results for Each GroupMaster networkPartition info of GroupsSingleTableUnitTable S and TMaster networkSingleTableUnitselect * from S partitioned by S.keyselect * from T partitioned by T.foreignkeyJoinUnitTable S and TMaster networkselect * from S, T where S.key=T.foreignkeySingleTableUnitTable SMaster networkCreate Partition Tmp as (select key from S)SingleTableUnitTable T and TmpMaster networkCreate Partition Tmp2 as(select * from T where T.foreignkey in (select * from Tmp))JoinUnitTable S and Tmp2Master networkselect * from S, Tmp2 where S.key =Tmp2.foreignkeylineitemordersσl_shipdate>date ‘:2’π(l_orderkey, l_extendedprice, l_discount)σo_orderdate<date ‘:2’π(o_orderkey, o_custkey, o_orderdate, o_shippriority)customerσc_mktsegment=date ‘:1’π(c_custkey)GroupByo_orderdaye, o_shippriorirysum(l_extendedprice * (1 -l_discount)) as revenueSingleTableUnitSingleTableUnitSingleTableUnitJoinUnitJoinUnitSingleTableUnitAggregateUnitto the master, reporting the completed task identity. The master will
perform the call, updating its status, and responds to the worker
tracker.

This request-reply scheme is inefﬁcient for client to continuously
query information stored at the server. Consider the example of task
assignments. To get a new task for execution, the worker tracker
must periodically make getTask() RPC calls to the master since
the master hosts all task information and the worker tracker has no
idea of whether there are pending tasks. This periodical-pulling
scheme introduces non-negligible delays to the job startup since
users may submit jobs at arbitrary time point but the task assign-
ment is only performed at the ﬁxed time points. Suppose the worker
tracker queries a new task at time t0 and the query interval is T ,
then all tasks of jobs submitted at t1 > t0 will be delayed to t0 + T
for task assignment.

Since continuously querying server-side information is a com-
mon communication pattern in epiC, we develop a new RPC scheme
to eliminate the pulling interval in successive RPC calls for low la-
tency data processing.

Our approach is called the TTL RPC which is an extension of
the standard RPC scheme by associating each RPC call with a user
speciﬁed Time To Live (TTL) parameter T . The TTL parameter
T captures the duration the RPC can live on the server if no re-
sults are returned from the server; when the TTL expires, the RPC
is considered to have been served. For example, suppose we call
getTask() with T = 10s (seconds), when there is no task to as-
sign, instead of returning a null task immediately, the master holds
the call for at most 10 seconds. During that period, if the master
ﬁnds any pending tasks (e.g., due to new job submission), the mas-
ter returns the calling worker tracker with a new task. Otherwise, if
10 seconds passed and there are still no tasks to assign, the master
returns a null task to the worker tracker. The standard request-reply
RPC can be implemented by setting T = 0, namely no live.

We use a double-evaluation scheme to process a TTL-RPC call.
When the server receives a TTL-RPC call C, it performs an ini-
tial evaluation of C by treating it as a standard RPC call. If this
initial evaluation returns nothing, the server puts C into a pending
list. The TTL-RPC call will stay in the pending list for at most
T time. The server performs a second evaluation of C if either 1)
the information that C queries changes or 2) T time has passed.
The outcome of the second evaluation is returned as the ﬁnal result
to the client. Using TTL-RPC, the client can continuously make
RPC calls to the server in a loop without pulling interval and thus
receives server-side information in real time. We found that TTL-
RPC signiﬁcantly improves the performance of small jobs and re-
duces startup costs.

Even though the TTL-RPC scheme is a simple extension to the
standard RPC scheme, the implementation of TTL-RPC poses cer-
tain challenges for the threading model that the classical Java net-
work programs adopt. A typical Java network program employs
a per-thread per-request threading model. When a network con-
nection is established, the server serves the client by ﬁrst picking
up a thread from a thread pool, then reading data from the socket,
and ﬁnally performing the appropriate computations and writing
result back to the socket. The serving thread is returned to the
thread pool after the client is served. This per-thread per-request
threading model works well with the standard RPC communica-
tion. But it is not appropriate for our TTL RPC scheme since TTL
RPC request will stay at the server for a long time (We typically set
T = 20 (cid:24) 30 seconds). When multiple worker trackers make TTL
RPC calls to the master, the per-thread per-request threading model
produces a large number of hanging threads, quickly exhausting the
thread pool, and thus makes the master unable to respond.

Figure 14: The architecture of an epiC cluster

isolated way; 3) the model is more ﬂexible to support complex data
manipulation jobs (either synchronized or asynchronized).

4.

IMPLEMENTATION DETAILS

epiC is written in Java and built from scratch although we reuse
some Hadoop codes to implement a MapReduce extension. This
section describes the internals of epiC.

Like Hadoop, epiC is expected to be deployed on a shared-nothing
cluster of commodity machines connected with switched Ethernet.
It is designed to process data stored in any data sources such as
databases or distributed ﬁle systems. The epiC software mainly
consists of three components: master, worker tracker and worker
process. The architecture of epiC is shown in Figure 14. epiC
adopts a single master (this master is different from the servers
in the master network, which are mainly responsible for routing
messages and maintaining namespaces) multi-slaves architecture.
There is only one master node in an epiC cluster, running a mas-
ter daemon. The main function of the master is to command the
worker trackers to execute jobs. The master is also responsible
for managing and monitoring the health of the cluster. The master
runs a HTTP server which hosts such status information for human
consumption. It communicates with worker trackers and worker
processes through remote procedure call (RPC).

Each slave node in an epiC cluster runs a worker tracker dae-
mon. The worker tracker manages a worker pool, a ﬁxed number
of worker processes, for running units. We run each unit in a sin-
gle worker process. We adopt this ‘pooling’ process model instead
of an on-demand process model which launches worker processes
on demand for two reasons. First, pre-launching a pool of worker
processes reduces the startup latency of job execution since launch-
ing a brand new Java process introduces non-trivial startup costs
(typically 2(cid:24)3 seconds). Second, the latest HotSpot Java Virtual
Machine (JVM) employs a Just-In-Time (JIT) compilation tech-
nique to incrementally compile the Java byte codes into native ma-
chine codes for better performance. To fully unleash the power
of HotSpot JVM, one must run a Java program for a long time so
that every hot spot (a code segment, performing expensive com-
putations) of the program can be compiled by the JIT compiler.
Therefore, a never-ending worker process is the most appropriate
one for this purpose.

Here, we will focus on two most important parts of the imple-

mentations, the TTL RPC and the failure recovery.
4.1 The TTL RPC

The standard RPC scheme adopts a client-server request-reply
scheme to process RPC calls. In this scheme, a client sends a RPC
request to the server. The server processes this request and returns
its client with results. For example, when a task completes, the
worker tracker will perform a RPC call taskComplete(taskId)

(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:39)(cid:68)(cid:72)(cid:80)(cid:82)(cid:81)(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:17)(cid:17)(cid:17)(cid:53)(cid:51)(cid:38)(cid:53)(cid:51)(cid:38)We develop a pipeline threading model to ﬁx the above prob-
lems. The pipeline threading model uses a dedicated thread to per-
form the network I/O (i.e., reading request from and writing re-
sults to the socket) and a thread pool to perform the RPC calls.
When the network I/O thread receives a TTL RPC request, it noti-
ﬁes the server and keeps the established connection to be opened.
The server then picks up a serving thread from the thread pool and
performs the initial evaluation. The serving thread will return to the
thread pool after the initial evaluation no matter whether the initial
evaluation produces the results or not. The server will re-pickup a
thread from the thread pool for the second evaluation, if necessary,
and notify the network I/O thread to complete the client request by
sending out the results of the second evaluation. Using the pipeline
threading model, no thread (serving threads or network I/O thread)
will be hanged during the processing of TTL RPC call. Thus the
threading model is scalable to thousands of concurrent TTL RPC
calls.

4.2 Fault Tolerance

Like all single master cluster architecture, epiC is designed to
be resilient to a large-scale slave machines failures. epiC treats a
slave machine failure as a network partition from that slave ma-
chine to the master. To detect such a failure, the master communi-
cates with worker trackers running on the slave machines by heart-
beat RPCs. If the master cannot receive heartbeat messages from
a worker tracker many times, it marks that worker tracker as dead
and the machine where that worker tracker runs on as “failed”.

When a worker tracker is marked as failed, the master will deter-
mine whether the tasks that the worker tracker processed need to be
recovered. We assume that users persist the output of an epiC job
into a reliable storage system like HDFS or databases. Therefore,
all completed terminal tasks (i.e., tasks hosting units in the termi-
nal group) need not to be recovered. We only recover in-progress
terminal tasks and all non-terminal tasks (no matter completed or
in-progress).

We adopt task re-execution as the main technique for task recov-
ery and employ an asynchronous output backup scheme to speedup
the recovering process. The task re-execution strategy is concep-
tually simple. However, to make it work, we need to make some
reﬁnements to the basic design. The problem is that, in some cases,
the system may not ﬁnd idle worker processes for re-running the
failed tasks.

For example, let us consider a user job that consists of three unit
groups: a map unit group M with two reduce groups R1 and R2.
The output of M is processed by R1 and the output of R1 is further
processed by R2, the terminal unit group for producing the ﬁnal
output. epiC evaluates this job by placing three unit groups M, R1
and R2, in three stages S1, S2 and S3 respectively. The system ﬁrst
launches tasks in S1 and S2. When the tasks in S1 complete, the
system will launch tasks in S3, and at the same time, shufﬂe data
from S1’s units to S2’s units.
Suppose at this time, a work tracker failure causes a task m’s
(m 2 M) output to be lost, the master will fail to ﬁnd an idle
worker process for re-executing that failed task. This is because all
worker processes are running tasks in S2 and S3 and the data lost
introduced by m causes all tasks in S2 to be stalled. Therefore, no
worker process can complete and go back to the idle state.

We introduce a preemption scheduling scheme to solve the above
deadlock problem.
If a task A fails to fetch data produced by
task B, the task A will notify the master and update its state to
in (cid:0) stick. If the master cannot ﬁnd idle worker processes for re-
covering failed tasks for a given period of time, it will kill in(cid:0)stick
tasks by sending killTask() RPCs to the corresponding worker

Algorithm 1 Generate the list of completed tasks to backup
Input: the worker tracker list W
Output: the list of tasks L to backup
1: for each worker tracker w 2 W do
T   the list of completed tasks performed by w
2:
for each completed task t 2 T do
3:
4:
5:
6:
7:
8: end for

if EB(t) < ER(t) then

L   L [ ftg

end if
end for

trackers. The worker trackers then kill the in (cid:0) stick tasks and
release the corresponding worker processes. Finally, the master
marks the killed in (cid:0) stick tasks as failed and adds them to the
failed task list for scheduling. The preemption scheduling scheme
solves the deadlock problem since epiC executes tasks based on
the stage order. The released worker processes will ﬁrst execute
predecessor failed tasks and then the killed in (cid:0) stick tasks.

Re-execution is the only approach for recovering in-progress tasks.

For completed tasks, we also adopt a task output backup strategy
for recovering. This scheme works as follows. Periodically, the
master notiﬁes the worker trackers to upload the output of com-
pleted tasks to HDFS. When the master detects a worker tracker Wi
fails, it ﬁrst commands another live worker tracker Wj to download
Wi’s completed tasks’ output and then notiﬁes all in-progress tasks
that Wj will server Wi’s completed tasks’ output.

Backing up data to HDFS consumes network bandwidth. So, the
master decides to backup a completed task’s output only if the out-
put backup can yield better performance than task re-execution re-
covery. To make such a decision, for a completed task t, the master
estimates two expected execution time ER and EB of t where ER
is the expected execution time when the task re-execution scheme
is adopted and EB is the expected execution time when the output
backup strategy is chosen. ER and EB are computed as follows

ER = Tt (cid:2) P + 2Tt (cid:2) (1 (cid:0) P )

EB = (Tt + Tu) (cid:2) P + Td (cid:2) (1 (cid:0) P )

(1)

(2)

where P is the probability that the worker track is available during
the job execution; Tt is the execution time of t; Tu is the elapsed
time for uploading output to HDFS; and Td is the elapsed time for
downloading output from HDFS. The three parameters Tt, Tu and
Td are easily collected or estimated. The parameter P is estimated
by the availability of a worker tracker in one day.

The master uses Alg. 1 for determining which completed tasks
should be backed up. The master iterates over each worker tracker
(line 1). For each worker tracker, the master retrieves its completed
task list (line 2). Then, for each task in the completed task list, the
master computes EB and ER and adds the task t into the result list
L if EB < ER (line 4 to line 5).

5. EXPERIMENTS

We evaluate the performance of epiC on different kinds of data
processing tasks, including unstructured data processing, relational
data processing and graph processing. We benchmark epiC against
Hadoop, an open source implementation of MapReduce for pro-
cessing unstructured data (i.e., text data) and relational data and
GPS [24], an open source implementation of Pregel [20] for graph
processing, respectively. For relational data, we also run additional

experiments to benchmark epiC with two new in-memory data pro-
cessing systems, namely Shark and Impala. For all experiments, the
results are reported by averaging six runs.
5.1 Benchmark Environment

The experimental study is conducted on an in-house cluster, con-
sisting of 72 nodes hosted on two racks. The nodes within each
rack are connected by a 1 Gbps switch. The two racks are con-
nected by a 10 Gbps cluster switch. Each cluster node is equipped
with a quad-core Intel Xeon 2.4GHz CPU, 8GB memory and two
500 GB SCSI disks. The hdparm utility reports that the buffered
read throughput of the disk is roughly 110 MB/sec. However, due
to the JVM costs, our tested Java program can only read local ﬁles
at 70 (cid:24) 80 MB/sec.

We choose 65 nodes out of the 72 nodes for our benchmark.
For the 65-node cluster, one node acts as the master for running
Hadoop’s NameNode, JobTracker daemons, GPS’s server node and
epiC’s master daemon. For scalability benchmark, we vary the
number of slave nodes from 1, 4, 16, to 64.
5.2 System Settings

In our experiments, we conﬁgure the three systems as follows:

1. The Hadoop settings consist of two parts: HDFS settings and
MapReduce settings.
In HDFS settings, we set the block
size to be 512 MB. As indicated in [17], this setting can sig-
niﬁcantly reduce Hadoop’s cost for scheduling MapReduce
tasks. We also set the I/O buffer size to 128 KB and the repli-
cation factor of HDFS to one (i.e., no replication). In MapRe-
duce settings, each slave is conﬁgured to run two concurrent
map and reduce tasks. The JVM runs in the server mode with
maximal 1.5 GB heap memory. The size of map task’s sort
buffer is 512 MB. We set the merge factor to be 500 and turn
off speculation scheduling. Finally, we set the JVM reuse
number to -1.

2. For each worker tracker in epiC, we set the size of the worker
pool to be four. In the worker pool, two workers are current
workers (running current units) and the remaining two work-
ers are appending workers. Similar to Hadoop’s setting, each
worker process has 1.5 GB memory. For the MapReduce ex-
tension, we set the bucket size of burst sort to be 8192 keys
(string pointers).

3. For GPS, we employ the default settings of the system with-

out further tuning.

5.3 Benchmark Tasks and Datasets

5.3.1 Benchmark Tasks
The benchmark consists of four tasks: Grep, TeraSort, TPC-H
Q3, and PageRank. The Grep task and TeraSort task are presented
in the original MapReduce paper for demonstrating the scalability
and the efﬁciency of using MapReduce for processing unstructured
data (i.e., plain text data). The Grep task requires us to check each
record (i.e., a line of text string) of the input dataset and output
all records containing a speciﬁc pattern string. The TeraSort task
requires the system to arrange the input records in an ascending or-
der. The TPC-H Q3 task is a standard benchmark query in TPC-H
benchmark and is presented in Section 3.2. The PageRank algo-
rithm [22] is an iterative graph processing algorithm. We refer the
readers to the original paper [22] for the details of the algorithm.

5.3.2 Datasets
We generate the Grep and TeraSort datasets according to the
original MapReduce paper published by Google. The generated
datasets consists of N ﬁxed length records. Each record is a string
and occupies a line in the input ﬁle with the ﬁrst 10 bytes as a key
and the remaining 90 bytes as a value. In the Grep task, we are
required to search the pattern in the value part and in the TeraSort
task, we need to order the input records according to their keys.
Google generates the datasets using 512 MB data per-node setting.
We, however, adopt 1 GB data per-node setting since our HDFS
block size is 512 MB. Therefore, for the 1, 4, 16, 64 nodes clus-
ter, we, for each task (Grep and TeraSort), generate four datasets:
1 GB, 4 GB, 16 GB and 64 GB, one for each cluster setting.

We generate the TPC-H dataset using the dbgen tool shipped
with TPC-H benchmark. We follow the benchmark guide of Hive,
a SQL engine built on top of Hadoop, and generate 10GB data per
node. For the PageRank task, we use a real dataset from Twitter1.
The user proﬁles were crawled from July 6th to July 31st 2009.
For our experiments, we select 8 million vertices and their edges to
construct a graph.
5.4 The Grep Task

Figure 15 and Figure 16 present the performance of employing
epiC and Hadoop for performing Grep task with the cold ﬁle system
cache and the warm ﬁle system cache settings, respectively.

In the cold ﬁle system cache setting (Figure 15), epiC runs twice
faster than Hadoop in all cluster settings. The performance gap
between epiC and Hadoop is mainly due to the startup costs. The
heavy startup cost of Hadoop comes from two factors. First, for
each new MapReduce job, Hadoop must launch brand new java
processes for running the map tasks and reduce tasks. The second,
which is also the most important factor, is the inefﬁcient pulling
mechanism introduced by the RPC that Hadoop employed. In a
64-node cluster, the pulling RPC takes about 10(cid:24)15 seconds for
Hadoop to assign tasks to all free map slots. epiC, however, uses
the worker pool technique to avoid launching java processes for
performing new jobs and employs TTL RPC scheme to assign tasks
in real time. We are aware that Google has recently also adopted the
worker pool technique to reduce the startup latency of MapReduce
[7]. However, from the analysis of this task, clearly, in addition to
the pooling technique, efﬁcient RPC is also important.

In the warm ﬁle system cache setting (Figure 16), the perfor-
mance gap between epiC and Hadoop is even larger, up to a factor
of 4.5. We found that the performance of Hadoop cannot beneﬁt
from warm ﬁle system cache. Even, in the warm cache setting,
the data is read from fast cache memory instead of slow disks, the
performance of Hadoop is only improved by 10%. The reason of
this problem is again due to the inefﬁcient task assignments caused
by RPC. epiC, on the other hand, only takes about 4 seconds to
complete the Grep task in this setting, three times faster than per-
forming the same Grep task in cold cache setting. This is because
the bottleneck of epiC in performing the Grep task is I/O. In the
warm cache setting, the epiC Grep job can read data from memory
rather than disk. Thus, the performance is approaching optimality.
5.5 The TeraSort Task

Figure 17 and Figure 18 show the performance of the two sys-
tems (epiC and Hadoop) for performing TeraSort task in two set-
tings (i.e., warm and cold cache). epiC beats Hadoop in terms of
performance by a factor of two. There are two reasons for the per-
formance gap. First, the map task of Hadoop is CPU bound. On

1http://an.kaist.ac.kr/traces/WWW2010.html

epiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itFigure 5: The Programming Stack of epiC

for running the written program on the epiC’s common concurrent
runtime system.

We point out that the data processing model is problem domain
speciﬁc. For example, a MapReduce model is best suited for pro-
cessing unstructured data, a relational model is best suited for struc-
tured data and a graph model is best suited for graph data. The
common requirement is that programs written with these models
are all needed to be parallelized. Since Big Data is inherently multi-
structured, we build an Actor-like concurrent programming model
for a common runtime framework and offer epiC extensions for
users to specify domain speciﬁc data manipulation operations for
each data type. In the previous section, we have introduced the ba-
sic programming model of epiC. In this section, we focus on two
customized data processing model, the MapReduce model and re-
lational model. We will show how to implement them on top of
epiC.
3.1 The MapReduce Extension

We ﬁrst consider the MapReduce framework, and extend it to
work with epiC’s runtime framework. The MapReduce data pro-
cessing model consists of two interfaces:

(k1, v1)
(k2, list(v2)) ! list(v2)

map
reduce
Our MapReduce extension reuses Hadoop’s implementation of
these interfaces and other useful functions such as the partition.
This section only describes the auxiliary support which enables
users to run MapReduce programs on epiC and our own optimiza-
tions which are not supported in Hadoop.

! list(k2, v2)

3.1.1 General Abstractions
Running MapReduce on top of epiC is straightforward. We ﬁrst
place the map() function in a map unit and the reduce() func-
tion in a reduce unit. Then, we instantiate M map units and R
reduce units. The master network assigns a unique namespace to
each map and reduce unit. In the simplest case, the name addresses
of the units are like “x@MapUnit” and “y@ReduceUnit”, where
0 (cid:20) x < M and 0 (cid:20) y < R.

Based on the namespace, the MapUnit loads a partition of in-
put data and applies the customized map() function to process it.
The results are a set of key-value pairs. Here, a partition()
function is required to split the key-value pairs into multiple HDFS
ﬁles. Based on the application’s requirement, the partition()
can choose to sort the data by keys. By default, the partition()
simply applies the hash function to generate R ﬁles and assigns a
namespace to each ﬁle. The meta-data of the HDFS ﬁles are com-
posed into a message, which is sent to the master network. The
recipient is speciﬁed as all the ReduceUnit.

The master network then collects the messages from all MapUnits

and broadcasts them to the ReduceUnit. When a ReduceUnit
starts up, it loads the HDFS ﬁles that share the same namespace
with it. A possible merge-sort is required, if the results should be
sorted. Then, the customized reduce() function is invoked to

generate the ﬁnal results.

class Map implements Mapper {

void map() {
}

}
class Reduce implements Reducer {

void reduce() {
}

}
class MapUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new HdfsReader(s);
MapRunner map = new MapRunner(reader, Map());
map.run();
o.sendMessage("*@ReduceUnit",
map.getOutputMessage());

}

}
class ReduceUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
ReduceRunner red = new ReduceRunner(in,

Reduce());

red.run();

}

}

Here, we highlight the advantage of our design decision to de-
couple the data processing model and the concurrent programming
model. Suppose we want to extend the MapReduce programming
model to the Map-Reduce-Merge programming model [30]. All
we need to do is to add a new unit mergeUnit() and modify
the codes in the ReduceUnit to send messages to the master net-
work for declaring its output ﬁles. Compared to this non-intrusive
scheme, Hadoop needs to make dramatic changes to its runtime
system to support the same functionality [30] since Hadoop’s de-
sign bundles data processing model with concurrent programming
model.
3.1.2 Optimizations for MapReduce
In addition to the basic MapReduce implementation which is
similar to Hadoop, we add an optimization for map unit data pro-
cessing. We found that the map unit computation is CPU-bound
instead of I/O bound. The high CPU cost comes from the ﬁnal
sorting phase.

The Map unit needs to sort the intermediate key-value pairs since
MapReduce requires the reduce function to process key-value pairs
in an increasing order. Sorting in MapReduce is expensive since 1)
the sorting algorithm (i.e., quick sort) itself is CPU intensive and
2) the data de-serialization cost is not negligible. We employ two
techniques to improve the map unit sorting performance: 1) order-
preserving serialization and 2) high performance string sort (i.e.,
burst sort).

Deﬁnition 1. For a data type T , an order-preserving serialization
is an encoding scheme which serializes a variable x 2 T to a string
sx such that, for any two variables x 2 T and y 2 T , if x < y then
sx < sy in string lexicographical order.

In other words, the order-preserving serialization scheme serial-
izes keys so that the keys can be ordered by directly sorting their se-
rialized strings (in string lexicographical order) without de-serialization.
Note that the order-preserving serialization scheme exists for all
Java built-in data types.

(cid:258)(cid:258)(cid:38)(cid:82)(cid:81)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:53)(cid:88)(cid:81)(cid:87)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)}
class SingleTableUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new TableReader(s);
EmbededDBEngine

e =

new EmbededDBEngine(reader, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class JoinUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s1 = m[r.getNameAddress(LEFT\_TABLE)];
InputSplit s2 = m[r.getNameAddress(RIGHT\_TABLE)];
Reader in1 = new MapOutputReader(s1);
Reader in2 = new MapOutputReader(s2);
EmbededDBEngine e =

new EmbededDBEngine(in1, in2, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class AggregateUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
EmbededDBEngine e =

new EmbededDBEngine(in, getQuery());

e.process();

}

}

The abstractions are straightforward and we discard the detailed
discussion.
In each unit, we embed a customized query engine,
which can process single table queries, join queries and aggrega-
tions. We have not speciﬁed the recipients of each message in the
unit abstraction. This must be implemented by users for different
queries. However, as discussed later, we provide a query optimizer
to automatically ﬁll in the recipients. To show how users can adopt
the above relational model to process queries, let us consider the
following query (a variant of TPC-H Q3):

SELECT l orderkey, sum(l extendedprice*(1-l discount))

as revenue, o orderdate, o shippriority

FROM customer, orders, lineitem
WHERE c mktsegment = ’:1’ and c custkey = o custkey

and l orderkey = o orderkey and o orderdate
< date ’:2’ and l shipdate > date ’:2’

Group By o orderdate, o shippriority

Figure 6 to Figure 10 illustrate the processing of Q3 in epiC.
In step 1 (Figure 6), three types of the SingleTableUnits are
started to process the select/project operators of Lineitem, Orders
and Customer respectively. Note that those SingleTableUnits
run the same code. The only differences are their name addresses
and processed queries. The results are written back to the storage
system (either HDFS or distributed database). The meta-data of the
corresponding ﬁles are forwarded to the JoinUnits.

In step 2 and step 3 (Figures 7 and 8), we apply the hash-join
approach to process the data. In previous SingleTableUnits,
the output data are partitioned by the join keys. So the JoinUnit
can selectively load the paired partitions to perform the join. We
will discuss other possible join implementations in the next section.

Figure 6: Step 1 of Q3

We adopt burst sort algorithm to order the serialized strings. We
choose burst sort as our sorting technique since it is specially de-
signed for sorting large string collections and has been shown to
be signiﬁcantly faster than other candidates [25]. We brieﬂy out-
line the algorithm here. Interested readers should refer to [25] for
details. The burst sort technique sorts a string collection in two
passes. In the ﬁrst pass, the algorithm processes each input string
and stores the pointer of each string into a leaf node (bucket) in
a burst trie. The burst trie has a nice property that all leaf nodes
(buckets) are ordered. Thus, in the second pass, the algorithm pro-
cesses each bucket in order, applies a standard sorting technique
such as quick sort to sort strings, and produces the ﬁnal results.
The original burst sort requires a lot of additional memory to hold
the trie structure and thus does not scale well to a very large string
collection. We, thus, developed a memory efﬁcient burst sort im-
plementation which requires only 2 bits of additional space for each
key entry. We also use the multi-key quick sort algorithm [5] to sort
strings resided in the same bucket.

Combining the two techniques (i.e., order-preserving serializa-
tion and burst sort), our sorting scheme outperforms Hadoop’s quick
sort implementation by a factor of three to four.
3.2 Relational Model Extension

As pointed out earlier, for structured data, the relational data pro-
cessing model is most suited. Like the MapReduce extensions, we
can implement the relational model on top of epiC.
3.2.1 General Abstractions
Currently, three core units (SingleTableUnit, JoinUnit
and AggregateUnit) are deﬁned for the relational model. They
are capable of handling non-nested SQL queries. The SingleTableUnit
processes queries that involve only a partition of a single table.
The JoinUnit reads partitions from two tables and merge them
into one partition of the join table. Finally, the AggregateUnit
collects the partitions of different groups and computes the aggre-
gation results for each group. The abstractions of these units are
shown below. Currently, we adopt the synchronization model as
in MapReduce. Namely, we will start the next types of units, only
when all current units complete their processing. We will study
the possibility of creating a pipeline model in future work. Due to
space limitation, we only show the most important part.

class SingleTableQuery implements DBQuery {

void getQuery() {
}

}
class JoinQuery implements DBQuery {

void getQuery() {
}

}
class AggregateQuery implements DBQuery {

void getQuery() {
}

SingleTableUnitSingleTableUnitSingleTableUnitRelational Data of Customer/Orders/Lineitemselect c_custkey from Customer where c_mktsegment = ':1'select o_orderdate, o_custkey, o_orderkey, o_shippriority from Orders where o_orderdate < date ':2'select l_orderkey, l_extendedprice, l_discount from Lineitem where l_shipdate > date ':2'Master networkPartition info of CustomerPartition info of OrdersPartition info of LineitemFigure 7: Step 2 of Q3

Figure 8: Step 3 of Q3

Figure 9: Step 4 of Q3

Figure 10: Step 5 of Q3

Figure 11: Basic Join Operation

Figure 12: Semi-Join Operation

Finally, in step 4 (Figure 9), we perform the group operation
for two attributes. As the join results are partitioned into multiple
chunks, one SingleTableUnit can only generate the grouping
results for its own chunk. To produce the complete grouping re-
sults, we merge groups generated by different SingleTableUnits.
Therefore, in step 5 (Figure 10), one AggregateUnit needs to
load the partitions generated by all SingleTableUnits for the
same group to compute the ﬁnal aggregation results.

Our relational model simpliﬁes the query processing, as users
only need to consider how to partition the tables by the three units.
Moreover, it also provides the ﬂexibility of customized optimiza-
tion.

3.2.2 Optimizations for Relational Model
The relational model on epiC can be optimized in two layers, the

unit layer and the job layer.

In the unit layer, the user can adaptively combine the units to
implement different database operations. They can even write their
own units, such as ThetaJoinUnit, to extend the functional-
ity of our model. In this section, we use the euqi-join as an ex-
ample to illustrate the ﬂexibility of the model. Figure 11 shows
how the basic equi-join (S ◃▹ T ) is implemented in epiC. We ﬁrst
use the SingleTableUnit to scan the corresponding tables and
partition the tables by join keys. Then, the JoinUnit loads the
corresponding partitions to generate the results. In fact, the same
approach is also used in processing Q3. We partition the tables by
the keys in step 1 (Figure 6). So the following JoinUnits can
perform the join correctly.

Figure 13: Job Plan of Q3

However, if most of the tuples in S do not match tuples of T ,
semi-join is a better approach to reduce the overhead. Figure 12 il-
lustrates the idea. The ﬁrst SingleTableUnit scans table S and
only outputs the keys as the results. The keys are used in the next
SingleTableUnit to ﬁlter the tuples in T that cannot join with
S. The intermediate results are joined with S in the last JoinUnit
to produce the ﬁnal results. As shown in the example, semi-join can
be efﬁciently implemented using our relational model.

In the job layer, we offer a general query optimizer to translate
the SQL queries into an epiC job. Users can leverage the opti-
mizer to process their queries, instead of writing the codes for the
relational model by themselves. The optimizer works as a conven-
tional database optimizer. It ﬁrst generates an operator expression
tree for the SQL query and then groups the operators into differ-
ent units. The message ﬂow between units is also generated based
on the expression tree. To avoid a bad query plan, the optimizer
estimates the cost of the units based on the histograms. Currently,
we only consider the I/O costs. The optimizer will iterate over all
variants of the expression trees and select the one with the minimal
estimated cost. The corresponding epiC job is submitted to the pro-
cessing engine for execution. Figure 13 shows how the expression
tree is partitioned into units for Q3.

The query optimizer acts as the AQUA [29] for MapReduce or
PACTs compiler in Nephele [4]. But in epiC, the DAG between
units are not used for data shufﬂing as in Nephele. Instead, all rela-
tionships between units are maintained through the message pass-
ing and namespaces. All units fetch their data from the storage sys-
tem directly. This design follows the core concept of Actor model.
The advantage is three-fold: 1) we reduce the overhead of main-
taining the DAG; 2) we simplify the model as each unit runs in an

Create Partition JoinView1 as (Lineitem join Orders)Partition info of the Partial Results of Lineitem and OrdersJoinUnitPartial Results of Customer/LineitemMaster networkCreate Partition JoinView2 as (Customer join JoinView1)Partition info of the Partial Results of Customer and JoinView1JoinUnitPartial Results of Customer/JoinView1Master networkSelect * from JoinView2 Group By o_orderdate, o_shippriorityPartition info of the Partial Results of JoinView2SingleTableUnitPartial Results of JoinView2Master networkAggregateUnitPartial Results of Group ByCompute Aggregation Results for Each GroupMaster networkPartition info of GroupsSingleTableUnitTable S and TMaster networkSingleTableUnitselect * from S partitioned by S.keyselect * from T partitioned by T.foreignkeyJoinUnitTable S and TMaster networkselect * from S, T where S.key=T.foreignkeySingleTableUnitTable SMaster networkCreate Partition Tmp as (select key from S)SingleTableUnitTable T and TmpMaster networkCreate Partition Tmp2 as(select * from T where T.foreignkey in (select * from Tmp))JoinUnitTable S and Tmp2Master networkselect * from S, Tmp2 where S.key =Tmp2.foreignkeylineitemordersσl_shipdate>date ‘:2’π(l_orderkey, l_extendedprice, l_discount)σo_orderdate<date ‘:2’π(o_orderkey, o_custkey, o_orderdate, o_shippriority)customerσc_mktsegment=date ‘:1’π(c_custkey)GroupByo_orderdaye, o_shippriorirysum(l_extendedprice * (1 -l_discount)) as revenueSingleTableUnitSingleTableUnitSingleTableUnitJoinUnitJoinUnitSingleTableUnitAggregateUnitto the master, reporting the completed task identity. The master will
perform the call, updating its status, and responds to the worker
tracker.

This request-reply scheme is inefﬁcient for client to continuously
query information stored at the server. Consider the example of task
assignments. To get a new task for execution, the worker tracker
must periodically make getTask() RPC calls to the master since
the master hosts all task information and the worker tracker has no
idea of whether there are pending tasks. This periodical-pulling
scheme introduces non-negligible delays to the job startup since
users may submit jobs at arbitrary time point but the task assign-
ment is only performed at the ﬁxed time points. Suppose the worker
tracker queries a new task at time t0 and the query interval is T ,
then all tasks of jobs submitted at t1 > t0 will be delayed to t0 + T
for task assignment.

Since continuously querying server-side information is a com-
mon communication pattern in epiC, we develop a new RPC scheme
to eliminate the pulling interval in successive RPC calls for low la-
tency data processing.

Our approach is called the TTL RPC which is an extension of
the standard RPC scheme by associating each RPC call with a user
speciﬁed Time To Live (TTL) parameter T . The TTL parameter
T captures the duration the RPC can live on the server if no re-
sults are returned from the server; when the TTL expires, the RPC
is considered to have been served. For example, suppose we call
getTask() with T = 10s (seconds), when there is no task to as-
sign, instead of returning a null task immediately, the master holds
the call for at most 10 seconds. During that period, if the master
ﬁnds any pending tasks (e.g., due to new job submission), the mas-
ter returns the calling worker tracker with a new task. Otherwise, if
10 seconds passed and there are still no tasks to assign, the master
returns a null task to the worker tracker. The standard request-reply
RPC can be implemented by setting T = 0, namely no live.

We use a double-evaluation scheme to process a TTL-RPC call.
When the server receives a TTL-RPC call C, it performs an ini-
tial evaluation of C by treating it as a standard RPC call. If this
initial evaluation returns nothing, the server puts C into a pending
list. The TTL-RPC call will stay in the pending list for at most
T time. The server performs a second evaluation of C if either 1)
the information that C queries changes or 2) T time has passed.
The outcome of the second evaluation is returned as the ﬁnal result
to the client. Using TTL-RPC, the client can continuously make
RPC calls to the server in a loop without pulling interval and thus
receives server-side information in real time. We found that TTL-
RPC signiﬁcantly improves the performance of small jobs and re-
duces startup costs.

Even though the TTL-RPC scheme is a simple extension to the
standard RPC scheme, the implementation of TTL-RPC poses cer-
tain challenges for the threading model that the classical Java net-
work programs adopt. A typical Java network program employs
a per-thread per-request threading model. When a network con-
nection is established, the server serves the client by ﬁrst picking
up a thread from a thread pool, then reading data from the socket,
and ﬁnally performing the appropriate computations and writing
result back to the socket. The serving thread is returned to the
thread pool after the client is served. This per-thread per-request
threading model works well with the standard RPC communica-
tion. But it is not appropriate for our TTL RPC scheme since TTL
RPC request will stay at the server for a long time (We typically set
T = 20 (cid:24) 30 seconds). When multiple worker trackers make TTL
RPC calls to the master, the per-thread per-request threading model
produces a large number of hanging threads, quickly exhausting the
thread pool, and thus makes the master unable to respond.

Figure 14: The architecture of an epiC cluster

isolated way; 3) the model is more ﬂexible to support complex data
manipulation jobs (either synchronized or asynchronized).

4.

IMPLEMENTATION DETAILS

epiC is written in Java and built from scratch although we reuse
some Hadoop codes to implement a MapReduce extension. This
section describes the internals of epiC.

Like Hadoop, epiC is expected to be deployed on a shared-nothing
cluster of commodity machines connected with switched Ethernet.
It is designed to process data stored in any data sources such as
databases or distributed ﬁle systems. The epiC software mainly
consists of three components: master, worker tracker and worker
process. The architecture of epiC is shown in Figure 14. epiC
adopts a single master (this master is different from the servers
in the master network, which are mainly responsible for routing
messages and maintaining namespaces) multi-slaves architecture.
There is only one master node in an epiC cluster, running a mas-
ter daemon. The main function of the master is to command the
worker trackers to execute jobs. The master is also responsible
for managing and monitoring the health of the cluster. The master
runs a HTTP server which hosts such status information for human
consumption. It communicates with worker trackers and worker
processes through remote procedure call (RPC).

Each slave node in an epiC cluster runs a worker tracker dae-
mon. The worker tracker manages a worker pool, a ﬁxed number
of worker processes, for running units. We run each unit in a sin-
gle worker process. We adopt this ‘pooling’ process model instead
of an on-demand process model which launches worker processes
on demand for two reasons. First, pre-launching a pool of worker
processes reduces the startup latency of job execution since launch-
ing a brand new Java process introduces non-trivial startup costs
(typically 2(cid:24)3 seconds). Second, the latest HotSpot Java Virtual
Machine (JVM) employs a Just-In-Time (JIT) compilation tech-
nique to incrementally compile the Java byte codes into native ma-
chine codes for better performance. To fully unleash the power
of HotSpot JVM, one must run a Java program for a long time so
that every hot spot (a code segment, performing expensive com-
putations) of the program can be compiled by the JIT compiler.
Therefore, a never-ending worker process is the most appropriate
one for this purpose.

Here, we will focus on two most important parts of the imple-

mentations, the TTL RPC and the failure recovery.
4.1 The TTL RPC

The standard RPC scheme adopts a client-server request-reply
scheme to process RPC calls. In this scheme, a client sends a RPC
request to the server. The server processes this request and returns
its client with results. For example, when a task completes, the
worker tracker will perform a RPC call taskComplete(taskId)

(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:39)(cid:68)(cid:72)(cid:80)(cid:82)(cid:81)(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:17)(cid:17)(cid:17)(cid:53)(cid:51)(cid:38)(cid:53)(cid:51)(cid:38)We develop a pipeline threading model to ﬁx the above prob-
lems. The pipeline threading model uses a dedicated thread to per-
form the network I/O (i.e., reading request from and writing re-
sults to the socket) and a thread pool to perform the RPC calls.
When the network I/O thread receives a TTL RPC request, it noti-
ﬁes the server and keeps the established connection to be opened.
The server then picks up a serving thread from the thread pool and
performs the initial evaluation. The serving thread will return to the
thread pool after the initial evaluation no matter whether the initial
evaluation produces the results or not. The server will re-pickup a
thread from the thread pool for the second evaluation, if necessary,
and notify the network I/O thread to complete the client request by
sending out the results of the second evaluation. Using the pipeline
threading model, no thread (serving threads or network I/O thread)
will be hanged during the processing of TTL RPC call. Thus the
threading model is scalable to thousands of concurrent TTL RPC
calls.

4.2 Fault Tolerance

Like all single master cluster architecture, epiC is designed to
be resilient to a large-scale slave machines failures. epiC treats a
slave machine failure as a network partition from that slave ma-
chine to the master. To detect such a failure, the master communi-
cates with worker trackers running on the slave machines by heart-
beat RPCs. If the master cannot receive heartbeat messages from
a worker tracker many times, it marks that worker tracker as dead
and the machine where that worker tracker runs on as “failed”.

When a worker tracker is marked as failed, the master will deter-
mine whether the tasks that the worker tracker processed need to be
recovered. We assume that users persist the output of an epiC job
into a reliable storage system like HDFS or databases. Therefore,
all completed terminal tasks (i.e., tasks hosting units in the termi-
nal group) need not to be recovered. We only recover in-progress
terminal tasks and all non-terminal tasks (no matter completed or
in-progress).

We adopt task re-execution as the main technique for task recov-
ery and employ an asynchronous output backup scheme to speedup
the recovering process. The task re-execution strategy is concep-
tually simple. However, to make it work, we need to make some
reﬁnements to the basic design. The problem is that, in some cases,
the system may not ﬁnd idle worker processes for re-running the
failed tasks.

For example, let us consider a user job that consists of three unit
groups: a map unit group M with two reduce groups R1 and R2.
The output of M is processed by R1 and the output of R1 is further
processed by R2, the terminal unit group for producing the ﬁnal
output. epiC evaluates this job by placing three unit groups M, R1
and R2, in three stages S1, S2 and S3 respectively. The system ﬁrst
launches tasks in S1 and S2. When the tasks in S1 complete, the
system will launch tasks in S3, and at the same time, shufﬂe data
from S1’s units to S2’s units.
Suppose at this time, a work tracker failure causes a task m’s
(m 2 M) output to be lost, the master will fail to ﬁnd an idle
worker process for re-executing that failed task. This is because all
worker processes are running tasks in S2 and S3 and the data lost
introduced by m causes all tasks in S2 to be stalled. Therefore, no
worker process can complete and go back to the idle state.

We introduce a preemption scheduling scheme to solve the above
deadlock problem.
If a task A fails to fetch data produced by
task B, the task A will notify the master and update its state to
in (cid:0) stick. If the master cannot ﬁnd idle worker processes for re-
covering failed tasks for a given period of time, it will kill in(cid:0)stick
tasks by sending killTask() RPCs to the corresponding worker

Algorithm 1 Generate the list of completed tasks to backup
Input: the worker tracker list W
Output: the list of tasks L to backup
1: for each worker tracker w 2 W do
T   the list of completed tasks performed by w
2:
for each completed task t 2 T do
3:
4:
5:
6:
7:
8: end for

if EB(t) < ER(t) then

L   L [ ftg

end if
end for

trackers. The worker trackers then kill the in (cid:0) stick tasks and
release the corresponding worker processes. Finally, the master
marks the killed in (cid:0) stick tasks as failed and adds them to the
failed task list for scheduling. The preemption scheduling scheme
solves the deadlock problem since epiC executes tasks based on
the stage order. The released worker processes will ﬁrst execute
predecessor failed tasks and then the killed in (cid:0) stick tasks.

Re-execution is the only approach for recovering in-progress tasks.

For completed tasks, we also adopt a task output backup strategy
for recovering. This scheme works as follows. Periodically, the
master notiﬁes the worker trackers to upload the output of com-
pleted tasks to HDFS. When the master detects a worker tracker Wi
fails, it ﬁrst commands another live worker tracker Wj to download
Wi’s completed tasks’ output and then notiﬁes all in-progress tasks
that Wj will server Wi’s completed tasks’ output.

Backing up data to HDFS consumes network bandwidth. So, the
master decides to backup a completed task’s output only if the out-
put backup can yield better performance than task re-execution re-
covery. To make such a decision, for a completed task t, the master
estimates two expected execution time ER and EB of t where ER
is the expected execution time when the task re-execution scheme
is adopted and EB is the expected execution time when the output
backup strategy is chosen. ER and EB are computed as follows

ER = Tt (cid:2) P + 2Tt (cid:2) (1 (cid:0) P )

EB = (Tt + Tu) (cid:2) P + Td (cid:2) (1 (cid:0) P )

(1)

(2)

where P is the probability that the worker track is available during
the job execution; Tt is the execution time of t; Tu is the elapsed
time for uploading output to HDFS; and Td is the elapsed time for
downloading output from HDFS. The three parameters Tt, Tu and
Td are easily collected or estimated. The parameter P is estimated
by the availability of a worker tracker in one day.

The master uses Alg. 1 for determining which completed tasks
should be backed up. The master iterates over each worker tracker
(line 1). For each worker tracker, the master retrieves its completed
task list (line 2). Then, for each task in the completed task list, the
master computes EB and ER and adds the task t into the result list
L if EB < ER (line 4 to line 5).

5. EXPERIMENTS

We evaluate the performance of epiC on different kinds of data
processing tasks, including unstructured data processing, relational
data processing and graph processing. We benchmark epiC against
Hadoop, an open source implementation of MapReduce for pro-
cessing unstructured data (i.e., text data) and relational data and
GPS [24], an open source implementation of Pregel [20] for graph
processing, respectively. For relational data, we also run additional

experiments to benchmark epiC with two new in-memory data pro-
cessing systems, namely Shark and Impala. For all experiments, the
results are reported by averaging six runs.
5.1 Benchmark Environment

The experimental study is conducted on an in-house cluster, con-
sisting of 72 nodes hosted on two racks. The nodes within each
rack are connected by a 1 Gbps switch. The two racks are con-
nected by a 10 Gbps cluster switch. Each cluster node is equipped
with a quad-core Intel Xeon 2.4GHz CPU, 8GB memory and two
500 GB SCSI disks. The hdparm utility reports that the buffered
read throughput of the disk is roughly 110 MB/sec. However, due
to the JVM costs, our tested Java program can only read local ﬁles
at 70 (cid:24) 80 MB/sec.

We choose 65 nodes out of the 72 nodes for our benchmark.
For the 65-node cluster, one node acts as the master for running
Hadoop’s NameNode, JobTracker daemons, GPS’s server node and
epiC’s master daemon. For scalability benchmark, we vary the
number of slave nodes from 1, 4, 16, to 64.
5.2 System Settings

In our experiments, we conﬁgure the three systems as follows:

1. The Hadoop settings consist of two parts: HDFS settings and
MapReduce settings.
In HDFS settings, we set the block
size to be 512 MB. As indicated in [17], this setting can sig-
niﬁcantly reduce Hadoop’s cost for scheduling MapReduce
tasks. We also set the I/O buffer size to 128 KB and the repli-
cation factor of HDFS to one (i.e., no replication). In MapRe-
duce settings, each slave is conﬁgured to run two concurrent
map and reduce tasks. The JVM runs in the server mode with
maximal 1.5 GB heap memory. The size of map task’s sort
buffer is 512 MB. We set the merge factor to be 500 and turn
off speculation scheduling. Finally, we set the JVM reuse
number to -1.

2. For each worker tracker in epiC, we set the size of the worker
pool to be four. In the worker pool, two workers are current
workers (running current units) and the remaining two work-
ers are appending workers. Similar to Hadoop’s setting, each
worker process has 1.5 GB memory. For the MapReduce ex-
tension, we set the bucket size of burst sort to be 8192 keys
(string pointers).

3. For GPS, we employ the default settings of the system with-

out further tuning.

5.3 Benchmark Tasks and Datasets

5.3.1 Benchmark Tasks
The benchmark consists of four tasks: Grep, TeraSort, TPC-H
Q3, and PageRank. The Grep task and TeraSort task are presented
in the original MapReduce paper for demonstrating the scalability
and the efﬁciency of using MapReduce for processing unstructured
data (i.e., plain text data). The Grep task requires us to check each
record (i.e., a line of text string) of the input dataset and output
all records containing a speciﬁc pattern string. The TeraSort task
requires the system to arrange the input records in an ascending or-
der. The TPC-H Q3 task is a standard benchmark query in TPC-H
benchmark and is presented in Section 3.2. The PageRank algo-
rithm [22] is an iterative graph processing algorithm. We refer the
readers to the original paper [22] for the details of the algorithm.

5.3.2 Datasets
We generate the Grep and TeraSort datasets according to the
original MapReduce paper published by Google. The generated
datasets consists of N ﬁxed length records. Each record is a string
and occupies a line in the input ﬁle with the ﬁrst 10 bytes as a key
and the remaining 90 bytes as a value. In the Grep task, we are
required to search the pattern in the value part and in the TeraSort
task, we need to order the input records according to their keys.
Google generates the datasets using 512 MB data per-node setting.
We, however, adopt 1 GB data per-node setting since our HDFS
block size is 512 MB. Therefore, for the 1, 4, 16, 64 nodes clus-
ter, we, for each task (Grep and TeraSort), generate four datasets:
1 GB, 4 GB, 16 GB and 64 GB, one for each cluster setting.

We generate the TPC-H dataset using the dbgen tool shipped
with TPC-H benchmark. We follow the benchmark guide of Hive,
a SQL engine built on top of Hadoop, and generate 10GB data per
node. For the PageRank task, we use a real dataset from Twitter1.
The user proﬁles were crawled from July 6th to July 31st 2009.
For our experiments, we select 8 million vertices and their edges to
construct a graph.
5.4 The Grep Task

Figure 15 and Figure 16 present the performance of employing
epiC and Hadoop for performing Grep task with the cold ﬁle system
cache and the warm ﬁle system cache settings, respectively.

In the cold ﬁle system cache setting (Figure 15), epiC runs twice
faster than Hadoop in all cluster settings. The performance gap
between epiC and Hadoop is mainly due to the startup costs. The
heavy startup cost of Hadoop comes from two factors. First, for
each new MapReduce job, Hadoop must launch brand new java
processes for running the map tasks and reduce tasks. The second,
which is also the most important factor, is the inefﬁcient pulling
mechanism introduced by the RPC that Hadoop employed. In a
64-node cluster, the pulling RPC takes about 10(cid:24)15 seconds for
Hadoop to assign tasks to all free map slots. epiC, however, uses
the worker pool technique to avoid launching java processes for
performing new jobs and employs TTL RPC scheme to assign tasks
in real time. We are aware that Google has recently also adopted the
worker pool technique to reduce the startup latency of MapReduce
[7]. However, from the analysis of this task, clearly, in addition to
the pooling technique, efﬁcient RPC is also important.

In the warm ﬁle system cache setting (Figure 16), the perfor-
mance gap between epiC and Hadoop is even larger, up to a factor
of 4.5. We found that the performance of Hadoop cannot beneﬁt
from warm ﬁle system cache. Even, in the warm cache setting,
the data is read from fast cache memory instead of slow disks, the
performance of Hadoop is only improved by 10%. The reason of
this problem is again due to the inefﬁcient task assignments caused
by RPC. epiC, on the other hand, only takes about 4 seconds to
complete the Grep task in this setting, three times faster than per-
forming the same Grep task in cold cache setting. This is because
the bottleneck of epiC in performing the Grep task is I/O. In the
warm cache setting, the epiC Grep job can read data from memory
rather than disk. Thus, the performance is approaching optimality.
5.5 The TeraSort Task

Figure 17 and Figure 18 show the performance of the two sys-
tems (epiC and Hadoop) for performing TeraSort task in two set-
tings (i.e., warm and cold cache). epiC beats Hadoop in terms of
performance by a factor of two. There are two reasons for the per-
formance gap. First, the map task of Hadoop is CPU bound. On

1http://an.kaist.ac.kr/traces/WWW2010.html

Figure 15: Results of Grep task with cold
ﬁle system cache

Figure 16: Results of Grep task with warm
ﬁle system cache

Figure 17: Results of TeraSort task with
cold ﬁle system cache

Figure 18: Results of TeraSort task with
warm ﬁle system cache

Figure 19: Results of TPC-H Q3

Figure 20: Results of PageRank

average, a map task takes about 7 seconds to read off data from disk
and then takes about 10 seconds to sort the intermediate data. Fi-
nally, another 8 seconds are required to write the intermediate data
to local disks. Sorting approximately occupies 50% of the map exe-
cution time. Second, due to the poor pulling RPC performance, the
notiﬁcations of map tasks cannot be propagated to the reduce tasks
in a timely manner. Therefore, there is a noticeable gap between
map completion and reduce shufﬂing.

epiC, however, has no such bottleneck. Equipped with order-
preserving encoding and burst sort technique, epiC, on average, is
able to sort the intermediate data at about 2.1 seconds, roughly ﬁve
times faster than Hadoop. Also, epiC’s TTL RPC scheme enables
reduce units to receive map completion notiﬁcations in real time.
epiC is able to start shufﬂing 5(cid:24)8 seconds earlier than Hadoop.

Compared to the performance of cold cache setting (Figure 17),
both epiC and Hadoop do not run much faster in the warm cache
setting (Figure 18); there is a 10% improvement at most. This is
because scanning data from disks is not the bottleneck of perform-
ing the TeraSort task. For Hadoop, the bottleneck is the map-side
sorting and data shufﬂing. For epiC, the bottleneck of the map unit
is in persisting intermediate data to disks and the bottleneck of the
reduce unit is in shufﬂing which is network bound. We are plan-
ning to eliminate the map unit data persisting cost by building an
in-memory ﬁle system for holding and shufﬂing intermediate data.

5.6 The TPC(cid:173)H Q3 Task

Figure 19 presents the results of employing epiC and Hadoop to
perform TPC-H Q3 under cold ﬁle system cache 2. For Hadoop,
we ﬁrst use Hive to generate the query plan. Then, according to the
generated query plan, we manually wrote MapReduce programs to
perform this task. Our manually coded MapReduce program runs
30% faster than Hive’s native interpreter based evaluation scheme.
The MapReduce programs consist of ﬁve jobs. The ﬁrst job joins

2For TPC-H Q3 task and PageRank task, the three systems cannot
get a signiﬁcant performance improvement from cache. Therefore,
we remove warm cache results to save space.

customer and orders and produces the join results I1. The sec-
ond job joins I1 with lineitem, followed by aggregating, sort-
ing, and limiting top ten results performed by the remaining three
jobs. The query plan and unit implementation of epiC is presented
in Section 3.2.

Figure 19 shows that epiC runs about 2.5 times faster than Hadoop.

This is because epiC uses fewer operations to evaluate the query (5
units vs. 5 maps and 5 reduces) than Hadoop and employs the asyn-
chronous mechanism for running units. In Hadoop, the ﬁve jobs
run sequentially. Thus, the down stream mappers must wait for the
completion of all up stream reducers to start. In epiC, however,
down stream units can start without waiting for the completion of
up stream units.
5.7 The PageRank Task

This experiment compares three systems in performing the PageR-
ank task. The GPS implementation of PageRank algorithm is iden-
tical to [20]. The epiC implementation of PageRank algorithm con-
sists of a single unit. The details are discussed in Section 2.2. The
Hadoop implementation includes a series of iterative jobs. Each
job reads the output of the previous job to compute the new PageR-
ank values. Similar to the unit of epiC, each mapper and reducer
in Hadoop will process a batch of vertices.
In all experiments,
the PageRank algorithm terminates after 20 iterations. Figure 20
presents the results of the experiment. We ﬁnd that all systems can
provide a scalable performance. However, among the three, epiC
has a better speedup. This is because epiC adopts an asynchronous
communication pattern based on message passing, whereas GPS
needs to synchronize the processing nodes and Hadoop repeatedly
creates new mappers and reducers for each job.
5.8 Fault Tolerance

The ﬁnal experiment studies the ability of epiC for handling
machine failures. In this experiment, both epiC and Hadoop are
employed for performing the TeraSort task. During the data pro-
cessing, we simulate slave machine failures by killing all daemon
processes (TaskTracker, DataNode and worker tracker) running on
those machines. The replication factor of HDFS is set to three,

 0 5 10 15 20 25 30 351 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 5 10 15 20 25 30 351 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 20 40 60 80 100 120 1401 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 20 40 60 80 100 120 1401 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 100 200 300 400 500 600 700 8001 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 100 200 300 400 500 600 700 800 9001 node4 nodes16 nodes64 nodesSecondsHadoopGPSepiCepiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itFigure 5: The Programming Stack of epiC

for running the written program on the epiC’s common concurrent
runtime system.

We point out that the data processing model is problem domain
speciﬁc. For example, a MapReduce model is best suited for pro-
cessing unstructured data, a relational model is best suited for struc-
tured data and a graph model is best suited for graph data. The
common requirement is that programs written with these models
are all needed to be parallelized. Since Big Data is inherently multi-
structured, we build an Actor-like concurrent programming model
for a common runtime framework and offer epiC extensions for
users to specify domain speciﬁc data manipulation operations for
each data type. In the previous section, we have introduced the ba-
sic programming model of epiC. In this section, we focus on two
customized data processing model, the MapReduce model and re-
lational model. We will show how to implement them on top of
epiC.
3.1 The MapReduce Extension

We ﬁrst consider the MapReduce framework, and extend it to
work with epiC’s runtime framework. The MapReduce data pro-
cessing model consists of two interfaces:

(k1, v1)
(k2, list(v2)) ! list(v2)

map
reduce
Our MapReduce extension reuses Hadoop’s implementation of
these interfaces and other useful functions such as the partition.
This section only describes the auxiliary support which enables
users to run MapReduce programs on epiC and our own optimiza-
tions which are not supported in Hadoop.

! list(k2, v2)

3.1.1 General Abstractions
Running MapReduce on top of epiC is straightforward. We ﬁrst
place the map() function in a map unit and the reduce() func-
tion in a reduce unit. Then, we instantiate M map units and R
reduce units. The master network assigns a unique namespace to
each map and reduce unit. In the simplest case, the name addresses
of the units are like “x@MapUnit” and “y@ReduceUnit”, where
0 (cid:20) x < M and 0 (cid:20) y < R.

Based on the namespace, the MapUnit loads a partition of in-
put data and applies the customized map() function to process it.
The results are a set of key-value pairs. Here, a partition()
function is required to split the key-value pairs into multiple HDFS
ﬁles. Based on the application’s requirement, the partition()
can choose to sort the data by keys. By default, the partition()
simply applies the hash function to generate R ﬁles and assigns a
namespace to each ﬁle. The meta-data of the HDFS ﬁles are com-
posed into a message, which is sent to the master network. The
recipient is speciﬁed as all the ReduceUnit.

The master network then collects the messages from all MapUnits

and broadcasts them to the ReduceUnit. When a ReduceUnit
starts up, it loads the HDFS ﬁles that share the same namespace
with it. A possible merge-sort is required, if the results should be
sorted. Then, the customized reduce() function is invoked to

generate the ﬁnal results.

class Map implements Mapper {

void map() {
}

}
class Reduce implements Reducer {

void reduce() {
}

}
class MapUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new HdfsReader(s);
MapRunner map = new MapRunner(reader, Map());
map.run();
o.sendMessage("*@ReduceUnit",
map.getOutputMessage());

}

}
class ReduceUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
ReduceRunner red = new ReduceRunner(in,

Reduce());

red.run();

}

}

Here, we highlight the advantage of our design decision to de-
couple the data processing model and the concurrent programming
model. Suppose we want to extend the MapReduce programming
model to the Map-Reduce-Merge programming model [30]. All
we need to do is to add a new unit mergeUnit() and modify
the codes in the ReduceUnit to send messages to the master net-
work for declaring its output ﬁles. Compared to this non-intrusive
scheme, Hadoop needs to make dramatic changes to its runtime
system to support the same functionality [30] since Hadoop’s de-
sign bundles data processing model with concurrent programming
model.
3.1.2 Optimizations for MapReduce
In addition to the basic MapReduce implementation which is
similar to Hadoop, we add an optimization for map unit data pro-
cessing. We found that the map unit computation is CPU-bound
instead of I/O bound. The high CPU cost comes from the ﬁnal
sorting phase.

The Map unit needs to sort the intermediate key-value pairs since
MapReduce requires the reduce function to process key-value pairs
in an increasing order. Sorting in MapReduce is expensive since 1)
the sorting algorithm (i.e., quick sort) itself is CPU intensive and
2) the data de-serialization cost is not negligible. We employ two
techniques to improve the map unit sorting performance: 1) order-
preserving serialization and 2) high performance string sort (i.e.,
burst sort).

Deﬁnition 1. For a data type T , an order-preserving serialization
is an encoding scheme which serializes a variable x 2 T to a string
sx such that, for any two variables x 2 T and y 2 T , if x < y then
sx < sy in string lexicographical order.

In other words, the order-preserving serialization scheme serial-
izes keys so that the keys can be ordered by directly sorting their se-
rialized strings (in string lexicographical order) without de-serialization.
Note that the order-preserving serialization scheme exists for all
Java built-in data types.

(cid:258)(cid:258)(cid:38)(cid:82)(cid:81)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:53)(cid:88)(cid:81)(cid:87)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)}
class SingleTableUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new TableReader(s);
EmbededDBEngine

e =

new EmbededDBEngine(reader, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class JoinUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s1 = m[r.getNameAddress(LEFT\_TABLE)];
InputSplit s2 = m[r.getNameAddress(RIGHT\_TABLE)];
Reader in1 = new MapOutputReader(s1);
Reader in2 = new MapOutputReader(s2);
EmbededDBEngine e =

new EmbededDBEngine(in1, in2, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class AggregateUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
EmbededDBEngine e =

new EmbededDBEngine(in, getQuery());

e.process();

}

}

The abstractions are straightforward and we discard the detailed
discussion.
In each unit, we embed a customized query engine,
which can process single table queries, join queries and aggrega-
tions. We have not speciﬁed the recipients of each message in the
unit abstraction. This must be implemented by users for different
queries. However, as discussed later, we provide a query optimizer
to automatically ﬁll in the recipients. To show how users can adopt
the above relational model to process queries, let us consider the
following query (a variant of TPC-H Q3):

SELECT l orderkey, sum(l extendedprice*(1-l discount))

as revenue, o orderdate, o shippriority

FROM customer, orders, lineitem
WHERE c mktsegment = ’:1’ and c custkey = o custkey

and l orderkey = o orderkey and o orderdate
< date ’:2’ and l shipdate > date ’:2’

Group By o orderdate, o shippriority

Figure 6 to Figure 10 illustrate the processing of Q3 in epiC.
In step 1 (Figure 6), three types of the SingleTableUnits are
started to process the select/project operators of Lineitem, Orders
and Customer respectively. Note that those SingleTableUnits
run the same code. The only differences are their name addresses
and processed queries. The results are written back to the storage
system (either HDFS or distributed database). The meta-data of the
corresponding ﬁles are forwarded to the JoinUnits.

In step 2 and step 3 (Figures 7 and 8), we apply the hash-join
approach to process the data. In previous SingleTableUnits,
the output data are partitioned by the join keys. So the JoinUnit
can selectively load the paired partitions to perform the join. We
will discuss other possible join implementations in the next section.

Figure 6: Step 1 of Q3

We adopt burst sort algorithm to order the serialized strings. We
choose burst sort as our sorting technique since it is specially de-
signed for sorting large string collections and has been shown to
be signiﬁcantly faster than other candidates [25]. We brieﬂy out-
line the algorithm here. Interested readers should refer to [25] for
details. The burst sort technique sorts a string collection in two
passes. In the ﬁrst pass, the algorithm processes each input string
and stores the pointer of each string into a leaf node (bucket) in
a burst trie. The burst trie has a nice property that all leaf nodes
(buckets) are ordered. Thus, in the second pass, the algorithm pro-
cesses each bucket in order, applies a standard sorting technique
such as quick sort to sort strings, and produces the ﬁnal results.
The original burst sort requires a lot of additional memory to hold
the trie structure and thus does not scale well to a very large string
collection. We, thus, developed a memory efﬁcient burst sort im-
plementation which requires only 2 bits of additional space for each
key entry. We also use the multi-key quick sort algorithm [5] to sort
strings resided in the same bucket.

Combining the two techniques (i.e., order-preserving serializa-
tion and burst sort), our sorting scheme outperforms Hadoop’s quick
sort implementation by a factor of three to four.
3.2 Relational Model Extension

As pointed out earlier, for structured data, the relational data pro-
cessing model is most suited. Like the MapReduce extensions, we
can implement the relational model on top of epiC.
3.2.1 General Abstractions
Currently, three core units (SingleTableUnit, JoinUnit
and AggregateUnit) are deﬁned for the relational model. They
are capable of handling non-nested SQL queries. The SingleTableUnit
processes queries that involve only a partition of a single table.
The JoinUnit reads partitions from two tables and merge them
into one partition of the join table. Finally, the AggregateUnit
collects the partitions of different groups and computes the aggre-
gation results for each group. The abstractions of these units are
shown below. Currently, we adopt the synchronization model as
in MapReduce. Namely, we will start the next types of units, only
when all current units complete their processing. We will study
the possibility of creating a pipeline model in future work. Due to
space limitation, we only show the most important part.

class SingleTableQuery implements DBQuery {

void getQuery() {
}

}
class JoinQuery implements DBQuery {

void getQuery() {
}

}
class AggregateQuery implements DBQuery {

void getQuery() {
}

SingleTableUnitSingleTableUnitSingleTableUnitRelational Data of Customer/Orders/Lineitemselect c_custkey from Customer where c_mktsegment = ':1'select o_orderdate, o_custkey, o_orderkey, o_shippriority from Orders where o_orderdate < date ':2'select l_orderkey, l_extendedprice, l_discount from Lineitem where l_shipdate > date ':2'Master networkPartition info of CustomerPartition info of OrdersPartition info of LineitemFigure 7: Step 2 of Q3

Figure 8: Step 3 of Q3

Figure 9: Step 4 of Q3

Figure 10: Step 5 of Q3

Figure 11: Basic Join Operation

Figure 12: Semi-Join Operation

Finally, in step 4 (Figure 9), we perform the group operation
for two attributes. As the join results are partitioned into multiple
chunks, one SingleTableUnit can only generate the grouping
results for its own chunk. To produce the complete grouping re-
sults, we merge groups generated by different SingleTableUnits.
Therefore, in step 5 (Figure 10), one AggregateUnit needs to
load the partitions generated by all SingleTableUnits for the
same group to compute the ﬁnal aggregation results.

Our relational model simpliﬁes the query processing, as users
only need to consider how to partition the tables by the three units.
Moreover, it also provides the ﬂexibility of customized optimiza-
tion.

3.2.2 Optimizations for Relational Model
The relational model on epiC can be optimized in two layers, the

unit layer and the job layer.

In the unit layer, the user can adaptively combine the units to
implement different database operations. They can even write their
own units, such as ThetaJoinUnit, to extend the functional-
ity of our model. In this section, we use the euqi-join as an ex-
ample to illustrate the ﬂexibility of the model. Figure 11 shows
how the basic equi-join (S ◃▹ T ) is implemented in epiC. We ﬁrst
use the SingleTableUnit to scan the corresponding tables and
partition the tables by join keys. Then, the JoinUnit loads the
corresponding partitions to generate the results. In fact, the same
approach is also used in processing Q3. We partition the tables by
the keys in step 1 (Figure 6). So the following JoinUnits can
perform the join correctly.

Figure 13: Job Plan of Q3

However, if most of the tuples in S do not match tuples of T ,
semi-join is a better approach to reduce the overhead. Figure 12 il-
lustrates the idea. The ﬁrst SingleTableUnit scans table S and
only outputs the keys as the results. The keys are used in the next
SingleTableUnit to ﬁlter the tuples in T that cannot join with
S. The intermediate results are joined with S in the last JoinUnit
to produce the ﬁnal results. As shown in the example, semi-join can
be efﬁciently implemented using our relational model.

In the job layer, we offer a general query optimizer to translate
the SQL queries into an epiC job. Users can leverage the opti-
mizer to process their queries, instead of writing the codes for the
relational model by themselves. The optimizer works as a conven-
tional database optimizer. It ﬁrst generates an operator expression
tree for the SQL query and then groups the operators into differ-
ent units. The message ﬂow between units is also generated based
on the expression tree. To avoid a bad query plan, the optimizer
estimates the cost of the units based on the histograms. Currently,
we only consider the I/O costs. The optimizer will iterate over all
variants of the expression trees and select the one with the minimal
estimated cost. The corresponding epiC job is submitted to the pro-
cessing engine for execution. Figure 13 shows how the expression
tree is partitioned into units for Q3.

The query optimizer acts as the AQUA [29] for MapReduce or
PACTs compiler in Nephele [4]. But in epiC, the DAG between
units are not used for data shufﬂing as in Nephele. Instead, all rela-
tionships between units are maintained through the message pass-
ing and namespaces. All units fetch their data from the storage sys-
tem directly. This design follows the core concept of Actor model.
The advantage is three-fold: 1) we reduce the overhead of main-
taining the DAG; 2) we simplify the model as each unit runs in an

Create Partition JoinView1 as (Lineitem join Orders)Partition info of the Partial Results of Lineitem and OrdersJoinUnitPartial Results of Customer/LineitemMaster networkCreate Partition JoinView2 as (Customer join JoinView1)Partition info of the Partial Results of Customer and JoinView1JoinUnitPartial Results of Customer/JoinView1Master networkSelect * from JoinView2 Group By o_orderdate, o_shippriorityPartition info of the Partial Results of JoinView2SingleTableUnitPartial Results of JoinView2Master networkAggregateUnitPartial Results of Group ByCompute Aggregation Results for Each GroupMaster networkPartition info of GroupsSingleTableUnitTable S and TMaster networkSingleTableUnitselect * from S partitioned by S.keyselect * from T partitioned by T.foreignkeyJoinUnitTable S and TMaster networkselect * from S, T where S.key=T.foreignkeySingleTableUnitTable SMaster networkCreate Partition Tmp as (select key from S)SingleTableUnitTable T and TmpMaster networkCreate Partition Tmp2 as(select * from T where T.foreignkey in (select * from Tmp))JoinUnitTable S and Tmp2Master networkselect * from S, Tmp2 where S.key =Tmp2.foreignkeylineitemordersσl_shipdate>date ‘:2’π(l_orderkey, l_extendedprice, l_discount)σo_orderdate<date ‘:2’π(o_orderkey, o_custkey, o_orderdate, o_shippriority)customerσc_mktsegment=date ‘:1’π(c_custkey)GroupByo_orderdaye, o_shippriorirysum(l_extendedprice * (1 -l_discount)) as revenueSingleTableUnitSingleTableUnitSingleTableUnitJoinUnitJoinUnitSingleTableUnitAggregateUnitto the master, reporting the completed task identity. The master will
perform the call, updating its status, and responds to the worker
tracker.

This request-reply scheme is inefﬁcient for client to continuously
query information stored at the server. Consider the example of task
assignments. To get a new task for execution, the worker tracker
must periodically make getTask() RPC calls to the master since
the master hosts all task information and the worker tracker has no
idea of whether there are pending tasks. This periodical-pulling
scheme introduces non-negligible delays to the job startup since
users may submit jobs at arbitrary time point but the task assign-
ment is only performed at the ﬁxed time points. Suppose the worker
tracker queries a new task at time t0 and the query interval is T ,
then all tasks of jobs submitted at t1 > t0 will be delayed to t0 + T
for task assignment.

Since continuously querying server-side information is a com-
mon communication pattern in epiC, we develop a new RPC scheme
to eliminate the pulling interval in successive RPC calls for low la-
tency data processing.

Our approach is called the TTL RPC which is an extension of
the standard RPC scheme by associating each RPC call with a user
speciﬁed Time To Live (TTL) parameter T . The TTL parameter
T captures the duration the RPC can live on the server if no re-
sults are returned from the server; when the TTL expires, the RPC
is considered to have been served. For example, suppose we call
getTask() with T = 10s (seconds), when there is no task to as-
sign, instead of returning a null task immediately, the master holds
the call for at most 10 seconds. During that period, if the master
ﬁnds any pending tasks (e.g., due to new job submission), the mas-
ter returns the calling worker tracker with a new task. Otherwise, if
10 seconds passed and there are still no tasks to assign, the master
returns a null task to the worker tracker. The standard request-reply
RPC can be implemented by setting T = 0, namely no live.

We use a double-evaluation scheme to process a TTL-RPC call.
When the server receives a TTL-RPC call C, it performs an ini-
tial evaluation of C by treating it as a standard RPC call. If this
initial evaluation returns nothing, the server puts C into a pending
list. The TTL-RPC call will stay in the pending list for at most
T time. The server performs a second evaluation of C if either 1)
the information that C queries changes or 2) T time has passed.
The outcome of the second evaluation is returned as the ﬁnal result
to the client. Using TTL-RPC, the client can continuously make
RPC calls to the server in a loop without pulling interval and thus
receives server-side information in real time. We found that TTL-
RPC signiﬁcantly improves the performance of small jobs and re-
duces startup costs.

Even though the TTL-RPC scheme is a simple extension to the
standard RPC scheme, the implementation of TTL-RPC poses cer-
tain challenges for the threading model that the classical Java net-
work programs adopt. A typical Java network program employs
a per-thread per-request threading model. When a network con-
nection is established, the server serves the client by ﬁrst picking
up a thread from a thread pool, then reading data from the socket,
and ﬁnally performing the appropriate computations and writing
result back to the socket. The serving thread is returned to the
thread pool after the client is served. This per-thread per-request
threading model works well with the standard RPC communica-
tion. But it is not appropriate for our TTL RPC scheme since TTL
RPC request will stay at the server for a long time (We typically set
T = 20 (cid:24) 30 seconds). When multiple worker trackers make TTL
RPC calls to the master, the per-thread per-request threading model
produces a large number of hanging threads, quickly exhausting the
thread pool, and thus makes the master unable to respond.

Figure 14: The architecture of an epiC cluster

isolated way; 3) the model is more ﬂexible to support complex data
manipulation jobs (either synchronized or asynchronized).

4.

IMPLEMENTATION DETAILS

epiC is written in Java and built from scratch although we reuse
some Hadoop codes to implement a MapReduce extension. This
section describes the internals of epiC.

Like Hadoop, epiC is expected to be deployed on a shared-nothing
cluster of commodity machines connected with switched Ethernet.
It is designed to process data stored in any data sources such as
databases or distributed ﬁle systems. The epiC software mainly
consists of three components: master, worker tracker and worker
process. The architecture of epiC is shown in Figure 14. epiC
adopts a single master (this master is different from the servers
in the master network, which are mainly responsible for routing
messages and maintaining namespaces) multi-slaves architecture.
There is only one master node in an epiC cluster, running a mas-
ter daemon. The main function of the master is to command the
worker trackers to execute jobs. The master is also responsible
for managing and monitoring the health of the cluster. The master
runs a HTTP server which hosts such status information for human
consumption. It communicates with worker trackers and worker
processes through remote procedure call (RPC).

Each slave node in an epiC cluster runs a worker tracker dae-
mon. The worker tracker manages a worker pool, a ﬁxed number
of worker processes, for running units. We run each unit in a sin-
gle worker process. We adopt this ‘pooling’ process model instead
of an on-demand process model which launches worker processes
on demand for two reasons. First, pre-launching a pool of worker
processes reduces the startup latency of job execution since launch-
ing a brand new Java process introduces non-trivial startup costs
(typically 2(cid:24)3 seconds). Second, the latest HotSpot Java Virtual
Machine (JVM) employs a Just-In-Time (JIT) compilation tech-
nique to incrementally compile the Java byte codes into native ma-
chine codes for better performance. To fully unleash the power
of HotSpot JVM, one must run a Java program for a long time so
that every hot spot (a code segment, performing expensive com-
putations) of the program can be compiled by the JIT compiler.
Therefore, a never-ending worker process is the most appropriate
one for this purpose.

Here, we will focus on two most important parts of the imple-

mentations, the TTL RPC and the failure recovery.
4.1 The TTL RPC

The standard RPC scheme adopts a client-server request-reply
scheme to process RPC calls. In this scheme, a client sends a RPC
request to the server. The server processes this request and returns
its client with results. For example, when a task completes, the
worker tracker will perform a RPC call taskComplete(taskId)

(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:39)(cid:68)(cid:72)(cid:80)(cid:82)(cid:81)(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:17)(cid:17)(cid:17)(cid:53)(cid:51)(cid:38)(cid:53)(cid:51)(cid:38)We develop a pipeline threading model to ﬁx the above prob-
lems. The pipeline threading model uses a dedicated thread to per-
form the network I/O (i.e., reading request from and writing re-
sults to the socket) and a thread pool to perform the RPC calls.
When the network I/O thread receives a TTL RPC request, it noti-
ﬁes the server and keeps the established connection to be opened.
The server then picks up a serving thread from the thread pool and
performs the initial evaluation. The serving thread will return to the
thread pool after the initial evaluation no matter whether the initial
evaluation produces the results or not. The server will re-pickup a
thread from the thread pool for the second evaluation, if necessary,
and notify the network I/O thread to complete the client request by
sending out the results of the second evaluation. Using the pipeline
threading model, no thread (serving threads or network I/O thread)
will be hanged during the processing of TTL RPC call. Thus the
threading model is scalable to thousands of concurrent TTL RPC
calls.

4.2 Fault Tolerance

Like all single master cluster architecture, epiC is designed to
be resilient to a large-scale slave machines failures. epiC treats a
slave machine failure as a network partition from that slave ma-
chine to the master. To detect such a failure, the master communi-
cates with worker trackers running on the slave machines by heart-
beat RPCs. If the master cannot receive heartbeat messages from
a worker tracker many times, it marks that worker tracker as dead
and the machine where that worker tracker runs on as “failed”.

When a worker tracker is marked as failed, the master will deter-
mine whether the tasks that the worker tracker processed need to be
recovered. We assume that users persist the output of an epiC job
into a reliable storage system like HDFS or databases. Therefore,
all completed terminal tasks (i.e., tasks hosting units in the termi-
nal group) need not to be recovered. We only recover in-progress
terminal tasks and all non-terminal tasks (no matter completed or
in-progress).

We adopt task re-execution as the main technique for task recov-
ery and employ an asynchronous output backup scheme to speedup
the recovering process. The task re-execution strategy is concep-
tually simple. However, to make it work, we need to make some
reﬁnements to the basic design. The problem is that, in some cases,
the system may not ﬁnd idle worker processes for re-running the
failed tasks.

For example, let us consider a user job that consists of three unit
groups: a map unit group M with two reduce groups R1 and R2.
The output of M is processed by R1 and the output of R1 is further
processed by R2, the terminal unit group for producing the ﬁnal
output. epiC evaluates this job by placing three unit groups M, R1
and R2, in three stages S1, S2 and S3 respectively. The system ﬁrst
launches tasks in S1 and S2. When the tasks in S1 complete, the
system will launch tasks in S3, and at the same time, shufﬂe data
from S1’s units to S2’s units.
Suppose at this time, a work tracker failure causes a task m’s
(m 2 M) output to be lost, the master will fail to ﬁnd an idle
worker process for re-executing that failed task. This is because all
worker processes are running tasks in S2 and S3 and the data lost
introduced by m causes all tasks in S2 to be stalled. Therefore, no
worker process can complete and go back to the idle state.

We introduce a preemption scheduling scheme to solve the above
deadlock problem.
If a task A fails to fetch data produced by
task B, the task A will notify the master and update its state to
in (cid:0) stick. If the master cannot ﬁnd idle worker processes for re-
covering failed tasks for a given period of time, it will kill in(cid:0)stick
tasks by sending killTask() RPCs to the corresponding worker

Algorithm 1 Generate the list of completed tasks to backup
Input: the worker tracker list W
Output: the list of tasks L to backup
1: for each worker tracker w 2 W do
T   the list of completed tasks performed by w
2:
for each completed task t 2 T do
3:
4:
5:
6:
7:
8: end for

if EB(t) < ER(t) then

L   L [ ftg

end if
end for

trackers. The worker trackers then kill the in (cid:0) stick tasks and
release the corresponding worker processes. Finally, the master
marks the killed in (cid:0) stick tasks as failed and adds them to the
failed task list for scheduling. The preemption scheduling scheme
solves the deadlock problem since epiC executes tasks based on
the stage order. The released worker processes will ﬁrst execute
predecessor failed tasks and then the killed in (cid:0) stick tasks.

Re-execution is the only approach for recovering in-progress tasks.

For completed tasks, we also adopt a task output backup strategy
for recovering. This scheme works as follows. Periodically, the
master notiﬁes the worker trackers to upload the output of com-
pleted tasks to HDFS. When the master detects a worker tracker Wi
fails, it ﬁrst commands another live worker tracker Wj to download
Wi’s completed tasks’ output and then notiﬁes all in-progress tasks
that Wj will server Wi’s completed tasks’ output.

Backing up data to HDFS consumes network bandwidth. So, the
master decides to backup a completed task’s output only if the out-
put backup can yield better performance than task re-execution re-
covery. To make such a decision, for a completed task t, the master
estimates two expected execution time ER and EB of t where ER
is the expected execution time when the task re-execution scheme
is adopted and EB is the expected execution time when the output
backup strategy is chosen. ER and EB are computed as follows

ER = Tt (cid:2) P + 2Tt (cid:2) (1 (cid:0) P )

EB = (Tt + Tu) (cid:2) P + Td (cid:2) (1 (cid:0) P )

(1)

(2)

where P is the probability that the worker track is available during
the job execution; Tt is the execution time of t; Tu is the elapsed
time for uploading output to HDFS; and Td is the elapsed time for
downloading output from HDFS. The three parameters Tt, Tu and
Td are easily collected or estimated. The parameter P is estimated
by the availability of a worker tracker in one day.

The master uses Alg. 1 for determining which completed tasks
should be backed up. The master iterates over each worker tracker
(line 1). For each worker tracker, the master retrieves its completed
task list (line 2). Then, for each task in the completed task list, the
master computes EB and ER and adds the task t into the result list
L if EB < ER (line 4 to line 5).

5. EXPERIMENTS

We evaluate the performance of epiC on different kinds of data
processing tasks, including unstructured data processing, relational
data processing and graph processing. We benchmark epiC against
Hadoop, an open source implementation of MapReduce for pro-
cessing unstructured data (i.e., text data) and relational data and
GPS [24], an open source implementation of Pregel [20] for graph
processing, respectively. For relational data, we also run additional

experiments to benchmark epiC with two new in-memory data pro-
cessing systems, namely Shark and Impala. For all experiments, the
results are reported by averaging six runs.
5.1 Benchmark Environment

The experimental study is conducted on an in-house cluster, con-
sisting of 72 nodes hosted on two racks. The nodes within each
rack are connected by a 1 Gbps switch. The two racks are con-
nected by a 10 Gbps cluster switch. Each cluster node is equipped
with a quad-core Intel Xeon 2.4GHz CPU, 8GB memory and two
500 GB SCSI disks. The hdparm utility reports that the buffered
read throughput of the disk is roughly 110 MB/sec. However, due
to the JVM costs, our tested Java program can only read local ﬁles
at 70 (cid:24) 80 MB/sec.

We choose 65 nodes out of the 72 nodes for our benchmark.
For the 65-node cluster, one node acts as the master for running
Hadoop’s NameNode, JobTracker daemons, GPS’s server node and
epiC’s master daemon. For scalability benchmark, we vary the
number of slave nodes from 1, 4, 16, to 64.
5.2 System Settings

In our experiments, we conﬁgure the three systems as follows:

1. The Hadoop settings consist of two parts: HDFS settings and
MapReduce settings.
In HDFS settings, we set the block
size to be 512 MB. As indicated in [17], this setting can sig-
niﬁcantly reduce Hadoop’s cost for scheduling MapReduce
tasks. We also set the I/O buffer size to 128 KB and the repli-
cation factor of HDFS to one (i.e., no replication). In MapRe-
duce settings, each slave is conﬁgured to run two concurrent
map and reduce tasks. The JVM runs in the server mode with
maximal 1.5 GB heap memory. The size of map task’s sort
buffer is 512 MB. We set the merge factor to be 500 and turn
off speculation scheduling. Finally, we set the JVM reuse
number to -1.

2. For each worker tracker in epiC, we set the size of the worker
pool to be four. In the worker pool, two workers are current
workers (running current units) and the remaining two work-
ers are appending workers. Similar to Hadoop’s setting, each
worker process has 1.5 GB memory. For the MapReduce ex-
tension, we set the bucket size of burst sort to be 8192 keys
(string pointers).

3. For GPS, we employ the default settings of the system with-

out further tuning.

5.3 Benchmark Tasks and Datasets

5.3.1 Benchmark Tasks
The benchmark consists of four tasks: Grep, TeraSort, TPC-H
Q3, and PageRank. The Grep task and TeraSort task are presented
in the original MapReduce paper for demonstrating the scalability
and the efﬁciency of using MapReduce for processing unstructured
data (i.e., plain text data). The Grep task requires us to check each
record (i.e., a line of text string) of the input dataset and output
all records containing a speciﬁc pattern string. The TeraSort task
requires the system to arrange the input records in an ascending or-
der. The TPC-H Q3 task is a standard benchmark query in TPC-H
benchmark and is presented in Section 3.2. The PageRank algo-
rithm [22] is an iterative graph processing algorithm. We refer the
readers to the original paper [22] for the details of the algorithm.

5.3.2 Datasets
We generate the Grep and TeraSort datasets according to the
original MapReduce paper published by Google. The generated
datasets consists of N ﬁxed length records. Each record is a string
and occupies a line in the input ﬁle with the ﬁrst 10 bytes as a key
and the remaining 90 bytes as a value. In the Grep task, we are
required to search the pattern in the value part and in the TeraSort
task, we need to order the input records according to their keys.
Google generates the datasets using 512 MB data per-node setting.
We, however, adopt 1 GB data per-node setting since our HDFS
block size is 512 MB. Therefore, for the 1, 4, 16, 64 nodes clus-
ter, we, for each task (Grep and TeraSort), generate four datasets:
1 GB, 4 GB, 16 GB and 64 GB, one for each cluster setting.

We generate the TPC-H dataset using the dbgen tool shipped
with TPC-H benchmark. We follow the benchmark guide of Hive,
a SQL engine built on top of Hadoop, and generate 10GB data per
node. For the PageRank task, we use a real dataset from Twitter1.
The user proﬁles were crawled from July 6th to July 31st 2009.
For our experiments, we select 8 million vertices and their edges to
construct a graph.
5.4 The Grep Task

Figure 15 and Figure 16 present the performance of employing
epiC and Hadoop for performing Grep task with the cold ﬁle system
cache and the warm ﬁle system cache settings, respectively.

In the cold ﬁle system cache setting (Figure 15), epiC runs twice
faster than Hadoop in all cluster settings. The performance gap
between epiC and Hadoop is mainly due to the startup costs. The
heavy startup cost of Hadoop comes from two factors. First, for
each new MapReduce job, Hadoop must launch brand new java
processes for running the map tasks and reduce tasks. The second,
which is also the most important factor, is the inefﬁcient pulling
mechanism introduced by the RPC that Hadoop employed. In a
64-node cluster, the pulling RPC takes about 10(cid:24)15 seconds for
Hadoop to assign tasks to all free map slots. epiC, however, uses
the worker pool technique to avoid launching java processes for
performing new jobs and employs TTL RPC scheme to assign tasks
in real time. We are aware that Google has recently also adopted the
worker pool technique to reduce the startup latency of MapReduce
[7]. However, from the analysis of this task, clearly, in addition to
the pooling technique, efﬁcient RPC is also important.

In the warm ﬁle system cache setting (Figure 16), the perfor-
mance gap between epiC and Hadoop is even larger, up to a factor
of 4.5. We found that the performance of Hadoop cannot beneﬁt
from warm ﬁle system cache. Even, in the warm cache setting,
the data is read from fast cache memory instead of slow disks, the
performance of Hadoop is only improved by 10%. The reason of
this problem is again due to the inefﬁcient task assignments caused
by RPC. epiC, on the other hand, only takes about 4 seconds to
complete the Grep task in this setting, three times faster than per-
forming the same Grep task in cold cache setting. This is because
the bottleneck of epiC in performing the Grep task is I/O. In the
warm cache setting, the epiC Grep job can read data from memory
rather than disk. Thus, the performance is approaching optimality.
5.5 The TeraSort Task

Figure 17 and Figure 18 show the performance of the two sys-
tems (epiC and Hadoop) for performing TeraSort task in two set-
tings (i.e., warm and cold cache). epiC beats Hadoop in terms of
performance by a factor of two. There are two reasons for the per-
formance gap. First, the map task of Hadoop is CPU bound. On

1http://an.kaist.ac.kr/traces/WWW2010.html

Figure 15: Results of Grep task with cold
ﬁle system cache

Figure 16: Results of Grep task with warm
ﬁle system cache

Figure 17: Results of TeraSort task with
cold ﬁle system cache

Figure 18: Results of TeraSort task with
warm ﬁle system cache

Figure 19: Results of TPC-H Q3

Figure 20: Results of PageRank

average, a map task takes about 7 seconds to read off data from disk
and then takes about 10 seconds to sort the intermediate data. Fi-
nally, another 8 seconds are required to write the intermediate data
to local disks. Sorting approximately occupies 50% of the map exe-
cution time. Second, due to the poor pulling RPC performance, the
notiﬁcations of map tasks cannot be propagated to the reduce tasks
in a timely manner. Therefore, there is a noticeable gap between
map completion and reduce shufﬂing.

epiC, however, has no such bottleneck. Equipped with order-
preserving encoding and burst sort technique, epiC, on average, is
able to sort the intermediate data at about 2.1 seconds, roughly ﬁve
times faster than Hadoop. Also, epiC’s TTL RPC scheme enables
reduce units to receive map completion notiﬁcations in real time.
epiC is able to start shufﬂing 5(cid:24)8 seconds earlier than Hadoop.

Compared to the performance of cold cache setting (Figure 17),
both epiC and Hadoop do not run much faster in the warm cache
setting (Figure 18); there is a 10% improvement at most. This is
because scanning data from disks is not the bottleneck of perform-
ing the TeraSort task. For Hadoop, the bottleneck is the map-side
sorting and data shufﬂing. For epiC, the bottleneck of the map unit
is in persisting intermediate data to disks and the bottleneck of the
reduce unit is in shufﬂing which is network bound. We are plan-
ning to eliminate the map unit data persisting cost by building an
in-memory ﬁle system for holding and shufﬂing intermediate data.

5.6 The TPC(cid:173)H Q3 Task

Figure 19 presents the results of employing epiC and Hadoop to
perform TPC-H Q3 under cold ﬁle system cache 2. For Hadoop,
we ﬁrst use Hive to generate the query plan. Then, according to the
generated query plan, we manually wrote MapReduce programs to
perform this task. Our manually coded MapReduce program runs
30% faster than Hive’s native interpreter based evaluation scheme.
The MapReduce programs consist of ﬁve jobs. The ﬁrst job joins

2For TPC-H Q3 task and PageRank task, the three systems cannot
get a signiﬁcant performance improvement from cache. Therefore,
we remove warm cache results to save space.

customer and orders and produces the join results I1. The sec-
ond job joins I1 with lineitem, followed by aggregating, sort-
ing, and limiting top ten results performed by the remaining three
jobs. The query plan and unit implementation of epiC is presented
in Section 3.2.

Figure 19 shows that epiC runs about 2.5 times faster than Hadoop.

This is because epiC uses fewer operations to evaluate the query (5
units vs. 5 maps and 5 reduces) than Hadoop and employs the asyn-
chronous mechanism for running units. In Hadoop, the ﬁve jobs
run sequentially. Thus, the down stream mappers must wait for the
completion of all up stream reducers to start. In epiC, however,
down stream units can start without waiting for the completion of
up stream units.
5.7 The PageRank Task

This experiment compares three systems in performing the PageR-
ank task. The GPS implementation of PageRank algorithm is iden-
tical to [20]. The epiC implementation of PageRank algorithm con-
sists of a single unit. The details are discussed in Section 2.2. The
Hadoop implementation includes a series of iterative jobs. Each
job reads the output of the previous job to compute the new PageR-
ank values. Similar to the unit of epiC, each mapper and reducer
in Hadoop will process a batch of vertices.
In all experiments,
the PageRank algorithm terminates after 20 iterations. Figure 20
presents the results of the experiment. We ﬁnd that all systems can
provide a scalable performance. However, among the three, epiC
has a better speedup. This is because epiC adopts an asynchronous
communication pattern based on message passing, whereas GPS
needs to synchronize the processing nodes and Hadoop repeatedly
creates new mappers and reducers for each job.
5.8 Fault Tolerance

The ﬁnal experiment studies the ability of epiC for handling
machine failures. In this experiment, both epiC and Hadoop are
employed for performing the TeraSort task. During the data pro-
cessing, we simulate slave machine failures by killing all daemon
processes (TaskTracker, DataNode and worker tracker) running on
those machines. The replication factor of HDFS is set to three,

 0 5 10 15 20 25 30 351 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 5 10 15 20 25 30 351 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 20 40 60 80 100 120 1401 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 20 40 60 80 100 120 1401 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 100 200 300 400 500 600 700 8001 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 100 200 300 400 500 600 700 800 9001 node4 nodes16 nodes64 nodesSecondsHadoopGPSepiCin-memory processing that Shark and Impala bring.

6. RELATED WORK

Big Data processing systems can be classiﬁed into the following
categories: 1) Parallel Databases, 2) MapReduce based systems, 3)
DAG based data processing systems, 4) Actor-like systems and 5)
hybrid systems. A comprehensive survey could be found in [19],
and a new benchmark called BigBench [13], was also recently pro-
posed to evaluate and compare the performance of different big data
processing systems.

Parallel databases are mainly designed for processing structured
data sets where each data (called a record) strictly forms a relational
schema [10, 9, 12]. These systems employ data partitioning and
partitioned execution techniques for high performance query pro-
cessing. Recent parallel database systems also employ the column-
oriented processing strategy to even improve the performance of
analytical workloads such as OLAP queries [26]. Parallel databases
have been shown to scale to at least peta-byte dataset but with a rel-
atively high cost on hardware and software [3]. The main drawback
of parallel databases is that those system cannot effectively pro-
cess unstructured data. However, there are recent proposals trying
to integrate Hadoop into database systems to mitigate the problem
[27]. Our epiC, on the other hand, has been designed and built from
scratch to provide the scalability, efﬁciency and ﬂexibility found in
both platforms.

MapReduce was proposed by Dean and Ghemawat in [8]. The
system was originally developed as a tool for building inverted in-
dex for large web corpus. However, the ability of using MapReduce
as a general data analysis tool for processing both structured data
and unstructured data was quickly recognized [30] [28]. MapRe-
duce gains popularity due to its simplicity and ﬂexibility. Even
though the programming model is relatively simple (only consists
of two functions), users, however, can specify any kinds of compu-
tations in the map() and reduce() implementations. MapRe-
duce is also extremely scalable and resilient to slave failures. The
main drawback of MapReduce is its inefﬁciency for processing
structured (relational) data and graph data. Many research work
have been proposed to improve the performance of MapReduce on
relational data processing [3][18]. The most recent work shows
that, in order to achieve better performance of relational process-
ing, one must relax the MapReduce programming model and make
non-trivial modiﬁcations to the runtime system [7]. Our work is in
parallel to these work. Instead of using a one-size-ﬁt-all solution,
we propose to use different data processing models to process dif-
ferent data and employ a common concurrent programming model
to parallelize all those data processing.

Dryad is an ongoing research project at Microsoft [15] [16].
Our work is different from that of Dryad - our concurrent pro-
gramming model is entirely independent of communication pat-
terns while Dryad enforces processing units to transfer data through
DAG.

The the concept of Actor was originally proposed for simplify-
ing concurrent programming [14]. Recently, systems like Storm
[2] and S4 [21] implement the Actor abstraction for streaming data
processing. Our concurrent programming model is also inspired
by the Actor model. However, different from Storm and S4, our
system is designed for batch data processing. A job of epiC will
complete eventually. However, jobs of Storm and S4 may never
end. Spark is also a data analytical system based on Actor pro-
gramming model [31]. The system introduces an abstraction called
resilient distributed dataset (RDD) for fault tolerance. In Spark, the
input and output of each operator must be a RDD. RDD is further
implemented as an in-memory data structure for better retrieving

Figure 21: Fault tolerance experiment on a 16 node cluster

Figure 22: Comparison between epiC and other Systems on TPC-H
Q3

so that input data can be resilient to DataNode lost. Both systems
(epiC and Hadoop) adopt heartbeating for failure detection. The
failure timeout threshold is set to 1 minute. We conﬁgure epiC
to use task re-execution scheme for recovery. The experiment is
launched at a 16 node cluster. We simulate 4 machine failures at
50% job completion.

Figure 21 presents the results of this experiment. It can be seen
that machine failures slow down the data processing. Both epiC and
Hadoop experience 2X slowdown when 25% of the nodes fail (H-
Normal and E-Normal respectively denotes the normal execution
time of Hadoop and epiC, while H-Failure and E-Failure respec-
tively denotes the execution time when machine failures occur).
5.9 Comparison with In(cid:173)memory Systems

We also benchmark epiC with two new in-memory database sys-
tems: Shark and Impala. Since epiC is designed as a disk-based
system, while Shark and Impala are in-memory-based systems, the
performance comparison is therefore only useful as an indication
of the efﬁciency of epiC.

For the experiments, since Shark and Impala adopt in-memory
data processing strategy and require the whole working set (both
the raw input and intermediate data produced during the data pro-
cessing) of a query to be in memory, we fail to run Q3 in these
systems on a 10GB per-node settings. Thus, we reduce the dataset
to be 1GB per-node. Shark uses Spark as its underlying data pro-
cessing engine. We set the worker memory of Spark to be 6GB ac-
cording to its manual. Since Impala can automatically detect avail-
able memory to use, we do not tune further. The Hadoop and epiC
settings are the same as in other experiments.

Figure 22 presents the results of this experiment. In one node
setting, both Shark and Impala outperform epiC since the two sys-
tems hold all data in memory while epiC requires to write the in-
termediate data to disks. However, the performance gap vanishes
in multi-node setting. This is because in these settings, all systems
need to shufﬂe data between nodes and thus the network becomes
the bottleneck. The cost of data shufﬂing offsets the beneﬁts of

 0 50 100 150 200Fault-toleranceSecondsH NormalH FailureE NormalE Failure 0 20 40 60 80 100 120 1401 node4 nodes16 nodes64 nodesSecondsHadoopSharkImpalaepiCepiC: an Extensible and Scalable System for Processing

Big Data

Dawei Jiang †, Gang Chen #, Beng Chin Ooi †, Kian(cid:173)Lee Tan †, Sai Wu #

† School of Computing, National University of Singapore

# College of Computer Science and Technology, Zhejiang University

† fjiangdw, ooibc, tanklg@comp.nus.edu.sg

# fcg, wusaig@zju.edu.cn

ABSTRACT
The Big Data problem is characterized by the so called 3V fea-
tures: Volume - a huge amount of data, Velocity - a high data in-
gestion rate, and Variety - a mix of structured data, semi-structured
data, and unstructured data. The state-of-the-art solutions to the
Big Data problem are largely based on the MapReduce framework
(aka its open source implementation Hadoop). Although Hadoop
handles the data volume challenge successfully, it does not deal
with the data variety well since the programming interfaces and its
associated data processing model is inconvenient and inefﬁcient for
handling structured data and graph data.

This paper presents epiC, an extensible system to tackle the Big
Data’s data variety challenge. epiC introduces a general Actor-like
concurrent programming model, independent of the data process-
ing models, for specifying parallel computations. Users process
multi-structured datasets with appropriate epiC extensions, the im-
plementation of a data processing model best suited for the data
type and auxiliary code for mapping that data processing model
into epiC’s concurrent programming model. Like Hadoop, pro-
grams written in this way can be automatically parallelized and the
runtime system takes care of fault tolerance and inter-machine com-
munications. We present the design and implementation of epiC’s
concurrent programming model. We also present two customized
data processing model, an optimized MapReduce extension and a
relational model, on top of epiC. Experiments demonstrate the ef-
fectiveness and efﬁciency of our proposed epiC.

1.

INTRODUCTION

Many of today’s enterprises are encountering the Big Data prob-
lems. A Big Data problem has three distinct characteristics (so
called 3V features): the data volume is huge; the data type is di-
verse (mixture of structured data, semi-structured data and unstruc-
tured data); and the data producing velocity is very high. These
3V features pose a grand challenge to traditional data processing
systems since these systems either cannot scale to the huge data
volume in a cost effective way or fail to handle data with variety of
types [3][7].

licensed under

This work is
the Creative Commons Attribution(cid:173)
NonCommercial(cid:173)NoDerivs 3.0 Unported License. To view a copy of this li(cid:173)
cense, visit http://creativecommons.org/licenses/by(cid:173)nc(cid:173)nd/3.0/. Obtain per(cid:173)
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st (cid:173) 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 7
Copyright 2014 VLDB Endowment 2150(cid:173)8097/14/03.

A popular approach to process Big Data adopts MapReduce pro-
gramming model and its open source implementation Hadoop [8][1].
The advantage of MapReduce is that the system tackles the data
volume challenge successfully and is resilient to machine failures
[8]. However, the MapReduce programming model does not han-
dle the data variety problem well - while it manages certain unstruc-
tured data (e.g., plain text data) effectively, the programming model
is inconvenient and inefﬁcient for processing structured data and
graph data that require DAG (Directed Acyclic Graph) like compu-
tation and iterative computation [17, 3, 23, 20, 30]. Thus, systems
like Dryad [15] and Pregel [20] are built to process those kinds of
analytical tasks.

As a result, to handle the data variety challenge, the state-of-
the-art approach favors a hybrid architecture [3, 11]. The approach
employs a hybrid system to process multi-structured datasets (i.e.,
datasets containing a variety of data types: structured data, text,
graph). The multi-structured dataset is stored in a variety of sys-
tems based on types (e.g., structured data are stored in a database,
unstructured data are stored in Hadoop). Then, a split execution
scheme is employed to process those data. The scheme splits the
whole data analytical job into sub-jobs and choose the appropriate
systems to perform those sub-jobs based on the data types. For ex-
ample, the scheme may choose MapReduce to process text data,
database systems to process relational data, and Pregel to process
graph data. Finally, the output of those sub-jobs will be loaded into
a single system (Hadoop or database) with proper data formation
to produce the ﬁnal results. Even though the hybrid approach is
able to employ the right data processing system to process the right
type of data, it introduces complexity in maintaining several clus-
ters (i.e., Hadoop cluster, Pregel cluster, database cluster) and the
overhead of frequent data formation and data loading for merging
output of sub-jobs during data processing.

This paper presents a new system called epiC to tackle the Big
Data’s data variety challenge. The major contribution of this work
is an architectural design that enables users to process multi-structured
datasets in a single system. We found that although different sys-
tems (Hadoop, Dryad, Database, Pregrel) are designed for different
types of data, they all share the same shared-nothing architecture
and decompose the whole computation into independent compu-
tations for parallelization. The differences between them are the
types of independent computation that these systems allow and the
computation patterns (intermediate data transmission) that they em-
ploy to coordinate those independent computations. For example,
MapReduce only allows two kinds of independent computations
(i.e., map and reduce) and one-way data transmission from mappers
to reducers. DAG systems (e.g., Dryad) allow arbitrary number of
independent computations and DAG-like data transmission. Graph
processing systems (e.g., Pregel) adopt iterative data transmission.

2.1 Programming Model

The basic abstraction of epiC programming model is unit.

It
works as follows. A unit becomes activated when it receives a
message from the master network. Based on the message con-
tent, it adaptively loads data from the storage system and applies
the user-speciﬁed function to process the data. After completing
the process, the unit writes the results back to the storage system
and the information of the intermediate results are summarized in a
message forwarded to the master network. Then, the unit becomes
inactive, waiting for the next message. Like MapReduce, the unit
accesses the storage through reader and writer interface and thus
can process data stored in any kinds of storage systems (e.g., ﬁle
systems, databases or key-value stores).

The master network consists of several synchronized masters,
which are responsible for three services: naming service, message
service and schedule service. Naming service assigns a unique
namespace to each unit.
In particular, we maintain a two-level
namespace. The ﬁrst level namespace indicates a group of units
running the same user code. For example, in Figure 1, all units
share the same ﬁrst level namespace PageRank [22]. The second
level namespace distinguishes the unit from the others. epiC al-
lows the users to customize the second level namespace. Suppose
we want to compute the PageRank values for a graph with 10,000
vertices. We can use the vertex ID range as the second level names-
pace. Namely, we evenly partition the vertex IDs into small ranges.
Each range is assigned to a unit. A possible full namespace may
be “[0, 999]@PageRank”, where @ is used to concatenate the two
namespaces. The master network maintains a mapping relationship
between the namespace and the IP address of the corresponding
unit process.

Based on the naming service, the master network collects and
disseminates the messages to different units. The workload is bal-
anced among the masters and we keep the replicas for the messages
for fault tolerance. Note that in epiC, the message only contains
the meta-information of the data. The units do not transfer the in-
termediate results via the message channel as in the shufﬂe phase
of MapReduce. Therefore, the message service is a light-weight
service with low overhead.

The schedule service of master network monitors the status of
the units. If a failed unit is detected, a new unit will be started to
take over its job. On the other hand, the schedule service also ac-
tivates/deactivates units when they receive new messages or com-
plete the processing. When all units become inactive and no more
messages are maintained by the master network, the scheduler ter-
minates the job.

Formally, the programming model of epiC is deﬁned by a triple
< M, U, S >, where M is the message set, U is the unit set and S
is the dataset. Let N and U denote the universes of the namespace
and data URIs. For a message m 2 M, m is expressed as:

m := f(ns, uri)jns 2 N ^ uri 2 Ug

Figure 1: Overview of epiC

Therefore, if we can decompose the computation and communi-
cation pattern by building a common runtime system for running
independent computations and developing plug-ins for implement-
ing speciﬁc communication patterns, we are able to run all those
kinds of computations in a single system. To achieve this goal,
epiC adopts an extensible design. The core abstraction of epiC is
an Actor-like concurrent programming model which is able to ex-
ecute any number of independent computations (called units). On
top of it, epiC provides a set of extensions that enable users to pro-
cess different types of data with different types of data processing
models (MapReduce, DAG or Graph). In our current implementa-
tion, epiC supports two data processing models, namely MapRe-
duce and relation database model.

The concrete design of epiC is summarized as follows. The sys-
tem employs a shared-nothing design. The computation consists of
units. Each unit performs I/O operations and user-deﬁned compu-
tation independently. Units coordinate with each other by message
passing. The message, sent by each unit, encodes the control in-
formation and metadata of the intermediate results. The processing
ﬂow is therefore represented as a message ﬂow. The epiC pro-
gramming model does not enforce the message ﬂow to be a DAG
(Directed Acyclic Graph). This ﬂexible design enable users to rep-
resent all kinds of computations (e.g., MapReduce, DAG, Pregel)
proposed so for and provides more opportunities for users to opti-
mize their computations. We will take equal-join task as an exam-
ple to illustrate this point.

The rest of the paper is organized as follows. Section 2 shows the
overview of epiC and motivates our design with an example. Sec-
tion 3 presents the programming abstractions introduced in epiC,
focusing on the concurrency programming model and the MapRe-
duce extension. Section 4 presents the internals of epiC. Section
5 evaluates the performance and scalability of epiC based on a se-
lected benchmark of tasks. Section 6 presents related work. Finally,
we conclude this paper in Section 7.

2. OVERVIEW OF EPIC

epiC adopts the Actor-like programming model. The computa-
tion is consisted of a set of units. Units are independent of each
other. Each unit applies user-deﬁned logic to process the data in-
dependently and communicate with other units through message
passing. Unlike systems such as Dryad and Pregel [20], units can-
not interact directly. All their messages are sent to the master net-
work and then disseminated to the corresponding recipients. The
master network is similar to the mail servers in the email system.
Figure 1 shows an overview of epiC.

We deﬁne a projection π function for m as:

π(m, u) = f(ns, uri)j(ns, uri) 2 m ^ ns = u.nsg

Namely, π returns the message content sharing the same names-
pace with u. π can be applied to M to recursively perform the
projection. Then, the processing logic of a unit u in epiC can be
expressed by function g as:

g := π(M, u) (cid:2) u (cid:2) S ! mout (cid:2) S

′

Unit: PageRankmessage queueepiC codeI/O libraryUnit: PageRankmessage queueepiC codeI/O librarymastermastermastermastermaster networkcontrol msgcontrol msgDistributed Storage System (DFS, Key-value store, Dstributed Database,...)naming servicemessage serviceschedule servicemessage queueepiC codeI/O libraryUnit: PageRankcontrol msgFigure 2: PageRank in MapReduce

Figure 3: PageRank in Pregel

Figure 4: PageRank in epiC

denotes the output data and mout is the message to the master

′

S
network satisfying:
8s 2 S

′ ) 9(ns, uri) 2 mout ^ ρ(uri) = s

′

where ρ(uri) maps a URI to the data ﬁle. After the processing, S
is updated as S [ S
. As the behaviors of units running the same
code are only affected by their received messages, we use (U, g) to
denote a set of units running code g. Finally, the job J of epiC is
represented as:

J := (U, g)+ (cid:2) Sin ) Sout

Sin is the initial input data, while Sout is the result data. The job
J does not specify the order of execution of different units, which
can be controlled by users for different applications.
2.2 Comparison with Other Systems

To appreciate the workings of epiC, we compare the way the

PageRank algorithm is implemented in MapReduce (Figure 2), Pregel
(Figure 3) and epiC (Figure 4). For simplicity, we assume the graph
data and score vector are maintained in the DFS. Each line of the
graph ﬁle represents a vertex and its neighbors. Each line of the
score vector records the latest PageRank value of a vertex. The
score vector is small enough to be buffered in memory.

To compute the PageRank value, MapReduce requires a set of
jobs. Each mapper loads the score vector into memory and scans
a chunk of the graph ﬁle. For each vertex, the mapper looks up
its score from the score vector and then distributes its scores to the
neighbors. The intermediate results are key-value pairs, where key
is the neighbor ID and value is the score assigned to the neighbor.
In the reduce phase, we aggregate the scores of the same vertex and
apply the PageRank algorithm to generate the new score, which is
written to the DFS as the new score vector. When the current job
completes, a new job starts up to repeat the above processing until
the PageRank values converge.

Compared to MapReduce, Pregel is more effective in handling it-
erative processing. The graph ﬁle is preloaded in the initial process
and the vertices are linked based on their edges. In each super-step,
the vertex gets the scores from its incoming neighbors and applies
the PageRank algorithm to generate the new score, which is broad-
cast to the outgoing neighbors. If the score of a vertex converges,
it stops the broadcasting. When all vertices stop sending messages,
the processing can be terminated.

The processing ﬂow of epiC is similar to Pregel. The master net-
work sends messages to the unit to activate it. The message con-
tains the information of partitions of the graph ﬁle and the score
vectors generated by other units. The unit scans a partition of the
graph ﬁle based on its namespace to compute the PageRank values.
Moreover, it needs to load the score vectors and merge them based
on the vertex IDs. As its namespace indicates, only a portion of the
score vector needs to be maintained in the computation. The new

score of the vertex is written back to the DFS as the new score vec-
tor and the unit sends messages about the newly generated vector
to the master network. The recipient is speciﬁed as “*@PageR-
ank”. Namely, the unit asks the master network to broadcast the
message to all units under the PageRank namespace. Then, the
master network can schedule other units to process the messages.
Although epiC allows the units to run asynchronously, to guaran-
tee the correctness of PageRank value, we can intentionally ask the
master network to block the messages, until all units complete their
processing. In this way, we simulate the BSP (Bulk Synchronous
Parallel Model) as Pregel.

We use the above example to show the design philosophy of epiC

and why it performs better than the other two.
Flexibility MapReduce is not designed for such iterative jobs. Users
have to split their codes into the map and reduce functions.
On the other hand, Pregel and epiC can express the logic in
a more natural way. The unit of epiC is analogous to the
worker in Pregel. Each unit processes the computation for a
set of vertices. However, Pregel requires to explicitly con-
struct and maintain the graph, while epiC hides the graph
structure by namespace and message passing. We note that
maintaining the graph structure, in fact, consumes many sys-
tem resources, which can be avoided by epiC.

Optimization Both MapReduce and epiC allow customized opti-
mization. For example, the Haloop system [6] buffers the
intermediate ﬁles to reduce the I/O cost and the units in epiC
can maintain their graph partitions to avoid repeated scan.
Such customized optimization is difﬁcult to implement in
Pregel.

Extensibility In MapReduce and Pregel, the users must follow
the pre-deﬁned programming model (e.g., map-reduce model
and vertex-centric model), whereas in epiC, the users can de-
sign their customized programming model. We will show
how the MapReduce model and the relational model are im-
plemented in epiC. Therefore, epiC provides a more general
platform for processing parallel jobs.

3. THE EPIC ABSTRACTIONS

epiC distinguishes two kinds of abstractions: a concurrent pro-
gramming model and a data processing model. A concurrent pro-
gramming model deﬁnes a set of abstractions (i.e., interfaces) for
users to specify parallel computations consisting of independent
computations and dependencies between those computations. A
data processing model deﬁnes a set of abstractions for users to spec-
ify data manipulation operations. Figure 5 shows the programming
stack of epiC. Users write data processing programs with exten-
sions. Each extension of epiC provides a concrete data processing
model (e.g., MapReduce extension offers a MapReduce program-
ming interface) and auxiliary code (shown as a bridge in Figure 5)

mapmapmap...reducereducereduce...graphdatadivide the score to neighborscompute the new score of each vertexscore vectorscore vectoriterationsreceive scores from neighborscompute new scorebroadcast scores to neighbors......Unit: PageRankmessage queueepiC codeI/O librarystorage system1. load graph data and score vector based on the received messagemaster network2. compute new score vector of vertices3. generate new score vector files4. send messages to master network102340. send messages to unit to activate itFigure 5: The Programming Stack of epiC

for running the written program on the epiC’s common concurrent
runtime system.

We point out that the data processing model is problem domain
speciﬁc. For example, a MapReduce model is best suited for pro-
cessing unstructured data, a relational model is best suited for struc-
tured data and a graph model is best suited for graph data. The
common requirement is that programs written with these models
are all needed to be parallelized. Since Big Data is inherently multi-
structured, we build an Actor-like concurrent programming model
for a common runtime framework and offer epiC extensions for
users to specify domain speciﬁc data manipulation operations for
each data type. In the previous section, we have introduced the ba-
sic programming model of epiC. In this section, we focus on two
customized data processing model, the MapReduce model and re-
lational model. We will show how to implement them on top of
epiC.
3.1 The MapReduce Extension

We ﬁrst consider the MapReduce framework, and extend it to
work with epiC’s runtime framework. The MapReduce data pro-
cessing model consists of two interfaces:

(k1, v1)
(k2, list(v2)) ! list(v2)

map
reduce
Our MapReduce extension reuses Hadoop’s implementation of
these interfaces and other useful functions such as the partition.
This section only describes the auxiliary support which enables
users to run MapReduce programs on epiC and our own optimiza-
tions which are not supported in Hadoop.

! list(k2, v2)

3.1.1 General Abstractions
Running MapReduce on top of epiC is straightforward. We ﬁrst
place the map() function in a map unit and the reduce() func-
tion in a reduce unit. Then, we instantiate M map units and R
reduce units. The master network assigns a unique namespace to
each map and reduce unit. In the simplest case, the name addresses
of the units are like “x@MapUnit” and “y@ReduceUnit”, where
0 (cid:20) x < M and 0 (cid:20) y < R.

Based on the namespace, the MapUnit loads a partition of in-
put data and applies the customized map() function to process it.
The results are a set of key-value pairs. Here, a partition()
function is required to split the key-value pairs into multiple HDFS
ﬁles. Based on the application’s requirement, the partition()
can choose to sort the data by keys. By default, the partition()
simply applies the hash function to generate R ﬁles and assigns a
namespace to each ﬁle. The meta-data of the HDFS ﬁles are com-
posed into a message, which is sent to the master network. The
recipient is speciﬁed as all the ReduceUnit.

The master network then collects the messages from all MapUnits

and broadcasts them to the ReduceUnit. When a ReduceUnit
starts up, it loads the HDFS ﬁles that share the same namespace
with it. A possible merge-sort is required, if the results should be
sorted. Then, the customized reduce() function is invoked to

generate the ﬁnal results.

class Map implements Mapper {

void map() {
}

}
class Reduce implements Reducer {

void reduce() {
}

}
class MapUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new HdfsReader(s);
MapRunner map = new MapRunner(reader, Map());
map.run();
o.sendMessage("*@ReduceUnit",
map.getOutputMessage());

}

}
class ReduceUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
ReduceRunner red = new ReduceRunner(in,

Reduce());

red.run();

}

}

Here, we highlight the advantage of our design decision to de-
couple the data processing model and the concurrent programming
model. Suppose we want to extend the MapReduce programming
model to the Map-Reduce-Merge programming model [30]. All
we need to do is to add a new unit mergeUnit() and modify
the codes in the ReduceUnit to send messages to the master net-
work for declaring its output ﬁles. Compared to this non-intrusive
scheme, Hadoop needs to make dramatic changes to its runtime
system to support the same functionality [30] since Hadoop’s de-
sign bundles data processing model with concurrent programming
model.
3.1.2 Optimizations for MapReduce
In addition to the basic MapReduce implementation which is
similar to Hadoop, we add an optimization for map unit data pro-
cessing. We found that the map unit computation is CPU-bound
instead of I/O bound. The high CPU cost comes from the ﬁnal
sorting phase.

The Map unit needs to sort the intermediate key-value pairs since
MapReduce requires the reduce function to process key-value pairs
in an increasing order. Sorting in MapReduce is expensive since 1)
the sorting algorithm (i.e., quick sort) itself is CPU intensive and
2) the data de-serialization cost is not negligible. We employ two
techniques to improve the map unit sorting performance: 1) order-
preserving serialization and 2) high performance string sort (i.e.,
burst sort).

Deﬁnition 1. For a data type T , an order-preserving serialization
is an encoding scheme which serializes a variable x 2 T to a string
sx such that, for any two variables x 2 T and y 2 T , if x < y then
sx < sy in string lexicographical order.

In other words, the order-preserving serialization scheme serial-
izes keys so that the keys can be ordered by directly sorting their se-
rialized strings (in string lexicographical order) without de-serialization.
Note that the order-preserving serialization scheme exists for all
Java built-in data types.

(cid:258)(cid:258)(cid:38)(cid:82)(cid:81)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:53)(cid:88)(cid:81)(cid:87)(cid:76)(cid:80)(cid:72)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:53)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:48)(cid:68)(cid:83)(cid:53)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:3)(cid:37)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)}
class SingleTableUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader reader = new TableReader(s);
EmbededDBEngine

e =

new EmbededDBEngine(reader, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class JoinUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s1 = m[r.getNameAddress(LEFT\_TABLE)];
InputSplit s2 = m[r.getNameAddress(RIGHT\_TABLE)];
Reader in1 = new MapOutputReader(s1);
Reader in2 = new MapOutputReader(s2);
EmbededDBEngine e =

new EmbededDBEngine(in1, in2, getQuery());

e.process();
o.sendMessage(r.getRecipient(),

e.getOutputMessage());

}

}
class AggregateUnit implements Unit {

void run(LocalRuntime r, Input i, Output o) {

Message m = i.getMessage();
InputSplit s = m[r.getNameAddress()];
Reader in = new MapOutputReader(s);
EmbededDBEngine e =

new EmbededDBEngine(in, getQuery());

e.process();

}

}

The abstractions are straightforward and we discard the detailed
discussion.
In each unit, we embed a customized query engine,
which can process single table queries, join queries and aggrega-
tions. We have not speciﬁed the recipients of each message in the
unit abstraction. This must be implemented by users for different
queries. However, as discussed later, we provide a query optimizer
to automatically ﬁll in the recipients. To show how users can adopt
the above relational model to process queries, let us consider the
following query (a variant of TPC-H Q3):

SELECT l orderkey, sum(l extendedprice*(1-l discount))

as revenue, o orderdate, o shippriority

FROM customer, orders, lineitem
WHERE c mktsegment = ’:1’ and c custkey = o custkey

and l orderkey = o orderkey and o orderdate
< date ’:2’ and l shipdate > date ’:2’

Group By o orderdate, o shippriority

Figure 6 to Figure 10 illustrate the processing of Q3 in epiC.
In step 1 (Figure 6), three types of the SingleTableUnits are
started to process the select/project operators of Lineitem, Orders
and Customer respectively. Note that those SingleTableUnits
run the same code. The only differences are their name addresses
and processed queries. The results are written back to the storage
system (either HDFS or distributed database). The meta-data of the
corresponding ﬁles are forwarded to the JoinUnits.

In step 2 and step 3 (Figures 7 and 8), we apply the hash-join
approach to process the data. In previous SingleTableUnits,
the output data are partitioned by the join keys. So the JoinUnit
can selectively load the paired partitions to perform the join. We
will discuss other possible join implementations in the next section.

Figure 6: Step 1 of Q3

We adopt burst sort algorithm to order the serialized strings. We
choose burst sort as our sorting technique since it is specially de-
signed for sorting large string collections and has been shown to
be signiﬁcantly faster than other candidates [25]. We brieﬂy out-
line the algorithm here. Interested readers should refer to [25] for
details. The burst sort technique sorts a string collection in two
passes. In the ﬁrst pass, the algorithm processes each input string
and stores the pointer of each string into a leaf node (bucket) in
a burst trie. The burst trie has a nice property that all leaf nodes
(buckets) are ordered. Thus, in the second pass, the algorithm pro-
cesses each bucket in order, applies a standard sorting technique
such as quick sort to sort strings, and produces the ﬁnal results.
The original burst sort requires a lot of additional memory to hold
the trie structure and thus does not scale well to a very large string
collection. We, thus, developed a memory efﬁcient burst sort im-
plementation which requires only 2 bits of additional space for each
key entry. We also use the multi-key quick sort algorithm [5] to sort
strings resided in the same bucket.

Combining the two techniques (i.e., order-preserving serializa-
tion and burst sort), our sorting scheme outperforms Hadoop’s quick
sort implementation by a factor of three to four.
3.2 Relational Model Extension

As pointed out earlier, for structured data, the relational data pro-
cessing model is most suited. Like the MapReduce extensions, we
can implement the relational model on top of epiC.
3.2.1 General Abstractions
Currently, three core units (SingleTableUnit, JoinUnit
and AggregateUnit) are deﬁned for the relational model. They
are capable of handling non-nested SQL queries. The SingleTableUnit
processes queries that involve only a partition of a single table.
The JoinUnit reads partitions from two tables and merge them
into one partition of the join table. Finally, the AggregateUnit
collects the partitions of different groups and computes the aggre-
gation results for each group. The abstractions of these units are
shown below. Currently, we adopt the synchronization model as
in MapReduce. Namely, we will start the next types of units, only
when all current units complete their processing. We will study
the possibility of creating a pipeline model in future work. Due to
space limitation, we only show the most important part.

class SingleTableQuery implements DBQuery {

void getQuery() {
}

}
class JoinQuery implements DBQuery {

void getQuery() {
}

}
class AggregateQuery implements DBQuery {

void getQuery() {
}

SingleTableUnitSingleTableUnitSingleTableUnitRelational Data of Customer/Orders/Lineitemselect c_custkey from Customer where c_mktsegment = ':1'select o_orderdate, o_custkey, o_orderkey, o_shippriority from Orders where o_orderdate < date ':2'select l_orderkey, l_extendedprice, l_discount from Lineitem where l_shipdate > date ':2'Master networkPartition info of CustomerPartition info of OrdersPartition info of LineitemFigure 7: Step 2 of Q3

Figure 8: Step 3 of Q3

Figure 9: Step 4 of Q3

Figure 10: Step 5 of Q3

Figure 11: Basic Join Operation

Figure 12: Semi-Join Operation

Finally, in step 4 (Figure 9), we perform the group operation
for two attributes. As the join results are partitioned into multiple
chunks, one SingleTableUnit can only generate the grouping
results for its own chunk. To produce the complete grouping re-
sults, we merge groups generated by different SingleTableUnits.
Therefore, in step 5 (Figure 10), one AggregateUnit needs to
load the partitions generated by all SingleTableUnits for the
same group to compute the ﬁnal aggregation results.

Our relational model simpliﬁes the query processing, as users
only need to consider how to partition the tables by the three units.
Moreover, it also provides the ﬂexibility of customized optimiza-
tion.

3.2.2 Optimizations for Relational Model
The relational model on epiC can be optimized in two layers, the

unit layer and the job layer.

In the unit layer, the user can adaptively combine the units to
implement different database operations. They can even write their
own units, such as ThetaJoinUnit, to extend the functional-
ity of our model. In this section, we use the euqi-join as an ex-
ample to illustrate the ﬂexibility of the model. Figure 11 shows
how the basic equi-join (S ◃▹ T ) is implemented in epiC. We ﬁrst
use the SingleTableUnit to scan the corresponding tables and
partition the tables by join keys. Then, the JoinUnit loads the
corresponding partitions to generate the results. In fact, the same
approach is also used in processing Q3. We partition the tables by
the keys in step 1 (Figure 6). So the following JoinUnits can
perform the join correctly.

Figure 13: Job Plan of Q3

However, if most of the tuples in S do not match tuples of T ,
semi-join is a better approach to reduce the overhead. Figure 12 il-
lustrates the idea. The ﬁrst SingleTableUnit scans table S and
only outputs the keys as the results. The keys are used in the next
SingleTableUnit to ﬁlter the tuples in T that cannot join with
S. The intermediate results are joined with S in the last JoinUnit
to produce the ﬁnal results. As shown in the example, semi-join can
be efﬁciently implemented using our relational model.

In the job layer, we offer a general query optimizer to translate
the SQL queries into an epiC job. Users can leverage the opti-
mizer to process their queries, instead of writing the codes for the
relational model by themselves. The optimizer works as a conven-
tional database optimizer. It ﬁrst generates an operator expression
tree for the SQL query and then groups the operators into differ-
ent units. The message ﬂow between units is also generated based
on the expression tree. To avoid a bad query plan, the optimizer
estimates the cost of the units based on the histograms. Currently,
we only consider the I/O costs. The optimizer will iterate over all
variants of the expression trees and select the one with the minimal
estimated cost. The corresponding epiC job is submitted to the pro-
cessing engine for execution. Figure 13 shows how the expression
tree is partitioned into units for Q3.

The query optimizer acts as the AQUA [29] for MapReduce or
PACTs compiler in Nephele [4]. But in epiC, the DAG between
units are not used for data shufﬂing as in Nephele. Instead, all rela-
tionships between units are maintained through the message pass-
ing and namespaces. All units fetch their data from the storage sys-
tem directly. This design follows the core concept of Actor model.
The advantage is three-fold: 1) we reduce the overhead of main-
taining the DAG; 2) we simplify the model as each unit runs in an

Create Partition JoinView1 as (Lineitem join Orders)Partition info of the Partial Results of Lineitem and OrdersJoinUnitPartial Results of Customer/LineitemMaster networkCreate Partition JoinView2 as (Customer join JoinView1)Partition info of the Partial Results of Customer and JoinView1JoinUnitPartial Results of Customer/JoinView1Master networkSelect * from JoinView2 Group By o_orderdate, o_shippriorityPartition info of the Partial Results of JoinView2SingleTableUnitPartial Results of JoinView2Master networkAggregateUnitPartial Results of Group ByCompute Aggregation Results for Each GroupMaster networkPartition info of GroupsSingleTableUnitTable S and TMaster networkSingleTableUnitselect * from S partitioned by S.keyselect * from T partitioned by T.foreignkeyJoinUnitTable S and TMaster networkselect * from S, T where S.key=T.foreignkeySingleTableUnitTable SMaster networkCreate Partition Tmp as (select key from S)SingleTableUnitTable T and TmpMaster networkCreate Partition Tmp2 as(select * from T where T.foreignkey in (select * from Tmp))JoinUnitTable S and Tmp2Master networkselect * from S, Tmp2 where S.key =Tmp2.foreignkeylineitemordersσl_shipdate>date ‘:2’π(l_orderkey, l_extendedprice, l_discount)σo_orderdate<date ‘:2’π(o_orderkey, o_custkey, o_orderdate, o_shippriority)customerσc_mktsegment=date ‘:1’π(c_custkey)GroupByo_orderdaye, o_shippriorirysum(l_extendedprice * (1 -l_discount)) as revenueSingleTableUnitSingleTableUnitSingleTableUnitJoinUnitJoinUnitSingleTableUnitAggregateUnitto the master, reporting the completed task identity. The master will
perform the call, updating its status, and responds to the worker
tracker.

This request-reply scheme is inefﬁcient for client to continuously
query information stored at the server. Consider the example of task
assignments. To get a new task for execution, the worker tracker
must periodically make getTask() RPC calls to the master since
the master hosts all task information and the worker tracker has no
idea of whether there are pending tasks. This periodical-pulling
scheme introduces non-negligible delays to the job startup since
users may submit jobs at arbitrary time point but the task assign-
ment is only performed at the ﬁxed time points. Suppose the worker
tracker queries a new task at time t0 and the query interval is T ,
then all tasks of jobs submitted at t1 > t0 will be delayed to t0 + T
for task assignment.

Since continuously querying server-side information is a com-
mon communication pattern in epiC, we develop a new RPC scheme
to eliminate the pulling interval in successive RPC calls for low la-
tency data processing.

Our approach is called the TTL RPC which is an extension of
the standard RPC scheme by associating each RPC call with a user
speciﬁed Time To Live (TTL) parameter T . The TTL parameter
T captures the duration the RPC can live on the server if no re-
sults are returned from the server; when the TTL expires, the RPC
is considered to have been served. For example, suppose we call
getTask() with T = 10s (seconds), when there is no task to as-
sign, instead of returning a null task immediately, the master holds
the call for at most 10 seconds. During that period, if the master
ﬁnds any pending tasks (e.g., due to new job submission), the mas-
ter returns the calling worker tracker with a new task. Otherwise, if
10 seconds passed and there are still no tasks to assign, the master
returns a null task to the worker tracker. The standard request-reply
RPC can be implemented by setting T = 0, namely no live.

We use a double-evaluation scheme to process a TTL-RPC call.
When the server receives a TTL-RPC call C, it performs an ini-
tial evaluation of C by treating it as a standard RPC call. If this
initial evaluation returns nothing, the server puts C into a pending
list. The TTL-RPC call will stay in the pending list for at most
T time. The server performs a second evaluation of C if either 1)
the information that C queries changes or 2) T time has passed.
The outcome of the second evaluation is returned as the ﬁnal result
to the client. Using TTL-RPC, the client can continuously make
RPC calls to the server in a loop without pulling interval and thus
receives server-side information in real time. We found that TTL-
RPC signiﬁcantly improves the performance of small jobs and re-
duces startup costs.

Even though the TTL-RPC scheme is a simple extension to the
standard RPC scheme, the implementation of TTL-RPC poses cer-
tain challenges for the threading model that the classical Java net-
work programs adopt. A typical Java network program employs
a per-thread per-request threading model. When a network con-
nection is established, the server serves the client by ﬁrst picking
up a thread from a thread pool, then reading data from the socket,
and ﬁnally performing the appropriate computations and writing
result back to the socket. The serving thread is returned to the
thread pool after the client is served. This per-thread per-request
threading model works well with the standard RPC communica-
tion. But it is not appropriate for our TTL RPC scheme since TTL
RPC request will stay at the server for a long time (We typically set
T = 20 (cid:24) 30 seconds). When multiple worker trackers make TTL
RPC calls to the master, the per-thread per-request threading model
produces a large number of hanging threads, quickly exhausting the
thread pool, and thus makes the master unable to respond.

Figure 14: The architecture of an epiC cluster

isolated way; 3) the model is more ﬂexible to support complex data
manipulation jobs (either synchronized or asynchronized).

4.

IMPLEMENTATION DETAILS

epiC is written in Java and built from scratch although we reuse
some Hadoop codes to implement a MapReduce extension. This
section describes the internals of epiC.

Like Hadoop, epiC is expected to be deployed on a shared-nothing
cluster of commodity machines connected with switched Ethernet.
It is designed to process data stored in any data sources such as
databases or distributed ﬁle systems. The epiC software mainly
consists of three components: master, worker tracker and worker
process. The architecture of epiC is shown in Figure 14. epiC
adopts a single master (this master is different from the servers
in the master network, which are mainly responsible for routing
messages and maintaining namespaces) multi-slaves architecture.
There is only one master node in an epiC cluster, running a mas-
ter daemon. The main function of the master is to command the
worker trackers to execute jobs. The master is also responsible
for managing and monitoring the health of the cluster. The master
runs a HTTP server which hosts such status information for human
consumption. It communicates with worker trackers and worker
processes through remote procedure call (RPC).

Each slave node in an epiC cluster runs a worker tracker dae-
mon. The worker tracker manages a worker pool, a ﬁxed number
of worker processes, for running units. We run each unit in a sin-
gle worker process. We adopt this ‘pooling’ process model instead
of an on-demand process model which launches worker processes
on demand for two reasons. First, pre-launching a pool of worker
processes reduces the startup latency of job execution since launch-
ing a brand new Java process introduces non-trivial startup costs
(typically 2(cid:24)3 seconds). Second, the latest HotSpot Java Virtual
Machine (JVM) employs a Just-In-Time (JIT) compilation tech-
nique to incrementally compile the Java byte codes into native ma-
chine codes for better performance. To fully unleash the power
of HotSpot JVM, one must run a Java program for a long time so
that every hot spot (a code segment, performing expensive com-
putations) of the program can be compiled by the JIT compiler.
Therefore, a never-ending worker process is the most appropriate
one for this purpose.

Here, we will focus on two most important parts of the imple-

mentations, the TTL RPC and the failure recovery.
4.1 The TTL RPC

The standard RPC scheme adopts a client-server request-reply
scheme to process RPC calls. In this scheme, a client sends a RPC
request to the server. The server processes this request and returns
its client with results. For example, when a task completes, the
worker tracker will perform a RPC call taskComplete(taskId)

(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:48)(cid:68)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:39)(cid:68)(cid:72)(cid:80)(cid:82)(cid:81)(cid:54)(cid:79)(cid:68)(cid:89)(cid:72)(cid:3)(cid:49)(cid:82)(cid:71)(cid:72)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:258)(cid:17)(cid:17)(cid:17)(cid:53)(cid:51)(cid:38)(cid:53)(cid:51)(cid:38)We develop a pipeline threading model to ﬁx the above prob-
lems. The pipeline threading model uses a dedicated thread to per-
form the network I/O (i.e., reading request from and writing re-
sults to the socket) and a thread pool to perform the RPC calls.
When the network I/O thread receives a TTL RPC request, it noti-
ﬁes the server and keeps the established connection to be opened.
The server then picks up a serving thread from the thread pool and
performs the initial evaluation. The serving thread will return to the
thread pool after the initial evaluation no matter whether the initial
evaluation produces the results or not. The server will re-pickup a
thread from the thread pool for the second evaluation, if necessary,
and notify the network I/O thread to complete the client request by
sending out the results of the second evaluation. Using the pipeline
threading model, no thread (serving threads or network I/O thread)
will be hanged during the processing of TTL RPC call. Thus the
threading model is scalable to thousands of concurrent TTL RPC
calls.

4.2 Fault Tolerance

Like all single master cluster architecture, epiC is designed to
be resilient to a large-scale slave machines failures. epiC treats a
slave machine failure as a network partition from that slave ma-
chine to the master. To detect such a failure, the master communi-
cates with worker trackers running on the slave machines by heart-
beat RPCs. If the master cannot receive heartbeat messages from
a worker tracker many times, it marks that worker tracker as dead
and the machine where that worker tracker runs on as “failed”.

When a worker tracker is marked as failed, the master will deter-
mine whether the tasks that the worker tracker processed need to be
recovered. We assume that users persist the output of an epiC job
into a reliable storage system like HDFS or databases. Therefore,
all completed terminal tasks (i.e., tasks hosting units in the termi-
nal group) need not to be recovered. We only recover in-progress
terminal tasks and all non-terminal tasks (no matter completed or
in-progress).

We adopt task re-execution as the main technique for task recov-
ery and employ an asynchronous output backup scheme to speedup
the recovering process. The task re-execution strategy is concep-
tually simple. However, to make it work, we need to make some
reﬁnements to the basic design. The problem is that, in some cases,
the system may not ﬁnd idle worker processes for re-running the
failed tasks.

For example, let us consider a user job that consists of three unit
groups: a map unit group M with two reduce groups R1 and R2.
The output of M is processed by R1 and the output of R1 is further
processed by R2, the terminal unit group for producing the ﬁnal
output. epiC evaluates this job by placing three unit groups M, R1
and R2, in three stages S1, S2 and S3 respectively. The system ﬁrst
launches tasks in S1 and S2. When the tasks in S1 complete, the
system will launch tasks in S3, and at the same time, shufﬂe data
from S1’s units to S2’s units.
Suppose at this time, a work tracker failure causes a task m’s
(m 2 M) output to be lost, the master will fail to ﬁnd an idle
worker process for re-executing that failed task. This is because all
worker processes are running tasks in S2 and S3 and the data lost
introduced by m causes all tasks in S2 to be stalled. Therefore, no
worker process can complete and go back to the idle state.

We introduce a preemption scheduling scheme to solve the above
deadlock problem.
If a task A fails to fetch data produced by
task B, the task A will notify the master and update its state to
in (cid:0) stick. If the master cannot ﬁnd idle worker processes for re-
covering failed tasks for a given period of time, it will kill in(cid:0)stick
tasks by sending killTask() RPCs to the corresponding worker

Algorithm 1 Generate the list of completed tasks to backup
Input: the worker tracker list W
Output: the list of tasks L to backup
1: for each worker tracker w 2 W do
T   the list of completed tasks performed by w
2:
for each completed task t 2 T do
3:
4:
5:
6:
7:
8: end for

if EB(t) < ER(t) then

L   L [ ftg

end if
end for

trackers. The worker trackers then kill the in (cid:0) stick tasks and
release the corresponding worker processes. Finally, the master
marks the killed in (cid:0) stick tasks as failed and adds them to the
failed task list for scheduling. The preemption scheduling scheme
solves the deadlock problem since epiC executes tasks based on
the stage order. The released worker processes will ﬁrst execute
predecessor failed tasks and then the killed in (cid:0) stick tasks.

Re-execution is the only approach for recovering in-progress tasks.

For completed tasks, we also adopt a task output backup strategy
for recovering. This scheme works as follows. Periodically, the
master notiﬁes the worker trackers to upload the output of com-
pleted tasks to HDFS. When the master detects a worker tracker Wi
fails, it ﬁrst commands another live worker tracker Wj to download
Wi’s completed tasks’ output and then notiﬁes all in-progress tasks
that Wj will server Wi’s completed tasks’ output.

Backing up data to HDFS consumes network bandwidth. So, the
master decides to backup a completed task’s output only if the out-
put backup can yield better performance than task re-execution re-
covery. To make such a decision, for a completed task t, the master
estimates two expected execution time ER and EB of t where ER
is the expected execution time when the task re-execution scheme
is adopted and EB is the expected execution time when the output
backup strategy is chosen. ER and EB are computed as follows

ER = Tt (cid:2) P + 2Tt (cid:2) (1 (cid:0) P )

EB = (Tt + Tu) (cid:2) P + Td (cid:2) (1 (cid:0) P )

(1)

(2)

where P is the probability that the worker track is available during
the job execution; Tt is the execution time of t; Tu is the elapsed
time for uploading output to HDFS; and Td is the elapsed time for
downloading output from HDFS. The three parameters Tt, Tu and
Td are easily collected or estimated. The parameter P is estimated
by the availability of a worker tracker in one day.

The master uses Alg. 1 for determining which completed tasks
should be backed up. The master iterates over each worker tracker
(line 1). For each worker tracker, the master retrieves its completed
task list (line 2). Then, for each task in the completed task list, the
master computes EB and ER and adds the task t into the result list
L if EB < ER (line 4 to line 5).

5. EXPERIMENTS

We evaluate the performance of epiC on different kinds of data
processing tasks, including unstructured data processing, relational
data processing and graph processing. We benchmark epiC against
Hadoop, an open source implementation of MapReduce for pro-
cessing unstructured data (i.e., text data) and relational data and
GPS [24], an open source implementation of Pregel [20] for graph
processing, respectively. For relational data, we also run additional

experiments to benchmark epiC with two new in-memory data pro-
cessing systems, namely Shark and Impala. For all experiments, the
results are reported by averaging six runs.
5.1 Benchmark Environment

The experimental study is conducted on an in-house cluster, con-
sisting of 72 nodes hosted on two racks. The nodes within each
rack are connected by a 1 Gbps switch. The two racks are con-
nected by a 10 Gbps cluster switch. Each cluster node is equipped
with a quad-core Intel Xeon 2.4GHz CPU, 8GB memory and two
500 GB SCSI disks. The hdparm utility reports that the buffered
read throughput of the disk is roughly 110 MB/sec. However, due
to the JVM costs, our tested Java program can only read local ﬁles
at 70 (cid:24) 80 MB/sec.

We choose 65 nodes out of the 72 nodes for our benchmark.
For the 65-node cluster, one node acts as the master for running
Hadoop’s NameNode, JobTracker daemons, GPS’s server node and
epiC’s master daemon. For scalability benchmark, we vary the
number of slave nodes from 1, 4, 16, to 64.
5.2 System Settings

In our experiments, we conﬁgure the three systems as follows:

1. The Hadoop settings consist of two parts: HDFS settings and
MapReduce settings.
In HDFS settings, we set the block
size to be 512 MB. As indicated in [17], this setting can sig-
niﬁcantly reduce Hadoop’s cost for scheduling MapReduce
tasks. We also set the I/O buffer size to 128 KB and the repli-
cation factor of HDFS to one (i.e., no replication). In MapRe-
duce settings, each slave is conﬁgured to run two concurrent
map and reduce tasks. The JVM runs in the server mode with
maximal 1.5 GB heap memory. The size of map task’s sort
buffer is 512 MB. We set the merge factor to be 500 and turn
off speculation scheduling. Finally, we set the JVM reuse
number to -1.

2. For each worker tracker in epiC, we set the size of the worker
pool to be four. In the worker pool, two workers are current
workers (running current units) and the remaining two work-
ers are appending workers. Similar to Hadoop’s setting, each
worker process has 1.5 GB memory. For the MapReduce ex-
tension, we set the bucket size of burst sort to be 8192 keys
(string pointers).

3. For GPS, we employ the default settings of the system with-

out further tuning.

5.3 Benchmark Tasks and Datasets

5.3.1 Benchmark Tasks
The benchmark consists of four tasks: Grep, TeraSort, TPC-H
Q3, and PageRank. The Grep task and TeraSort task are presented
in the original MapReduce paper for demonstrating the scalability
and the efﬁciency of using MapReduce for processing unstructured
data (i.e., plain text data). The Grep task requires us to check each
record (i.e., a line of text string) of the input dataset and output
all records containing a speciﬁc pattern string. The TeraSort task
requires the system to arrange the input records in an ascending or-
der. The TPC-H Q3 task is a standard benchmark query in TPC-H
benchmark and is presented in Section 3.2. The PageRank algo-
rithm [22] is an iterative graph processing algorithm. We refer the
readers to the original paper [22] for the details of the algorithm.

5.3.2 Datasets
We generate the Grep and TeraSort datasets according to the
original MapReduce paper published by Google. The generated
datasets consists of N ﬁxed length records. Each record is a string
and occupies a line in the input ﬁle with the ﬁrst 10 bytes as a key
and the remaining 90 bytes as a value. In the Grep task, we are
required to search the pattern in the value part and in the TeraSort
task, we need to order the input records according to their keys.
Google generates the datasets using 512 MB data per-node setting.
We, however, adopt 1 GB data per-node setting since our HDFS
block size is 512 MB. Therefore, for the 1, 4, 16, 64 nodes clus-
ter, we, for each task (Grep and TeraSort), generate four datasets:
1 GB, 4 GB, 16 GB and 64 GB, one for each cluster setting.

We generate the TPC-H dataset using the dbgen tool shipped
with TPC-H benchmark. We follow the benchmark guide of Hive,
a SQL engine built on top of Hadoop, and generate 10GB data per
node. For the PageRank task, we use a real dataset from Twitter1.
The user proﬁles were crawled from July 6th to July 31st 2009.
For our experiments, we select 8 million vertices and their edges to
construct a graph.
5.4 The Grep Task

Figure 15 and Figure 16 present the performance of employing
epiC and Hadoop for performing Grep task with the cold ﬁle system
cache and the warm ﬁle system cache settings, respectively.

In the cold ﬁle system cache setting (Figure 15), epiC runs twice
faster than Hadoop in all cluster settings. The performance gap
between epiC and Hadoop is mainly due to the startup costs. The
heavy startup cost of Hadoop comes from two factors. First, for
each new MapReduce job, Hadoop must launch brand new java
processes for running the map tasks and reduce tasks. The second,
which is also the most important factor, is the inefﬁcient pulling
mechanism introduced by the RPC that Hadoop employed. In a
64-node cluster, the pulling RPC takes about 10(cid:24)15 seconds for
Hadoop to assign tasks to all free map slots. epiC, however, uses
the worker pool technique to avoid launching java processes for
performing new jobs and employs TTL RPC scheme to assign tasks
in real time. We are aware that Google has recently also adopted the
worker pool technique to reduce the startup latency of MapReduce
[7]. However, from the analysis of this task, clearly, in addition to
the pooling technique, efﬁcient RPC is also important.

In the warm ﬁle system cache setting (Figure 16), the perfor-
mance gap between epiC and Hadoop is even larger, up to a factor
of 4.5. We found that the performance of Hadoop cannot beneﬁt
from warm ﬁle system cache. Even, in the warm cache setting,
the data is read from fast cache memory instead of slow disks, the
performance of Hadoop is only improved by 10%. The reason of
this problem is again due to the inefﬁcient task assignments caused
by RPC. epiC, on the other hand, only takes about 4 seconds to
complete the Grep task in this setting, three times faster than per-
forming the same Grep task in cold cache setting. This is because
the bottleneck of epiC in performing the Grep task is I/O. In the
warm cache setting, the epiC Grep job can read data from memory
rather than disk. Thus, the performance is approaching optimality.
5.5 The TeraSort Task

Figure 17 and Figure 18 show the performance of the two sys-
tems (epiC and Hadoop) for performing TeraSort task in two set-
tings (i.e., warm and cold cache). epiC beats Hadoop in terms of
performance by a factor of two. There are two reasons for the per-
formance gap. First, the map task of Hadoop is CPU bound. On

1http://an.kaist.ac.kr/traces/WWW2010.html

Figure 15: Results of Grep task with cold
ﬁle system cache

Figure 16: Results of Grep task with warm
ﬁle system cache

Figure 17: Results of TeraSort task with
cold ﬁle system cache

Figure 18: Results of TeraSort task with
warm ﬁle system cache

Figure 19: Results of TPC-H Q3

Figure 20: Results of PageRank

average, a map task takes about 7 seconds to read off data from disk
and then takes about 10 seconds to sort the intermediate data. Fi-
nally, another 8 seconds are required to write the intermediate data
to local disks. Sorting approximately occupies 50% of the map exe-
cution time. Second, due to the poor pulling RPC performance, the
notiﬁcations of map tasks cannot be propagated to the reduce tasks
in a timely manner. Therefore, there is a noticeable gap between
map completion and reduce shufﬂing.

epiC, however, has no such bottleneck. Equipped with order-
preserving encoding and burst sort technique, epiC, on average, is
able to sort the intermediate data at about 2.1 seconds, roughly ﬁve
times faster than Hadoop. Also, epiC’s TTL RPC scheme enables
reduce units to receive map completion notiﬁcations in real time.
epiC is able to start shufﬂing 5(cid:24)8 seconds earlier than Hadoop.

Compared to the performance of cold cache setting (Figure 17),
both epiC and Hadoop do not run much faster in the warm cache
setting (Figure 18); there is a 10% improvement at most. This is
because scanning data from disks is not the bottleneck of perform-
ing the TeraSort task. For Hadoop, the bottleneck is the map-side
sorting and data shufﬂing. For epiC, the bottleneck of the map unit
is in persisting intermediate data to disks and the bottleneck of the
reduce unit is in shufﬂing which is network bound. We are plan-
ning to eliminate the map unit data persisting cost by building an
in-memory ﬁle system for holding and shufﬂing intermediate data.

5.6 The TPC(cid:173)H Q3 Task

Figure 19 presents the results of employing epiC and Hadoop to
perform TPC-H Q3 under cold ﬁle system cache 2. For Hadoop,
we ﬁrst use Hive to generate the query plan. Then, according to the
generated query plan, we manually wrote MapReduce programs to
perform this task. Our manually coded MapReduce program runs
30% faster than Hive’s native interpreter based evaluation scheme.
The MapReduce programs consist of ﬁve jobs. The ﬁrst job joins

2For TPC-H Q3 task and PageRank task, the three systems cannot
get a signiﬁcant performance improvement from cache. Therefore,
we remove warm cache results to save space.

customer and orders and produces the join results I1. The sec-
ond job joins I1 with lineitem, followed by aggregating, sort-
ing, and limiting top ten results performed by the remaining three
jobs. The query plan and unit implementation of epiC is presented
in Section 3.2.

Figure 19 shows that epiC runs about 2.5 times faster than Hadoop.

This is because epiC uses fewer operations to evaluate the query (5
units vs. 5 maps and 5 reduces) than Hadoop and employs the asyn-
chronous mechanism for running units. In Hadoop, the ﬁve jobs
run sequentially. Thus, the down stream mappers must wait for the
completion of all up stream reducers to start. In epiC, however,
down stream units can start without waiting for the completion of
up stream units.
5.7 The PageRank Task

This experiment compares three systems in performing the PageR-
ank task. The GPS implementation of PageRank algorithm is iden-
tical to [20]. The epiC implementation of PageRank algorithm con-
sists of a single unit. The details are discussed in Section 2.2. The
Hadoop implementation includes a series of iterative jobs. Each
job reads the output of the previous job to compute the new PageR-
ank values. Similar to the unit of epiC, each mapper and reducer
in Hadoop will process a batch of vertices.
In all experiments,
the PageRank algorithm terminates after 20 iterations. Figure 20
presents the results of the experiment. We ﬁnd that all systems can
provide a scalable performance. However, among the three, epiC
has a better speedup. This is because epiC adopts an asynchronous
communication pattern based on message passing, whereas GPS
needs to synchronize the processing nodes and Hadoop repeatedly
creates new mappers and reducers for each job.
5.8 Fault Tolerance

The ﬁnal experiment studies the ability of epiC for handling
machine failures. In this experiment, both epiC and Hadoop are
employed for performing the TeraSort task. During the data pro-
cessing, we simulate slave machine failures by killing all daemon
processes (TaskTracker, DataNode and worker tracker) running on
those machines. The replication factor of HDFS is set to three,

 0 5 10 15 20 25 30 351 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 5 10 15 20 25 30 351 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 20 40 60 80 100 120 1401 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 20 40 60 80 100 120 1401 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 100 200 300 400 500 600 700 8001 node4 nodes16 nodes64 nodesSecondsHadoopepiC 0 100 200 300 400 500 600 700 800 9001 node4 nodes16 nodes64 nodesSecondsHadoopGPSepiCin-memory processing that Shark and Impala bring.

6. RELATED WORK

Big Data processing systems can be classiﬁed into the following
categories: 1) Parallel Databases, 2) MapReduce based systems, 3)
DAG based data processing systems, 4) Actor-like systems and 5)
hybrid systems. A comprehensive survey could be found in [19],
and a new benchmark called BigBench [13], was also recently pro-
posed to evaluate and compare the performance of different big data
processing systems.

Parallel databases are mainly designed for processing structured
data sets where each data (called a record) strictly forms a relational
schema [10, 9, 12]. These systems employ data partitioning and
partitioned execution techniques for high performance query pro-
cessing. Recent parallel database systems also employ the column-
oriented processing strategy to even improve the performance of
analytical workloads such as OLAP queries [26]. Parallel databases
have been shown to scale to at least peta-byte dataset but with a rel-
atively high cost on hardware and software [3]. The main drawback
of parallel databases is that those system cannot effectively pro-
cess unstructured data. However, there are recent proposals trying
to integrate Hadoop into database systems to mitigate the problem
[27]. Our epiC, on the other hand, has been designed and built from
scratch to provide the scalability, efﬁciency and ﬂexibility found in
both platforms.

MapReduce was proposed by Dean and Ghemawat in [8]. The
system was originally developed as a tool for building inverted in-
dex for large web corpus. However, the ability of using MapReduce
as a general data analysis tool for processing both structured data
and unstructured data was quickly recognized [30] [28]. MapRe-
duce gains popularity due to its simplicity and ﬂexibility. Even
though the programming model is relatively simple (only consists
of two functions), users, however, can specify any kinds of compu-
tations in the map() and reduce() implementations. MapRe-
duce is also extremely scalable and resilient to slave failures. The
main drawback of MapReduce is its inefﬁciency for processing
structured (relational) data and graph data. Many research work
have been proposed to improve the performance of MapReduce on
relational data processing [3][18]. The most recent work shows
that, in order to achieve better performance of relational process-
ing, one must relax the MapReduce programming model and make
non-trivial modiﬁcations to the runtime system [7]. Our work is in
parallel to these work. Instead of using a one-size-ﬁt-all solution,
we propose to use different data processing models to process dif-
ferent data and employ a common concurrent programming model
to parallelize all those data processing.

Dryad is an ongoing research project at Microsoft [15] [16].
Our work is different from that of Dryad - our concurrent pro-
gramming model is entirely independent of communication pat-
terns while Dryad enforces processing units to transfer data through
DAG.

The the concept of Actor was originally proposed for simplify-
ing concurrent programming [14]. Recently, systems like Storm
[2] and S4 [21] implement the Actor abstraction for streaming data
processing. Our concurrent programming model is also inspired
by the Actor model. However, different from Storm and S4, our
system is designed for batch data processing. A job of epiC will
complete eventually. However, jobs of Storm and S4 may never
end. Spark is also a data analytical system based on Actor pro-
gramming model [31]. The system introduces an abstraction called
resilient distributed dataset (RDD) for fault tolerance. In Spark, the
input and output of each operator must be a RDD. RDD is further
implemented as an in-memory data structure for better retrieving

Figure 21: Fault tolerance experiment on a 16 node cluster

Figure 22: Comparison between epiC and other Systems on TPC-H
Q3

so that input data can be resilient to DataNode lost. Both systems
(epiC and Hadoop) adopt heartbeating for failure detection. The
failure timeout threshold is set to 1 minute. We conﬁgure epiC
to use task re-execution scheme for recovery. The experiment is
launched at a 16 node cluster. We simulate 4 machine failures at
50% job completion.

Figure 21 presents the results of this experiment. It can be seen
that machine failures slow down the data processing. Both epiC and
Hadoop experience 2X slowdown when 25% of the nodes fail (H-
Normal and E-Normal respectively denotes the normal execution
time of Hadoop and epiC, while H-Failure and E-Failure respec-
tively denotes the execution time when machine failures occur).
5.9 Comparison with In(cid:173)memory Systems

We also benchmark epiC with two new in-memory database sys-
tems: Shark and Impala. Since epiC is designed as a disk-based
system, while Shark and Impala are in-memory-based systems, the
performance comparison is therefore only useful as an indication
of the efﬁciency of epiC.

For the experiments, since Shark and Impala adopt in-memory
data processing strategy and require the whole working set (both
the raw input and intermediate data produced during the data pro-
cessing) of a query to be in memory, we fail to run Q3 in these
systems on a 10GB per-node settings. Thus, we reduce the dataset
to be 1GB per-node. Shark uses Spark as its underlying data pro-
cessing engine. We set the worker memory of Spark to be 6GB ac-
cording to its manual. Since Impala can automatically detect avail-
able memory to use, we do not tune further. The Hadoop and epiC
settings are the same as in other experiments.

Figure 22 presents the results of this experiment. In one node
setting, both Shark and Impala outperform epiC since the two sys-
tems hold all data in memory while epiC requires to write the in-
termediate data to disks. However, the performance gap vanishes
in multi-node setting. This is because in these settings, all systems
need to shufﬂe data between nodes and thus the network becomes
the bottleneck. The cost of data shufﬂing offsets the beneﬁts of

 0 50 100 150 200Fault-toleranceSecondsH NormalH FailureE NormalE Failure 0 20 40 60 80 100 120 1401 node4 nodes16 nodes64 nodesSecondsHadoopSharkImpalaepiCperformance. Our approach is different from Spark in that unit is
independent of the underline storage system. Users can perform
in-memory data analytical tasks by loading data into certain in-
memory storage systems. Furthermore, different from RDD, epiC
employs a hybrid scheme to achieve fault tolerance.

HadoopDB [3] and PolyBase [11] are new systems for handling
the the variety challenge of Big Data. The difference between these
systems and ours is that the two systems adopt a hybrid architec-
ture and use a combination of a relational database and Hadoop to
process structured data and unstructured data respectively. Our sys-
tem, on the other hand, do not employ the split execution strategy
and use a single system to process all types of data.
7. CONCLUSIONS

This paper presents epiC, a scalable and extensible system for
processing BigData. epiC solves BigData’s data volume challenge
by parallelization and tackles the data variety challenge by decou-
pling the concurrent programming model and the data processing
model. To handle a multi-structured data, users process each data
type with the most appropriate data processing model and wrap
those computations in a simple unit interface. Programs written in
this way can be automatically executed in parallel by epiC’s con-
current runtime system. In addition to the simple yet effective inter-
face design for handling multi-structured data, epiC also introduces
several optimizations in its Actor-like programming model. We
use MapReduce extension and relational extension as two exam-
ples to show the power of epiC. The benchmarking of epiC against
Hadoop and GPS conﬁrms its efﬁciency.
8. ACKNOWLEDGMENTS

The work in this paper was supported by the Singapore Ministry

of Education Grant No. R-252-000-454-112.
9. REFERENCES
[1] The hadoop ofﬁcal website. http://hadoop.apache.org/.
[2] The storm project ofﬁcal website. http://storm-project.net/.
[3] A. Abouzeid, K. Bajda-Pawlikowski, D. Abadi,

A. Silberschatz, and A. Rasin. HadoopDB: an architectural
hybrid of mapreduce and dbms technologies for analytical
workloads. PVLDB, 2(1), Aug. 2009.

[4] D. Battr´e, S. Ewen, F. Hueske, O. Kao, V. Markl, and

D. Warneke. Nephele/pacts: a programming model and
execution framework for web-scale analytical processing. In
SoCC, 2010.

[5] J. L. Bentley and R. Sedgewick. Fast algorithms for sorting

and searching strings. In SODA, 1997.

[6] Y. Bu, B. Howe, M. Balazinska, and M. D. Ernst. HaLoop:
efﬁcient iterative data processing on large clusters. VLDB,
3(1-2), Sept. 2010.

[7] B. Chattopadhyay, L. Lin, W. Liu, S. Mittal, P. Aragonda,

V. Lychagina, Y. Kwon, and M. Wong. Tenzing a SQL
implementation on the MapReduce framework. In VLDB,
2011.

[8] J. Dean and S. Ghemawat. MapReduce: simpliﬁed data
processing on large clusters. Commun. ACM, 51(1), Jan.
2008.

[9] D. J. DeWitt, R. H. Gerber, G. Graefe, M. L. Heytens, K. B.
Kumar, and M. Muralikrishna. Gamma - a high performance
dataﬂow database machine. In VLDB, 1986.

[10] D. J. DeWitt and J. Gray. Parallel database systems: The
future of high performance database systems. Commun.
ACM, 35(6), 1992.

[11] D. J. DeWitt, A. Halverson, R. Nehme, S. Shankar,

J. Aguilar-Saborit, A. Avanes, M. Flasza, and J. Gramling.
Split query processing in polybase. In SIGMOD, 2013.

[12] S. Fushimi, M. Kitsuregawa, and H. Tanaka. An overview of
the system software of a parallel relational database machine
grace. In VLDB, 1986.

[13] A. Ghazal, T. Rabl, M. Hu, F. Raab, M. Poess, A. Crolotte,

and H.-A. Jacobsen. BigBench: Towards an industry
standard benchmark for big data analytics. In SIGMOD,
2013.

[14] C. Hewitt, P. Bishop, and R. Steiger. A universal modular
actor formalism for artiﬁcial intelligence. In IJCAI, 1973.

[15] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad:

distributed data-parallel programs from sequential building
blocks. SIGOPS Oper. Syst. Rev., 41(3), Mar. 2007.

[16] M. Isard and Y. Yu. Distributed data-parallel computing
using a high-level programming language. In SIGMOD,
2009.

[17] D. Jiang, B. C. Ooi, L. Shi, and S. Wu. The performance of
MapReduce: an in-depth study. PVLDB, 3(1-2), Sept. 2010.

[18] D. Jiang, A. K. H. Tung, and G. Chen.

MAP-JOIN-REDUCE: Toward scalable and efﬁcient data
analysis on large clusters. IEEE TKDE, 23(9), Sept. 2011.
[19] F. Li, B. C. Ooi, M. T. ¨Ozsu, and S. Wu. Distributed data
management using MapReduce. ACM Comput. Surv. (to
appear), 46(3), 2014.

[20] G. Malewicz, M. H. Austern, A. J. C. Bik, J. C. Dehnert,

I. Horn, N. Leiser, and G. Czajkowski. Pregel: a system for
large-scale graph processing. In SIGMOD, 2010.

[21] L. Neumeyer, B. Robbins, A. Nair, and A. Kesari. S4:

Distributed stream computing platform. In ICDMW, 2010.

[22] L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web.
Technical report, Stanford InfoLab, November 1999.

[23] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt,

S. Madden, and M. Stonebraker. A comparison of
approaches to large-scale data analysis. In SIGMOD, 2009.

[24] S. Salihoglu and J. Widom. GPS: A graph processing

system. In SSDBM (Technical Report), 2013.

[25] R. Sinha and J. Zobel. Cache-conscious sorting of large sets
of strings with dynamic tries. J. Exp. Algorithmics, 9, Dec.
2004.

[26] M. Stonebraker, D. J. Abadi, A. Batkin, X. Chen,

M. Cherniack, M. Ferreira, E. Lau, A. Lin, S. Madden,
E. O’Neil, P. O’Neil, A. Rasin, N. Tran, and S. Zdonik.
C-store: a column-oriented dbms. In VLDB, 2005.

[27] X. Su and G. Swart. Oracle in-database Hadoop: when

MapReduce meets RDBMS. In SIGMOD, 2012.

[28] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka,

S. Anthony, H. Liu, P. Wyckoff, and R. Murthy. Hive: a
warehousing solution over a map-reduce framework. VLDB,
2(2), Aug. 2009.

[29] S. Wu, F. Li, S. Mehrotra, and B. C. Ooi. Query optimization

for massively parallel data processing. In SoCC, 2011.

[30] H. Yang, A. Dasdan, R. Hsiao, and D. S. Parker.

Map-Reduce-Merge: simpliﬁed relational data processing on
large clusters. In SIGMOD, 2007.

[31] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,

M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica.
Resilient distributed datasets: A fault-tolerant abstraction for
in-memory cluster computing. In NSDI’12, 2012.

