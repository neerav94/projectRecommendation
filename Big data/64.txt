 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 

	  
	  

	  

	  

	  

	  
	  
	  

	  

 

3.1	  Heterogeneity	  and	  Incompleteness	  

When	  humans	  consume	  information,	  a	  great	  deal	  of	  heterogeneity	  is	  comfortably	  tolerated.	  	  In	  
fact,	   the	   nuance	   and	   richness	   of	   natural	   language	   can	   provide	   valuable	   depth.	  	  	   However,	   machine	  
analysis	   algorithms	   expect	   homogeneous	   data,	   and	   cannot	   understand	   nuance.	  	   In	   consequence,	   data	  
must	  be	  carefully	  structured	  as	  a	  first	  step	  in	  (or	  prior	  to)	  data	  analysis.	  	  Consider,	  for	  example,	  a	  patient	  
who	  has	  multiple	  medical	  procedures	  at	  a	  hospital.	  	  We	  could	  create	  one	  record	  per	  medical	  procedure	  
or	   laboratory	   test,	   one	   record	   for	   the	   entire	   hospital	   stay,	   or	   one	   record	   for	   all	   lifetime	   hospital	  
interactions	  	  of	  	  this	  	  patient.	  	  	  	   With	  	  anything	  	  other	  	  than	  	  the	  	  first	  	  design,	  	  the	  	  number	  	  of	  	  medical	  
procedures	  and	  lab	  tests	  per	  record	  would	  be	  different	  for	  each	  patient.	  	  	  The	  three	  design	  choices	  listed	  
have	  successively	  less	  structure	  and,	  conversely,	  successively	  greater	  variety.	  	   Greater	  structure	  is	  likely	  
to	  be	  required	  by	  many	  (traditional)	  data	  analysis	  systems.	  	  However,	  the	  less	  structured	  design	  is	  likely	  
to	   be	   more	   effective	   for	   many	   purposes	   –	   for	   example	   questions	   relating	   to	   disease	   progression	   over	  
time	   will	   require	   an	   expensive	   join	   operation	   with	   the	   first	   two	   designs,	   but	   can	   be	   avoided	   with	   the	  
latter.	  	  	  	  However,	  computer	  systems	  work	  most	  efficiently	  if	  they	  can	  store	  multiple	  items	  that	  are	  all	  
identical	   in	   size	   and	   structure.	   	   Efficient	   representation,	   access,	   and	   analysis	   of	   semi-­‐structured	   data	  
require	  further	  work.	  

Consider	  an	  electronic	  health	  record	  database	  design	  that	  has	  fields	  for	  birth	  date,	  occupation,	  
and	  blood	  type	  for	  each	  patient.	  	   What	  do	  we	  do	  if	  one	  or	  more	  of	  these	  pieces	  of	  information	  is	  not	  
provided	   by	   a	   patient?	   	   	   Obviously,	   the	   health	   record	   is	   still	   placed	   in	   the	   database,	   but	   with	   the	  
corresponding	  attribute	  values	  being	  set	  to	  NULL.	  	  	  A	  data	  analysis	  that	  looks	  to	  classify	  patients	  by,	  say,	  
occupation,	   must	   take	   into	   account	   patients	   for	   which	   this	   information	   is	   not	   known.	   	   Worse,	   these	  
patients	  with	  unknown	  occupations	  can	  be	  ignored	  in	  the	  analysis	  only	  if	  we	  have	  reason	  to	  believe	  that	  
they	  	   are	  	   otherwise	  	   statistically	  	   similar	  	   to	  	   the	  	   patients	  	   with	  	   known	  	   occupation	  	   for	  	   the	  	   analysis	  
performed.	   	   For	   example,	   if	   unemployed	   patients	   are	   more	   likely	   to	   hide	   their	   employment	   status,	  
analysis	   results	   may	   be	   skewed	   in	   that	   it	   considers	   a	   more	   employed	   population	   mix	   than	   exists,	   and	  
hence	  potentially	  one	  that	  has	  differences	  in	  occupation-­‐related	  health-­‐profiles.	  

Even	  after	  data	  cleaning	  and	  error	  correction,	  some	  incompleteness	  and	  some	  errors	  in	  data	  are	  
likely	  to	  remain.	  	   This	  incompleteness	  and	  these	  errors	  must	  be	  managed	  during	  data	  analysis.	  	   Doing	  
this	   correctly	   is	   a	   challenge.	   	   Recent	   work	   on	   managing	   probabilistic	   data	   suggests	   one	   way	   to	   make	  
progress.	  

3.2	  Scale	  

Of	  course,	  the	  first	  thing	  anyone	  thinks	  of	  with	  Big	  Data	  is	  its	  size.	  	   After	  all,	  the	  word	  “big”	  is	  
there	  in	  the	  very	  name.	  	   Managing	  large	  and	  rapidly	  increasing	  volumes	  of	  data	  has	  been	  a	  challenging	  
issue	  for	  many	  decades.	  	  	  In	  the	  past,	  this	  challenge	  was	  mitigated	  by	  processors	  getting	  faster,	  following	  
Moore’s	  law,	  to	  provide	  us	  with	  the	  resources	  needed	  to	  cope	  with	  increasing	  volumes	  of	  data.	  	   But,	  

8 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 

	  
	  

	  

	  

	  

	  
	  
	  

	  

 

3.1	  Heterogeneity	  and	  Incompleteness	  

When	  humans	  consume	  information,	  a	  great	  deal	  of	  heterogeneity	  is	  comfortably	  tolerated.	  	  In	  
fact,	   the	   nuance	   and	   richness	   of	   natural	   language	   can	   provide	   valuable	   depth.	  	  	   However,	   machine	  
analysis	   algorithms	   expect	   homogeneous	   data,	   and	   cannot	   understand	   nuance.	  	   In	   consequence,	   data	  
must	  be	  carefully	  structured	  as	  a	  first	  step	  in	  (or	  prior	  to)	  data	  analysis.	  	  Consider,	  for	  example,	  a	  patient	  
who	  has	  multiple	  medical	  procedures	  at	  a	  hospital.	  	  We	  could	  create	  one	  record	  per	  medical	  procedure	  
or	   laboratory	   test,	   one	   record	   for	   the	   entire	   hospital	   stay,	   or	   one	   record	   for	   all	   lifetime	   hospital	  
interactions	  	  of	  	  this	  	  patient.	  	  	  	   With	  	  anything	  	  other	  	  than	  	  the	  	  first	  	  design,	  	  the	  	  number	  	  of	  	  medical	  
procedures	  and	  lab	  tests	  per	  record	  would	  be	  different	  for	  each	  patient.	  	  	  The	  three	  design	  choices	  listed	  
have	  successively	  less	  structure	  and,	  conversely,	  successively	  greater	  variety.	  	   Greater	  structure	  is	  likely	  
to	  be	  required	  by	  many	  (traditional)	  data	  analysis	  systems.	  	  However,	  the	  less	  structured	  design	  is	  likely	  
to	   be	   more	   effective	   for	   many	   purposes	   –	   for	   example	   questions	   relating	   to	   disease	   progression	   over	  
time	   will	   require	   an	   expensive	   join	   operation	   with	   the	   first	   two	   designs,	   but	   can	   be	   avoided	   with	   the	  
latter.	  	  	  	  However,	  computer	  systems	  work	  most	  efficiently	  if	  they	  can	  store	  multiple	  items	  that	  are	  all	  
identical	   in	   size	   and	   structure.	   	   Efficient	   representation,	   access,	   and	   analysis	   of	   semi-­‐structured	   data	  
require	  further	  work.	  

Consider	  an	  electronic	  health	  record	  database	  design	  that	  has	  fields	  for	  birth	  date,	  occupation,	  
and	  blood	  type	  for	  each	  patient.	  	   What	  do	  we	  do	  if	  one	  or	  more	  of	  these	  pieces	  of	  information	  is	  not	  
provided	   by	   a	   patient?	   	   	   Obviously,	   the	   health	   record	   is	   still	   placed	   in	   the	   database,	   but	   with	   the	  
corresponding	  attribute	  values	  being	  set	  to	  NULL.	  	  	  A	  data	  analysis	  that	  looks	  to	  classify	  patients	  by,	  say,	  
occupation,	   must	   take	   into	   account	   patients	   for	   which	   this	   information	   is	   not	   known.	   	   Worse,	   these	  
patients	  with	  unknown	  occupations	  can	  be	  ignored	  in	  the	  analysis	  only	  if	  we	  have	  reason	  to	  believe	  that	  
they	  	   are	  	   otherwise	  	   statistically	  	   similar	  	   to	  	   the	  	   patients	  	   with	  	   known	  	   occupation	  	   for	  	   the	  	   analysis	  
performed.	   	   For	   example,	   if	   unemployed	   patients	   are	   more	   likely	   to	   hide	   their	   employment	   status,	  
analysis	   results	   may	   be	   skewed	   in	   that	   it	   considers	   a	   more	   employed	   population	   mix	   than	   exists,	   and	  
hence	  potentially	  one	  that	  has	  differences	  in	  occupation-­‐related	  health-­‐profiles.	  

Even	  after	  data	  cleaning	  and	  error	  correction,	  some	  incompleteness	  and	  some	  errors	  in	  data	  are	  
likely	  to	  remain.	  	   This	  incompleteness	  and	  these	  errors	  must	  be	  managed	  during	  data	  analysis.	  	   Doing	  
this	   correctly	   is	   a	   challenge.	   	   Recent	   work	   on	   managing	   probabilistic	   data	   suggests	   one	   way	   to	   make	  
progress.	  

3.2	  Scale	  

Of	  course,	  the	  first	  thing	  anyone	  thinks	  of	  with	  Big	  Data	  is	  its	  size.	  	   After	  all,	  the	  word	  “big”	  is	  
there	  in	  the	  very	  name.	  	   Managing	  large	  and	  rapidly	  increasing	  volumes	  of	  data	  has	  been	  a	  challenging	  
issue	  for	  many	  decades.	  	  	  In	  the	  past,	  this	  challenge	  was	  mitigated	  by	  processors	  getting	  faster,	  following	  
Moore’s	  law,	  to	  provide	  us	  with	  the	  resources	  needed	  to	  cope	  with	  increasing	  volumes	  of	  data.	  	   But,	  

8 

 

	  

	  

	  

 

there	  is	  a	  fundamental	  shift	  underway	  now:	  data	  volume	  is	  scaling	  faster	  than	  compute	  resources,	  and	  
CPU	  speeds	  are	  static.	  

First,	  over	  the	  last	  five	  years	  the	  processor	  technology	  has	  made	  a	  dramatic	  shift	  -­‐	  rather	  than	  
processors	   doubling	   their	   clock	   cycle	   frequency	   every	   18-­‐24	   months,	   now,	   due	   to	   power	   constraints,	  
clock	  speeds	  have	  largely	  stalled	  and	  processors	  are	  being	  built	  with	  increasing	  numbers	  of	  cores.	  	  In	  the	  
past,	  large	  data	  processing	  systems	  had	  to	  worry	  about	  parallelism	  across	  nodes	  in	  a	  cluster;	  now,	  one	  
has	   to	   deal	   with	   parallelism	   within	   a	   single	   node.	  	  	  Unfortunately,	   parallel	   data	   processing	   techniques	  
that	   were	   applied	   in	   the	   past	   for	   processing	   data	   across	   nodes	   don’t	   directly	   apply	   for	   intra-­‐node	  
parallelism,	   since	   the	   architecture	   looks	   very	   different;	   for	   example,	   there	   are	   many	   more	   hardware	  
resources	   such	  as	   processor	   caches	   and	   processor	  memory	   channels	   that	   are	  shared	   across	  cores	   in	  a	  
single	   node.	  	  	  Furthermore,	   the	   move	   towards	   packing	   multiple	   sockets	   (each	   with	   10s	   of	   cores)	   adds	  
another	  level	  of	  complexity	  for	  intra-­‐node	  parallelism.	  	  Finally,	  with	  predictions	  of	  “dark	  silicon”,	  namely	  
that	  power	  consideration	  will	  likely	  in	  the	  future	  prohibit	  us	  from	  using	  all	  of	  the	  hardware	  in	  the	  system	  
continuously,	  data	  processing	  systems	  will	  likely	  have	  to	  actively	  manage	  the	  power	  consumption	  of	  the	  
processor.	  These	  unprecedented	  changes	  require	  us	  to	  rethink	  how	  we	  design,	  build	  and	  operate	  data	  
processing	  components.	  

The	   second	  dramatic	   shift	   that	   is	   underway	   is	   the	   move	   towards	   cloud	  computing,	   which	  now	  
aggregates	   multiple	   disparate	   workloads	   with	   varying	   performance	   goals	   (e.g.	   interactive	   services	  
demand	  that	  the	  data	  processing	  engine	  return	  back	  an	  answer	  within	  a	  fixed	  response	  time	  cap)	  into	  
very	  large	  clusters.	  This	  level	  of	  sharing	  of	  resources	  on	  expensive	  and	  large	  clusters	  requires	  new	  ways	  
of	   determining	   how	   to	   run	   and	   execute	   data	   processing	   jobs	   so	   that	   we	   can	   meet	   the	   goals	   of	   each	  
workload	  cost-­‐effectively,	  and	  to	  deal	  with	  system	  failures,	  which	  occur	  more	  frequently	  as	  we	  operate	  
on	   larger	   and	   larger	   clusters	   (that	   are	   required	   to	   deal	   with	   the	   rapid	   growth	   in	   data	   volumes).	  	  	  This	  
places	  	  a	  	  premium	  	  on	  	  declarative	  	  approaches	  	  to	  	  expressing	  	  programs,	  	  even	  	  those	  	  doing	  	  complex	  
machine	  learning	  tasks,	  since	  global	  optimization	  across	  multiple	  users’	  programs	  is	  necessary	  for	  good	  
overall	   performance.	   	   Reliance	   on	   user-­‐driven	   program	   optimizations	   is	   likely	   to	   lead	   to	   poor	   cluster	  
utilization,	   since	   users	   are	   unaware	   of	   other	   users’	   programs.	   	   	   	   System-­‐driven	   holistic	   optimization	  
requires	  	   programs	  	   to	  	   be	  	   sufficiently	  	   transparent,	  	   e.g.,	  	   as	  	   in	  	   relational	  	   database	  	   systems,	  	   where	  
declarative	  query	  languages	  are	  designed	  with	  this	  in	  mind.	  

A	   third	   dramatic	   shift	   that	   is	   underway	   is	   the	   transformative	   change	   of	   the	   traditional	   I/O	  
subsystem.	  For	  many	  decades,	  hard	  disk	  drives	  (HDDs)	  were	  used	  to	  store	  persistent	  data.	  HDDs	  had	  far	  
slower	  random	  IO	  performance	  than	  sequential	  IO	  performance,	  and	  data	  processing	  engines	  formatted	  
their	  data	  and	  designed	  their	  query	  processing	  methods	  to	  “work	  around”	  this	  limitation.	  But,	  HDDs	  are	  
increasingly	   being	   replaced	   by	   solid	   state	   drives	   today,	   and	   other	   technologies	   such	   as	   Phase	   Change	  
Memory	  are	  around	  the	  corner.	  These	  newer	  storage	  technologies	  do	  not	  have	  the	  same	  large	  spread	  in	  
performance	  between	  the	  sequential	  and	  random	  I/O	  performance,	  which	  requires	  a	  rethinking	  of	  how	  
we	   design	   storage	   subsystems	   for	   data	   processing	   systems.	   Implications	   of	   this	   changing	   storage	  
subsystem	   potentially	   touch	   every	   aspect	   of	   data	   processing,	   including	   query	   processing	   algorithms,	  
query	  scheduling,	  database	  design,	  concurrency	  control	  methods	  and	  recovery	  methods.	  

9 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 

	  
	  

	  

	  

	  

	  
	  
	  

	  

 

3.1	  Heterogeneity	  and	  Incompleteness	  

When	  humans	  consume	  information,	  a	  great	  deal	  of	  heterogeneity	  is	  comfortably	  tolerated.	  	  In	  
fact,	   the	   nuance	   and	   richness	   of	   natural	   language	   can	   provide	   valuable	   depth.	  	  	   However,	   machine	  
analysis	   algorithms	   expect	   homogeneous	   data,	   and	   cannot	   understand	   nuance.	  	   In	   consequence,	   data	  
must	  be	  carefully	  structured	  as	  a	  first	  step	  in	  (or	  prior	  to)	  data	  analysis.	  	  Consider,	  for	  example,	  a	  patient	  
who	  has	  multiple	  medical	  procedures	  at	  a	  hospital.	  	  We	  could	  create	  one	  record	  per	  medical	  procedure	  
or	   laboratory	   test,	   one	   record	   for	   the	   entire	   hospital	   stay,	   or	   one	   record	   for	   all	   lifetime	   hospital	  
interactions	  	  of	  	  this	  	  patient.	  	  	  	   With	  	  anything	  	  other	  	  than	  	  the	  	  first	  	  design,	  	  the	  	  number	  	  of	  	  medical	  
procedures	  and	  lab	  tests	  per	  record	  would	  be	  different	  for	  each	  patient.	  	  	  The	  three	  design	  choices	  listed	  
have	  successively	  less	  structure	  and,	  conversely,	  successively	  greater	  variety.	  	   Greater	  structure	  is	  likely	  
to	  be	  required	  by	  many	  (traditional)	  data	  analysis	  systems.	  	  However,	  the	  less	  structured	  design	  is	  likely	  
to	   be	   more	   effective	   for	   many	   purposes	   –	   for	   example	   questions	   relating	   to	   disease	   progression	   over	  
time	   will	   require	   an	   expensive	   join	   operation	   with	   the	   first	   two	   designs,	   but	   can	   be	   avoided	   with	   the	  
latter.	  	  	  	  However,	  computer	  systems	  work	  most	  efficiently	  if	  they	  can	  store	  multiple	  items	  that	  are	  all	  
identical	   in	   size	   and	   structure.	   	   Efficient	   representation,	   access,	   and	   analysis	   of	   semi-­‐structured	   data	  
require	  further	  work.	  

Consider	  an	  electronic	  health	  record	  database	  design	  that	  has	  fields	  for	  birth	  date,	  occupation,	  
and	  blood	  type	  for	  each	  patient.	  	   What	  do	  we	  do	  if	  one	  or	  more	  of	  these	  pieces	  of	  information	  is	  not	  
provided	   by	   a	   patient?	   	   	   Obviously,	   the	   health	   record	   is	   still	   placed	   in	   the	   database,	   but	   with	   the	  
corresponding	  attribute	  values	  being	  set	  to	  NULL.	  	  	  A	  data	  analysis	  that	  looks	  to	  classify	  patients	  by,	  say,	  
occupation,	   must	   take	   into	   account	   patients	   for	   which	   this	   information	   is	   not	   known.	   	   Worse,	   these	  
patients	  with	  unknown	  occupations	  can	  be	  ignored	  in	  the	  analysis	  only	  if	  we	  have	  reason	  to	  believe	  that	  
they	  	   are	  	   otherwise	  	   statistically	  	   similar	  	   to	  	   the	  	   patients	  	   with	  	   known	  	   occupation	  	   for	  	   the	  	   analysis	  
performed.	   	   For	   example,	   if	   unemployed	   patients	   are	   more	   likely	   to	   hide	   their	   employment	   status,	  
analysis	   results	   may	   be	   skewed	   in	   that	   it	   considers	   a	   more	   employed	   population	   mix	   than	   exists,	   and	  
hence	  potentially	  one	  that	  has	  differences	  in	  occupation-­‐related	  health-­‐profiles.	  

Even	  after	  data	  cleaning	  and	  error	  correction,	  some	  incompleteness	  and	  some	  errors	  in	  data	  are	  
likely	  to	  remain.	  	   This	  incompleteness	  and	  these	  errors	  must	  be	  managed	  during	  data	  analysis.	  	   Doing	  
this	   correctly	   is	   a	   challenge.	   	   Recent	   work	   on	   managing	   probabilistic	   data	   suggests	   one	   way	   to	   make	  
progress.	  

3.2	  Scale	  

Of	  course,	  the	  first	  thing	  anyone	  thinks	  of	  with	  Big	  Data	  is	  its	  size.	  	   After	  all,	  the	  word	  “big”	  is	  
there	  in	  the	  very	  name.	  	   Managing	  large	  and	  rapidly	  increasing	  volumes	  of	  data	  has	  been	  a	  challenging	  
issue	  for	  many	  decades.	  	  	  In	  the	  past,	  this	  challenge	  was	  mitigated	  by	  processors	  getting	  faster,	  following	  
Moore’s	  law,	  to	  provide	  us	  with	  the	  resources	  needed	  to	  cope	  with	  increasing	  volumes	  of	  data.	  	   But,	  

8 

 

	  

	  

	  

 

there	  is	  a	  fundamental	  shift	  underway	  now:	  data	  volume	  is	  scaling	  faster	  than	  compute	  resources,	  and	  
CPU	  speeds	  are	  static.	  

First,	  over	  the	  last	  five	  years	  the	  processor	  technology	  has	  made	  a	  dramatic	  shift	  -­‐	  rather	  than	  
processors	   doubling	   their	   clock	   cycle	   frequency	   every	   18-­‐24	   months,	   now,	   due	   to	   power	   constraints,	  
clock	  speeds	  have	  largely	  stalled	  and	  processors	  are	  being	  built	  with	  increasing	  numbers	  of	  cores.	  	  In	  the	  
past,	  large	  data	  processing	  systems	  had	  to	  worry	  about	  parallelism	  across	  nodes	  in	  a	  cluster;	  now,	  one	  
has	   to	   deal	   with	   parallelism	   within	   a	   single	   node.	  	  	  Unfortunately,	   parallel	   data	   processing	   techniques	  
that	   were	   applied	   in	   the	   past	   for	   processing	   data	   across	   nodes	   don’t	   directly	   apply	   for	   intra-­‐node	  
parallelism,	   since	   the	   architecture	   looks	   very	   different;	   for	   example,	   there	   are	   many	   more	   hardware	  
resources	   such	  as	   processor	   caches	   and	   processor	  memory	   channels	   that	   are	  shared	   across	  cores	   in	  a	  
single	   node.	  	  	  Furthermore,	   the	   move	   towards	   packing	   multiple	   sockets	   (each	   with	   10s	   of	   cores)	   adds	  
another	  level	  of	  complexity	  for	  intra-­‐node	  parallelism.	  	  Finally,	  with	  predictions	  of	  “dark	  silicon”,	  namely	  
that	  power	  consideration	  will	  likely	  in	  the	  future	  prohibit	  us	  from	  using	  all	  of	  the	  hardware	  in	  the	  system	  
continuously,	  data	  processing	  systems	  will	  likely	  have	  to	  actively	  manage	  the	  power	  consumption	  of	  the	  
processor.	  These	  unprecedented	  changes	  require	  us	  to	  rethink	  how	  we	  design,	  build	  and	  operate	  data	  
processing	  components.	  

The	   second	  dramatic	   shift	   that	   is	   underway	   is	   the	   move	   towards	   cloud	  computing,	   which	  now	  
aggregates	   multiple	   disparate	   workloads	   with	   varying	   performance	   goals	   (e.g.	   interactive	   services	  
demand	  that	  the	  data	  processing	  engine	  return	  back	  an	  answer	  within	  a	  fixed	  response	  time	  cap)	  into	  
very	  large	  clusters.	  This	  level	  of	  sharing	  of	  resources	  on	  expensive	  and	  large	  clusters	  requires	  new	  ways	  
of	   determining	   how	   to	   run	   and	   execute	   data	   processing	   jobs	   so	   that	   we	   can	   meet	   the	   goals	   of	   each	  
workload	  cost-­‐effectively,	  and	  to	  deal	  with	  system	  failures,	  which	  occur	  more	  frequently	  as	  we	  operate	  
on	   larger	   and	   larger	   clusters	   (that	   are	   required	   to	   deal	   with	   the	   rapid	   growth	   in	   data	   volumes).	  	  	  This	  
places	  	  a	  	  premium	  	  on	  	  declarative	  	  approaches	  	  to	  	  expressing	  	  programs,	  	  even	  	  those	  	  doing	  	  complex	  
machine	  learning	  tasks,	  since	  global	  optimization	  across	  multiple	  users’	  programs	  is	  necessary	  for	  good	  
overall	   performance.	   	   Reliance	   on	   user-­‐driven	   program	   optimizations	   is	   likely	   to	   lead	   to	   poor	   cluster	  
utilization,	   since	   users	   are	   unaware	   of	   other	   users’	   programs.	   	   	   	   System-­‐driven	   holistic	   optimization	  
requires	  	   programs	  	   to	  	   be	  	   sufficiently	  	   transparent,	  	   e.g.,	  	   as	  	   in	  	   relational	  	   database	  	   systems,	  	   where	  
declarative	  query	  languages	  are	  designed	  with	  this	  in	  mind.	  

A	   third	   dramatic	   shift	   that	   is	   underway	   is	   the	   transformative	   change	   of	   the	   traditional	   I/O	  
subsystem.	  For	  many	  decades,	  hard	  disk	  drives	  (HDDs)	  were	  used	  to	  store	  persistent	  data.	  HDDs	  had	  far	  
slower	  random	  IO	  performance	  than	  sequential	  IO	  performance,	  and	  data	  processing	  engines	  formatted	  
their	  data	  and	  designed	  their	  query	  processing	  methods	  to	  “work	  around”	  this	  limitation.	  But,	  HDDs	  are	  
increasingly	   being	   replaced	   by	   solid	   state	   drives	   today,	   and	   other	   technologies	   such	   as	   Phase	   Change	  
Memory	  are	  around	  the	  corner.	  These	  newer	  storage	  technologies	  do	  not	  have	  the	  same	  large	  spread	  in	  
performance	  between	  the	  sequential	  and	  random	  I/O	  performance,	  which	  requires	  a	  rethinking	  of	  how	  
we	   design	   storage	   subsystems	   for	   data	   processing	   systems.	   Implications	   of	   this	   changing	   storage	  
subsystem	   potentially	   touch	   every	   aspect	   of	   data	   processing,	   including	   query	   processing	   algorithms,	  
query	  scheduling,	  database	  design,	  concurrency	  control	  methods	  and	  recovery	  methods.	  

9 

 

	  

	  

	  

	  
	  
	  

	  

	  

 

3.3	  Timeliness	  

The	  flip	  side	  of	  size	  is	  speed.	  	   The	  larger	  the	  data	  set	  to	  be	  processed,	  the	  longer	  it	  will	  take	  to	  
analyze.	  	  The	  design	  of	  a	  system	  that	  effectively	  deals	  with	  size	  is	  likely	  also	  to	  result	  in	  a	  system	  that	  can	  
process	  a	  given	  size	  of	  data	  set	  faster.	  	  However,	  it	  is	  not	  just	  this	  speed	  that	  is	  usually	  meant	  when	  one	  
speaks	  of	  Velocity	  in	  the	  context	  of	  Big	  Data.	  	  Rather,	  there	  is	  an	  acquisition	  rate	  challenge	  as	  described	  
in	  Sec.	  2.1,	  and	  a	  timeliness	  challenge	  described	  next.	  

There	   are	   many	   situations	   in	   which	   the	   result	   of	   the	   analysis	   is	   required	   immediately.	   	   For	  
example,	   if	   a	   fraudulent	   credit	   card	   transaction	   is	   suspected,	   it	   should	   ideally	   be	   flagged	   before	   the	  
transaction	  is	  completed	  –	  potentially	  preventing	  the	  transaction	  from	  taking	  place	  at	  all.	  	   Obviously,	  a	  
full	   analysis	   of	   a	   user’s	   purchase	   history	   is	   not	   likely	   to	   be	   feasible	   in	   real-­‐time.	   	   Rather,	   we	   need	   to	  
develop	  partial	  results	  in	  advance	  so	  that	  a	  small	  amount	  of	  incremental	  computation	  with	  new	  data	  can	  
be	  used	  to	  arrive	  at	  a	  quick	  determination.	  

Given	  a	  large	  data	  set,	  it	  is	  often	  necessary	  to	  find	  elements	  in	  it	  that	  meet	  a	  specified	  criterion.	  
In	  the	  course	  of	  data	  analysis,	  this	  sort	  of	  search	  is	  likely	  to	  occur	  repeatedly.	  	   Scanning	  the	  entire	  data	  
set	  to	  find	  suitable	  elements	  is	  obviously	  impractical.	  	  Rather,	  index	  structures	  are	  created	  in	  advance	  to	  
permit	   finding	   qualifying	   elements	   quickly.	   	   The	   problem	   is	   that	   each	   index	   structure	   is	   designed	   to	  
support	  only	  some	  classes	  of	  criteria.	  	  With	  new	  analyses	  desired	  using	  Big	  Data,	  there	  are	  new	  types	  of	  
criteria	   specified,	   and	   a	   need	   to	   devise	   new	   index	   structures	   to	   support	   such	   criteria.	   	   For	   example,	  
consider	   a	   traffic	   management	   system	   with	   information	   regarding	   thousands	   of	   vehicles	   and	   local	   hot	  
spots	  on	  roadways.	  	   The	  system	  may	  need	  to	  predict	  potential	  congestion	  points	  along	  a	  route	  chosen	  
by	   a	   user,	   and	   suggest	   alternatives.	   	   Doing	   so	   requires	   evaluating	   multiple	   spatial	   proximity	   queries	  
working	   with	   the	   trajectories	   of	   moving	   objects.	   	   New	   index	   structures	   are	   required	   to	   support	   such	  
queries.	   Designing	   such	   structures	   becomes	   particularly	   challenging	   when	   the	   data	   volume	   is	   growing	  
rapidly	  and	  the	  queries	  have	  tight	  response	  time	  limits.	  

3.4	  Privacy	  

The	  privacy	  of	  data	  is	  another	  huge	  concern,	  and	  one	  that	  increases	  in	  the	  context	  of	  Big	  Data.	  
For	  electronic	  health	  records,	  there	  are	  strict	  laws	  governing	  what	  can	  and	  cannot	  be	  done.	  	   For	  other	  
data,	  regulations,	  particularly	  in	  the	  US,	  are	  less	  forceful.	  	   However,	  there	  is	  great	  public	  fear	  regarding	  
the	   inappropriate	   use	   of	   personal	   data,	   particularly	   through	   linking	   of	   data	   from	   multiple	   sources.	  
Managing	   privacy	   is	   effectively	   both	   a	   technical	   and	   a	   sociological	   problem,	  which	   must	   be	   addressed	  
jointly	  from	  both	  perspectives	  to	  realize	  the	  promise	  of	  big	  data.	  

Consider,	   for	   example,	   data	   gleaned	   from	   location-­‐based	   services.	   	   These	   new	   architectures	  
require	  a	  user	  to	  share	  his/her	  location	  with	  the	  service	  provider,	  resulting	  in	  obvious	  privacy	  concerns.	  
Note	  that	  hiding	  the	  user’s	  identity	  alone	  without	  hiding	  her	  location	  would	  not	  properly	  address	  these	  
privacy	  concerns.	  	  An	  attacker	  or	  a	  (potentially	  malicious)	  location-­‐based	  server	  can	  infer	  the	  identity	  of	  
the	  query	  source	  from	  its	  (subsequent)	  location	  information.	  For	  example,	  a	  user’s	  location	  information	  
can	  be	  tracked	  through	  several	  stationary	  connection	  points	  (e.g.,	  cell	  towers).	  	   After	  a	  while,	  the	  user	  

10 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 

	  
	  

	  

	  

	  

	  
	  
	  

	  

 

3.1	  Heterogeneity	  and	  Incompleteness	  

When	  humans	  consume	  information,	  a	  great	  deal	  of	  heterogeneity	  is	  comfortably	  tolerated.	  	  In	  
fact,	   the	   nuance	   and	   richness	   of	   natural	   language	   can	   provide	   valuable	   depth.	  	  	   However,	   machine	  
analysis	   algorithms	   expect	   homogeneous	   data,	   and	   cannot	   understand	   nuance.	  	   In	   consequence,	   data	  
must	  be	  carefully	  structured	  as	  a	  first	  step	  in	  (or	  prior	  to)	  data	  analysis.	  	  Consider,	  for	  example,	  a	  patient	  
who	  has	  multiple	  medical	  procedures	  at	  a	  hospital.	  	  We	  could	  create	  one	  record	  per	  medical	  procedure	  
or	   laboratory	   test,	   one	   record	   for	   the	   entire	   hospital	   stay,	   or	   one	   record	   for	   all	   lifetime	   hospital	  
interactions	  	  of	  	  this	  	  patient.	  	  	  	   With	  	  anything	  	  other	  	  than	  	  the	  	  first	  	  design,	  	  the	  	  number	  	  of	  	  medical	  
procedures	  and	  lab	  tests	  per	  record	  would	  be	  different	  for	  each	  patient.	  	  	  The	  three	  design	  choices	  listed	  
have	  successively	  less	  structure	  and,	  conversely,	  successively	  greater	  variety.	  	   Greater	  structure	  is	  likely	  
to	  be	  required	  by	  many	  (traditional)	  data	  analysis	  systems.	  	  However,	  the	  less	  structured	  design	  is	  likely	  
to	   be	   more	   effective	   for	   many	   purposes	   –	   for	   example	   questions	   relating	   to	   disease	   progression	   over	  
time	   will	   require	   an	   expensive	   join	   operation	   with	   the	   first	   two	   designs,	   but	   can	   be	   avoided	   with	   the	  
latter.	  	  	  	  However,	  computer	  systems	  work	  most	  efficiently	  if	  they	  can	  store	  multiple	  items	  that	  are	  all	  
identical	   in	   size	   and	   structure.	   	   Efficient	   representation,	   access,	   and	   analysis	   of	   semi-­‐structured	   data	  
require	  further	  work.	  

Consider	  an	  electronic	  health	  record	  database	  design	  that	  has	  fields	  for	  birth	  date,	  occupation,	  
and	  blood	  type	  for	  each	  patient.	  	   What	  do	  we	  do	  if	  one	  or	  more	  of	  these	  pieces	  of	  information	  is	  not	  
provided	   by	   a	   patient?	   	   	   Obviously,	   the	   health	   record	   is	   still	   placed	   in	   the	   database,	   but	   with	   the	  
corresponding	  attribute	  values	  being	  set	  to	  NULL.	  	  	  A	  data	  analysis	  that	  looks	  to	  classify	  patients	  by,	  say,	  
occupation,	   must	   take	   into	   account	   patients	   for	   which	   this	   information	   is	   not	   known.	   	   Worse,	   these	  
patients	  with	  unknown	  occupations	  can	  be	  ignored	  in	  the	  analysis	  only	  if	  we	  have	  reason	  to	  believe	  that	  
they	  	   are	  	   otherwise	  	   statistically	  	   similar	  	   to	  	   the	  	   patients	  	   with	  	   known	  	   occupation	  	   for	  	   the	  	   analysis	  
performed.	   	   For	   example,	   if	   unemployed	   patients	   are	   more	   likely	   to	   hide	   their	   employment	   status,	  
analysis	   results	   may	   be	   skewed	   in	   that	   it	   considers	   a	   more	   employed	   population	   mix	   than	   exists,	   and	  
hence	  potentially	  one	  that	  has	  differences	  in	  occupation-­‐related	  health-­‐profiles.	  

Even	  after	  data	  cleaning	  and	  error	  correction,	  some	  incompleteness	  and	  some	  errors	  in	  data	  are	  
likely	  to	  remain.	  	   This	  incompleteness	  and	  these	  errors	  must	  be	  managed	  during	  data	  analysis.	  	   Doing	  
this	   correctly	   is	   a	   challenge.	   	   Recent	   work	   on	   managing	   probabilistic	   data	   suggests	   one	   way	   to	   make	  
progress.	  

3.2	  Scale	  

Of	  course,	  the	  first	  thing	  anyone	  thinks	  of	  with	  Big	  Data	  is	  its	  size.	  	   After	  all,	  the	  word	  “big”	  is	  
there	  in	  the	  very	  name.	  	   Managing	  large	  and	  rapidly	  increasing	  volumes	  of	  data	  has	  been	  a	  challenging	  
issue	  for	  many	  decades.	  	  	  In	  the	  past,	  this	  challenge	  was	  mitigated	  by	  processors	  getting	  faster,	  following	  
Moore’s	  law,	  to	  provide	  us	  with	  the	  resources	  needed	  to	  cope	  with	  increasing	  volumes	  of	  data.	  	   But,	  

8 

 

	  

	  

	  

 

there	  is	  a	  fundamental	  shift	  underway	  now:	  data	  volume	  is	  scaling	  faster	  than	  compute	  resources,	  and	  
CPU	  speeds	  are	  static.	  

First,	  over	  the	  last	  five	  years	  the	  processor	  technology	  has	  made	  a	  dramatic	  shift	  -­‐	  rather	  than	  
processors	   doubling	   their	   clock	   cycle	   frequency	   every	   18-­‐24	   months,	   now,	   due	   to	   power	   constraints,	  
clock	  speeds	  have	  largely	  stalled	  and	  processors	  are	  being	  built	  with	  increasing	  numbers	  of	  cores.	  	  In	  the	  
past,	  large	  data	  processing	  systems	  had	  to	  worry	  about	  parallelism	  across	  nodes	  in	  a	  cluster;	  now,	  one	  
has	   to	   deal	   with	   parallelism	   within	   a	   single	   node.	  	  	  Unfortunately,	   parallel	   data	   processing	   techniques	  
that	   were	   applied	   in	   the	   past	   for	   processing	   data	   across	   nodes	   don’t	   directly	   apply	   for	   intra-­‐node	  
parallelism,	   since	   the	   architecture	   looks	   very	   different;	   for	   example,	   there	   are	   many	   more	   hardware	  
resources	   such	  as	   processor	   caches	   and	   processor	  memory	   channels	   that	   are	  shared	   across	  cores	   in	  a	  
single	   node.	  	  	  Furthermore,	   the	   move	   towards	   packing	   multiple	   sockets	   (each	   with	   10s	   of	   cores)	   adds	  
another	  level	  of	  complexity	  for	  intra-­‐node	  parallelism.	  	  Finally,	  with	  predictions	  of	  “dark	  silicon”,	  namely	  
that	  power	  consideration	  will	  likely	  in	  the	  future	  prohibit	  us	  from	  using	  all	  of	  the	  hardware	  in	  the	  system	  
continuously,	  data	  processing	  systems	  will	  likely	  have	  to	  actively	  manage	  the	  power	  consumption	  of	  the	  
processor.	  These	  unprecedented	  changes	  require	  us	  to	  rethink	  how	  we	  design,	  build	  and	  operate	  data	  
processing	  components.	  

The	   second	  dramatic	   shift	   that	   is	   underway	   is	   the	   move	   towards	   cloud	  computing,	   which	  now	  
aggregates	   multiple	   disparate	   workloads	   with	   varying	   performance	   goals	   (e.g.	   interactive	   services	  
demand	  that	  the	  data	  processing	  engine	  return	  back	  an	  answer	  within	  a	  fixed	  response	  time	  cap)	  into	  
very	  large	  clusters.	  This	  level	  of	  sharing	  of	  resources	  on	  expensive	  and	  large	  clusters	  requires	  new	  ways	  
of	   determining	   how	   to	   run	   and	   execute	   data	   processing	   jobs	   so	   that	   we	   can	   meet	   the	   goals	   of	   each	  
workload	  cost-­‐effectively,	  and	  to	  deal	  with	  system	  failures,	  which	  occur	  more	  frequently	  as	  we	  operate	  
on	   larger	   and	   larger	   clusters	   (that	   are	   required	   to	   deal	   with	   the	   rapid	   growth	   in	   data	   volumes).	  	  	  This	  
places	  	  a	  	  premium	  	  on	  	  declarative	  	  approaches	  	  to	  	  expressing	  	  programs,	  	  even	  	  those	  	  doing	  	  complex	  
machine	  learning	  tasks,	  since	  global	  optimization	  across	  multiple	  users’	  programs	  is	  necessary	  for	  good	  
overall	   performance.	   	   Reliance	   on	   user-­‐driven	   program	   optimizations	   is	   likely	   to	   lead	   to	   poor	   cluster	  
utilization,	   since	   users	   are	   unaware	   of	   other	   users’	   programs.	   	   	   	   System-­‐driven	   holistic	   optimization	  
requires	  	   programs	  	   to	  	   be	  	   sufficiently	  	   transparent,	  	   e.g.,	  	   as	  	   in	  	   relational	  	   database	  	   systems,	  	   where	  
declarative	  query	  languages	  are	  designed	  with	  this	  in	  mind.	  

A	   third	   dramatic	   shift	   that	   is	   underway	   is	   the	   transformative	   change	   of	   the	   traditional	   I/O	  
subsystem.	  For	  many	  decades,	  hard	  disk	  drives	  (HDDs)	  were	  used	  to	  store	  persistent	  data.	  HDDs	  had	  far	  
slower	  random	  IO	  performance	  than	  sequential	  IO	  performance,	  and	  data	  processing	  engines	  formatted	  
their	  data	  and	  designed	  their	  query	  processing	  methods	  to	  “work	  around”	  this	  limitation.	  But,	  HDDs	  are	  
increasingly	   being	   replaced	   by	   solid	   state	   drives	   today,	   and	   other	   technologies	   such	   as	   Phase	   Change	  
Memory	  are	  around	  the	  corner.	  These	  newer	  storage	  technologies	  do	  not	  have	  the	  same	  large	  spread	  in	  
performance	  between	  the	  sequential	  and	  random	  I/O	  performance,	  which	  requires	  a	  rethinking	  of	  how	  
we	   design	   storage	   subsystems	   for	   data	   processing	   systems.	   Implications	   of	   this	   changing	   storage	  
subsystem	   potentially	   touch	   every	   aspect	   of	   data	   processing,	   including	   query	   processing	   algorithms,	  
query	  scheduling,	  database	  design,	  concurrency	  control	  methods	  and	  recovery	  methods.	  

9 

 

	  

	  

	  

	  
	  
	  

	  

	  

 

3.3	  Timeliness	  

The	  flip	  side	  of	  size	  is	  speed.	  	   The	  larger	  the	  data	  set	  to	  be	  processed,	  the	  longer	  it	  will	  take	  to	  
analyze.	  	  The	  design	  of	  a	  system	  that	  effectively	  deals	  with	  size	  is	  likely	  also	  to	  result	  in	  a	  system	  that	  can	  
process	  a	  given	  size	  of	  data	  set	  faster.	  	  However,	  it	  is	  not	  just	  this	  speed	  that	  is	  usually	  meant	  when	  one	  
speaks	  of	  Velocity	  in	  the	  context	  of	  Big	  Data.	  	  Rather,	  there	  is	  an	  acquisition	  rate	  challenge	  as	  described	  
in	  Sec.	  2.1,	  and	  a	  timeliness	  challenge	  described	  next.	  

There	   are	   many	   situations	   in	   which	   the	   result	   of	   the	   analysis	   is	   required	   immediately.	   	   For	  
example,	   if	   a	   fraudulent	   credit	   card	   transaction	   is	   suspected,	   it	   should	   ideally	   be	   flagged	   before	   the	  
transaction	  is	  completed	  –	  potentially	  preventing	  the	  transaction	  from	  taking	  place	  at	  all.	  	   Obviously,	  a	  
full	   analysis	   of	   a	   user’s	   purchase	   history	   is	   not	   likely	   to	   be	   feasible	   in	   real-­‐time.	   	   Rather,	   we	   need	   to	  
develop	  partial	  results	  in	  advance	  so	  that	  a	  small	  amount	  of	  incremental	  computation	  with	  new	  data	  can	  
be	  used	  to	  arrive	  at	  a	  quick	  determination.	  

Given	  a	  large	  data	  set,	  it	  is	  often	  necessary	  to	  find	  elements	  in	  it	  that	  meet	  a	  specified	  criterion.	  
In	  the	  course	  of	  data	  analysis,	  this	  sort	  of	  search	  is	  likely	  to	  occur	  repeatedly.	  	   Scanning	  the	  entire	  data	  
set	  to	  find	  suitable	  elements	  is	  obviously	  impractical.	  	  Rather,	  index	  structures	  are	  created	  in	  advance	  to	  
permit	   finding	   qualifying	   elements	   quickly.	   	   The	   problem	   is	   that	   each	   index	   structure	   is	   designed	   to	  
support	  only	  some	  classes	  of	  criteria.	  	  With	  new	  analyses	  desired	  using	  Big	  Data,	  there	  are	  new	  types	  of	  
criteria	   specified,	   and	   a	   need	   to	   devise	   new	   index	   structures	   to	   support	   such	   criteria.	   	   For	   example,	  
consider	   a	   traffic	   management	   system	   with	   information	   regarding	   thousands	   of	   vehicles	   and	   local	   hot	  
spots	  on	  roadways.	  	   The	  system	  may	  need	  to	  predict	  potential	  congestion	  points	  along	  a	  route	  chosen	  
by	   a	   user,	   and	   suggest	   alternatives.	   	   Doing	   so	   requires	   evaluating	   multiple	   spatial	   proximity	   queries	  
working	   with	   the	   trajectories	   of	   moving	   objects.	   	   New	   index	   structures	   are	   required	   to	   support	   such	  
queries.	   Designing	   such	   structures	   becomes	   particularly	   challenging	   when	   the	   data	   volume	   is	   growing	  
rapidly	  and	  the	  queries	  have	  tight	  response	  time	  limits.	  

3.4	  Privacy	  

The	  privacy	  of	  data	  is	  another	  huge	  concern,	  and	  one	  that	  increases	  in	  the	  context	  of	  Big	  Data.	  
For	  electronic	  health	  records,	  there	  are	  strict	  laws	  governing	  what	  can	  and	  cannot	  be	  done.	  	   For	  other	  
data,	  regulations,	  particularly	  in	  the	  US,	  are	  less	  forceful.	  	   However,	  there	  is	  great	  public	  fear	  regarding	  
the	   inappropriate	   use	   of	   personal	   data,	   particularly	   through	   linking	   of	   data	   from	   multiple	   sources.	  
Managing	   privacy	   is	   effectively	   both	   a	   technical	   and	   a	   sociological	   problem,	  which	   must	   be	   addressed	  
jointly	  from	  both	  perspectives	  to	  realize	  the	  promise	  of	  big	  data.	  

Consider,	   for	   example,	   data	   gleaned	   from	   location-­‐based	   services.	   	   These	   new	   architectures	  
require	  a	  user	  to	  share	  his/her	  location	  with	  the	  service	  provider,	  resulting	  in	  obvious	  privacy	  concerns.	  
Note	  that	  hiding	  the	  user’s	  identity	  alone	  without	  hiding	  her	  location	  would	  not	  properly	  address	  these	  
privacy	  concerns.	  	  An	  attacker	  or	  a	  (potentially	  malicious)	  location-­‐based	  server	  can	  infer	  the	  identity	  of	  
the	  query	  source	  from	  its	  (subsequent)	  location	  information.	  For	  example,	  a	  user’s	  location	  information	  
can	  be	  tracked	  through	  several	  stationary	  connection	  points	  (e.g.,	  cell	  towers).	  	   After	  a	  while,	  the	  user	  

10 

 

	  

	  
	  
	  

	  

	  

	  

 

leaves	  “a	  trail	  of	  packet	  crumbs”	  which	  could	  be	  associated	  to	  a	  certain	  residence	  or	  office	  location	  and	  
thereby	   used	   to	   determine	   the	   user’s	   identity.	  	   Several	   other	   types	   of	   surprisingly	   private	   information	  
such	  	  as	  	  health	  	  issues	  	  (e.g.,	  	  presence	  	  in	  	  a	  	  cancer	  	  treatment	  	  center)	  	  or	  	  religious	  	  preferences	  	  (e.g.,	  
presence	   in	   a	   church)	   can	   also	   be	   revealed	   by	   just	   observing	   anonymous	   users’	   movement	   and	   usage	  
pattern	  over	  time.	  In	  general,	  Barabási	  et	  al.	  showed	  that	  there	  is	  a	  close	  correlation	  between	  people’s	  
identities	   and	   their	   movement	   patterns	   [Gon2008].	   	   Note	   that	   hiding	   a	   user	   location	   is	   much	   more	  
challenging	  than	  hiding	  his/her	  identity.	  	  This	  is	  because	  with	  location-­‐based	  services,	  the	  location	  of	  the	  
user	   is	   needed	   for	   a	   successful	   data	   access	   or	   data	   collection,	   while	   the	   identity	   of	   the	   user	   is	   not	  
necessary.	  

There	   are	   many	   additional	   challenging	   research	   problems.	  	  	  For	   example,	   we	   do	   not	   know	   yet	  
how	  to	  share	  private	  data	  while	  limiting	  disclosure	  and	  ensuring	  sufficient	  data	  utility	  in	  the	  shared	  data.	  
The	   existing	   paradigm	   of	   differential	   privacy	   is	   a	   very	   important	   step	   in	   the	   right	   direction,	   but	   it	  
unfortunately	   reduces	   information	   content	   too	   far	   in	   order	   to	   be	   useful	   in	   most	   practical	   cases.	   	   In	  
addition,	  real	  data	  is	  not	  static	  but	  gets	  larger	  and	  changes	  over	  time;	  none	  of	  the	  prevailing	  techniques	  
results	  in	  any	  useful	  content	  being	  released	  in	  this	  scenario.	  	   Yet	  another	  very	  important	  direction	  is	  to	  
rethink	  security	  for	  information	  sharing	  in	  Big	  Data	  use	  cases.	  	  Many	  online	  services	  today	  require	  us	  to	  
share	  private	  information	  (think	  of	  Facebook	  applications),	  but	  beyond	  record-­‐level	  access	  control	  we	  do	  
not	  understand	  what	  it	  means	  to	  share	  data,	  how	  the	  shared	  data	  can	  be	  linked,	  and	  how	  to	  give	  users	  
fine-­‐grained	  control	  over	  this	  sharing.	  

3.5	  Human	  Collaboration	  

In	   spite	   of	   the	   tremendous	   advances	   made	   in	   computational	   analysis,	   there	   remain	   many	  
patterns	   that	   humans	   can	   easily	   detect	   but	   computer	   algorithms	   have	   a	   hard	   time	   finding.	   	   Indeed,	  
CAPTCHAs	  exploit	  precisely	   this	  fact	  to	   tell	  human	  web	  users	  apart	  from	   computer	  programs.	  	   Ideally,	  
analytics	  for	  Big	  Data	  will	  not	  be	  all	  computational	  –	  rather	  it	  will	  be	  designed	  explicitly	  to	  have	  a	  human	  
in	  the	  loop.	  	   The	  new	  sub-­‐field	  of	  visual	  analytics	  is	  attempting	  to	  do	  this,	  at	  least	  with	  respect	  to	  the	  
modeling	  and	  analysis	  phase	  in	  the	  pipeline.	  	   There	  is	  similar	  value	  to	  human	  input	  at	  all	  stages	  of	  the	  
analysis	  pipeline.	  

In	   today’s	   complex	   world,	   it	   often	   takes	   multiple	   experts	   from	   different	   domains	   to	   really	  
understand	   what	   is	   going	   on.	   	   A	   Big	   Data	   analysis	   system	   must	   support	   input	   from	   multiple	   human	  
experts,	  and	  shared	  exploration	  of	  results.	  	  These	  multiple	  experts	  may	  be	  separated	  in	  space	  and	  time	  
when	   it	   is	   too	   expensive	   to	   assemble	   an	   entire	   team	   together	   in	   one	   room.	  	  	  The	   data	   system	   has	   to	  
accept	  this	  distributed	  expert	  input,	  and	  support	  their	  collaboration.	  

A	   popular	   new	   method	   of	   harnessing	   human	   ingenuity	   to	   solve	   problems	   is	   through	   crowd-­‐	  
sourcing.	  	  	  Wikipedia,	   the	   online	   encyclopedia,	   is	   perhaps	   the	   best	   known	   example	   of	   crowd-­‐sourced	  
data.	  	   We	  are	  relying	  upon	  information	  provided	  by	  unvetted	  strangers.	  	   Most	  often,	  what	  they	   say	   is	  
correct.	  	  	  However,	   we	   should	   expect	   there	   to	   be	   individuals	   who	   have	   other	   motives	   and	   abilities	   –	  
some	  may	  have	  a	  reason	  to	  provide	  false	  information	  in	  an	  intentional	  attempt	  to	  mislead.	  	  While	  most	  

11 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 

	  
	  

	  

	  

	  

	  
	  
	  

	  

 

3.1	  Heterogeneity	  and	  Incompleteness	  

When	  humans	  consume	  information,	  a	  great	  deal	  of	  heterogeneity	  is	  comfortably	  tolerated.	  	  In	  
fact,	   the	   nuance	   and	   richness	   of	   natural	   language	   can	   provide	   valuable	   depth.	  	  	   However,	   machine	  
analysis	   algorithms	   expect	   homogeneous	   data,	   and	   cannot	   understand	   nuance.	  	   In	   consequence,	   data	  
must	  be	  carefully	  structured	  as	  a	  first	  step	  in	  (or	  prior	  to)	  data	  analysis.	  	  Consider,	  for	  example,	  a	  patient	  
who	  has	  multiple	  medical	  procedures	  at	  a	  hospital.	  	  We	  could	  create	  one	  record	  per	  medical	  procedure	  
or	   laboratory	   test,	   one	   record	   for	   the	   entire	   hospital	   stay,	   or	   one	   record	   for	   all	   lifetime	   hospital	  
interactions	  	  of	  	  this	  	  patient.	  	  	  	   With	  	  anything	  	  other	  	  than	  	  the	  	  first	  	  design,	  	  the	  	  number	  	  of	  	  medical	  
procedures	  and	  lab	  tests	  per	  record	  would	  be	  different	  for	  each	  patient.	  	  	  The	  three	  design	  choices	  listed	  
have	  successively	  less	  structure	  and,	  conversely,	  successively	  greater	  variety.	  	   Greater	  structure	  is	  likely	  
to	  be	  required	  by	  many	  (traditional)	  data	  analysis	  systems.	  	  However,	  the	  less	  structured	  design	  is	  likely	  
to	   be	   more	   effective	   for	   many	   purposes	   –	   for	   example	   questions	   relating	   to	   disease	   progression	   over	  
time	   will	   require	   an	   expensive	   join	   operation	   with	   the	   first	   two	   designs,	   but	   can	   be	   avoided	   with	   the	  
latter.	  	  	  	  However,	  computer	  systems	  work	  most	  efficiently	  if	  they	  can	  store	  multiple	  items	  that	  are	  all	  
identical	   in	   size	   and	   structure.	   	   Efficient	   representation,	   access,	   and	   analysis	   of	   semi-­‐structured	   data	  
require	  further	  work.	  

Consider	  an	  electronic	  health	  record	  database	  design	  that	  has	  fields	  for	  birth	  date,	  occupation,	  
and	  blood	  type	  for	  each	  patient.	  	   What	  do	  we	  do	  if	  one	  or	  more	  of	  these	  pieces	  of	  information	  is	  not	  
provided	   by	   a	   patient?	   	   	   Obviously,	   the	   health	   record	   is	   still	   placed	   in	   the	   database,	   but	   with	   the	  
corresponding	  attribute	  values	  being	  set	  to	  NULL.	  	  	  A	  data	  analysis	  that	  looks	  to	  classify	  patients	  by,	  say,	  
occupation,	   must	   take	   into	   account	   patients	   for	   which	   this	   information	   is	   not	   known.	   	   Worse,	   these	  
patients	  with	  unknown	  occupations	  can	  be	  ignored	  in	  the	  analysis	  only	  if	  we	  have	  reason	  to	  believe	  that	  
they	  	   are	  	   otherwise	  	   statistically	  	   similar	  	   to	  	   the	  	   patients	  	   with	  	   known	  	   occupation	  	   for	  	   the	  	   analysis	  
performed.	   	   For	   example,	   if	   unemployed	   patients	   are	   more	   likely	   to	   hide	   their	   employment	   status,	  
analysis	   results	   may	   be	   skewed	   in	   that	   it	   considers	   a	   more	   employed	   population	   mix	   than	   exists,	   and	  
hence	  potentially	  one	  that	  has	  differences	  in	  occupation-­‐related	  health-­‐profiles.	  

Even	  after	  data	  cleaning	  and	  error	  correction,	  some	  incompleteness	  and	  some	  errors	  in	  data	  are	  
likely	  to	  remain.	  	   This	  incompleteness	  and	  these	  errors	  must	  be	  managed	  during	  data	  analysis.	  	   Doing	  
this	   correctly	   is	   a	   challenge.	   	   Recent	   work	   on	   managing	   probabilistic	   data	   suggests	   one	   way	   to	   make	  
progress.	  

3.2	  Scale	  

Of	  course,	  the	  first	  thing	  anyone	  thinks	  of	  with	  Big	  Data	  is	  its	  size.	  	   After	  all,	  the	  word	  “big”	  is	  
there	  in	  the	  very	  name.	  	   Managing	  large	  and	  rapidly	  increasing	  volumes	  of	  data	  has	  been	  a	  challenging	  
issue	  for	  many	  decades.	  	  	  In	  the	  past,	  this	  challenge	  was	  mitigated	  by	  processors	  getting	  faster,	  following	  
Moore’s	  law,	  to	  provide	  us	  with	  the	  resources	  needed	  to	  cope	  with	  increasing	  volumes	  of	  data.	  	   But,	  

8 

 

	  

	  

	  

 

there	  is	  a	  fundamental	  shift	  underway	  now:	  data	  volume	  is	  scaling	  faster	  than	  compute	  resources,	  and	  
CPU	  speeds	  are	  static.	  

First,	  over	  the	  last	  five	  years	  the	  processor	  technology	  has	  made	  a	  dramatic	  shift	  -­‐	  rather	  than	  
processors	   doubling	   their	   clock	   cycle	   frequency	   every	   18-­‐24	   months,	   now,	   due	   to	   power	   constraints,	  
clock	  speeds	  have	  largely	  stalled	  and	  processors	  are	  being	  built	  with	  increasing	  numbers	  of	  cores.	  	  In	  the	  
past,	  large	  data	  processing	  systems	  had	  to	  worry	  about	  parallelism	  across	  nodes	  in	  a	  cluster;	  now,	  one	  
has	   to	   deal	   with	   parallelism	   within	   a	   single	   node.	  	  	  Unfortunately,	   parallel	   data	   processing	   techniques	  
that	   were	   applied	   in	   the	   past	   for	   processing	   data	   across	   nodes	   don’t	   directly	   apply	   for	   intra-­‐node	  
parallelism,	   since	   the	   architecture	   looks	   very	   different;	   for	   example,	   there	   are	   many	   more	   hardware	  
resources	   such	  as	   processor	   caches	   and	   processor	  memory	   channels	   that	   are	  shared	   across	  cores	   in	  a	  
single	   node.	  	  	  Furthermore,	   the	   move	   towards	   packing	   multiple	   sockets	   (each	   with	   10s	   of	   cores)	   adds	  
another	  level	  of	  complexity	  for	  intra-­‐node	  parallelism.	  	  Finally,	  with	  predictions	  of	  “dark	  silicon”,	  namely	  
that	  power	  consideration	  will	  likely	  in	  the	  future	  prohibit	  us	  from	  using	  all	  of	  the	  hardware	  in	  the	  system	  
continuously,	  data	  processing	  systems	  will	  likely	  have	  to	  actively	  manage	  the	  power	  consumption	  of	  the	  
processor.	  These	  unprecedented	  changes	  require	  us	  to	  rethink	  how	  we	  design,	  build	  and	  operate	  data	  
processing	  components.	  

The	   second	  dramatic	   shift	   that	   is	   underway	   is	   the	   move	   towards	   cloud	  computing,	   which	  now	  
aggregates	   multiple	   disparate	   workloads	   with	   varying	   performance	   goals	   (e.g.	   interactive	   services	  
demand	  that	  the	  data	  processing	  engine	  return	  back	  an	  answer	  within	  a	  fixed	  response	  time	  cap)	  into	  
very	  large	  clusters.	  This	  level	  of	  sharing	  of	  resources	  on	  expensive	  and	  large	  clusters	  requires	  new	  ways	  
of	   determining	   how	   to	   run	   and	   execute	   data	   processing	   jobs	   so	   that	   we	   can	   meet	   the	   goals	   of	   each	  
workload	  cost-­‐effectively,	  and	  to	  deal	  with	  system	  failures,	  which	  occur	  more	  frequently	  as	  we	  operate	  
on	   larger	   and	   larger	   clusters	   (that	   are	   required	   to	   deal	   with	   the	   rapid	   growth	   in	   data	   volumes).	  	  	  This	  
places	  	  a	  	  premium	  	  on	  	  declarative	  	  approaches	  	  to	  	  expressing	  	  programs,	  	  even	  	  those	  	  doing	  	  complex	  
machine	  learning	  tasks,	  since	  global	  optimization	  across	  multiple	  users’	  programs	  is	  necessary	  for	  good	  
overall	   performance.	   	   Reliance	   on	   user-­‐driven	   program	   optimizations	   is	   likely	   to	   lead	   to	   poor	   cluster	  
utilization,	   since	   users	   are	   unaware	   of	   other	   users’	   programs.	   	   	   	   System-­‐driven	   holistic	   optimization	  
requires	  	   programs	  	   to	  	   be	  	   sufficiently	  	   transparent,	  	   e.g.,	  	   as	  	   in	  	   relational	  	   database	  	   systems,	  	   where	  
declarative	  query	  languages	  are	  designed	  with	  this	  in	  mind.	  

A	   third	   dramatic	   shift	   that	   is	   underway	   is	   the	   transformative	   change	   of	   the	   traditional	   I/O	  
subsystem.	  For	  many	  decades,	  hard	  disk	  drives	  (HDDs)	  were	  used	  to	  store	  persistent	  data.	  HDDs	  had	  far	  
slower	  random	  IO	  performance	  than	  sequential	  IO	  performance,	  and	  data	  processing	  engines	  formatted	  
their	  data	  and	  designed	  their	  query	  processing	  methods	  to	  “work	  around”	  this	  limitation.	  But,	  HDDs	  are	  
increasingly	   being	   replaced	   by	   solid	   state	   drives	   today,	   and	   other	   technologies	   such	   as	   Phase	   Change	  
Memory	  are	  around	  the	  corner.	  These	  newer	  storage	  technologies	  do	  not	  have	  the	  same	  large	  spread	  in	  
performance	  between	  the	  sequential	  and	  random	  I/O	  performance,	  which	  requires	  a	  rethinking	  of	  how	  
we	   design	   storage	   subsystems	   for	   data	   processing	   systems.	   Implications	   of	   this	   changing	   storage	  
subsystem	   potentially	   touch	   every	   aspect	   of	   data	   processing,	   including	   query	   processing	   algorithms,	  
query	  scheduling,	  database	  design,	  concurrency	  control	  methods	  and	  recovery	  methods.	  

9 

 

	  

	  

	  

	  
	  
	  

	  

	  

 

3.3	  Timeliness	  

The	  flip	  side	  of	  size	  is	  speed.	  	   The	  larger	  the	  data	  set	  to	  be	  processed,	  the	  longer	  it	  will	  take	  to	  
analyze.	  	  The	  design	  of	  a	  system	  that	  effectively	  deals	  with	  size	  is	  likely	  also	  to	  result	  in	  a	  system	  that	  can	  
process	  a	  given	  size	  of	  data	  set	  faster.	  	  However,	  it	  is	  not	  just	  this	  speed	  that	  is	  usually	  meant	  when	  one	  
speaks	  of	  Velocity	  in	  the	  context	  of	  Big	  Data.	  	  Rather,	  there	  is	  an	  acquisition	  rate	  challenge	  as	  described	  
in	  Sec.	  2.1,	  and	  a	  timeliness	  challenge	  described	  next.	  

There	   are	   many	   situations	   in	   which	   the	   result	   of	   the	   analysis	   is	   required	   immediately.	   	   For	  
example,	   if	   a	   fraudulent	   credit	   card	   transaction	   is	   suspected,	   it	   should	   ideally	   be	   flagged	   before	   the	  
transaction	  is	  completed	  –	  potentially	  preventing	  the	  transaction	  from	  taking	  place	  at	  all.	  	   Obviously,	  a	  
full	   analysis	   of	   a	   user’s	   purchase	   history	   is	   not	   likely	   to	   be	   feasible	   in	   real-­‐time.	   	   Rather,	   we	   need	   to	  
develop	  partial	  results	  in	  advance	  so	  that	  a	  small	  amount	  of	  incremental	  computation	  with	  new	  data	  can	  
be	  used	  to	  arrive	  at	  a	  quick	  determination.	  

Given	  a	  large	  data	  set,	  it	  is	  often	  necessary	  to	  find	  elements	  in	  it	  that	  meet	  a	  specified	  criterion.	  
In	  the	  course	  of	  data	  analysis,	  this	  sort	  of	  search	  is	  likely	  to	  occur	  repeatedly.	  	   Scanning	  the	  entire	  data	  
set	  to	  find	  suitable	  elements	  is	  obviously	  impractical.	  	  Rather,	  index	  structures	  are	  created	  in	  advance	  to	  
permit	   finding	   qualifying	   elements	   quickly.	   	   The	   problem	   is	   that	   each	   index	   structure	   is	   designed	   to	  
support	  only	  some	  classes	  of	  criteria.	  	  With	  new	  analyses	  desired	  using	  Big	  Data,	  there	  are	  new	  types	  of	  
criteria	   specified,	   and	   a	   need	   to	   devise	   new	   index	   structures	   to	   support	   such	   criteria.	   	   For	   example,	  
consider	   a	   traffic	   management	   system	   with	   information	   regarding	   thousands	   of	   vehicles	   and	   local	   hot	  
spots	  on	  roadways.	  	   The	  system	  may	  need	  to	  predict	  potential	  congestion	  points	  along	  a	  route	  chosen	  
by	   a	   user,	   and	   suggest	   alternatives.	   	   Doing	   so	   requires	   evaluating	   multiple	   spatial	   proximity	   queries	  
working	   with	   the	   trajectories	   of	   moving	   objects.	   	   New	   index	   structures	   are	   required	   to	   support	   such	  
queries.	   Designing	   such	   structures	   becomes	   particularly	   challenging	   when	   the	   data	   volume	   is	   growing	  
rapidly	  and	  the	  queries	  have	  tight	  response	  time	  limits.	  

3.4	  Privacy	  

The	  privacy	  of	  data	  is	  another	  huge	  concern,	  and	  one	  that	  increases	  in	  the	  context	  of	  Big	  Data.	  
For	  electronic	  health	  records,	  there	  are	  strict	  laws	  governing	  what	  can	  and	  cannot	  be	  done.	  	   For	  other	  
data,	  regulations,	  particularly	  in	  the	  US,	  are	  less	  forceful.	  	   However,	  there	  is	  great	  public	  fear	  regarding	  
the	   inappropriate	   use	   of	   personal	   data,	   particularly	   through	   linking	   of	   data	   from	   multiple	   sources.	  
Managing	   privacy	   is	   effectively	   both	   a	   technical	   and	   a	   sociological	   problem,	  which	   must	   be	   addressed	  
jointly	  from	  both	  perspectives	  to	  realize	  the	  promise	  of	  big	  data.	  

Consider,	   for	   example,	   data	   gleaned	   from	   location-­‐based	   services.	   	   These	   new	   architectures	  
require	  a	  user	  to	  share	  his/her	  location	  with	  the	  service	  provider,	  resulting	  in	  obvious	  privacy	  concerns.	  
Note	  that	  hiding	  the	  user’s	  identity	  alone	  without	  hiding	  her	  location	  would	  not	  properly	  address	  these	  
privacy	  concerns.	  	  An	  attacker	  or	  a	  (potentially	  malicious)	  location-­‐based	  server	  can	  infer	  the	  identity	  of	  
the	  query	  source	  from	  its	  (subsequent)	  location	  information.	  For	  example,	  a	  user’s	  location	  information	  
can	  be	  tracked	  through	  several	  stationary	  connection	  points	  (e.g.,	  cell	  towers).	  	   After	  a	  while,	  the	  user	  

10 

 

	  

	  
	  
	  

	  

	  

	  

 

leaves	  “a	  trail	  of	  packet	  crumbs”	  which	  could	  be	  associated	  to	  a	  certain	  residence	  or	  office	  location	  and	  
thereby	   used	   to	   determine	   the	   user’s	   identity.	  	   Several	   other	   types	   of	   surprisingly	   private	   information	  
such	  	  as	  	  health	  	  issues	  	  (e.g.,	  	  presence	  	  in	  	  a	  	  cancer	  	  treatment	  	  center)	  	  or	  	  religious	  	  preferences	  	  (e.g.,	  
presence	   in	   a	   church)	   can	   also	   be	   revealed	   by	   just	   observing	   anonymous	   users’	   movement	   and	   usage	  
pattern	  over	  time.	  In	  general,	  Barabási	  et	  al.	  showed	  that	  there	  is	  a	  close	  correlation	  between	  people’s	  
identities	   and	   their	   movement	   patterns	   [Gon2008].	   	   Note	   that	   hiding	   a	   user	   location	   is	   much	   more	  
challenging	  than	  hiding	  his/her	  identity.	  	  This	  is	  because	  with	  location-­‐based	  services,	  the	  location	  of	  the	  
user	   is	   needed	   for	   a	   successful	   data	   access	   or	   data	   collection,	   while	   the	   identity	   of	   the	   user	   is	   not	  
necessary.	  

There	   are	   many	   additional	   challenging	   research	   problems.	  	  	  For	   example,	   we	   do	   not	   know	   yet	  
how	  to	  share	  private	  data	  while	  limiting	  disclosure	  and	  ensuring	  sufficient	  data	  utility	  in	  the	  shared	  data.	  
The	   existing	   paradigm	   of	   differential	   privacy	   is	   a	   very	   important	   step	   in	   the	   right	   direction,	   but	   it	  
unfortunately	   reduces	   information	   content	   too	   far	   in	   order	   to	   be	   useful	   in	   most	   practical	   cases.	   	   In	  
addition,	  real	  data	  is	  not	  static	  but	  gets	  larger	  and	  changes	  over	  time;	  none	  of	  the	  prevailing	  techniques	  
results	  in	  any	  useful	  content	  being	  released	  in	  this	  scenario.	  	   Yet	  another	  very	  important	  direction	  is	  to	  
rethink	  security	  for	  information	  sharing	  in	  Big	  Data	  use	  cases.	  	  Many	  online	  services	  today	  require	  us	  to	  
share	  private	  information	  (think	  of	  Facebook	  applications),	  but	  beyond	  record-­‐level	  access	  control	  we	  do	  
not	  understand	  what	  it	  means	  to	  share	  data,	  how	  the	  shared	  data	  can	  be	  linked,	  and	  how	  to	  give	  users	  
fine-­‐grained	  control	  over	  this	  sharing.	  

3.5	  Human	  Collaboration	  

In	   spite	   of	   the	   tremendous	   advances	   made	   in	   computational	   analysis,	   there	   remain	   many	  
patterns	   that	   humans	   can	   easily	   detect	   but	   computer	   algorithms	   have	   a	   hard	   time	   finding.	   	   Indeed,	  
CAPTCHAs	  exploit	  precisely	   this	  fact	  to	   tell	  human	  web	  users	  apart	  from	   computer	  programs.	  	   Ideally,	  
analytics	  for	  Big	  Data	  will	  not	  be	  all	  computational	  –	  rather	  it	  will	  be	  designed	  explicitly	  to	  have	  a	  human	  
in	  the	  loop.	  	   The	  new	  sub-­‐field	  of	  visual	  analytics	  is	  attempting	  to	  do	  this,	  at	  least	  with	  respect	  to	  the	  
modeling	  and	  analysis	  phase	  in	  the	  pipeline.	  	   There	  is	  similar	  value	  to	  human	  input	  at	  all	  stages	  of	  the	  
analysis	  pipeline.	  

In	   today’s	   complex	   world,	   it	   often	   takes	   multiple	   experts	   from	   different	   domains	   to	   really	  
understand	   what	   is	   going	   on.	   	   A	   Big	   Data	   analysis	   system	   must	   support	   input	   from	   multiple	   human	  
experts,	  and	  shared	  exploration	  of	  results.	  	  These	  multiple	  experts	  may	  be	  separated	  in	  space	  and	  time	  
when	   it	   is	   too	   expensive	   to	   assemble	   an	   entire	   team	   together	   in	   one	   room.	  	  	  The	   data	   system	   has	   to	  
accept	  this	  distributed	  expert	  input,	  and	  support	  their	  collaboration.	  

A	   popular	   new	   method	   of	   harnessing	   human	   ingenuity	   to	   solve	   problems	   is	   through	   crowd-­‐	  
sourcing.	  	  	  Wikipedia,	   the	   online	   encyclopedia,	   is	   perhaps	   the	   best	   known	   example	   of	   crowd-­‐sourced	  
data.	  	   We	  are	  relying	  upon	  information	  provided	  by	  unvetted	  strangers.	  	   Most	  often,	  what	  they	   say	   is	  
correct.	  	  	  However,	   we	   should	   expect	   there	   to	   be	   individuals	   who	   have	   other	   motives	   and	   abilities	   –	  
some	  may	  have	  a	  reason	  to	  provide	  false	  information	  in	  an	  intentional	  attempt	  to	  mislead.	  	  While	  most	  

11 

 

	  
	  
	  

	  

	  

	  

 

such	   errors	   will	   be	   detected	   and	   corrected	   by	   others	   in	   the	   crowd,	   we	   need	   technologies	   to	   facilitate	  
this.	   	   	   We	   also	   need	   a	   framework	   to	   use	   in	   analysis	   of	   such	   crowd-­‐sourced	   data	   with	   conflicting	  
statements.	  	  As	  humans,	  we	  can	  look	  at	  reviews	  of	  a	  restaurant,	  some	  of	  which	  are	  positive	  and	  others	  
critical,	  and	  come	  up	  with	  a	  summary	  assessment	  based	  on	  which	  we	  can	  decide	  whether	  to	  try	  eating	  
there.	  	  We	  need	  computers	  to	  be	  able	  to	  do	  the	  equivalent.	  	  The	  issues	  of	  uncertainty	  and	  error	  become	  
even	  more	  pronounced	  in	  a	  specific	  type	  of	  crowd-­‐sourcing,	  termed	  participatory-­‐sensing.	  	  In	  this	  case,	  
every	   person	   with	   a	   mobile	   phone	   can	   act	   as	   a	   multi-­‐modal	   sensor	   collecting	   various	   types	   of	   data	  
instantaneously	   (e.g.,	   picture,	   video,	   audio,	   location,	   time,	   speed,	   direction,	   acceleration).	   	   The	   extra	  
challenge	  here	  is	  the	  inherent	  uncertainty	  of	  the	  data	  collection	  devices.	  	  The	  fact	  that	  collected	  data	  are	  
probably	  spatially	  and	  temporally	  correlated	  can	  be	  exploited	  to	  better	  assess	  their	  correctness.	  	  When	  
crowd-­‐sourced	  data	  is	  obtained	  for	  hire,	  such	  as	  with	  “Mechanical	  Turks,”	  much	  of	  the	  data	  created	  may	  
be	   with	   a	   primary	   objective	   of	   getting	   it	   done	   quickly	   rather	   than	   correctly.	  	   This	   is	   yet	   another	   error	  
model,	  which	  must	  be	  planned	  for	  explicitly	  when	  it	  applies.	  

4.	  System	  Architecture	  

Companies	  today	  already	  use,	  and	  appreciate	  the	  value	  of,	  business	  intelligence.	  	  	  Business	  data	  
is	  analyzed	  for	  many	  purposes:	  a	  company	  may	  perform	  system	  log	  analytics	  and	  social	  media	  analytics	  
for	   risk	   assessment,	   customer	   retention,	   brand	   management,	   and	   so	   on.	  	  	  Typically,	   such	   varied	   tasks	  
have	   been	   handled	   by	   separate	   systems,	   even	   if	   each	   system	   includes	   common	   steps	   of	   information	  
extraction,	   data	   cleaning,	   relational-­‐like	   processing	   (joins,	   group-­‐by,	   aggregation),	   statistical	   and	  
predictive	  modeling,	  and	  appropriate	  exploration	  and	  visualization	  tools	  as	  shown	  in	  Fig.	  1.	  

With	  Big	  Data,	  the	  use	  of	  separate	  systems	  in	  this	  fashion	  becomes	  prohibitively	  expensive	  given	  
the	  large	  size	  of	  the	  data	  sets.	  	  	   The	  expense	  is	  due	  not	  only	  to	  the	  cost	  of	  the	  systems	  themselves,	  but	  
also	  the	  time	  to	  load	  the	  data	  into	  multiple	  systems.	  	  In	  consequence,	  Big	  Data	  has	  made	  it	  necessary	  to	  
run	   heterogeneous	   workloads	   on	   a	   single	   infrastructure	   that	   is	   sufficiently	   flexible	   to	   handle	   all	   these	  
workloads.	  	  	  	  The	   challenge	   here	   is	  not	   to	   build	   a	   system	   that	   is	   ideally	   suited	  for	   all	   processing	  tasks.	  
Instead,	  the	  need	  is	  for	  the	  underlying	  system	  architecture	  to	  be	  flexible	  enough	  that	  the	  components	  
built	  on	  top	  of	  it	  for	  expressing	  the	  various	  kinds	  of	  processing	  tasks	  can	  tune	  it	  to	  efficiently	  run	  these	  
different	  workloads.	  	  The	  effects	  of	  scale	  on	  the	  physical	  architecture	  were	  considered	  in	  Sec	  3.2.	  	  In	  this	  
section,	  we	  focus	  on	  the	  programmability	  requirements.	  

If	  users	  are	  to	  compose	  and	  build	  complex	  analytical	  pipelines	  over	  Big	  Data,	  it	  is	  essential	  that	  
they	  have	  appropriate	  high-­‐level	  primitives	  to	  specify	  their	  needs	  in	  such	  flexible	  systems.	  	  	  	  The	  Map-­‐	  
Reduce	  framework	  has	  been	  tremendously	  valuable,	  but	  is	  only	  a	  first	  step.	  	  Even	  declarative	  languages	  
that	   exploit	   it,	   such	   as	   Pig	   Latin,	   are	   at	   a	   rather	   low	   level	   when	   it	   comes	   to	   complex	   analysis	   tasks.	  
Similar	   declarative	   specifications	   are	   required	   at	   higher	   levels	   to	   meet	   the	   programmability	   and	  
composition	   needs	   of	   these	   analysis	   pipelines.	  	  	  	  	  Besides	   the	   basic	   technical	   need,	   there	   is	   a	   strong	  
business	  imperative	  as	  well.	  	  Businesses	  typically	  will	  outsource	  Big	  Data	  processing,	  or	  many	  aspects	  of	  
it.	  	  	  Declarative	   specifications	   are	   required	   to	   enable	   technically	   meaningful	   service	   level	   agreements,	  

12 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 

	  
	  

	  

	  

	  

	  
	  
	  

	  

 

3.1	  Heterogeneity	  and	  Incompleteness	  

When	  humans	  consume	  information,	  a	  great	  deal	  of	  heterogeneity	  is	  comfortably	  tolerated.	  	  In	  
fact,	   the	   nuance	   and	   richness	   of	   natural	   language	   can	   provide	   valuable	   depth.	  	  	   However,	   machine	  
analysis	   algorithms	   expect	   homogeneous	   data,	   and	   cannot	   understand	   nuance.	  	   In	   consequence,	   data	  
must	  be	  carefully	  structured	  as	  a	  first	  step	  in	  (or	  prior	  to)	  data	  analysis.	  	  Consider,	  for	  example,	  a	  patient	  
who	  has	  multiple	  medical	  procedures	  at	  a	  hospital.	  	  We	  could	  create	  one	  record	  per	  medical	  procedure	  
or	   laboratory	   test,	   one	   record	   for	   the	   entire	   hospital	   stay,	   or	   one	   record	   for	   all	   lifetime	   hospital	  
interactions	  	  of	  	  this	  	  patient.	  	  	  	   With	  	  anything	  	  other	  	  than	  	  the	  	  first	  	  design,	  	  the	  	  number	  	  of	  	  medical	  
procedures	  and	  lab	  tests	  per	  record	  would	  be	  different	  for	  each	  patient.	  	  	  The	  three	  design	  choices	  listed	  
have	  successively	  less	  structure	  and,	  conversely,	  successively	  greater	  variety.	  	   Greater	  structure	  is	  likely	  
to	  be	  required	  by	  many	  (traditional)	  data	  analysis	  systems.	  	  However,	  the	  less	  structured	  design	  is	  likely	  
to	   be	   more	   effective	   for	   many	   purposes	   –	   for	   example	   questions	   relating	   to	   disease	   progression	   over	  
time	   will	   require	   an	   expensive	   join	   operation	   with	   the	   first	   two	   designs,	   but	   can	   be	   avoided	   with	   the	  
latter.	  	  	  	  However,	  computer	  systems	  work	  most	  efficiently	  if	  they	  can	  store	  multiple	  items	  that	  are	  all	  
identical	   in	   size	   and	   structure.	   	   Efficient	   representation,	   access,	   and	   analysis	   of	   semi-­‐structured	   data	  
require	  further	  work.	  

Consider	  an	  electronic	  health	  record	  database	  design	  that	  has	  fields	  for	  birth	  date,	  occupation,	  
and	  blood	  type	  for	  each	  patient.	  	   What	  do	  we	  do	  if	  one	  or	  more	  of	  these	  pieces	  of	  information	  is	  not	  
provided	   by	   a	   patient?	   	   	   Obviously,	   the	   health	   record	   is	   still	   placed	   in	   the	   database,	   but	   with	   the	  
corresponding	  attribute	  values	  being	  set	  to	  NULL.	  	  	  A	  data	  analysis	  that	  looks	  to	  classify	  patients	  by,	  say,	  
occupation,	   must	   take	   into	   account	   patients	   for	   which	   this	   information	   is	   not	   known.	   	   Worse,	   these	  
patients	  with	  unknown	  occupations	  can	  be	  ignored	  in	  the	  analysis	  only	  if	  we	  have	  reason	  to	  believe	  that	  
they	  	   are	  	   otherwise	  	   statistically	  	   similar	  	   to	  	   the	  	   patients	  	   with	  	   known	  	   occupation	  	   for	  	   the	  	   analysis	  
performed.	   	   For	   example,	   if	   unemployed	   patients	   are	   more	   likely	   to	   hide	   their	   employment	   status,	  
analysis	   results	   may	   be	   skewed	   in	   that	   it	   considers	   a	   more	   employed	   population	   mix	   than	   exists,	   and	  
hence	  potentially	  one	  that	  has	  differences	  in	  occupation-­‐related	  health-­‐profiles.	  

Even	  after	  data	  cleaning	  and	  error	  correction,	  some	  incompleteness	  and	  some	  errors	  in	  data	  are	  
likely	  to	  remain.	  	   This	  incompleteness	  and	  these	  errors	  must	  be	  managed	  during	  data	  analysis.	  	   Doing	  
this	   correctly	   is	   a	   challenge.	   	   Recent	   work	   on	   managing	   probabilistic	   data	   suggests	   one	   way	   to	   make	  
progress.	  

3.2	  Scale	  

Of	  course,	  the	  first	  thing	  anyone	  thinks	  of	  with	  Big	  Data	  is	  its	  size.	  	   After	  all,	  the	  word	  “big”	  is	  
there	  in	  the	  very	  name.	  	   Managing	  large	  and	  rapidly	  increasing	  volumes	  of	  data	  has	  been	  a	  challenging	  
issue	  for	  many	  decades.	  	  	  In	  the	  past,	  this	  challenge	  was	  mitigated	  by	  processors	  getting	  faster,	  following	  
Moore’s	  law,	  to	  provide	  us	  with	  the	  resources	  needed	  to	  cope	  with	  increasing	  volumes	  of	  data.	  	   But,	  

8 

 

	  

	  

	  

 

there	  is	  a	  fundamental	  shift	  underway	  now:	  data	  volume	  is	  scaling	  faster	  than	  compute	  resources,	  and	  
CPU	  speeds	  are	  static.	  

First,	  over	  the	  last	  five	  years	  the	  processor	  technology	  has	  made	  a	  dramatic	  shift	  -­‐	  rather	  than	  
processors	   doubling	   their	   clock	   cycle	   frequency	   every	   18-­‐24	   months,	   now,	   due	   to	   power	   constraints,	  
clock	  speeds	  have	  largely	  stalled	  and	  processors	  are	  being	  built	  with	  increasing	  numbers	  of	  cores.	  	  In	  the	  
past,	  large	  data	  processing	  systems	  had	  to	  worry	  about	  parallelism	  across	  nodes	  in	  a	  cluster;	  now,	  one	  
has	   to	   deal	   with	   parallelism	   within	   a	   single	   node.	  	  	  Unfortunately,	   parallel	   data	   processing	   techniques	  
that	   were	   applied	   in	   the	   past	   for	   processing	   data	   across	   nodes	   don’t	   directly	   apply	   for	   intra-­‐node	  
parallelism,	   since	   the	   architecture	   looks	   very	   different;	   for	   example,	   there	   are	   many	   more	   hardware	  
resources	   such	  as	   processor	   caches	   and	   processor	  memory	   channels	   that	   are	  shared	   across	  cores	   in	  a	  
single	   node.	  	  	  Furthermore,	   the	   move	   towards	   packing	   multiple	   sockets	   (each	   with	   10s	   of	   cores)	   adds	  
another	  level	  of	  complexity	  for	  intra-­‐node	  parallelism.	  	  Finally,	  with	  predictions	  of	  “dark	  silicon”,	  namely	  
that	  power	  consideration	  will	  likely	  in	  the	  future	  prohibit	  us	  from	  using	  all	  of	  the	  hardware	  in	  the	  system	  
continuously,	  data	  processing	  systems	  will	  likely	  have	  to	  actively	  manage	  the	  power	  consumption	  of	  the	  
processor.	  These	  unprecedented	  changes	  require	  us	  to	  rethink	  how	  we	  design,	  build	  and	  operate	  data	  
processing	  components.	  

The	   second	  dramatic	   shift	   that	   is	   underway	   is	   the	   move	   towards	   cloud	  computing,	   which	  now	  
aggregates	   multiple	   disparate	   workloads	   with	   varying	   performance	   goals	   (e.g.	   interactive	   services	  
demand	  that	  the	  data	  processing	  engine	  return	  back	  an	  answer	  within	  a	  fixed	  response	  time	  cap)	  into	  
very	  large	  clusters.	  This	  level	  of	  sharing	  of	  resources	  on	  expensive	  and	  large	  clusters	  requires	  new	  ways	  
of	   determining	   how	   to	   run	   and	   execute	   data	   processing	   jobs	   so	   that	   we	   can	   meet	   the	   goals	   of	   each	  
workload	  cost-­‐effectively,	  and	  to	  deal	  with	  system	  failures,	  which	  occur	  more	  frequently	  as	  we	  operate	  
on	   larger	   and	   larger	   clusters	   (that	   are	   required	   to	   deal	   with	   the	   rapid	   growth	   in	   data	   volumes).	  	  	  This	  
places	  	  a	  	  premium	  	  on	  	  declarative	  	  approaches	  	  to	  	  expressing	  	  programs,	  	  even	  	  those	  	  doing	  	  complex	  
machine	  learning	  tasks,	  since	  global	  optimization	  across	  multiple	  users’	  programs	  is	  necessary	  for	  good	  
overall	   performance.	   	   Reliance	   on	   user-­‐driven	   program	   optimizations	   is	   likely	   to	   lead	   to	   poor	   cluster	  
utilization,	   since	   users	   are	   unaware	   of	   other	   users’	   programs.	   	   	   	   System-­‐driven	   holistic	   optimization	  
requires	  	   programs	  	   to	  	   be	  	   sufficiently	  	   transparent,	  	   e.g.,	  	   as	  	   in	  	   relational	  	   database	  	   systems,	  	   where	  
declarative	  query	  languages	  are	  designed	  with	  this	  in	  mind.	  

A	   third	   dramatic	   shift	   that	   is	   underway	   is	   the	   transformative	   change	   of	   the	   traditional	   I/O	  
subsystem.	  For	  many	  decades,	  hard	  disk	  drives	  (HDDs)	  were	  used	  to	  store	  persistent	  data.	  HDDs	  had	  far	  
slower	  random	  IO	  performance	  than	  sequential	  IO	  performance,	  and	  data	  processing	  engines	  formatted	  
their	  data	  and	  designed	  their	  query	  processing	  methods	  to	  “work	  around”	  this	  limitation.	  But,	  HDDs	  are	  
increasingly	   being	   replaced	   by	   solid	   state	   drives	   today,	   and	   other	   technologies	   such	   as	   Phase	   Change	  
Memory	  are	  around	  the	  corner.	  These	  newer	  storage	  technologies	  do	  not	  have	  the	  same	  large	  spread	  in	  
performance	  between	  the	  sequential	  and	  random	  I/O	  performance,	  which	  requires	  a	  rethinking	  of	  how	  
we	   design	   storage	   subsystems	   for	   data	   processing	   systems.	   Implications	   of	   this	   changing	   storage	  
subsystem	   potentially	   touch	   every	   aspect	   of	   data	   processing,	   including	   query	   processing	   algorithms,	  
query	  scheduling,	  database	  design,	  concurrency	  control	  methods	  and	  recovery	  methods.	  

9 

 

	  

	  

	  

	  
	  
	  

	  

	  

 

3.3	  Timeliness	  

The	  flip	  side	  of	  size	  is	  speed.	  	   The	  larger	  the	  data	  set	  to	  be	  processed,	  the	  longer	  it	  will	  take	  to	  
analyze.	  	  The	  design	  of	  a	  system	  that	  effectively	  deals	  with	  size	  is	  likely	  also	  to	  result	  in	  a	  system	  that	  can	  
process	  a	  given	  size	  of	  data	  set	  faster.	  	  However,	  it	  is	  not	  just	  this	  speed	  that	  is	  usually	  meant	  when	  one	  
speaks	  of	  Velocity	  in	  the	  context	  of	  Big	  Data.	  	  Rather,	  there	  is	  an	  acquisition	  rate	  challenge	  as	  described	  
in	  Sec.	  2.1,	  and	  a	  timeliness	  challenge	  described	  next.	  

There	   are	   many	   situations	   in	   which	   the	   result	   of	   the	   analysis	   is	   required	   immediately.	   	   For	  
example,	   if	   a	   fraudulent	   credit	   card	   transaction	   is	   suspected,	   it	   should	   ideally	   be	   flagged	   before	   the	  
transaction	  is	  completed	  –	  potentially	  preventing	  the	  transaction	  from	  taking	  place	  at	  all.	  	   Obviously,	  a	  
full	   analysis	   of	   a	   user’s	   purchase	   history	   is	   not	   likely	   to	   be	   feasible	   in	   real-­‐time.	   	   Rather,	   we	   need	   to	  
develop	  partial	  results	  in	  advance	  so	  that	  a	  small	  amount	  of	  incremental	  computation	  with	  new	  data	  can	  
be	  used	  to	  arrive	  at	  a	  quick	  determination.	  

Given	  a	  large	  data	  set,	  it	  is	  often	  necessary	  to	  find	  elements	  in	  it	  that	  meet	  a	  specified	  criterion.	  
In	  the	  course	  of	  data	  analysis,	  this	  sort	  of	  search	  is	  likely	  to	  occur	  repeatedly.	  	   Scanning	  the	  entire	  data	  
set	  to	  find	  suitable	  elements	  is	  obviously	  impractical.	  	  Rather,	  index	  structures	  are	  created	  in	  advance	  to	  
permit	   finding	   qualifying	   elements	   quickly.	   	   The	   problem	   is	   that	   each	   index	   structure	   is	   designed	   to	  
support	  only	  some	  classes	  of	  criteria.	  	  With	  new	  analyses	  desired	  using	  Big	  Data,	  there	  are	  new	  types	  of	  
criteria	   specified,	   and	   a	   need	   to	   devise	   new	   index	   structures	   to	   support	   such	   criteria.	   	   For	   example,	  
consider	   a	   traffic	   management	   system	   with	   information	   regarding	   thousands	   of	   vehicles	   and	   local	   hot	  
spots	  on	  roadways.	  	   The	  system	  may	  need	  to	  predict	  potential	  congestion	  points	  along	  a	  route	  chosen	  
by	   a	   user,	   and	   suggest	   alternatives.	   	   Doing	   so	   requires	   evaluating	   multiple	   spatial	   proximity	   queries	  
working	   with	   the	   trajectories	   of	   moving	   objects.	   	   New	   index	   structures	   are	   required	   to	   support	   such	  
queries.	   Designing	   such	   structures	   becomes	   particularly	   challenging	   when	   the	   data	   volume	   is	   growing	  
rapidly	  and	  the	  queries	  have	  tight	  response	  time	  limits.	  

3.4	  Privacy	  

The	  privacy	  of	  data	  is	  another	  huge	  concern,	  and	  one	  that	  increases	  in	  the	  context	  of	  Big	  Data.	  
For	  electronic	  health	  records,	  there	  are	  strict	  laws	  governing	  what	  can	  and	  cannot	  be	  done.	  	   For	  other	  
data,	  regulations,	  particularly	  in	  the	  US,	  are	  less	  forceful.	  	   However,	  there	  is	  great	  public	  fear	  regarding	  
the	   inappropriate	   use	   of	   personal	   data,	   particularly	   through	   linking	   of	   data	   from	   multiple	   sources.	  
Managing	   privacy	   is	   effectively	   both	   a	   technical	   and	   a	   sociological	   problem,	  which	   must	   be	   addressed	  
jointly	  from	  both	  perspectives	  to	  realize	  the	  promise	  of	  big	  data.	  

Consider,	   for	   example,	   data	   gleaned	   from	   location-­‐based	   services.	   	   These	   new	   architectures	  
require	  a	  user	  to	  share	  his/her	  location	  with	  the	  service	  provider,	  resulting	  in	  obvious	  privacy	  concerns.	  
Note	  that	  hiding	  the	  user’s	  identity	  alone	  without	  hiding	  her	  location	  would	  not	  properly	  address	  these	  
privacy	  concerns.	  	  An	  attacker	  or	  a	  (potentially	  malicious)	  location-­‐based	  server	  can	  infer	  the	  identity	  of	  
the	  query	  source	  from	  its	  (subsequent)	  location	  information.	  For	  example,	  a	  user’s	  location	  information	  
can	  be	  tracked	  through	  several	  stationary	  connection	  points	  (e.g.,	  cell	  towers).	  	   After	  a	  while,	  the	  user	  

10 

 

	  

	  
	  
	  

	  

	  

	  

 

leaves	  “a	  trail	  of	  packet	  crumbs”	  which	  could	  be	  associated	  to	  a	  certain	  residence	  or	  office	  location	  and	  
thereby	   used	   to	   determine	   the	   user’s	   identity.	  	   Several	   other	   types	   of	   surprisingly	   private	   information	  
such	  	  as	  	  health	  	  issues	  	  (e.g.,	  	  presence	  	  in	  	  a	  	  cancer	  	  treatment	  	  center)	  	  or	  	  religious	  	  preferences	  	  (e.g.,	  
presence	   in	   a	   church)	   can	   also	   be	   revealed	   by	   just	   observing	   anonymous	   users’	   movement	   and	   usage	  
pattern	  over	  time.	  In	  general,	  Barabási	  et	  al.	  showed	  that	  there	  is	  a	  close	  correlation	  between	  people’s	  
identities	   and	   their	   movement	   patterns	   [Gon2008].	   	   Note	   that	   hiding	   a	   user	   location	   is	   much	   more	  
challenging	  than	  hiding	  his/her	  identity.	  	  This	  is	  because	  with	  location-­‐based	  services,	  the	  location	  of	  the	  
user	   is	   needed	   for	   a	   successful	   data	   access	   or	   data	   collection,	   while	   the	   identity	   of	   the	   user	   is	   not	  
necessary.	  

There	   are	   many	   additional	   challenging	   research	   problems.	  	  	  For	   example,	   we	   do	   not	   know	   yet	  
how	  to	  share	  private	  data	  while	  limiting	  disclosure	  and	  ensuring	  sufficient	  data	  utility	  in	  the	  shared	  data.	  
The	   existing	   paradigm	   of	   differential	   privacy	   is	   a	   very	   important	   step	   in	   the	   right	   direction,	   but	   it	  
unfortunately	   reduces	   information	   content	   too	   far	   in	   order	   to	   be	   useful	   in	   most	   practical	   cases.	   	   In	  
addition,	  real	  data	  is	  not	  static	  but	  gets	  larger	  and	  changes	  over	  time;	  none	  of	  the	  prevailing	  techniques	  
results	  in	  any	  useful	  content	  being	  released	  in	  this	  scenario.	  	   Yet	  another	  very	  important	  direction	  is	  to	  
rethink	  security	  for	  information	  sharing	  in	  Big	  Data	  use	  cases.	  	  Many	  online	  services	  today	  require	  us	  to	  
share	  private	  information	  (think	  of	  Facebook	  applications),	  but	  beyond	  record-­‐level	  access	  control	  we	  do	  
not	  understand	  what	  it	  means	  to	  share	  data,	  how	  the	  shared	  data	  can	  be	  linked,	  and	  how	  to	  give	  users	  
fine-­‐grained	  control	  over	  this	  sharing.	  

3.5	  Human	  Collaboration	  

In	   spite	   of	   the	   tremendous	   advances	   made	   in	   computational	   analysis,	   there	   remain	   many	  
patterns	   that	   humans	   can	   easily	   detect	   but	   computer	   algorithms	   have	   a	   hard	   time	   finding.	   	   Indeed,	  
CAPTCHAs	  exploit	  precisely	   this	  fact	  to	   tell	  human	  web	  users	  apart	  from	   computer	  programs.	  	   Ideally,	  
analytics	  for	  Big	  Data	  will	  not	  be	  all	  computational	  –	  rather	  it	  will	  be	  designed	  explicitly	  to	  have	  a	  human	  
in	  the	  loop.	  	   The	  new	  sub-­‐field	  of	  visual	  analytics	  is	  attempting	  to	  do	  this,	  at	  least	  with	  respect	  to	  the	  
modeling	  and	  analysis	  phase	  in	  the	  pipeline.	  	   There	  is	  similar	  value	  to	  human	  input	  at	  all	  stages	  of	  the	  
analysis	  pipeline.	  

In	   today’s	   complex	   world,	   it	   often	   takes	   multiple	   experts	   from	   different	   domains	   to	   really	  
understand	   what	   is	   going	   on.	   	   A	   Big	   Data	   analysis	   system	   must	   support	   input	   from	   multiple	   human	  
experts,	  and	  shared	  exploration	  of	  results.	  	  These	  multiple	  experts	  may	  be	  separated	  in	  space	  and	  time	  
when	   it	   is	   too	   expensive	   to	   assemble	   an	   entire	   team	   together	   in	   one	   room.	  	  	  The	   data	   system	   has	   to	  
accept	  this	  distributed	  expert	  input,	  and	  support	  their	  collaboration.	  

A	   popular	   new	   method	   of	   harnessing	   human	   ingenuity	   to	   solve	   problems	   is	   through	   crowd-­‐	  
sourcing.	  	  	  Wikipedia,	   the	   online	   encyclopedia,	   is	   perhaps	   the	   best	   known	   example	   of	   crowd-­‐sourced	  
data.	  	   We	  are	  relying	  upon	  information	  provided	  by	  unvetted	  strangers.	  	   Most	  often,	  what	  they	   say	   is	  
correct.	  	  	  However,	   we	   should	   expect	   there	   to	   be	   individuals	   who	   have	   other	   motives	   and	   abilities	   –	  
some	  may	  have	  a	  reason	  to	  provide	  false	  information	  in	  an	  intentional	  attempt	  to	  mislead.	  	  While	  most	  

11 

 

	  
	  
	  

	  

	  

	  

 

such	   errors	   will	   be	   detected	   and	   corrected	   by	   others	   in	   the	   crowd,	   we	   need	   technologies	   to	   facilitate	  
this.	   	   	   We	   also	   need	   a	   framework	   to	   use	   in	   analysis	   of	   such	   crowd-­‐sourced	   data	   with	   conflicting	  
statements.	  	  As	  humans,	  we	  can	  look	  at	  reviews	  of	  a	  restaurant,	  some	  of	  which	  are	  positive	  and	  others	  
critical,	  and	  come	  up	  with	  a	  summary	  assessment	  based	  on	  which	  we	  can	  decide	  whether	  to	  try	  eating	  
there.	  	  We	  need	  computers	  to	  be	  able	  to	  do	  the	  equivalent.	  	  The	  issues	  of	  uncertainty	  and	  error	  become	  
even	  more	  pronounced	  in	  a	  specific	  type	  of	  crowd-­‐sourcing,	  termed	  participatory-­‐sensing.	  	  In	  this	  case,	  
every	   person	   with	   a	   mobile	   phone	   can	   act	   as	   a	   multi-­‐modal	   sensor	   collecting	   various	   types	   of	   data	  
instantaneously	   (e.g.,	   picture,	   video,	   audio,	   location,	   time,	   speed,	   direction,	   acceleration).	   	   The	   extra	  
challenge	  here	  is	  the	  inherent	  uncertainty	  of	  the	  data	  collection	  devices.	  	  The	  fact	  that	  collected	  data	  are	  
probably	  spatially	  and	  temporally	  correlated	  can	  be	  exploited	  to	  better	  assess	  their	  correctness.	  	  When	  
crowd-­‐sourced	  data	  is	  obtained	  for	  hire,	  such	  as	  with	  “Mechanical	  Turks,”	  much	  of	  the	  data	  created	  may	  
be	   with	   a	   primary	   objective	   of	   getting	   it	   done	   quickly	   rather	   than	   correctly.	  	   This	   is	   yet	   another	   error	  
model,	  which	  must	  be	  planned	  for	  explicitly	  when	  it	  applies.	  

4.	  System	  Architecture	  

Companies	  today	  already	  use,	  and	  appreciate	  the	  value	  of,	  business	  intelligence.	  	  	  Business	  data	  
is	  analyzed	  for	  many	  purposes:	  a	  company	  may	  perform	  system	  log	  analytics	  and	  social	  media	  analytics	  
for	   risk	   assessment,	   customer	   retention,	   brand	   management,	   and	   so	   on.	  	  	  Typically,	   such	   varied	   tasks	  
have	   been	   handled	   by	   separate	   systems,	   even	   if	   each	   system	   includes	   common	   steps	   of	   information	  
extraction,	   data	   cleaning,	   relational-­‐like	   processing	   (joins,	   group-­‐by,	   aggregation),	   statistical	   and	  
predictive	  modeling,	  and	  appropriate	  exploration	  and	  visualization	  tools	  as	  shown	  in	  Fig.	  1.	  

With	  Big	  Data,	  the	  use	  of	  separate	  systems	  in	  this	  fashion	  becomes	  prohibitively	  expensive	  given	  
the	  large	  size	  of	  the	  data	  sets.	  	  	   The	  expense	  is	  due	  not	  only	  to	  the	  cost	  of	  the	  systems	  themselves,	  but	  
also	  the	  time	  to	  load	  the	  data	  into	  multiple	  systems.	  	  In	  consequence,	  Big	  Data	  has	  made	  it	  necessary	  to	  
run	   heterogeneous	   workloads	   on	   a	   single	   infrastructure	   that	   is	   sufficiently	   flexible	   to	   handle	   all	   these	  
workloads.	  	  	  	  The	   challenge	   here	   is	  not	   to	   build	   a	   system	   that	   is	   ideally	   suited	  for	   all	   processing	  tasks.	  
Instead,	  the	  need	  is	  for	  the	  underlying	  system	  architecture	  to	  be	  flexible	  enough	  that	  the	  components	  
built	  on	  top	  of	  it	  for	  expressing	  the	  various	  kinds	  of	  processing	  tasks	  can	  tune	  it	  to	  efficiently	  run	  these	  
different	  workloads.	  	  The	  effects	  of	  scale	  on	  the	  physical	  architecture	  were	  considered	  in	  Sec	  3.2.	  	  In	  this	  
section,	  we	  focus	  on	  the	  programmability	  requirements.	  

If	  users	  are	  to	  compose	  and	  build	  complex	  analytical	  pipelines	  over	  Big	  Data,	  it	  is	  essential	  that	  
they	  have	  appropriate	  high-­‐level	  primitives	  to	  specify	  their	  needs	  in	  such	  flexible	  systems.	  	  	  	  The	  Map-­‐	  
Reduce	  framework	  has	  been	  tremendously	  valuable,	  but	  is	  only	  a	  first	  step.	  	  Even	  declarative	  languages	  
that	   exploit	   it,	   such	   as	   Pig	   Latin,	   are	   at	   a	   rather	   low	   level	   when	   it	   comes	   to	   complex	   analysis	   tasks.	  
Similar	   declarative	   specifications	   are	   required	   at	   higher	   levels	   to	   meet	   the	   programmability	   and	  
composition	   needs	   of	   these	   analysis	   pipelines.	  	  	  	  	  Besides	   the	   basic	   technical	   need,	   there	   is	   a	   strong	  
business	  imperative	  as	  well.	  	  Businesses	  typically	  will	  outsource	  Big	  Data	  processing,	  or	  many	  aspects	  of	  
it.	  	  	  Declarative	   specifications	   are	   required	   to	   enable	   technically	   meaningful	   service	   level	   agreements,	  

12 

 

	  

	  

	  
	  
	  

	  

 

since	  the	  point	  of	  the	  out-­‐sourcing	  is	  to	  specify	  precisely	  what	  task	  will	  be	  performed	  without	  going	  into	  
details	  of	  how	  to	  do	  it.	  

Declarative	   specification	   is	   needed	   not	   just	   for	   the	   pipeline	   composition,	   but	   also	   for	   the	  
individual	  operations	  themselves.	  	   Each	  operation	  (cleaning,	   extraction,	  modeling	  etc.)	   potentially	   runs	  
on	  a	  very	  large	  data	  set.	  	  Furthermore,	  each	  operation	  itself	  is	  sufficiently	  complex	  that	  there	  are	  many	  
choices	  and	  optimizations	  possible	  in	  how	  it	  is	  implemented.	  	   In	  databases,	  there	  is	  considerable	  work	  
on	  optimizing	  individual	  operations,	  such	  as	  joins.	  	  It	  is	  well-­‐known	  that	  there	  can	  be	  multiple	  orders	  of	  
magnitude	  difference	  in	  the	  cost	  of	  two	  different	  ways	  to	  execute	  the	  same	  query.	  	  Fortunately,	  the	  user	  
does	  not	  have	  to	  make	  this	  choice	  –	  the	  database	  system	  makes	  it	  for	  her.	  	  In	  the	  case	  of	  Big	  Data,	  these	  
optimizations	   may	   be	   more	   complex	   because	   not	   all	   operations	   will	   be	   I/O	   intensive	   as	   in	   databases.	  
Some	  operations	  may	  be,	  but	  others	  may	  be	  CPU	  intensive,	  or	  a	  mix.	  	  So	  standard	  database	  optimization	  
techniques	  cannot	  directly	  be	  used.	  	   However,	  it	  should	  be	  possible	  to	  develop	  new	  techniques	  for	  Big	  
Data	  operations	  inspired	  by	  database	  techniques.	  

The	  very	  fact	  that	  Big	  Data	  analysis	  typically	  involves	  multiple	  phases	  highlights	  a	  challenge	  that	  
arises	   routinely	   in	  practice:	  	   production	  systems	  must	   run	  complex	  analytic	   pipelines,	  or	   workflows,	   at	  
routine	   intervals,	   e.g.,	   hourly	   or	   daily.	  	  	   New	   data	   must	   be	   incrementally	   accounted	   for,	   taking	   into	  
account	  	  the	  	  results	  	  of	  	  prior	  	  analysis	  	  and	  	  pre-­‐existing	  	  data.	  	  	  	  And	  	  of	  	  course,	  	  provenance	  	  must	  	  be	  
preserved,	   and	   must	   include	   the	   phases	   in	   the	   analytic	   pipeline.	  	  	   Current	   systems	   offer	   little	   to	   no	  
support	  for	  such	  Big	  Data	  pipelines,	  and	  this	  is	  in	  itself	  a	  challenging	  objective.	  

5.	  Conclusion	  

We	  have	  entered	  an	  era	  of	  Big	  Data.	  	   Through	  better	  analysis	  of	  the	  large	  volumes	  of	  data	  that	  
are	  becoming	  available,	  there	  is	  the	  potential	  for	  making	  faster	  advances	  in	  many	   scientific	  disciplines	  
and	   improving	   the	   profitability	   and	   success	   of	   many	   enterprises.	  	   However,	   many	   technical	   challenges	  
described	   in	   this	   paper	   must	   be	   addressed	   before	   this	   potential	   can	   be	   realized	   fully.	  	   The	   challenges	  
include	   not	   just	   the	   obvious	   issues	   of	   scale,	   but	   also	   heterogeneity,	   lack	   of	   structure,	   error-­‐handling,	  
privacy,	   timeliness,	   provenance,	   and	   visualization,	   at	   all	   stages	   of	   the	   analysis	   pipeline	   from	   data	  
acquisition	  to	  result	  interpretation.	  	  	  	  These	  technical	  challenges	  are	  common	  across	  a	  large	  variety	  of	  
application	   domains,	   and	   therefore	   not	   cost-­‐effective	   to	   address	   in	   the	   context	   of	   one	   domain	   alone.	  
Furthermore,	  these	  challenges	  will	  require	  transformative	  solutions,	  and	  will	  not	  be	  addressed	  naturally	  
by	  the	  next	  generation	  of	  industrial	  products.	  	  	   We	  must	  support	  and	  encourage	  fundamental	  research	  
towards	  addressing	  these	  technical	  challenges	  if	  we	  are	  to	  achieve	  the	  promised	  benefits	  of	  Big	  Data.	  

13 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 

	  
	  

	  

	  

	  

	  
	  
	  

	  

 

3.1	  Heterogeneity	  and	  Incompleteness	  

When	  humans	  consume	  information,	  a	  great	  deal	  of	  heterogeneity	  is	  comfortably	  tolerated.	  	  In	  
fact,	   the	   nuance	   and	   richness	   of	   natural	   language	   can	   provide	   valuable	   depth.	  	  	   However,	   machine	  
analysis	   algorithms	   expect	   homogeneous	   data,	   and	   cannot	   understand	   nuance.	  	   In	   consequence,	   data	  
must	  be	  carefully	  structured	  as	  a	  first	  step	  in	  (or	  prior	  to)	  data	  analysis.	  	  Consider,	  for	  example,	  a	  patient	  
who	  has	  multiple	  medical	  procedures	  at	  a	  hospital.	  	  We	  could	  create	  one	  record	  per	  medical	  procedure	  
or	   laboratory	   test,	   one	   record	   for	   the	   entire	   hospital	   stay,	   or	   one	   record	   for	   all	   lifetime	   hospital	  
interactions	  	  of	  	  this	  	  patient.	  	  	  	   With	  	  anything	  	  other	  	  than	  	  the	  	  first	  	  design,	  	  the	  	  number	  	  of	  	  medical	  
procedures	  and	  lab	  tests	  per	  record	  would	  be	  different	  for	  each	  patient.	  	  	  The	  three	  design	  choices	  listed	  
have	  successively	  less	  structure	  and,	  conversely,	  successively	  greater	  variety.	  	   Greater	  structure	  is	  likely	  
to	  be	  required	  by	  many	  (traditional)	  data	  analysis	  systems.	  	  However,	  the	  less	  structured	  design	  is	  likely	  
to	   be	   more	   effective	   for	   many	   purposes	   –	   for	   example	   questions	   relating	   to	   disease	   progression	   over	  
time	   will	   require	   an	   expensive	   join	   operation	   with	   the	   first	   two	   designs,	   but	   can	   be	   avoided	   with	   the	  
latter.	  	  	  	  However,	  computer	  systems	  work	  most	  efficiently	  if	  they	  can	  store	  multiple	  items	  that	  are	  all	  
identical	   in	   size	   and	   structure.	   	   Efficient	   representation,	   access,	   and	   analysis	   of	   semi-­‐structured	   data	  
require	  further	  work.	  

Consider	  an	  electronic	  health	  record	  database	  design	  that	  has	  fields	  for	  birth	  date,	  occupation,	  
and	  blood	  type	  for	  each	  patient.	  	   What	  do	  we	  do	  if	  one	  or	  more	  of	  these	  pieces	  of	  information	  is	  not	  
provided	   by	   a	   patient?	   	   	   Obviously,	   the	   health	   record	   is	   still	   placed	   in	   the	   database,	   but	   with	   the	  
corresponding	  attribute	  values	  being	  set	  to	  NULL.	  	  	  A	  data	  analysis	  that	  looks	  to	  classify	  patients	  by,	  say,	  
occupation,	   must	   take	   into	   account	   patients	   for	   which	   this	   information	   is	   not	   known.	   	   Worse,	   these	  
patients	  with	  unknown	  occupations	  can	  be	  ignored	  in	  the	  analysis	  only	  if	  we	  have	  reason	  to	  believe	  that	  
they	  	   are	  	   otherwise	  	   statistically	  	   similar	  	   to	  	   the	  	   patients	  	   with	  	   known	  	   occupation	  	   for	  	   the	  	   analysis	  
performed.	   	   For	   example,	   if	   unemployed	   patients	   are	   more	   likely	   to	   hide	   their	   employment	   status,	  
analysis	   results	   may	   be	   skewed	   in	   that	   it	   considers	   a	   more	   employed	   population	   mix	   than	   exists,	   and	  
hence	  potentially	  one	  that	  has	  differences	  in	  occupation-­‐related	  health-­‐profiles.	  

Even	  after	  data	  cleaning	  and	  error	  correction,	  some	  incompleteness	  and	  some	  errors	  in	  data	  are	  
likely	  to	  remain.	  	   This	  incompleteness	  and	  these	  errors	  must	  be	  managed	  during	  data	  analysis.	  	   Doing	  
this	   correctly	   is	   a	   challenge.	   	   Recent	   work	   on	   managing	   probabilistic	   data	   suggests	   one	   way	   to	   make	  
progress.	  

3.2	  Scale	  

Of	  course,	  the	  first	  thing	  anyone	  thinks	  of	  with	  Big	  Data	  is	  its	  size.	  	   After	  all,	  the	  word	  “big”	  is	  
there	  in	  the	  very	  name.	  	   Managing	  large	  and	  rapidly	  increasing	  volumes	  of	  data	  has	  been	  a	  challenging	  
issue	  for	  many	  decades.	  	  	  In	  the	  past,	  this	  challenge	  was	  mitigated	  by	  processors	  getting	  faster,	  following	  
Moore’s	  law,	  to	  provide	  us	  with	  the	  resources	  needed	  to	  cope	  with	  increasing	  volumes	  of	  data.	  	   But,	  

8 

 

	  

	  

	  

 

there	  is	  a	  fundamental	  shift	  underway	  now:	  data	  volume	  is	  scaling	  faster	  than	  compute	  resources,	  and	  
CPU	  speeds	  are	  static.	  

First,	  over	  the	  last	  five	  years	  the	  processor	  technology	  has	  made	  a	  dramatic	  shift	  -­‐	  rather	  than	  
processors	   doubling	   their	   clock	   cycle	   frequency	   every	   18-­‐24	   months,	   now,	   due	   to	   power	   constraints,	  
clock	  speeds	  have	  largely	  stalled	  and	  processors	  are	  being	  built	  with	  increasing	  numbers	  of	  cores.	  	  In	  the	  
past,	  large	  data	  processing	  systems	  had	  to	  worry	  about	  parallelism	  across	  nodes	  in	  a	  cluster;	  now,	  one	  
has	   to	   deal	   with	   parallelism	   within	   a	   single	   node.	  	  	  Unfortunately,	   parallel	   data	   processing	   techniques	  
that	   were	   applied	   in	   the	   past	   for	   processing	   data	   across	   nodes	   don’t	   directly	   apply	   for	   intra-­‐node	  
parallelism,	   since	   the	   architecture	   looks	   very	   different;	   for	   example,	   there	   are	   many	   more	   hardware	  
resources	   such	  as	   processor	   caches	   and	   processor	  memory	   channels	   that	   are	  shared	   across	  cores	   in	  a	  
single	   node.	  	  	  Furthermore,	   the	   move	   towards	   packing	   multiple	   sockets	   (each	   with	   10s	   of	   cores)	   adds	  
another	  level	  of	  complexity	  for	  intra-­‐node	  parallelism.	  	  Finally,	  with	  predictions	  of	  “dark	  silicon”,	  namely	  
that	  power	  consideration	  will	  likely	  in	  the	  future	  prohibit	  us	  from	  using	  all	  of	  the	  hardware	  in	  the	  system	  
continuously,	  data	  processing	  systems	  will	  likely	  have	  to	  actively	  manage	  the	  power	  consumption	  of	  the	  
processor.	  These	  unprecedented	  changes	  require	  us	  to	  rethink	  how	  we	  design,	  build	  and	  operate	  data	  
processing	  components.	  

The	   second	  dramatic	   shift	   that	   is	   underway	   is	   the	   move	   towards	   cloud	  computing,	   which	  now	  
aggregates	   multiple	   disparate	   workloads	   with	   varying	   performance	   goals	   (e.g.	   interactive	   services	  
demand	  that	  the	  data	  processing	  engine	  return	  back	  an	  answer	  within	  a	  fixed	  response	  time	  cap)	  into	  
very	  large	  clusters.	  This	  level	  of	  sharing	  of	  resources	  on	  expensive	  and	  large	  clusters	  requires	  new	  ways	  
of	   determining	   how	   to	   run	   and	   execute	   data	   processing	   jobs	   so	   that	   we	   can	   meet	   the	   goals	   of	   each	  
workload	  cost-­‐effectively,	  and	  to	  deal	  with	  system	  failures,	  which	  occur	  more	  frequently	  as	  we	  operate	  
on	   larger	   and	   larger	   clusters	   (that	   are	   required	   to	   deal	   with	   the	   rapid	   growth	   in	   data	   volumes).	  	  	  This	  
places	  	  a	  	  premium	  	  on	  	  declarative	  	  approaches	  	  to	  	  expressing	  	  programs,	  	  even	  	  those	  	  doing	  	  complex	  
machine	  learning	  tasks,	  since	  global	  optimization	  across	  multiple	  users’	  programs	  is	  necessary	  for	  good	  
overall	   performance.	   	   Reliance	   on	   user-­‐driven	   program	   optimizations	   is	   likely	   to	   lead	   to	   poor	   cluster	  
utilization,	   since	   users	   are	   unaware	   of	   other	   users’	   programs.	   	   	   	   System-­‐driven	   holistic	   optimization	  
requires	  	   programs	  	   to	  	   be	  	   sufficiently	  	   transparent,	  	   e.g.,	  	   as	  	   in	  	   relational	  	   database	  	   systems,	  	   where	  
declarative	  query	  languages	  are	  designed	  with	  this	  in	  mind.	  

A	   third	   dramatic	   shift	   that	   is	   underway	   is	   the	   transformative	   change	   of	   the	   traditional	   I/O	  
subsystem.	  For	  many	  decades,	  hard	  disk	  drives	  (HDDs)	  were	  used	  to	  store	  persistent	  data.	  HDDs	  had	  far	  
slower	  random	  IO	  performance	  than	  sequential	  IO	  performance,	  and	  data	  processing	  engines	  formatted	  
their	  data	  and	  designed	  their	  query	  processing	  methods	  to	  “work	  around”	  this	  limitation.	  But,	  HDDs	  are	  
increasingly	   being	   replaced	   by	   solid	   state	   drives	   today,	   and	   other	   technologies	   such	   as	   Phase	   Change	  
Memory	  are	  around	  the	  corner.	  These	  newer	  storage	  technologies	  do	  not	  have	  the	  same	  large	  spread	  in	  
performance	  between	  the	  sequential	  and	  random	  I/O	  performance,	  which	  requires	  a	  rethinking	  of	  how	  
we	   design	   storage	   subsystems	   for	   data	   processing	   systems.	   Implications	   of	   this	   changing	   storage	  
subsystem	   potentially	   touch	   every	   aspect	   of	   data	   processing,	   including	   query	   processing	   algorithms,	  
query	  scheduling,	  database	  design,	  concurrency	  control	  methods	  and	  recovery	  methods.	  

9 

 

	  

	  

	  

	  
	  
	  

	  

	  

 

3.3	  Timeliness	  

The	  flip	  side	  of	  size	  is	  speed.	  	   The	  larger	  the	  data	  set	  to	  be	  processed,	  the	  longer	  it	  will	  take	  to	  
analyze.	  	  The	  design	  of	  a	  system	  that	  effectively	  deals	  with	  size	  is	  likely	  also	  to	  result	  in	  a	  system	  that	  can	  
process	  a	  given	  size	  of	  data	  set	  faster.	  	  However,	  it	  is	  not	  just	  this	  speed	  that	  is	  usually	  meant	  when	  one	  
speaks	  of	  Velocity	  in	  the	  context	  of	  Big	  Data.	  	  Rather,	  there	  is	  an	  acquisition	  rate	  challenge	  as	  described	  
in	  Sec.	  2.1,	  and	  a	  timeliness	  challenge	  described	  next.	  

There	   are	   many	   situations	   in	   which	   the	   result	   of	   the	   analysis	   is	   required	   immediately.	   	   For	  
example,	   if	   a	   fraudulent	   credit	   card	   transaction	   is	   suspected,	   it	   should	   ideally	   be	   flagged	   before	   the	  
transaction	  is	  completed	  –	  potentially	  preventing	  the	  transaction	  from	  taking	  place	  at	  all.	  	   Obviously,	  a	  
full	   analysis	   of	   a	   user’s	   purchase	   history	   is	   not	   likely	   to	   be	   feasible	   in	   real-­‐time.	   	   Rather,	   we	   need	   to	  
develop	  partial	  results	  in	  advance	  so	  that	  a	  small	  amount	  of	  incremental	  computation	  with	  new	  data	  can	  
be	  used	  to	  arrive	  at	  a	  quick	  determination.	  

Given	  a	  large	  data	  set,	  it	  is	  often	  necessary	  to	  find	  elements	  in	  it	  that	  meet	  a	  specified	  criterion.	  
In	  the	  course	  of	  data	  analysis,	  this	  sort	  of	  search	  is	  likely	  to	  occur	  repeatedly.	  	   Scanning	  the	  entire	  data	  
set	  to	  find	  suitable	  elements	  is	  obviously	  impractical.	  	  Rather,	  index	  structures	  are	  created	  in	  advance	  to	  
permit	   finding	   qualifying	   elements	   quickly.	   	   The	   problem	   is	   that	   each	   index	   structure	   is	   designed	   to	  
support	  only	  some	  classes	  of	  criteria.	  	  With	  new	  analyses	  desired	  using	  Big	  Data,	  there	  are	  new	  types	  of	  
criteria	   specified,	   and	   a	   need	   to	   devise	   new	   index	   structures	   to	   support	   such	   criteria.	   	   For	   example,	  
consider	   a	   traffic	   management	   system	   with	   information	   regarding	   thousands	   of	   vehicles	   and	   local	   hot	  
spots	  on	  roadways.	  	   The	  system	  may	  need	  to	  predict	  potential	  congestion	  points	  along	  a	  route	  chosen	  
by	   a	   user,	   and	   suggest	   alternatives.	   	   Doing	   so	   requires	   evaluating	   multiple	   spatial	   proximity	   queries	  
working	   with	   the	   trajectories	   of	   moving	   objects.	   	   New	   index	   structures	   are	   required	   to	   support	   such	  
queries.	   Designing	   such	   structures	   becomes	   particularly	   challenging	   when	   the	   data	   volume	   is	   growing	  
rapidly	  and	  the	  queries	  have	  tight	  response	  time	  limits.	  

3.4	  Privacy	  

The	  privacy	  of	  data	  is	  another	  huge	  concern,	  and	  one	  that	  increases	  in	  the	  context	  of	  Big	  Data.	  
For	  electronic	  health	  records,	  there	  are	  strict	  laws	  governing	  what	  can	  and	  cannot	  be	  done.	  	   For	  other	  
data,	  regulations,	  particularly	  in	  the	  US,	  are	  less	  forceful.	  	   However,	  there	  is	  great	  public	  fear	  regarding	  
the	   inappropriate	   use	   of	   personal	   data,	   particularly	   through	   linking	   of	   data	   from	   multiple	   sources.	  
Managing	   privacy	   is	   effectively	   both	   a	   technical	   and	   a	   sociological	   problem,	  which	   must	   be	   addressed	  
jointly	  from	  both	  perspectives	  to	  realize	  the	  promise	  of	  big	  data.	  

Consider,	   for	   example,	   data	   gleaned	   from	   location-­‐based	   services.	   	   These	   new	   architectures	  
require	  a	  user	  to	  share	  his/her	  location	  with	  the	  service	  provider,	  resulting	  in	  obvious	  privacy	  concerns.	  
Note	  that	  hiding	  the	  user’s	  identity	  alone	  without	  hiding	  her	  location	  would	  not	  properly	  address	  these	  
privacy	  concerns.	  	  An	  attacker	  or	  a	  (potentially	  malicious)	  location-­‐based	  server	  can	  infer	  the	  identity	  of	  
the	  query	  source	  from	  its	  (subsequent)	  location	  information.	  For	  example,	  a	  user’s	  location	  information	  
can	  be	  tracked	  through	  several	  stationary	  connection	  points	  (e.g.,	  cell	  towers).	  	   After	  a	  while,	  the	  user	  

10 

 

	  

	  
	  
	  

	  

	  

	  

 

leaves	  “a	  trail	  of	  packet	  crumbs”	  which	  could	  be	  associated	  to	  a	  certain	  residence	  or	  office	  location	  and	  
thereby	   used	   to	   determine	   the	   user’s	   identity.	  	   Several	   other	   types	   of	   surprisingly	   private	   information	  
such	  	  as	  	  health	  	  issues	  	  (e.g.,	  	  presence	  	  in	  	  a	  	  cancer	  	  treatment	  	  center)	  	  or	  	  religious	  	  preferences	  	  (e.g.,	  
presence	   in	   a	   church)	   can	   also	   be	   revealed	   by	   just	   observing	   anonymous	   users’	   movement	   and	   usage	  
pattern	  over	  time.	  In	  general,	  Barabási	  et	  al.	  showed	  that	  there	  is	  a	  close	  correlation	  between	  people’s	  
identities	   and	   their	   movement	   patterns	   [Gon2008].	   	   Note	   that	   hiding	   a	   user	   location	   is	   much	   more	  
challenging	  than	  hiding	  his/her	  identity.	  	  This	  is	  because	  with	  location-­‐based	  services,	  the	  location	  of	  the	  
user	   is	   needed	   for	   a	   successful	   data	   access	   or	   data	   collection,	   while	   the	   identity	   of	   the	   user	   is	   not	  
necessary.	  

There	   are	   many	   additional	   challenging	   research	   problems.	  	  	  For	   example,	   we	   do	   not	   know	   yet	  
how	  to	  share	  private	  data	  while	  limiting	  disclosure	  and	  ensuring	  sufficient	  data	  utility	  in	  the	  shared	  data.	  
The	   existing	   paradigm	   of	   differential	   privacy	   is	   a	   very	   important	   step	   in	   the	   right	   direction,	   but	   it	  
unfortunately	   reduces	   information	   content	   too	   far	   in	   order	   to	   be	   useful	   in	   most	   practical	   cases.	   	   In	  
addition,	  real	  data	  is	  not	  static	  but	  gets	  larger	  and	  changes	  over	  time;	  none	  of	  the	  prevailing	  techniques	  
results	  in	  any	  useful	  content	  being	  released	  in	  this	  scenario.	  	   Yet	  another	  very	  important	  direction	  is	  to	  
rethink	  security	  for	  information	  sharing	  in	  Big	  Data	  use	  cases.	  	  Many	  online	  services	  today	  require	  us	  to	  
share	  private	  information	  (think	  of	  Facebook	  applications),	  but	  beyond	  record-­‐level	  access	  control	  we	  do	  
not	  understand	  what	  it	  means	  to	  share	  data,	  how	  the	  shared	  data	  can	  be	  linked,	  and	  how	  to	  give	  users	  
fine-­‐grained	  control	  over	  this	  sharing.	  

3.5	  Human	  Collaboration	  

In	   spite	   of	   the	   tremendous	   advances	   made	   in	   computational	   analysis,	   there	   remain	   many	  
patterns	   that	   humans	   can	   easily	   detect	   but	   computer	   algorithms	   have	   a	   hard	   time	   finding.	   	   Indeed,	  
CAPTCHAs	  exploit	  precisely	   this	  fact	  to	   tell	  human	  web	  users	  apart	  from	   computer	  programs.	  	   Ideally,	  
analytics	  for	  Big	  Data	  will	  not	  be	  all	  computational	  –	  rather	  it	  will	  be	  designed	  explicitly	  to	  have	  a	  human	  
in	  the	  loop.	  	   The	  new	  sub-­‐field	  of	  visual	  analytics	  is	  attempting	  to	  do	  this,	  at	  least	  with	  respect	  to	  the	  
modeling	  and	  analysis	  phase	  in	  the	  pipeline.	  	   There	  is	  similar	  value	  to	  human	  input	  at	  all	  stages	  of	  the	  
analysis	  pipeline.	  

In	   today’s	   complex	   world,	   it	   often	   takes	   multiple	   experts	   from	   different	   domains	   to	   really	  
understand	   what	   is	   going	   on.	   	   A	   Big	   Data	   analysis	   system	   must	   support	   input	   from	   multiple	   human	  
experts,	  and	  shared	  exploration	  of	  results.	  	  These	  multiple	  experts	  may	  be	  separated	  in	  space	  and	  time	  
when	   it	   is	   too	   expensive	   to	   assemble	   an	   entire	   team	   together	   in	   one	   room.	  	  	  The	   data	   system	   has	   to	  
accept	  this	  distributed	  expert	  input,	  and	  support	  their	  collaboration.	  

A	   popular	   new	   method	   of	   harnessing	   human	   ingenuity	   to	   solve	   problems	   is	   through	   crowd-­‐	  
sourcing.	  	  	  Wikipedia,	   the	   online	   encyclopedia,	   is	   perhaps	   the	   best	   known	   example	   of	   crowd-­‐sourced	  
data.	  	   We	  are	  relying	  upon	  information	  provided	  by	  unvetted	  strangers.	  	   Most	  often,	  what	  they	   say	   is	  
correct.	  	  	  However,	   we	   should	   expect	   there	   to	   be	   individuals	   who	   have	   other	   motives	   and	   abilities	   –	  
some	  may	  have	  a	  reason	  to	  provide	  false	  information	  in	  an	  intentional	  attempt	  to	  mislead.	  	  While	  most	  

11 

 

	  
	  
	  

	  

	  

	  

 

such	   errors	   will	   be	   detected	   and	   corrected	   by	   others	   in	   the	   crowd,	   we	   need	   technologies	   to	   facilitate	  
this.	   	   	   We	   also	   need	   a	   framework	   to	   use	   in	   analysis	   of	   such	   crowd-­‐sourced	   data	   with	   conflicting	  
statements.	  	  As	  humans,	  we	  can	  look	  at	  reviews	  of	  a	  restaurant,	  some	  of	  which	  are	  positive	  and	  others	  
critical,	  and	  come	  up	  with	  a	  summary	  assessment	  based	  on	  which	  we	  can	  decide	  whether	  to	  try	  eating	  
there.	  	  We	  need	  computers	  to	  be	  able	  to	  do	  the	  equivalent.	  	  The	  issues	  of	  uncertainty	  and	  error	  become	  
even	  more	  pronounced	  in	  a	  specific	  type	  of	  crowd-­‐sourcing,	  termed	  participatory-­‐sensing.	  	  In	  this	  case,	  
every	   person	   with	   a	   mobile	   phone	   can	   act	   as	   a	   multi-­‐modal	   sensor	   collecting	   various	   types	   of	   data	  
instantaneously	   (e.g.,	   picture,	   video,	   audio,	   location,	   time,	   speed,	   direction,	   acceleration).	   	   The	   extra	  
challenge	  here	  is	  the	  inherent	  uncertainty	  of	  the	  data	  collection	  devices.	  	  The	  fact	  that	  collected	  data	  are	  
probably	  spatially	  and	  temporally	  correlated	  can	  be	  exploited	  to	  better	  assess	  their	  correctness.	  	  When	  
crowd-­‐sourced	  data	  is	  obtained	  for	  hire,	  such	  as	  with	  “Mechanical	  Turks,”	  much	  of	  the	  data	  created	  may	  
be	   with	   a	   primary	   objective	   of	   getting	   it	   done	   quickly	   rather	   than	   correctly.	  	   This	   is	   yet	   another	   error	  
model,	  which	  must	  be	  planned	  for	  explicitly	  when	  it	  applies.	  

4.	  System	  Architecture	  

Companies	  today	  already	  use,	  and	  appreciate	  the	  value	  of,	  business	  intelligence.	  	  	  Business	  data	  
is	  analyzed	  for	  many	  purposes:	  a	  company	  may	  perform	  system	  log	  analytics	  and	  social	  media	  analytics	  
for	   risk	   assessment,	   customer	   retention,	   brand	   management,	   and	   so	   on.	  	  	  Typically,	   such	   varied	   tasks	  
have	   been	   handled	   by	   separate	   systems,	   even	   if	   each	   system	   includes	   common	   steps	   of	   information	  
extraction,	   data	   cleaning,	   relational-­‐like	   processing	   (joins,	   group-­‐by,	   aggregation),	   statistical	   and	  
predictive	  modeling,	  and	  appropriate	  exploration	  and	  visualization	  tools	  as	  shown	  in	  Fig.	  1.	  

With	  Big	  Data,	  the	  use	  of	  separate	  systems	  in	  this	  fashion	  becomes	  prohibitively	  expensive	  given	  
the	  large	  size	  of	  the	  data	  sets.	  	  	   The	  expense	  is	  due	  not	  only	  to	  the	  cost	  of	  the	  systems	  themselves,	  but	  
also	  the	  time	  to	  load	  the	  data	  into	  multiple	  systems.	  	  In	  consequence,	  Big	  Data	  has	  made	  it	  necessary	  to	  
run	   heterogeneous	   workloads	   on	   a	   single	   infrastructure	   that	   is	   sufficiently	   flexible	   to	   handle	   all	   these	  
workloads.	  	  	  	  The	   challenge	   here	   is	  not	   to	   build	   a	   system	   that	   is	   ideally	   suited	  for	   all	   processing	  tasks.	  
Instead,	  the	  need	  is	  for	  the	  underlying	  system	  architecture	  to	  be	  flexible	  enough	  that	  the	  components	  
built	  on	  top	  of	  it	  for	  expressing	  the	  various	  kinds	  of	  processing	  tasks	  can	  tune	  it	  to	  efficiently	  run	  these	  
different	  workloads.	  	  The	  effects	  of	  scale	  on	  the	  physical	  architecture	  were	  considered	  in	  Sec	  3.2.	  	  In	  this	  
section,	  we	  focus	  on	  the	  programmability	  requirements.	  

If	  users	  are	  to	  compose	  and	  build	  complex	  analytical	  pipelines	  over	  Big	  Data,	  it	  is	  essential	  that	  
they	  have	  appropriate	  high-­‐level	  primitives	  to	  specify	  their	  needs	  in	  such	  flexible	  systems.	  	  	  	  The	  Map-­‐	  
Reduce	  framework	  has	  been	  tremendously	  valuable,	  but	  is	  only	  a	  first	  step.	  	  Even	  declarative	  languages	  
that	   exploit	   it,	   such	   as	   Pig	   Latin,	   are	   at	   a	   rather	   low	   level	   when	   it	   comes	   to	   complex	   analysis	   tasks.	  
Similar	   declarative	   specifications	   are	   required	   at	   higher	   levels	   to	   meet	   the	   programmability	   and	  
composition	   needs	   of	   these	   analysis	   pipelines.	  	  	  	  	  Besides	   the	   basic	   technical	   need,	   there	   is	   a	   strong	  
business	  imperative	  as	  well.	  	  Businesses	  typically	  will	  outsource	  Big	  Data	  processing,	  or	  many	  aspects	  of	  
it.	  	  	  Declarative	   specifications	   are	   required	   to	   enable	   technically	   meaningful	   service	   level	   agreements,	  

12 

 

	  

	  

	  
	  
	  

	  

 

since	  the	  point	  of	  the	  out-­‐sourcing	  is	  to	  specify	  precisely	  what	  task	  will	  be	  performed	  without	  going	  into	  
details	  of	  how	  to	  do	  it.	  

Declarative	   specification	   is	   needed	   not	   just	   for	   the	   pipeline	   composition,	   but	   also	   for	   the	  
individual	  operations	  themselves.	  	   Each	  operation	  (cleaning,	   extraction,	  modeling	  etc.)	   potentially	   runs	  
on	  a	  very	  large	  data	  set.	  	  Furthermore,	  each	  operation	  itself	  is	  sufficiently	  complex	  that	  there	  are	  many	  
choices	  and	  optimizations	  possible	  in	  how	  it	  is	  implemented.	  	   In	  databases,	  there	  is	  considerable	  work	  
on	  optimizing	  individual	  operations,	  such	  as	  joins.	  	  It	  is	  well-­‐known	  that	  there	  can	  be	  multiple	  orders	  of	  
magnitude	  difference	  in	  the	  cost	  of	  two	  different	  ways	  to	  execute	  the	  same	  query.	  	  Fortunately,	  the	  user	  
does	  not	  have	  to	  make	  this	  choice	  –	  the	  database	  system	  makes	  it	  for	  her.	  	  In	  the	  case	  of	  Big	  Data,	  these	  
optimizations	   may	   be	   more	   complex	   because	   not	   all	   operations	   will	   be	   I/O	   intensive	   as	   in	   databases.	  
Some	  operations	  may	  be,	  but	  others	  may	  be	  CPU	  intensive,	  or	  a	  mix.	  	  So	  standard	  database	  optimization	  
techniques	  cannot	  directly	  be	  used.	  	   However,	  it	  should	  be	  possible	  to	  develop	  new	  techniques	  for	  Big	  
Data	  operations	  inspired	  by	  database	  techniques.	  

The	  very	  fact	  that	  Big	  Data	  analysis	  typically	  involves	  multiple	  phases	  highlights	  a	  challenge	  that	  
arises	   routinely	   in	  practice:	  	   production	  systems	  must	   run	  complex	  analytic	   pipelines,	  or	   workflows,	   at	  
routine	   intervals,	   e.g.,	   hourly	   or	   daily.	  	  	   New	   data	   must	   be	   incrementally	   accounted	   for,	   taking	   into	  
account	  	  the	  	  results	  	  of	  	  prior	  	  analysis	  	  and	  	  pre-­‐existing	  	  data.	  	  	  	  And	  	  of	  	  course,	  	  provenance	  	  must	  	  be	  
preserved,	   and	   must	   include	   the	   phases	   in	   the	   analytic	   pipeline.	  	  	   Current	   systems	   offer	   little	   to	   no	  
support	  for	  such	  Big	  Data	  pipelines,	  and	  this	  is	  in	  itself	  a	  challenging	  objective.	  

5.	  Conclusion	  

We	  have	  entered	  an	  era	  of	  Big	  Data.	  	   Through	  better	  analysis	  of	  the	  large	  volumes	  of	  data	  that	  
are	  becoming	  available,	  there	  is	  the	  potential	  for	  making	  faster	  advances	  in	  many	   scientific	  disciplines	  
and	   improving	   the	   profitability	   and	   success	   of	   many	   enterprises.	  	   However,	   many	   technical	   challenges	  
described	   in	   this	   paper	   must	   be	   addressed	   before	   this	   potential	   can	   be	   realized	   fully.	  	   The	   challenges	  
include	   not	   just	   the	   obvious	   issues	   of	   scale,	   but	   also	   heterogeneity,	   lack	   of	   structure,	   error-­‐handling,	  
privacy,	   timeliness,	   provenance,	   and	   visualization,	   at	   all	   stages	   of	   the	   analysis	   pipeline	   from	   data	  
acquisition	  to	  result	  interpretation.	  	  	  	  These	  technical	  challenges	  are	  common	  across	  a	  large	  variety	  of	  
application	   domains,	   and	   therefore	   not	   cost-­‐effective	   to	   address	   in	   the	   context	   of	   one	   domain	   alone.	  
Furthermore,	  these	  challenges	  will	  require	  transformative	  solutions,	  and	  will	  not	  be	  addressed	  naturally	  
by	  the	  next	  generation	  of	  industrial	  products.	  	  	   We	  must	  support	  and	  encourage	  fundamental	  research	  
towards	  addressing	  these	  technical	  challenges	  if	  we	  are	  to	  achieve	  the	  promised	  benefits	  of	  Big	  Data.	  

13 

 

	  

	  

	  

	  

	  

	  

	  

	  

	  

	  

	  

	  

 

Bibliography	  

[CCC2011a]	   Advancing	  Discovery	  in	  Science	  and	  Engineering.	  	  Computing	  Community	  Consortium.	  

Spring	  2011.	  

[CCC2011b]	   Advancing	  Personalized	  Education.	  	  Computing	  Community	  Consortium.	  	  Spring	  2011.	  

[CCC2011c]	  

Smart	  Health	  and	  Wellbeing.	  	  Computing	  Community	  Consortium.	   Spring	  2011.	  

[CCC2011d]	   A	  Sustainable	  Future.	   Computing	  Community	  Consortium.	  	  Summer	  2011.	  

[DF2011]	  

[Eco2011]	  

[FJ+2011]	  

Getting	  Beneath	  the	  Veil	  of	  Effective	  Schools:	  Evidence	  from	  New	  York	  City.	   Will	  Dobbie,	  
Roland	  G.	  Fryer,	  Jr.	   NBER	  Working	  Paper	  No.	  17632.	   Issued	  Dec.	  2011.	  

Drowning	  in	  numbers	  -­‐-­‐	  Digital	  data	  will	  flood	  the	  planet—and	  help	  us	  understand	  it	  
better.	   The	  Economist,	  Nov	  18,	  2011.	  
http://www.economist.com/blogs/dailychart/2011/11/big-­‐data-­‐0	  

Using	  Data	  for	  Systemic	  Financial	  Risk	  Management.	   Mark	  Flood,	  H	  V	  Jagadish,	  Albert	  
Kyle,	  Frank	  Olken,	  and	  Louiqa	  Raschid.	   Proc.	  Fifth	  Biennial	  Conf.	  Innovative	  Data	  Systems	  
Research,	  Jan.	  2011.	  

[Gar2011]	  

Pattern-­‐Based	  Strategy:	  Getting	  Value	  from	  Big	  Data.	   Gartner	  Group	  press	  release.	  	  July	  
2011.	   Available	  at	  http://www.gartner.com/it/page.jsp?id=1731916	  

[Gon2008]	  

Understanding	  individual	  human	  mobility	  patterns.	   Marta	  C.	  González,	  César	  A.	  Hidalgo,	  
and	  Albert-­‐László	  Barabási.	   Nature	  453,	  779-­‐782	  (5	  June	  2008)	  

[LP+2009]	  

Computational	  Social	  Science.	   David	  Lazer,	  Alex	  Pentland,	  Lada	  Adamic,	  Sinan	  Aral,	  
Albert-­‐László	  Barabási,	  Devon	  Brewer,Nicholas	  Christakis,	  Noshir	  Contractor,	  James	  
Fowler,	  Myron	  Gutmann,	  Tony	  Jebara,	  Gary	  King,	  Michael	  Macy,	  Deb	  Roy,	  and	  Marshall	  
Van	  Alstyne.	   Science	  6	  February	  2009:	  323	  (5915),	  721-­‐723.	  

[McK2011]	  

Big	  data:	  The	  next	  frontier	  for	  innovation,	  competition,	  and	  productivity.	  	   James	  Manyika,	  
Michael	  Chui,	  Brad	  Brown,	  Jacques	  Bughin,	  Richard	  Dobbs,	  Charles	  Roxburgh,	  and	  Angela	  
Hung	  Byers.	   McKinsey	  Global	  Institute.	   May	  2011.	  

[MGI2011]	   Materials	  Genome	  Initiative	  for	  Global	  Competitiveness.	  	  National	  Science	  and	  

Technology	  Council.	   June	  2011.	  

[NPR2011a]	   Folowing	  the	  Breadcrumbs	  to	  Big	  Data	  Gold.	  	  Yuki	  Noguchi.	  	  National	  Public	  Radio,	  Nov.	  

29,	  2011.	   	  http://www.npr.org/2011/11/29/142521910/the-­‐digital-­‐breadcrumbs-­‐that-­‐lead-­‐to-­‐big-­‐	  
data	  

[NPR2011b]	   The	  Search	  for	  Analysts	  to	  Make	  Sense	  of	  Big	  Data.	   Yuki	  Noguchi.	  	  National	  Public	  Radio,	  

Nov.	  30,	  2011.	  
http://www.npr.org/2011/11/30/142893065/the-­‐search-­‐for-­‐analysts-­‐to-­‐make-­‐sense-­‐of-­‐big-­‐data	  

[NYT2012]	  

The	  Age	  of	  Big	  Data.	  	  Steve	  Lohr.	  	  New	  York	  Times,	  Feb	  11,	  2012.	  
http://www.nytimes.com/2012/02/12/sunday-­‐review/big-­‐datas-­‐impact-­‐in-­‐the-­‐world.html	  

14 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 

	  
	  

	  

	  

	  

	  
	  
	  

	  

 

3.1	  Heterogeneity	  and	  Incompleteness	  

When	  humans	  consume	  information,	  a	  great	  deal	  of	  heterogeneity	  is	  comfortably	  tolerated.	  	  In	  
fact,	   the	   nuance	   and	   richness	   of	   natural	   language	   can	   provide	   valuable	   depth.	  	  	   However,	   machine	  
analysis	   algorithms	   expect	   homogeneous	   data,	   and	   cannot	   understand	   nuance.	  	   In	   consequence,	   data	  
must	  be	  carefully	  structured	  as	  a	  first	  step	  in	  (or	  prior	  to)	  data	  analysis.	  	  Consider,	  for	  example,	  a	  patient	  
who	  has	  multiple	  medical	  procedures	  at	  a	  hospital.	  	  We	  could	  create	  one	  record	  per	  medical	  procedure	  
or	   laboratory	   test,	   one	   record	   for	   the	   entire	   hospital	   stay,	   or	   one	   record	   for	   all	   lifetime	   hospital	  
interactions	  	  of	  	  this	  	  patient.	  	  	  	   With	  	  anything	  	  other	  	  than	  	  the	  	  first	  	  design,	  	  the	  	  number	  	  of	  	  medical	  
procedures	  and	  lab	  tests	  per	  record	  would	  be	  different	  for	  each	  patient.	  	  	  The	  three	  design	  choices	  listed	  
have	  successively	  less	  structure	  and,	  conversely,	  successively	  greater	  variety.	  	   Greater	  structure	  is	  likely	  
to	  be	  required	  by	  many	  (traditional)	  data	  analysis	  systems.	  	  However,	  the	  less	  structured	  design	  is	  likely	  
to	   be	   more	   effective	   for	   many	   purposes	   –	   for	   example	   questions	   relating	   to	   disease	   progression	   over	  
time	   will	   require	   an	   expensive	   join	   operation	   with	   the	   first	   two	   designs,	   but	   can	   be	   avoided	   with	   the	  
latter.	  	  	  	  However,	  computer	  systems	  work	  most	  efficiently	  if	  they	  can	  store	  multiple	  items	  that	  are	  all	  
identical	   in	   size	   and	   structure.	   	   Efficient	   representation,	   access,	   and	   analysis	   of	   semi-­‐structured	   data	  
require	  further	  work.	  

Consider	  an	  electronic	  health	  record	  database	  design	  that	  has	  fields	  for	  birth	  date,	  occupation,	  
and	  blood	  type	  for	  each	  patient.	  	   What	  do	  we	  do	  if	  one	  or	  more	  of	  these	  pieces	  of	  information	  is	  not	  
provided	   by	   a	   patient?	   	   	   Obviously,	   the	   health	   record	   is	   still	   placed	   in	   the	   database,	   but	   with	   the	  
corresponding	  attribute	  values	  being	  set	  to	  NULL.	  	  	  A	  data	  analysis	  that	  looks	  to	  classify	  patients	  by,	  say,	  
occupation,	   must	   take	   into	   account	   patients	   for	   which	   this	   information	   is	   not	   known.	   	   Worse,	   these	  
patients	  with	  unknown	  occupations	  can	  be	  ignored	  in	  the	  analysis	  only	  if	  we	  have	  reason	  to	  believe	  that	  
they	  	   are	  	   otherwise	  	   statistically	  	   similar	  	   to	  	   the	  	   patients	  	   with	  	   known	  	   occupation	  	   for	  	   the	  	   analysis	  
performed.	   	   For	   example,	   if	   unemployed	   patients	   are	   more	   likely	   to	   hide	   their	   employment	   status,	  
analysis	   results	   may	   be	   skewed	   in	   that	   it	   considers	   a	   more	   employed	   population	   mix	   than	   exists,	   and	  
hence	  potentially	  one	  that	  has	  differences	  in	  occupation-­‐related	  health-­‐profiles.	  

Even	  after	  data	  cleaning	  and	  error	  correction,	  some	  incompleteness	  and	  some	  errors	  in	  data	  are	  
likely	  to	  remain.	  	   This	  incompleteness	  and	  these	  errors	  must	  be	  managed	  during	  data	  analysis.	  	   Doing	  
this	   correctly	   is	   a	   challenge.	   	   Recent	   work	   on	   managing	   probabilistic	   data	   suggests	   one	   way	   to	   make	  
progress.	  

3.2	  Scale	  

Of	  course,	  the	  first	  thing	  anyone	  thinks	  of	  with	  Big	  Data	  is	  its	  size.	  	   After	  all,	  the	  word	  “big”	  is	  
there	  in	  the	  very	  name.	  	   Managing	  large	  and	  rapidly	  increasing	  volumes	  of	  data	  has	  been	  a	  challenging	  
issue	  for	  many	  decades.	  	  	  In	  the	  past,	  this	  challenge	  was	  mitigated	  by	  processors	  getting	  faster,	  following	  
Moore’s	  law,	  to	  provide	  us	  with	  the	  resources	  needed	  to	  cope	  with	  increasing	  volumes	  of	  data.	  	   But,	  

8 

 

	  

	  

	  

 

there	  is	  a	  fundamental	  shift	  underway	  now:	  data	  volume	  is	  scaling	  faster	  than	  compute	  resources,	  and	  
CPU	  speeds	  are	  static.	  

First,	  over	  the	  last	  five	  years	  the	  processor	  technology	  has	  made	  a	  dramatic	  shift	  -­‐	  rather	  than	  
processors	   doubling	   their	   clock	   cycle	   frequency	   every	   18-­‐24	   months,	   now,	   due	   to	   power	   constraints,	  
clock	  speeds	  have	  largely	  stalled	  and	  processors	  are	  being	  built	  with	  increasing	  numbers	  of	  cores.	  	  In	  the	  
past,	  large	  data	  processing	  systems	  had	  to	  worry	  about	  parallelism	  across	  nodes	  in	  a	  cluster;	  now,	  one	  
has	   to	   deal	   with	   parallelism	   within	   a	   single	   node.	  	  	  Unfortunately,	   parallel	   data	   processing	   techniques	  
that	   were	   applied	   in	   the	   past	   for	   processing	   data	   across	   nodes	   don’t	   directly	   apply	   for	   intra-­‐node	  
parallelism,	   since	   the	   architecture	   looks	   very	   different;	   for	   example,	   there	   are	   many	   more	   hardware	  
resources	   such	  as	   processor	   caches	   and	   processor	  memory	   channels	   that	   are	  shared	   across	  cores	   in	  a	  
single	   node.	  	  	  Furthermore,	   the	   move	   towards	   packing	   multiple	   sockets	   (each	   with	   10s	   of	   cores)	   adds	  
another	  level	  of	  complexity	  for	  intra-­‐node	  parallelism.	  	  Finally,	  with	  predictions	  of	  “dark	  silicon”,	  namely	  
that	  power	  consideration	  will	  likely	  in	  the	  future	  prohibit	  us	  from	  using	  all	  of	  the	  hardware	  in	  the	  system	  
continuously,	  data	  processing	  systems	  will	  likely	  have	  to	  actively	  manage	  the	  power	  consumption	  of	  the	  
processor.	  These	  unprecedented	  changes	  require	  us	  to	  rethink	  how	  we	  design,	  build	  and	  operate	  data	  
processing	  components.	  

The	   second	  dramatic	   shift	   that	   is	   underway	   is	   the	   move	   towards	   cloud	  computing,	   which	  now	  
aggregates	   multiple	   disparate	   workloads	   with	   varying	   performance	   goals	   (e.g.	   interactive	   services	  
demand	  that	  the	  data	  processing	  engine	  return	  back	  an	  answer	  within	  a	  fixed	  response	  time	  cap)	  into	  
very	  large	  clusters.	  This	  level	  of	  sharing	  of	  resources	  on	  expensive	  and	  large	  clusters	  requires	  new	  ways	  
of	   determining	   how	   to	   run	   and	   execute	   data	   processing	   jobs	   so	   that	   we	   can	   meet	   the	   goals	   of	   each	  
workload	  cost-­‐effectively,	  and	  to	  deal	  with	  system	  failures,	  which	  occur	  more	  frequently	  as	  we	  operate	  
on	   larger	   and	   larger	   clusters	   (that	   are	   required	   to	   deal	   with	   the	   rapid	   growth	   in	   data	   volumes).	  	  	  This	  
places	  	  a	  	  premium	  	  on	  	  declarative	  	  approaches	  	  to	  	  expressing	  	  programs,	  	  even	  	  those	  	  doing	  	  complex	  
machine	  learning	  tasks,	  since	  global	  optimization	  across	  multiple	  users’	  programs	  is	  necessary	  for	  good	  
overall	   performance.	   	   Reliance	   on	   user-­‐driven	   program	   optimizations	   is	   likely	   to	   lead	   to	   poor	   cluster	  
utilization,	   since	   users	   are	   unaware	   of	   other	   users’	   programs.	   	   	   	   System-­‐driven	   holistic	   optimization	  
requires	  	   programs	  	   to	  	   be	  	   sufficiently	  	   transparent,	  	   e.g.,	  	   as	  	   in	  	   relational	  	   database	  	   systems,	  	   where	  
declarative	  query	  languages	  are	  designed	  with	  this	  in	  mind.	  

A	   third	   dramatic	   shift	   that	   is	   underway	   is	   the	   transformative	   change	   of	   the	   traditional	   I/O	  
subsystem.	  For	  many	  decades,	  hard	  disk	  drives	  (HDDs)	  were	  used	  to	  store	  persistent	  data.	  HDDs	  had	  far	  
slower	  random	  IO	  performance	  than	  sequential	  IO	  performance,	  and	  data	  processing	  engines	  formatted	  
their	  data	  and	  designed	  their	  query	  processing	  methods	  to	  “work	  around”	  this	  limitation.	  But,	  HDDs	  are	  
increasingly	   being	   replaced	   by	   solid	   state	   drives	   today,	   and	   other	   technologies	   such	   as	   Phase	   Change	  
Memory	  are	  around	  the	  corner.	  These	  newer	  storage	  technologies	  do	  not	  have	  the	  same	  large	  spread	  in	  
performance	  between	  the	  sequential	  and	  random	  I/O	  performance,	  which	  requires	  a	  rethinking	  of	  how	  
we	   design	   storage	   subsystems	   for	   data	   processing	   systems.	   Implications	   of	   this	   changing	   storage	  
subsystem	   potentially	   touch	   every	   aspect	   of	   data	   processing,	   including	   query	   processing	   algorithms,	  
query	  scheduling,	  database	  design,	  concurrency	  control	  methods	  and	  recovery	  methods.	  

9 

 

	  

	  

	  

	  
	  
	  

	  

	  

 

3.3	  Timeliness	  

The	  flip	  side	  of	  size	  is	  speed.	  	   The	  larger	  the	  data	  set	  to	  be	  processed,	  the	  longer	  it	  will	  take	  to	  
analyze.	  	  The	  design	  of	  a	  system	  that	  effectively	  deals	  with	  size	  is	  likely	  also	  to	  result	  in	  a	  system	  that	  can	  
process	  a	  given	  size	  of	  data	  set	  faster.	  	  However,	  it	  is	  not	  just	  this	  speed	  that	  is	  usually	  meant	  when	  one	  
speaks	  of	  Velocity	  in	  the	  context	  of	  Big	  Data.	  	  Rather,	  there	  is	  an	  acquisition	  rate	  challenge	  as	  described	  
in	  Sec.	  2.1,	  and	  a	  timeliness	  challenge	  described	  next.	  

There	   are	   many	   situations	   in	   which	   the	   result	   of	   the	   analysis	   is	   required	   immediately.	   	   For	  
example,	   if	   a	   fraudulent	   credit	   card	   transaction	   is	   suspected,	   it	   should	   ideally	   be	   flagged	   before	   the	  
transaction	  is	  completed	  –	  potentially	  preventing	  the	  transaction	  from	  taking	  place	  at	  all.	  	   Obviously,	  a	  
full	   analysis	   of	   a	   user’s	   purchase	   history	   is	   not	   likely	   to	   be	   feasible	   in	   real-­‐time.	   	   Rather,	   we	   need	   to	  
develop	  partial	  results	  in	  advance	  so	  that	  a	  small	  amount	  of	  incremental	  computation	  with	  new	  data	  can	  
be	  used	  to	  arrive	  at	  a	  quick	  determination.	  

Given	  a	  large	  data	  set,	  it	  is	  often	  necessary	  to	  find	  elements	  in	  it	  that	  meet	  a	  specified	  criterion.	  
In	  the	  course	  of	  data	  analysis,	  this	  sort	  of	  search	  is	  likely	  to	  occur	  repeatedly.	  	   Scanning	  the	  entire	  data	  
set	  to	  find	  suitable	  elements	  is	  obviously	  impractical.	  	  Rather,	  index	  structures	  are	  created	  in	  advance	  to	  
permit	   finding	   qualifying	   elements	   quickly.	   	   The	   problem	   is	   that	   each	   index	   structure	   is	   designed	   to	  
support	  only	  some	  classes	  of	  criteria.	  	  With	  new	  analyses	  desired	  using	  Big	  Data,	  there	  are	  new	  types	  of	  
criteria	   specified,	   and	   a	   need	   to	   devise	   new	   index	   structures	   to	   support	   such	   criteria.	   	   For	   example,	  
consider	   a	   traffic	   management	   system	   with	   information	   regarding	   thousands	   of	   vehicles	   and	   local	   hot	  
spots	  on	  roadways.	  	   The	  system	  may	  need	  to	  predict	  potential	  congestion	  points	  along	  a	  route	  chosen	  
by	   a	   user,	   and	   suggest	   alternatives.	   	   Doing	   so	   requires	   evaluating	   multiple	   spatial	   proximity	   queries	  
working	   with	   the	   trajectories	   of	   moving	   objects.	   	   New	   index	   structures	   are	   required	   to	   support	   such	  
queries.	   Designing	   such	   structures	   becomes	   particularly	   challenging	   when	   the	   data	   volume	   is	   growing	  
rapidly	  and	  the	  queries	  have	  tight	  response	  time	  limits.	  

3.4	  Privacy	  

The	  privacy	  of	  data	  is	  another	  huge	  concern,	  and	  one	  that	  increases	  in	  the	  context	  of	  Big	  Data.	  
For	  electronic	  health	  records,	  there	  are	  strict	  laws	  governing	  what	  can	  and	  cannot	  be	  done.	  	   For	  other	  
data,	  regulations,	  particularly	  in	  the	  US,	  are	  less	  forceful.	  	   However,	  there	  is	  great	  public	  fear	  regarding	  
the	   inappropriate	   use	   of	   personal	   data,	   particularly	   through	   linking	   of	   data	   from	   multiple	   sources.	  
Managing	   privacy	   is	   effectively	   both	   a	   technical	   and	   a	   sociological	   problem,	  which	   must	   be	   addressed	  
jointly	  from	  both	  perspectives	  to	  realize	  the	  promise	  of	  big	  data.	  

Consider,	   for	   example,	   data	   gleaned	   from	   location-­‐based	   services.	   	   These	   new	   architectures	  
require	  a	  user	  to	  share	  his/her	  location	  with	  the	  service	  provider,	  resulting	  in	  obvious	  privacy	  concerns.	  
Note	  that	  hiding	  the	  user’s	  identity	  alone	  without	  hiding	  her	  location	  would	  not	  properly	  address	  these	  
privacy	  concerns.	  	  An	  attacker	  or	  a	  (potentially	  malicious)	  location-­‐based	  server	  can	  infer	  the	  identity	  of	  
the	  query	  source	  from	  its	  (subsequent)	  location	  information.	  For	  example,	  a	  user’s	  location	  information	  
can	  be	  tracked	  through	  several	  stationary	  connection	  points	  (e.g.,	  cell	  towers).	  	   After	  a	  while,	  the	  user	  

10 

 

	  

	  
	  
	  

	  

	  

	  

 

leaves	  “a	  trail	  of	  packet	  crumbs”	  which	  could	  be	  associated	  to	  a	  certain	  residence	  or	  office	  location	  and	  
thereby	   used	   to	   determine	   the	   user’s	   identity.	  	   Several	   other	   types	   of	   surprisingly	   private	   information	  
such	  	  as	  	  health	  	  issues	  	  (e.g.,	  	  presence	  	  in	  	  a	  	  cancer	  	  treatment	  	  center)	  	  or	  	  religious	  	  preferences	  	  (e.g.,	  
presence	   in	   a	   church)	   can	   also	   be	   revealed	   by	   just	   observing	   anonymous	   users’	   movement	   and	   usage	  
pattern	  over	  time.	  In	  general,	  Barabási	  et	  al.	  showed	  that	  there	  is	  a	  close	  correlation	  between	  people’s	  
identities	   and	   their	   movement	   patterns	   [Gon2008].	   	   Note	   that	   hiding	   a	   user	   location	   is	   much	   more	  
challenging	  than	  hiding	  his/her	  identity.	  	  This	  is	  because	  with	  location-­‐based	  services,	  the	  location	  of	  the	  
user	   is	   needed	   for	   a	   successful	   data	   access	   or	   data	   collection,	   while	   the	   identity	   of	   the	   user	   is	   not	  
necessary.	  

There	   are	   many	   additional	   challenging	   research	   problems.	  	  	  For	   example,	   we	   do	   not	   know	   yet	  
how	  to	  share	  private	  data	  while	  limiting	  disclosure	  and	  ensuring	  sufficient	  data	  utility	  in	  the	  shared	  data.	  
The	   existing	   paradigm	   of	   differential	   privacy	   is	   a	   very	   important	   step	   in	   the	   right	   direction,	   but	   it	  
unfortunately	   reduces	   information	   content	   too	   far	   in	   order	   to	   be	   useful	   in	   most	   practical	   cases.	   	   In	  
addition,	  real	  data	  is	  not	  static	  but	  gets	  larger	  and	  changes	  over	  time;	  none	  of	  the	  prevailing	  techniques	  
results	  in	  any	  useful	  content	  being	  released	  in	  this	  scenario.	  	   Yet	  another	  very	  important	  direction	  is	  to	  
rethink	  security	  for	  information	  sharing	  in	  Big	  Data	  use	  cases.	  	  Many	  online	  services	  today	  require	  us	  to	  
share	  private	  information	  (think	  of	  Facebook	  applications),	  but	  beyond	  record-­‐level	  access	  control	  we	  do	  
not	  understand	  what	  it	  means	  to	  share	  data,	  how	  the	  shared	  data	  can	  be	  linked,	  and	  how	  to	  give	  users	  
fine-­‐grained	  control	  over	  this	  sharing.	  

3.5	  Human	  Collaboration	  

In	   spite	   of	   the	   tremendous	   advances	   made	   in	   computational	   analysis,	   there	   remain	   many	  
patterns	   that	   humans	   can	   easily	   detect	   but	   computer	   algorithms	   have	   a	   hard	   time	   finding.	   	   Indeed,	  
CAPTCHAs	  exploit	  precisely	   this	  fact	  to	   tell	  human	  web	  users	  apart	  from	   computer	  programs.	  	   Ideally,	  
analytics	  for	  Big	  Data	  will	  not	  be	  all	  computational	  –	  rather	  it	  will	  be	  designed	  explicitly	  to	  have	  a	  human	  
in	  the	  loop.	  	   The	  new	  sub-­‐field	  of	  visual	  analytics	  is	  attempting	  to	  do	  this,	  at	  least	  with	  respect	  to	  the	  
modeling	  and	  analysis	  phase	  in	  the	  pipeline.	  	   There	  is	  similar	  value	  to	  human	  input	  at	  all	  stages	  of	  the	  
analysis	  pipeline.	  

In	   today’s	   complex	   world,	   it	   often	   takes	   multiple	   experts	   from	   different	   domains	   to	   really	  
understand	   what	   is	   going	   on.	   	   A	   Big	   Data	   analysis	   system	   must	   support	   input	   from	   multiple	   human	  
experts,	  and	  shared	  exploration	  of	  results.	  	  These	  multiple	  experts	  may	  be	  separated	  in	  space	  and	  time	  
when	   it	   is	   too	   expensive	   to	   assemble	   an	   entire	   team	   together	   in	   one	   room.	  	  	  The	   data	   system	   has	   to	  
accept	  this	  distributed	  expert	  input,	  and	  support	  their	  collaboration.	  

A	   popular	   new	   method	   of	   harnessing	   human	   ingenuity	   to	   solve	   problems	   is	   through	   crowd-­‐	  
sourcing.	  	  	  Wikipedia,	   the	   online	   encyclopedia,	   is	   perhaps	   the	   best	   known	   example	   of	   crowd-­‐sourced	  
data.	  	   We	  are	  relying	  upon	  information	  provided	  by	  unvetted	  strangers.	  	   Most	  often,	  what	  they	   say	   is	  
correct.	  	  	  However,	   we	   should	   expect	   there	   to	   be	   individuals	   who	   have	   other	   motives	   and	   abilities	   –	  
some	  may	  have	  a	  reason	  to	  provide	  false	  information	  in	  an	  intentional	  attempt	  to	  mislead.	  	  While	  most	  

11 

 

	  
	  
	  

	  

	  

	  

 

such	   errors	   will	   be	   detected	   and	   corrected	   by	   others	   in	   the	   crowd,	   we	   need	   technologies	   to	   facilitate	  
this.	   	   	   We	   also	   need	   a	   framework	   to	   use	   in	   analysis	   of	   such	   crowd-­‐sourced	   data	   with	   conflicting	  
statements.	  	  As	  humans,	  we	  can	  look	  at	  reviews	  of	  a	  restaurant,	  some	  of	  which	  are	  positive	  and	  others	  
critical,	  and	  come	  up	  with	  a	  summary	  assessment	  based	  on	  which	  we	  can	  decide	  whether	  to	  try	  eating	  
there.	  	  We	  need	  computers	  to	  be	  able	  to	  do	  the	  equivalent.	  	  The	  issues	  of	  uncertainty	  and	  error	  become	  
even	  more	  pronounced	  in	  a	  specific	  type	  of	  crowd-­‐sourcing,	  termed	  participatory-­‐sensing.	  	  In	  this	  case,	  
every	   person	   with	   a	   mobile	   phone	   can	   act	   as	   a	   multi-­‐modal	   sensor	   collecting	   various	   types	   of	   data	  
instantaneously	   (e.g.,	   picture,	   video,	   audio,	   location,	   time,	   speed,	   direction,	   acceleration).	   	   The	   extra	  
challenge	  here	  is	  the	  inherent	  uncertainty	  of	  the	  data	  collection	  devices.	  	  The	  fact	  that	  collected	  data	  are	  
probably	  spatially	  and	  temporally	  correlated	  can	  be	  exploited	  to	  better	  assess	  their	  correctness.	  	  When	  
crowd-­‐sourced	  data	  is	  obtained	  for	  hire,	  such	  as	  with	  “Mechanical	  Turks,”	  much	  of	  the	  data	  created	  may	  
be	   with	   a	   primary	   objective	   of	   getting	   it	   done	   quickly	   rather	   than	   correctly.	  	   This	   is	   yet	   another	   error	  
model,	  which	  must	  be	  planned	  for	  explicitly	  when	  it	  applies.	  

4.	  System	  Architecture	  

Companies	  today	  already	  use,	  and	  appreciate	  the	  value	  of,	  business	  intelligence.	  	  	  Business	  data	  
is	  analyzed	  for	  many	  purposes:	  a	  company	  may	  perform	  system	  log	  analytics	  and	  social	  media	  analytics	  
for	   risk	   assessment,	   customer	   retention,	   brand	   management,	   and	   so	   on.	  	  	  Typically,	   such	   varied	   tasks	  
have	   been	   handled	   by	   separate	   systems,	   even	   if	   each	   system	   includes	   common	   steps	   of	   information	  
extraction,	   data	   cleaning,	   relational-­‐like	   processing	   (joins,	   group-­‐by,	   aggregation),	   statistical	   and	  
predictive	  modeling,	  and	  appropriate	  exploration	  and	  visualization	  tools	  as	  shown	  in	  Fig.	  1.	  

With	  Big	  Data,	  the	  use	  of	  separate	  systems	  in	  this	  fashion	  becomes	  prohibitively	  expensive	  given	  
the	  large	  size	  of	  the	  data	  sets.	  	  	   The	  expense	  is	  due	  not	  only	  to	  the	  cost	  of	  the	  systems	  themselves,	  but	  
also	  the	  time	  to	  load	  the	  data	  into	  multiple	  systems.	  	  In	  consequence,	  Big	  Data	  has	  made	  it	  necessary	  to	  
run	   heterogeneous	   workloads	   on	   a	   single	   infrastructure	   that	   is	   sufficiently	   flexible	   to	   handle	   all	   these	  
workloads.	  	  	  	  The	   challenge	   here	   is	  not	   to	   build	   a	   system	   that	   is	   ideally	   suited	  for	   all	   processing	  tasks.	  
Instead,	  the	  need	  is	  for	  the	  underlying	  system	  architecture	  to	  be	  flexible	  enough	  that	  the	  components	  
built	  on	  top	  of	  it	  for	  expressing	  the	  various	  kinds	  of	  processing	  tasks	  can	  tune	  it	  to	  efficiently	  run	  these	  
different	  workloads.	  	  The	  effects	  of	  scale	  on	  the	  physical	  architecture	  were	  considered	  in	  Sec	  3.2.	  	  In	  this	  
section,	  we	  focus	  on	  the	  programmability	  requirements.	  

If	  users	  are	  to	  compose	  and	  build	  complex	  analytical	  pipelines	  over	  Big	  Data,	  it	  is	  essential	  that	  
they	  have	  appropriate	  high-­‐level	  primitives	  to	  specify	  their	  needs	  in	  such	  flexible	  systems.	  	  	  	  The	  Map-­‐	  
Reduce	  framework	  has	  been	  tremendously	  valuable,	  but	  is	  only	  a	  first	  step.	  	  Even	  declarative	  languages	  
that	   exploit	   it,	   such	   as	   Pig	   Latin,	   are	   at	   a	   rather	   low	   level	   when	   it	   comes	   to	   complex	   analysis	   tasks.	  
Similar	   declarative	   specifications	   are	   required	   at	   higher	   levels	   to	   meet	   the	   programmability	   and	  
composition	   needs	   of	   these	   analysis	   pipelines.	  	  	  	  	  Besides	   the	   basic	   technical	   need,	   there	   is	   a	   strong	  
business	  imperative	  as	  well.	  	  Businesses	  typically	  will	  outsource	  Big	  Data	  processing,	  or	  many	  aspects	  of	  
it.	  	  	  Declarative	   specifications	   are	   required	   to	   enable	   technically	   meaningful	   service	   level	   agreements,	  

12 

 

	  

	  

	  
	  
	  

	  

 

since	  the	  point	  of	  the	  out-­‐sourcing	  is	  to	  specify	  precisely	  what	  task	  will	  be	  performed	  without	  going	  into	  
details	  of	  how	  to	  do	  it.	  

Declarative	   specification	   is	   needed	   not	   just	   for	   the	   pipeline	   composition,	   but	   also	   for	   the	  
individual	  operations	  themselves.	  	   Each	  operation	  (cleaning,	   extraction,	  modeling	  etc.)	   potentially	   runs	  
on	  a	  very	  large	  data	  set.	  	  Furthermore,	  each	  operation	  itself	  is	  sufficiently	  complex	  that	  there	  are	  many	  
choices	  and	  optimizations	  possible	  in	  how	  it	  is	  implemented.	  	   In	  databases,	  there	  is	  considerable	  work	  
on	  optimizing	  individual	  operations,	  such	  as	  joins.	  	  It	  is	  well-­‐known	  that	  there	  can	  be	  multiple	  orders	  of	  
magnitude	  difference	  in	  the	  cost	  of	  two	  different	  ways	  to	  execute	  the	  same	  query.	  	  Fortunately,	  the	  user	  
does	  not	  have	  to	  make	  this	  choice	  –	  the	  database	  system	  makes	  it	  for	  her.	  	  In	  the	  case	  of	  Big	  Data,	  these	  
optimizations	   may	   be	   more	   complex	   because	   not	   all	   operations	   will	   be	   I/O	   intensive	   as	   in	   databases.	  
Some	  operations	  may	  be,	  but	  others	  may	  be	  CPU	  intensive,	  or	  a	  mix.	  	  So	  standard	  database	  optimization	  
techniques	  cannot	  directly	  be	  used.	  	   However,	  it	  should	  be	  possible	  to	  develop	  new	  techniques	  for	  Big	  
Data	  operations	  inspired	  by	  database	  techniques.	  

The	  very	  fact	  that	  Big	  Data	  analysis	  typically	  involves	  multiple	  phases	  highlights	  a	  challenge	  that	  
arises	   routinely	   in	  practice:	  	   production	  systems	  must	   run	  complex	  analytic	   pipelines,	  or	   workflows,	   at	  
routine	   intervals,	   e.g.,	   hourly	   or	   daily.	  	  	   New	   data	   must	   be	   incrementally	   accounted	   for,	   taking	   into	  
account	  	  the	  	  results	  	  of	  	  prior	  	  analysis	  	  and	  	  pre-­‐existing	  	  data.	  	  	  	  And	  	  of	  	  course,	  	  provenance	  	  must	  	  be	  
preserved,	   and	   must	   include	   the	   phases	   in	   the	   analytic	   pipeline.	  	  	   Current	   systems	   offer	   little	   to	   no	  
support	  for	  such	  Big	  Data	  pipelines,	  and	  this	  is	  in	  itself	  a	  challenging	  objective.	  

5.	  Conclusion	  

We	  have	  entered	  an	  era	  of	  Big	  Data.	  	   Through	  better	  analysis	  of	  the	  large	  volumes	  of	  data	  that	  
are	  becoming	  available,	  there	  is	  the	  potential	  for	  making	  faster	  advances	  in	  many	   scientific	  disciplines	  
and	   improving	   the	   profitability	   and	   success	   of	   many	   enterprises.	  	   However,	   many	   technical	   challenges	  
described	   in	   this	   paper	   must	   be	   addressed	   before	   this	   potential	   can	   be	   realized	   fully.	  	   The	   challenges	  
include	   not	   just	   the	   obvious	   issues	   of	   scale,	   but	   also	   heterogeneity,	   lack	   of	   structure,	   error-­‐handling,	  
privacy,	   timeliness,	   provenance,	   and	   visualization,	   at	   all	   stages	   of	   the	   analysis	   pipeline	   from	   data	  
acquisition	  to	  result	  interpretation.	  	  	  	  These	  technical	  challenges	  are	  common	  across	  a	  large	  variety	  of	  
application	   domains,	   and	   therefore	   not	   cost-­‐effective	   to	   address	   in	   the	   context	   of	   one	   domain	   alone.	  
Furthermore,	  these	  challenges	  will	  require	  transformative	  solutions,	  and	  will	  not	  be	  addressed	  naturally	  
by	  the	  next	  generation	  of	  industrial	  products.	  	  	   We	  must	  support	  and	  encourage	  fundamental	  research	  
towards	  addressing	  these	  technical	  challenges	  if	  we	  are	  to	  achieve	  the	  promised	  benefits	  of	  Big	  Data.	  

13 

 

	  

	  

	  

	  

	  

	  

	  

	  

	  

	  

	  

	  

 

Bibliography	  

[CCC2011a]	   Advancing	  Discovery	  in	  Science	  and	  Engineering.	  	  Computing	  Community	  Consortium.	  

Spring	  2011.	  

[CCC2011b]	   Advancing	  Personalized	  Education.	  	  Computing	  Community	  Consortium.	  	  Spring	  2011.	  

[CCC2011c]	  

Smart	  Health	  and	  Wellbeing.	  	  Computing	  Community	  Consortium.	   Spring	  2011.	  

[CCC2011d]	   A	  Sustainable	  Future.	   Computing	  Community	  Consortium.	  	  Summer	  2011.	  

[DF2011]	  

[Eco2011]	  

[FJ+2011]	  

Getting	  Beneath	  the	  Veil	  of	  Effective	  Schools:	  Evidence	  from	  New	  York	  City.	   Will	  Dobbie,	  
Roland	  G.	  Fryer,	  Jr.	   NBER	  Working	  Paper	  No.	  17632.	   Issued	  Dec.	  2011.	  

Drowning	  in	  numbers	  -­‐-­‐	  Digital	  data	  will	  flood	  the	  planet—and	  help	  us	  understand	  it	  
better.	   The	  Economist,	  Nov	  18,	  2011.	  
http://www.economist.com/blogs/dailychart/2011/11/big-­‐data-­‐0	  

Using	  Data	  for	  Systemic	  Financial	  Risk	  Management.	   Mark	  Flood,	  H	  V	  Jagadish,	  Albert	  
Kyle,	  Frank	  Olken,	  and	  Louiqa	  Raschid.	   Proc.	  Fifth	  Biennial	  Conf.	  Innovative	  Data	  Systems	  
Research,	  Jan.	  2011.	  

[Gar2011]	  

Pattern-­‐Based	  Strategy:	  Getting	  Value	  from	  Big	  Data.	   Gartner	  Group	  press	  release.	  	  July	  
2011.	   Available	  at	  http://www.gartner.com/it/page.jsp?id=1731916	  

[Gon2008]	  

Understanding	  individual	  human	  mobility	  patterns.	   Marta	  C.	  González,	  César	  A.	  Hidalgo,	  
and	  Albert-­‐László	  Barabási.	   Nature	  453,	  779-­‐782	  (5	  June	  2008)	  

[LP+2009]	  

Computational	  Social	  Science.	   David	  Lazer,	  Alex	  Pentland,	  Lada	  Adamic,	  Sinan	  Aral,	  
Albert-­‐László	  Barabási,	  Devon	  Brewer,Nicholas	  Christakis,	  Noshir	  Contractor,	  James	  
Fowler,	  Myron	  Gutmann,	  Tony	  Jebara,	  Gary	  King,	  Michael	  Macy,	  Deb	  Roy,	  and	  Marshall	  
Van	  Alstyne.	   Science	  6	  February	  2009:	  323	  (5915),	  721-­‐723.	  

[McK2011]	  

Big	  data:	  The	  next	  frontier	  for	  innovation,	  competition,	  and	  productivity.	  	   James	  Manyika,	  
Michael	  Chui,	  Brad	  Brown,	  Jacques	  Bughin,	  Richard	  Dobbs,	  Charles	  Roxburgh,	  and	  Angela	  
Hung	  Byers.	   McKinsey	  Global	  Institute.	   May	  2011.	  

[MGI2011]	   Materials	  Genome	  Initiative	  for	  Global	  Competitiveness.	  	  National	  Science	  and	  

Technology	  Council.	   June	  2011.	  

[NPR2011a]	   Folowing	  the	  Breadcrumbs	  to	  Big	  Data	  Gold.	  	  Yuki	  Noguchi.	  	  National	  Public	  Radio,	  Nov.	  

29,	  2011.	   	  http://www.npr.org/2011/11/29/142521910/the-­‐digital-­‐breadcrumbs-­‐that-­‐lead-­‐to-­‐big-­‐	  
data	  

[NPR2011b]	   The	  Search	  for	  Analysts	  to	  Make	  Sense	  of	  Big	  Data.	   Yuki	  Noguchi.	  	  National	  Public	  Radio,	  

Nov.	  30,	  2011.	  
http://www.npr.org/2011/11/30/142893065/the-­‐search-­‐for-­‐analysts-­‐to-­‐make-­‐sense-­‐of-­‐big-­‐data	  

[NYT2012]	  

The	  Age	  of	  Big	  Data.	  	  Steve	  Lohr.	  	  New	  York	  Times,	  Feb	  11,	  2012.	  
http://www.nytimes.com/2012/02/12/sunday-­‐review/big-­‐datas-­‐impact-­‐in-­‐the-­‐world.html	  

14 

 

	  

 

[PCAST2010]	  	   Designing	  a	  Digital	  Future:	  Federally	  Funded	  Research	  and	  Development	  in	  Networking	  

and	  Information	  Technology.	   PCAST	  Report,	  Dec.	  2010.	  	  Available	  at	  
http://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-­‐nitrd-­‐report-­‐2010.pdf	  

[SDSS2008]	  

SDSS-­‐III:	  Massive	  Spectroscopic	  Surveys	  of	  the	  Distant	  Universe,	  the	  Milky	  Way	  Galaxy,	  
and	  Extra-­‐Solar	  Planetary	  Systems.	   Jan.	  2008.	  	  Available	  at	  
http://www.sdss3.org/collaboration/description.pdf	  

15 

 
 
 

	  
	  

	  
	  

	  

	  

	  

Challenges	  and	  Opportunities	  with	  Big	  Data	  

A	  community	  white	  paper	  developed	  by	  leading	  researchers	  across	  the	  United	  States	  

Executive	  Summary	  

The	   promise	   of	   data-­‐driven	   decision-­‐making	   is	   now	   being	   recognized	   broadly,	   and	   there	   is	  
growing	  enthusiasm	  for	  the	  notion	  of	  ``Big	  Data.’’	  	  While	  the	  promise	  of	  Big	  Data	  is	  real	  -­‐-­‐	  for	  example,	  it	  
is	   estimated	   that	   Google	   alone	   contributed	   54	   billion	   dollars	   to	   the	   US	   economy	   in	   2009	   -­‐-­‐	   there	   is	  
currently	  a	  wide	  gap	  between	  its	  potential	  and	  its	  realization.	  

Heterogeneity,	  	  scale,	  	  timeliness,	  	  complexity,	  	  and	  	  privacy	  	  problems	  	  with	  	  Big	  	  Data	  	  impede	  
progress	   at	   all	   phases	  of	   the	   pipeline	   that	  can	   create	   value	   from	  data.	  	   The	   problems	   start	   right	   away	  
during	   data	   acquisition,	   when	   the	   data	   tsunami	   requires	   us	   to	   make	   decisions,	   currently	   in	   an	   ad	   hoc	  
manner,	  about	  what	  data	  to	  keep	  and	  what	  to	  discard,	  and	  how	  to	  store	  what	  we	  keep	  reliably	  with	  the	  
right	  metadata.	  	  Much	  data	  today	  is	  not	  natively	  in	  structured	  format;	  for	  example,	  tweets	  and	  blogs	  are	  
weakly	  structured	  pieces	  of	  text,	  while	  images	  and	  video	  are	  structured	  for	  storage	  and	  display,	  but	  not	  
for	  semantic	  content	  and	  search:	  transforming	  such	  content	  into	  a	  structured	  format	  for	  later	  analysis	  is	  
a	   major	   challenge.	   	   	   The	   value	   of	   data	   explodes	   when	   it	   can	   be	   linked	   with	   other	   data,	   thus	   data	  
integration	  is	  a	  major	  creator	  of	  value.	  	  Since	  most	  data	  is	  directly	  generated	  in	  digital	  format	  today,	  we	  
have	  the	  opportunity	  and	  the	  challenge	  both	  to	  influence	  the	  creation	  to	  facilitate	  later	  linkage	  and	  to	  
automatically	  link	  previously	  created	  data.	  	  Data	  analysis,	  organization,	  retrieval,	  and	  modeling	  are	  other	  
foundational	   challenges.	  	   Data	   analysis	   is	   a	   clear	   bottleneck	   in	   many	   applications,	   both	   due	   to	   lack	   of	  
scalability	  of	  the	  underlying	  algorithms	  and	  due	  to	  the	  complexity	  of	  the	  data	  that	  needs	  to	  be	  analyzed.	  
Finally,	   presentation	  of	   the	   results	   and	   its	   interpretation	   by	   non-­‐technical	   domain	   experts	   is	   crucial	   to	  
extracting	  actionable	  knowledge.	  

During	  the	  last	  35	  years,	  data	  management	  principles	  such	  as	  physical	  and	  logical	  independence,	  
declarative	   querying	   and	   cost-­‐based	   optimization	   have	   led,	   during	   the	   last	   35	   years,	   to	   a	   multi-­‐billion	  
dollar	   industry.	  	   More	   importantly,	   these	   technical	   advances	   have	   enabled	   the	   first	   round	   of	   business	  
intelligence	  applications	  and	  laid	  the	  foundation	  for	  managing	  and	  analyzing	  Big	  Data	  today.	  	  The	  many	  
novel	   challenges	   and	   opportunities	   associated	   with	   Big	   Data	   necessitate	   rethinking	   many	   aspects	   of	  
these	  	   data	  	   management	  	   platforms,	  	   while	  	   retaining	  	   other	  	   desirable	  	   aspects.	  	  	  	  	  	  	   We	  	   believe	  	   that	  
appropriate	  investment	  in	  Big	  Data	  will	  lead	  to	  a	  new	  wave	  of	  fundamental	  technological	  advances	  that	  
will	  be	  embodied	  in	  the	  next	  generations	  of	  Big	  Data	  management	  and	  analysis	  platforms,	  products,	  and	  
systems.	  

We	   believe	   that	   these	   research	   problems	   are	   not	   only	   timely,	   but	   also	   have	   the	   potential	   to	  
create	   huge	   economic	   value	   in	   the	   US	   economy	   for	   years	   to	   come.	  	  	  	  	  However,	   they	   are	   also	   hard,	  
requiring	   us	   to	   rethink	   data	   analysis	   systems	   in	   fundamental	   ways.	   	   A	   major	   investment	   in	   Big	   Data,	  
properly	   directed,	   can	   result	   not	   only	   in	   major	   scientific	   advances,	   but	   also	   lay	   the	   foundation	   for	   the	  
next	  generation	  of	  advances	  in	  science,	  medicine,	  and	  business.	  

	  

 
	  
	  
	  

	  

	  

	  

	  

	  

 

Challenges	  and	  Opportunities	  with	  Big	  Data	  

1.	  Introduction	  

We	   are	   awash	   in	   a	   flood	   of	   data	   today.	   	   In	   a	   broad	   range	   of	   application	   areas,	   data	   is	   being	  
collected	   at	   unprecedented	   scale.	   	   	   Decisions	   that	   previously	   were	   based	   on	   guesswork,	   or	   on	  
painstakingly	  constructed	  models	  of	  reality,	  can	  now	  be	  made	  based	  on	  the	  data	  itself.	  	   Such	  Big	  Data	  
analysis	   now	   drives	   nearly	   every	   aspect	   of	   our	   modern	   society,	   including	   mobile	   services,	   retail,	  
manufacturing,	  financial	  services,	  life	  sciences,	  and	  physical	  sciences.	  

Scientific	  research	  has	  been	  revolutionized	  by	  Big	  Data	  [CCC2011a].	  	  The	  Sloan	  Digital	  Sky	  Survey	  
[SDSS2008]	  	  has	  	  today	  	  become	  	  a	  	  central	  	  resource	  	  for	  	  astronomers	  	  the	  	  world	  	  over.	  	  	  	   The	  	  field	  	  of	  
Astronomy	   is	   being	   transformed	   from	   one	   where	   taking	   pictures	   of	   the	   sky	   was	   a	   large	   part	   of	   an	  
astronomer’s	  job	  to	  one	  where	  the	  pictures	  are	  all	  in	  a	  database	  already	  and	  the	  astronomer’s	  task	  is	  to	  
find	  interesting	  objects	  and	  phenomena	  in	  the	  database.	  	  In	  the	  biological	  sciences,	  there	  is	  now	  a	  well-­‐	  
established	   tradition	   of	   depositing	   scientific	   data	   into	   a	   public	   repository,	   and	   also	   of	   creating	   public	  
databases	  for	  use	  by	  other	  scientists.	  	  In	  fact,	  there	  is	  an	  entire	  discipline	  of	  bioinformatics	  that	  is	  largely	  
devoted	  to	  the	  curation	  and	  analysis	  of	  such	  data.	  	  As	  technology	  advances,	  particularly	  with	  the	  advent	  
of	   Next	   Generation	   Sequencing,	   the	   size	   and	   number	   of	   experimental	   data	   sets	   available	   is	   increasing	  
exponentially.	  

Big	  Data	  has	  the	  potential	  to	  revolutionize	  not	  just	  research,	  but	  also	  education	  [CCC2011b].	  	   A	  
recent	  detailed	  quantitative	  comparison	  of	  different	  approaches	  taken	  by	  35	  charter	  schools	  in	  NYC	  has	  
found	  that	  one	  of	  the	  top	  five	  policies	  correlated	  with	  measurable	  academic	  effectiveness	  was	  the	  use	  of	  
data	  to	  guide	  instruction	  [DF2011].	  	  Imagine	  a	  world	  in	  which	  we	  have	  access	  to	  a	  huge	  database	  where	  
we	  collect	  every	  detailed	  measure	  of	  every	  student's	  academic	  performance.	  	  This	  data	  could	  be	  used	  to	  
design	  	  the	  	  most	  	  effective	  	  approaches	  	  to	  	  education,	  	  starting	  	  from	  	  reading,	  	  writing,	  	  and	  	  math,	  	  to	  
advanced,	   college-­‐level,	   courses.	  	   We	   are	  far	   from	   having	   access	   to	   such	   data,	  but	   there	  are	   powerful	  
trends	  	  in	  	  this	  	  direction.	  	  	  	  	  	   In	  	  particular,	  	  there	  	  is	  	  a	  	  strong	  	  trend	  	  for	  	  massive	  	  Web	  	  deployment	  	  of	  
educational	  	  activities,	  	  and	  	  this	  	  will	  	  generate	  	  an	  	  increasingly	  	  large	  	  amount	  	  of	  	  detailed	  	  data	  	  about	  
students'	  performance.	  

It	   is	   widely	   believed	  that	   the	   use	  of	   information	   technology	   can	   reduce	  the	   cost	   of	   healthcare	  
while	  improving	  its	  quality	  [CCC2011c],	  by	  making	  care	  more	  preventive	  and	  personalized	  and	  basing	  it	  
on	  more	  extensive	  (home-­‐based)	  continuous	  monitoring.	  	  	  	  McKinsey	  estimates	  [McK2011]	  a	  savings	  of	  
300	  billion	  dollars	  every	  year	  in	  the	  US	  alone.	  

In	   a	   similar	   vein,	   there	   have	   been	   persuasive	   cases	   made	   for	   the	   value	   of	   Big	   Data	   for	   urban	  
planning	  (through	  fusion	  of	  high-­‐fidelity	  geographical	  data),	  intelligent	  transportation	  (through	  analysis	  
and	   visualization	   of	   live	   and	   detailed	   road	   network	   data),	   environmental	   modeling	   (through	   sensor	  
networks	   ubiquitously	   collecting	   data)	   [CCC2011d],	   energy	   saving	   (through	   unveiling	   patterns	   of	   use),	  
smart	  materials	  (through	  the	  new	  materials	  genome	  initiative	  [MGI2011]),	  computational	  social	  sciences	  

1 

 

	  

	  

	  

 

(a	   new	   methodology	   fast	   growing	   in	   popularity	   because	   of	   the	   dramatically	   lowered	   cost	   of	  obtaining	  
data)	   [LP+2009],	   financial	  systemic	   risk	  	   analysis	   (through	   integrated	   analysis	   of	   a	   web	   of	   contracts	   to	  
find	   dependencies	   between	   financial	   entities)	   [FJ+2011],	   homeland	   security	   (through	   analysis	   of	   social	  
networks	  and	  financial	  transactions	  of	  possible	  terrorists),	  computer	  security	  (through	  analysis	  of	  logged	  
information	   and	   other	   events,	   known	   as	   Security	   Information	   and	   Event	   Management	   (SIEM)),	   and	   so	  
on.	  

In	   2010,	   enterprises	   and	   users	   stored	   more	   than	   13	   exabytes	   of	   new	   data;	   this	  is	   over	   50,000	  
times	  	  the	  	  data	  	  in	  	  the	  	  Library	  	  of	  	  Congress.	  	  The	  	  potential	  	  value	  	  of	  	  global	  	  personal	  	  location	  	  data	  	  is	  
estimated	   to	   be	   $700	   billion	   to	   end	   users,	   and	   it	   can	   result	   in	   an	   up	   to	   50%	   decrease	   in	   product	  
development	  and	  assembly	  costs,	  according	  to	  a	  recent	  McKinsey	  report	  [McK2011].	  	  McKinsey	  predicts	  
an	  	  equally	  	  great	  	  effect	  	  of	  	  Big	  	  Data	  	  in	  	  employment,	  	  where	  	  140,000-­‐190,000	  	  workers	  	  with	  	  “deep	  
analytical”	  experience	  will	  be	  needed	  in	  the	  US;	  furthermore,	  1.5	  million	  managers	  will	  need	  to	  become	  
data-­‐literate.	  	  	  	  Not	  	  surprisingly,	  	  the	  	  recent	  	  PCAST	  	  report	  	  on	  	  Networking	  	  and	  	  IT	  	  R&D	  	  [PCAST2010]	  
identified	   Big	   Data	   as	   a	   “research	   frontier”	   that	   can	   “accelerate	   progress	   across	   a	   broad	   range	   of	  
priorities.”	  	  Even	  popular	  news	  media	  now	  appreciates	  the	  value	  of	  Big	  Data	  as	  evidenced	  by	  coverage	  in	  
the	   Economist	   [Eco2011],	   the	   New	   York	   Times	   [NYT2012],	   and	   National	   Public	   Radio	   [NPR2011a,	  
NPR2011b].	  

While	  the	  potential	  benefits	  of	  Big	  Data	  are	  real	  and	  significant,	  and	  some	  initial	  successes	  have	  
already	   been	   achieved	   (such	   as	   the	   Sloan	   Digital	   Sky	   Survey),	   there	   remain	   many	   technical	   challenges	  
that	  must	  be	  addressed	  to	  fully	  realize	  this	  potential.	  	  	   The	  sheer	  size	  of	  the	  data,	  of	  course,	  is	  a	  major	  
challenge,	  and	  is	  the	  one	  that	  is	  most	  easily	  recognized.	  	   However,	  there	  are	  others.	  	   Industry	  analysis	  
companies	  like	  to	  point	  out	  that	  there	  are	  challenges	  not	  just	  in	  Volume,	  but	  also	  in	  Variety	  and	  Velocity	  
[Gar2011],	  and	  that	  companies	  should	  not	  focus	  on	  just	  the	  first	  of	  these.	  	  By	  Variety,	  they	  usually	  mean	  
heterogeneity	  of	  data	  types,	  representation,	  and	  semantic	  interpretation.	  	   By	  Velocity,	  they	  mean	  both	  
the	   rate	   at	   which	   data	   arrive	   and	   the	   time	   in	   which	   it	   must	   be	   acted	   upon.	   	   While	   these	   three	   are	  
important,	  this	  short	  list	  fails	  to	  include	  additional	  important	  requirements	  such	  as	  privacy	  and	  usability.	  

The	  analysis	  of	  Big	  Data	  involves	  multiple	  distinct	  phases	  as	  shown	  in	  the	  figure	  below,	  each	  of	  
which	   introduces	   challenges.	  	  	  	   Many	   people	   unfortunately	   focus	   just	   on	   the	   analysis/modeling	   phase:	  
while	  that	  phase	  is	  crucial,	  it	  is	  of	  little	  use	  without	  the	  other	  phases	  of	  the	  data	  analysis	  pipeline.	  	  Even	  
in	  the	  analysis	  phase,	  which	  has	  received	  much	  attention,	  there	  are	  poorly	  understood	  complexities	  in	  
the	  context	  of	  multi-­‐tenanted	  clusters	  where	  several	  users’	  programs	  run	  concurrently.	   Many	  significant	  
challenges	   extend	   beyond	   the	   analysis	   phase.	  	  	  For	   example,	   Big	   Data	   has	   to	   be	   managed	   in	   context,	  
which	  may	  be	  noisy,	  heterogeneous	  and	  not	  include	  an	  upfront	  model.	  Doing	  so	  raises	  the	  need	  to	  track	  
provenance	   and	   to	   handle	   uncertainty	   and	   error:	   topics	   that	   are	   crucial	   to	   success,	   and	   yet	   rarely	  
mentioned	   in	   the	   same	   breath	   as	   Big	   Data.	   	   Similarly,	   the	   questions	   to	   the	   data	   analysis	   pipeline	   will	  
typically	  not	  all	  be	  laid	  out	  in	  advance.	  	  	   We	  may	  need	  to	  figure	  out	  good	  questions	  based	  on	  the	  data.	  
Doing	   this	   will	   require	   smarter	   systems	   and	   also	   better	   support	   for	   user	   interaction	   with	   the	   analysis	  
pipeline.	   	   In	   fact,	   we	   currently	   have	   a	   major	   bottleneck	   in	   the	   number	   of	   people	   empowered	   to	   ask	  
questions	  of	  the	  data	  and	  analyze	  it	  [NYT2012].	  	  We	  can	  drastically	  increase	  this	  number	  by	  supporting	  

2 

 

	  
	  
	  

 
 
 

	  

 

many	   levels	   of	   engagement	   with	   the	   data,	   not	   all	   requiring	   deep	   database	   expertise.	   	   Solutions	   to	  
problems	   such	   as	   this	   will	   not	   come	   from	   incremental	   improvements	   to	   business	   as	   usual	   such	   as	  
industry	  may	  make	  on	  its	  own.	  	  Rather,	  they	  require	  us	  to	  fundamentally	  rethink	  how	  we	  manage	  data	  
analysis.	  

 

Fortunately,	   existing	   computational	   techniques	   can	   be	   applied,	   either	   as	   is	   or	   with	   some	  
extensions,	  to	  at	  least	  some	  aspects	  of	  the	  Big	  Data	  problem.	  	  	  	  For	  example,	  relational	  databases	  rely	  on	  
the	  notion	  of	  logical	  data	  independence:	  users	  can	  think	  about	  what	  they	  want	  to	  compute,	  while	  the	  
system	   (with	   skilled	   engineers	   designing	   those	   systems)	   determines	   how	   to	   compute	   it	   efficiently.	  
Similarly,	   the	   SQL	   standard	   and	   the	   relational	   data	   model	   provide	   a	   uniform,	   powerful	   language	   to	  
express	   many	   query	   needs	   and,	   in	   principle,	   allows	   customers	   to	   choose	   between	   vendors,	   increasing	  
competition.	   The	   challenge	   ahead	   of	   us	   is	   to	   combine	   these	   healthy	   features	   of	   prior	   systems	   as	   we	  
devise	  novel	  solutions	  to	  the	  many	  new	  challenges	  of	  Big	  Data.	  

In	   this	   paper,	   we	   consider	   each	   of	   the	   boxes	   in	   the	   figure	   above,	   and	   discuss	   both	   what	   has	  
already	  been	  done	  and	  what	  challenges	  remain	  as	  we	  seek	  to	  exploit	  Big	  Data.	  	  We	  begin	  by	  considering	  

3 

 

	  
	  
	  

	  

	  

	  

	  

	  
	  
	  

	  

 

the	   five	   stages	   in	   the	   pipeline,	   then	   move	   on	   to	   the	   five	   cross-­‐cutting	   challenges,	   and	   end	   with	   a	  
discussion	  of	  the	  architecture	  of	  the	  overall	  system	  that	  combines	  all	  these	  functions.	  

2.	  Phases	  in	  the	  Processing	  Pipeline	  

2.1	  Data	  Acquisition	  and	  Recording	  

Big	  Data	  does	  not	  arise	  out	  of	  a	  vacuum:	  it	  is	  recorded	  from	  some	  data	  generating	  source.	  	   For	  
example,	   consider	   our	   ability	   to	   sense	   and	   observe	   the	   world	   around	   us,	   from	   the	   heart	   rate	   of	   an	  
elderly	   citizen,	   and	   presence	   of	   toxins	   in	   the	   air	   we	   breathe,	   to	   the	   planned	   square	   kilometer	   array	  
telescope,	   which	   will	   produce	   up	   to	   1	   million	   terabytes	   of	   raw	   data	   per	   day.	   	   	   Similarly,	   scientific	  
experiments	  and	  simulations	  can	  easily	  produce	  petabytes	  of	  data	  today.	  

Much	  	  of	  	  this	  	  data	  	  is	  	  of	  	  no	  	  interest,	  	  and	  	  it	  	  can	  	  be	  	  filtered	  	  and	  	  compressed	  	  by	  	  orders	  	  of	  
magnitude.	   One	   challenge	   is	   to	   define	   these	   filters	   in	   such	   a	   way	   that	   they	   do	   not	   discard	   useful	  
information.	  	   For	  example,	  suppose	  one	  sensor	  reading	  differs	  substantially	  from	  the	  rest:	  it	  is	  likely	  to	  
be	   due	   to	   the	   sensor	   being	   faulty,	   but	   how	   can	   we	   be	   sure	   that	   it	   is	   not	   an	   artifact	   that	   deserves	  
attention?	   	   In	   addition,	   the	   data	   collected	   by	   these	   sensors	   most	   often	   are	   spatially	   and	   temporally	  
correlated	   (e.g.,	   traffic	   sensors	  on	   the	   same	   road	   segment).	  	   We	   need	   research	  in	   the	   science	  of	   data	  
reduction	  that	  can	  intelligently	  process	  this	  raw	  data	  to	  a	  size	  that	  its	  users	  can	  handle	  while	  not	  missing	  
the	  needle	  in	  the	  haystack.	   Furthermore,	  we	  require	  “on-­‐line”	  analysis	  techniques	  that	  can	  process	  such	  
streaming	  data	  on	  the	  fly,	  since	  we	  cannot	  afford	  to	  store	  first	  and	  reduce	  afterward.	  

The	  second	  big	  challenge	  is	  to	  automatically	  generate	  the	  right	  metadata	  to	  describe	  what	  data	  
is	  recorded	  and	  how	  it	  is	  recorded	  and	  measured.	  	   For	  example,	  in	  scientific	  experiments,	  considerable	  
detail	  regarding	  specific	  experimental	  conditions	  and	  procedures	  may	  be	  required	  to	  be	  able	  to	  interpret	  
the	   results	   correctly,	   and	   it	   is	   important	   that	   such	   metadata	   be	   recorded	   with	   observational	   data.	  
Metadata	   acquisition	   systems	   can	   minimize	   the	   human	   burden	   in	   recording	   metadata.	   	   	   Another	  
important	  issue	  here	  is	  data	  provenance.	  	  Recording	  information	  about	  the	  data	  at	  its	  birth	  is	  not	  useful	  
unless	   this	   information	   can	   be	   interpreted	   and	   carried	   along	   through	   the	   data	   analysis	   pipeline.	   	   For	  
example,	   a	   processing	   error	   at	   one	   step	   can	   render	   subsequent	   analysis	   useless;	   with	   suitable	  
provenance,	  we	  can	  easily	  identify	  all	  subsequent	  processing	  that	  dependent	  on	  this	  step.	   Thus	  we	  need	  
research	   both	   into	   generating	   suitable	   metadata	   and	   into	   data	   systems	   that	   carry	   the	   provenance	   of	  
data	  and	  its	  metadata	  through	  data	  analysis	  pipelines.	  

2.2	  Information	  Extraction	  and	  Cleaning	  

Frequently,	   the	   information	   collected	   will	   not	   be	   in	   a	   format	   ready	   for	   analysis.	  	   For	   example,	  
consider	  the	  collection	  of	  electronic	  health	  records	  in	  a	  hospital,	  comprising	  transcribed	  dictations	  from	  
several	   physicians,	   structured	   data	   from	   sensors	   and	   measurements	   (possibly	   with	   some	   associated	  
uncertainty),	  and	  image	  data	  such	  as	  x-­‐rays.	  We	  cannot	  leave	  the	  data	  in	  this	  form	  and	  still	  effectively	  

4 

 

	  

	  
	  
	  

	  

	  

	  

 

analyze	  it.	  	  Rather	  we	  require	  an	  information	  extraction	  process	  that	  pulls	  out	  the	  required	  information	  
from	   the	   underlying	   sources	   and	   expresses	   it	   in	   a	   structured	   form	   suitable	   for	   analysis.	   	   Doing	   this	  
correctly	   and	   completely	   is	   a	   continuing	   technical	   challenge.	  	   Note	   that	  this	   data	   also	   includes	   images	  
and	  will	  in	  the	  future	  include	  video;	  such	  extraction	  is	  often	  highly	  application	  dependent	  (e.g.,	  what	  you	  
want	  to	  pull	  out	  of	  an	  MRI	  is	  very	  different	  from	  what	  you	  would	  pull	  out	  of	  a	  picture	  of	  the	  stars,	  or	  a	  
surveillance	   photo).	   	   In	   addition,	   due	   to	   the	   ubiquity	   of	   surveillance	   cameras	   and	   popularity	   of	   GPS-­‐	  
enabled	  	  mobile	  	  phones,	  	  cameras,	  	  and	  	  other	  	  portable	  	  devices,	  	  rich	  	  and	  	  high	  	  fidelity	  	  location	  	  and	  
trajectory	  (i.e.,	  movement	  in	  space)	  data	  can	  also	  be	  extracted.	  

We	  are	  used	  to	  thinking	  of	  Big	  Data	  as	  always	  telling	  us	  the	  truth,	  but	  this	  is	  actually	  far	  from	  
reality.	   For	   example,	   patients	   may	   choose	   to	   hide	   risky	   behavior	   and	   caregivers	   may	   sometimes	   mis-­‐	  
diagnose	   a	  condition;	   patients	  may	   also	   inaccurately	   recall	  the	   name	  of	  a	   drug	   or	  even	   that	  they	   ever	  
took	  it,	  leading	  to	  missing	  information	  in	  (the	  history	  portion	  of)	  their	  medical	  record.	  Existing	  work	  on	  
data	   cleaning	   assumes	   well-­‐recognized	   constraints	   on	   valid	   data	   or	   well-­‐understood	   error	   models;	   for	  
many	  emerging	  Big	  Data	  domains	  these	  do	  not	  exist.	  

2.3	  Data	  Integration,	  Aggregation,	  and	  Representation	  

Given	  the	  heterogeneity	  of	  the	  flood	  of	  data,	  it	  is	  not	  enough	  merely	  to	  record	  it	  and	  throw	  it	  
into	  a	  repository.	  	  	   Consider,	  for	  example,	  data	  from	  a	  range	  of	  scientific	  experiments.	  	  If	  we	  just	  have	  a	  
bunch	  of	  data	  sets	  in	  a	  repository,	  it	  is	  unlikely	  anyone	  will	  ever	  be	  able	  to	  find,	  let	  alone	  reuse,	  any	  of	  
this	   data.	   	   With	   adequate	   metadata,	   there	   is	   some	   hope,	   but	   even	   so,	   challenges	   will	   remain	   due	   to	  
differences	  in	  experimental	  details	  and	  in	  data	  record	  structure.	  

Data	  analysis	  is	  considerably	  more	  challenging	  than	  simply	  locating,	  identifying,	  understanding,	  
and	   citing	  data.	  	   For	  effective	   large-­‐scale	  analysis	   all	  of	   this	  has	  to	  happen	   in	   a	  completely	   automated	  
manner.	  	  	  This	   requires	   differences	   in	   data	   structure	   and	   semantics	   to	   be	   expressed	   in	   forms	   that	   are	  
computer	   understandable,	   and	   then	   “robotically”	   resolvable.	   	   There	   is	   a	   strong	   body	   of	   work	   in	   data	  
integration	  that	  can	  provide	  some	  of	  the	  answers.	  	  However,	  considerable	  additional	  work	  is	  required	  to	  
achieve	  automated	  error-­‐free	  difference	  resolution.	  

Even	  	  for	  	  simpler	  	  analyses	  	  that	  	  depend	  	  on	  	  only	  	  one	  	  data	  	  set,	  	  there	  	  remains	  	  an	  	  important	  
question	  of	  suitable	  database	  design.	  	  Usually,	  there	  will	  be	  many	  alternative	  ways	  in	  which	  to	  store	  the	  
same	  information.	  	   Certain	  designs	  will	  have	  advantages	  over	  others	  for	  certain	  purposes,	  and	  possibly	  
drawbacks	   for	   other	   purposes.	   	   	   Witness,	   for	   instance,	   the	   tremendous	   variety	   in	   the	   structure	   of	  
bioinformatics	   databases	   with	   information	   regarding	   substantially	   similar	   entities,	   such	   as	   genes.	  
Database	   design	   is	   today	   an	   art,	   and	   is	   carefully	   executed	   in	   the	   enterprise	   context	   by	   highly-­‐paid	  
professionals.	   	   	   We	   must	   enable	   other	   professionals,	   such	   as	   domain	   scientists,	   to	   create	   effective	  
database	  designs,	  either	  through	  devising	  tools	  to	  assist	  them	  in	  the	  design	  process	  or	  through	  forgoing	  
the	  design	  process	  completely	  and	  developing	  techniques	  so	  that	  databases	  can	  be	  used	  effectively	   in	  
the	  absence	  of	  intelligent	  database	  design.	  

5 

 

	  

	  

	  

	  

	  
	  
	  

	  

 

2.4	  Query	  Processing,	  Data	  Modeling,	  and	  Analysis	  

Methods	  	   for	  	   querying	  	   and	  	   mining	  	   Big	  	   Data	  	   are	  	   fundamentally	  	   different	  	   from	  	   traditional	  
statistical	  analysis	  on	  small	  samples.	  	  Big	  Data	  is	  often	  noisy,	  dynamic,	  heterogeneous,	  inter-­‐related	  and	  
untrustworthy.	  	  	  	  Nevertheless,	  even	  noisy	  Big	  Data	  could	  be	  more	  valuable	  than	  tiny	  samples	  because	  
general	  statistics	  obtained	  from	  frequent	  patterns	  and	  correlation	  analysis	  usually	  overpower	  individual	  
fluctuations	  and	  often	  disclose	  more	  reliable	  hidden	  patterns	  and	  knowledge.	  	   Further,	  interconnected	  
Big	  Data	  forms	  large	  heterogeneous	  information	  networks,	  with	  which	  information	  redundancy	  can	  be	  
explored	   to	   compensate	   for	   missing	   data,	   to	   crosscheck	   conflicting	   cases,	   to	   validate	   trustworthy	  
relationships,	  to	  disclose	  inherent	  clusters,	  and	  to	  uncover	  hidden	  relationships	  and	  models.	  

Mining	   requires	   integrated,	   cleaned,	   trustworthy,	   and	   efficiently	   accessible	   data,	   declarative	  
query	  and	  mining	  interfaces,	  scalable	  mining	  algorithms,	  and	  big-­‐data	  computing	  environments.	  	  	  At	  the	  
same	  time,	  data	  mining	  itself	  can	  also	  be	  used	  to	  help	  improve	  the	  quality	  and	  trustworthiness	  of	  the	  
data,	  understand	  its	  semantics,	  and	  provide	  intelligent	  querying	  functions.	  	  	  As	  noted	  previously,	  real-­‐life	  
medical	  records	  have	  errors,	  are	  heterogeneous,	  and	  frequently	  are	  distributed	  across	  multiple	  systems.	  
The	  value	  of	  Big	  Data	  analysis	  in	  health	  care,	  to	  take	  just	  one	  example	  application	  domain,	  can	  only	  be	  
realized	   if	   it	   can	   be	   applied	   robustly	   under	   these	   difficult	   conditions.	   	   On	   the	   flip	   side,	   knowledge	  
developed	   from	   data	   can	  help	   in	   correcting	   errors	   and	   removing	   ambiguity.	  	   For	   example,	   a	   physician	  
may	  write	  “DVT”	  as	  the	  diagnosis	  for	  a	  patient.	  	  This	  abbreviation	  is	  commonly	  used	  for	  both	  “deep	  vein	  
thrombosis”	  and	  “diverticulitis,”	  two	  very	  different	  medical	  conditions.	  	  	   A	  knowledge-­‐base	  constructed	  
from	  related	  data	  can	  use	  associated	  symptoms	  or	  medications	  to	  determine	  which	  of	  two	  the	  physician	  
meant.	  

Big	  Data	  is	  also	  enabling	  the	  next	  generation	  of	  interactive	  data	  analysis	  with	  real-­‐time	  answers.	  
In	  the	  future,	  queries	  towards	  Big	  Data	  will	  be	  automatically	  generated	  for	  content	  creation	  on	  websites,	  
to	  populate	  hot-­‐lists	  or	  recommendations,	  and	  to	  provide	  an	  ad	  hoc	  analysis	  of	  the	  value	  of	  a	  data	  set	  to	  
decide	  whether	  to	  store	  or	  to	  discard	  it.	  	  Scaling	  complex	  query	  processing	  techniques	  to	  terabytes	  while	  
enabling	  interactive	  response	  times	  is	  a	  major	  open	  research	  problem	  today.	  

A	  problem	  with	  current	  Big	  Data	  analysis	  is	  the	  lack	  of	  coordination	  between	  database	  systems,	  
which	   host	  the	   data	   and	   provide	   SQL	   querying,	   with	  analytics	   packages	   that	   perform	   various	   forms	  of	  
non-­‐SQL	   processing,	   such	   as	   data	   mining	   and	   statistical	   analyses.	   Today’s	   analysts	   are	   impeded	   by	   a	  
tedious	   process	   of	   exporting	   data	   from	   the	   database,	   performing	   a	   non-­‐SQL	   process	   and	   bringing	   the	  
data	   back.	   This	   is	   an	   obstacle	   to	   carrying	   over	   the	   interactive	   elegance	   of	   the	   first	   generation	   of	   SQL-­‐	  
driven	  OLAP	  systems	  into	  the	  data	  mining	  type	  of	  analysis	  that	  is	  in	  increasing	  demand.	  	  A	  tight	  coupling	  
between	  	  	  declarative	  	  	  query	  	  	  languages	  	  	  and	  	  	  the	  	  	  functions	  	  	  of	  	  	  such	  	  	  packages	  	  	  will	  	  	  benefit	  	  	  both	  
expressiveness	  and	  performance	  of	  the	  analysis.	  

2.5	  Interpretation	  

Having	  the	  ability	  to	  analyze	  Big	  Data	  is	  of	  limited	  value	  if	  users	  cannot	  understand	  the	  analysis.	  
Ultimately,	  a	  decision-­‐maker,	  provided	  with	  the	  result	  of	  analysis,	  has	  to	  interpret	  these	  results.	  	   This	  

6 

 

	  

	  

	  

	  
	  
	  

	  

 

interpretation	  cannot	  happen	  in	  a	  vacuum.	  	  Usually,	  it	  involves	  examining	  all	  the	  assumptions	  made	  and	  
retracing	   the	   analysis.	   	   	   Furthermore,	   as	   we	   saw	   above,	   there	   are	   many	   possible	   sources	   of	   error:	  
computer	  systems	  can	  have	  bugs,	  models	  almost	  always	  have	  assumptions,	  and	  results	  can	  be	  based	  on	  
erroneous	   data.	  	  	   For	   all	   of	   these	   reasons,	   no	   responsible	   user	   will	   cede	   authority	   to	   the	   computer	  
system.	   	   Rather	   she	   will	   try	   to	   understand,	   and	   verify,	   the	   results	   produced	   by	   the	   computer.	   	   The	  
computer	  system	  must	  make	  it	  easy	  for	  her	  to	  do	  so.	   This	  is	  particularly	  a	  challenge	  with	  Big	  Data	  due	  to	  
its	  complexity.	  	   There	  are	  often	  crucial	  assumptions	  behind	  the	  data	  recorded.	  	   Analytical	  pipelines	  can	  
often	  involve	  multiple	  steps,	  again	  with	  assumptions	  built	  in.	  	  The	  recent	  mortgage-­‐related	  shock	  to	  the	  
financial	   system	   dramatically	   underscored	   the	   need	   for	   such	   decision-­‐maker	   diligence	   -­‐-­‐	   rather	   than	  
accept	   the	   stated	   solvency	   of	   a	   financial	   institution	   at	   face	   value,	   a	   decision-­‐maker	   has	   to	   examine	  
critically	  the	  many	  assumptions	  at	  multiple	  stages	  of	  analysis.	  

In	  short,	  it	  is	  rarely	  enough	  to	  provide	  just	  the	  results.	  	  Rather,	  one	  must	  provide	  supplementary	  
information	   that	   explains	   how	   each	   result	   was	   derived,	   and	   based	   upon	   precisely	   what	   inputs.	   	   Such	  
supplementary	   information	   is	   called	   the	   provenance	   of	   the	   (result)	   data.	   	   By	   studying	   how	   best	   to	  
capture,	   store,	   and	   query	   provenance,	   in	   conjunction	   with	   techniques	   to	   capture	   adequate	   metadata,	  
we	   can	   create	   an	   infrastructure	   to	   provide	   users	   with	   the	   ability	   both	   to	   interpret	   analytical	   results	  
obtained	  and	  to	  repeat	  the	  analysis	  with	  different	  assumptions,	  parameters,	  or	  data	  sets.	  

Systems	   with	   a	   rich	   palette	   of	   visualizations	   become	   important	   in	   conveying	   to	   the	   users	   the	  
results	  of	  the	  queries	  in	  a	  way	  that	  is	  best	  understood	  in	  the	  particular	  domain.	  	  Whereas	  early	  business	  
intelligence	  systems’	  users	  were	  content	  with	  tabular	  presentations,	  today’s	  analysts	  need	  to	  pack	  and	  
present	   results	   in	   powerful	   visualizations	   that	   assist	   interpretation,	   and	   support	   user	   collaboration	   as	  
discussed	  in	  Sec.	  3.5.	  

Furthermore,	  with	  a	  few	  clicks	  the	  user	  should	  be	  able	  to	  drill	  down	  into	  each	  piece	  of	  data	  that	  
she	  sees	  and	  understand	  its	  provenance,	  which	  is	  a	  key	  feature	  to	  understanding	  the	  data.	  	  That	  is,	  users	  
need	   to	   be	   able	   to	   see	   not	   just	   the	   results,	   but	   also	   understand	   why	   they	   are	   seeing	   those	   results.	  
However,	  raw	  provenance,	  particularly	  regarding	  the	  phases	  in	  the	  analytics	  pipeline,	  is	  likely	  to	  be	  too	  
technical	  for	  many	  users	  to	  grasp	  completely.	  	   One	  alternative	  is	  to	  enable	  the	  users	  to	  “play”	  with	  the	  
steps	   in	   the	   analysis	   –	   make	   small	   changes	   to	   the	   pipeline,	   for	   example,	   or	   modify	   values	   for	   some	  
parameters.	  	   The	  users	  can	  then	  view	  the	  results	  of	  these	  incremental	  changes.	  	   By	  these	  means,	  users	  
can	   develop	   an	   intuitive	   feeling	  for	   the	   analysis	   and	   also	   verify	  that	   it	   performs	   as	   expected	   in	   corner	  
cases.	   	   Accomplishing	   this	   requires	   the	   system	   to	   provide	   convenient	   facilities	   for	   the	   user	   to	   specify	  
analyses.	   Declarative	  specification,	  discussed	  in	  Sec.	  4,	  is	  one	  component	  of	  such	  a	  system.	  

3.	  Challenges	  in	  Big	  Data	  Analysis	  

Having	   described	   the	   multiple	   phases	   in	   the	   Big	   Data	   analysis	   pipeline,	   we	   now	   turn	   to	   some	  
common	  challenges	  that	  underlie	  many,	  and	  sometimes	  all,	  of	  these	  phases.	  	   These	  are	  shown	  as	  five	  
boxes	  in	  the	  second	  row	  of	  Fig.	  1.	  

7 

 

	  
	  

	  

	  

	  

	  
	  
	  

	  

 

3.1	  Heterogeneity	  and	  Incompleteness	  

When	  humans	  consume	  information,	  a	  great	  deal	  of	  heterogeneity	  is	  comfortably	  tolerated.	  	  In	  
fact,	   the	   nuance	   and	   richness	   of	   natural	   language	   can	   provide	   valuable	   depth.	  	  	   However,	   machine	  
analysis	   algorithms	   expect	   homogeneous	   data,	   and	   cannot	   understand	   nuance.	  	   In	   consequence,	   data	  
must	  be	  carefully	  structured	  as	  a	  first	  step	  in	  (or	  prior	  to)	  data	  analysis.	  	  Consider,	  for	  example,	  a	  patient	  
who	  has	  multiple	  medical	  procedures	  at	  a	  hospital.	  	  We	  could	  create	  one	  record	  per	  medical	  procedure	  
or	   laboratory	   test,	   one	   record	   for	   the	   entire	   hospital	   stay,	   or	   one	   record	   for	   all	   lifetime	   hospital	  
interactions	  	  of	  	  this	  	  patient.	  	  	  	   With	  	  anything	  	  other	  	  than	  	  the	  	  first	  	  design,	  	  the	  	  number	  	  of	  	  medical	  
procedures	  and	  lab	  tests	  per	  record	  would	  be	  different	  for	  each	  patient.	  	  	  The	  three	  design	  choices	  listed	  
have	  successively	  less	  structure	  and,	  conversely,	  successively	  greater	  variety.	  	   Greater	  structure	  is	  likely	  
to	  be	  required	  by	  many	  (traditional)	  data	  analysis	  systems.	  	  However,	  the	  less	  structured	  design	  is	  likely	  
to	   be	   more	   effective	   for	   many	   purposes	   –	   for	   example	   questions	   relating	   to	   disease	   progression	   over	  
time	   will	   require	   an	   expensive	   join	   operation	   with	   the	   first	   two	   designs,	   but	   can	   be	   avoided	   with	   the	  
latter.	  	  	  	  However,	  computer	  systems	  work	  most	  efficiently	  if	  they	  can	  store	  multiple	  items	  that	  are	  all	  
identical	   in	   size	   and	   structure.	   	   Efficient	   representation,	   access,	   and	   analysis	   of	   semi-­‐structured	   data	  
require	  further	  work.	  

Consider	  an	  electronic	  health	  record	  database	  design	  that	  has	  fields	  for	  birth	  date,	  occupation,	  
and	  blood	  type	  for	  each	  patient.	  	   What	  do	  we	  do	  if	  one	  or	  more	  of	  these	  pieces	  of	  information	  is	  not	  
provided	   by	   a	   patient?	   	   	   Obviously,	   the	   health	   record	   is	   still	   placed	   in	   the	   database,	   but	   with	   the	  
corresponding	  attribute	  values	  being	  set	  to	  NULL.	  	  	  A	  data	  analysis	  that	  looks	  to	  classify	  patients	  by,	  say,	  
occupation,	   must	   take	   into	   account	   patients	   for	   which	   this	   information	   is	   not	   known.	   	   Worse,	   these	  
patients	  with	  unknown	  occupations	  can	  be	  ignored	  in	  the	  analysis	  only	  if	  we	  have	  reason	  to	  believe	  that	  
they	  	   are	  	   otherwise	  	   statistically	  	   similar	  	   to	  	   the	  	   patients	  	   with	  	   known	  	   occupation	  	   for	  	   the	  	   analysis	  
performed.	   	   For	   example,	   if	   unemployed	   patients	   are	   more	   likely	   to	   hide	   their	   employment	   status,	  
analysis	   results	   may	   be	   skewed	   in	   that	   it	   considers	   a	   more	   employed	   population	   mix	   than	   exists,	   and	  
hence	  potentially	  one	  that	  has	  differences	  in	  occupation-­‐related	  health-­‐profiles.	  

Even	  after	  data	  cleaning	  and	  error	  correction,	  some	  incompleteness	  and	  some	  errors	  in	  data	  are	  
likely	  to	  remain.	  	   This	  incompleteness	  and	  these	  errors	  must	  be	  managed	  during	  data	  analysis.	  	   Doing	  
this	   correctly	   is	   a	   challenge.	   	   Recent	   work	   on	   managing	   probabilistic	   data	   suggests	   one	   way	   to	   make	  
progress.	  

3.2	  Scale	  

Of	  course,	  the	  first	  thing	  anyone	  thinks	  of	  with	  Big	  Data	  is	  its	  size.	  	   After	  all,	  the	  word	  “big”	  is	  
there	  in	  the	  very	  name.	  	   Managing	  large	  and	  rapidly	  increasing	  volumes	  of	  data	  has	  been	  a	  challenging	  
issue	  for	  many	  decades.	  	  	  In	  the	  past,	  this	  challenge	  was	  mitigated	  by	  processors	  getting	  faster,	  following	  
Moore’s	  law,	  to	  provide	  us	  with	  the	  resources	  needed	  to	  cope	  with	  increasing	  volumes	  of	  data.	  	   But,	  

8 

 

	  

	  

	  

 

there	  is	  a	  fundamental	  shift	  underway	  now:	  data	  volume	  is	  scaling	  faster	  than	  compute	  resources,	  and	  
CPU	  speeds	  are	  static.	  

First,	  over	  the	  last	  five	  years	  the	  processor	  technology	  has	  made	  a	  dramatic	  shift	  -­‐	  rather	  than	  
processors	   doubling	   their	   clock	   cycle	   frequency	   every	   18-­‐24	   months,	   now,	   due	   to	   power	   constraints,	  
clock	  speeds	  have	  largely	  stalled	  and	  processors	  are	  being	  built	  with	  increasing	  numbers	  of	  cores.	  	  In	  the	  
past,	  large	  data	  processing	  systems	  had	  to	  worry	  about	  parallelism	  across	  nodes	  in	  a	  cluster;	  now,	  one	  
has	   to	   deal	   with	   parallelism	   within	   a	   single	   node.	  	  	  Unfortunately,	   parallel	   data	   processing	   techniques	  
that	   were	   applied	   in	   the	   past	   for	   processing	   data	   across	   nodes	   don’t	   directly	   apply	   for	   intra-­‐node	  
parallelism,	   since	   the	   architecture	   looks	   very	   different;	   for	   example,	   there	   are	   many	   more	   hardware	  
resources	   such	  as	   processor	   caches	   and	   processor	  memory	   channels	   that	   are	  shared	   across	  cores	   in	  a	  
single	   node.	  	  	  Furthermore,	   the	   move	   towards	   packing	   multiple	   sockets	   (each	   with	   10s	   of	   cores)	   adds	  
another	  level	  of	  complexity	  for	  intra-­‐node	  parallelism.	  	  Finally,	  with	  predictions	  of	  “dark	  silicon”,	  namely	  
that	  power	  consideration	  will	  likely	  in	  the	  future	  prohibit	  us	  from	  using	  all	  of	  the	  hardware	  in	  the	  system	  
continuously,	  data	  processing	  systems	  will	  likely	  have	  to	  actively	  manage	  the	  power	  consumption	  of	  the	  
processor.	  These	  unprecedented	  changes	  require	  us	  to	  rethink	  how	  we	  design,	  build	  and	  operate	  data	  
processing	  components.	  

The	   second	  dramatic	   shift	   that	   is	   underway	   is	   the	   move	   towards	   cloud	  computing,	   which	  now	  
aggregates	   multiple	   disparate	   workloads	   with	   varying	   performance	   goals	   (e.g.	   interactive	   services	  
demand	  that	  the	  data	  processing	  engine	  return	  back	  an	  answer	  within	  a	  fixed	  response	  time	  cap)	  into	  
very	  large	  clusters.	  This	  level	  of	  sharing	  of	  resources	  on	  expensive	  and	  large	  clusters	  requires	  new	  ways	  
of	   determining	   how	   to	   run	   and	   execute	   data	   processing	   jobs	   so	   that	   we	   can	   meet	   the	   goals	   of	   each	  
workload	  cost-­‐effectively,	  and	  to	  deal	  with	  system	  failures,	  which	  occur	  more	  frequently	  as	  we	  operate	  
on	   larger	   and	   larger	   clusters	   (that	   are	   required	   to	   deal	   with	   the	   rapid	   growth	   in	   data	   volumes).	  	  	  This	  
places	  	  a	  	  premium	  	  on	  	  declarative	  	  approaches	  	  to	  	  expressing	  	  programs,	  	  even	  	  those	  	  doing	  	  complex	  
machine	  learning	  tasks,	  since	  global	  optimization	  across	  multiple	  users’	  programs	  is	  necessary	  for	  good	  
overall	   performance.	   	   Reliance	   on	   user-­‐driven	   program	   optimizations	   is	   likely	   to	   lead	   to	   poor	   cluster	  
utilization,	   since	   users	   are	   unaware	   of	   other	   users’	   programs.	   	   	   	   System-­‐driven	   holistic	   optimization	  
requires	  	   programs	  	   to	  	   be	  	   sufficiently	  	   transparent,	  	   e.g.,	  	   as	  	   in	  	   relational	  	   database	  	   systems,	  	   where	  
declarative	  query	  languages	  are	  designed	  with	  this	  in	  mind.	  

A	   third	   dramatic	   shift	   that	   is	   underway	   is	   the	   transformative	   change	   of	   the	   traditional	   I/O	  
subsystem.	  For	  many	  decades,	  hard	  disk	  drives	  (HDDs)	  were	  used	  to	  store	  persistent	  data.	  HDDs	  had	  far	  
slower	  random	  IO	  performance	  than	  sequential	  IO	  performance,	  and	  data	  processing	  engines	  formatted	  
their	  data	  and	  designed	  their	  query	  processing	  methods	  to	  “work	  around”	  this	  limitation.	  But,	  HDDs	  are	  
increasingly	   being	   replaced	   by	   solid	   state	   drives	   today,	   and	   other	   technologies	   such	   as	   Phase	   Change	  
Memory	  are	  around	  the	  corner.	  These	  newer	  storage	  technologies	  do	  not	  have	  the	  same	  large	  spread	  in	  
performance	  between	  the	  sequential	  and	  random	  I/O	  performance,	  which	  requires	  a	  rethinking	  of	  how	  
we	   design	   storage	   subsystems	   for	   data	   processing	   systems.	   Implications	   of	   this	   changing	   storage	  
subsystem	   potentially	   touch	   every	   aspect	   of	   data	   processing,	   including	   query	   processing	   algorithms,	  
query	  scheduling,	  database	  design,	  concurrency	  control	  methods	  and	  recovery	  methods.	  

9 

 

	  

	  

	  

	  
	  
	  

	  

	  

 

3.3	  Timeliness	  

The	  flip	  side	  of	  size	  is	  speed.	  	   The	  larger	  the	  data	  set	  to	  be	  processed,	  the	  longer	  it	  will	  take	  to	  
analyze.	  	  The	  design	  of	  a	  system	  that	  effectively	  deals	  with	  size	  is	  likely	  also	  to	  result	  in	  a	  system	  that	  can	  
process	  a	  given	  size	  of	  data	  set	  faster.	  	  However,	  it	  is	  not	  just	  this	  speed	  that	  is	  usually	  meant	  when	  one	  
speaks	  of	  Velocity	  in	  the	  context	  of	  Big	  Data.	  	  Rather,	  there	  is	  an	  acquisition	  rate	  challenge	  as	  described	  
in	  Sec.	  2.1,	  and	  a	  timeliness	  challenge	  described	  next.	  

There	   are	   many	   situations	   in	   which	   the	   result	   of	   the	   analysis	   is	   required	   immediately.	   	   For	  
example,	   if	   a	   fraudulent	   credit	   card	   transaction	   is	   suspected,	   it	   should	   ideally	   be	   flagged	   before	   the	  
transaction	  is	  completed	  –	  potentially	  preventing	  the	  transaction	  from	  taking	  place	  at	  all.	  	   Obviously,	  a	  
full	   analysis	   of	   a	   user’s	   purchase	   history	   is	   not	   likely	   to	   be	   feasible	   in	   real-­‐time.	   	   Rather,	   we	   need	   to	  
develop	  partial	  results	  in	  advance	  so	  that	  a	  small	  amount	  of	  incremental	  computation	  with	  new	  data	  can	  
be	  used	  to	  arrive	  at	  a	  quick	  determination.	  

Given	  a	  large	  data	  set,	  it	  is	  often	  necessary	  to	  find	  elements	  in	  it	  that	  meet	  a	  specified	  criterion.	  
In	  the	  course	  of	  data	  analysis,	  this	  sort	  of	  search	  is	  likely	  to	  occur	  repeatedly.	  	   Scanning	  the	  entire	  data	  
set	  to	  find	  suitable	  elements	  is	  obviously	  impractical.	  	  Rather,	  index	  structures	  are	  created	  in	  advance	  to	  
permit	   finding	   qualifying	   elements	   quickly.	   	   The	   problem	   is	   that	   each	   index	   structure	   is	   designed	   to	  
support	  only	  some	  classes	  of	  criteria.	  	  With	  new	  analyses	  desired	  using	  Big	  Data,	  there	  are	  new	  types	  of	  
criteria	   specified,	   and	   a	   need	   to	   devise	   new	   index	   structures	   to	   support	   such	   criteria.	   	   For	   example,	  
consider	   a	   traffic	   management	   system	   with	   information	   regarding	   thousands	   of	   vehicles	   and	   local	   hot	  
spots	  on	  roadways.	  	   The	  system	  may	  need	  to	  predict	  potential	  congestion	  points	  along	  a	  route	  chosen	  
by	   a	   user,	   and	   suggest	   alternatives.	   	   Doing	   so	   requires	   evaluating	   multiple	   spatial	   proximity	   queries	  
working	   with	   the	   trajectories	   of	   moving	   objects.	   	   New	   index	   structures	   are	   required	   to	   support	   such	  
queries.	   Designing	   such	   structures	   becomes	   particularly	   challenging	   when	   the	   data	   volume	   is	   growing	  
rapidly	  and	  the	  queries	  have	  tight	  response	  time	  limits.	  

3.4	  Privacy	  

The	  privacy	  of	  data	  is	  another	  huge	  concern,	  and	  one	  that	  increases	  in	  the	  context	  of	  Big	  Data.	  
For	  electronic	  health	  records,	  there	  are	  strict	  laws	  governing	  what	  can	  and	  cannot	  be	  done.	  	   For	  other	  
data,	  regulations,	  particularly	  in	  the	  US,	  are	  less	  forceful.	  	   However,	  there	  is	  great	  public	  fear	  regarding	  
the	   inappropriate	   use	   of	   personal	   data,	   particularly	   through	   linking	   of	   data	   from	   multiple	   sources.	  
Managing	   privacy	   is	   effectively	   both	   a	   technical	   and	   a	   sociological	   problem,	  which	   must	   be	   addressed	  
jointly	  from	  both	  perspectives	  to	  realize	  the	  promise	  of	  big	  data.	  

Consider,	   for	   example,	   data	   gleaned	   from	   location-­‐based	   services.	   	   These	   new	   architectures	  
require	  a	  user	  to	  share	  his/her	  location	  with	  the	  service	  provider,	  resulting	  in	  obvious	  privacy	  concerns.	  
Note	  that	  hiding	  the	  user’s	  identity	  alone	  without	  hiding	  her	  location	  would	  not	  properly	  address	  these	  
privacy	  concerns.	  	  An	  attacker	  or	  a	  (potentially	  malicious)	  location-­‐based	  server	  can	  infer	  the	  identity	  of	  
the	  query	  source	  from	  its	  (subsequent)	  location	  information.	  For	  example,	  a	  user’s	  location	  information	  
can	  be	  tracked	  through	  several	  stationary	  connection	  points	  (e.g.,	  cell	  towers).	  	   After	  a	  while,	  the	  user	  

10 

 

	  

	  
	  
	  

	  

	  

	  

 

leaves	  “a	  trail	  of	  packet	  crumbs”	  which	  could	  be	  associated	  to	  a	  certain	  residence	  or	  office	  location	  and	  
thereby	   used	   to	   determine	   the	   user’s	   identity.	  	   Several	   other	   types	   of	   surprisingly	   private	   information	  
such	  	  as	  	  health	  	  issues	  	  (e.g.,	  	  presence	  	  in	  	  a	  	  cancer	  	  treatment	  	  center)	  	  or	  	  religious	  	  preferences	  	  (e.g.,	  
presence	   in	   a	   church)	   can	   also	   be	   revealed	   by	   just	   observing	   anonymous	   users’	   movement	   and	   usage	  
pattern	  over	  time.	  In	  general,	  Barabási	  et	  al.	  showed	  that	  there	  is	  a	  close	  correlation	  between	  people’s	  
identities	   and	   their	   movement	   patterns	   [Gon2008].	   	   Note	   that	   hiding	   a	   user	   location	   is	   much	   more	  
challenging	  than	  hiding	  his/her	  identity.	  	  This	  is	  because	  with	  location-­‐based	  services,	  the	  location	  of	  the	  
user	   is	   needed	   for	   a	   successful	   data	   access	   or	   data	   collection,	   while	   the	   identity	   of	   the	   user	   is	   not	  
necessary.	  

There	   are	   many	   additional	   challenging	   research	   problems.	  	  	  For	   example,	   we	   do	   not	   know	   yet	  
how	  to	  share	  private	  data	  while	  limiting	  disclosure	  and	  ensuring	  sufficient	  data	  utility	  in	  the	  shared	  data.	  
The	   existing	   paradigm	   of	   differential	   privacy	   is	   a	   very	   important	   step	   in	   the	   right	   direction,	   but	   it	  
unfortunately	   reduces	   information	   content	   too	   far	   in	   order	   to	   be	   useful	   in	   most	   practical	   cases.	   	   In	  
addition,	  real	  data	  is	  not	  static	  but	  gets	  larger	  and	  changes	  over	  time;	  none	  of	  the	  prevailing	  techniques	  
results	  in	  any	  useful	  content	  being	  released	  in	  this	  scenario.	  	   Yet	  another	  very	  important	  direction	  is	  to	  
rethink	  security	  for	  information	  sharing	  in	  Big	  Data	  use	  cases.	  	  Many	  online	  services	  today	  require	  us	  to	  
share	  private	  information	  (think	  of	  Facebook	  applications),	  but	  beyond	  record-­‐level	  access	  control	  we	  do	  
not	  understand	  what	  it	  means	  to	  share	  data,	  how	  the	  shared	  data	  can	  be	  linked,	  and	  how	  to	  give	  users	  
fine-­‐grained	  control	  over	  this	  sharing.	  

3.5	  Human	  Collaboration	  

In	   spite	   of	   the	   tremendous	   advances	   made	   in	   computational	   analysis,	   there	   remain	   many	  
patterns	   that	   humans	   can	   easily	   detect	   but	   computer	   algorithms	   have	   a	   hard	   time	   finding.	   	   Indeed,	  
CAPTCHAs	  exploit	  precisely	   this	  fact	  to	   tell	  human	  web	  users	  apart	  from	   computer	  programs.	  	   Ideally,	  
analytics	  for	  Big	  Data	  will	  not	  be	  all	  computational	  –	  rather	  it	  will	  be	  designed	  explicitly	  to	  have	  a	  human	  
in	  the	  loop.	  	   The	  new	  sub-­‐field	  of	  visual	  analytics	  is	  attempting	  to	  do	  this,	  at	  least	  with	  respect	  to	  the	  
modeling	  and	  analysis	  phase	  in	  the	  pipeline.	  	   There	  is	  similar	  value	  to	  human	  input	  at	  all	  stages	  of	  the	  
analysis	  pipeline.	  

In	   today’s	   complex	   world,	   it	   often	   takes	   multiple	   experts	   from	   different	   domains	   to	   really	  
understand	   what	   is	   going	   on.	   	   A	   Big	   Data	   analysis	   system	   must	   support	   input	   from	   multiple	   human	  
experts,	  and	  shared	  exploration	  of	  results.	  	  These	  multiple	  experts	  may	  be	  separated	  in	  space	  and	  time	  
when	   it	   is	   too	   expensive	   to	   assemble	   an	   entire	   team	   together	   in	   one	   room.	  	  	  The	   data	   system	   has	   to	  
accept	  this	  distributed	  expert	  input,	  and	  support	  their	  collaboration.	  

A	   popular	   new	   method	   of	   harnessing	   human	   ingenuity	   to	   solve	   problems	   is	   through	   crowd-­‐	  
sourcing.	  	  	  Wikipedia,	   the	   online	   encyclopedia,	   is	   perhaps	   the	   best	   known	   example	   of	   crowd-­‐sourced	  
data.	  	   We	  are	  relying	  upon	  information	  provided	  by	  unvetted	  strangers.	  	   Most	  often,	  what	  they	   say	   is	  
correct.	  	  	  However,	   we	   should	   expect	   there	   to	   be	   individuals	   who	   have	   other	   motives	   and	   abilities	   –	  
some	  may	  have	  a	  reason	  to	  provide	  false	  information	  in	  an	  intentional	  attempt	  to	  mislead.	  	  While	  most	  

11 

 

	  
	  
	  

	  

	  

	  

 

such	   errors	   will	   be	   detected	   and	   corrected	   by	   others	   in	   the	   crowd,	   we	   need	   technologies	   to	   facilitate	  
this.	   	   	   We	   also	   need	   a	   framework	   to	   use	   in	   analysis	   of	   such	   crowd-­‐sourced	   data	   with	   conflicting	  
statements.	  	  As	  humans,	  we	  can	  look	  at	  reviews	  of	  a	  restaurant,	  some	  of	  which	  are	  positive	  and	  others	  
critical,	  and	  come	  up	  with	  a	  summary	  assessment	  based	  on	  which	  we	  can	  decide	  whether	  to	  try	  eating	  
there.	  	  We	  need	  computers	  to	  be	  able	  to	  do	  the	  equivalent.	  	  The	  issues	  of	  uncertainty	  and	  error	  become	  
even	  more	  pronounced	  in	  a	  specific	  type	  of	  crowd-­‐sourcing,	  termed	  participatory-­‐sensing.	  	  In	  this	  case,	  
every	   person	   with	   a	   mobile	   phone	   can	   act	   as	   a	   multi-­‐modal	   sensor	   collecting	   various	   types	   of	   data	  
instantaneously	   (e.g.,	   picture,	   video,	   audio,	   location,	   time,	   speed,	   direction,	   acceleration).	   	   The	   extra	  
challenge	  here	  is	  the	  inherent	  uncertainty	  of	  the	  data	  collection	  devices.	  	  The	  fact	  that	  collected	  data	  are	  
probably	  spatially	  and	  temporally	  correlated	  can	  be	  exploited	  to	  better	  assess	  their	  correctness.	  	  When	  
crowd-­‐sourced	  data	  is	  obtained	  for	  hire,	  such	  as	  with	  “Mechanical	  Turks,”	  much	  of	  the	  data	  created	  may	  
be	   with	   a	   primary	   objective	   of	   getting	   it	   done	   quickly	   rather	   than	   correctly.	  	   This	   is	   yet	   another	   error	  
model,	  which	  must	  be	  planned	  for	  explicitly	  when	  it	  applies.	  

4.	  System	  Architecture	  

Companies	  today	  already	  use,	  and	  appreciate	  the	  value	  of,	  business	  intelligence.	  	  	  Business	  data	  
is	  analyzed	  for	  many	  purposes:	  a	  company	  may	  perform	  system	  log	  analytics	  and	  social	  media	  analytics	  
for	   risk	   assessment,	   customer	   retention,	   brand	   management,	   and	   so	   on.	  	  	  Typically,	   such	   varied	   tasks	  
have	   been	   handled	   by	   separate	   systems,	   even	   if	   each	   system	   includes	   common	   steps	   of	   information	  
extraction,	   data	   cleaning,	   relational-­‐like	   processing	   (joins,	   group-­‐by,	   aggregation),	   statistical	   and	  
predictive	  modeling,	  and	  appropriate	  exploration	  and	  visualization	  tools	  as	  shown	  in	  Fig.	  1.	  

With	  Big	  Data,	  the	  use	  of	  separate	  systems	  in	  this	  fashion	  becomes	  prohibitively	  expensive	  given	  
the	  large	  size	  of	  the	  data	  sets.	  	  	   The	  expense	  is	  due	  not	  only	  to	  the	  cost	  of	  the	  systems	  themselves,	  but	  
also	  the	  time	  to	  load	  the	  data	  into	  multiple	  systems.	  	  In	  consequence,	  Big	  Data	  has	  made	  it	  necessary	  to	  
run	   heterogeneous	   workloads	   on	   a	   single	   infrastructure	   that	   is	   sufficiently	   flexible	   to	   handle	   all	   these	  
workloads.	  	  	  	  The	   challenge	   here	   is	  not	   to	   build	   a	   system	   that	   is	   ideally	   suited	  for	   all	   processing	  tasks.	  
Instead,	  the	  need	  is	  for	  the	  underlying	  system	  architecture	  to	  be	  flexible	  enough	  that	  the	  components	  
built	  on	  top	  of	  it	  for	  expressing	  the	  various	  kinds	  of	  processing	  tasks	  can	  tune	  it	  to	  efficiently	  run	  these	  
different	  workloads.	  	  The	  effects	  of	  scale	  on	  the	  physical	  architecture	  were	  considered	  in	  Sec	  3.2.	  	  In	  this	  
section,	  we	  focus	  on	  the	  programmability	  requirements.	  

If	  users	  are	  to	  compose	  and	  build	  complex	  analytical	  pipelines	  over	  Big	  Data,	  it	  is	  essential	  that	  
they	  have	  appropriate	  high-­‐level	  primitives	  to	  specify	  their	  needs	  in	  such	  flexible	  systems.	  	  	  	  The	  Map-­‐	  
Reduce	  framework	  has	  been	  tremendously	  valuable,	  but	  is	  only	  a	  first	  step.	  	  Even	  declarative	  languages	  
that	   exploit	   it,	   such	   as	   Pig	   Latin,	   are	   at	   a	   rather	   low	   level	   when	   it	   comes	   to	   complex	   analysis	   tasks.	  
Similar	   declarative	   specifications	   are	   required	   at	   higher	   levels	   to	   meet	   the	   programmability	   and	  
composition	   needs	   of	   these	   analysis	   pipelines.	  	  	  	  	  Besides	   the	   basic	   technical	   need,	   there	   is	   a	   strong	  
business	  imperative	  as	  well.	  	  Businesses	  typically	  will	  outsource	  Big	  Data	  processing,	  or	  many	  aspects	  of	  
it.	  	  	  Declarative	   specifications	   are	   required	   to	   enable	   technically	   meaningful	   service	   level	   agreements,	  

12 

 

	  

	  

	  
	  
	  

	  

 

since	  the	  point	  of	  the	  out-­‐sourcing	  is	  to	  specify	  precisely	  what	  task	  will	  be	  performed	  without	  going	  into	  
details	  of	  how	  to	  do	  it.	  

Declarative	   specification	   is	   needed	   not	   just	   for	   the	   pipeline	   composition,	   but	   also	   for	   the	  
individual	  operations	  themselves.	  	   Each	  operation	  (cleaning,	   extraction,	  modeling	  etc.)	   potentially	   runs	  
on	  a	  very	  large	  data	  set.	  	  Furthermore,	  each	  operation	  itself	  is	  sufficiently	  complex	  that	  there	  are	  many	  
choices	  and	  optimizations	  possible	  in	  how	  it	  is	  implemented.	  	   In	  databases,	  there	  is	  considerable	  work	  
on	  optimizing	  individual	  operations,	  such	  as	  joins.	  	  It	  is	  well-­‐known	  that	  there	  can	  be	  multiple	  orders	  of	  
magnitude	  difference	  in	  the	  cost	  of	  two	  different	  ways	  to	  execute	  the	  same	  query.	  	  Fortunately,	  the	  user	  
does	  not	  have	  to	  make	  this	  choice	  –	  the	  database	  system	  makes	  it	  for	  her.	  	  In	  the	  case	  of	  Big	  Data,	  these	  
optimizations	   may	   be	   more	   complex	   because	   not	   all	   operations	   will	   be	   I/O	   intensive	   as	   in	   databases.	  
Some	  operations	  may	  be,	  but	  others	  may	  be	  CPU	  intensive,	  or	  a	  mix.	  	  So	  standard	  database	  optimization	  
techniques	  cannot	  directly	  be	  used.	  	   However,	  it	  should	  be	  possible	  to	  develop	  new	  techniques	  for	  Big	  
Data	  operations	  inspired	  by	  database	  techniques.	  

The	  very	  fact	  that	  Big	  Data	  analysis	  typically	  involves	  multiple	  phases	  highlights	  a	  challenge	  that	  
arises	   routinely	   in	  practice:	  	   production	  systems	  must	   run	  complex	  analytic	   pipelines,	  or	   workflows,	   at	  
routine	   intervals,	   e.g.,	   hourly	   or	   daily.	  	  	   New	   data	   must	   be	   incrementally	   accounted	   for,	   taking	   into	  
account	  	  the	  	  results	  	  of	  	  prior	  	  analysis	  	  and	  	  pre-­‐existing	  	  data.	  	  	  	  And	  	  of	  	  course,	  	  provenance	  	  must	  	  be	  
preserved,	   and	   must	   include	   the	   phases	   in	   the	   analytic	   pipeline.	  	  	   Current	   systems	   offer	   little	   to	   no	  
support	  for	  such	  Big	  Data	  pipelines,	  and	  this	  is	  in	  itself	  a	  challenging	  objective.	  

5.	  Conclusion	  

We	  have	  entered	  an	  era	  of	  Big	  Data.	  	   Through	  better	  analysis	  of	  the	  large	  volumes	  of	  data	  that	  
are	  becoming	  available,	  there	  is	  the	  potential	  for	  making	  faster	  advances	  in	  many	   scientific	  disciplines	  
and	   improving	   the	   profitability	   and	   success	   of	   many	   enterprises.	  	   However,	   many	   technical	   challenges	  
described	   in	   this	   paper	   must	   be	   addressed	   before	   this	   potential	   can	   be	   realized	   fully.	  	   The	   challenges	  
include	   not	   just	   the	   obvious	   issues	   of	   scale,	   but	   also	   heterogeneity,	   lack	   of	   structure,	   error-­‐handling,	  
privacy,	   timeliness,	   provenance,	   and	   visualization,	   at	   all	   stages	   of	   the	   analysis	   pipeline	   from	   data	  
acquisition	  to	  result	  interpretation.	  	  	  	  These	  technical	  challenges	  are	  common	  across	  a	  large	  variety	  of	  
application	   domains,	   and	   therefore	   not	   cost-­‐effective	   to	   address	   in	   the	   context	   of	   one	   domain	   alone.	  
Furthermore,	  these	  challenges	  will	  require	  transformative	  solutions,	  and	  will	  not	  be	  addressed	  naturally	  
by	  the	  next	  generation	  of	  industrial	  products.	  	  	   We	  must	  support	  and	  encourage	  fundamental	  research	  
towards	  addressing	  these	  technical	  challenges	  if	  we	  are	  to	  achieve	  the	  promised	  benefits	  of	  Big	  Data.	  

13 

 

	  

	  

	  

	  

	  

	  

	  

	  

	  

	  

	  

	  

 

Bibliography	  

[CCC2011a]	   Advancing	  Discovery	  in	  Science	  and	  Engineering.	  	  Computing	  Community	  Consortium.	  

Spring	  2011.	  

[CCC2011b]	   Advancing	  Personalized	  Education.	  	  Computing	  Community	  Consortium.	  	  Spring	  2011.	  

[CCC2011c]	  

Smart	  Health	  and	  Wellbeing.	  	  Computing	  Community	  Consortium.	   Spring	  2011.	  

[CCC2011d]	   A	  Sustainable	  Future.	   Computing	  Community	  Consortium.	  	  Summer	  2011.	  

[DF2011]	  

[Eco2011]	  

[FJ+2011]	  

Getting	  Beneath	  the	  Veil	  of	  Effective	  Schools:	  Evidence	  from	  New	  York	  City.	   Will	  Dobbie,	  
Roland	  G.	  Fryer,	  Jr.	   NBER	  Working	  Paper	  No.	  17632.	   Issued	  Dec.	  2011.	  

Drowning	  in	  numbers	  -­‐-­‐	  Digital	  data	  will	  flood	  the	  planet—and	  help	  us	  understand	  it	  
better.	   The	  Economist,	  Nov	  18,	  2011.	  
http://www.economist.com/blogs/dailychart/2011/11/big-­‐data-­‐0	  

Using	  Data	  for	  Systemic	  Financial	  Risk	  Management.	   Mark	  Flood,	  H	  V	  Jagadish,	  Albert	  
Kyle,	  Frank	  Olken,	  and	  Louiqa	  Raschid.	   Proc.	  Fifth	  Biennial	  Conf.	  Innovative	  Data	  Systems	  
Research,	  Jan.	  2011.	  

[Gar2011]	  

Pattern-­‐Based	  Strategy:	  Getting	  Value	  from	  Big	  Data.	   Gartner	  Group	  press	  release.	  	  July	  
2011.	   Available	  at	  http://www.gartner.com/it/page.jsp?id=1731916	  

[Gon2008]	  

Understanding	  individual	  human	  mobility	  patterns.	   Marta	  C.	  González,	  César	  A.	  Hidalgo,	  
and	  Albert-­‐László	  Barabási.	   Nature	  453,	  779-­‐782	  (5	  June	  2008)	  

[LP+2009]	  

Computational	  Social	  Science.	   David	  Lazer,	  Alex	  Pentland,	  Lada	  Adamic,	  Sinan	  Aral,	  
Albert-­‐László	  Barabási,	  Devon	  Brewer,Nicholas	  Christakis,	  Noshir	  Contractor,	  James	  
Fowler,	  Myron	  Gutmann,	  Tony	  Jebara,	  Gary	  King,	  Michael	  Macy,	  Deb	  Roy,	  and	  Marshall	  
Van	  Alstyne.	   Science	  6	  February	  2009:	  323	  (5915),	  721-­‐723.	  

[McK2011]	  

Big	  data:	  The	  next	  frontier	  for	  innovation,	  competition,	  and	  productivity.	  	   James	  Manyika,	  
Michael	  Chui,	  Brad	  Brown,	  Jacques	  Bughin,	  Richard	  Dobbs,	  Charles	  Roxburgh,	  and	  Angela	  
Hung	  Byers.	   McKinsey	  Global	  Institute.	   May	  2011.	  

[MGI2011]	   Materials	  Genome	  Initiative	  for	  Global	  Competitiveness.	  	  National	  Science	  and	  

Technology	  Council.	   June	  2011.	  

[NPR2011a]	   Folowing	  the	  Breadcrumbs	  to	  Big	  Data	  Gold.	  	  Yuki	  Noguchi.	  	  National	  Public	  Radio,	  Nov.	  

29,	  2011.	   	  http://www.npr.org/2011/11/29/142521910/the-­‐digital-­‐breadcrumbs-­‐that-­‐lead-­‐to-­‐big-­‐	  
data	  

[NPR2011b]	   The	  Search	  for	  Analysts	  to	  Make	  Sense	  of	  Big	  Data.	   Yuki	  Noguchi.	  	  National	  Public	  Radio,	  

Nov.	  30,	  2011.	  
http://www.npr.org/2011/11/30/142893065/the-­‐search-­‐for-­‐analysts-­‐to-­‐make-­‐sense-­‐of-­‐big-­‐data	  

[NYT2012]	  

The	  Age	  of	  Big	  Data.	  	  Steve	  Lohr.	  	  New	  York	  Times,	  Feb	  11,	  2012.	  
http://www.nytimes.com/2012/02/12/sunday-­‐review/big-­‐datas-­‐impact-­‐in-­‐the-­‐world.html	  

14 

 

	  

 

[PCAST2010]	  	   Designing	  a	  Digital	  Future:	  Federally	  Funded	  Research	  and	  Development	  in	  Networking	  

and	  Information	  Technology.	   PCAST	  Report,	  Dec.	  2010.	  	  Available	  at	  
http://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-­‐nitrd-­‐report-­‐2010.pdf	  

[SDSS2008]	  

SDSS-­‐III:	  Massive	  Spectroscopic	  Surveys	  of	  the	  Distant	  Universe,	  the	  Milky	  Way	  Galaxy,	  
and	  Extra-­‐Solar	  Planetary	  Systems.	   Jan.	  2008.	  	  Available	  at	  
http://www.sdss3.org/collaboration/description.pdf	  

15 

 

	  
	  
	  

	  

	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  

 

About	  this	  Document	  

This	  white	  paper	  was	  created	  through	  a	  distributed	  conversation	  among	  many	  prominent	  researchers	  
listed	  below.	  	  This	  conversation	  lasted	  a	  period	  of	  approximately	  three	  months	  from	  Nov.	  2011	  to	  Feb.	  
2012.	   Collaborative	  writing	  was	  supported	  by	  a	  distributed	  document	  editor.	  

Divyakant	  Agrawal,	  UC	  Santa	  Barbara	  
Philip	  Bernstein,	  Microsoft	  
Elisa	  Bertino,	  Purdue	  Univ.	  
Susan	  Davidson,	  Univ.	  of	  Pennsylvania	  
Umeshwar	  Dayal,	  HP	  
Michael	  Franklin,	  UC	  Berkeley	  
Johannes	  Gehrke,	  Cornell	  Univ.	  
Laura	  Haas,	  IBM	  
Alon	  Halevy,	  Google	  
Jiawei	  Han,	  UIUC	  
H.	  V.	  Jagadish,	  Univ.	  of	  Michigan	  (Coordinator)	  
Alexandros	  Labrinidis,	  Univ.	  of	  Pittsburgh	  
Sam	  Madden,	  MIT	  
Yannis	  Papakonstantinou,	  UC	  San	  Diego	  
Jignesh	  M.	  Patel,	  Univ.	  of	  Wisconsin	  
Raghu	  Ramakrishnan,	  Yahoo!	  
Kenneth	  Ross,	  Columbia	  Univ.	  
Cyrus	  Shahabi,	  Univ.	  of	  Southern	  California	  
Dan	  Suciu,	  Univ.	  of	  Washington	  
Shiv	  Vaithyanathan,	  IBM	  
Jennifer	  Widom,	  Stanford	  Univ.	  

For citation use: Agrawal D., Bernstein P., Bertino E., Davidson S., Dayal U., Franklin M., . . . . Widom J. (2012). 
Challenges and Opportunities with Big Data: A white paper prepared for the Computing Community Consortium 
committee of the Computing Research Association. http://cra.org/ccc/resources/ccc-led-whitepapers/ 

16 

