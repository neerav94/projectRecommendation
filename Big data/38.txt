Journal of Applied Science and Engineering, Vol. 18, No. 2, pp. 135-142 (2015)

DOI: 10.6180/jase.2015.18.2.05

Research on IT Architecture of

Heterogeneous Big Data

Yun Liu*, Qi Wang and Hai-Qiang Chen

School of Communication and Information Engineering,

Key Laboratory of Communication and Information Systems,

Beijing Municipal Commission of Education, Beijing JiaoTong University,

Beijing 100044, P.R. China

Abstract

The amount of data has grown exponentially in industry and internet, and we are facing a

significant problem of information explosion. The challenge is not only to store and manage the vast

volume of data (“big data”), but also to analyze and extract meaningful value from it. This paper

analyzed the characters of the future network and the features of data generated by it. Furthermore, we

studied key issues and challenges of the multiple heterogeneous data IT infrastructure. The main

focuses of this paper are on three aspects of distributed storage, cloud computing, and data fusion.

There are several techniques to address these issues. At last, we proposed an architectural solution and

key technology program which can be adapted to developing more application functionality and meet

future demand for Internet services and new business for big data processing requirements.

Key Words: Cloud Computing, Hadoop, Data Mining, Distributed Storage

1. Introduction

tion of data processing, so before to storage data, prepro-

cessing of data cleaning, classification, filtering and in-

In recent years, the volume of data in Internet has

dexing is necessary. Secondly, big data has the charac-

been increasing significantly with the rapid development

ters of diversity and heterogeneity, and diverse hetero-

of Internet. Such kind of data called big data not only has

geneous data includes not only structured data but also

mass and high-speed characteristics, but also the diver-

unstructured data. According to the statistics, 90% data

sity and variability. Big data have many sources includ-

through the internet is unstructured data. However, pro-

ing online activities (social networking, social media),

cessing unstructured data is much more complicated

telecommunications (mobile computing, call statistics),

than structured data. Thirdly, the amount of big data is

scientific activities (simulations, experiments, environ-

so huge that the computer memory has no more space

mental sensors), and the collation of traditional sources

and a single computer cannot process it efficiently and in

(forms, surveys). Large data contains the value, but there

addition to the requirements of real-time processing and

are still many difficulties and challenges in the use of big

the processing of complicated structured data, the com-

data technologies. The larger the scale of data, the more

plication of big data processing will be considerable.

difficult the processing and storage is. Firstly, data stor-

This paper studies the characteristics of the Internet

age is required to achieve the requirements of low cost,

in the future and the challenges and technical difficulties

high capacity, high reliability, great scalability and adap-

in the conditions of big data. Then we analyzed some ex-

isting technologies and methods to deal with big data and

*Corresponding author. E-mail: liuyun@bjtu.edu.cn

also presented what challenges we were going to face in

Journal of Applied Science and Engineering, Vol. 18, No. 2, pp. 135-142 (2015)

DOI: 10.6180/jase.2015.18.2.05

Research on IT Architecture of

Heterogeneous Big Data

Yun Liu*, Qi Wang and Hai-Qiang Chen

School of Communication and Information Engineering,

Key Laboratory of Communication and Information Systems,

Beijing Municipal Commission of Education, Beijing JiaoTong University,

Beijing 100044, P.R. China

Abstract

The amount of data has grown exponentially in industry and internet, and we are facing a

significant problem of information explosion. The challenge is not only to store and manage the vast

volume of data (“big data”), but also to analyze and extract meaningful value from it. This paper

analyzed the characters of the future network and the features of data generated by it. Furthermore, we

studied key issues and challenges of the multiple heterogeneous data IT infrastructure. The main

focuses of this paper are on three aspects of distributed storage, cloud computing, and data fusion.

There are several techniques to address these issues. At last, we proposed an architectural solution and

key technology program which can be adapted to developing more application functionality and meet

future demand for Internet services and new business for big data processing requirements.

Key Words: Cloud Computing, Hadoop, Data Mining, Distributed Storage

1. Introduction

tion of data processing, so before to storage data, prepro-

cessing of data cleaning, classification, filtering and in-

In recent years, the volume of data in Internet has

dexing is necessary. Secondly, big data has the charac-

been increasing significantly with the rapid development

ters of diversity and heterogeneity, and diverse hetero-

of Internet. Such kind of data called big data not only has

geneous data includes not only structured data but also

mass and high-speed characteristics, but also the diver-

unstructured data. According to the statistics, 90% data

sity and variability. Big data have many sources includ-

through the internet is unstructured data. However, pro-

ing online activities (social networking, social media),

cessing unstructured data is much more complicated

telecommunications (mobile computing, call statistics),

than structured data. Thirdly, the amount of big data is

scientific activities (simulations, experiments, environ-

so huge that the computer memory has no more space

mental sensors), and the collation of traditional sources

and a single computer cannot process it efficiently and in

(forms, surveys). Large data contains the value, but there

addition to the requirements of real-time processing and

are still many difficulties and challenges in the use of big

the processing of complicated structured data, the com-

data technologies. The larger the scale of data, the more

plication of big data processing will be considerable.

difficult the processing and storage is. Firstly, data stor-

This paper studies the characteristics of the Internet

age is required to achieve the requirements of low cost,

in the future and the challenges and technical difficulties

high capacity, high reliability, great scalability and adap-

in the conditions of big data. Then we analyzed some ex-

isting technologies and methods to deal with big data and

*Corresponding author. E-mail: liuyun@bjtu.edu.cn

also presented what challenges we were going to face in

136

Yun Liu et al.

the future. In order to address these challenges, we pro-

vices are very few. So we are far behind foreign at both

posed an architectural solution and key technology pro-

techniques and services.

gram which can be adapted to developing more applica-

Multiple heterogeneous big data have the character-

tion functionality and meet future demand for Internet

istics of multiple dimensions, multiple sources, structured

services and new business for big data processing re-

+ semi-structured + unstructured heterogeneous data and

quirements.

the huge amount of data, thus the traditional relational

The remainder of this paper is organized as follows:

database has been very difficult to store such data. In ad-

section 2 gives an overview of major approaches for big

dition, the demand for the processing of such data is not

data processing. Section 3 describes our multiple hetero-

limited to the mode supported by SQL, but shows the

geneous data IT Infrastructure, including four parts. A

features of the connection and interweaving between the

detailed discussion of the proposed infrastructure is pre-

data on the Internet. There are a lot of storage methods

sented in section 4. Finally, section 5 provides some con-

for heterogeneous data [6], such as serial method, graph

cluding remarks.

method, tree method, file system method, database field

method and object method. Due to the difference be-

2. Related Work

tween the services of application layer, the characteris-

tics of multi-source heterogeneous data also are different

Google is the advocator and promoter of big data

with different services, so our data storage and manage-

processing, and its famous modules of Bigtable [1], GFS,

ment must use corresponding methods for differing ser-

MapReduce cause the many commercial systems and

vices, for example, Bigtable [1], the key value of NoSQL

open source tools birth. Bigtable is a distributed structured

and so on.

data storage system, which is designed to handle massive

Data fusion is a research hotspot and has several ad-

amounts of data: usually the PB level data which is dis-

vantages [7,8]. These advantages mainly involved en-

tributed in the thousands of ordinary servers. GFS [2] is a

hancements in data authenticity or availability. Examples

scalable distributed file system. It is a dedicated file sys-

include improved detection, confidence, reliability, as

tem designed to store massive amounts of search data.

well as reduction in data ambiguity while extending spa-

GFS is used for large-scale, distributed, large amounts of

tial and temporal coverage. There are a lot of concep-

data access applications and runs on inexpensive com-

tualizations of data fusion. Among them, JDL model [9]

modity hardware, but can provide fault tolerance, and

is the most common and popular. The JDL is originated

higher overall performance of the services provided to a

from the military domain and it considers the fusion pro-

large number of users. MapReduce [3] is an algorithm

cess in four increasing level of abstraction, namely ob-

model and associated implementation for processing and

ject, situation, impact, and process refinement. However,

generating big data sets. Yahoo’s outstanding contribution

it has many shortcomings, like the too much limitation

to big data processing is that they created the open source

which has been the subject of several extension resear-

framework Hadoop [4,5] based on the MapReduce model

ches [10,11] attempting to alleviate them. Dasarathy’s fra-

and promoted HBase, Hive, and other peripheral tech-

mework [12] views the fusion system, from a software

nologies, making Hadoop the base of most current big

engineering perspective, as a data flow characterized by

data processing products. In our country, Baidu, Tencent,

input/output as well as functionalities. One of the most

Alibaba and other Internet companies to provide ser-

recent fusion frameworks is proposed by Kokar et al.

vices for the network has been faced with large data pro-

[13].

cessing needs, but has also been developing and using

Web services is an online application service pub-

large data processing technologies. However, solutions

lished by service providers in order to solve some spe-

and frameworks purely providing data processing ser-

cific business requirements. The target of web service is

Journal of Applied Science and Engineering, Vol. 18, No. 2, pp. 135-142 (2015)

DOI: 10.6180/jase.2015.18.2.05

Research on IT Architecture of

Heterogeneous Big Data

Yun Liu*, Qi Wang and Hai-Qiang Chen

School of Communication and Information Engineering,

Key Laboratory of Communication and Information Systems,

Beijing Municipal Commission of Education, Beijing JiaoTong University,

Beijing 100044, P.R. China

Abstract

The amount of data has grown exponentially in industry and internet, and we are facing a

significant problem of information explosion. The challenge is not only to store and manage the vast

volume of data (“big data”), but also to analyze and extract meaningful value from it. This paper

analyzed the characters of the future network and the features of data generated by it. Furthermore, we

studied key issues and challenges of the multiple heterogeneous data IT infrastructure. The main

focuses of this paper are on three aspects of distributed storage, cloud computing, and data fusion.

There are several techniques to address these issues. At last, we proposed an architectural solution and

key technology program which can be adapted to developing more application functionality and meet

future demand for Internet services and new business for big data processing requirements.

Key Words: Cloud Computing, Hadoop, Data Mining, Distributed Storage

1. Introduction

tion of data processing, so before to storage data, prepro-

cessing of data cleaning, classification, filtering and in-

In recent years, the volume of data in Internet has

dexing is necessary. Secondly, big data has the charac-

been increasing significantly with the rapid development

ters of diversity and heterogeneity, and diverse hetero-

of Internet. Such kind of data called big data not only has

geneous data includes not only structured data but also

mass and high-speed characteristics, but also the diver-

unstructured data. According to the statistics, 90% data

sity and variability. Big data have many sources includ-

through the internet is unstructured data. However, pro-

ing online activities (social networking, social media),

cessing unstructured data is much more complicated

telecommunications (mobile computing, call statistics),

than structured data. Thirdly, the amount of big data is

scientific activities (simulations, experiments, environ-

so huge that the computer memory has no more space

mental sensors), and the collation of traditional sources

and a single computer cannot process it efficiently and in

(forms, surveys). Large data contains the value, but there

addition to the requirements of real-time processing and

are still many difficulties and challenges in the use of big

the processing of complicated structured data, the com-

data technologies. The larger the scale of data, the more

plication of big data processing will be considerable.

difficult the processing and storage is. Firstly, data stor-

This paper studies the characteristics of the Internet

age is required to achieve the requirements of low cost,

in the future and the challenges and technical difficulties

high capacity, high reliability, great scalability and adap-

in the conditions of big data. Then we analyzed some ex-

isting technologies and methods to deal with big data and

*Corresponding author. E-mail: liuyun@bjtu.edu.cn

also presented what challenges we were going to face in

136

Yun Liu et al.

the future. In order to address these challenges, we pro-

vices are very few. So we are far behind foreign at both

posed an architectural solution and key technology pro-

techniques and services.

gram which can be adapted to developing more applica-

Multiple heterogeneous big data have the character-

tion functionality and meet future demand for Internet

istics of multiple dimensions, multiple sources, structured

services and new business for big data processing re-

+ semi-structured + unstructured heterogeneous data and

quirements.

the huge amount of data, thus the traditional relational

The remainder of this paper is organized as follows:

database has been very difficult to store such data. In ad-

section 2 gives an overview of major approaches for big

dition, the demand for the processing of such data is not

data processing. Section 3 describes our multiple hetero-

limited to the mode supported by SQL, but shows the

geneous data IT Infrastructure, including four parts. A

features of the connection and interweaving between the

detailed discussion of the proposed infrastructure is pre-

data on the Internet. There are a lot of storage methods

sented in section 4. Finally, section 5 provides some con-

for heterogeneous data [6], such as serial method, graph

cluding remarks.

method, tree method, file system method, database field

method and object method. Due to the difference be-

2. Related Work

tween the services of application layer, the characteris-

tics of multi-source heterogeneous data also are different

Google is the advocator and promoter of big data

with different services, so our data storage and manage-

processing, and its famous modules of Bigtable [1], GFS,

ment must use corresponding methods for differing ser-

MapReduce cause the many commercial systems and

vices, for example, Bigtable [1], the key value of NoSQL

open source tools birth. Bigtable is a distributed structured

and so on.

data storage system, which is designed to handle massive

Data fusion is a research hotspot and has several ad-

amounts of data: usually the PB level data which is dis-

vantages [7,8]. These advantages mainly involved en-

tributed in the thousands of ordinary servers. GFS [2] is a

hancements in data authenticity or availability. Examples

scalable distributed file system. It is a dedicated file sys-

include improved detection, confidence, reliability, as

tem designed to store massive amounts of search data.

well as reduction in data ambiguity while extending spa-

GFS is used for large-scale, distributed, large amounts of

tial and temporal coverage. There are a lot of concep-

data access applications and runs on inexpensive com-

tualizations of data fusion. Among them, JDL model [9]

modity hardware, but can provide fault tolerance, and

is the most common and popular. The JDL is originated

higher overall performance of the services provided to a

from the military domain and it considers the fusion pro-

large number of users. MapReduce [3] is an algorithm

cess in four increasing level of abstraction, namely ob-

model and associated implementation for processing and

ject, situation, impact, and process refinement. However,

generating big data sets. Yahoo’s outstanding contribution

it has many shortcomings, like the too much limitation

to big data processing is that they created the open source

which has been the subject of several extension resear-

framework Hadoop [4,5] based on the MapReduce model

ches [10,11] attempting to alleviate them. Dasarathy’s fra-

and promoted HBase, Hive, and other peripheral tech-

mework [12] views the fusion system, from a software

nologies, making Hadoop the base of most current big

engineering perspective, as a data flow characterized by

data processing products. In our country, Baidu, Tencent,

input/output as well as functionalities. One of the most

Alibaba and other Internet companies to provide ser-

recent fusion frameworks is proposed by Kokar et al.

vices for the network has been faced with large data pro-

[13].

cessing needs, but has also been developing and using

Web services is an online application service pub-

large data processing technologies. However, solutions

lished by service providers in order to solve some spe-

and frameworks purely providing data processing ser-

cific business requirements. The target of web service is

Research on IT Architecture of Heterogeneous Big Data

137

transferring software to subscription services through

mass storage, so we cannot storage and manage them

Internet and its essence is realizing the online data con-

like the traditional solutions treating them as equally im-

version by XML [14]. Fan et al. [15] proposed enterprise

portant. However, hierarchical storage is using different

application integration based on web service and Min-

storage methods corresponding to the characteristics and

Hsiung Hung et al. [16,17] presented a web services based

values of different data and utilizes the hierarchical stor-

e-diagnostics framework for semiconductor manufac-

age management software to achieve automatic sorting

turing industry. Therefore, web service is a significant

and automatic storage, which greatly increases the effec-

way to solve heterogeneous data exchange problem in

tiveness and speed of data storage and meets the storage

big data. However, the existing frameworks are mainly

needs of different kinds of data. In our system, there are

aimed at special businesses, which hardly to extensive

two kinds of data which are static data and dynamic data

apply in other industries, and there is lack of overall ar-

respectively. The framework of hierarchical storage is

chitecture including heterogeneous data storage and ex-

shown in Figure 1.

change module.

In order to achieve the storage of big data, especially

the heterogeneous data storage of the structured data,

3. Multiple Heterogeneous Data IT

semi-structured data and unstructured data, we super-

Infrastructure

impose a relational database and a distributed non-rela-

tional database on a distributed file system to store huge

Multiple heterogeneous big data have the character-

amounts of data which are managed by the master +

istics of multiple dimensions, multiple sources, structured

multi-slave physical structure. Master nodes and multi-

+ semi-structured + unstructured heterogeneous data and

node slave nodes are connected via Internet. Applica-

the huge amount of data. These features of big data bring

tions get access to data through the master host, and

us three aspects of challenges at the heterogeneous big

each storage node in the network is a separated database

data tiered storage and storage management, the inte-

without sharing with other storage nodes. Data is ex-

gration of heterogeneous data, the off-line and realtime

changed between master node and multi-node storage

computing architecture and the efficient transmission

nodes. Slave nodes are connected via the Internet and

mode of big data. This technical structure consists of un-

they complete the same task, so it is a server system from

derlying file system, structured data storage system, semi-

structured data and unstructured data storage system, the

distributed storage of data, the unified data access inter-

face, data indexing and positioning, processing task de-

composition and scheduling management, e distributed

execution of processing tasks, e service interface of the

system and the secondary development interface and sys-

tem manageability and security and completely defined a

data storage and processing platform meeting the speci-

fic needs. In the following paper, we presented the four

aspects of our big data framework respectively.

3.1 Multiple Heterogeneous Data Tiered Storage

and Distributed Storage

In this paper, we used the multiple heterogeneous

data tiered storage as our storage method. Big data needs

Figure 1. Hierarchical storage diagram.

Journal of Applied Science and Engineering, Vol. 18, No. 2, pp. 135-142 (2015)

DOI: 10.6180/jase.2015.18.2.05

Research on IT Architecture of

Heterogeneous Big Data

Yun Liu*, Qi Wang and Hai-Qiang Chen

School of Communication and Information Engineering,

Key Laboratory of Communication and Information Systems,

Beijing Municipal Commission of Education, Beijing JiaoTong University,

Beijing 100044, P.R. China

Abstract

The amount of data has grown exponentially in industry and internet, and we are facing a

significant problem of information explosion. The challenge is not only to store and manage the vast

volume of data (“big data”), but also to analyze and extract meaningful value from it. This paper

analyzed the characters of the future network and the features of data generated by it. Furthermore, we

studied key issues and challenges of the multiple heterogeneous data IT infrastructure. The main

focuses of this paper are on three aspects of distributed storage, cloud computing, and data fusion.

There are several techniques to address these issues. At last, we proposed an architectural solution and

key technology program which can be adapted to developing more application functionality and meet

future demand for Internet services and new business for big data processing requirements.

Key Words: Cloud Computing, Hadoop, Data Mining, Distributed Storage

1. Introduction

tion of data processing, so before to storage data, prepro-

cessing of data cleaning, classification, filtering and in-

In recent years, the volume of data in Internet has

dexing is necessary. Secondly, big data has the charac-

been increasing significantly with the rapid development

ters of diversity and heterogeneity, and diverse hetero-

of Internet. Such kind of data called big data not only has

geneous data includes not only structured data but also

mass and high-speed characteristics, but also the diver-

unstructured data. According to the statistics, 90% data

sity and variability. Big data have many sources includ-

through the internet is unstructured data. However, pro-

ing online activities (social networking, social media),

cessing unstructured data is much more complicated

telecommunications (mobile computing, call statistics),

than structured data. Thirdly, the amount of big data is

scientific activities (simulations, experiments, environ-

so huge that the computer memory has no more space

mental sensors), and the collation of traditional sources

and a single computer cannot process it efficiently and in

(forms, surveys). Large data contains the value, but there

addition to the requirements of real-time processing and

are still many difficulties and challenges in the use of big

the processing of complicated structured data, the com-

data technologies. The larger the scale of data, the more

plication of big data processing will be considerable.

difficult the processing and storage is. Firstly, data stor-

This paper studies the characteristics of the Internet

age is required to achieve the requirements of low cost,

in the future and the challenges and technical difficulties

high capacity, high reliability, great scalability and adap-

in the conditions of big data. Then we analyzed some ex-

isting technologies and methods to deal with big data and

*Corresponding author. E-mail: liuyun@bjtu.edu.cn

also presented what challenges we were going to face in

136

Yun Liu et al.

the future. In order to address these challenges, we pro-

vices are very few. So we are far behind foreign at both

posed an architectural solution and key technology pro-

techniques and services.

gram which can be adapted to developing more applica-

Multiple heterogeneous big data have the character-

tion functionality and meet future demand for Internet

istics of multiple dimensions, multiple sources, structured

services and new business for big data processing re-

+ semi-structured + unstructured heterogeneous data and

quirements.

the huge amount of data, thus the traditional relational

The remainder of this paper is organized as follows:

database has been very difficult to store such data. In ad-

section 2 gives an overview of major approaches for big

dition, the demand for the processing of such data is not

data processing. Section 3 describes our multiple hetero-

limited to the mode supported by SQL, but shows the

geneous data IT Infrastructure, including four parts. A

features of the connection and interweaving between the

detailed discussion of the proposed infrastructure is pre-

data on the Internet. There are a lot of storage methods

sented in section 4. Finally, section 5 provides some con-

for heterogeneous data [6], such as serial method, graph

cluding remarks.

method, tree method, file system method, database field

method and object method. Due to the difference be-

2. Related Work

tween the services of application layer, the characteris-

tics of multi-source heterogeneous data also are different

Google is the advocator and promoter of big data

with different services, so our data storage and manage-

processing, and its famous modules of Bigtable [1], GFS,

ment must use corresponding methods for differing ser-

MapReduce cause the many commercial systems and

vices, for example, Bigtable [1], the key value of NoSQL

open source tools birth. Bigtable is a distributed structured

and so on.

data storage system, which is designed to handle massive

Data fusion is a research hotspot and has several ad-

amounts of data: usually the PB level data which is dis-

vantages [7,8]. These advantages mainly involved en-

tributed in the thousands of ordinary servers. GFS [2] is a

hancements in data authenticity or availability. Examples

scalable distributed file system. It is a dedicated file sys-

include improved detection, confidence, reliability, as

tem designed to store massive amounts of search data.

well as reduction in data ambiguity while extending spa-

GFS is used for large-scale, distributed, large amounts of

tial and temporal coverage. There are a lot of concep-

data access applications and runs on inexpensive com-

tualizations of data fusion. Among them, JDL model [9]

modity hardware, but can provide fault tolerance, and

is the most common and popular. The JDL is originated

higher overall performance of the services provided to a

from the military domain and it considers the fusion pro-

large number of users. MapReduce [3] is an algorithm

cess in four increasing level of abstraction, namely ob-

model and associated implementation for processing and

ject, situation, impact, and process refinement. However,

generating big data sets. Yahoo’s outstanding contribution

it has many shortcomings, like the too much limitation

to big data processing is that they created the open source

which has been the subject of several extension resear-

framework Hadoop [4,5] based on the MapReduce model

ches [10,11] attempting to alleviate them. Dasarathy’s fra-

and promoted HBase, Hive, and other peripheral tech-

mework [12] views the fusion system, from a software

nologies, making Hadoop the base of most current big

engineering perspective, as a data flow characterized by

data processing products. In our country, Baidu, Tencent,

input/output as well as functionalities. One of the most

Alibaba and other Internet companies to provide ser-

recent fusion frameworks is proposed by Kokar et al.

vices for the network has been faced with large data pro-

[13].

cessing needs, but has also been developing and using

Web services is an online application service pub-

large data processing technologies. However, solutions

lished by service providers in order to solve some spe-

and frameworks purely providing data processing ser-

cific business requirements. The target of web service is

Research on IT Architecture of Heterogeneous Big Data

137

transferring software to subscription services through

mass storage, so we cannot storage and manage them

Internet and its essence is realizing the online data con-

like the traditional solutions treating them as equally im-

version by XML [14]. Fan et al. [15] proposed enterprise

portant. However, hierarchical storage is using different

application integration based on web service and Min-

storage methods corresponding to the characteristics and

Hsiung Hung et al. [16,17] presented a web services based

values of different data and utilizes the hierarchical stor-

e-diagnostics framework for semiconductor manufac-

age management software to achieve automatic sorting

turing industry. Therefore, web service is a significant

and automatic storage, which greatly increases the effec-

way to solve heterogeneous data exchange problem in

tiveness and speed of data storage and meets the storage

big data. However, the existing frameworks are mainly

needs of different kinds of data. In our system, there are

aimed at special businesses, which hardly to extensive

two kinds of data which are static data and dynamic data

apply in other industries, and there is lack of overall ar-

respectively. The framework of hierarchical storage is

chitecture including heterogeneous data storage and ex-

shown in Figure 1.

change module.

In order to achieve the storage of big data, especially

the heterogeneous data storage of the structured data,

3. Multiple Heterogeneous Data IT

semi-structured data and unstructured data, we super-

Infrastructure

impose a relational database and a distributed non-rela-

tional database on a distributed file system to store huge

Multiple heterogeneous big data have the character-

amounts of data which are managed by the master +

istics of multiple dimensions, multiple sources, structured

multi-slave physical structure. Master nodes and multi-

+ semi-structured + unstructured heterogeneous data and

node slave nodes are connected via Internet. Applica-

the huge amount of data. These features of big data bring

tions get access to data through the master host, and

us three aspects of challenges at the heterogeneous big

each storage node in the network is a separated database

data tiered storage and storage management, the inte-

without sharing with other storage nodes. Data is ex-

gration of heterogeneous data, the off-line and realtime

changed between master node and multi-node storage

computing architecture and the efficient transmission

nodes. Slave nodes are connected via the Internet and

mode of big data. This technical structure consists of un-

they complete the same task, so it is a server system from

derlying file system, structured data storage system, semi-

structured data and unstructured data storage system, the

distributed storage of data, the unified data access inter-

face, data indexing and positioning, processing task de-

composition and scheduling management, e distributed

execution of processing tasks, e service interface of the

system and the secondary development interface and sys-

tem manageability and security and completely defined a

data storage and processing platform meeting the speci-

fic needs. In the following paper, we presented the four

aspects of our big data framework respectively.

3.1 Multiple Heterogeneous Data Tiered Storage

and Distributed Storage

In this paper, we used the multiple heterogeneous

data tiered storage as our storage method. Big data needs

Figure 1. Hierarchical storage diagram.

138

Yun Liu et al.

the user’s prospective. In this structure, each node only

fusion, feature-level fusion and decision fusion. The pixel-

have right to get access to the local resources, including

based fusion also has a function similar to the data de-

memory, storage and etc. It is a completely non-sharing

noising and the cleaning process in addition to the sig-

structure with great scalability, and theoretically unli-

nificance of fusion. Feature-level fusion classifies data

mited period of expansion, the current technology can

based on the features and data attributes. Decision fu-

achieve 512 nodes interconnected, thousands of CPU.

sion uses data to implement the trend assessment and

Each node can run its own database, operating system,

the macro perspective service processing at the highest

but each node cannot access the memory of other nodes,

level. According to the specific services and the specific

and information exchange between nodes is implemented

characteristics of the data, the corresponding fusion al-

via the Internet node. This process is known as data real-

gorithms and computational structures are used to treat

location. In this basic data storage mode, according to

on different data. Utilizing data fusion technology, we

the specific business needs and the upper data features,

can remove redundant information to reduce the amount

the designed specific big data storage architecture is

of data increase the access efficiency. Moreover, we can

shown as Figure 2.

extract useful information from a large number of heter-

ogeneous data, providing services and support for the

3.2 Heterogeneous Data Fusion

business of the upper services. Figure 3 shows the Func-

Heterogeneous data fusion refers to using appropri-

tional model of heterogeneous data fusion.

ate processing methods to process the heterogeneous data

and producing complete accurate timely and effective in-

3.3 Off-line and Online Computation Framework

tegrated information. For data fusion, multi-sensor sys-

There are off-line and online computing demands in

tem is the hardware basic, multi-source heterogeneous

a large data environment. Off-line computation is more

information is its processing objects and optimism and

relaxed in the computing time requirement. However, un-

integrated processing is its core. Sensor is the source of

der the conditions of mass data, there are still computing

heterogeneous data, but it is not necessarily physical

time limit and the balance between recalculation and the

form. In other words, all the data sources and manual data

incremental calculation becomes an inevitable problem

source can be called sensor.

with the increasing of data. Online calculation has a higher

In our paper, we adopted fusion framework which is

requirement on the computing time, which is a quit hard

based on category theory and is claimed to be sufficiently

problem for the mass data and needs to a variety of me-

general to capture all kinds of fusion, including pixel-

thods to guarantee the calculation real-time.

based fusion, feature fusion and decision fusion. Fusion

of the data can be divided into three levels of pixel-based

Figure 2. Distributed file system structure.

Figure 3. Heterogeneous data fusion diagram.

Journal of Applied Science and Engineering, Vol. 18, No. 2, pp. 135-142 (2015)

DOI: 10.6180/jase.2015.18.2.05

Research on IT Architecture of

Heterogeneous Big Data

Yun Liu*, Qi Wang and Hai-Qiang Chen

School of Communication and Information Engineering,

Key Laboratory of Communication and Information Systems,

Beijing Municipal Commission of Education, Beijing JiaoTong University,

Beijing 100044, P.R. China

Abstract

The amount of data has grown exponentially in industry and internet, and we are facing a

significant problem of information explosion. The challenge is not only to store and manage the vast

volume of data (“big data”), but also to analyze and extract meaningful value from it. This paper

analyzed the characters of the future network and the features of data generated by it. Furthermore, we

studied key issues and challenges of the multiple heterogeneous data IT infrastructure. The main

focuses of this paper are on three aspects of distributed storage, cloud computing, and data fusion.

There are several techniques to address these issues. At last, we proposed an architectural solution and

key technology program which can be adapted to developing more application functionality and meet

future demand for Internet services and new business for big data processing requirements.

Key Words: Cloud Computing, Hadoop, Data Mining, Distributed Storage

1. Introduction

tion of data processing, so before to storage data, prepro-

cessing of data cleaning, classification, filtering and in-

In recent years, the volume of data in Internet has

dexing is necessary. Secondly, big data has the charac-

been increasing significantly with the rapid development

ters of diversity and heterogeneity, and diverse hetero-

of Internet. Such kind of data called big data not only has

geneous data includes not only structured data but also

mass and high-speed characteristics, but also the diver-

unstructured data. According to the statistics, 90% data

sity and variability. Big data have many sources includ-

through the internet is unstructured data. However, pro-

ing online activities (social networking, social media),

cessing unstructured data is much more complicated

telecommunications (mobile computing, call statistics),

than structured data. Thirdly, the amount of big data is

scientific activities (simulations, experiments, environ-

so huge that the computer memory has no more space

mental sensors), and the collation of traditional sources

and a single computer cannot process it efficiently and in

(forms, surveys). Large data contains the value, but there

addition to the requirements of real-time processing and

are still many difficulties and challenges in the use of big

the processing of complicated structured data, the com-

data technologies. The larger the scale of data, the more

plication of big data processing will be considerable.

difficult the processing and storage is. Firstly, data stor-

This paper studies the characteristics of the Internet

age is required to achieve the requirements of low cost,

in the future and the challenges and technical difficulties

high capacity, high reliability, great scalability and adap-

in the conditions of big data. Then we analyzed some ex-

isting technologies and methods to deal with big data and

*Corresponding author. E-mail: liuyun@bjtu.edu.cn

also presented what challenges we were going to face in

136

Yun Liu et al.

the future. In order to address these challenges, we pro-

vices are very few. So we are far behind foreign at both

posed an architectural solution and key technology pro-

techniques and services.

gram which can be adapted to developing more applica-

Multiple heterogeneous big data have the character-

tion functionality and meet future demand for Internet

istics of multiple dimensions, multiple sources, structured

services and new business for big data processing re-

+ semi-structured + unstructured heterogeneous data and

quirements.

the huge amount of data, thus the traditional relational

The remainder of this paper is organized as follows:

database has been very difficult to store such data. In ad-

section 2 gives an overview of major approaches for big

dition, the demand for the processing of such data is not

data processing. Section 3 describes our multiple hetero-

limited to the mode supported by SQL, but shows the

geneous data IT Infrastructure, including four parts. A

features of the connection and interweaving between the

detailed discussion of the proposed infrastructure is pre-

data on the Internet. There are a lot of storage methods

sented in section 4. Finally, section 5 provides some con-

for heterogeneous data [6], such as serial method, graph

cluding remarks.

method, tree method, file system method, database field

method and object method. Due to the difference be-

2. Related Work

tween the services of application layer, the characteris-

tics of multi-source heterogeneous data also are different

Google is the advocator and promoter of big data

with different services, so our data storage and manage-

processing, and its famous modules of Bigtable [1], GFS,

ment must use corresponding methods for differing ser-

MapReduce cause the many commercial systems and

vices, for example, Bigtable [1], the key value of NoSQL

open source tools birth. Bigtable is a distributed structured

and so on.

data storage system, which is designed to handle massive

Data fusion is a research hotspot and has several ad-

amounts of data: usually the PB level data which is dis-

vantages [7,8]. These advantages mainly involved en-

tributed in the thousands of ordinary servers. GFS [2] is a

hancements in data authenticity or availability. Examples

scalable distributed file system. It is a dedicated file sys-

include improved detection, confidence, reliability, as

tem designed to store massive amounts of search data.

well as reduction in data ambiguity while extending spa-

GFS is used for large-scale, distributed, large amounts of

tial and temporal coverage. There are a lot of concep-

data access applications and runs on inexpensive com-

tualizations of data fusion. Among them, JDL model [9]

modity hardware, but can provide fault tolerance, and

is the most common and popular. The JDL is originated

higher overall performance of the services provided to a

from the military domain and it considers the fusion pro-

large number of users. MapReduce [3] is an algorithm

cess in four increasing level of abstraction, namely ob-

model and associated implementation for processing and

ject, situation, impact, and process refinement. However,

generating big data sets. Yahoo’s outstanding contribution

it has many shortcomings, like the too much limitation

to big data processing is that they created the open source

which has been the subject of several extension resear-

framework Hadoop [4,5] based on the MapReduce model

ches [10,11] attempting to alleviate them. Dasarathy’s fra-

and promoted HBase, Hive, and other peripheral tech-

mework [12] views the fusion system, from a software

nologies, making Hadoop the base of most current big

engineering perspective, as a data flow characterized by

data processing products. In our country, Baidu, Tencent,

input/output as well as functionalities. One of the most

Alibaba and other Internet companies to provide ser-

recent fusion frameworks is proposed by Kokar et al.

vices for the network has been faced with large data pro-

[13].

cessing needs, but has also been developing and using

Web services is an online application service pub-

large data processing technologies. However, solutions

lished by service providers in order to solve some spe-

and frameworks purely providing data processing ser-

cific business requirements. The target of web service is

Research on IT Architecture of Heterogeneous Big Data

137

transferring software to subscription services through

mass storage, so we cannot storage and manage them

Internet and its essence is realizing the online data con-

like the traditional solutions treating them as equally im-

version by XML [14]. Fan et al. [15] proposed enterprise

portant. However, hierarchical storage is using different

application integration based on web service and Min-

storage methods corresponding to the characteristics and

Hsiung Hung et al. [16,17] presented a web services based

values of different data and utilizes the hierarchical stor-

e-diagnostics framework for semiconductor manufac-

age management software to achieve automatic sorting

turing industry. Therefore, web service is a significant

and automatic storage, which greatly increases the effec-

way to solve heterogeneous data exchange problem in

tiveness and speed of data storage and meets the storage

big data. However, the existing frameworks are mainly

needs of different kinds of data. In our system, there are

aimed at special businesses, which hardly to extensive

two kinds of data which are static data and dynamic data

apply in other industries, and there is lack of overall ar-

respectively. The framework of hierarchical storage is

chitecture including heterogeneous data storage and ex-

shown in Figure 1.

change module.

In order to achieve the storage of big data, especially

the heterogeneous data storage of the structured data,

3. Multiple Heterogeneous Data IT

semi-structured data and unstructured data, we super-

Infrastructure

impose a relational database and a distributed non-rela-

tional database on a distributed file system to store huge

Multiple heterogeneous big data have the character-

amounts of data which are managed by the master +

istics of multiple dimensions, multiple sources, structured

multi-slave physical structure. Master nodes and multi-

+ semi-structured + unstructured heterogeneous data and

node slave nodes are connected via Internet. Applica-

the huge amount of data. These features of big data bring

tions get access to data through the master host, and

us three aspects of challenges at the heterogeneous big

each storage node in the network is a separated database

data tiered storage and storage management, the inte-

without sharing with other storage nodes. Data is ex-

gration of heterogeneous data, the off-line and realtime

changed between master node and multi-node storage

computing architecture and the efficient transmission

nodes. Slave nodes are connected via the Internet and

mode of big data. This technical structure consists of un-

they complete the same task, so it is a server system from

derlying file system, structured data storage system, semi-

structured data and unstructured data storage system, the

distributed storage of data, the unified data access inter-

face, data indexing and positioning, processing task de-

composition and scheduling management, e distributed

execution of processing tasks, e service interface of the

system and the secondary development interface and sys-

tem manageability and security and completely defined a

data storage and processing platform meeting the speci-

fic needs. In the following paper, we presented the four

aspects of our big data framework respectively.

3.1 Multiple Heterogeneous Data Tiered Storage

and Distributed Storage

In this paper, we used the multiple heterogeneous

data tiered storage as our storage method. Big data needs

Figure 1. Hierarchical storage diagram.

138

Yun Liu et al.

the user’s prospective. In this structure, each node only

fusion, feature-level fusion and decision fusion. The pixel-

have right to get access to the local resources, including

based fusion also has a function similar to the data de-

memory, storage and etc. It is a completely non-sharing

noising and the cleaning process in addition to the sig-

structure with great scalability, and theoretically unli-

nificance of fusion. Feature-level fusion classifies data

mited period of expansion, the current technology can

based on the features and data attributes. Decision fu-

achieve 512 nodes interconnected, thousands of CPU.

sion uses data to implement the trend assessment and

Each node can run its own database, operating system,

the macro perspective service processing at the highest

but each node cannot access the memory of other nodes,

level. According to the specific services and the specific

and information exchange between nodes is implemented

characteristics of the data, the corresponding fusion al-

via the Internet node. This process is known as data real-

gorithms and computational structures are used to treat

location. In this basic data storage mode, according to

on different data. Utilizing data fusion technology, we

the specific business needs and the upper data features,

can remove redundant information to reduce the amount

the designed specific big data storage architecture is

of data increase the access efficiency. Moreover, we can

shown as Figure 2.

extract useful information from a large number of heter-

ogeneous data, providing services and support for the

3.2 Heterogeneous Data Fusion

business of the upper services. Figure 3 shows the Func-

Heterogeneous data fusion refers to using appropri-

tional model of heterogeneous data fusion.

ate processing methods to process the heterogeneous data

and producing complete accurate timely and effective in-

3.3 Off-line and Online Computation Framework

tegrated information. For data fusion, multi-sensor sys-

There are off-line and online computing demands in

tem is the hardware basic, multi-source heterogeneous

a large data environment. Off-line computation is more

information is its processing objects and optimism and

relaxed in the computing time requirement. However, un-

integrated processing is its core. Sensor is the source of

der the conditions of mass data, there are still computing

heterogeneous data, but it is not necessarily physical

time limit and the balance between recalculation and the

form. In other words, all the data sources and manual data

incremental calculation becomes an inevitable problem

source can be called sensor.

with the increasing of data. Online calculation has a higher

In our paper, we adopted fusion framework which is

requirement on the computing time, which is a quit hard

based on category theory and is claimed to be sufficiently

problem for the mass data and needs to a variety of me-

general to capture all kinds of fusion, including pixel-

thods to guarantee the calculation real-time.

based fusion, feature fusion and decision fusion. Fusion

of the data can be divided into three levels of pixel-based

Figure 2. Distributed file system structure.

Figure 3. Heterogeneous data fusion diagram.

Research on IT Architecture of Heterogeneous Big Data

139

In this paper, we classed the data into static data and

cessing. Storm is a common stream computing frame-

dynamic data. Static data are the historical read-only data,

work which has been widely used for real-time log pro-

but dynamic data are the read and write data including

cessing, real-time statistics, real-time risk control. Storm

intermediate result. For static data, when we implement

also is used to process the data preliminarily and it can

off-line computation, we must design a reasonable storage

store the data in a distributed database like HBase in or-

structure and create effective index to improve the pro-

der to facilitate the subsequent queries. Storm is a scal-

cessing efficiency according to the demands of specific

able, low-latency, reliable and fault- tolerant distributed

services. Hadoop [4] use the idea of MapReduce [3]. Ha-

computing platform. In addition, we will employ the di-

doop is a Java implementation of Google’s MapReduce.

stributed computing mode for data computation and pro-

It slices the data to deal with large amount of off-line

cessing and the assignment, scheming and management

data and then assign computing tasks to more than one

of the computing tasks. The off- line and online compu-

computer, which will greatly improve the speed and ef-

tation framework is shown in Figure 5.

ficiency of computation. Online computation needs to

use reasonable caching mechanism to solve the massive

3.4 Efficient Interactive Transmission Mode of Big

data processing problem. MapReduce is a simplified

Data

distributed programming model that allows the pro-

Storage and processing of big data necessarily invo-

gram automatically distributed to ordinary machine con-

lves the network-based distributed storage and computa-

sisting of a large cluster of concurrent and be executed.

tion, the collection of multi-source data, the remote data

A MapReduce job will usually slice the input data set

into separate blocks of data and deal with in a comp-

letely parallel way by the Map task. The framework will

first sort the output of Map and then distribute the out-

put to Reduce task. Usually the input and output opera-

tions will be stored in the file system. The framework

takes the task scheduling and monitoring, and re-run

the failed tasks. Hadoop MapReduce processing flow is

shown in Figure 4. For dynamic data, we will combine

data structure designing, index designing and caching

mechanism designing together to complete the data pro-

Figure 5. Off-line and online computation.

Figure 4. Hadoop MapReduce Processing flow.

Journal of Applied Science and Engineering, Vol. 18, No. 2, pp. 135-142 (2015)

DOI: 10.6180/jase.2015.18.2.05

Research on IT Architecture of

Heterogeneous Big Data

Yun Liu*, Qi Wang and Hai-Qiang Chen

School of Communication and Information Engineering,

Key Laboratory of Communication and Information Systems,

Beijing Municipal Commission of Education, Beijing JiaoTong University,

Beijing 100044, P.R. China

Abstract

The amount of data has grown exponentially in industry and internet, and we are facing a

significant problem of information explosion. The challenge is not only to store and manage the vast

volume of data (“big data”), but also to analyze and extract meaningful value from it. This paper

analyzed the characters of the future network and the features of data generated by it. Furthermore, we

studied key issues and challenges of the multiple heterogeneous data IT infrastructure. The main

focuses of this paper are on three aspects of distributed storage, cloud computing, and data fusion.

There are several techniques to address these issues. At last, we proposed an architectural solution and

key technology program which can be adapted to developing more application functionality and meet

future demand for Internet services and new business for big data processing requirements.

Key Words: Cloud Computing, Hadoop, Data Mining, Distributed Storage

1. Introduction

tion of data processing, so before to storage data, prepro-

cessing of data cleaning, classification, filtering and in-

In recent years, the volume of data in Internet has

dexing is necessary. Secondly, big data has the charac-

been increasing significantly with the rapid development

ters of diversity and heterogeneity, and diverse hetero-

of Internet. Such kind of data called big data not only has

geneous data includes not only structured data but also

mass and high-speed characteristics, but also the diver-

unstructured data. According to the statistics, 90% data

sity and variability. Big data have many sources includ-

through the internet is unstructured data. However, pro-

ing online activities (social networking, social media),

cessing unstructured data is much more complicated

telecommunications (mobile computing, call statistics),

than structured data. Thirdly, the amount of big data is

scientific activities (simulations, experiments, environ-

so huge that the computer memory has no more space

mental sensors), and the collation of traditional sources

and a single computer cannot process it efficiently and in

(forms, surveys). Large data contains the value, but there

addition to the requirements of real-time processing and

are still many difficulties and challenges in the use of big

the processing of complicated structured data, the com-

data technologies. The larger the scale of data, the more

plication of big data processing will be considerable.

difficult the processing and storage is. Firstly, data stor-

This paper studies the characteristics of the Internet

age is required to achieve the requirements of low cost,

in the future and the challenges and technical difficulties

high capacity, high reliability, great scalability and adap-

in the conditions of big data. Then we analyzed some ex-

isting technologies and methods to deal with big data and

*Corresponding author. E-mail: liuyun@bjtu.edu.cn

also presented what challenges we were going to face in

136

Yun Liu et al.

the future. In order to address these challenges, we pro-

vices are very few. So we are far behind foreign at both

posed an architectural solution and key technology pro-

techniques and services.

gram which can be adapted to developing more applica-

Multiple heterogeneous big data have the character-

tion functionality and meet future demand for Internet

istics of multiple dimensions, multiple sources, structured

services and new business for big data processing re-

+ semi-structured + unstructured heterogeneous data and

quirements.

the huge amount of data, thus the traditional relational

The remainder of this paper is organized as follows:

database has been very difficult to store such data. In ad-

section 2 gives an overview of major approaches for big

dition, the demand for the processing of such data is not

data processing. Section 3 describes our multiple hetero-

limited to the mode supported by SQL, but shows the

geneous data IT Infrastructure, including four parts. A

features of the connection and interweaving between the

detailed discussion of the proposed infrastructure is pre-

data on the Internet. There are a lot of storage methods

sented in section 4. Finally, section 5 provides some con-

for heterogeneous data [6], such as serial method, graph

cluding remarks.

method, tree method, file system method, database field

method and object method. Due to the difference be-

2. Related Work

tween the services of application layer, the characteris-

tics of multi-source heterogeneous data also are different

Google is the advocator and promoter of big data

with different services, so our data storage and manage-

processing, and its famous modules of Bigtable [1], GFS,

ment must use corresponding methods for differing ser-

MapReduce cause the many commercial systems and

vices, for example, Bigtable [1], the key value of NoSQL

open source tools birth. Bigtable is a distributed structured

and so on.

data storage system, which is designed to handle massive

Data fusion is a research hotspot and has several ad-

amounts of data: usually the PB level data which is dis-

vantages [7,8]. These advantages mainly involved en-

tributed in the thousands of ordinary servers. GFS [2] is a

hancements in data authenticity or availability. Examples

scalable distributed file system. It is a dedicated file sys-

include improved detection, confidence, reliability, as

tem designed to store massive amounts of search data.

well as reduction in data ambiguity while extending spa-

GFS is used for large-scale, distributed, large amounts of

tial and temporal coverage. There are a lot of concep-

data access applications and runs on inexpensive com-

tualizations of data fusion. Among them, JDL model [9]

modity hardware, but can provide fault tolerance, and

is the most common and popular. The JDL is originated

higher overall performance of the services provided to a

from the military domain and it considers the fusion pro-

large number of users. MapReduce [3] is an algorithm

cess in four increasing level of abstraction, namely ob-

model and associated implementation for processing and

ject, situation, impact, and process refinement. However,

generating big data sets. Yahoo’s outstanding contribution

it has many shortcomings, like the too much limitation

to big data processing is that they created the open source

which has been the subject of several extension resear-

framework Hadoop [4,5] based on the MapReduce model

ches [10,11] attempting to alleviate them. Dasarathy’s fra-

and promoted HBase, Hive, and other peripheral tech-

mework [12] views the fusion system, from a software

nologies, making Hadoop the base of most current big

engineering perspective, as a data flow characterized by

data processing products. In our country, Baidu, Tencent,

input/output as well as functionalities. One of the most

Alibaba and other Internet companies to provide ser-

recent fusion frameworks is proposed by Kokar et al.

vices for the network has been faced with large data pro-

[13].

cessing needs, but has also been developing and using

Web services is an online application service pub-

large data processing technologies. However, solutions

lished by service providers in order to solve some spe-

and frameworks purely providing data processing ser-

cific business requirements. The target of web service is

Research on IT Architecture of Heterogeneous Big Data

137

transferring software to subscription services through

mass storage, so we cannot storage and manage them

Internet and its essence is realizing the online data con-

like the traditional solutions treating them as equally im-

version by XML [14]. Fan et al. [15] proposed enterprise

portant. However, hierarchical storage is using different

application integration based on web service and Min-

storage methods corresponding to the characteristics and

Hsiung Hung et al. [16,17] presented a web services based

values of different data and utilizes the hierarchical stor-

e-diagnostics framework for semiconductor manufac-

age management software to achieve automatic sorting

turing industry. Therefore, web service is a significant

and automatic storage, which greatly increases the effec-

way to solve heterogeneous data exchange problem in

tiveness and speed of data storage and meets the storage

big data. However, the existing frameworks are mainly

needs of different kinds of data. In our system, there are

aimed at special businesses, which hardly to extensive

two kinds of data which are static data and dynamic data

apply in other industries, and there is lack of overall ar-

respectively. The framework of hierarchical storage is

chitecture including heterogeneous data storage and ex-

shown in Figure 1.

change module.

In order to achieve the storage of big data, especially

the heterogeneous data storage of the structured data,

3. Multiple Heterogeneous Data IT

semi-structured data and unstructured data, we super-

Infrastructure

impose a relational database and a distributed non-rela-

tional database on a distributed file system to store huge

Multiple heterogeneous big data have the character-

amounts of data which are managed by the master +

istics of multiple dimensions, multiple sources, structured

multi-slave physical structure. Master nodes and multi-

+ semi-structured + unstructured heterogeneous data and

node slave nodes are connected via Internet. Applica-

the huge amount of data. These features of big data bring

tions get access to data through the master host, and

us three aspects of challenges at the heterogeneous big

each storage node in the network is a separated database

data tiered storage and storage management, the inte-

without sharing with other storage nodes. Data is ex-

gration of heterogeneous data, the off-line and realtime

changed between master node and multi-node storage

computing architecture and the efficient transmission

nodes. Slave nodes are connected via the Internet and

mode of big data. This technical structure consists of un-

they complete the same task, so it is a server system from

derlying file system, structured data storage system, semi-

structured data and unstructured data storage system, the

distributed storage of data, the unified data access inter-

face, data indexing and positioning, processing task de-

composition and scheduling management, e distributed

execution of processing tasks, e service interface of the

system and the secondary development interface and sys-

tem manageability and security and completely defined a

data storage and processing platform meeting the speci-

fic needs. In the following paper, we presented the four

aspects of our big data framework respectively.

3.1 Multiple Heterogeneous Data Tiered Storage

and Distributed Storage

In this paper, we used the multiple heterogeneous

data tiered storage as our storage method. Big data needs

Figure 1. Hierarchical storage diagram.

138

Yun Liu et al.

the user’s prospective. In this structure, each node only

fusion, feature-level fusion and decision fusion. The pixel-

have right to get access to the local resources, including

based fusion also has a function similar to the data de-

memory, storage and etc. It is a completely non-sharing

noising and the cleaning process in addition to the sig-

structure with great scalability, and theoretically unli-

nificance of fusion. Feature-level fusion classifies data

mited period of expansion, the current technology can

based on the features and data attributes. Decision fu-

achieve 512 nodes interconnected, thousands of CPU.

sion uses data to implement the trend assessment and

Each node can run its own database, operating system,

the macro perspective service processing at the highest

but each node cannot access the memory of other nodes,

level. According to the specific services and the specific

and information exchange between nodes is implemented

characteristics of the data, the corresponding fusion al-

via the Internet node. This process is known as data real-

gorithms and computational structures are used to treat

location. In this basic data storage mode, according to

on different data. Utilizing data fusion technology, we

the specific business needs and the upper data features,

can remove redundant information to reduce the amount

the designed specific big data storage architecture is

of data increase the access efficiency. Moreover, we can

shown as Figure 2.

extract useful information from a large number of heter-

ogeneous data, providing services and support for the

3.2 Heterogeneous Data Fusion

business of the upper services. Figure 3 shows the Func-

Heterogeneous data fusion refers to using appropri-

tional model of heterogeneous data fusion.

ate processing methods to process the heterogeneous data

and producing complete accurate timely and effective in-

3.3 Off-line and Online Computation Framework

tegrated information. For data fusion, multi-sensor sys-

There are off-line and online computing demands in

tem is the hardware basic, multi-source heterogeneous

a large data environment. Off-line computation is more

information is its processing objects and optimism and

relaxed in the computing time requirement. However, un-

integrated processing is its core. Sensor is the source of

der the conditions of mass data, there are still computing

heterogeneous data, but it is not necessarily physical

time limit and the balance between recalculation and the

form. In other words, all the data sources and manual data

incremental calculation becomes an inevitable problem

source can be called sensor.

with the increasing of data. Online calculation has a higher

In our paper, we adopted fusion framework which is

requirement on the computing time, which is a quit hard

based on category theory and is claimed to be sufficiently

problem for the mass data and needs to a variety of me-

general to capture all kinds of fusion, including pixel-

thods to guarantee the calculation real-time.

based fusion, feature fusion and decision fusion. Fusion

of the data can be divided into three levels of pixel-based

Figure 2. Distributed file system structure.

Figure 3. Heterogeneous data fusion diagram.

Research on IT Architecture of Heterogeneous Big Data

139

In this paper, we classed the data into static data and

cessing. Storm is a common stream computing frame-

dynamic data. Static data are the historical read-only data,

work which has been widely used for real-time log pro-

but dynamic data are the read and write data including

cessing, real-time statistics, real-time risk control. Storm

intermediate result. For static data, when we implement

also is used to process the data preliminarily and it can

off-line computation, we must design a reasonable storage

store the data in a distributed database like HBase in or-

structure and create effective index to improve the pro-

der to facilitate the subsequent queries. Storm is a scal-

cessing efficiency according to the demands of specific

able, low-latency, reliable and fault- tolerant distributed

services. Hadoop [4] use the idea of MapReduce [3]. Ha-

computing platform. In addition, we will employ the di-

doop is a Java implementation of Google’s MapReduce.

stributed computing mode for data computation and pro-

It slices the data to deal with large amount of off-line

cessing and the assignment, scheming and management

data and then assign computing tasks to more than one

of the computing tasks. The off- line and online compu-

computer, which will greatly improve the speed and ef-

tation framework is shown in Figure 5.

ficiency of computation. Online computation needs to

use reasonable caching mechanism to solve the massive

3.4 Efficient Interactive Transmission Mode of Big

data processing problem. MapReduce is a simplified

Data

distributed programming model that allows the pro-

Storage and processing of big data necessarily invo-

gram automatically distributed to ordinary machine con-

lves the network-based distributed storage and computa-

sisting of a large cluster of concurrent and be executed.

tion, the collection of multi-source data, the remote data

A MapReduce job will usually slice the input data set

into separate blocks of data and deal with in a comp-

letely parallel way by the Map task. The framework will

first sort the output of Map and then distribute the out-

put to Reduce task. Usually the input and output opera-

tions will be stored in the file system. The framework

takes the task scheduling and monitoring, and re-run

the failed tasks. Hadoop MapReduce processing flow is

shown in Figure 4. For dynamic data, we will combine

data structure designing, index designing and caching

mechanism designing together to complete the data pro-

Figure 5. Off-line and online computation.

Figure 4. Hadoop MapReduce Processing flow.

140

Yun Liu et al.

management on the basis of network and data sharing

dom datasets and two real datasets including Netflix and

and exchange. So data exchange and interactive data

MovieLens. The results of our framework are proposed

service is an indispensable supporting technology.

in Tables 1 and 2. We tested our framework on three en-

In this paper, we designed the data exchange frame-

vironments which are single-core server, dual-core ser-

work according to two modes of data service and hete-

ver and quad-core server. The results of running time are

rogeneous data exchange. Data service is established da-

proposed in Table 1 with unit of one thousand seconds.

ta access function for large-scale predefined data trans-

The speed-up ratio and efficiency of our framework

ferring and migration and heterogeneous data exchange

are proposed in Table 2 which shows our framework can

refers to the data exchange between the different mo-

actually improve the performance and efficiency of data

dules within a framework and clients. Data service mode

fusion. Additionally, our framework is suitable for multi-

uses connection-oriented channel exchange. However,

core processing with good expandability. The experi-

Heterogeneous data uses connectionless datagram ex-

ment result shows the parallelization fusion method in

change and defines the structure of datagram by XML or

our framework has better performance than traditional

JSON.

clustering fusion. Therefore, our framework based on

The framework defines three types of service inter-

MapReduce is more suitable for large-scale data pro-

faces for the data interactive interface: the interfaces be-

cessing.

tween the modules within the system, which include two

modes of service and API; the external interfaces of the

system functions, which appear in the form of API and

are used for the developing and data access of the service

applications; interactive service interfaces, which have

two modes of service and REST API. System interfaces

can support each other to form the overall function. The

external interfaces in the form of API can be used for the

developing the service applications and REST API can

Figure 6. Data exchange module structure.

be used for extending the service applications of the

system and create a more convenient remote data access.

Table 1. Running time (ks) of four datasets

In this paper, large-scale data, which is predefined, is

transferred through the interface between the system mo-

dules; on the contrary, heterogeneous data can be ex-

changed through the heterogeneous data exchange mode

via the external interface. In addition, the datagram struc-

ture of heterogeneous data is defined by XML or JSON.

Data exchange modes are shown in Figure 6.

4. Discussion

We tested our framework in three aspects of speed-

up ratio, efficiency and expandability in Hadoop cluster

circumstance using Java programing language to realize

the data fusion algorithm based on MapReduce techno-

logy. In order to test our framework, we chose two ran-

Dataset

RandomData_1
RandomData_2
Netflix
MovieLens

Single-core

(traditional method)

Dual-
core

Quad-
core

8.64
6.52

10.860

9.44

4.57
3.38
5.77
4.97

2.25
1.68
2.80
2.44

Table 2. Speed-up ratio and efficiency of four datasets

Dataset

Speed-up ratio

Efficiency

Dual-
core

Quad-
core

Dual-
core

Quad-
core

RandomData_1
RandomData_2
Netflix
MovieLens

1.89
1.92
1.88
1.90

3.84
3.88
3.87
3.87

0.95
0.96
0.94
0.95

0.96
0.97
0.97
0.97

Journal of Applied Science and Engineering, Vol. 18, No. 2, pp. 135-142 (2015)

DOI: 10.6180/jase.2015.18.2.05

Research on IT Architecture of

Heterogeneous Big Data

Yun Liu*, Qi Wang and Hai-Qiang Chen

School of Communication and Information Engineering,

Key Laboratory of Communication and Information Systems,

Beijing Municipal Commission of Education, Beijing JiaoTong University,

Beijing 100044, P.R. China

Abstract

The amount of data has grown exponentially in industry and internet, and we are facing a

significant problem of information explosion. The challenge is not only to store and manage the vast

volume of data (“big data”), but also to analyze and extract meaningful value from it. This paper

analyzed the characters of the future network and the features of data generated by it. Furthermore, we

studied key issues and challenges of the multiple heterogeneous data IT infrastructure. The main

focuses of this paper are on three aspects of distributed storage, cloud computing, and data fusion.

There are several techniques to address these issues. At last, we proposed an architectural solution and

key technology program which can be adapted to developing more application functionality and meet

future demand for Internet services and new business for big data processing requirements.

Key Words: Cloud Computing, Hadoop, Data Mining, Distributed Storage

1. Introduction

tion of data processing, so before to storage data, prepro-

cessing of data cleaning, classification, filtering and in-

In recent years, the volume of data in Internet has

dexing is necessary. Secondly, big data has the charac-

been increasing significantly with the rapid development

ters of diversity and heterogeneity, and diverse hetero-

of Internet. Such kind of data called big data not only has

geneous data includes not only structured data but also

mass and high-speed characteristics, but also the diver-

unstructured data. According to the statistics, 90% data

sity and variability. Big data have many sources includ-

through the internet is unstructured data. However, pro-

ing online activities (social networking, social media),

cessing unstructured data is much more complicated

telecommunications (mobile computing, call statistics),

than structured data. Thirdly, the amount of big data is

scientific activities (simulations, experiments, environ-

so huge that the computer memory has no more space

mental sensors), and the collation of traditional sources

and a single computer cannot process it efficiently and in

(forms, surveys). Large data contains the value, but there

addition to the requirements of real-time processing and

are still many difficulties and challenges in the use of big

the processing of complicated structured data, the com-

data technologies. The larger the scale of data, the more

plication of big data processing will be considerable.

difficult the processing and storage is. Firstly, data stor-

This paper studies the characteristics of the Internet

age is required to achieve the requirements of low cost,

in the future and the challenges and technical difficulties

high capacity, high reliability, great scalability and adap-

in the conditions of big data. Then we analyzed some ex-

isting technologies and methods to deal with big data and

*Corresponding author. E-mail: liuyun@bjtu.edu.cn

also presented what challenges we were going to face in

136

Yun Liu et al.

the future. In order to address these challenges, we pro-

vices are very few. So we are far behind foreign at both

posed an architectural solution and key technology pro-

techniques and services.

gram which can be adapted to developing more applica-

Multiple heterogeneous big data have the character-

tion functionality and meet future demand for Internet

istics of multiple dimensions, multiple sources, structured

services and new business for big data processing re-

+ semi-structured + unstructured heterogeneous data and

quirements.

the huge amount of data, thus the traditional relational

The remainder of this paper is organized as follows:

database has been very difficult to store such data. In ad-

section 2 gives an overview of major approaches for big

dition, the demand for the processing of such data is not

data processing. Section 3 describes our multiple hetero-

limited to the mode supported by SQL, but shows the

geneous data IT Infrastructure, including four parts. A

features of the connection and interweaving between the

detailed discussion of the proposed infrastructure is pre-

data on the Internet. There are a lot of storage methods

sented in section 4. Finally, section 5 provides some con-

for heterogeneous data [6], such as serial method, graph

cluding remarks.

method, tree method, file system method, database field

method and object method. Due to the difference be-

2. Related Work

tween the services of application layer, the characteris-

tics of multi-source heterogeneous data also are different

Google is the advocator and promoter of big data

with different services, so our data storage and manage-

processing, and its famous modules of Bigtable [1], GFS,

ment must use corresponding methods for differing ser-

MapReduce cause the many commercial systems and

vices, for example, Bigtable [1], the key value of NoSQL

open source tools birth. Bigtable is a distributed structured

and so on.

data storage system, which is designed to handle massive

Data fusion is a research hotspot and has several ad-

amounts of data: usually the PB level data which is dis-

vantages [7,8]. These advantages mainly involved en-

tributed in the thousands of ordinary servers. GFS [2] is a

hancements in data authenticity or availability. Examples

scalable distributed file system. It is a dedicated file sys-

include improved detection, confidence, reliability, as

tem designed to store massive amounts of search data.

well as reduction in data ambiguity while extending spa-

GFS is used for large-scale, distributed, large amounts of

tial and temporal coverage. There are a lot of concep-

data access applications and runs on inexpensive com-

tualizations of data fusion. Among them, JDL model [9]

modity hardware, but can provide fault tolerance, and

is the most common and popular. The JDL is originated

higher overall performance of the services provided to a

from the military domain and it considers the fusion pro-

large number of users. MapReduce [3] is an algorithm

cess in four increasing level of abstraction, namely ob-

model and associated implementation for processing and

ject, situation, impact, and process refinement. However,

generating big data sets. Yahoo’s outstanding contribution

it has many shortcomings, like the too much limitation

to big data processing is that they created the open source

which has been the subject of several extension resear-

framework Hadoop [4,5] based on the MapReduce model

ches [10,11] attempting to alleviate them. Dasarathy’s fra-

and promoted HBase, Hive, and other peripheral tech-

mework [12] views the fusion system, from a software

nologies, making Hadoop the base of most current big

engineering perspective, as a data flow characterized by

data processing products. In our country, Baidu, Tencent,

input/output as well as functionalities. One of the most

Alibaba and other Internet companies to provide ser-

recent fusion frameworks is proposed by Kokar et al.

vices for the network has been faced with large data pro-

[13].

cessing needs, but has also been developing and using

Web services is an online application service pub-

large data processing technologies. However, solutions

lished by service providers in order to solve some spe-

and frameworks purely providing data processing ser-

cific business requirements. The target of web service is

Research on IT Architecture of Heterogeneous Big Data

137

transferring software to subscription services through

mass storage, so we cannot storage and manage them

Internet and its essence is realizing the online data con-

like the traditional solutions treating them as equally im-

version by XML [14]. Fan et al. [15] proposed enterprise

portant. However, hierarchical storage is using different

application integration based on web service and Min-

storage methods corresponding to the characteristics and

Hsiung Hung et al. [16,17] presented a web services based

values of different data and utilizes the hierarchical stor-

e-diagnostics framework for semiconductor manufac-

age management software to achieve automatic sorting

turing industry. Therefore, web service is a significant

and automatic storage, which greatly increases the effec-

way to solve heterogeneous data exchange problem in

tiveness and speed of data storage and meets the storage

big data. However, the existing frameworks are mainly

needs of different kinds of data. In our system, there are

aimed at special businesses, which hardly to extensive

two kinds of data which are static data and dynamic data

apply in other industries, and there is lack of overall ar-

respectively. The framework of hierarchical storage is

chitecture including heterogeneous data storage and ex-

shown in Figure 1.

change module.

In order to achieve the storage of big data, especially

the heterogeneous data storage of the structured data,

3. Multiple Heterogeneous Data IT

semi-structured data and unstructured data, we super-

Infrastructure

impose a relational database and a distributed non-rela-

tional database on a distributed file system to store huge

Multiple heterogeneous big data have the character-

amounts of data which are managed by the master +

istics of multiple dimensions, multiple sources, structured

multi-slave physical structure. Master nodes and multi-

+ semi-structured + unstructured heterogeneous data and

node slave nodes are connected via Internet. Applica-

the huge amount of data. These features of big data bring

tions get access to data through the master host, and

us three aspects of challenges at the heterogeneous big

each storage node in the network is a separated database

data tiered storage and storage management, the inte-

without sharing with other storage nodes. Data is ex-

gration of heterogeneous data, the off-line and realtime

changed between master node and multi-node storage

computing architecture and the efficient transmission

nodes. Slave nodes are connected via the Internet and

mode of big data. This technical structure consists of un-

they complete the same task, so it is a server system from

derlying file system, structured data storage system, semi-

structured data and unstructured data storage system, the

distributed storage of data, the unified data access inter-

face, data indexing and positioning, processing task de-

composition and scheduling management, e distributed

execution of processing tasks, e service interface of the

system and the secondary development interface and sys-

tem manageability and security and completely defined a

data storage and processing platform meeting the speci-

fic needs. In the following paper, we presented the four

aspects of our big data framework respectively.

3.1 Multiple Heterogeneous Data Tiered Storage

and Distributed Storage

In this paper, we used the multiple heterogeneous

data tiered storage as our storage method. Big data needs

Figure 1. Hierarchical storage diagram.

138

Yun Liu et al.

the user’s prospective. In this structure, each node only

fusion, feature-level fusion and decision fusion. The pixel-

have right to get access to the local resources, including

based fusion also has a function similar to the data de-

memory, storage and etc. It is a completely non-sharing

noising and the cleaning process in addition to the sig-

structure with great scalability, and theoretically unli-

nificance of fusion. Feature-level fusion classifies data

mited period of expansion, the current technology can

based on the features and data attributes. Decision fu-

achieve 512 nodes interconnected, thousands of CPU.

sion uses data to implement the trend assessment and

Each node can run its own database, operating system,

the macro perspective service processing at the highest

but each node cannot access the memory of other nodes,

level. According to the specific services and the specific

and information exchange between nodes is implemented

characteristics of the data, the corresponding fusion al-

via the Internet node. This process is known as data real-

gorithms and computational structures are used to treat

location. In this basic data storage mode, according to

on different data. Utilizing data fusion technology, we

the specific business needs and the upper data features,

can remove redundant information to reduce the amount

the designed specific big data storage architecture is

of data increase the access efficiency. Moreover, we can

shown as Figure 2.

extract useful information from a large number of heter-

ogeneous data, providing services and support for the

3.2 Heterogeneous Data Fusion

business of the upper services. Figure 3 shows the Func-

Heterogeneous data fusion refers to using appropri-

tional model of heterogeneous data fusion.

ate processing methods to process the heterogeneous data

and producing complete accurate timely and effective in-

3.3 Off-line and Online Computation Framework

tegrated information. For data fusion, multi-sensor sys-

There are off-line and online computing demands in

tem is the hardware basic, multi-source heterogeneous

a large data environment. Off-line computation is more

information is its processing objects and optimism and

relaxed in the computing time requirement. However, un-

integrated processing is its core. Sensor is the source of

der the conditions of mass data, there are still computing

heterogeneous data, but it is not necessarily physical

time limit and the balance between recalculation and the

form. In other words, all the data sources and manual data

incremental calculation becomes an inevitable problem

source can be called sensor.

with the increasing of data. Online calculation has a higher

In our paper, we adopted fusion framework which is

requirement on the computing time, which is a quit hard

based on category theory and is claimed to be sufficiently

problem for the mass data and needs to a variety of me-

general to capture all kinds of fusion, including pixel-

thods to guarantee the calculation real-time.

based fusion, feature fusion and decision fusion. Fusion

of the data can be divided into three levels of pixel-based

Figure 2. Distributed file system structure.

Figure 3. Heterogeneous data fusion diagram.

Research on IT Architecture of Heterogeneous Big Data

139

In this paper, we classed the data into static data and

cessing. Storm is a common stream computing frame-

dynamic data. Static data are the historical read-only data,

work which has been widely used for real-time log pro-

but dynamic data are the read and write data including

cessing, real-time statistics, real-time risk control. Storm

intermediate result. For static data, when we implement

also is used to process the data preliminarily and it can

off-line computation, we must design a reasonable storage

store the data in a distributed database like HBase in or-

structure and create effective index to improve the pro-

der to facilitate the subsequent queries. Storm is a scal-

cessing efficiency according to the demands of specific

able, low-latency, reliable and fault- tolerant distributed

services. Hadoop [4] use the idea of MapReduce [3]. Ha-

computing platform. In addition, we will employ the di-

doop is a Java implementation of Google’s MapReduce.

stributed computing mode for data computation and pro-

It slices the data to deal with large amount of off-line

cessing and the assignment, scheming and management

data and then assign computing tasks to more than one

of the computing tasks. The off- line and online compu-

computer, which will greatly improve the speed and ef-

tation framework is shown in Figure 5.

ficiency of computation. Online computation needs to

use reasonable caching mechanism to solve the massive

3.4 Efficient Interactive Transmission Mode of Big

data processing problem. MapReduce is a simplified

Data

distributed programming model that allows the pro-

Storage and processing of big data necessarily invo-

gram automatically distributed to ordinary machine con-

lves the network-based distributed storage and computa-

sisting of a large cluster of concurrent and be executed.

tion, the collection of multi-source data, the remote data

A MapReduce job will usually slice the input data set

into separate blocks of data and deal with in a comp-

letely parallel way by the Map task. The framework will

first sort the output of Map and then distribute the out-

put to Reduce task. Usually the input and output opera-

tions will be stored in the file system. The framework

takes the task scheduling and monitoring, and re-run

the failed tasks. Hadoop MapReduce processing flow is

shown in Figure 4. For dynamic data, we will combine

data structure designing, index designing and caching

mechanism designing together to complete the data pro-

Figure 5. Off-line and online computation.

Figure 4. Hadoop MapReduce Processing flow.

140

Yun Liu et al.

management on the basis of network and data sharing

dom datasets and two real datasets including Netflix and

and exchange. So data exchange and interactive data

MovieLens. The results of our framework are proposed

service is an indispensable supporting technology.

in Tables 1 and 2. We tested our framework on three en-

In this paper, we designed the data exchange frame-

vironments which are single-core server, dual-core ser-

work according to two modes of data service and hete-

ver and quad-core server. The results of running time are

rogeneous data exchange. Data service is established da-

proposed in Table 1 with unit of one thousand seconds.

ta access function for large-scale predefined data trans-

The speed-up ratio and efficiency of our framework

ferring and migration and heterogeneous data exchange

are proposed in Table 2 which shows our framework can

refers to the data exchange between the different mo-

actually improve the performance and efficiency of data

dules within a framework and clients. Data service mode

fusion. Additionally, our framework is suitable for multi-

uses connection-oriented channel exchange. However,

core processing with good expandability. The experi-

Heterogeneous data uses connectionless datagram ex-

ment result shows the parallelization fusion method in

change and defines the structure of datagram by XML or

our framework has better performance than traditional

JSON.

clustering fusion. Therefore, our framework based on

The framework defines three types of service inter-

MapReduce is more suitable for large-scale data pro-

faces for the data interactive interface: the interfaces be-

cessing.

tween the modules within the system, which include two

modes of service and API; the external interfaces of the

system functions, which appear in the form of API and

are used for the developing and data access of the service

applications; interactive service interfaces, which have

two modes of service and REST API. System interfaces

can support each other to form the overall function. The

external interfaces in the form of API can be used for the

developing the service applications and REST API can

Figure 6. Data exchange module structure.

be used for extending the service applications of the

system and create a more convenient remote data access.

Table 1. Running time (ks) of four datasets

In this paper, large-scale data, which is predefined, is

transferred through the interface between the system mo-

dules; on the contrary, heterogeneous data can be ex-

changed through the heterogeneous data exchange mode

via the external interface. In addition, the datagram struc-

ture of heterogeneous data is defined by XML or JSON.

Data exchange modes are shown in Figure 6.

4. Discussion

We tested our framework in three aspects of speed-

up ratio, efficiency and expandability in Hadoop cluster

circumstance using Java programing language to realize

the data fusion algorithm based on MapReduce techno-

logy. In order to test our framework, we chose two ran-

Dataset

RandomData_1
RandomData_2
Netflix
MovieLens

Single-core

(traditional method)

Dual-
core

Quad-
core

8.64
6.52

10.860

9.44

4.57
3.38
5.77
4.97

2.25
1.68
2.80
2.44

Table 2. Speed-up ratio and efficiency of four datasets

Dataset

Speed-up ratio

Efficiency

Dual-
core

Quad-
core

Dual-
core

Quad-
core

RandomData_1
RandomData_2
Netflix
MovieLens

1.89
1.92
1.88
1.90

3.84
3.88
3.87
3.87

0.95
0.96
0.94
0.95

0.96
0.97
0.97
0.97

Research on IT Architecture of Heterogeneous Big Data

141

5. Conclusions

[5] Information on http://research.yahoo.com/files/ycsb.

pdf.

In this paper, we described the framework of mul-

[6] Information on https://www.google.com/patents/US2

tiple heterogeneous big data from four aspects: (1) stor-

0030051097.

age management; (2) data fusion; (3) online and offline

[7] Hall, D. L. and Llinas, J., “An Introduction to Mul-

computation; (4) efficient interactive transmission mode

tisensor Fusion,” Proceedings of the IEEE, Vol. 85, Is-

of big data.

sue 1, pp. 6-23 (1997). doi: 10.1109/5.554205

Based on these technologies, we proposed a frame-

[8] Walts, E. L., Data Fusion for C3I: a Tutorial, in: Com-

work of multiple heterogeneous data which can be ada-

mand, Control, Communications Intelligence (C3I)

pted to developing more application functionality and

Handbook, EW Communications Inc., Palo Alto, CA,

meet future demand for Internet services and new busi-

pp. 217-226 (1986).

ness for big data processing requirements. Our frame-

[9] White, F. E., Data Fusion Lexicon, Joint Directors of

work can solve the problems of storage limitations and

Laboratories, Technical Panel for C3, Data Fusion

suit to variety businesses.

Sub-Panel, Naval Ocean Systems Center, San Diego

(1991).

Acknowledgements

[10] Steinberg, A. N., Bowman, C. L. and White, F. E., “Re-

visions to the JDL Data Fusion Model,” in: Proc. of the

This work has been supported by the National Natural

SPIE Conference on Sensor Fusion: Architectures, Al -

Science Foundation of China under Grant 61172072,

gorithms, and Applications III, pp. 430-441 (1999).

61271308, the Beijing Natural Science Foundation un-

doi: 10.1117/12.341367

der Grant 4112045, the Research Fund for the Doctoral

[11] Llinas, J., Bowman, C., Rogova, G., Steinberg, A.,

Program of Higher Education of China under Grant

Waltz, E. and White, F. E., “Revisiting the JDL Data

W11C100030, the Beijing Science and Technology Pro-

Fusion Model II,” in: Proc. of the International Con-

gram under Grant Z121100000312024.

ference on Information Fusion, pp. 1218-1230 (2004).

References

[12] Dasarathy, B. V., Decision Fusion, IEEE Computer

doi: 10.1109/ICIF.2005.1591959

Society Press, Los Alamitos CA (1994).

[1] Chang, F., Dean, J., Ghemawat, S., et al., “Bigtable: A

[13] Kokar, M. M., Tomasik, J. A. and Weyman, J., “For-

Distributed Storage System for Structured Data,”

malizing Classes of Information Fusion Systems,” In-

ACM Transactions on Computer Systems, Vol. 26,

formation Fusion, Vol. 5, No. 3, pp. 189-202 (2004).

Issue 2, Article No. 4 (2008). doi: 10.1145/1365815.

doi: 10.1016/j.inffus.2003.11.001

1365816

[14] Champion, M., Ferris, C., Newcomer, E., et al., Web

[2] Ghemawat, S., Gobioff, H. and Leung, S. T., “The

Services Architecture, W3C Working Draft, (2002-11-

Google File System,” ACM SIGOPS Operating Sys-

14). http://www.w3.org/TR/ws-arch.html.

tems Review. ACM, Vol. 37, Issue 5, pp. 29-43 (2003).

[15] Huang, S. G., Fan, Y. S., Zhao, D., et al., “Web Service

doi: 10.1145/1165389.945450

Based Enterprise Application Integration,” Computer

[3] Dean, J. and Ghemawat, S., “MapReduce: Simplified

Integrated Manufacturing Systems, Vol. 9, No. 10,

Data Processing on Large Clusters,” Communications

pp. 864-867 (2003).

of the ACM, Vol. 51, Issue 1, pp. 107-113 (2008). doi:

[16] Hung, M.-H., Cheng, F.-T. and Yeh, S.-C., “Develop-

10.1145/1629175.1629198

ment of a Web-Services-based E-diagnostics Frame-

[4] White, T., Hadoop: The Definitive Guide. 1st, O’Reilly

work for Semiconductor Manufacturing Industry,”

Media (2009).

IEEE Transactions on Semiconductor Manufacturing,

Journal of Applied Science and Engineering, Vol. 18, No. 2, pp. 135-142 (2015)

DOI: 10.6180/jase.2015.18.2.05

Research on IT Architecture of

Heterogeneous Big Data

Yun Liu*, Qi Wang and Hai-Qiang Chen

School of Communication and Information Engineering,

Key Laboratory of Communication and Information Systems,

Beijing Municipal Commission of Education, Beijing JiaoTong University,

Beijing 100044, P.R. China

Abstract

The amount of data has grown exponentially in industry and internet, and we are facing a

significant problem of information explosion. The challenge is not only to store and manage the vast

volume of data (“big data”), but also to analyze and extract meaningful value from it. This paper

analyzed the characters of the future network and the features of data generated by it. Furthermore, we

studied key issues and challenges of the multiple heterogeneous data IT infrastructure. The main

focuses of this paper are on three aspects of distributed storage, cloud computing, and data fusion.

There are several techniques to address these issues. At last, we proposed an architectural solution and

key technology program which can be adapted to developing more application functionality and meet

future demand for Internet services and new business for big data processing requirements.

Key Words: Cloud Computing, Hadoop, Data Mining, Distributed Storage

1. Introduction

tion of data processing, so before to storage data, prepro-

cessing of data cleaning, classification, filtering and in-

In recent years, the volume of data in Internet has

dexing is necessary. Secondly, big data has the charac-

been increasing significantly with the rapid development

ters of diversity and heterogeneity, and diverse hetero-

of Internet. Such kind of data called big data not only has

geneous data includes not only structured data but also

mass and high-speed characteristics, but also the diver-

unstructured data. According to the statistics, 90% data

sity and variability. Big data have many sources includ-

through the internet is unstructured data. However, pro-

ing online activities (social networking, social media),

cessing unstructured data is much more complicated

telecommunications (mobile computing, call statistics),

than structured data. Thirdly, the amount of big data is

scientific activities (simulations, experiments, environ-

so huge that the computer memory has no more space

mental sensors), and the collation of traditional sources

and a single computer cannot process it efficiently and in

(forms, surveys). Large data contains the value, but there

addition to the requirements of real-time processing and

are still many difficulties and challenges in the use of big

the processing of complicated structured data, the com-

data technologies. The larger the scale of data, the more

plication of big data processing will be considerable.

difficult the processing and storage is. Firstly, data stor-

This paper studies the characteristics of the Internet

age is required to achieve the requirements of low cost,

in the future and the challenges and technical difficulties

high capacity, high reliability, great scalability and adap-

in the conditions of big data. Then we analyzed some ex-

isting technologies and methods to deal with big data and

*Corresponding author. E-mail: liuyun@bjtu.edu.cn

also presented what challenges we were going to face in

136

Yun Liu et al.

the future. In order to address these challenges, we pro-

vices are very few. So we are far behind foreign at both

posed an architectural solution and key technology pro-

techniques and services.

gram which can be adapted to developing more applica-

Multiple heterogeneous big data have the character-

tion functionality and meet future demand for Internet

istics of multiple dimensions, multiple sources, structured

services and new business for big data processing re-

+ semi-structured + unstructured heterogeneous data and

quirements.

the huge amount of data, thus the traditional relational

The remainder of this paper is organized as follows:

database has been very difficult to store such data. In ad-

section 2 gives an overview of major approaches for big

dition, the demand for the processing of such data is not

data processing. Section 3 describes our multiple hetero-

limited to the mode supported by SQL, but shows the

geneous data IT Infrastructure, including four parts. A

features of the connection and interweaving between the

detailed discussion of the proposed infrastructure is pre-

data on the Internet. There are a lot of storage methods

sented in section 4. Finally, section 5 provides some con-

for heterogeneous data [6], such as serial method, graph

cluding remarks.

method, tree method, file system method, database field

method and object method. Due to the difference be-

2. Related Work

tween the services of application layer, the characteris-

tics of multi-source heterogeneous data also are different

Google is the advocator and promoter of big data

with different services, so our data storage and manage-

processing, and its famous modules of Bigtable [1], GFS,

ment must use corresponding methods for differing ser-

MapReduce cause the many commercial systems and

vices, for example, Bigtable [1], the key value of NoSQL

open source tools birth. Bigtable is a distributed structured

and so on.

data storage system, which is designed to handle massive

Data fusion is a research hotspot and has several ad-

amounts of data: usually the PB level data which is dis-

vantages [7,8]. These advantages mainly involved en-

tributed in the thousands of ordinary servers. GFS [2] is a

hancements in data authenticity or availability. Examples

scalable distributed file system. It is a dedicated file sys-

include improved detection, confidence, reliability, as

tem designed to store massive amounts of search data.

well as reduction in data ambiguity while extending spa-

GFS is used for large-scale, distributed, large amounts of

tial and temporal coverage. There are a lot of concep-

data access applications and runs on inexpensive com-

tualizations of data fusion. Among them, JDL model [9]

modity hardware, but can provide fault tolerance, and

is the most common and popular. The JDL is originated

higher overall performance of the services provided to a

from the military domain and it considers the fusion pro-

large number of users. MapReduce [3] is an algorithm

cess in four increasing level of abstraction, namely ob-

model and associated implementation for processing and

ject, situation, impact, and process refinement. However,

generating big data sets. Yahoo’s outstanding contribution

it has many shortcomings, like the too much limitation

to big data processing is that they created the open source

which has been the subject of several extension resear-

framework Hadoop [4,5] based on the MapReduce model

ches [10,11] attempting to alleviate them. Dasarathy’s fra-

and promoted HBase, Hive, and other peripheral tech-

mework [12] views the fusion system, from a software

nologies, making Hadoop the base of most current big

engineering perspective, as a data flow characterized by

data processing products. In our country, Baidu, Tencent,

input/output as well as functionalities. One of the most

Alibaba and other Internet companies to provide ser-

recent fusion frameworks is proposed by Kokar et al.

vices for the network has been faced with large data pro-

[13].

cessing needs, but has also been developing and using

Web services is an online application service pub-

large data processing technologies. However, solutions

lished by service providers in order to solve some spe-

and frameworks purely providing data processing ser-

cific business requirements. The target of web service is

Research on IT Architecture of Heterogeneous Big Data

137

transferring software to subscription services through

mass storage, so we cannot storage and manage them

Internet and its essence is realizing the online data con-

like the traditional solutions treating them as equally im-

version by XML [14]. Fan et al. [15] proposed enterprise

portant. However, hierarchical storage is using different

application integration based on web service and Min-

storage methods corresponding to the characteristics and

Hsiung Hung et al. [16,17] presented a web services based

values of different data and utilizes the hierarchical stor-

e-diagnostics framework for semiconductor manufac-

age management software to achieve automatic sorting

turing industry. Therefore, web service is a significant

and automatic storage, which greatly increases the effec-

way to solve heterogeneous data exchange problem in

tiveness and speed of data storage and meets the storage

big data. However, the existing frameworks are mainly

needs of different kinds of data. In our system, there are

aimed at special businesses, which hardly to extensive

two kinds of data which are static data and dynamic data

apply in other industries, and there is lack of overall ar-

respectively. The framework of hierarchical storage is

chitecture including heterogeneous data storage and ex-

shown in Figure 1.

change module.

In order to achieve the storage of big data, especially

the heterogeneous data storage of the structured data,

3. Multiple Heterogeneous Data IT

semi-structured data and unstructured data, we super-

Infrastructure

impose a relational database and a distributed non-rela-

tional database on a distributed file system to store huge

Multiple heterogeneous big data have the character-

amounts of data which are managed by the master +

istics of multiple dimensions, multiple sources, structured

multi-slave physical structure. Master nodes and multi-

+ semi-structured + unstructured heterogeneous data and

node slave nodes are connected via Internet. Applica-

the huge amount of data. These features of big data bring

tions get access to data through the master host, and

us three aspects of challenges at the heterogeneous big

each storage node in the network is a separated database

data tiered storage and storage management, the inte-

without sharing with other storage nodes. Data is ex-

gration of heterogeneous data, the off-line and realtime

changed between master node and multi-node storage

computing architecture and the efficient transmission

nodes. Slave nodes are connected via the Internet and

mode of big data. This technical structure consists of un-

they complete the same task, so it is a server system from

derlying file system, structured data storage system, semi-

structured data and unstructured data storage system, the

distributed storage of data, the unified data access inter-

face, data indexing and positioning, processing task de-

composition and scheduling management, e distributed

execution of processing tasks, e service interface of the

system and the secondary development interface and sys-

tem manageability and security and completely defined a

data storage and processing platform meeting the speci-

fic needs. In the following paper, we presented the four

aspects of our big data framework respectively.

3.1 Multiple Heterogeneous Data Tiered Storage

and Distributed Storage

In this paper, we used the multiple heterogeneous

data tiered storage as our storage method. Big data needs

Figure 1. Hierarchical storage diagram.

138

Yun Liu et al.

the user’s prospective. In this structure, each node only

fusion, feature-level fusion and decision fusion. The pixel-

have right to get access to the local resources, including

based fusion also has a function similar to the data de-

memory, storage and etc. It is a completely non-sharing

noising and the cleaning process in addition to the sig-

structure with great scalability, and theoretically unli-

nificance of fusion. Feature-level fusion classifies data

mited period of expansion, the current technology can

based on the features and data attributes. Decision fu-

achieve 512 nodes interconnected, thousands of CPU.

sion uses data to implement the trend assessment and

Each node can run its own database, operating system,

the macro perspective service processing at the highest

but each node cannot access the memory of other nodes,

level. According to the specific services and the specific

and information exchange between nodes is implemented

characteristics of the data, the corresponding fusion al-

via the Internet node. This process is known as data real-

gorithms and computational structures are used to treat

location. In this basic data storage mode, according to

on different data. Utilizing data fusion technology, we

the specific business needs and the upper data features,

can remove redundant information to reduce the amount

the designed specific big data storage architecture is

of data increase the access efficiency. Moreover, we can

shown as Figure 2.

extract useful information from a large number of heter-

ogeneous data, providing services and support for the

3.2 Heterogeneous Data Fusion

business of the upper services. Figure 3 shows the Func-

Heterogeneous data fusion refers to using appropri-

tional model of heterogeneous data fusion.

ate processing methods to process the heterogeneous data

and producing complete accurate timely and effective in-

3.3 Off-line and Online Computation Framework

tegrated information. For data fusion, multi-sensor sys-

There are off-line and online computing demands in

tem is the hardware basic, multi-source heterogeneous

a large data environment. Off-line computation is more

information is its processing objects and optimism and

relaxed in the computing time requirement. However, un-

integrated processing is its core. Sensor is the source of

der the conditions of mass data, there are still computing

heterogeneous data, but it is not necessarily physical

time limit and the balance between recalculation and the

form. In other words, all the data sources and manual data

incremental calculation becomes an inevitable problem

source can be called sensor.

with the increasing of data. Online calculation has a higher

In our paper, we adopted fusion framework which is

requirement on the computing time, which is a quit hard

based on category theory and is claimed to be sufficiently

problem for the mass data and needs to a variety of me-

general to capture all kinds of fusion, including pixel-

thods to guarantee the calculation real-time.

based fusion, feature fusion and decision fusion. Fusion

of the data can be divided into three levels of pixel-based

Figure 2. Distributed file system structure.

Figure 3. Heterogeneous data fusion diagram.

Research on IT Architecture of Heterogeneous Big Data

139

In this paper, we classed the data into static data and

cessing. Storm is a common stream computing frame-

dynamic data. Static data are the historical read-only data,

work which has been widely used for real-time log pro-

but dynamic data are the read and write data including

cessing, real-time statistics, real-time risk control. Storm

intermediate result. For static data, when we implement

also is used to process the data preliminarily and it can

off-line computation, we must design a reasonable storage

store the data in a distributed database like HBase in or-

structure and create effective index to improve the pro-

der to facilitate the subsequent queries. Storm is a scal-

cessing efficiency according to the demands of specific

able, low-latency, reliable and fault- tolerant distributed

services. Hadoop [4] use the idea of MapReduce [3]. Ha-

computing platform. In addition, we will employ the di-

doop is a Java implementation of Google’s MapReduce.

stributed computing mode for data computation and pro-

It slices the data to deal with large amount of off-line

cessing and the assignment, scheming and management

data and then assign computing tasks to more than one

of the computing tasks. The off- line and online compu-

computer, which will greatly improve the speed and ef-

tation framework is shown in Figure 5.

ficiency of computation. Online computation needs to

use reasonable caching mechanism to solve the massive

3.4 Efficient Interactive Transmission Mode of Big

data processing problem. MapReduce is a simplified

Data

distributed programming model that allows the pro-

Storage and processing of big data necessarily invo-

gram automatically distributed to ordinary machine con-

lves the network-based distributed storage and computa-

sisting of a large cluster of concurrent and be executed.

tion, the collection of multi-source data, the remote data

A MapReduce job will usually slice the input data set

into separate blocks of data and deal with in a comp-

letely parallel way by the Map task. The framework will

first sort the output of Map and then distribute the out-

put to Reduce task. Usually the input and output opera-

tions will be stored in the file system. The framework

takes the task scheduling and monitoring, and re-run

the failed tasks. Hadoop MapReduce processing flow is

shown in Figure 4. For dynamic data, we will combine

data structure designing, index designing and caching

mechanism designing together to complete the data pro-

Figure 5. Off-line and online computation.

Figure 4. Hadoop MapReduce Processing flow.

140

Yun Liu et al.

management on the basis of network and data sharing

dom datasets and two real datasets including Netflix and

and exchange. So data exchange and interactive data

MovieLens. The results of our framework are proposed

service is an indispensable supporting technology.

in Tables 1 and 2. We tested our framework on three en-

In this paper, we designed the data exchange frame-

vironments which are single-core server, dual-core ser-

work according to two modes of data service and hete-

ver and quad-core server. The results of running time are

rogeneous data exchange. Data service is established da-

proposed in Table 1 with unit of one thousand seconds.

ta access function for large-scale predefined data trans-

The speed-up ratio and efficiency of our framework

ferring and migration and heterogeneous data exchange

are proposed in Table 2 which shows our framework can

refers to the data exchange between the different mo-

actually improve the performance and efficiency of data

dules within a framework and clients. Data service mode

fusion. Additionally, our framework is suitable for multi-

uses connection-oriented channel exchange. However,

core processing with good expandability. The experi-

Heterogeneous data uses connectionless datagram ex-

ment result shows the parallelization fusion method in

change and defines the structure of datagram by XML or

our framework has better performance than traditional

JSON.

clustering fusion. Therefore, our framework based on

The framework defines three types of service inter-

MapReduce is more suitable for large-scale data pro-

faces for the data interactive interface: the interfaces be-

cessing.

tween the modules within the system, which include two

modes of service and API; the external interfaces of the

system functions, which appear in the form of API and

are used for the developing and data access of the service

applications; interactive service interfaces, which have

two modes of service and REST API. System interfaces

can support each other to form the overall function. The

external interfaces in the form of API can be used for the

developing the service applications and REST API can

Figure 6. Data exchange module structure.

be used for extending the service applications of the

system and create a more convenient remote data access.

Table 1. Running time (ks) of four datasets

In this paper, large-scale data, which is predefined, is

transferred through the interface between the system mo-

dules; on the contrary, heterogeneous data can be ex-

changed through the heterogeneous data exchange mode

via the external interface. In addition, the datagram struc-

ture of heterogeneous data is defined by XML or JSON.

Data exchange modes are shown in Figure 6.

4. Discussion

We tested our framework in three aspects of speed-

up ratio, efficiency and expandability in Hadoop cluster

circumstance using Java programing language to realize

the data fusion algorithm based on MapReduce techno-

logy. In order to test our framework, we chose two ran-

Dataset

RandomData_1
RandomData_2
Netflix
MovieLens

Single-core

(traditional method)

Dual-
core

Quad-
core

8.64
6.52

10.860

9.44

4.57
3.38
5.77
4.97

2.25
1.68
2.80
2.44

Table 2. Speed-up ratio and efficiency of four datasets

Dataset

Speed-up ratio

Efficiency

Dual-
core

Quad-
core

Dual-
core

Quad-
core

RandomData_1
RandomData_2
Netflix
MovieLens

1.89
1.92
1.88
1.90

3.84
3.88
3.87
3.87

0.95
0.96
0.94
0.95

0.96
0.97
0.97
0.97

Research on IT Architecture of Heterogeneous Big Data

141

5. Conclusions

[5] Information on http://research.yahoo.com/files/ycsb.

pdf.

In this paper, we described the framework of mul-

[6] Information on https://www.google.com/patents/US2

tiple heterogeneous big data from four aspects: (1) stor-

0030051097.

age management; (2) data fusion; (3) online and offline

[7] Hall, D. L. and Llinas, J., “An Introduction to Mul-

computation; (4) efficient interactive transmission mode

tisensor Fusion,” Proceedings of the IEEE, Vol. 85, Is-

of big data.

sue 1, pp. 6-23 (1997). doi: 10.1109/5.554205

Based on these technologies, we proposed a frame-

[8] Walts, E. L., Data Fusion for C3I: a Tutorial, in: Com-

work of multiple heterogeneous data which can be ada-

mand, Control, Communications Intelligence (C3I)

pted to developing more application functionality and

Handbook, EW Communications Inc., Palo Alto, CA,

meet future demand for Internet services and new busi-

pp. 217-226 (1986).

ness for big data processing requirements. Our frame-

[9] White, F. E., Data Fusion Lexicon, Joint Directors of

work can solve the problems of storage limitations and

Laboratories, Technical Panel for C3, Data Fusion

suit to variety businesses.

Sub-Panel, Naval Ocean Systems Center, San Diego

(1991).

Acknowledgements

[10] Steinberg, A. N., Bowman, C. L. and White, F. E., “Re-

visions to the JDL Data Fusion Model,” in: Proc. of the

This work has been supported by the National Natural

SPIE Conference on Sensor Fusion: Architectures, Al -

Science Foundation of China under Grant 61172072,

gorithms, and Applications III, pp. 430-441 (1999).

61271308, the Beijing Natural Science Foundation un-

doi: 10.1117/12.341367

der Grant 4112045, the Research Fund for the Doctoral

[11] Llinas, J., Bowman, C., Rogova, G., Steinberg, A.,

Program of Higher Education of China under Grant

Waltz, E. and White, F. E., “Revisiting the JDL Data

W11C100030, the Beijing Science and Technology Pro-

Fusion Model II,” in: Proc. of the International Con-

gram under Grant Z121100000312024.

ference on Information Fusion, pp. 1218-1230 (2004).

References

[12] Dasarathy, B. V., Decision Fusion, IEEE Computer

doi: 10.1109/ICIF.2005.1591959

Society Press, Los Alamitos CA (1994).

[1] Chang, F., Dean, J., Ghemawat, S., et al., “Bigtable: A

[13] Kokar, M. M., Tomasik, J. A. and Weyman, J., “For-

Distributed Storage System for Structured Data,”

malizing Classes of Information Fusion Systems,” In-

ACM Transactions on Computer Systems, Vol. 26,

formation Fusion, Vol. 5, No. 3, pp. 189-202 (2004).

Issue 2, Article No. 4 (2008). doi: 10.1145/1365815.

doi: 10.1016/j.inffus.2003.11.001

1365816

[14] Champion, M., Ferris, C., Newcomer, E., et al., Web

[2] Ghemawat, S., Gobioff, H. and Leung, S. T., “The

Services Architecture, W3C Working Draft, (2002-11-

Google File System,” ACM SIGOPS Operating Sys-

14). http://www.w3.org/TR/ws-arch.html.

tems Review. ACM, Vol. 37, Issue 5, pp. 29-43 (2003).

[15] Huang, S. G., Fan, Y. S., Zhao, D., et al., “Web Service

doi: 10.1145/1165389.945450

Based Enterprise Application Integration,” Computer

[3] Dean, J. and Ghemawat, S., “MapReduce: Simplified

Integrated Manufacturing Systems, Vol. 9, No. 10,

Data Processing on Large Clusters,” Communications

pp. 864-867 (2003).

of the ACM, Vol. 51, Issue 1, pp. 107-113 (2008). doi:

[16] Hung, M.-H., Cheng, F.-T. and Yeh, S.-C., “Develop-

10.1145/1629175.1629198

ment of a Web-Services-based E-diagnostics Frame-

[4] White, T., Hadoop: The Definitive Guide. 1st, O’Reilly

work for Semiconductor Manufacturing Industry,”

Media (2009).

IEEE Transactions on Semiconductor Manufacturing,

142

Yun Liu et al.

Vol. 18, No. 1, pp. 122-135 (2005). doi: 10.1109/TSM.

Vol. 3190, pp. 133-140 (2004). doi: 10.1007/978-3-

2004.836664

540-30103-5_15

[17] Otal, M. and Jelinekl, I., “The Method of Unified In-

ternet-based Communication for Manufacturing Com-

panies,” Lecture Notes in Computer Science, Springer,

Manuscript Received: May 14, 2014

Accepted: May 20, 2015

