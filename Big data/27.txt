                                                                                                               

 

          ISSN (Print):   2347-6710                                                                                                                                  

                

                      
                   
                  ISSN(Online): 2319-8753 
    

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

The Evolution of Big Data Security through 

Hadoop Incremental Security Model 

Vinod Sharma1, Prof. N.K. Joshi2 

Research Scholar, Department of Computer Science, Career Point University, Kota, Rajasthan, India1 

Director, Department of Computer Science, Modi Institute of Technology, Kota, Rajasthan, India2 

 

ABSTRACT:Data pours in millions of computers and millions of process every moment of every day so today is the 
era  of  Big  Data  where  data  interrelate  to  the  volume,  velocity,  and  variety  of  data  interrelate.  Huge  volume,  various 
varieties  and  high  velocity  create  lots  of  other  challenges  and  issues  regarding  its  management  and  processing.  Big 
Data enable any organization to collect, manage, analyze and making decision incredibly from large data sets. Big data 
is growing at an exponential rate but  security feature not growing at an same rate.so it becomes important to develop 
new technologies to deal with it securities. So require latest technology and moderate theory about data, other than the 
traditional tools and technique to manage it due its nature. 
This paper introduces the big data technology along with its importance in the modern world and existing projects like 
hadoop  which are effective and important in changing  the  concept of science into big science. Hadoop, Map Reduce 
and No SQL are the major big data technology. This paper also throws some light on other challenges and issues. The 
various  challenges  and  issues  in  adapting  and  accepting  Big  data  security  and  suggest  some  more  security  standards 
and concept that make robust hadoop ecosystem without any processing overhead. 
 
KEYWORDS:BigData, Hadoop, MapReduce, ABAC, RBAC. 

I.  INTRODUCTION 

 
Data pours in millions of computers and millions of process every moment of every day so today is the era of Big Data. 
Big  data  refers  to  technologies  that  involve  data  that  is  too  divers,  fast  changing  or  massive  for  conventional 
technologies,  skill  and  infrastructure  to  address  efficiently.  Said  differently  the  volume,  velocity,  and  variety  of  data 
interrelation is too great. Big Data enable any organization to data creation, collection, retrieval, manage, analyze and 
making decision that is remarkable in terms of volume, velocity, and variety. 
 

In Big Data 3 V’s are [2]. 
1. 

Volume: At present the data existing is in petabytes and is supposed to increase to zettabytes in nearby future. 
The social media, financial institution, medical institution, government, Sensors, Logs producing data in order 
of terabytes every day and this amount of data is definitely difficult to be handled using the existing traditional 
systems.. 
Velocity: At present data change rapidly through the archived data, legacy collections and from streamed data 
that  comes  from  multiple  resources  sensors,  traditional  file  records,  cellular  technology,  social  media  and 
many more. 

2. 

3.  Variety: At present data comes in different forms including data-streams, text, picture, audio, video, structured, 
semi  structured,  unstructured.  Unstructured  data  is  difficult  to  handle  with  traditional  tools  and  techniques. 
Thus  our  traditional  systems  are  not  capable  enough  on  performing  the  analytics  on  the  data  which  is 
constantly in motion. 
There  are  volume,  velocity  and  variety  are  main  concern  in  big  data  technology.  Some  other  issues  are  also 
considerable such as veracity, variability, complexity, Value. 

4. 

 

The efflux of Big Data and the need to move this information throughout an organization has created a massive new 
target  for  hackers  and  other  cybercriminal  activity.  Now  this  data  is  highly  valuable,  is  subject  to  privacy  laws  and 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3489 

                                                                                                               

 

          ISSN (Print):   2347-6710                                                                                                                                  

                

                      
                   
                  ISSN(Online): 2319-8753 
    

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

The Evolution of Big Data Security through 

Hadoop Incremental Security Model 

Vinod Sharma1, Prof. N.K. Joshi2 

Research Scholar, Department of Computer Science, Career Point University, Kota, Rajasthan, India1 

Director, Department of Computer Science, Modi Institute of Technology, Kota, Rajasthan, India2 

 

ABSTRACT:Data pours in millions of computers and millions of process every moment of every day so today is the 
era  of  Big  Data  where  data  interrelate  to  the  volume,  velocity,  and  variety  of  data  interrelate.  Huge  volume,  various 
varieties  and  high  velocity  create  lots  of  other  challenges  and  issues  regarding  its  management  and  processing.  Big 
Data enable any organization to collect, manage, analyze and making decision incredibly from large data sets. Big data 
is growing at an exponential rate but  security feature not growing at an same rate.so it becomes important to develop 
new technologies to deal with it securities. So require latest technology and moderate theory about data, other than the 
traditional tools and technique to manage it due its nature. 
This paper introduces the big data technology along with its importance in the modern world and existing projects like 
hadoop  which are effective and important in changing  the  concept of science into big science. Hadoop, Map Reduce 
and No SQL are the major big data technology. This paper also throws some light on other challenges and issues. The 
various  challenges  and  issues  in  adapting  and  accepting  Big  data  security  and  suggest  some  more  security  standards 
and concept that make robust hadoop ecosystem without any processing overhead. 
 
KEYWORDS:BigData, Hadoop, MapReduce, ABAC, RBAC. 

I.  INTRODUCTION 

 
Data pours in millions of computers and millions of process every moment of every day so today is the era of Big Data. 
Big  data  refers  to  technologies  that  involve  data  that  is  too  divers,  fast  changing  or  massive  for  conventional 
technologies,  skill  and  infrastructure  to  address  efficiently.  Said  differently  the  volume,  velocity,  and  variety  of  data 
interrelation is too great. Big Data enable any organization to data creation, collection, retrieval, manage, analyze and 
making decision that is remarkable in terms of volume, velocity, and variety. 
 

In Big Data 3 V’s are [2]. 
1. 

Volume: At present the data existing is in petabytes and is supposed to increase to zettabytes in nearby future. 
The social media, financial institution, medical institution, government, Sensors, Logs producing data in order 
of terabytes every day and this amount of data is definitely difficult to be handled using the existing traditional 
systems.. 
Velocity: At present data change rapidly through the archived data, legacy collections and from streamed data 
that  comes  from  multiple  resources  sensors,  traditional  file  records,  cellular  technology,  social  media  and 
many more. 

2. 

3.  Variety: At present data comes in different forms including data-streams, text, picture, audio, video, structured, 
semi  structured,  unstructured.  Unstructured  data  is  difficult  to  handle  with  traditional  tools  and  techniques. 
Thus  our  traditional  systems  are  not  capable  enough  on  performing  the  analytics  on  the  data  which  is 
constantly in motion. 
There  are  volume,  velocity  and  variety  are  main  concern  in  big  data  technology.  Some  other  issues  are  also 
considerable such as veracity, variability, complexity, Value. 

4. 

 

The efflux of Big Data and the need to move this information throughout an organization has created a massive new 
target  for  hackers  and  other  cybercriminal  activity.  Now  this  data  is  highly  valuable,  is  subject  to  privacy  laws  and 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3489 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

compliance  regulation,  and  must  be  protected.  Today  the  biggest  concerns  in  our  present  age  resolves  around  the 
security,  privacy  with  audit  access  control,  robustness,  reliability,  availability  and  protection  of  sensitive  information 
such as financial data, sensors information, medical records, and social information on the social networking. 
Big Data’s security  in this process is becoming increasingly  more important and same time organizations required to 
enforce  access  control  and  privacy  restrictions  on  these  data  sets  to  meet  regulatory  requirements  such  information 
privacy  laws.  Most  of  Network  security  breaches  from  internal  and  external  attackers  are  on  the  rise,  often  taking 
months to be detected, and those affected are paying the price. Organizations that have not properly controlled access to 
their data sets are facing lawsuits, negative publicity, and regulatory fines.  
 
Hadoop  is  the  core  platform  for  structuring  big  data,  and  solves  the  problem  of  making  it  useful  for  analytical  and 
operational purposes. Hadoop is an apache based open source software framework, comprised at its core of the hadoop 
file system and map reduce, and is very well designed to handle huge volumes of data across a large number of nodes. 
 
At a high level, hadoop leverages parallel processing across many commodity servers to respond to client applications. 
The  key  difference  is,  rather  than  only  looking  at  parallel  computing,  it  looks  at  parallelizing  the  data  access.  Map 
Reduce programming model provide divide and conquer based highly parallelizable and distributed algorithms across 
massive  data  sets  using  a  large  number  of  commodity  machines.  The  basic  idea  is  to  partition  a  large  problem  into 
smaller  independent  sub  problems  tackle  by  different  workers.  Fine  grained  Map  and  Reduce  task  provide  enhanced 
load  balancing  and  faster  recovery  from  failed  tasks.[6]  Hadoop  divides  the  input  to  a MapReduce  job  into  fixed-size 
pieces  called  input  splits,  or  just  splits  and  creates  one  map  task  for  each  split,  which  runs  the  user-defined  map 
function  for  each  record  in  the  split.  So  process  each  split  is  small  compared  to  the  time  to  process  the  whole  input. 
There we are processing the splits in parallel, the processing is better load balanced when the splits are small, since a 
faster machine will be able to process proportionally more splits over the course of the job than a slower machine. Even 
if  machine  are  identical,  failed  processes  or  other  jobs  running  concurrently  make  load  balancing  desirable,  and  the 
quality  of  the  load  balancing  increases  as  the  splits  become  more  fine-grained.  There  are  various  types  of  modular 
deficiency exist so require a robust framework that can discover deficiency, control, and revoke. 

II.  RELATED WORK 

 

Currently Hadoop is in initial phase of development many of companies participating in it, our literature also based on 
companies reports. Some of Hortonworks  [3] works with the Hadoop community to bring innovation to the platform, 
for the enterprise. Employees have collectively contributed more lines of code to Hadoop than any other company.  
Hortonworks have brought together a collection of resources that are of particular interest of developers, analyst, and 
system administration. Also provide tools and training and hadoop solution for business users, java developers, data 
analyst, data scientist and administrators. 
Security  is  a  top  agenda  item  and  represents  critical  requirements  for  Hadoop  projects.  Over  the  years,  Hadoop  has 
evolved  to  address  key  concerns  regarding  authentication,  authorization,  accounting,  and  data  protection  natively 
within  a  cluster  and  there  are  many  secure  Hadoop  clusters  in  production.  Hadoop  is  being  used  securely  and 
successfully  today  in  sensitive  financial  services  applications,  private  healthcare  initiatives  and  in  a  range  of  other 
security-sensitive environments. As enterprise adoption of Hadoop grows, so do the security concerns and a roadmap 
to embrace and incorporate these enterprise security features has emerged. [4] 
 

1.  Securing a Hadoop cluster today according to Hortonworks 

a.  Authentication verifies the identity of a system or user accessing the system 
b.  Authorization specifies access privileges for a user or system. 
c.  Accounting provides the ability to track resource use within a system. 
d.  Data  Protection  ensures  privacy  and  confidentiality  of  information.  Hadoop  and  HDP  allow  you  to 

protect data in motion. 

2.  Securing a Hadoop cluster tomorrow according to Hortonworks 

a.  Perimeter level Security with Apache Knox 
b. 

Improved Authentication 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3490 

                                                                                                               

 

          ISSN (Print):   2347-6710                                                                                                                                  

                

                      
                   
                  ISSN(Online): 2319-8753 
    

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

The Evolution of Big Data Security through 

Hadoop Incremental Security Model 

Vinod Sharma1, Prof. N.K. Joshi2 

Research Scholar, Department of Computer Science, Career Point University, Kota, Rajasthan, India1 

Director, Department of Computer Science, Modi Institute of Technology, Kota, Rajasthan, India2 

 

ABSTRACT:Data pours in millions of computers and millions of process every moment of every day so today is the 
era  of  Big  Data  where  data  interrelate  to  the  volume,  velocity,  and  variety  of  data  interrelate.  Huge  volume,  various 
varieties  and  high  velocity  create  lots  of  other  challenges  and  issues  regarding  its  management  and  processing.  Big 
Data enable any organization to collect, manage, analyze and making decision incredibly from large data sets. Big data 
is growing at an exponential rate but  security feature not growing at an same rate.so it becomes important to develop 
new technologies to deal with it securities. So require latest technology and moderate theory about data, other than the 
traditional tools and technique to manage it due its nature. 
This paper introduces the big data technology along with its importance in the modern world and existing projects like 
hadoop  which are effective and important in changing  the  concept of science into big science. Hadoop, Map Reduce 
and No SQL are the major big data technology. This paper also throws some light on other challenges and issues. The 
various  challenges  and  issues  in  adapting  and  accepting  Big  data  security  and  suggest  some  more  security  standards 
and concept that make robust hadoop ecosystem without any processing overhead. 
 
KEYWORDS:BigData, Hadoop, MapReduce, ABAC, RBAC. 

I.  INTRODUCTION 

 
Data pours in millions of computers and millions of process every moment of every day so today is the era of Big Data. 
Big  data  refers  to  technologies  that  involve  data  that  is  too  divers,  fast  changing  or  massive  for  conventional 
technologies,  skill  and  infrastructure  to  address  efficiently.  Said  differently  the  volume,  velocity,  and  variety  of  data 
interrelation is too great. Big Data enable any organization to data creation, collection, retrieval, manage, analyze and 
making decision that is remarkable in terms of volume, velocity, and variety. 
 

In Big Data 3 V’s are [2]. 
1. 

Volume: At present the data existing is in petabytes and is supposed to increase to zettabytes in nearby future. 
The social media, financial institution, medical institution, government, Sensors, Logs producing data in order 
of terabytes every day and this amount of data is definitely difficult to be handled using the existing traditional 
systems.. 
Velocity: At present data change rapidly through the archived data, legacy collections and from streamed data 
that  comes  from  multiple  resources  sensors,  traditional  file  records,  cellular  technology,  social  media  and 
many more. 

2. 

3.  Variety: At present data comes in different forms including data-streams, text, picture, audio, video, structured, 
semi  structured,  unstructured.  Unstructured  data  is  difficult  to  handle  with  traditional  tools  and  techniques. 
Thus  our  traditional  systems  are  not  capable  enough  on  performing  the  analytics  on  the  data  which  is 
constantly in motion. 
There  are  volume,  velocity  and  variety  are  main  concern  in  big  data  technology.  Some  other  issues  are  also 
considerable such as veracity, variability, complexity, Value. 

4. 

 

The efflux of Big Data and the need to move this information throughout an organization has created a massive new 
target  for  hackers  and  other  cybercriminal  activity.  Now  this  data  is  highly  valuable,  is  subject  to  privacy  laws  and 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3489 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

compliance  regulation,  and  must  be  protected.  Today  the  biggest  concerns  in  our  present  age  resolves  around  the 
security,  privacy  with  audit  access  control,  robustness,  reliability,  availability  and  protection  of  sensitive  information 
such as financial data, sensors information, medical records, and social information on the social networking. 
Big Data’s security  in this process is becoming increasingly  more important and same time organizations required to 
enforce  access  control  and  privacy  restrictions  on  these  data  sets  to  meet  regulatory  requirements  such  information 
privacy  laws.  Most  of  Network  security  breaches  from  internal  and  external  attackers  are  on  the  rise,  often  taking 
months to be detected, and those affected are paying the price. Organizations that have not properly controlled access to 
their data sets are facing lawsuits, negative publicity, and regulatory fines.  
 
Hadoop  is  the  core  platform  for  structuring  big  data,  and  solves  the  problem  of  making  it  useful  for  analytical  and 
operational purposes. Hadoop is an apache based open source software framework, comprised at its core of the hadoop 
file system and map reduce, and is very well designed to handle huge volumes of data across a large number of nodes. 
 
At a high level, hadoop leverages parallel processing across many commodity servers to respond to client applications. 
The  key  difference  is,  rather  than  only  looking  at  parallel  computing,  it  looks  at  parallelizing  the  data  access.  Map 
Reduce programming model provide divide and conquer based highly parallelizable and distributed algorithms across 
massive  data  sets  using  a  large  number  of  commodity  machines.  The  basic  idea  is  to  partition  a  large  problem  into 
smaller  independent  sub  problems  tackle  by  different  workers.  Fine  grained  Map  and  Reduce  task  provide  enhanced 
load  balancing  and  faster  recovery  from  failed  tasks.[6]  Hadoop  divides  the  input  to  a MapReduce  job  into  fixed-size 
pieces  called  input  splits,  or  just  splits  and  creates  one  map  task  for  each  split,  which  runs  the  user-defined  map 
function  for  each  record  in  the  split.  So  process  each  split  is  small  compared  to  the  time  to  process  the  whole  input. 
There we are processing the splits in parallel, the processing is better load balanced when the splits are small, since a 
faster machine will be able to process proportionally more splits over the course of the job than a slower machine. Even 
if  machine  are  identical,  failed  processes  or  other  jobs  running  concurrently  make  load  balancing  desirable,  and  the 
quality  of  the  load  balancing  increases  as  the  splits  become  more  fine-grained.  There  are  various  types  of  modular 
deficiency exist so require a robust framework that can discover deficiency, control, and revoke. 

II.  RELATED WORK 

 

Currently Hadoop is in initial phase of development many of companies participating in it, our literature also based on 
companies reports. Some of Hortonworks  [3] works with the Hadoop community to bring innovation to the platform, 
for the enterprise. Employees have collectively contributed more lines of code to Hadoop than any other company.  
Hortonworks have brought together a collection of resources that are of particular interest of developers, analyst, and 
system administration. Also provide tools and training and hadoop solution for business users, java developers, data 
analyst, data scientist and administrators. 
Security  is  a  top  agenda  item  and  represents  critical  requirements  for  Hadoop  projects.  Over  the  years,  Hadoop  has 
evolved  to  address  key  concerns  regarding  authentication,  authorization,  accounting,  and  data  protection  natively 
within  a  cluster  and  there  are  many  secure  Hadoop  clusters  in  production.  Hadoop  is  being  used  securely  and 
successfully  today  in  sensitive  financial  services  applications,  private  healthcare  initiatives  and  in  a  range  of  other 
security-sensitive environments. As enterprise adoption of Hadoop grows, so do the security concerns and a roadmap 
to embrace and incorporate these enterprise security features has emerged. [4] 
 

1.  Securing a Hadoop cluster today according to Hortonworks 

a.  Authentication verifies the identity of a system or user accessing the system 
b.  Authorization specifies access privileges for a user or system. 
c.  Accounting provides the ability to track resource use within a system. 
d.  Data  Protection  ensures  privacy  and  confidentiality  of  information.  Hadoop  and  HDP  allow  you  to 

protect data in motion. 

2.  Securing a Hadoop cluster tomorrow according to Hortonworks 

a.  Perimeter level Security with Apache Knox 
b. 

Improved Authentication 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3490 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

c.  Granular Authorization 
d.  Accounting & Audit 
e.  Protecting data with Encryption 

 
Hadoop is a secure system and offers key features for securely processing enterprise data. But the security work 
never ends. 

 
3.  According to IBM, Security within the hadoop today [5]: Hadoop supports strong security at the file system level. 
Recall that the Hadoop Distributed File System (HDFS) is implemented within another native file system (such 
as  the  third  extensible  file  system  [ext3]).  Access  controls  for  Hadoop  are  implemented  by  using  file-based 
permissions  that  follow  the  UNIX®  permissions  model.  Although  this  model  provides  file-level  permissions 
within the HDFS, it lacks more fine-grained access controls 

4.  Right now IBM associated following project with Hadoop Ecosystem 

a.  Sentry with HDFS, Hive, and Impala 
b.  Project Rhino provide multicomponent security 
c.  Apache Knox Gateway 
d.  Delegations Tokens 

 
As an example, consider a file within the HDFS that contains movie reviews for a set of users. This data consists of a 
user  ID,  zip  code,  gender,  age,  movie  title,  and  review.  In  Hadoop,  access  is  an  all-or  nothing  model.  If  you  can 
access the file using the permissions model, you can access all fields  within  the  file.  What's  needed  is  a  more  fine-
grained model of access. Where more secure access is granted to all data within the file, lower security access could 
be provided for individual fields of the   data  (such  as  all  data  except  the  user  ID  and  zip  code).  Lower  security 
access minimizes the possibility of leaking user information, and the role-based access of individual fields makes it 
possible to restrict access within files instead of all-or-nothing file access.[7] 

 
The overall problem of data security within Hadoop becomes even more difficult when you consider its implementation. 
Hadoop,  and  its  underlying  file  system,  is  a  complex  distributed  system  with  many  points  of  contact.  Given  its 
complexity  and  scale,  the  application  of  security  to  this  system  is  a  challenge  by  itself.  Any  security  implementation 
must integrate with the overall architecture to ensure proper security coverage 

III. SECURITY AND PRIVACY ISSUES 

 
There  we apply  some security  concept over  hadoop ecosystem  and  mainly  in data  processing job. But  first of all 

consider following cases and use of this incremental security process in following condition. 
There are some following cases due to security breaches. 

1.  Case 1: 

The  2006  incident,  known  as  the  Data  Valdez  [4],  occurred  when  employees  at  AOL  posted  three  months' 
worth of search queries from 650,000 members. AOL employees did so for research purposes, and took steps 
to "anonymize" the members. AOL made the data available for several weeks on the site research.aol.com. By 
the  time  the  company  realized  the  privacy  implications  and  pulled  the  material,  the  data  had  already  been 
downloaded by third parties and  made available on  mirror sites. It’s  not  yet clear how  many  AOL  members 
will  submit  claims  --  especially  because  many  users  don't  know  whether  their  search  queries  were  publicly 
released.  The  settlement  notice  itself  states  there  is  no  way  for  people  to  determine  whether  their  data  was 
published, based on their usernames. 
Hadoop Incremental Security Model provide authorization, authentication and control with encryption using a 
policy that consider right user meet with its regulatory data. 

2.  Case 2: 

In 2006, Netflix offered a $1 million prize for a 10 percent improvement in its movie recommendation system, 
and released an “anonymized” training data set of the movie viewing history of half a million subscribers so 
that developers participating in the contest would have some data to use for the contest. This data set had the 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3491 

                                                                                                               

 

          ISSN (Print):   2347-6710                                                                                                                                  

                

                      
                   
                  ISSN(Online): 2319-8753 
    

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

The Evolution of Big Data Security through 

Hadoop Incremental Security Model 

Vinod Sharma1, Prof. N.K. Joshi2 

Research Scholar, Department of Computer Science, Career Point University, Kota, Rajasthan, India1 

Director, Department of Computer Science, Modi Institute of Technology, Kota, Rajasthan, India2 

 

ABSTRACT:Data pours in millions of computers and millions of process every moment of every day so today is the 
era  of  Big  Data  where  data  interrelate  to  the  volume,  velocity,  and  variety  of  data  interrelate.  Huge  volume,  various 
varieties  and  high  velocity  create  lots  of  other  challenges  and  issues  regarding  its  management  and  processing.  Big 
Data enable any organization to collect, manage, analyze and making decision incredibly from large data sets. Big data 
is growing at an exponential rate but  security feature not growing at an same rate.so it becomes important to develop 
new technologies to deal with it securities. So require latest technology and moderate theory about data, other than the 
traditional tools and technique to manage it due its nature. 
This paper introduces the big data technology along with its importance in the modern world and existing projects like 
hadoop  which are effective and important in changing  the  concept of science into big science. Hadoop, Map Reduce 
and No SQL are the major big data technology. This paper also throws some light on other challenges and issues. The 
various  challenges  and  issues  in  adapting  and  accepting  Big  data  security  and  suggest  some  more  security  standards 
and concept that make robust hadoop ecosystem without any processing overhead. 
 
KEYWORDS:BigData, Hadoop, MapReduce, ABAC, RBAC. 

I.  INTRODUCTION 

 
Data pours in millions of computers and millions of process every moment of every day so today is the era of Big Data. 
Big  data  refers  to  technologies  that  involve  data  that  is  too  divers,  fast  changing  or  massive  for  conventional 
technologies,  skill  and  infrastructure  to  address  efficiently.  Said  differently  the  volume,  velocity,  and  variety  of  data 
interrelation is too great. Big Data enable any organization to data creation, collection, retrieval, manage, analyze and 
making decision that is remarkable in terms of volume, velocity, and variety. 
 

In Big Data 3 V’s are [2]. 
1. 

Volume: At present the data existing is in petabytes and is supposed to increase to zettabytes in nearby future. 
The social media, financial institution, medical institution, government, Sensors, Logs producing data in order 
of terabytes every day and this amount of data is definitely difficult to be handled using the existing traditional 
systems.. 
Velocity: At present data change rapidly through the archived data, legacy collections and from streamed data 
that  comes  from  multiple  resources  sensors,  traditional  file  records,  cellular  technology,  social  media  and 
many more. 

2. 

3.  Variety: At present data comes in different forms including data-streams, text, picture, audio, video, structured, 
semi  structured,  unstructured.  Unstructured  data  is  difficult  to  handle  with  traditional  tools  and  techniques. 
Thus  our  traditional  systems  are  not  capable  enough  on  performing  the  analytics  on  the  data  which  is 
constantly in motion. 
There  are  volume,  velocity  and  variety  are  main  concern  in  big  data  technology.  Some  other  issues  are  also 
considerable such as veracity, variability, complexity, Value. 

4. 

 

The efflux of Big Data and the need to move this information throughout an organization has created a massive new 
target  for  hackers  and  other  cybercriminal  activity.  Now  this  data  is  highly  valuable,  is  subject  to  privacy  laws  and 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3489 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

compliance  regulation,  and  must  be  protected.  Today  the  biggest  concerns  in  our  present  age  resolves  around  the 
security,  privacy  with  audit  access  control,  robustness,  reliability,  availability  and  protection  of  sensitive  information 
such as financial data, sensors information, medical records, and social information on the social networking. 
Big Data’s security  in this process is becoming increasingly  more important and same time organizations required to 
enforce  access  control  and  privacy  restrictions  on  these  data  sets  to  meet  regulatory  requirements  such  information 
privacy  laws.  Most  of  Network  security  breaches  from  internal  and  external  attackers  are  on  the  rise,  often  taking 
months to be detected, and those affected are paying the price. Organizations that have not properly controlled access to 
their data sets are facing lawsuits, negative publicity, and regulatory fines.  
 
Hadoop  is  the  core  platform  for  structuring  big  data,  and  solves  the  problem  of  making  it  useful  for  analytical  and 
operational purposes. Hadoop is an apache based open source software framework, comprised at its core of the hadoop 
file system and map reduce, and is very well designed to handle huge volumes of data across a large number of nodes. 
 
At a high level, hadoop leverages parallel processing across many commodity servers to respond to client applications. 
The  key  difference  is,  rather  than  only  looking  at  parallel  computing,  it  looks  at  parallelizing  the  data  access.  Map 
Reduce programming model provide divide and conquer based highly parallelizable and distributed algorithms across 
massive  data  sets  using  a  large  number  of  commodity  machines.  The  basic  idea  is  to  partition  a  large  problem  into 
smaller  independent  sub  problems  tackle  by  different  workers.  Fine  grained  Map  and  Reduce  task  provide  enhanced 
load  balancing  and  faster  recovery  from  failed  tasks.[6]  Hadoop  divides  the  input  to  a MapReduce  job  into  fixed-size 
pieces  called  input  splits,  or  just  splits  and  creates  one  map  task  for  each  split,  which  runs  the  user-defined  map 
function  for  each  record  in  the  split.  So  process  each  split  is  small  compared  to  the  time  to  process  the  whole  input. 
There we are processing the splits in parallel, the processing is better load balanced when the splits are small, since a 
faster machine will be able to process proportionally more splits over the course of the job than a slower machine. Even 
if  machine  are  identical,  failed  processes  or  other  jobs  running  concurrently  make  load  balancing  desirable,  and  the 
quality  of  the  load  balancing  increases  as  the  splits  become  more  fine-grained.  There  are  various  types  of  modular 
deficiency exist so require a robust framework that can discover deficiency, control, and revoke. 

II.  RELATED WORK 

 

Currently Hadoop is in initial phase of development many of companies participating in it, our literature also based on 
companies reports. Some of Hortonworks  [3] works with the Hadoop community to bring innovation to the platform, 
for the enterprise. Employees have collectively contributed more lines of code to Hadoop than any other company.  
Hortonworks have brought together a collection of resources that are of particular interest of developers, analyst, and 
system administration. Also provide tools and training and hadoop solution for business users, java developers, data 
analyst, data scientist and administrators. 
Security  is  a  top  agenda  item  and  represents  critical  requirements  for  Hadoop  projects.  Over  the  years,  Hadoop  has 
evolved  to  address  key  concerns  regarding  authentication,  authorization,  accounting,  and  data  protection  natively 
within  a  cluster  and  there  are  many  secure  Hadoop  clusters  in  production.  Hadoop  is  being  used  securely  and 
successfully  today  in  sensitive  financial  services  applications,  private  healthcare  initiatives  and  in  a  range  of  other 
security-sensitive environments. As enterprise adoption of Hadoop grows, so do the security concerns and a roadmap 
to embrace and incorporate these enterprise security features has emerged. [4] 
 

1.  Securing a Hadoop cluster today according to Hortonworks 

a.  Authentication verifies the identity of a system or user accessing the system 
b.  Authorization specifies access privileges for a user or system. 
c.  Accounting provides the ability to track resource use within a system. 
d.  Data  Protection  ensures  privacy  and  confidentiality  of  information.  Hadoop  and  HDP  allow  you  to 

protect data in motion. 

2.  Securing a Hadoop cluster tomorrow according to Hortonworks 

a.  Perimeter level Security with Apache Knox 
b. 

Improved Authentication 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3490 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

c.  Granular Authorization 
d.  Accounting & Audit 
e.  Protecting data with Encryption 

 
Hadoop is a secure system and offers key features for securely processing enterprise data. But the security work 
never ends. 

 
3.  According to IBM, Security within the hadoop today [5]: Hadoop supports strong security at the file system level. 
Recall that the Hadoop Distributed File System (HDFS) is implemented within another native file system (such 
as  the  third  extensible  file  system  [ext3]).  Access  controls  for  Hadoop  are  implemented  by  using  file-based 
permissions  that  follow  the  UNIX®  permissions  model.  Although  this  model  provides  file-level  permissions 
within the HDFS, it lacks more fine-grained access controls 

4.  Right now IBM associated following project with Hadoop Ecosystem 

a.  Sentry with HDFS, Hive, and Impala 
b.  Project Rhino provide multicomponent security 
c.  Apache Knox Gateway 
d.  Delegations Tokens 

 
As an example, consider a file within the HDFS that contains movie reviews for a set of users. This data consists of a 
user  ID,  zip  code,  gender,  age,  movie  title,  and  review.  In  Hadoop,  access  is  an  all-or  nothing  model.  If  you  can 
access the file using the permissions model, you can access all fields  within  the  file.  What's  needed  is  a  more  fine-
grained model of access. Where more secure access is granted to all data within the file, lower security access could 
be provided for individual fields of the   data  (such  as  all  data  except  the  user  ID  and  zip  code).  Lower  security 
access minimizes the possibility of leaking user information, and the role-based access of individual fields makes it 
possible to restrict access within files instead of all-or-nothing file access.[7] 

 
The overall problem of data security within Hadoop becomes even more difficult when you consider its implementation. 
Hadoop,  and  its  underlying  file  system,  is  a  complex  distributed  system  with  many  points  of  contact.  Given  its 
complexity  and  scale,  the  application  of  security  to  this  system  is  a  challenge  by  itself.  Any  security  implementation 
must integrate with the overall architecture to ensure proper security coverage 

III. SECURITY AND PRIVACY ISSUES 

 
There  we apply  some security  concept over  hadoop ecosystem  and  mainly  in data  processing job. But  first of all 

consider following cases and use of this incremental security process in following condition. 
There are some following cases due to security breaches. 

1.  Case 1: 

The  2006  incident,  known  as  the  Data  Valdez  [4],  occurred  when  employees  at  AOL  posted  three  months' 
worth of search queries from 650,000 members. AOL employees did so for research purposes, and took steps 
to "anonymize" the members. AOL made the data available for several weeks on the site research.aol.com. By 
the  time  the  company  realized  the  privacy  implications  and  pulled  the  material,  the  data  had  already  been 
downloaded by third parties and  made available on  mirror sites. It’s  not  yet clear how  many  AOL  members 
will  submit  claims  --  especially  because  many  users  don't  know  whether  their  search  queries  were  publicly 
released.  The  settlement  notice  itself  states  there  is  no  way  for  people  to  determine  whether  their  data  was 
published, based on their usernames. 
Hadoop Incremental Security Model provide authorization, authentication and control with encryption using a 
policy that consider right user meet with its regulatory data. 

2.  Case 2: 

In 2006, Netflix offered a $1 million prize for a 10 percent improvement in its movie recommendation system, 
and released an “anonymized” training data set of the movie viewing history of half a million subscribers so 
that developers participating in the contest would have some data to use for the contest. This data set had the 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3491 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

ratings  of  movies  that  the  Netflix  subscribers  had  watched,  with  all  personally  identifying  information 
removed.Netflix  pays  $9M  to  settle  user-data  misuse  charges;  aims  to  misuse  more  data  with  Facebook 
[6].Netflix accounts that the video-streaming company had kept copies of their personal information and rental 
history  from  accounts  that  had  been  closed  long  before.Retaining  data  on  individual  users,  as  well  as 
anonymized aggregations of data showing user behaviour, makes it easier to recreate recommendation lists for 
customers  returning  to  the  service  after  having  closed  previous  accounts.  Though  restrictions  on  the  type  of 
data  a  service  company  can  keep  and  the  length  of  time  it  can  retain  personally  identifiable  records  could 
cause  endless  trouble  for  non-video-rental  companies  such  as  Facebook,  they  are  currently  keeping  Netflix 
itself away from Facebook. 
Hadoop  Incremental  Security  Model  concern  all  data  and  its  accessibility  and  use  of  sensitive  information 
over the system with auditing of the track data provenance. 

3.  Case 3: 

Two  researchers,  Dr.Arvind  Narayanan  and  Dr.VitalyShmatikov  from  the  University  of  Texas  at  Austin, 
linked together the Netflix data set with the Internet Movie Database (IMDB) review database, applying a new 
“de-anonymization  algorithm.”[7]  They  published  a  research  paper  showing  that  they  could  mathematically 
identify  many  of  the  users  in  the  released  Netflix  data  set.  Based  on  a  user’s  IMDB  ratings  of  just  a  few 
movies,  the  researchers  showed  that  their  algorithm  could  personally  identify  the  same  individuals  in  the 
Netflix  data  set  to  find  the  Netflix  subscriber’s  entire  movie  viewing  history  prior  to  2005,  resulting  in 
potential revelations related to the subscriber’s religious beliefs, sexuality, and political leanings. As a result, a 
Netflix  subscriber  filed  a  lawsuit  against  Netflix,  claiming  that  its  release  of  their  data  violated  the  Video 
Protection Privacy Act (VPPA) and “outed” her as a lesbian. Netflix settled the lawsuit for $9 million in 2010. 

Hadoop  Incremental  Security  Model  revoke  this  types  of  activity  that  based  on  cross  domain  and  retain  all 
information from the server also check third party authentication from ABAC or RBAC. 

 

 
Hadoop Incremental Security consider following 

IV. INCREMENTAL SECURITY MODEL 

1.  Access Control by Attribute Based Access Control (ABAC)  [1] or Role Based Access Control (RBAC)  [9] for 

access, modify and control jobs or precise data access.  

2.  Encryption of data in transit and rest state.  
3.  Accountable Audit of the events and track of data provenance.  
4.  Compliance Assurance for storing sensitive and non-sensitive without replication.  
5.  Broad usage that cover foundation of concurrency, authentication and authorization.  
6.  Easier Administration that based on functional role with appropriate access control.  
7.  Cleansing/Sanitization/Destruction. 
8.  Data ingest: Data ingestion is the process of importing, extracting and processing data for later use or storage 
in  a  database. This  process  often  involves  altering  individual  files  by  editing  their  content  and/or  formatting 
them to fit into a larger document that begins by validating the individual files, then prioritize, the source for 
optimal processing and validate results. 

V.  CONCLUSION  

 

This paper described the new concept of big data, its importance and the existing projects. To accept and adapt to 
this  new  technology  many  challenges  and  security  issues  exist  which  need  to  be  brought  up  right  in  the  beginning 
before it is too late. All those issues and challenges have been described in this paper. These challenges and issues will 
help the business organizations  which are  moving towards this technology for increasing the value of the business to 
consider them right in the beginning and to find the ways to protest them. 

Hadoop, the system and its usage grew over the last decade. The early experiment use did not require security. Now 
security  became  critical  issue  in  current  scenario.  As  a  result,  security  was  recently  added  to  Hadoop  in  spite  of  the 
axiom that states it is best to design and implement security in from the beginning. 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3492 

                                                                                                               

 

          ISSN (Print):   2347-6710                                                                                                                                  

                

                      
                   
                  ISSN(Online): 2319-8753 
    

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

The Evolution of Big Data Security through 

Hadoop Incremental Security Model 

Vinod Sharma1, Prof. N.K. Joshi2 

Research Scholar, Department of Computer Science, Career Point University, Kota, Rajasthan, India1 

Director, Department of Computer Science, Modi Institute of Technology, Kota, Rajasthan, India2 

 

ABSTRACT:Data pours in millions of computers and millions of process every moment of every day so today is the 
era  of  Big  Data  where  data  interrelate  to  the  volume,  velocity,  and  variety  of  data  interrelate.  Huge  volume,  various 
varieties  and  high  velocity  create  lots  of  other  challenges  and  issues  regarding  its  management  and  processing.  Big 
Data enable any organization to collect, manage, analyze and making decision incredibly from large data sets. Big data 
is growing at an exponential rate but  security feature not growing at an same rate.so it becomes important to develop 
new technologies to deal with it securities. So require latest technology and moderate theory about data, other than the 
traditional tools and technique to manage it due its nature. 
This paper introduces the big data technology along with its importance in the modern world and existing projects like 
hadoop  which are effective and important in changing  the  concept of science into big science. Hadoop, Map Reduce 
and No SQL are the major big data technology. This paper also throws some light on other challenges and issues. The 
various  challenges  and  issues  in  adapting  and  accepting  Big  data  security  and  suggest  some  more  security  standards 
and concept that make robust hadoop ecosystem without any processing overhead. 
 
KEYWORDS:BigData, Hadoop, MapReduce, ABAC, RBAC. 

I.  INTRODUCTION 

 
Data pours in millions of computers and millions of process every moment of every day so today is the era of Big Data. 
Big  data  refers  to  technologies  that  involve  data  that  is  too  divers,  fast  changing  or  massive  for  conventional 
technologies,  skill  and  infrastructure  to  address  efficiently.  Said  differently  the  volume,  velocity,  and  variety  of  data 
interrelation is too great. Big Data enable any organization to data creation, collection, retrieval, manage, analyze and 
making decision that is remarkable in terms of volume, velocity, and variety. 
 

In Big Data 3 V’s are [2]. 
1. 

Volume: At present the data existing is in petabytes and is supposed to increase to zettabytes in nearby future. 
The social media, financial institution, medical institution, government, Sensors, Logs producing data in order 
of terabytes every day and this amount of data is definitely difficult to be handled using the existing traditional 
systems.. 
Velocity: At present data change rapidly through the archived data, legacy collections and from streamed data 
that  comes  from  multiple  resources  sensors,  traditional  file  records,  cellular  technology,  social  media  and 
many more. 

2. 

3.  Variety: At present data comes in different forms including data-streams, text, picture, audio, video, structured, 
semi  structured,  unstructured.  Unstructured  data  is  difficult  to  handle  with  traditional  tools  and  techniques. 
Thus  our  traditional  systems  are  not  capable  enough  on  performing  the  analytics  on  the  data  which  is 
constantly in motion. 
There  are  volume,  velocity  and  variety  are  main  concern  in  big  data  technology.  Some  other  issues  are  also 
considerable such as veracity, variability, complexity, Value. 

4. 

 

The efflux of Big Data and the need to move this information throughout an organization has created a massive new 
target  for  hackers  and  other  cybercriminal  activity.  Now  this  data  is  highly  valuable,  is  subject  to  privacy  laws  and 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3489 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

compliance  regulation,  and  must  be  protected.  Today  the  biggest  concerns  in  our  present  age  resolves  around  the 
security,  privacy  with  audit  access  control,  robustness,  reliability,  availability  and  protection  of  sensitive  information 
such as financial data, sensors information, medical records, and social information on the social networking. 
Big Data’s security  in this process is becoming increasingly  more important and same time organizations required to 
enforce  access  control  and  privacy  restrictions  on  these  data  sets  to  meet  regulatory  requirements  such  information 
privacy  laws.  Most  of  Network  security  breaches  from  internal  and  external  attackers  are  on  the  rise,  often  taking 
months to be detected, and those affected are paying the price. Organizations that have not properly controlled access to 
their data sets are facing lawsuits, negative publicity, and regulatory fines.  
 
Hadoop  is  the  core  platform  for  structuring  big  data,  and  solves  the  problem  of  making  it  useful  for  analytical  and 
operational purposes. Hadoop is an apache based open source software framework, comprised at its core of the hadoop 
file system and map reduce, and is very well designed to handle huge volumes of data across a large number of nodes. 
 
At a high level, hadoop leverages parallel processing across many commodity servers to respond to client applications. 
The  key  difference  is,  rather  than  only  looking  at  parallel  computing,  it  looks  at  parallelizing  the  data  access.  Map 
Reduce programming model provide divide and conquer based highly parallelizable and distributed algorithms across 
massive  data  sets  using  a  large  number  of  commodity  machines.  The  basic  idea  is  to  partition  a  large  problem  into 
smaller  independent  sub  problems  tackle  by  different  workers.  Fine  grained  Map  and  Reduce  task  provide  enhanced 
load  balancing  and  faster  recovery  from  failed  tasks.[6]  Hadoop  divides  the  input  to  a MapReduce  job  into  fixed-size 
pieces  called  input  splits,  or  just  splits  and  creates  one  map  task  for  each  split,  which  runs  the  user-defined  map 
function  for  each  record  in  the  split.  So  process  each  split  is  small  compared  to  the  time  to  process  the  whole  input. 
There we are processing the splits in parallel, the processing is better load balanced when the splits are small, since a 
faster machine will be able to process proportionally more splits over the course of the job than a slower machine. Even 
if  machine  are  identical,  failed  processes  or  other  jobs  running  concurrently  make  load  balancing  desirable,  and  the 
quality  of  the  load  balancing  increases  as  the  splits  become  more  fine-grained.  There  are  various  types  of  modular 
deficiency exist so require a robust framework that can discover deficiency, control, and revoke. 

II.  RELATED WORK 

 

Currently Hadoop is in initial phase of development many of companies participating in it, our literature also based on 
companies reports. Some of Hortonworks  [3] works with the Hadoop community to bring innovation to the platform, 
for the enterprise. Employees have collectively contributed more lines of code to Hadoop than any other company.  
Hortonworks have brought together a collection of resources that are of particular interest of developers, analyst, and 
system administration. Also provide tools and training and hadoop solution for business users, java developers, data 
analyst, data scientist and administrators. 
Security  is  a  top  agenda  item  and  represents  critical  requirements  for  Hadoop  projects.  Over  the  years,  Hadoop  has 
evolved  to  address  key  concerns  regarding  authentication,  authorization,  accounting,  and  data  protection  natively 
within  a  cluster  and  there  are  many  secure  Hadoop  clusters  in  production.  Hadoop  is  being  used  securely  and 
successfully  today  in  sensitive  financial  services  applications,  private  healthcare  initiatives  and  in  a  range  of  other 
security-sensitive environments. As enterprise adoption of Hadoop grows, so do the security concerns and a roadmap 
to embrace and incorporate these enterprise security features has emerged. [4] 
 

1.  Securing a Hadoop cluster today according to Hortonworks 

a.  Authentication verifies the identity of a system or user accessing the system 
b.  Authorization specifies access privileges for a user or system. 
c.  Accounting provides the ability to track resource use within a system. 
d.  Data  Protection  ensures  privacy  and  confidentiality  of  information.  Hadoop  and  HDP  allow  you  to 

protect data in motion. 

2.  Securing a Hadoop cluster tomorrow according to Hortonworks 

a.  Perimeter level Security with Apache Knox 
b. 

Improved Authentication 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3490 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

c.  Granular Authorization 
d.  Accounting & Audit 
e.  Protecting data with Encryption 

 
Hadoop is a secure system and offers key features for securely processing enterprise data. But the security work 
never ends. 

 
3.  According to IBM, Security within the hadoop today [5]: Hadoop supports strong security at the file system level. 
Recall that the Hadoop Distributed File System (HDFS) is implemented within another native file system (such 
as  the  third  extensible  file  system  [ext3]).  Access  controls  for  Hadoop  are  implemented  by  using  file-based 
permissions  that  follow  the  UNIX®  permissions  model.  Although  this  model  provides  file-level  permissions 
within the HDFS, it lacks more fine-grained access controls 

4.  Right now IBM associated following project with Hadoop Ecosystem 

a.  Sentry with HDFS, Hive, and Impala 
b.  Project Rhino provide multicomponent security 
c.  Apache Knox Gateway 
d.  Delegations Tokens 

 
As an example, consider a file within the HDFS that contains movie reviews for a set of users. This data consists of a 
user  ID,  zip  code,  gender,  age,  movie  title,  and  review.  In  Hadoop,  access  is  an  all-or  nothing  model.  If  you  can 
access the file using the permissions model, you can access all fields  within  the  file.  What's  needed  is  a  more  fine-
grained model of access. Where more secure access is granted to all data within the file, lower security access could 
be provided for individual fields of the   data  (such  as  all  data  except  the  user  ID  and  zip  code).  Lower  security 
access minimizes the possibility of leaking user information, and the role-based access of individual fields makes it 
possible to restrict access within files instead of all-or-nothing file access.[7] 

 
The overall problem of data security within Hadoop becomes even more difficult when you consider its implementation. 
Hadoop,  and  its  underlying  file  system,  is  a  complex  distributed  system  with  many  points  of  contact.  Given  its 
complexity  and  scale,  the  application  of  security  to  this  system  is  a  challenge  by  itself.  Any  security  implementation 
must integrate with the overall architecture to ensure proper security coverage 

III. SECURITY AND PRIVACY ISSUES 

 
There  we apply  some security  concept over  hadoop ecosystem  and  mainly  in data  processing job. But  first of all 

consider following cases and use of this incremental security process in following condition. 
There are some following cases due to security breaches. 

1.  Case 1: 

The  2006  incident,  known  as  the  Data  Valdez  [4],  occurred  when  employees  at  AOL  posted  three  months' 
worth of search queries from 650,000 members. AOL employees did so for research purposes, and took steps 
to "anonymize" the members. AOL made the data available for several weeks on the site research.aol.com. By 
the  time  the  company  realized  the  privacy  implications  and  pulled  the  material,  the  data  had  already  been 
downloaded by third parties and  made available on  mirror sites. It’s  not  yet clear how  many  AOL  members 
will  submit  claims  --  especially  because  many  users  don't  know  whether  their  search  queries  were  publicly 
released.  The  settlement  notice  itself  states  there  is  no  way  for  people  to  determine  whether  their  data  was 
published, based on their usernames. 
Hadoop Incremental Security Model provide authorization, authentication and control with encryption using a 
policy that consider right user meet with its regulatory data. 

2.  Case 2: 

In 2006, Netflix offered a $1 million prize for a 10 percent improvement in its movie recommendation system, 
and released an “anonymized” training data set of the movie viewing history of half a million subscribers so 
that developers participating in the contest would have some data to use for the contest. This data set had the 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3491 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

ratings  of  movies  that  the  Netflix  subscribers  had  watched,  with  all  personally  identifying  information 
removed.Netflix  pays  $9M  to  settle  user-data  misuse  charges;  aims  to  misuse  more  data  with  Facebook 
[6].Netflix accounts that the video-streaming company had kept copies of their personal information and rental 
history  from  accounts  that  had  been  closed  long  before.Retaining  data  on  individual  users,  as  well  as 
anonymized aggregations of data showing user behaviour, makes it easier to recreate recommendation lists for 
customers  returning  to  the  service  after  having  closed  previous  accounts.  Though  restrictions  on  the  type  of 
data  a  service  company  can  keep  and  the  length  of  time  it  can  retain  personally  identifiable  records  could 
cause  endless  trouble  for  non-video-rental  companies  such  as  Facebook,  they  are  currently  keeping  Netflix 
itself away from Facebook. 
Hadoop  Incremental  Security  Model  concern  all  data  and  its  accessibility  and  use  of  sensitive  information 
over the system with auditing of the track data provenance. 

3.  Case 3: 

Two  researchers,  Dr.Arvind  Narayanan  and  Dr.VitalyShmatikov  from  the  University  of  Texas  at  Austin, 
linked together the Netflix data set with the Internet Movie Database (IMDB) review database, applying a new 
“de-anonymization  algorithm.”[7]  They  published  a  research  paper  showing  that  they  could  mathematically 
identify  many  of  the  users  in  the  released  Netflix  data  set.  Based  on  a  user’s  IMDB  ratings  of  just  a  few 
movies,  the  researchers  showed  that  their  algorithm  could  personally  identify  the  same  individuals  in  the 
Netflix  data  set  to  find  the  Netflix  subscriber’s  entire  movie  viewing  history  prior  to  2005,  resulting  in 
potential revelations related to the subscriber’s religious beliefs, sexuality, and political leanings. As a result, a 
Netflix  subscriber  filed  a  lawsuit  against  Netflix,  claiming  that  its  release  of  their  data  violated  the  Video 
Protection Privacy Act (VPPA) and “outed” her as a lesbian. Netflix settled the lawsuit for $9 million in 2010. 

Hadoop  Incremental  Security  Model  revoke  this  types  of  activity  that  based  on  cross  domain  and  retain  all 
information from the server also check third party authentication from ABAC or RBAC. 

 

 
Hadoop Incremental Security consider following 

IV. INCREMENTAL SECURITY MODEL 

1.  Access Control by Attribute Based Access Control (ABAC)  [1] or Role Based Access Control (RBAC)  [9] for 

access, modify and control jobs or precise data access.  

2.  Encryption of data in transit and rest state.  
3.  Accountable Audit of the events and track of data provenance.  
4.  Compliance Assurance for storing sensitive and non-sensitive without replication.  
5.  Broad usage that cover foundation of concurrency, authentication and authorization.  
6.  Easier Administration that based on functional role with appropriate access control.  
7.  Cleansing/Sanitization/Destruction. 
8.  Data ingest: Data ingestion is the process of importing, extracting and processing data for later use or storage 
in  a  database. This  process  often  involves  altering  individual  files  by  editing  their  content  and/or  formatting 
them to fit into a larger document that begins by validating the individual files, then prioritize, the source for 
optimal processing and validate results. 

V.  CONCLUSION  

 

This paper described the new concept of big data, its importance and the existing projects. To accept and adapt to 
this  new  technology  many  challenges  and  security  issues  exist  which  need  to  be  brought  up  right  in  the  beginning 
before it is too late. All those issues and challenges have been described in this paper. These challenges and issues will 
help the business organizations  which are  moving towards this technology for increasing the value of the business to 
consider them right in the beginning and to find the ways to protest them. 

Hadoop, the system and its usage grew over the last decade. The early experiment use did not require security. Now 
security  became  critical  issue  in  current  scenario.  As  a  result,  security  was  recently  added  to  Hadoop  in  spite  of  the 
axiom that states it is best to design and implement security in from the beginning. 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3492 

                

                      
                   
                  ISSN(Online): 2319-8753 
    

          ISSN (Print):   2347-6710                                                                                                                                  

                                                                                                               

 

International Journal of Innovative Research in Science, 

Engineering and Technology 

(An ISO 3297: 2007 Certified Organization) 

Vol. 4, Issue 5, May 2015 

 

REFERENCES 

[1]  Computer 

Security  Division  Computer 

Security  Resource  Center 

(CSRC), 

"Attribute  Based  Access  Control 

(ABAC)",http://csrc.nist.gov/projects/abac/, 2015 

[2]  Bermen, Jules J. “Principle of Big Data”, Morgan Kaufmann, Waltham, 2013 
[3]  Das D., & O'Malley O.,  Security for Enterprise Hadoop Webpage, http://hortonworks.com/labs/security/, 2011 
[4]  Davis  W.,  AOL  Settles  Data  Valdez  Lawsuit  For  $5  Million  Page,  http://www.mediapost.com/publications/article/193831/aol-settles-data-

[5] 
[6] 

S.,  A 

valdez-lawsuit-for-5-million.html , 2013 
Jones M. T., Hadoop data security and Sentry, http://www.ibm.com/developerworks/security/library/se-hadoop/index.html 2014 
LOHR 
for  Netflix, 
http://www.nytimes.com/2009/09/22/technology/internet/22netflix.html 2009 
Lemos R., Researchers reverse Netflix anonymization Webpage, http://www.securityfocus.com/news/11497/1 2007 

$1  Million  Research  Bargain 

[7] 
[8]  Roy, I., Setty, S. T. V. S. T. V, Kilzer, A., Shmatikov, V., &Witchel, E., Airavat: Security and privacy for MapReduce. In Proceedings of the 

for  Others  Webpage, 

and  Maybe 

a  Model 

7th USENIX conference on Networked systems design and implementation (pp. 20–20). http://doi.org/10.1.1.149.25332010 
[9]  Understanding Role Based Access Control, http://technet.microsoft.com/en-us/library/dd298183%28v=exchg.150%29.aspx 
[10]  Zettaset, The Big Data Security Gap : Protecting the Hadoop Cluster. (n.d.).,2014 
[11]  J. Singh, “Big Data : Tools and Technologies in Big Data,” vol. 112, no. 15, pp. 6–10, 2015 

Copyright to IJIRSET                                                       DOI: 10.15680/IJIRSET.2015.0405097                                                   3493 

